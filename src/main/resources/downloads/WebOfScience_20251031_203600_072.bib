
@article{ WOS:000358218600040,
Author = {Jordan, M. I. and Mitchell, T. M.},
Title = {Machine learning: Trends, perspectives, and prospects},
Journal = {SCIENCE},
Year = {2015},
Volume = {349},
Number = {6245, SI},
Pages = {255-260},
Month = {JUL 17},
Abstract = {Machine learning addresses the question of how to build computers that
   improve automatically through experience. It is one of today's most
   rapidly growing technical fields, lying at the intersection of computer
   science and statistics, and at the core of artificial intelligence and
   data science. Recent progress in machine learning has been driven both
   by the development of new learning algorithms and theory and by the
   ongoing explosion in the availability of online data and low-cost
   computation. The adoption of data-intensive machine-learning methods can
   be found throughout science, technology and commerce, leading to more
   evidence-based decision-making across many walks of life, including
   health care, manufacturing, education, financial modeling, policing, and
   marketing.},
DOI = {10.1126/science.aaa8415},
ISSN = {0036-8075},
EISSN = {1095-9203},
ResearcherID-Numbers = {Jordan, Michael/C-5253-2013},
Unique-ID = {WOS:000358218600040},
}

@article{ WOS:000439850800044,
Author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev,
   Olexandr and Walsh, Aron},
Title = {Machine learning for molecular and materials science},
Journal = {NATURE},
Year = {2018},
Volume = {559},
Number = {7715},
Pages = {547-555},
Month = {JUL 26},
Abstract = {Here we summarize recent progress in machine learning for the chemical
   sciences. We outline machine-learning techniques that are suitable for
   addressing research questions in this domain, as well as future
   directions for the field. We envisage a future in which the design,
   synthesis, characterization and application of molecules and materials
   is accelerated by artificial intelligence.},
DOI = {10.1038/s41586-018-0337-2},
ISSN = {0028-0836},
EISSN = {1476-4687},
ResearcherID-Numbers = {Walsh, Aron/A-7843-2008
   Isayev, Olexandr/B-7944-2008
   },
ORCID-Numbers = {Butler, Keith/0000-0001-5432-5597
   Walsh, Aron/0000-0001-5460-7033
   Isayev, Olexandr/0000-0001-7581-8497
   Cartwright, Hugh/0000-0003-0949-2657},
Unique-ID = {WOS:000439850800044},
}

@article{ WOS:001172157700001,
Author = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and
   Fragulis, George F.},
Title = {A Review of Machine Learning and Deep Learning for Object Detection,
   Semantic Segmentation, and Human Action Recognition in Machine and
   Robotic Vision},
Journal = {TECHNOLOGIES},
Year = {2024},
Volume = {12},
Number = {2},
Month = {FEB},
Abstract = {Machine vision, an interdisciplinary field that aims to replicate human
   visual perception in computers, has experienced rapid progress and
   significant contributions. This paper traces the origins of machine
   vision, from early image processing algorithms to its convergence with
   computer science, mathematics, and robotics, resulting in a distinct
   branch of artificial intelligence. The integration of machine learning
   techniques, particularly deep learning, has driven its growth and
   adoption in everyday devices. This study focuses on the objectives of
   computer vision systems: replicating human visual capabilities including
   recognition, comprehension, and interpretation. Notably, image
   classification, object detection, and image segmentation are crucial
   tasks requiring robust mathematical foundations. Despite the
   advancements, challenges persist, such as clarifying terminology related
   to artificial intelligence, machine learning, and deep learning. Precise
   definitions and interpretations are vital for establishing a solid
   research foundation. The evolution of machine vision reflects an
   ambitious journey to emulate human visual perception. Interdisciplinary
   collaboration and the integration of deep learning techniques have
   propelled remarkable advancements in emulating human behavior and
   perception. Through this research, the field of machine vision continues
   to shape the future of computer systems and artificial intelligence
   applications.},
DOI = {10.3390/technologies12020015},
Article-Number = {15},
EISSN = {2227-7080},
ResearcherID-Numbers = {Fragulis, George/G-7566-2014
   Moysis, Lazaros/K-9944-2019
   Fragulis, George/AAE-1468-2021
   Maraslidis, George S./KFT-0276-2024
   },
ORCID-Numbers = {Maraslidis, George S./0000-0002-5453-1611
   Moysis, Lazaros/0000-0002-5652-2532
   Fragulis, George/0000-0002-8961-7423
   Manakitsa, Nikoleta/0009-0005-5423-5977},
Unique-ID = {WOS:001172157700001},
}

@article{ WOS:001296591100001,
Author = {Eyring, Veronika and Collins, William D. and Gentine, Pierre and Barnes,
   Elizabeth A. and Barreiro, Marcelo and Beucler, Tom and Bocquet, Marc
   and Bretherton, Christopher S. and Christensen, Hannah M. and Dagon,
   Katherine and Gagne, David John and Hall, David and Hammerling, Dorit
   and Hoyer, Stephan and Iglesias-Suarez, Fernando and Lopez-Gomez,
   Ignacio and Mcgraw, Marie C. and Meehl, Gerald A. and Molina, Maria J.
   and Monteleoni, Claire and Mueller, Juliane and Pritchard, Michael S.
   and Rolnick, David and Runge, Jakob and Stier, Philip and Watt-Meyer,
   Oliver and Weigel, Katja and Yu, Rose and Zanna, Laure},
Title = {Pushing the frontiers in climate modelling and analysis with machine
   learning},
Journal = {NATURE CLIMATE CHANGE},
Year = {2024},
Month = {2024 AUG 23},
Abstract = {Climate modelling and analysis are facing new demands to enhance
   projections and climate information. Here we argue that now is the time
   to push the frontiers of machine learning beyond state-of-the-art
   approaches, not only by developing machine-learning-based Earth system
   models with greater fidelity, but also by providing new capabilities
   through emulators for extreme event projections with large ensembles,
   enhanced detection and attribution methods for extreme events, and
   advanced climate model analysis and benchmarking. Utilizing this
   potential requires key machine learning challenges to be addressed, in
   particular generalization, uncertainty quantification, explainable
   artificial intelligence and causality. This interdisciplinary effort
   requires bringing together machine learning and climate scientists,
   while also leveraging the private sector, to accelerate progress towards
   actionable climate science.
   Machine learning methods allow for advances in many aspects of climate
   research. In this Perspective, the authors give an overview of recent
   progress and remaining challenges to harvest the full potential of
   machine learning methods.},
DOI = {10.1038/s41558-024-02095-y},
EarlyAccessDate = {AUG 2024},
ISSN = {1758-678X},
EISSN = {1758-6798},
ResearcherID-Numbers = {Gagne, David/H-7070-2019
   Beucler, Tom/OGN-5298-2025
   Zanna, Laure/HPG-1772-2023
   HALL, DAVID/F-2342-2011
   Gentine, Pierre/C-1495-2013
   Eyring, Veronika/O-9999-2016
   Iglesias-Suarez, Fernando/AAE-1388-2021
   Han, Yilun/JAC-7311-2023
   Lopez-Gomez, Ignacio/OHT-6787-2025
   Collins, William/J-3147-2014
   Bocquet, Marc/E-1966-2011
   Christensen, Hannah/L-1180-2016
   Bretherton, Christopher/KYR-0774-2024
   Barreiro, Marcelo/AAT-7377-2020
   Müller, Juliane/L-1875-2013
   Hall, David/F-2342-2011
   Barnes, Elizabeth/O-1790-2014
   Molina, Maria/P-5465-2018
   Stier, Philip/B-2258-2008},
ORCID-Numbers = {Stier, Philip/0000-0002-1191-0128
   Lopez-Gomez, Ignacio/0000-0002-7255-5895
   Dagon, Katherine/0000-0002-4518-8225
   HALL, DAVID/0000-0002-0961-1196
   Gentine, Pierre/0000-0002-0845-8345
   Eyring, Veronika/0000-0002-6887-4885
   McGraw, Marie/0000-0002-4469-226X
   Zanna, Laure/0000-0002-8472-4828
   Gagne, David/0000-0002-0469-2740
   Collins, William/0000-0002-4463-9848
   Bocquet, Marc/0000-0003-2675-0347
   Christensen, Hannah/0000-0001-8244-0218
   Beucler, Tom/0000-0002-5731-1040
   Barreiro, Marcelo/0000-0002-7819-1607
   Molina, Maria/0000-0001-8539-8916
   },
Unique-ID = {WOS:001296591100001},
}

@article{ WOS:001203939100001,
Author = {Nozarijouybari, Zahra and Fathy, Hosam K.},
Title = {Machine learning for battery systems applications: Progress, challenges,
   and opportunities},
Journal = {JOURNAL OF POWER SOURCES},
Year = {2024},
Volume = {601},
Month = {MAY 1},
Abstract = {Machine learning has emerged as a transformative force throughout the
   entire engineering life cycle of electrochemical batteries. Its
   applications encompass a wide array of critical domains, including
   material discovery, model development, quality control during
   manufacturing, real-time monitoring, state estimation, optimization of
   charge cycles, fault detection, and life cycle management. Machine
   learning excels in its ability to identify and capture complex
   behavioral trends in batteries, which may be challenging to model using
   more traditional methods. The goal of this survey paper is to synthesize
   the rich existing literature on battery machine learning into a
   structured perspective on the successes, challenges, and prospects
   within this research domain. This critical examination highlights
   several key insights. Firstly, the selection of data sets, features, and
   algorithms significantly influences the success of machine learning
   applications, yet it remains an open research area with vast potential.
   Secondly, data set richness and size are both pivotal for the efficacy
   of machine learning algorithms, suggesting a potential for active
   machine learning techniques in the battery systems domain. Lastly, the
   field of machine learning in battery systems has extensive room for
   growth, moving beyond its current focus on specific applications like
   state of charge (SOC) and state of health (SOH) estimation, offering
   ample opportunities for innovation and expansion.},
DOI = {10.1016/j.jpowsour.2024.234272},
EarlyAccessDate = {MAR 2024},
Article-Number = {234272},
ISSN = {0378-7753},
EISSN = {1873-2755},
Unique-ID = {WOS:001203939100001},
}

@article{ WOS:001507295200001,
Author = {Binson, V. A. and Thomas, Sania and Subramoniam, M. and Arun, J. and
   Naveen, S. and Madhu, S.},
Title = {A Review of Machine Learning Algorithms for Biomedical Applications},
Journal = {ANNALS OF BIOMEDICAL ENGINEERING},
Year = {2024},
Volume = {52},
Number = {5},
Pages = {1159-1183},
Month = {MAY},
Abstract = {As the amount and complexity of biomedical data continue to increase,
   machine learning methods are becoming a popular tool in creating
   prediction models for the underlying biomedical processes. Although all
   machine learning methods aim to fit models to data, the methodologies
   used can vary greatly and may seem daunting at first. A comprehensive
   review of various machine learning algorithms per biomedical
   applications is presented. The key concepts of machine learning are
   supervised and unsupervised learning, feature selection, and evaluation
   metrics. Technical insights on the major machine learning methods such
   as decision trees, random forests, support vector machines, and
   k-nearest neighbors are analyzed. Next, the dimensionality reduction
   methods like principal component analysis and t-distributed stochastic
   neighbor embedding methods, and their applications in biomedical data
   analysis were reviewed. Moreover, in biomedical applications
   predominantly feedforward neural networks, convolutional neural
   networks, and recurrent neural networks are utilized. In addition, the
   identification of emerging directions in machine learning methodology
   will serve as a useful reference for individuals involved in biomedical
   research, clinical practice, and related professions who are interested
   in understanding and applying machine learning algorithms in their
   research or practice.},
DOI = {10.1007/s10439-024-03459-3},
ISSN = {0090-6964},
EISSN = {1573-9686},
ResearcherID-Numbers = {V A, Binson/ABB-1967-2021
   M, SUBRAMONIAM/A-4502-2017
   Madhu, Dr.S/N-2575-2016
   Madhu, Shobhika/OKT-0959-2025
   Subramoniam, M/ABI-7985-2020
   },
ORCID-Numbers = {M, SUBRAMONIAM/0000-0002-3004-5141
   Madhu, Dr.S/0000-0002-2200-2109
   S, Naveen/0000-0003-1659-2509},
Unique-ID = {WOS:001507295200001},
}

@article{ WOS:001064661300018,
Author = {Daidone, Mario and Ferrantelli, Sergio and Tuttolomondo, Antonino},
Title = {Machine learning applications in stroke medicine: advancements,
   challenges, and future prospectives},
Journal = {NEURAL REGENERATION RESEARCH},
Year = {2024},
Volume = {19},
Number = {4},
Pages = {769-773},
Month = {APR},
Abstract = {Stroke is a leading cause of disability and mortality worldwide,
   necessitating the development of advanced technologies to improve its
   diagnosis, treatment, and patient outcomes. In recent years, machine
   learning techniques have emerged as promising tools in stroke medicine,
   enabling efficient analysis of large-scale datasets and facilitating
   personalized and precision medicine approaches. This abstract provides a
   comprehensive overview of machine learning's applications, challenges,
   and future directions in stroke medicine. Recently introduced machine
   learning algorithms have been extensively employed in all the fields of
   stroke medicine. Machine learning models have demonstrated remarkable
   accuracy in imaging analysis, diagnosing stroke subtypes, risk
   stratifications, guiding medical treatment, and predicting patient
   prognosis. Despite the tremendous potential of machine learning in
   stroke medicine, several challenges must be addressed. These include the
   need for standardized and interoperable data collection, robust model
   validation and generalization, and the ethical considerations
   surrounding privacy and bias. In addition, integrating machine learning
   models into clinical workflows and establishing regulatory frameworks
   are critical for ensuring their widespread adoption and impact in
   routine stroke care. Machine learning promises to revolutionize stroke
   medicine by enabling precise diagnosis, tailored treatment selection,
   and improved prognostication. Continued research and collaboration among
   clinicians, researchers, and technologists are essential for overcoming
   challenges and realizing the full potential of machine learning in
   stroke care, ultimately leading to enhanced patient outcomes and quality
   of life. This review aims to summarize all the current implications of
   machine learning in stroke diagnosis, treatment, and prognostic
   evaluation. At the same time, another purpose of this paper is to
   explore all the future perspectives these techniques can provide in
   combating this disabling disease.},
DOI = {10.4103/1673-5374.382228},
ISSN = {1673-5374},
EISSN = {1876-7958},
ResearcherID-Numbers = {Daidone, Mario/IVH-4804-2023
   },
ORCID-Numbers = {Daidone, Mario/0000-0003-1413-5945},
Unique-ID = {WOS:001064661300018},
}

@article{ WOS:001219213300001,
Author = {Gao, Yu and Wang, Jiayuan and Xu, Xiaoxiao},
Title = {Machine learning in construction and demolition waste management:
   Progress, challenges, and future directions},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2024},
Volume = {162},
Month = {JUN},
Abstract = {The application of machine learning contributes to intelligent and
   efficient management of construction and demolition waste, leading to a
   reduction in waste generation and an increased emphasis on recycling.
   This research conducts a comprehensive analysis of 98 journals related
   to the application of machine learning in construction waste management
   from 2012 to 2023 to identify current hot topics and emerging trends.
   The results reveal that machine learning is applied in four main areas
   and 15 subfields, specifically focusing on construction and demolition
   waste generation, on-site handling, transportation, and disposal.
   Various models, such as artificial neural networks, deep learning,
   convolutional neural networks, and support vector machines, demonstrate
   their effectiveness in different processes of construction and
   demolition waste management. The findings of this research will aid
   researchers in gaining a comprehensive understanding of the current
   state and future directions of machine learning in construction waste
   management.},
DOI = {10.1016/j.autcon.2024.105380},
EarlyAccessDate = {MAR 2024},
Article-Number = {105380},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Xu, Xiaoxiao/KIH-1685-2024
   },
ORCID-Numbers = {Xu, Xiaoxiao/0000-0002-2567-5739},
Unique-ID = {WOS:001219213300001},
}

@article{ WOS:001137399100001,
Author = {Bogyrbayeva, Aigerim and Meraliyev, Meraryslan and Mustakhov, Taukekhan
   and Dauletbayev, Bissenbay},
Title = {Machine Learning to Solve Vehicle Routing Problems: A Survey},
Journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
Year = {2024},
Volume = {25},
Number = {6},
Pages = {4754-4772},
Month = {JUN},
Abstract = {This paper provides a systematic overview of machine learning methods
   applied to solve NP-hard Vehicle Routing Problems (VRPs). Recently,
   there has been great interest from both the machine learning and
   operations research communities in solving VRPs either through pure
   learning methods or by combining them with traditional handcrafted
   heuristics. We present a taxonomy of studies on learning paradigms,
   solution structures, underlying models, and algorithms. Detailed results
   of state-of-the-art methods are presented, demonstrating their
   competitiveness with traditional approaches. The survey highlights the
   advantages of the machine learning-based models that aim to exploit the
   symmetry of VRP solutions. The paper outlines future research directions
   to incorporate learning-based solutions to address the challenges of
   modern transportation systems.},
DOI = {10.1109/TITS.2023.3334976},
EarlyAccessDate = {JAN 2024},
ISSN = {1524-9050},
EISSN = {1558-0016},
ResearcherID-Numbers = {Meraliyev, Meraryslan/KIK-9525-2024
   Bogyrbayeva, Aigerim/GXG-6631-2022
   Dauletbayev, Bissenbay/MHQ-6470-2025},
ORCID-Numbers = {Meraliyev, Meraryslan/0000-0003-2627-0837
   Dauletbayev, Bissenbay/0000-0003-4906-6409
   },
Unique-ID = {WOS:001137399100001},
}

@article{ WOS:001310661500002,
Author = {Sadr, Hossein and Salari, Arsalan and Ashoobi, Mohammad Taghi and
   Nazari, Mojdeh},
Title = {Cardiovascular disease diagnosis: a holistic approach using the
   integration of machine learning and deep learning models},
Journal = {EUROPEAN JOURNAL OF MEDICAL RESEARCH},
Year = {2024},
Volume = {29},
Number = {1},
Month = {SEP 11},
Abstract = {BackgroundThe incidence and mortality rates of cardiovascular disease
   worldwide are a major concern in the healthcare industry. Precise
   prediction of cardiovascular disease is essential, and the use of
   machine learning and deep learning can aid in decision-making and
   enhance predictive abilities.ObjectiveThe goal of this paper is to
   introduce a model for precise cardiovascular disease prediction by
   combining machine learning and deep learning.MethodTwo public heart
   disease classification datasets with 70,000 and 1190 records besides a
   locally collected dataset with 600 records were used in our experiments.
   Then, a model which makes use of both machine learning and deep learning
   was proposed in this paper. The proposed model employed CNN and LSTM, as
   the representatives of deep learning models, besides KNN and XGB, as the
   representatives of machine learning models. As each classifier defined
   the output classes, majority voting was then used as an ensemble learner
   to predict the final output class.ResultThe proposed model obtained the
   highest classification performance based on all evaluation metrics on
   all datasets, demonstrating its suitability and reliability in
   forecasting the probability of cardiovascular disease.},
DOI = {10.1186/s40001-024-02044-7},
Article-Number = {455},
ISSN = {0949-2321},
EISSN = {2047-783X},
ResearcherID-Numbers = {Sadr, Hossein/AAC-6722-2019
   Ashoubi, Mohammad Taghi/P-9338-2018
   Salari, Arsalan/D-2855-2019},
Unique-ID = {WOS:001310661500002},
}

@article{ WOS:001263598700003,
Author = {Lu, Jie and Ma, Guangzhi and Zhang, Guangquan},
Title = {Fuzzy Machine Learning: A Comprehensive Framework and Systematic Review},
Journal = {IEEE TRANSACTIONS ON FUZZY SYSTEMS},
Year = {2024},
Volume = {32},
Number = {7},
Pages = {3861-3878},
Month = {JUL},
Abstract = {Machine learning draws its power from various disciplines, including
   computer science, cognitive science, and statistics. Although machine
   learning has achieved great advancements in both theory and practice,
   its methods have some limitations when dealing with complex situations
   and highly uncertain environments. Insufficient data, imprecise
   observations, and ambiguous information/relationships can all confound
   traditional machine learning systems. To address these problems,
   researchers have integrated machine learning from different aspects and
   fuzzy techniques, including fuzzy sets, fuzzy systems, fuzzy logic,
   fuzzy measures, fuzzy relations, and so on. This article presents a
   systematic review of fuzzy machine learning, from theory, approach to
   application, with the overall objective of providing an overview of
   recent achievements in the field of fuzzy machine learning. To this end,
   the concepts and frameworks discussed are divided into five categories:
   1) fuzzy classical machine learning; 2) fuzzy transfer learning; 3)
   fuzzy data stream learning; 4) fuzzy reinforcement learning; and 5)
   fuzzy recommender systems. The literature presented should provide
   researchers with a solid understanding of the current progress in fuzzy
   machine learning research and its applications.},
DOI = {10.1109/TFUZZ.2024.3387429},
ISSN = {1063-6706},
EISSN = {1941-0034},
ResearcherID-Numbers = {MA, GUANGZHI/OEO-7855-2025
   Zhang, Guangquan/G-2553-2017
   Lu, Jie/S-3581-2016},
ORCID-Numbers = {Zhang, Guangquan/0000-0003-3960-0583
   Lu, Jie/0000-0003-0690-4732},
Unique-ID = {WOS:001263598700003},
}

@article{ WOS:001166743400001,
Author = {Zhou, Zixuan and Tian, Daoming and Yang, Yingao and Cui, Han and Li,
   Yanchun and Ren, Shuyue and Han, Tie and Gao, Zhixian},
Title = {Machine learning assisted biosensing technology: An emerging powerful
   tool for improving the intelligence of food safety detection},
Journal = {CURRENT RESEARCH IN FOOD SCIENCE},
Year = {2024},
Volume = {8},
Abstract = {Recently, the application of biosensors in food safety assessment has
   gained considerable research attention. Nevertheless, the evaluation of
   biosensors' sensitivity, accuracy, and efficiency is still ongoing. The
   advent of machine learning has enhanced the application of biosensors in
   food security assessment, yielding improved results. Machine learning
   has been preliminarily applied in combination with different biosensors
   in food safety assessment, with positive results. This review offers a
   comprehensive summary of the diverse machine learning methods employed
   in biosensors for food safety. Initially, the primary machine learning
   methods were outlined, and the integrated application of biosensors and
   machine learning in food safety was thoroughly examined. Lastly, the
   challenges and limitations of machine learning and biosensors in the
   realm of food safety were underscored, and potential solutions were
   explored. The review's findings demonstrated that algorithms grounded in
   machine learning can aid in the early detection of food safety issues.
   Furthermore, preliminary research suggests that biosensors could be
   optimized through machine learning for real-time, multifaceted analyses
   of food safety variables and their interactions. The potential of
   machine learning and biosensors in realtime monitoring of food quality
   has been discussed.},
DOI = {10.1016/j.crfs.2024.100679},
EarlyAccessDate = {JAN 2024},
Article-Number = {100679},
EISSN = {2665-9271},
ResearcherID-Numbers = {Cui, Han/KPB-7522-2024
   Zhou, Zixuan/KEI-0421-2024},
Unique-ID = {WOS:001166743400001},
}

@article{ WOS:001155473600001,
Author = {Kuznetsova, Vera and Coogan, Aine and Botov, Dmitry and Gromova, Yulia
   and Ushakova, Elena V. and Gun'ko, Yurii K.},
Title = {Expanding the Horizons of Machine Learning in Nanomaterials to Chiral
   Nanostructures},
Journal = {ADVANCED MATERIALS},
Year = {2024},
Volume = {36},
Number = {18},
Month = {MAY},
Abstract = {Machine learning holds significant research potential in the field of
   nanotechnology, enabling nanomaterial structure and property
   predictions, facilitating materials design and discovery, and reducing
   the need for time-consuming and labor-intensive experiments and
   simulations. In contrast to their achiral counterparts, the application
   of machine learning for chiral nanomaterials is still in its infancy,
   with a limited number of publications to date. This is despite the great
   potential of machine learning to advance the development of new
   sustainable chiral materials with high values of optical activity,
   circularly polarized luminescence, and enantioselectivity, as well as
   for the analysis of structural chirality by electron microscopy. In this
   review, an analysis of machine learning methods used for studying
   achiral nanomaterials is provided, subsequently offering guidance on
   adapting and extending this work to chiral nanomaterials. An overview of
   chiral nanomaterials within the framework of
   synthesis-structure-property-application relationships is presented and
   insights on how to leverage machine learning for the study of these
   highly complex relationships are provided. Some key recent publications
   are reviewed and discussed on the application of machine learning for
   chiral nanomaterials. Finally, the review captures the key achievements,
   ongoing challenges, and the prospective outlook for this very important
   research field.
   This review analyzes machine learning methods for studying achiral
   nanomaterials and offers guidance for adapting and extending this work
   to chiral nanomaterials. An overview of chiral nanomaterials in the
   context of synthesis-structure-property-application relationships is
   presented, offering insights on leveraging machine learning for these
   complex relationships. Key achievements, challenges, and outlook for
   machine learning in chiral nanomaterials research are discussed. image},
DOI = {10.1002/adma.202308912},
EarlyAccessDate = {FEB 2024},
ISSN = {0935-9648},
EISSN = {1521-4095},
ResearcherID-Numbers = {Gun'ko, Yurii/R-5348-2016
   Ushakova, Elena/I-2382-2014
   Coogan, Áine/KHD-7921-2024
   },
ORCID-Numbers = {Gun'ko, Yurii/0000-0002-4772-778X
   Ushakova, Elena/0000-0001-6841-6975
   Coogan, Aine/0000-0002-3410-0136},
Unique-ID = {WOS:001155473600001},
}

@article{ WOS:001156411300001,
Author = {Arnold, Christian and Biedebach, Luka and Kuepfer, Andreas and
   Neunhoeffer, Marcel},
Title = {The role of hyperparameters in machine learning models and how to tune
   them},
Journal = {POLITICAL SCIENCE RESEARCH AND METHODS},
Year = {2024},
Volume = {12},
Number = {4},
Pages = {841-848},
Month = {OCT},
Abstract = {Hyperparameters critically influence how well machine learning models
   perform on unseen, out-of-sample data. Systematically comparing the
   performance of different hyperparameter settings will often go a long
   way in building confidence about a model's performance. However,
   analyzing 64 machine learning related manuscripts published in three
   leading political science journals (APSR, PA, and PSRM) between 2016 and
   2021, we find that only 13 publications (20.31 percent) report the
   hyperparameters and also how they tuned them in either the paper or the
   appendix. We illustrate the dangers of cursory attention to model and
   tuning transparency in comparing machine learning models' capability to
   predict electoral violence from tweets. The tuning of hyperparameters
   and their documentation should become a standard component of robustness
   checks for machine learning models.},
DOI = {10.1017/psrm.2023.61},
EarlyAccessDate = {FEB 2024},
ISSN = {2049-8470},
EISSN = {2049-8489},
ResearcherID-Numbers = {Neunhoeffer, Marcel/J-2504-2019
   Biedebach, Luka/JYQ-2167-2024
   },
ORCID-Numbers = {Neunhoeffer, Marcel/0000-0002-9137-5785
   Arnold, Christian/0000-0002-7042-594X
   Kupfer, Andreas/0000-0002-4110-0775},
Unique-ID = {WOS:001156411300001},
}

@article{ WOS:001335131500001,
Author = {Lones, Michael A.},
Title = {Avoiding common machine learning pitfalls},
Journal = {PATTERNS},
Year = {2024},
Volume = {5},
Number = {10},
Month = {OCT 11},
Abstract = {Mistakes in machine learning practice are commonplace and can result in
   loss of confidence in the findings and products of machine learning.
   This tutorial outlines common mistakes that occur when using machine
   learning and what can be done to avoid them. While it should be
   accessible to anyone with a basic understanding of machine learning
   techniques, it focuses on issues that are of particular concern within
   academic research, such as the need to make rigorous comparisons and
   reach valid conclusions. It covers five stages of the machine learning
   process: what to do before model building, how to reliably build models,
   how to robustly evaluate models, how to compare models fairly, and how
   to report results.},
DOI = {10.1016/j.patter.2024.101046},
EarlyAccessDate = {OCT 2024},
Article-Number = {101046},
ISSN = {2666-3899},
ORCID-Numbers = {Lones, Michael/0000-0002-2745-9896},
Unique-ID = {WOS:001335131500001},
}

@article{ WOS:001230185600011,
Author = {Zheng, Changgang and Hong, Xinpeng and Ding, Damu and Vargaftik, Shay
   and Ben-Itzhak, Yaniv and Zilberman, Noa},
Title = {In-Network Machine Learning Using Programmable Network Devices: A Survey},
Journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS},
Year = {2024},
Volume = {26},
Number = {2},
Pages = {1171-1200},
Month = {APR-JUN},
Abstract = {Machine learning is widely used to solve networking challenges, ranging
   from traffic classification and anomaly detection to network
   configuration. However, machine learning also requires significant
   processing and often increases the load on both networks and servers.
   The introduction of in-network computing, enabled by programmable
   network devices, has allowed to run applications within the network,
   providing higher throughput and lower latency. Soon after, in-network
   machine learning solutions started to emerge, enabling machine learning
   functionality within the network itself. This survey introduces the
   concept of in-network machine learning and provides a comprehensive
   taxonomy. The survey provides an introduction to the technology and
   explains the different types of machine learning solutions built upon
   programmable network devices. It explores the different types of machine
   learning models implemented within the network, and discusses related
   challenges and solutions. In-network machine learning can significantly
   benefit cloud computing and next-generation networks, and this survey
   concludes with a discussion of future trends.},
DOI = {10.1109/COMST.2023.3344351},
EISSN = {1553-877X},
ResearcherID-Numbers = {Hong, Xinpeng/JQW-5447-2023
   Ding, Damu/AAS-9364-2020
   },
ORCID-Numbers = {Zheng, Changgang/0000-0003-1894-722X
   ding, damu/0000-0001-9692-7756
   Zilberman, Noa/0000-0002-3655-2873
   Hong, Xinpeng/0000-0001-8525-6424},
Unique-ID = {WOS:001230185600011},
}

@article{ WOS:001359921300001,
Author = {Vashishtha, Govind and Chauhan, Sumika and Sehri, Mert and Zimroz,
   Radoslaw and Dumond, Patrick and Kumar, Rajesh and Gupta, Munish Kumar},
Title = {A roadmap to fault diagnosis of industrial machines via machine
   learning: A brief review},
Journal = {MEASUREMENT},
Year = {2025},
Volume = {242},
Number = {D},
Month = {JAN},
Abstract = {In fault diagnosis, machine learning theories are gaining popularity as
   they proved to be an efficient tool that not only reduces human effort
   but also identifies the health conditions of the machines automatically.
   In this work, an attempt has been made to systematically review the
   progress of machine learning theories in fault diagnosis from scratch to
   future perspectives. Initially, artificial intelligence came into the
   picture which started to weaken the human effort whose efficiency relies
   on feature extraction which depends on expert knowledge. The
   introduction of deep learning theories has reformed the fault diagnosis
   process by realising the artificial aid, encouraging end-to-end
   encryption in the diagnostic procedure. The deep learning theories have
   also filled the gap between the large amount of monitoring data and the
   health conditions of industrial machines. The future of deep learning
   theories i.e. transfer learning which uses the knowledge of one domain
   to another related domain during fault diagnosis has been reviewed. In
   last, the research trends of the machine learning theories have been
   briefly discussed along with their challenges in fault diagnostics.},
DOI = {10.1016/j.measurement.2024.116216},
EarlyAccessDate = {NOV 2024},
Article-Number = {116216},
ISSN = {0263-2241},
EISSN = {1873-412X},
ResearcherID-Numbers = {Sehri, Mert/HZL-7782-2023
   VASHISHTHA, GOVIND/AAT-3400-2021
   Zimroz, Radoslaw/HHS-9875-2022
   Gupta, Munish/AAT-5708-2020
   Kumar, Rajesh/AAY-2468-2021
   Chauhan, Sumika/ADQ-0010-2022
   Vashishtha, Govind/AAT-3400-2021},
ORCID-Numbers = {Sehri, Mert/0000-0003-4781-1719
   Zimroz, Radoslaw/0000-0003-4781-9972
   Dumond, Patrick/0000-0002-2200-4614
   Chauhan, Sumika/0000-0003-0367-4193
   KUMAR, RAJESH/0000-0003-3955-8282
   Gupta, Munish/0000-0002-0777-1559
   Vashishtha, Govind/0000-0002-5160-9647},
Unique-ID = {WOS:001359921300001},
}

@article{ WOS:001334283900001,
Author = {Wang, Yunfei and Liu, Junyu},
Title = {A comprehensive review of quantum machine learning: from NISQ to fault
   tolerance},
Journal = {REPORTS ON PROGRESS IN PHYSICS},
Year = {2024},
Volume = {87},
Number = {11},
Month = {NOV 1},
Abstract = {Quantum machine learning, which involves running machine learning
   algorithms on quantum devices, has garnered significant attention in
   both academic and business circles. In this paper, we offer a
   comprehensive and unbiased review of the various concepts that have
   emerged in the field of quantum machine learning. This includes
   techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies
   and approaches for algorithms compatible with fault-tolerant quantum
   computing hardware. Our review covers fundamental concepts, algorithms,
   and the statistical learning theory pertinent to quantum machine
   learning.},
DOI = {10.1088/1361-6633/ad7f69},
Article-Number = {116402},
ISSN = {0034-4885},
EISSN = {1361-6633},
ORCID-Numbers = {Liu, Junyu/0000-0003-1669-8039},
Unique-ID = {WOS:001334283900001},
}

@article{ WOS:001166777200003,
Author = {Harrie, Lars and Touya, Guillaume and Oucheikh, Rachid and Ai, Tinghua
   and Courtial, Azelle and Richter, Kai-Florian},
Title = {Machine learning in cartography},
Journal = {CARTOGRAPHY AND GEOGRAPHIC INFORMATION SCIENCE},
Year = {2024},
Volume = {51},
Number = {1, SI},
Pages = {1-19},
Month = {JAN 2},
Abstract = {Machine learning is increasingly used as a computing paradigm in
   cartographic research. In this extended editorial, we provide some
   background of the papers in the CaGIS special issue Machine Learning in
   Cartography with a special focus on pattern recognition in maps,
   cartographic generalization, style transfer, and map labeling. In
   addition, the paper includes a discussion about map encodings for
   machine learning applications and the possible need for explicit
   cartographic knowledge and procedural modeling in cartographic machine
   learning models.},
DOI = {10.1080/15230406.2023.2295948},
ISSN = {1523-0406},
EISSN = {1545-0465},
ResearcherID-Numbers = {Ai, Tinghua/O-1624-2017
   Richter, Kai-Florian/I-4770-2013
   Touya, Guillaume/AAA-5941-2020
   Oucheikh, Rachid/AAT-2735-2020
   },
ORCID-Numbers = {Ai, Tinghua/0000-0002-6581-9872
   Oucheikh, Rachid/0000-0001-9996-9759
   Richter, Kai-Florian/0000-0001-5629-0981
   Touya, Guillaume/0000-0001-6113-6903
   Harrie, Lars/0000-0003-3252-1495
   Courtial, Azelle/0000-0002-0416-3398},
Unique-ID = {WOS:001166777200003},
}

@article{ WOS:001331425700001,
Author = {Riyadi, Tri W. B. and Herawan, Safarudin G. and Tirta, Andy and Ee, Yit
   Jing and Hananto, April Lia and Paristiawan, Permana A. and Yusuf,
   Abdulfatah Abdu and Venu, Harish and Irianto and Veza, Ibham},
Title = {Nanofluid heat transfer and machine learning: Insightful review of
   machine learning for nanofluid heat transfer enhancement in porous media
   and heat exchangers as sustainable and renewable energy solutions},
Journal = {RESULTS IN ENGINEERING},
Year = {2024},
Volume = {24},
Month = {DEC},
Abstract = {Nanofluid, coupled with machine learning, is at the forefront of
   cutting-edge research in sustainable and renewable energy sector. This
   review paper examines the latest developments in the intersection of
   nanofluid and machine learning for heat transfer enhancement. This
   hybrid nanofluid-machine learning review investigates nanofluid heat
   transfer enhancement leveraged by machine learning both in porous media
   as well as heat exchangers. Several studies in porous media nanofluid
   transport utilize advanced methodologies that integrate machine learning
   and computational techniques. Machine learning and computational methods
   are employed to tackle complex thermodynamics, transport processes, and
   heat transfer challenges in complex multiphysics systems. An interesting
   hybrid nanofluid-machine learning application involves applying a
   machine learning method such as Support Vector Machine (SVM) to forecast
   movement of hybrid nanofluid flows across porous surfaces. Such hybrid
   nanofluid-machine learning technique involves utilising training data
   obtained from computational fluid dynamics (CFD) to decrease
   computational time and expenses. Machine learning offers a more
   efficient and cost-effective modelling for nanofluid heat transfer
   enhancement. Techniques such as scanning electron microscopy (SEM) along
   with X-ray diffraction (XRD) are also often used for assessing the forms
   as well as nanocomposites configurations in heat exchangers while
   studying nanofluids. The importance of machine learning models,
   especially artificial neural networks (ANNs) and genetic algorithms, is
   evident in their ability to predict and optimize thermal performance of
   nanofluid application for nanofluid heat transfer enhancement.
   Furthermore, integrating nanofluids into various heat exchanger designs
   has demonstrated significant enhancements in efficiency, decreased
   energy usage, and total cost reduction. These achievements align with
   the research goal in sustainable and renewable energy, highlighting the
   critical role of nanofluid-enhanced heat exchange systems in tackling
   current difficulties related to energy efficiency and sustainability.
   Overall, combining nanofluids with machine learning shows promising
   advancements, providing a route toward creating more efficient and
   eco-friendly heat exchange systems.},
DOI = {10.1016/j.rineng.2024.103002},
EarlyAccessDate = {OCT 2024},
Article-Number = {103002},
ISSN = {2590-1230},
ResearcherID-Numbers = {Yusuf, Abdulfatah/Z-5326-2019
   Yit, Jing Ee/LCE-2779-2024
   Hananto, April/U-6496-2019
   Tirta, Andy/HTQ-4929-2023
   Veza, Ibham/AAX-2642-2020
   Herawan, Safarudin Gazali/GNW-2829-2022
   Venu, Harish/GWV-1931-2022
   Riyadi, Tri/AAJ-7305-2020},
ORCID-Numbers = {Yit, Jing Ee/0000-0002-5315-1393
   Veza, Ibham/0000-0002-1674-4798
   venu, harish/0000-0002-9997-7190
   },
Unique-ID = {WOS:001331425700001},
}

@article{ WOS:001225931500001,
Author = {Alzoubi, Yehia Ibrahim and Mishra, Alok and Topcu, Ahmet Ercan},
Title = {Research trends in deep learning and machine learning for cloud
   computing security},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2024},
Volume = {57},
Number = {5},
Month = {MAY 2},
Abstract = {Deep learning and machine learning show effectiveness in identifying and
   addressing cloud security threats. Despite the large number of articles
   published in this field, there remains a dearth of comprehensive reviews
   that synthesize the techniques, trends, and challenges of using deep
   learning and machine learning for cloud computing security. Accordingly,
   this paper aims to provide the most updated statistics on the
   development and research in cloud computing security utilizing deep
   learning and machine learning. Up to the middle of December 2023, 4051
   publications were identified after we searched the Scopus database. This
   paper highlights key trend solutions for cloud computing security
   utilizing machine learning and deep learning, such as anomaly detection,
   security automation, and emerging technology's role. However, challenges
   such as data privacy, scalability, and explainability, among others, are
   also identified as challenges of using machine learning and deep
   learning for cloud security. The findings of this paper reveal that deep
   learning and machine learning for cloud computing security are emerging
   research areas. Future research directions may include addressing these
   challenges when utilizing machine learning and deep learning for cloud
   security. Additionally, exploring the development of algorithms and
   techniques that comply with relevant laws and regulations is essential
   for effective implementation in this domain.},
DOI = {10.1007/s10462-024-10776-5},
Article-Number = {132},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Topcu, Ahmet/IQU-5347-2023
   Mishra, Alok/D-7937-2012
   Mishra, Alok/AAE-2673-2019
   Alzoubi, Yehia/ACQ-8013-2022},
ORCID-Numbers = {Mishra, Alok/0000-0003-1275-2050
   },
Unique-ID = {WOS:001225931500001},
}

@article{ WOS:001230996600003,
Author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E. and
   Wiemann, Thomas},
Title = {ddml: Double/debiased machine learning in Stata},
Journal = {STATA JOURNAL},
Year = {2024},
Volume = {24},
Number = {1},
Pages = {3-45},
Month = {MAR},
Abstract = {In this article, we introduce a package, ddml, for double/debiased
   machine learning in Stata. Estimators of causal parameters for five
   different econometric models are supported, allowing for flexible
   estimation of causal effects of endogenous variables in settings with
   unknown functional forms or many exogenous variables. ddml is compatible
   with many existing supervised machine learning programs in Stata. We
   recommend using double/debiased machine learning in combination with
   stacking estimation, which combines multiple machine learners into a
   final predictor. We provide Monte Carlo evidence to support our
   recommendation.},
DOI = {10.1177/1536867X241233641},
ISSN = {1536-867X},
EISSN = {1536-8734},
ORCID-Numbers = {Ahrens, Achim/0000-0002-3201-6395},
Unique-ID = {WOS:001230996600003},
}

@article{ WOS:001167870100001,
Author = {Bach, Philipp and Kurz, Malte S. and Chernozhukov, Victor and Spindler,
   Martin and Klaassen, Sven},
Title = {DoubleML: An Object-Oriented Implementation of Double Machine Learning
   in R},
Journal = {JOURNAL OF STATISTICAL SOFTWARE},
Year = {2024},
Volume = {108},
Number = {3},
Month = {FEB},
Abstract = {The R package DoubleML implements the double/debiased machine learning
   framework of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey,
   and Robins (2018). It provides functionalities to estimate parameters in
   causal models based on machine learning methods. The double machine
   learning framework consists of three key ingredients: Neyman
   orthogonality, high -quality machine learning estimation and sample
   splitting. Estimation of nuisance components can be performed by various
   state-of-the-art machine learning methods that are available in the mlr3
   ecosystem. DoubleML makes it possible to perform inference in a variety
   of causal models, including partially linear and interactive regression
   models and their extensions to instrumental variable estimation. The
   object -oriented implementation of DoubleML enables a high flexibility
   for the model specification and makes it easily extendable. This paper
   serves as an introduction to the double machine learning framework and
   the R package DoubleML. In reproducible code examples with simulated and
   real data sets, we demonstrate how DoubleML users can perform valid
   inference based on machine learning methods.},
DOI = {10.18637/jss.v108.i03},
Article-Number = {03},
ISSN = {1548-7660},
Unique-ID = {WOS:001167870100001},
}

@article{ WOS:001309841200001,
Author = {Zheng, Changgang and Zang, Mingyuan and Hong, Xinpeng and Perreault,
   Liam and Bensoussane, Riyad and Vargaftik, Shay and Ben-Itzhak, Yaniv
   and Zilberman, Noa},
Title = {Planter: Rapid Prototyping of In-Network Machine Learning Inference},
Journal = {ACM SIGCOMM COMPUTER COMMUNICATION REVIEW},
Year = {2024},
Volume = {54},
Number = {1},
Pages = {2-20},
Month = {JAN},
Abstract = {In-network machine learning inference provides high throughput and low
   latency. It is ideally located within the network, power efficient, and
   improves applications' performance. Despite its advantages, the bar to
   in-network machine learning research is high, requiring significant
   expertise in programmable data planes, in addition to knowledge of
   machine learning and the application area. Existing solutions are mostly
   one-time efforts, hard to reproduce, change, or port across platforms.
   In this paper, we present Planter: a modular and efficient open-source
   framework for rapid prototyping of in-network machine learning models
   across a range of platforms and pipeline architectures. By identifying
   general mapping methodologies for machine learning algorithms, Planter
   introduces new machine learning mappings and improves existing ones. It
   provides users with several example use cases and supports different
   datasets, and was already extended by users to new fields and
   applications. Our evaluation shows that Planter improves machine
   learning performance compared with previous model-tailored works, while
   significantly reducing resource consumption and co-existing with network
   functionality. Planter-supported algorithms run at line rate on
   unmodified commodity hardware, providing billions of inference decisions
   per second.},
ISSN = {0146-4833},
EISSN = {1943-5819},
ResearcherID-Numbers = {Hong, Xinpeng/JQW-5447-2023},
Unique-ID = {WOS:001309841200001},
}

@article{ WOS:001206759600001,
Author = {Ureel, Yannick and Dobbelaere, Maarten R. and Ouyang, Yi and De Ras,
   Kevin and Sabbe, Maarten K. and Marin, Guy B. and Van Geem, Kevin M.},
Title = {Active Machine Learning for Chemical Engineers: A Bright Future Lies
   Ahead!},
Journal = {ENGINEERING},
Year = {2023},
Volume = {27},
Pages = {23-30},
Month = {AUG},
Abstract = {By combining machine learning with the design of experiments, thereby
   achieving so-called active machine learning, more efficient and cheaper
   research can be conducted. Machine learning algorithms are more flexible
   and are better than traditional design of experiment algorithms at
   investigating processes spanning all length scales of chemical
   engineering. While active machine learning algorithms are maturing,
   their applications are falling behind. In this article, three types of
   challenges presented by active machine learning-namely, convincing the
   experimental researcher, the flexibility of data creation, and the
   robustness of active machine learning algorithms-are identified, and
   ways to overcome them are discussed. A bright future lies ahead for
   active machine learning in chemical engineering, thanks to increasing
   automation and more efficient algorithms that can drive novel
   discoveries. (c) 2023 THE AUTHORS. Published by Elsevier LTD on behalf
   of Chinese Academy of Engineering and Higher Education Press Limited
   Company. This is an open access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
DOI = {10.1016/j.eng.2023.02.019},
EarlyAccessDate = {MAR 2024},
ISSN = {2095-8099},
EISSN = {2096-0026},
ResearcherID-Numbers = {De Ras, Kevin/HKE-8715-2023
   Van Geem, Kevin/J-3294-2014
   Ureel, Yannick/MHQ-7692-2025
   , M Sabbe/LLK-5302-2024
   Ouyang, Yi/ABD-6750-2020
   Dobbelaere, Maarten/ABD-4954-2021},
ORCID-Numbers = {Dobbelaere, Maarten/0000-0002-8977-8569
   Ureel, Yannick/0000-0001-6883-320X
   Marin, Guy/0000-0002-6733-1213
   Ouyang, Yi/0000-0002-1950-4538
   },
Unique-ID = {WOS:001206759600001},
}

@article{ WOS:001139747600005,
Author = {Zhou, Hao-ran and Yang, Hao and Li, Huai-qian and Ma, Ying-chun and Yu,
   Sen and Shi, Jian and Cheng, Jing-chang and Gao, Peng and Yu, Bo and
   Miao, Zhi-quan and Wei, Yan-peng},
Title = {Advancements in machine learning for material design and process
   optimization in the field of additive manufacturing},
Journal = {CHINA FOUNDRY},
Year = {2024},
Volume = {21},
Number = {2},
Pages = {101-115},
Month = {MAR},
Abstract = {Additive manufacturing technology is highly regarded due to its
   advantages, such as high precision and the ability to address complex
   geometric challenges. However, the development of additive manufacturing
   process is constrained by issues like unclear fundamental principles,
   complex experimental cycles, and high costs. Machine learning, as a
   novel artificial intelligence technology, has the potential to deeply
   engage in the development of additive manufacturing process, assisting
   engineers in learning and developing new techniques. This paper provides
   a comprehensive overview of the research and applications of machine
   learning in the field of additive manufacturing, particularly in model
   design and process development. Firstly, it introduces the background
   and significance of machine learning-assisted design in additive
   manufacturing process. It then further delves into the application of
   machine learning in additive manufacturing, focusing on model design and
   process guidance. Finally, it concludes by summarizing and forecasting
   the development trends of machine learning technology in the field of
   additive manufacturing.},
DOI = {10.1007/s41230-024-3145-3},
EarlyAccessDate = {JAN 2024},
ISSN = {1672-6421},
EISSN = {2365-9459},
ResearcherID-Numbers = {Zhou, Haoran/AAW-9462-2021},
ORCID-Numbers = {Wei, Yanpeng/0000-0002-2314-0407
   },
Unique-ID = {WOS:001139747600005},
}

@article{ WOS:000611065800001,
Author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis,
   Sotiris},
Title = {Explainable AI: A Review of Machine Learning Interpretability Methods},
Journal = {ENTROPY},
Year = {2021},
Volume = {23},
Number = {1},
Month = {JAN},
Abstract = {Recent advances in artificial intelligence (AI) have led to its
   widespread industrial adoption, with machine learning systems
   demonstrating superhuman performance in a significant number of tasks.
   However, this surge in performance, has often been achieved through
   increased model complexity, turning such systems into ``black box{''}
   approaches and causing uncertainty regarding the way they operate and,
   ultimately, the way that they come to decisions. This ambiguity has made
   it problematic for machine learning systems to be adopted in sensitive
   yet critical domains, where their value could be immense, such as
   healthcare. As a result, scientific interest in the field of Explainable
   Artificial Intelligence (XAI), a field that is concerned with the
   development of new methods that explain and interpret machine learning
   models, has been tremendously reignited over recent years. This study
   focuses on machine learning interpretability methods; more specifically,
   a literature review and taxonomy of these methods are presented, as well
   as links to their programming implementations, in the hope that this
   survey would serve as a reference point for both theorists and
   practitioners.},
DOI = {10.3390/e23010018},
Article-Number = {18},
EISSN = {1099-4300},
ResearcherID-Numbers = {kotsiantis, sotiris/C-5640-2009
   Papastefanopoulos, Vasileios/ABI-3803-2020
   Kotsiantis, Sotiris/A-8308-2016
   },
ORCID-Numbers = {kotsiantis, sotiris/0000-0002-2247-3082
   Papastefanopoulos, Vasilis/0000-0001-8169-1373
   Linardatos, Pantelis/0000-0003-1132-4724},
Unique-ID = {WOS:000611065800001},
}

@article{ WOS:000895445500045,
Author = {von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and
   Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch,
   Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar
   and Walczak, Michal and Garcke, Jochen and Bauckhage, Christian and
   Schuecker, Jannis},
Title = {Informed Machine Learning - A Taxonomy and Survey of Integrating Prior
   Knowledge into Learning Systems},
Journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
Year = {2023},
Volume = {35},
Number = {1},
Pages = {614-633},
Month = {JAN 1},
Abstract = {Despite its great success, machine learning can have its limits when
   dealing with insufficient training data. A potential solution is the
   additional integration of prior knowledge into the training process
   which leads to the notion of informed machine learning. In this paper,
   we present a structured overview of various approaches in this field. We
   provide a definition and propose a concept for informed machine learning
   which illustrates its building blocks and distinguishes it from
   conventional machine learning. We introduce a taxonomy that serves as a
   classification framework for informed machine learning approaches. It
   considers the source of knowledge, its representation, and its
   integration into the machine learning pipeline. Based on this taxonomy,
   we survey related research and describe how different knowledge
   representations such as algebraic equations, logic rules, or simulation
   results can be used in learning systems. This evaluation of numerous
   papers on the basis of our taxonomy uncovers key methods in the field of
   informed machine learning.},
DOI = {10.1109/TKDE.2021.3079836},
ISSN = {1041-4347},
EISSN = {1558-2191},
ResearcherID-Numbers = {Padavagodu Ramamurthy, Rahul/OIR-9141-2025
   Bauckhage, Christian/M-7872-2014},
ORCID-Numbers = {Giesselbach, Sven/0000-0002-2691-1406
   Mayer, Sebastian/0000-0002-1909-7805
   },
Unique-ID = {WOS:000895445500045},
}

@article{ WOS:000957059700001,
Author = {Xu, Pengcheng and Ji, Xiaobo and Li, Minjie and Lu, Wencong},
Title = {Small data machine learning in materials science},
Journal = {NPJ COMPUTATIONAL MATERIALS},
Year = {2023},
Volume = {9},
Number = {1},
Month = {MAR 25},
Abstract = {This review discussed the dilemma of small data faced by materials
   machine learning. First, we analyzed the limitations brought by small
   data. Then, the workflow of materials machine learning has been
   introduced. Next, the methods of dealing with small data were
   introduced, including data extraction from publications, materials
   database construction, high-throughput computations and experiments from
   the data source level; modeling algorithms for small data and imbalanced
   learning from the algorithm level; active learning and transfer learning
   from the machine learning strategy level. Finally, the future directions
   for small data machine learning in materials science were proposed.},
DOI = {10.1038/s41524-023-01000-z},
Article-Number = {42},
EISSN = {2057-3960},
ResearcherID-Numbers = {LI, Minjie/GXH-7711-2022
   Ji, Xiaobo/AFO-0372-2022},
Unique-ID = {WOS:000957059700001},
}

@article{ WOS:001171070000005,
Author = {Cho, Hunyong and She, Jane and De Marchi, Daniel and El-Zaatari, Helal
   and Barnes, Edward L. and Kahkoska, Anna R. and Kosorok, Michael R. and
   Virkud, Arti V.},
Title = {Machine Learning and Health Science Research: Tutorial},
Journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
Year = {2024},
Volume = {26},
Month = {JAN 30},
Abstract = {Machine learning (ML) has seen impressive growth in health science
   research due to its capacity for handling complex data to perform a
   range of tasks, including unsupervised learning, supervised learning,
   and reinforcement learning. To aid health science researchers in
   understanding the strengths and limitations of ML and to facilitate its
   integration into their studies, we present here a guideline for
   integrating ML into an analysis through a structured framework, covering
   steps from framing a research question to study design and analysis
   techniques for specialized data types.},
DOI = {10.2196/50890},
Article-Number = {e50890},
ISSN = {1438-8871},
ResearcherID-Numbers = {Kosorok, Michael/ABB-7427-2021
   },
ORCID-Numbers = {Virkud, Arti/0000-0003-0673-5780
   El-Zaatari, Helal/0000-0003-0617-1559
   She, Jane/0000-0001-8488-4357
   Kahkoska, Anna/0000-0003-2701-101X
   Barnes, Edward/0000-0001-9894-8796},
Unique-ID = {WOS:001171070000005},
}

@article{ WOS:000893245700006,
Author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
Title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
Journal = {ACM COMPUTING SURVEYS},
Year = {2023},
Volume = {55},
Number = {6},
Month = {JUN},
Abstract = {In recent years, machine learning has transitioned from a field of
   academic research interest to a field capable of solving real-world
   business problems. However, the deployment of machine learning models in
   production systems can present a number of issues and concerns. This
   survey reviews published reports of deploying machine learning solutions
   in a variety of use cases, industries, and applications and extracts
   practical considerations corresponding to stages of the machine learning
   deployment workflow. By mapping found challenges to the steps of the
   machine learning deployment workflow, we show that practitioners face
   issues at each stage of the deployment process. The goal of this article
   is to lay out a research agenda to explore approaches addressing these
   challenges.},
DOI = {10.1145/3533378},
Article-Number = {114},
ISSN = {0360-0300},
EISSN = {1557-7341},
ORCID-Numbers = {Paleyes, Andrei/0000-0002-3703-8163
   Lawrence, Neil/0000-0001-9258-1030},
Unique-ID = {WOS:000893245700006},
}

@article{ WOS:001028153700001,
Author = {Chen, Zhe and Xiao, Fu and Guo, Fangzhou and Yan, Jinyue},
Title = {Interpretable machine learning for building energy management: A
   state-of-the-art review},
Journal = {ADVANCES IN APPLIED ENERGY},
Year = {2023},
Volume = {9},
Month = {FEB},
Abstract = {Machine learning has been widely adopted for improving building energy
   efficiency and flexibility in the past decade owing to the
   ever-increasing availability of massive building operational data.
   However, it is challenging for end-users to understand and trust machine
   learning models because of their black-box nature. To this end, the
   interpretability of machine learning models has attracted increasing
   attention in recent studies because it helps users understand the
   decisions made by these models. This article reviews previous studies
   that adopted interpretable machine learning techniques for building
   energy management to analyze how model interpretability is improved.
   First, the studies are categorized according to the application stages
   of interpretable machine learning techniques: ante-hoc and post-hoc
   approaches. Then, the studies are analyzed in detail according to
   specific techniques with critical comparisons. Through the review, we
   find that the broad application of interpretable machine learning in
   building energy management faces the following significant challenges:
   (1) different terminologies are used to describe model interpretability
   which could cause confusion, (2) performance of interpretable ML in
   different tasks is difficult to compare, and (3) current prevalent
   techniques such as SHAP and LIME can only provide limited
   interpretability. Finally, we discuss the future R \& D needs for
   improving the interpretability of black-box models that could be
   significant to accelerate the application of machine learning for
   building energy management.},
DOI = {10.1016/j.adapen.2023.100123},
EarlyAccessDate = {JAN 2023},
Article-Number = {100123},
ISSN = {2666-7924},
ResearcherID-Numbers = {YAN, JINYUE/Y-3099-2019
   Xiao, Fu/F-7680-2014
   chen, zhe/ADG-7451-2022},
ORCID-Numbers = {Yan, Jinyue/0000-0003-0300-0762
   Xiao, Fu/0000-0002-3779-3943
   chen, zhe/0000-0003-4715-5651
   },
Unique-ID = {WOS:001028153700001},
}

@article{ WOS:001194621500001,
Author = {Hendrickx, Kilian and Perini, Lorenzo and van der Plas, Dries and Meert,
   Wannes and Davis, Jesse},
Title = {Machine learning with a reject option: a survey},
Journal = {MACHINE LEARNING},
Year = {2024},
Volume = {113},
Number = {5},
Pages = {3073-3110},
Month = {MAY},
Abstract = {Machine learning models always make a prediction, even when it is likely
   to be inaccurate. This behavior should be avoided in many decision
   support applications, where mistakes can have severe consequences.
   Albeit already studied in 1970, machine learning with rejection recently
   gained interest. This machine learning subfield enables machine learning
   models to abstain from making a prediction when likely to make a
   mistake. This survey aims to provide an overview on machine learning
   with rejection. We introduce the conditions leading to two types of
   rejection, ambiguity and novelty rejection, which we carefully
   formalize. Moreover, we review and categorize strategies to evaluate a
   model's predictive and rejective quality. Additionally, we define the
   existing architectures for models with rejection and describe the
   standard techniques for learning such models. Finally, we provide
   examples of relevant application domains and show how machine learning
   with rejection relates to other machine learning research areas.},
DOI = {10.1007/s10994-024-06534-x},
EarlyAccessDate = {MAR 2024},
ISSN = {0885-6125},
EISSN = {1573-0565},
ResearcherID-Numbers = {Meert, Wannes/JHU-3652-2023
   Meert, Wannes/C-7019-2013
   Davis, Jesse/A-3596-2015},
ORCID-Numbers = {Davis, Jesse/0000-0002-3748-9263
   Van der Plas, Dries/0000-0001-5341-1354
   Meert, Wannes/0000-0001-9560-3872
   },
Unique-ID = {WOS:001194621500001},
}

@article{ WOS:000298103200003,
Author = {Pedregosa, Fabian and Varoquaux, Gaeel and Gramfort, Alexandre and
   Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel,
   Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and
   Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and
   Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
Title = {Scikit-learn: Machine Learning in Python},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2011},
Volume = {12},
Pages = {2825-2830},
Month = {OCT},
Abstract = {Scikit-learn is a Python module integrating a wide range of
   state-of-the-art machine learning algorithms for medium-scale supervised
   and unsupervised problems. This package focuses on bringing machine
   learning to non-specialists using a general-purpose high-level language.
   Emphasis is put on ease of use, performance, documentation, and API
   consistency. It has minimal dependencies and is distributed under the
   simplified BSD license, encouraging its use in both academic and
   commercial settings. Source code, binaries, and documentation can be
   downloaded from http://scikit-learn.sourceforge.net.},
ISSN = {1532-4435},
ResearcherID-Numbers = {Passos, Alexandre/B-5958-2016
   Varoquaux, Gael/ABQ-5456-2022
   Pedregosa, Fabian/U-3477-2019
   DUCHESNAY, EDOUARD/AAS-4046-2020
   },
ORCID-Numbers = {VanderPlas, Jacob/0000-0002-9623-3401
   Duchesnay, Edouard/0000-0002-4073-3490
   Gramfort, Alexandre/0000-0001-9791-4404
   Varoquaux, Gael/0000-0003-1076-5122},
Unique-ID = {WOS:000298103200003},
}

@article{ WOS:001370097700001,
Author = {Noordijk, Ben and Gomez, Monica L. Garcia and ten Tusscher, Kirsten H.
   W. J. and de Ridder, Dick and van Dijk, Aalt D. J. and Smith, Robert W.},
Title = {The rise of scientific machine learning: a perspective on combining
   mechanistic modelling with machine learning for systems biology},
Journal = {FRONTIERS IN SYSTEMS BIOLOGY},
Year = {2024},
Volume = {4},
Month = {AUG 2},
Abstract = {Both machine learning and mechanistic modelling approaches have been
   used independently with great success in systems biology. Machine
   learning excels in deriving statistical relationships and quantitative
   prediction from data, while mechanistic modelling is a powerful approach
   to capture knowledge and infer causal mechanisms underpinning biological
   phenomena. Importantly, the strengths of one are the weaknesses of the
   other, which suggests that substantial gains can be made by combining
   machine learning with mechanistic modelling, a field referred to as
   Scientific Machine Learning (SciML). In this review we discuss recent
   advances in combining these two approaches for systems biology, and
   point out future avenues for its application in the biological sciences.},
DOI = {10.3389/fsysb.2024.1407994},
Article-Number = {1407994},
EISSN = {2674-0702},
ResearcherID-Numbers = {de Ridder, Dick/F-3169-2010
   Tusscher, Kirsten/B-9427-2011
   Garcia-Gomez, Monica/AAQ-6134-2021},
ORCID-Numbers = {Noordijk, Ben/0000-0001-7667-179X
   Garcia-Gomez, Monica L/0000-0002-0234-0418
   },
Unique-ID = {WOS:001370097700001},
}

@article{ WOS:001086797900003,
Author = {Li, Jin and Wu, Naiteng and Zhang, Jian and Wu, Hong-Hui and Pan,
   Kunming and Wang, Yingxue and Liu, Guilong and Liu, Xianming and Yao,
   Zhenpeng and Zhang, Qiaobao},
Title = {Machine Learning-Assisted Low-Dimensional Electrocatalysts Design for
   Hydrogen Evolution Reaction},
Journal = {NANO-MICRO LETTERS},
Year = {2023},
Volume = {15},
Number = {1},
Month = {DEC},
Abstract = {Efficient electrocatalysts are crucial for hydrogen generation from
   electrolyzing water. Nevertheless, the conventional ``trial and
   error{''} method for producing advanced electrocatalysts is not only
   cost-ineffective but also time-consuming and labor-intensive.
   Fortunately, the advancement of machine learning brings new
   opportunities for electrocatalysts discovery and design. By analyzing
   experimental and theoretical data, machine learning can effectively
   predict their hydrogen evolution reaction (HER) performance. This review
   summarizes recent developments in machine learning for low-dimensional
   electrocatalysts, including zero-dimension nanoparticles and
   nanoclusters, one-dimensional nanotubes and nanowires, two-dimensional
   nanosheets, as well as other electrocatalysts. In particular, the
   effects of descriptors and algorithms on screening low-dimensional
   electrocatalysts and investigating their HER performance are
   highlighted. Finally, the future directions and perspectives for machine
   learning in electrocatalysis are discussed, emphasizing the potential
   for machine learning to accelerate electrocatalyst discovery, optimize
   their performance, and provide new insights into electrocatalytic
   mechanisms. Overall, this work offers an in-depth understanding of the
   current state of machine learning in electrocatalysis and its potential
   for future research.
   {[}GRAPHICS]},
DOI = {10.1007/s40820-023-01192-5},
Article-Number = {227},
ISSN = {2311-6706},
EISSN = {2150-5551},
ResearcherID-Numbers = {Wu, Naiteng/GYV-0408-2022
   Yao, Zhenpeng/LTC-7513-2024
   Wu, Hong-Hui/M-2215-2013
   Zhang, Qiaobao/HCH-4895-2022},
ORCID-Numbers = {Li, Jin/0000-0002-1066-8923
   },
Unique-ID = {WOS:001086797900003},
}

@article{ WOS:000987798300001,
Author = {An, Qi and Rahman, Saifur and Zhou, Jingwen and Kang, James Jin},
Title = {A Comprehensive Review on Machine Learning in Healthcare Industry:
   Classification, Restrictions, Opportunities and Challenges},
Journal = {SENSORS},
Year = {2023},
Volume = {23},
Number = {9},
Month = {APR 22},
Abstract = {Recently, various sophisticated methods, including machine learning and
   artificial intelligence, have been employed to examine health-related
   data. Medical professionals are acquiring enhanced diagnostic and
   treatment abilities by utilizing machine learning applications in the
   healthcare domain. Medical data have been used by many researchers to
   detect diseases and identify patterns. In the current literature, there
   are very few studies that address machine learning algorithms to improve
   healthcare data accuracy and efficiency. We examined the effectiveness
   of machine learning algorithms in improving time series healthcare
   metrics for heart rate data transmission (accuracy and efficiency). In
   this paper, we reviewed several machine learning algorithms in
   healthcare applications. After a comprehensive overview and
   investigation of supervised and unsupervised machine learning
   algorithms, we also demonstrated time series tasks based on past values
   (along with reviewing their feasibility for both small and large
   datasets).},
DOI = {10.3390/s23094178},
Article-Number = {4178},
EISSN = {1424-8220},
ResearcherID-Numbers = {Rahman, MD Saifur/ABY-2112-2022
   Rahman, PhD, MD Saifur/ABY-2112-2022
   Kang, James Jin/E-4848-2016},
ORCID-Numbers = {An, Qi/0000-0003-4869-1763
   Rahman, PhD, MD Saifur/0000-0001-8345-0952
   Kang, James Jin/0000-0002-0242-4187},
Unique-ID = {WOS:000987798300001},
}

@article{ WOS:001102175100001,
Author = {Asnicar, Francesco and Thomas, Andrew Maltez and Passerini, Andrea and
   Waldron, Levi and Segata, Nicola},
Title = {Machine learning for microbiologists},
Journal = {NATURE REVIEWS MICROBIOLOGY},
Year = {2024},
Volume = {22},
Number = {4},
Pages = {191-205},
Month = {APR},
Abstract = {Machine learning is increasingly important in microbiology where it is
   used for tasks such as predicting antibiotic resistance and associating
   human microbiome features with complex host diseases. The applications
   in microbiology are quickly expanding and the machine learning tools
   frequently used in basic and clinical research range from classification
   and regression to clustering and dimensionality reduction. In this
   Review, we examine the main machine learning concepts, tasks and
   applications that are relevant for experimental and clinical
   microbiologists. We provide the minimal toolbox for a microbiologist to
   be able to understand, interpret and use machine learning in their
   experimental and translational activities.
   In this Review, Segata, Waldron and colleagues discuss important key
   concepts of machine learning that are relevant to microbiologists and
   provide them with a set of tools essential to apply machine learning in
   microbiology research.},
DOI = {10.1038/s41579-023-00984-1},
EarlyAccessDate = {NOV 2023},
ISSN = {1740-1526},
EISSN = {1740-1534},
ResearcherID-Numbers = {Asnicar, Francesco/AAB-9255-2020
   Waldron, Levi/H-2967-2019
   Waldron, Levi David/H-2967-2019
   passerini, andrea/JBS-4696-2023
   Segata, Nicola/K-7240-2016},
ORCID-Numbers = {Thomas, Andrew Maltez/0000-0001-5789-3354
   Asnicar, Francesco/0000-0003-3732-1468
   Waldron, Levi David/0000-0003-2725-0694
   Segata, Nicola/0000-0002-1583-5794},
Unique-ID = {WOS:001102175100001},
}

@article{ WOS:000906478900001,
Author = {Liu, Songlin and Wang, Luqi and Zhang, Wengang and He, Yuwei and Pijush,
   Samui},
Title = {A comprehensive review of machine learning-based methods in landslide
   susceptibility mapping},
Journal = {GEOLOGICAL JOURNAL},
Year = {2023},
Volume = {58},
Number = {6},
Pages = {2283-2301},
Month = {JUN},
Abstract = {Landslide susceptibility mapping (LSM) has been widely used as an
   important reference for development and construction planning to
   mitigate the potential social-eco impact caused by landslides.
   Originally, most of those maps were generated by the judgements of
   experts, which is time-consuming and laborious, and whose accuracy is
   difficult to be quantified because of the subjective effects. With the
   development of machine learning algorithms and the methods of data
   collection, big data and artificial intelligence have now been
   popularized in this field, significantly improving mapping accuracy and
   efficiency. Various machine learning-based methods, mainly including
   conventional machine learning, deep learning, and transfer learning have
   been applied and compared in LSM in different areas by previous
   researchers. Nevertheless, none of them can be effective in all cases.
   Although deep learning-based methods were proven more accurate than
   conventional machine learning-based methods in most data-rich
   situations, the latter is sometimes more popularly used in LSM, as there
   is not that much data in this field to train a deep learning network
   perfectly. In a more rigorous situation when there is very limited data,
   transfer learning-based approaches are applied by several researchers,
   which have contributed to improve the workability and the accuracy of
   LSM in data-limited areas. Such technical explosion has promoted the
   application of landslide susceptibility maps, thus contributing to
   mitigating the social-eco impact associated with landslides. This paper
   comprehensively reviews the whole process of generating landslide
   susceptibility maps based on machine learning methods, introduces and
   compares the commonly used machine learning methods, and discusses the
   topics for future research.},
DOI = {10.1002/gj.4666},
EarlyAccessDate = {JAN 2023},
ISSN = {0072-1050},
EISSN = {1099-1034},
ResearcherID-Numbers = {He, Yuwei/ABD-4266-2021
   samui, pijush/W-5860-2019
   Wang, Luqi/IVH-3736-2023
   Zhang, Wengang/A-5427-2019},
Unique-ID = {WOS:000906478900001},
}

@article{ WOS:001438192600001,
Author = {Li, Fei and Yigitcanlar, Tan and Nepal, Madhav and Nguyen, Kien and Dur,
   Fatih},
Title = {Machine learning and remote sensing integration for leveraging urban
   sustainability: A review and framework},
Journal = {SUSTAINABLE CITIES AND SOCIETY},
Year = {2023},
Volume = {96},
Month = {SEP},
Abstract = {Climate change and rapid urbanisation exacerbated multiple urban issues
   threatening urban sustainability. Numerous studies integrated machine
   learning and remote sensing to monitor urban issues and develop
   mitigation strategies for sustainability. However, few studies
   comparatively analysed joint applications of machine learning and remote
   sensing for urban issues and sustainability. This paper presents a
   systematic review and formulates a framework integrating machine
   learning and remote sensing in urban studies. The literature analysis
   reveals: Most studies occurred in Asia, Europe, and North America,
   driven by technical and ethical factors, highlighting responsible
   approaches for data-scarce regions; Reviewed studies prioritised
   physical spatial aspects over socioeconomic factors, requiring
   multi-source data for comprehensive analysis; Conventional satellite,
   aerial images, and Lidar data are prevalent due to affordability,
   quality, and accessibility; Although supervised machine learning
   dominates, unsupervised methods and algorithm selection paradigms
   require exploration; Integration offers accurate results and thorough
   analysis in image processing and analytics, while image acquisition and
   decision-making necessitate human supervision. This paper provides a
   comprehensive review and an integrative framework for machine learning
   and remote sensing, enriching insights into their potential for urban
   studies and spatial analytics. The study informs urban planning and
   policymaking by promoting efficient management via enhanced machine
   learning and remote sensing integration, and bolstering data-driven
   decision-making.},
DOI = {10.1016/j.scs.2023.104653},
Article-Number = {104653},
ISSN = {2210-6707},
EISSN = {2210-6715},
ResearcherID-Numbers = {Yigitcanlar, Tan/J-1142-2012
   Nepal, Madhav/AAW-9947-2020
   Thanh, Kien/G-2571-2017
   },
ORCID-Numbers = {Yigitcanlar, Tan/0000-0001-7262-7118
   Li, Fei/0000-0001-9529-1629
   Nepal, Madhav/0000-0001-8465-8886
   Nguyen, Kien/0000-0002-3466-9218},
Unique-ID = {WOS:001438192600001},
}

@article{ WOS:001165437500001,
Author = {Mobarak, Md Hosne and Mimona, Mariam Akter and Islam, Md. Aminul and
   Hossain, Nayem and Zohura, Fatema Tuz and Imtiaz, Ibnul and Rimon, Md
   Israfil Hossain},
Title = {Scope of machine learning in materials research-A review},
Journal = {APPLIED SURFACE SCIENCE ADVANCES},
Year = {2023},
Volume = {18},
Month = {DEC},
Abstract = {This comprehensive review investigates the multifaceted applications of
   machine learning in materials research across six key dimensions,
   redefining the field's boundaries. It explains various knowledge
   acquisition mechanisms starting with supervised, unsupervised,
   reinforcement, and deep learning techniques. These techniques are
   transformative tools for transforming unactionable data into insightful
   actions. Moving on to the materials synthesis, the review emphasizes the
   profound influence of machine learning, as demonstrated by predictive
   models that speed up material selection, structure-property
   relationships that reveal crucial connections, and data-driven discovery
   that fosters innovation. Machine learning reshapes our comprehension and
   manipulation of materials by accelerating discovery and enabling
   tailored design through property prediction models and
   structure-property relationships. Machine learning extends its influence
   to image processing, improving object detection, classification, and
   segmentation precision and enabling methods like image generation,
   revolutionizing the potential of image processing in materials research.
   The most recent developments show how machine learning can have a
   transformative impact at the atomic level by enabling precise property
   prediction and intricate data extraction, representing significant
   advancements in material understanding and innovation. The review
   highlights how machine learning has the potential to revolutionize
   materials research by accelerating discovery, improving performance, and
   stimulating innovation. It does so while acknowledging obstacles like
   poor data quality and complicated algorithms. Machine learning offers a
   wide range of exciting prospects for scientific investigation and
   technological advancement, positioning it as a powerful force for
   influencing the future of materials research.},
DOI = {10.1016/j.apsadv.2023.100523},
EarlyAccessDate = {NOV 2023},
Article-Number = {100523},
ISSN = {2666-5239},
ResearcherID-Numbers = {Hossain, Nayem/AAD-4022-2021
   Mobarak, Md Hosne/KEZ-8543-2024
   ISLAM, MD AMINUL/GVU-3150-2022
   },
ORCID-Numbers = {Islam, Md. Aminul/0009-0002-5753-791X
   Rimon, Md Israfil Hossain/0009-0005-2035-0360
   Mobarak, Md Hosne/0009-0004-8563-9336},
Unique-ID = {WOS:001165437500001},
}

@article{ WOS:000638010100001,
Author = {Janiesch, Christian and Zschech, Patrick and Heinrich, Kai},
Title = {Machine learning and deep learning},
Journal = {ELECTRONIC MARKETS},
Year = {2021},
Volume = {31},
Number = {3},
Pages = {685-695},
Month = {SEP},
Abstract = {Today, intelligent systems that offer artificial intelligence
   capabilities often rely on machine learning. Machine learning describes
   the capacity of systems to learn from problem-specific training data to
   automate the process of analytical model building and solve associated
   tasks. Deep learning is a machine learning concept based on artificial
   neural networks. For many applications, deep learning models outperform
   shallow machine learning models and traditional data analysis
   approaches. In this article, we summarize the fundamentals of machine
   learning and deep learning to generate a broader understanding of the
   methodical underpinning of current intelligent systems. In particular,
   we provide a conceptual distinction between relevant terms and concepts,
   explain the process of automated analytical model building through
   machine learning and deep learning, and discuss the challenges that
   arise when implementing such intelligent systems in the field of
   electronic markets and networked business. These naturally go beyond
   technological aspects and highlight issues in human-machine interaction
   and artificial intelligence servitization.},
DOI = {10.1007/s12525-021-00475-2},
EarlyAccessDate = {APR 2021},
ISSN = {1019-6781},
EISSN = {1422-8890},
ResearcherID-Numbers = {Diaz-Ramirez, Jorge/AAU-6053-2020
   Heinrich, Kai/KYQ-2254-2024
   Janiesch, Christian/HZJ-2795-2023
   Zschech, Patrick/AAG-6517-2019},
ORCID-Numbers = {Heinrich, Kai/0000-0002-4907-6802
   Diaz-Ramirez, Jorge/0000-0001-5335-576X
   Janiesch, Christian/0000-0002-8050-123X
   Zschech, Patrick/0000-0002-1105-8086
   },
Unique-ID = {WOS:000638010100001},
}

@article{ WOS:000977155100001,
Author = {Tufail, Shahid and Riggs, Hugo and Tariq, Mohd and Sarwat, Arif I.},
Title = {Advancements and Challenges in Machine Learning: A Comprehensive Review
   of Models, Libraries, Applications, and Algorithms},
Journal = {ELECTRONICS},
Year = {2023},
Volume = {12},
Number = {8},
Month = {APR},
Abstract = {In the current world of the Internet of Things, cyberspace, mobile
   devices, businesses, social media platforms, healthcare systems, etc.,
   there is a lot of data online today. Machine learning (ML) is something
   we need to understand to do smart analyses of these data and make smart,
   automated applications that use them. There are many different kinds of
   machine learning algorithms. The most well-known ones are supervised,
   unsupervised, semi-supervised, and reinforcement learning. This article
   goes over all the different kinds of machine-learning problems and the
   machine-learning algorithms that are used to solve them. The main thing
   this study adds is a better understanding of the theory behind many
   machine learning methods and how they can be used in the real world,
   such as in energy, healthcare, finance, autonomous driving, e-commerce,
   and many more fields. This article is meant to be a go-to resource for
   academic researchers, data scientists, and machine learning engineers
   when it comes to making decisions about a wide range of data and methods
   to start extracting information from the data and figuring out what kind
   of machine learning algorithm will work best for their problem and what
   results they can expect. Additionally, this article presents the major
   challenges in building machine learning models and explores the research
   gaps in this area. In this article, we also provided a brief overview of
   data protection laws and their provisions in different countries.},
DOI = {10.3390/electronics12081789},
Article-Number = {1789},
ISSN = {2079-9292},
ResearcherID-Numbers = {Riggs, Hugo/KEH-1646-2024
   Sarwat, Arif/A-9582-2017
   Tariq, Mohd/ABB-5031-2021
   Tufail, Shahid/GPX-2777-2022},
ORCID-Numbers = {Tufail, Shahid/0000-0001-6469-015X
   Sarwat, Arif/0000-0003-1179-438X
   Riggs, Hugo/0000-0001-8884-7270
   Tariq, Mohd/0000-0002-5162-7626
   },
Unique-ID = {WOS:000977155100001},
}

@article{ WOS:000939178900001,
Author = {Zeguendry, Amine and Jarir, Zahi and Quafafou, Mohamed},
Title = {Quantum Machine Learning: A Review and Case Studies},
Journal = {ENTROPY},
Year = {2023},
Volume = {25},
Number = {2},
Month = {FEB},
Abstract = {Despite its undeniable success, classical machine learning remains a
   resource-intensive process. Practical computational efforts for training
   state-of-the-art models can now only be handled by high speed computer
   hardware. As this trend is expected to continue, it should come as no
   surprise that an increasing number of machine learning researchers are
   investigating the possible advantages of quantum computing. The
   scientific literature on Quantum Machine Learning is now enormous, and a
   review of its current state that can be comprehended without a physics
   background is necessary. The objective of this study is to present a
   review of Quantum Machine Learning from the perspective of conventional
   techniques. Departing from giving a research path from fundamental
   quantum theory through Quantum Machine Learning algorithms from a
   computer scientist's perspective, we discuss a set of basic algorithms
   for Quantum Machine Learning, which are the fundamental components for
   Quantum Machine Learning algorithms. We implement the Quanvolutional
   Neural Networks (QNNs) on a quantum computer to recognize handwritten
   digits, and compare its performance to that of its classical
   counterpart, the Convolutional Neural Networks (CNNs). Additionally, we
   implement the QSVM on the breast cancer dataset and compare it to the
   classical SVM. Finally, we implement the Variational Quantum Classifier
   (VQC) and many classical classifiers on the Iris dataset to compare
   their accuracies.},
DOI = {10.3390/e25020287},
Article-Number = {287},
EISSN = {1099-4300},
ResearcherID-Numbers = {Zeguendry, Amine/IWE-0642-2023},
ORCID-Numbers = {Zeguendry, Amine/0000-0002-5864-5667},
Unique-ID = {WOS:000939178900001},
}

@article{ WOS:000926890700009,
Author = {Parmar, Jitendra and Chouhan, Satyendra and Raychoudhury, Vaskar and
   Rathore, Santosh},
Title = {Open-world Machine Learning: Applications, Challenges, and Opportunities},
Journal = {ACM COMPUTING SURVEYS},
Year = {2023},
Volume = {55},
Number = {10},
Month = {OCT},
Abstract = {Traditional machine learning, mainly supervised learning, follows the
   assumptions of closed-world learning, i.e., for each testing class, a
   training class is available. However, such machine learning models fail
   to identify the classes, which were not available during training time.
   These classes can be referred to as unseen classes. Open-world Machine
   Learning (OWML) is a novel technique, which deals with unseen classes.
   Although OWML is around for a few years and many significant research
   works have been carried out in this domain, there is no comprehensive
   survey of the characteristics, applications, and impact of OWML on the
   major research areas. In this article, we aimed to capture the different
   dimensions of OWML with respect to other traditional machine learning
   models. We have thoroughly analyzed the existing literature and provided
   a novel taxonomy of OWML considering its two major application domains:
   Computer Vision and Natural Language Processing. We listed the available
   software packages and open datasets in OWML for future researchers.
   Finally, the article concludes with a set of research gaps, open
   challenges, and future directions.},
DOI = {10.1145/3561381},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Rathore, Santosh/T-4516-2019
   RAYCHOUDHURY, VASKAR/L-4021-2019
   Parmar, Jitendra/HKW-6642-2023
   },
ORCID-Numbers = {Parmar, Jitendra/0000-0002-8290-7903
   Rathore, Santosh Singh/0000-0003-2087-1666
   Chouhan, Satyendra Singh/0000-0002-0280-7364},
Unique-ID = {WOS:000926890700009},
}

@article{ WOS:001131493500001,
Author = {Araujo, Sara Oleiro and Peres, Ricardo Silva and Ramalho, Jose Cochicho
   and Lidon, Fernando and Barata, Jose},
Title = {Machine Learning Applications in Agriculture: Current Trends,
   Challenges, and Future Perspectives},
Journal = {AGRONOMY-BASEL},
Year = {2023},
Volume = {13},
Number = {12},
Month = {DEC},
Abstract = {Progress in agricultural productivity and sustainability hinges on
   strategic investments in technological research. Evolving technologies
   such as the Internet of Things, sensors, robotics, Artificial
   Intelligence, Machine Learning, Big Data, and Cloud Computing are
   propelling the agricultural sector towards the transformative
   Agriculture 4.0 paradigm. The present systematic literature review
   employs the Preferred Reporting Items for Systematic Reviews and
   Meta-Analyses (PRISMA) methodology to explore the usage of Machine
   Learning in agriculture. The study investigates the foremost
   applications of Machine Learning, including crop, water, soil, and
   animal management, revealing its important role in revolutionising
   traditional agricultural practices. Furthermore, it assesses the
   substantial impacts and outcomes of Machine Learning adoption and
   highlights some challenges associated with its integration in
   agricultural systems. This review not only provides valuable insights
   into the current landscape of Machine Learning applications in
   agriculture, but it also outlines promising directions for future
   research and innovation in this rapidly evolving field.},
DOI = {10.3390/agronomy13122976},
Article-Number = {2976},
EISSN = {2073-4395},
ResearcherID-Numbers = {Ramalho, Jose/G-2427-2014
   Ramalho, José/G-2427-2014
   Araújo, Sara/AAC-4018-2022
   Barata, Jose/A-4204-2012
   Peres, Ricardo/P-9610-2019},
ORCID-Numbers = {Peres, Ricardo/0000-0003-3777-1346
   Ramalho, Jose/0000-0002-7639-7214
   Oleiro Araujo, Sara/0000-0001-6192-8409
   Barata, Jose/0000-0002-6348-1847
   },
Unique-ID = {WOS:001131493500001},
}

@article{ WOS:000991648200011,
Author = {Demirhan, Umut and Alkhateeb, Ahmed},
Title = {Integrated Sensing and Communication for 6G: Ten Key Machine Learning
   Roles},
Journal = {IEEE COMMUNICATIONS MAGAZINE},
Year = {2023},
Volume = {61},
Number = {5},
Pages = {113-119},
Month = {MAY},
Abstract = {Integrating sensing and communication is a defining theme for future
   wireless systems. This is motivated by the promising performance gains,
   especially as they assist each other, and by the better utilization of
   the wireless and hardware resources. Realizing these gains in practice,
   however, is subject to several challenges where leveraging machine
   learning can provide a potential solution. This article focuses on ten
   key machine learning roles for joint sensing and communication,
   sensing-aided communication, and communication-aided sensing systems,
   explains why and how machine learning can be utilized, and highlights
   important directions for future research. The article also presents
   real-world results for some of these machine learning roles based on the
   large-scale real-world dataset DeepSense 6G, which could be adopted in
   investigating a wide range of integrated sensing and communication
   problems.},
DOI = {10.1109/MCOM.006.2200480},
ISSN = {0163-6804},
EISSN = {1558-1896},
ResearcherID-Numbers = {Alkhateeb, Ahmed/I-8485-2015
   Demirhan, Umut/GQO-8949-2022},
Unique-ID = {WOS:000991648200011},
}

@article{ WOS:000972340500002,
Author = {Pan, Guangfei and Wang, Feiyang and Shang, Chunlei and Wu, Honghui and
   Wu, Guilin and Gao, Junheng and Wang, Shuize and Gao, Zhijun and Zhou,
   Xiaoye and Mao, Xinping},
Title = {Advances in machine learning- and artificial intelligence-assisted
   material design of steels},
Journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS},
Year = {2023},
Volume = {30},
Number = {6},
Pages = {1003-1024},
Month = {JUN},
Abstract = {With the rapid development of artificial intelligence technology and
   increasing material data, machine learning- and artificial
   intelligence-assisted design of high-performance steel materials is
   becoming a mainstream paradigm in materials science. Machine learning
   methods, based on an interdisciplinary discipline between computer
   science, statistics and material science, are good at discovering
   correlations between numerous data points. Compared with the traditional
   physical modeling method in material science, the main advantage of
   machine learning is that it overcomes the complex physical mechanisms of
   the material itself and provides a new perspective for the research and
   development of novel materials. This review starts with data
   preprocessing and the introduction of different machine learning models,
   including algorithm selection and model evaluation. Then, some
   successful cases of applying machine learning methods in the field of
   steel research are reviewed based on the main theme of optimizing
   composition, structure, processing, and performance. The application of
   machine learning methods to the performance-oriented inverse design of
   material composition and detection of steel defects is also reviewed.
   Finally, the applicability and limitations of machine learning in the
   material field are summarized, and future directions and prospects are
   discussed.},
DOI = {10.1007/s12613-022-2595-0},
ISSN = {1674-4799},
EISSN = {1869-103X},
ResearcherID-Numbers = {mao, xinping/HJA-8555-2022
   Wu, Hong-Hui/M-2215-2013
   Zhou, Xiaoye/HJP-2370-2023
   WU, Guilin/GWV-2324-2022},
Unique-ID = {WOS:000972340500002},
}

@article{ WOS:000935967900001,
Author = {Melnikov, Alexey and Kordzanganeh, Mohammad and Alodjants, Alexander and
   Lee, Ray-Kuang},
Title = {Quantum machine learning: from physics to software engineering},
Journal = {ADVANCES IN PHYSICS-X},
Year = {2023},
Volume = {8},
Number = {1},
Month = {DEC 31},
Abstract = {Quantum machine learning is a rapidly growing field at the intersection
   of quantum technology and artificial intelligence. This review provides
   a two-fold overview of several key approaches that can offer
   advancements in both the development of quantum technologies and the
   power of artificial intelligence. Among these approaches are
   quantum-enhanced algorithms, which apply quantum software engineering to
   classical information processing to improve keystone machine learning
   solutions. In this context, we explore the capability of hybrid
   quantum-classical neural networks to improve model generalization and
   increase accuracy while reducing computational resources. We also
   illustrate how machine learning can be used both to mitigate the effects
   of errors on presently available noisy intermediate-scale quantum
   devices, and to understand quantum advantage via an automatic study of
   quantum walk processes on graphs. In addition, we review how quantum
   hardware can be enhanced by applying machine learning to fundamental and
   applied physics problems as well as quantum tomography and photonics. We
   aim to demonstrate how concepts in physics can be translated into
   practical engineering of machine learning solutions using quantum
   software.},
DOI = {10.1080/23746149.2023.2165452},
Article-Number = {2165452},
ISSN = {2374-6149},
ResearcherID-Numbers = {Alodjants, Alexander/G-7297-2016
   Melnikov, Alexey/L-7441-2016
   Lee, Ray-Kuang/C-1039-2009},
ORCID-Numbers = {Kordzanganeh, Mo/0000-0003-3150-5397
   Melnikov, Alexey/0000-0002-5033-4063
   Lee, Ray-Kuang/0000-0002-7171-7274},
Unique-ID = {WOS:000935967900001},
}

@article{ WOS:001193522300001,
Author = {Chen, Linshu and Li, Tao and Chen, Yuxiang and Chen, Xiaoyan and
   Wozniak, Marcin and Xiong, Neal and Liang, Wei},
Title = {Design and analysis of quantum machine learning: a survey},
Journal = {CONNECTION SCIENCE},
Year = {2024},
Volume = {36},
Number = {1},
Month = {DEC 31},
Abstract = {Machine learning has demonstrated tremendous potential in solving
   real-world problems. However, with the exponential growth of data amount
   and the increase of model complexity, the processing efficiency of
   machine learning declines rapidly. Meanwhile, the emergence of quantum
   computing has given rise to quantum machine learning, which relies on
   superposition and entanglement, exhibiting exponential optimisation
   compared to traditional machine learning. Therefore, in the paper, we
   survey the basic concepts, algorithms, applications and challenges of
   quantum machine learning. Concretely, we first review the basic concepts
   of quantum computing including qubit, quantum gates, quantum
   entanglement, etc.. Secondly, we in-depth discuss 5 quantum machine
   learning algorithms of quantum support vector machine, quantum neural
   network, quantum k-nearest neighbour, quantum principal component
   analysis and quantum k-Means algorithm. Thirdly, we conduct discussions
   on the applications of quantum machine learning in image recognition,
   drug efficacy prediction and cybersecurity. Finally, we summarise the
   challenges of quantum machine learning consisting of algorithm design,
   hardware limitations, data encoding, quantum landscapes, noise and
   decoherence.},
DOI = {10.1080/09540091.2024.2312121},
Article-Number = {2312121},
ISSN = {0954-0091},
EISSN = {1360-0494},
ResearcherID-Numbers = {Woźniak, Marcin/GLS-5850-2022
   Chen, Yuxiang/GZK-9342-2022
   },
ORCID-Numbers = {Liang, Wei/0000-0002-5074-1363},
Unique-ID = {WOS:001193522300001},
}

@article{ WOS:001359559300001,
Author = {Lu, Wei and Lu, Yang and Li, Jin and Sigov, Alexander and Ratkin, Leonid
   and Ivanov, Leonid A.},
Title = {Quantum machine learning: Classifications, challenges, and solutions},
Journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION},
Year = {2024},
Volume = {42},
Month = {NOV},
Abstract = {Recently, research at the intersection of quantum mechanics and machine
   learning has gained attention. This interdisciplinary field aims to
   tackle the computational efficiency of machine learning by leveraging
   quantum computing and to derive novel machine learning algorithms
   inspired by quantum principles. Despite substantial progress in quantum
   science research, several challenges persist, including the preservation
   of quantum coherence, mitigation of environmental constraints, advancing
   quantum computer development, and formulating comprehensive quantum
   machine learning algorithms. To date, a comprehensive theoretical
   framework for quantum machine learning is lacking, with much of the
   research still in the exploratory and experimental stages. This study
   conducts a thorough survey on quantum machine learning, with the aim of
   classifying quantum machine learning algorithms while addressing the
   existing challenges and potential solutions in this emerging field.},
DOI = {10.1016/j.jii.2024.100736},
EarlyAccessDate = {NOV 2024},
Article-Number = {100736},
ISSN = {2467-964X},
EISSN = {2452-414X},
ResearcherID-Numbers = {Ivanov, Leonid/JBS-0068-2023
   Li, Jin/J-7532-2019},
Unique-ID = {WOS:001359559300001},
}

@article{ WOS:001061188500001,
Author = {Huang, Guannan and Guo, Yani and Chen, Ye and Nie, Zhengwei},
Title = {Application of Machine Learning in Material Synthesis and Property
   Prediction},
Journal = {MATERIALS},
Year = {2023},
Volume = {16},
Number = {17},
Month = {SEP},
Abstract = {Material innovation plays a very important role in technological
   progress and industrial development. Traditional experimental
   exploration and numerical simulation often require considerable time and
   resources. A new approach is urgently needed to accelerate the discovery
   and exploration of new materials. Machine learning can greatly reduce
   computational costs, shorten the development cycle, and improve
   computational accuracy. It has become one of the most promising research
   approaches in the process of novel material screening and material
   property prediction. In recent years, machine learning has been widely
   used in many fields of research, such as superconductivity,
   thermoelectrics, photovoltaics, catalysis, and high-entropy alloys. In
   this review, the basic principles of machine learning are briefly
   outlined. Several commonly used algorithms in machine learning models
   and their primary applications are then introduced. The research
   progress of machine learning in predicting material properties and
   guiding material synthesis is discussed. Finally, a future outlook on
   machine learning in the materials science field is presented.},
DOI = {10.3390/ma16175977},
Article-Number = {5977},
EISSN = {1996-1944},
ResearcherID-Numbers = {黄, 冠南/GPX-5727-2022
   },
ORCID-Numbers = {Nie, Zhengwei/0000-0002-4374-8620
   Huang, Guannan/0009-0003-8631-304X},
Unique-ID = {WOS:001061188500001},
}

@article{ WOS:000978351900001,
Author = {Zhang, Wentao and Chen, Ronghua and Li, Jie and Huang, Tianyin and Wu,
   Bingdang and Ma, Jun and Wen, Qingqi and Tan, Jie and Huang, Wenguang},
Title = {Synthesis optimization and adsorption modeling of biochar for pollutant
   removal via machine learning},
Journal = {BIOCHAR},
Year = {2023},
Volume = {5},
Number = {1},
Month = {APR 23},
Abstract = {Due to large specific surface area, abundant functional groups and low
   cost, biochar is widely used for pollutant removal. The adsorption
   performance of biochar is related to biochar synthesis and adsorption
   parameters. But the influence factor is numerous, the traditional
   experimental enumeration is powerless. In recent years, machine learning
   has been gradually employed for biochar, but there is no comprehensive
   review on the whole process regulation of biochar adsorbents, covering
   synthesis optimization and adsorption modeling. This review article
   systematically summarized the application of machine learning in biochar
   adsorbents from the perspective of all-round regulation for the first
   time, including the synthesis optimization and adsorption modeling of
   biochar adsorbents. Firstly, the overview of machine learning was
   introduced. Then, the latest advances of machine learning in biochar
   synthesis for pollutant removal were summarized, including prediction of
   biochar yield and physicochemical properties, optimal synthetic
   conditions and economic cost. And the application of machine learning in
   pollutant adsorption by biochar was reviewed, covering prediction of
   adsorption efficiency, optimization of experimental conditions and
   revelation of adsorption mechanism. General guidelines for the
   application of machine learning in whole-process optimization of biochar
   from synthesis to adsorption were presented. Finally, the existing
   problems and future perspectives of machine learning for biochar
   adsorbents were put forward. We hope that this review can promote the
   integration of machine learning and biochar, and thus light up the
   industrialization of biochar.},
DOI = {10.1007/s42773-023-00225-x},
Article-Number = {25},
ISSN = {2524-7972},
EISSN = {2524-7867},
ResearcherID-Numbers = {Li, Jie/GNP-6986-2022
   Zhang, Wentao/AGC-4653-2022
   ma, jun/F-2506-2013},
ORCID-Numbers = {Zhang, Wentao/0000-0003-1452-7009
   },
Unique-ID = {WOS:000978351900001},
}

@article{ WOS:001073553500001,
Author = {Sonkavde, Gaurang and Dharrao, Deepak Sudhakar and Bongale, Anupkumar M.
   and Deokate, Sarika T. and Doreswamy, Deepak and Bhat, Subraya Krishna},
Title = {Forecasting Stock Market Prices Using Machine Learning and Deep Learning
   Models: A Systematic Review, Performance Analysis and Discussion of
   Implications},
Journal = {INTERNATIONAL JOURNAL OF FINANCIAL STUDIES},
Year = {2023},
Volume = {11},
Number = {3},
Month = {SEP},
Abstract = {The financial sector has greatly impacted the monetary well-being of
   consumers, traders, and financial institutions. In the current era,
   artificial intelligence is redefining the limits of the financial
   markets based on state-of-the-art machine learning and deep learning
   algorithms. There is extensive use of these techniques in financial
   instrument price prediction, market trend analysis, establishing
   investment opportunities, portfolio optimization, etc. Investors and
   traders are using machine learning and deep learning models for
   forecasting financial instrument movements. With the widespread adoption
   of AI in finance, it is imperative to summarize the recent machine
   learning and deep learning models, which motivated us to present this
   comprehensive review of the practical applications of machine learning
   in the financial industry. This article examines algorithms such as
   supervised and unsupervised machine learning algorithms, ensemble
   algorithms, time series analysis algorithms, and deep learning
   algorithms for stock price prediction and solving classification
   problems. The contributions of this review article are as follows: (a)
   it provides a description of machine learning and deep learning models
   used in the financial sector; (b) it provides a generic framework for
   stock price prediction and classification; and (c) it implements an
   ensemble model-{''}Random Forest + XG-Boost + LSTM{''}-for forecasting
   TAINIWALCHM and AGROPHOS stock prices and performs a comparative
   analysis with popular machine learning and deep learning models.},
DOI = {10.3390/ijfs11030094},
Article-Number = {94},
ISSN = {2227-7072},
ResearcherID-Numbers = {Bongale, Anupkumar/U-3537-2019
   Bhat, Subraya Krishna/GZK-2448-2022
   Deokate, Sarika/HPH-1373-2023
   Dharrao, Deepak/HGI-9336-2022
   deokate, sarika/HPH-1373-2023},
ORCID-Numbers = {Doreswamy, Deepak/0000-0001-7194-463X
   Bhat, Subraya Krishna/0000-0003-1798-3480
   Bongale, Anupkumar/0000-0002-5897-0283
   Dharrao, Deepak/0000-0002-2540-6942
   deokate, sarika/0000-0002-5102-4104},
Unique-ID = {WOS:001073553500001},
}

@article{ WOS:001041998200001,
Author = {Rubbens, Peter and Brodie, Stephanie and Cordier, Tristan and Destro
   Barcellos, Diogo and Devos, Paul and Fernandes-Salvador, Jose A. and
   Fincham, I, Jennifer and Gomes, Alessandra and Handegard, Nils Olav and
   Howell, Kerry and Jamet, Cedric and Kartveit, Kyrre Heldal and
   Moustahfid, Hassan and Parcerisas, Clea and Politikos, Dimitris and
   Sauzede, Raphaelle and Sokolova, Maria and Uusitalo, Laura and Van den
   Bulcke, Laure and van Helmond, Aloysius T. M. and Watson, Jordan T. and
   Welch, Heather and Beltran-Perez, Oscar and Chaffron, Samuel and
   Greenberg, David S. and Kuehn, Bernhard and Kiko, Rainer and Lo, Madiop
   and Lopes, Rubens M. and Moeller, Klas Ove and Michaels, William and
   Pala, Ahmet and Romagnan, Jean-Baptiste and Schuchert, Pia and Seydi,
   Vahid and Villasante, Sebastian and Malde, Ketil and Irisson,
   Jean-Olivier},
Title = {Machine learning in marine ecology: an overview of techniques and
   applications},
Journal = {ICES JOURNAL OF MARINE SCIENCE},
Year = {2023},
Volume = {80},
Number = {7},
Pages = {1829-1853},
Month = {SEP 26},
Abstract = {Machine learning covers a large set of algorithms that can be trained to
   identify patterns in data. Thanks to the increase in the amount of data
   and computing power available, it has become pervasive across scientific
   disciplines. We first highlight why machine learning is needed in marine
   ecology. Then we provide a quick primer on machine learning techniques
   and vocabulary. We built a database of \& SIM;1000 publications that
   implement such techniques to analyse marine ecology data. For various
   data types (images, optical spectra, acoustics, omics, geolocations,
   biogeochemical profiles, and satellite imagery), we present a historical
   perspective on applications that proved influential, can serve as
   templates for new work, or represent the diversity of approaches. Then,
   we illustrate how machine learning can be used to better understand
   ecological systems, by combining various sources of marine data. Through
   this coverage of the literature, we demonstrate an increase in the
   proportion of marine ecology studies that use machine learning, the
   pervasiveness of images as a data source, the dominance of machine
   learning for classification-type problems, and a shift towards deep
   learning for all data types. This overview is meant to guide researchers
   who wish to apply machine learning methods to their marine datasets.},
DOI = {10.1093/icesjms/fsad100},
EarlyAccessDate = {AUG 2023},
ISSN = {1054-3139},
EISSN = {1095-9289},
ResearcherID-Numbers = {Villasante, Sebastian/F-5019-2012
   Sauzède, Raphaëlle/AAJ-8122-2021
   Sokolova, Maria/IST-5468-2023
   Irisson, Jean-Olivier/B-3909-2010
   Parcerisas, Clea/GRX-7176-2022
   Barcellos, Diogo/F-3528-2015
   Handegard, Nils/I-3047-2012
   Möller, Klas/AAF-7107-2021
   Chaffron, Samuel/K-1104-2019
   Uusitalo, Laura/MEO-5735-2025
   Cordier, Tristan/B-7836-2018
   Howell, Kerry/H-5044-2013
   Schuchert, Pia/IZP-9807-2023
   M, Ketil/AAH-5604-2020
   Welch, Heather/T-3494-2019
   Handegard, Nils Olav/I-3047-2012
   Watson, Jordan/U-9798-2019
   de Vos, Paul/A-3170-2013
   Rubbens, Peter/O-4737-2016
   Salvador, Jose/AAH-7939-2019
   Lopes, Rubens/C-6335-2012
   Van den Bulcke, Laure/GWW-5593-2022
   Seydi, Vahid/KHZ-7414-2024},
ORCID-Numbers = {Fincham, Jennifer Irene/0000-0001-9172-5514
   Parcerisas, Clea/0000-0001-7466-0288
   Fernandes Salvador, Jose Antonio/0000-0003-4677-6077
   Sokolova, Maria/0000-0002-6380-4052
   PALA, AHMET/0000-0002-5232-0917
   Devos, Paul/0000-0003-4940-8886
   Cordier, Tristan/0000-0001-7398-4790
   Destro Barcellos, Diogo/0000-0002-2903-7320
   Welch, Heather/0000-0002-5464-1140
   Rubbens, Peter/0000-0001-5595-4758
   Beltran Perez, Oscar Dario/0000-0002-7886-6424
   Howell, Kerry/0000-0003-3359-1778
   Handegard, Nils Olav/0000-0002-9708-9042
   Watson, Jordan/0000-0002-1686-0377
   },
Unique-ID = {WOS:001041998200001},
}

@article{ WOS:000935501400001,
Author = {Jiang, Jingchao},
Title = {A survey of machine learning in additive manufacturing technologies},
Journal = {INTERNATIONAL JOURNAL OF COMPUTER INTEGRATED MANUFACTURING},
Year = {2023},
Volume = {36},
Number = {9},
Pages = {1258-1280},
Month = {SEP 2},
Abstract = {Thirty years into its development, additive manufacturing has become a
   mainstream manufacturing process. Additive manufacturing fabricates
   products by adding materials layer-by-layer directly based on a 3D
   model. It is able to manufacture complex parts and allows more freedom
   of design optimization compared with traditional manufacturing
   techniques. Machine learning is now a hot technology that has been used
   in medical diagnosis, image processing, prediction, classification,
   learning association, regression, etc. Currently, focuses are
   increasingly given to using machine learning in the manufacturing
   industry, including additive manufacturing. Due to the rapid development
   of machine learning in additive manufacturing, a special issue `Machine
   Learning in Additive Manufacturing' in International Journal of Computer
   Integrated Manufacturing is organized. This paper gives a comprehensive
   understanding of the current status of machine learning enhanced
   additive manufacturing technologies for this special issue. Discussions
   and future perspectives are also provided.},
DOI = {10.1080/0951192X.2023.2177740},
EarlyAccessDate = {FEB 2023},
ISSN = {0951-192X},
EISSN = {1362-3052},
ResearcherID-Numbers = {Jiang, Jingchao/R-1303-2019},
ORCID-Numbers = {Jiang, Jingchao/0000-0002-0446-3454},
Unique-ID = {WOS:000935501400001},
}

@article{ WOS:001020157200009,
Author = {Attri, Ishana and Awasthi, Lalit Kumar and Sharma, Teek Parval},
Title = {Machine learning in agriculture: a review of crop management
   applications},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2024},
Volume = {83},
Number = {5, SI},
Pages = {12875-12915},
Month = {FEB},
Abstract = {Machine learning has created new opportunities for data-intensive study
   in interdisciplinary domains as a result of the advancement of big data
   technologies and high-performance computers. Search engines, email spam
   filters, websites that offer personalized recommendations, banking
   software that alerts users to suspicious activity, and a plethora of
   smartphone apps that perform tasks like voice recognition, image
   recognition, and natural language processing are just a few examples of
   the online and offline services that have incorporated machine learning
   in recent years. One of the most crucial areas where machine learning
   applications still has to be investigated is agriculture, which directly
   affects people's well-being. In this article, a literature review on
   machine learning algorithms used in agriculture is presented. The
   proposed paper deal with various crop management applications which are
   categorised into five parts i.e., Weed and pest detection, Plant disease
   detection, Stress detection in plants, Smart farms or automation in
   farms and the last one is Crop yield estimation and prediction. The
   articles' filtering and categorization show how machine learning may
   improve agriculture. This article examines machine learning
   breakthroughs in agriculture. This paper's findings show that by using
   novel machine learning approaches, models may achieve improved accuracy
   and shorter inference time for real-world applications.},
DOI = {10.1007/s11042-023-16105-2},
EarlyAccessDate = {JUL 2023},
ISSN = {1380-7501},
EISSN = {1573-7721},
ResearcherID-Numbers = {Sharma, Teek/ABE-2493-2021
   Awasthi, Lalit Kumar/V-3485-2019
   Attri, Ishana/JOJ-5753-2023
   Awasthi, Lalit/V-3485-2019},
ORCID-Numbers = {Sharma, Teek Parval/0000-0002-6324-5457
   Awasthi, Lalit Kumar/0000-0001-8396-9025
   attri, ishana/0000-0001-8779-3231
   },
Unique-ID = {WOS:001020157200009},
}

@article{ WOS:001008445700001,
Author = {Alagumalai, Avinash and Devarajan, Balaji and Song, Hua and Wongwises,
   Somchai and Ledesma-Amaro, Rodrigo and Mahian, Omid and Sheremet,
   Mikhail and Lichtfouse, Eric},
Title = {Machine learning in biohydrogen production: a review},
Journal = {BIOFUEL RESEARCH JOURNAL-BRJ},
Year = {2023},
Volume = {10},
Number = {2},
Pages = {1844-1858},
Month = {JUN},
Abstract = {Biohydrogen is emerging as a promising carbon-neutral and sustainable
   energy carrier with high energy yield to replace conventional fossil
   fuels. However, biohydrogen commercial uptake is mainly hindered by the
   supply side. As a result, various operating parameters must be optimized
   to realize biohydrogen commercial uptake on a large-scale. Recently,
   machine learning algorithms have demonstrated the ability to handle
   large amounts of data while requiring less in-depth knowledge of the
   system and being capable of adapting to evolving circumstances. This
   review critically reviews the role of machine learning in categorizing
   and predicting data related to biohydrogen production. The accuracy and
   potential of different machine learning algorithms are reported. Also,
   the practical implications of machine learning models to realize
   biohydrogen uptake by the transportation sector are discussed. The
   review indicates that machine learning algorithms can successfully model
   non-linear and complex interactions between operational and performance
   parameters in biohydrogen production. Additionally, machine learning
   algorithms can help researchers identify the most efficient methods for
   producing biohydrogen, leading to a more sustainable and cost-effective
   energy source. (c) 2023 BRTeam. All rights reserved.},
DOI = {10.18331/BRJ2023.10.2.4},
ISSN = {2292-8782},
ResearcherID-Numbers = {Song, Hua/D-7507-2015
   Ledesma-Amaro, Rodrigo/A-7545-2017
   LICHTFOUSE, ERIC/HZL-9873-2023
   Sheremet, Mikhail/LMO-5710-2024
   alagumalai, avinash/C-3780-2015
   Mahian, Omid/AAA-4550-2019
   Wongwises, Somchai/F-6086-2010
   Balaji, Devarajan/AGF-4909-2022},
ORCID-Numbers = {Song, Hua/0000-0002-2791-1723
   Lichtfouse, Eric/0000-0002-8535-8073
   Wongwises, Somchai/0000-0003-2648-6814
   },
Unique-ID = {WOS:001008445700001},
}

@article{ WOS:000695380500001,
Author = {Greener, Joe G. and Kandathil, Shaun M. and Moffat, Lewis and Jones,
   David T.},
Title = {A guide to machine learning for biologists},
Journal = {NATURE REVIEWS MOLECULAR CELL BIOLOGY},
Year = {2022},
Volume = {23},
Number = {1},
Pages = {40-55},
Month = {JAN},
Abstract = {Machine learning is becoming a widely used tool for the analysis of
   biological data. However, for experimentalists, proper use of machine
   learning methods can be challenging. This Review provides an overview of
   machine learning techniques and provides guidance on their applications
   in biology.
   The expanding scale and inherent complexity of biological data have
   encouraged a growing use of machine learning in biology to build
   informative and predictive models of the underlying biological
   processes. All machine learning techniques fit models to data; however,
   the specific methods are quite varied and can at first glance seem
   bewildering. In this Review, we aim to provide readers with a gentle
   introduction to a few key machine learning techniques, including the
   most recently developed and widely used techniques involving deep neural
   networks. We describe how different techniques may be suited to specific
   types of biological data, and also discuss some best practices and
   points to consider when one is embarking on experiments involving
   machine learning. Some emerging directions in machine learning
   methodology are also discussed.},
DOI = {10.1038/s41580-021-00407-0},
EarlyAccessDate = {SEP 2021},
ISSN = {1471-0072},
EISSN = {1471-0080},
ResearcherID-Numbers = {Kandathil, Shaun/IUO-7841-2023
   },
ORCID-Numbers = {Greener, Joe/0000-0002-5154-1929
   Kandathil, Shaun/0000-0002-2671-2140},
Unique-ID = {WOS:000695380500001},
}

@article{ WOS:001117835000001,
Author = {Guo, Wenjing and Liu, Jie and Dong, Fan and Song, Meng and Li, Zoe and
   Khan, Md Kamrul Hasan and Patterson, Tucker A. and Hong, Huixiao},
Title = {Review of machine learning and deep learning models for toxicity
   prediction},
Journal = {EXPERIMENTAL BIOLOGY AND MEDICINE},
Year = {2023},
Volume = {248},
Number = {21},
Pages = {1952-1973},
Month = {NOV},
Abstract = {The ever-increasing number of chemicals has raised public concerns due
   to their adverse effects on human health and the environment. To protect
   public health and the environment, it is critical to assess the toxicity
   of these chemicals. Traditional in vitro and in vivo toxicity assays are
   complicated, costly, and time-consuming and may face ethical issues.
   These constraints raise the need for alternative methods for assessing
   the toxicity of chemicals. Recently, due to the advancement of machine
   learning algorithms and the increase in computational power, many
   toxicity prediction models have been developed using various machine
   learning and deep learning algorithms such as support vector machine,
   random forest, k-nearest neighbors, ensemble learning, and deep neural
   network. This review summarizes the machine learning- and deep
   learning-based toxicity prediction models developed in recent years.
   Support vector machine and random forest are the most popular machine
   learning algorithms, and hepatotoxicity, cardiotoxicity, and
   carcinogenicity are the frequently modeled toxicity endpoints in
   predictive toxicology. It is known that datasets impact model
   performance. The quality of datasets used in the development of toxicity
   prediction models using machine learning and deep learning is vital to
   the performance of the developed models. The different toxicity
   assignments for the same chemicals among different datasets of the same
   type of toxicity have been observed, indicating benchmarking datasets is
   needed for developing reliable toxicity prediction models using machine
   learning and deep learning algorithms. This review provides insights
   into current machine learning models in predictive toxicology, which are
   expected to promote the development and application of toxicity
   prediction models in the future.},
DOI = {10.1177/15353702231209421},
EarlyAccessDate = {DEC 2023},
ISSN = {1535-3702},
EISSN = {1535-3699},
ResearcherID-Numbers = {guo, wenjing/AAP-6943-2021},
ORCID-Numbers = {Khan, Md Kamrul Hasan/0009-0004-9835-5594
   },
Unique-ID = {WOS:001117835000001},
}

@article{ WOS:000922087600001,
Author = {Wei, Lin and Liu, Haowen and Xu, Jing and Shi, Lei and Shan, Zheng and
   Zhao, Bo and Gao, Yufei},
Title = {Quantum machine learning in medical image analysis: A survey},
Journal = {NEUROCOMPUTING},
Year = {2023},
Volume = {525},
Pages = {42-53},
Month = {MAR 7},
Abstract = {With the outstanding superposition and entanglement properties of
   quantum computing, quantum machine learning has attracted widespread
   attention in many fields, such as medical image analysis, password
   cracking, and pattern recognition. Although classical machine learning
   is widely used and has shown great potential in medical image analysis,
   the bottlenecks of insufficient labeled data and low processing
   efficiency still exist. To overcome these challenges, massive studies
   combined quantum computing with machine learning to explore more
   advanced algorithms, which have achieved distin-guished improvements in
   parameter optimization, execution efficiency, and the reduction of error
   rates. Quantum machine learning provides new insights for the
   intersectional research of quantum technology and medical image analysis
   and contributes to the future development of medical image analysis.
   This review delivers an overview of the definition and taxonomy of
   quantum machine learning, as well as summarizes various quantum machine
   learning methods and their applications in medical image analy-sis over
   the past decade.(c) 2023 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.neucom.2023.01.049},
EarlyAccessDate = {JAN 2023},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Gao, Yufei/AAX-3459-2021
   Shi, Lei/ABW-6050-2022
   },
ORCID-Numbers = {Shi, Lei/0000-0002-1170-3911},
Unique-ID = {WOS:000922087600001},
}

@article{ WOS:000977859400001,
Author = {Ren, Zeyu and Wang, Shuihua and Zhang, Yudong},
Title = {Weakly supervised machine learning},
Journal = {CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY},
Year = {2023},
Volume = {8},
Number = {3},
Pages = {549-580},
Month = {SEP},
Abstract = {Supervised learning aims to build a function or model that seeks as many
   mappings as possible between the training data and outputs, where each
   training data will predict as a label to match its corresponding
   ground-truth value. Although supervised learning has achieved great
   success in many tasks, sufficient data supervision for labels is not
   accessible in many domains because accurate data labelling is costly and
   laborious, particularly in medical image analysis. The cost of the
   dataset with ground-truth labels is much higher than in other domains.
   Therefore, it is noteworthy to focus on weakly supervised learning for
   medical image analysis, as it is more applicable for practical
   applications. In this review, the authors give an overview of the latest
   process of weakly supervised learning in medical image analysis,
   including incomplete, inexact, and inaccurate supervision, and introduce
   the related works on different applications for medical image analysis.
   Related concepts are illustrated to help readers get an overview ranging
   from supervised to unsupervised learning within the scope of machine
   learning. Furthermore, the challenges and future works of weakly
   supervised learning in medical image analysis are discussed.},
DOI = {10.1049/cit2.12216},
EarlyAccessDate = {APR 2023},
ISSN = {2468-6557},
EISSN = {2468-2322},
ResearcherID-Numbers = {Ren, Zeyu/GYU-3257-2022
   Zhang, Yudong/I-7633-2013
   Wang, Shuihua/G-7326-2016
   },
ORCID-Numbers = {Ren, Zeyu/0000-0003-2303-5663},
Unique-ID = {WOS:000977859400001},
}

@article{ WOS:001162814300001,
Author = {Giudici, Paolo and Raffinetti, Emanuela and Riani, Marco},
Title = {Robust machine learning models: linear and nonlinear},
Journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS},
Year = {2025},
Volume = {20},
Number = {2},
Pages = {1043-1050},
Month = {AUG},
Abstract = {Artificial Intelligence relies on the application of machine learning
   models which, while reaching high predictive accuracy, lack
   explainability and robustness. This is a problem in regulated
   industries, as authorities aimed at monitoring the risks arising from
   the application of Artificial Intelligence methods may not validate
   them. No measurement methodologies are yet available to jointly assess
   accuracy, explainability and robustness of machine learning models. We
   propose a methodology which fills the gap, extending the Forward Search
   approach, employed in robust statistical learning, to machine learning
   models. Doing so, we will be able to evaluate, by means of interpretable
   statistical tests, whether a specific Artificial Intelligence
   application is accurate, explainable and robust, through a unified
   methodology. We apply our proposal to the context of Bitcoin price
   prediction, comparing a linear regression model against a nonlinear
   neural network model.},
DOI = {10.1007/s41060-024-00512-1},
EarlyAccessDate = {FEB 2024},
ISSN = {2364-415X},
EISSN = {2364-4168},
ResearcherID-Numbers = {Raffinetti, Emanuela/MCJ-7704-2025
   Giudici, Paolo/AAA-5126-2021
   Riani, Marco/V-2398-2018},
Unique-ID = {WOS:001162814300001},
}

@article{ WOS:001191466700001,
Author = {Kim, Song-Kyoo (Amang)},
Title = {Compact Data Learning for Machine Learning Classifications},
Journal = {AXIOMS},
Year = {2024},
Volume = {13},
Number = {3},
Month = {MAR},
Abstract = {This paper targets the area of optimizing machine learning (ML) training
   data by constructing compact data. The methods of optimizing ML training
   have improved and become a part of artificial intelligence (AI) system
   development. Compact data learning (CDL) is an alternative practical
   framework to optimize a classification system by reducing the size of
   the training dataset. CDL originated from compact data design, which
   provides the best assets without handling complex big data. CDL is a
   dedicated framework for improving the speed of the machine learning
   training phase without affecting the accuracy of the system. The
   performance of an ML-based arrhythmia detection system and its variants
   with CDL maintained the same statistical accuracy. ML training with CDL
   could be maximized by applying an 85\% reduced input dataset, which
   indicated that a trained ML system could have the same statistical
   accuracy by only using 15\% of the original training dataset.},
DOI = {10.3390/axioms13030137},
Article-Number = {137},
EISSN = {2075-1680},
ResearcherID-Numbers = {Kim, Song-Kyoo/IAM-8853-2023},
Unique-ID = {WOS:001191466700001},
}

@article{ WOS:001074642900001,
Author = {Freiesleben, Timo and Grote, Thomas},
Title = {Beyond generalization: a theory of robustness in machine learning},
Journal = {SYNTHESE},
Year = {2023},
Volume = {202},
Number = {4},
Month = {SEP 27},
Abstract = {The term robustness is ubiquitous in modern Machine Learning (ML).
   However, its meaning varies depending on context and community.
   Researchers either focus on narrow technical definitions, such as
   adversarial robustness, natural distribution shifts, and performativity,
   or they simply leave open what exactly they mean by robustness. In this
   paper, we provide a conceptual analysis of the term robustness, with the
   aim to develop a common language, that allows us to weave together
   different strands of robustness research. We define robustness as the
   relative stability of a robustness target with respect to specific
   interventions on a modifier. Our account captures the various sub-types
   of robustness that are discussed in the research literature, including
   robustness to distribution shifts, prediction robustness, or the
   robustness of algorithmic explanations. Finally, we delineate robustness
   from adjacent key concepts in ML, such as extrapolation, generalization,
   and uncertainty, and establish it as an independent epistemic concept.},
DOI = {10.1007/s11229-023-04334-9},
Article-Number = {109},
ISSN = {0039-7857},
EISSN = {1573-0964},
ResearcherID-Numbers = {Freiesleben, Timo/OOM-6045-2025},
Unique-ID = {WOS:001074642900001},
}

@article{ WOS:000875839900013,
Author = {Abdulsalam, Ghada and Meshoul, Souham and Shaiba, Hadil},
Title = {Explainable Heart Disease Prediction Using Ensemble-Quantum Machine
   Learning Approach},
Journal = {INTELLIGENT AUTOMATION AND SOFT COMPUTING},
Year = {2023},
Volume = {36},
Number = {1},
Pages = {761-779},
Abstract = {Nowadays, quantum machine learning is attracting great interest in a
   wide range of fields due to its potential superior performance and
   capabilities. The massive increase in computational capacity and speed
   of quantum computers can lead to a quantum leap in the healthcare field.
   Heart disease seriously threa-tens human health since it is the leading
   cause of death worldwide. Quantum machine learning methods can propose
   effective solutions to predict heart disease and aid in early diagnosis.
   In this study, an ensemble machine learning model based on quantum
   machine learning classifiers is proposed to predict the risk of heart
   disease. The proposed model is a bagging ensemble learning model where a
   quantum support vector classifier was used as a base classifier.
   Further-more, in order to make the model's outcomes more explainable,
   the importance of every single feature in the prediction is computed and
   visualized using SHapley Additive exPlanations (SHAP) framework. In the
   experimental study, other stand-alone quantum classifiers, namely,
   Quantum Support Vector Classifier (QSVC), Quantum Neural Network (QNN),
   and Variational Quantum Classifier (VQC) are applied and compared with
   classical machine learning classifiers such as Sup-port Vector Machine
   (SVM), and Artificial Neural Network (ANN). The experi-mental results on
   the Cleveland dataset reveal the superiority of QSVC compared to the
   others, which explains its use in the proposed bagging model. The
   Bagging-QSVC model outperforms all aforementioned classifiers with an
   accuracy of 90.16\% while showing great competitiveness compared to some
   state-of-the-art models using the same dataset. The results of the study
   indicate that quantum machine learning classifiers perform better than
   classical machine learning classi-fiers in predicting heart disease. In
   addition, the study reveals that the bagging ensemble learning technique
   is effective in improving the prediction accuracy of quantum
   classifiers.},
DOI = {10.32604/iasc.2023.032262},
ISSN = {1079-8587},
EISSN = {2326-005X},
ResearcherID-Numbers = {Meshoul, Souham/GQZ-9027-2022
   Shaiba, Hadil/HKN-8342-2023},
ORCID-Numbers = {Abdulsalam, Ghada/0000-0003-2774-8898
   Shaiba, Hadil/0000-0003-1652-6579},
Unique-ID = {WOS:000875839900013},
}

@article{ WOS:000579808700028,
Author = {Yang, Li and Shami, Abdallah},
Title = {On hyperparameter optimization of machine learning algorithms: Theory
   and practice},
Journal = {NEUROCOMPUTING},
Year = {2020},
Volume = {415},
Pages = {295-316},
Month = {NOV 20},
Abstract = {Machine learning algorithms have been used widely in various
   applications and areas. To fit a machine learning model into different
   problems, its hyper-parameters must be tuned. Selecting the best
   hyperparameter configuration for machine learning models has a direct
   impact on the model's performance. It often requires deep knowledge of
   machine learning algorithms and appropriate hyper-parameter optimization
   techniques. Although several automatic optimization techniques exist,
   they have different strengths and drawbacks when applied to different
   types of problems. In this paper, optimizing the hyper-parameters of
   common machine learning models is studied. We introduce several
   state-of-theart optimization techniques and discuss how to apply them to
   machine learning algorithms. Many available libraries and frameworks
   developed for hyper-parameter optimization problems are provided, and
   some open challenges of hyper-parameter optimization research are also
   discussed in this paper. Moreover, experiments are conducted on
   benchmark datasets to compare the performance of different optimization
   methods and provide practical examples of hyper-parameter optimization.
   This survey paper will help industrial users, data analysts, and
   researchers to better develop machine learning models by identifying the
   proper hyper-parameter configurations effectively. (C) 2020 Elsevier
   B.V. All rights reserved.},
DOI = {10.1016/j.neucom.2020.07.061},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Shami, Abdallah/HTT-3720-2023
   Yang, Li/AAL-4637-2021},
ORCID-Numbers = {Shami, Abdallah/0000-0003-2887-0350
   Yang, Li/0000-0001-9383-1097
   },
Unique-ID = {WOS:000579808700028},
}

@article{ WOS:000517855800037,
Author = {Lei, Yaguo and Yang, Bin and Jiang, Xinwei and Jia, Feng and Li, Naipeng
   and Nandi, Asoke K.},
Title = {Applications of machine learning to machine fault diagnosis: A review
   and roadmap},
Journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING},
Year = {2020},
Volume = {138},
Month = {APR},
Abstract = {Intelligent fault diagnosis (IFD) refers to applications of machine
   learning theories to machine fault diagnosis. This is a promising way to
   release the contribution from human labor and automatically recognize
   the health states of machines, thus it has attracted much attention in
   the last two or three decades. Although IFD has achieved a considerable
   number of successes, a review still leaves a blank space to
   systematically cover the development of IFD from the cradle to the
   bloom, and rarely provides potential guidelines for the future
   development. To bridge the gap, this article presents a review and
   roadmap to systematically cover the development of IFD following the
   progress of machine learning theories and offer a future perspective. In
   the past, traditional machine learning theories began to weak the
   contribution of human labor and brought the era of artificial
   intelligence to machine fault diagnosis. Over the recent years, the
   advent of deep learning theories has reformed IFD in further releasing
   the artificial assistance since the 2010s, which encourages to construct
   an end-to-end diagnosis procedure. It means to directly bridge the
   relationship between the increasingly-grown monitoring data and the
   health states of machines. In the future, transfer learning theories
   attempt to use the diagnosis knowledge from one or multiple diagnosis
   tasks to other related ones, which prospectively overcomes the obstacles
   in applications of IFD to engineering scenarios. Finally, the roadmap of
   IFD is pictured to show potential research trends when combined with the
   challenges in this field. (C) 2019 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.ymssp.2019.106587},
Article-Number = {106587},
ISSN = {0888-3270},
EISSN = {1096-1216},
ResearcherID-Numbers = {Nandi, Asoke/C-4572-2011
   Lei, Yaguo/N-4891-2014
   Yang, Bin/JGC-7746-2023
   Jia, Fengfeng/JVM-7977-2024
   Li, Naipeng/AFF-2789-2022
   },
ORCID-Numbers = {Yang, Bin/0000-0002-3015-3580
   Li, Naipeng/0000-0003-0678-8161},
Unique-ID = {WOS:000517855800037},
}

@article{ WOS:001015789300001,
Author = {Pandey, Vinay Kumar and Srivastava, Shivangi and Dash, Kshirod Kumar and
   Singh, Rahul and Mukarram, Shaikh Ayaz and Kovacs, Bela and Harsanyi,
   Endre},
Title = {Machine Learning Algorithms and Fundamentals as Emerging Safety Tools in
   Preservation of Fruits and Vegetables: A Review},
Journal = {PROCESSES},
Year = {2023},
Volume = {11},
Number = {6},
Month = {JUN},
Abstract = {Machine learning assists with food process optimization techniques by
   developing a model to predict the optimal solution for given input data.
   Machine learning includes unsupervised and supervised learning, data
   pre-processing, feature engineering, model selection, assessment, and
   optimization methods. Various problems with food processing optimization
   could be resolved using these techniques. Machine learning is
   increasingly being used in the food industry to improve production
   efficiency, reduce waste, and create personalized customer experiences.
   Machine learning may be used to improve ingredient utilization and save
   costs, automate operations such as packing and labeling, and even
   forecast consumer preferences to develop personalized products. Machine
   learning is also being used to identify food safety hazards before they
   reach the consumer, such as contaminants or spoiled food. The usage of
   machine learning in the food sector is predicted to rise in the near
   future as more businesses understand the potential of this technology to
   enhance customer experience and boost productivity. Machine learning may
   be utilized to enhance nano-technological operations and fruit and
   vegetable preservation. Machine learning algorithms may find trends
   regarding various factors that impact the quality of the product being
   preserved by examining data from prior tests. Furthermore, machine
   learning may be utilized to determine optimal parameter combinations
   that result in maximal produce preservation. The review discusses the
   relevance of machine learning in ready-to-eat foods and its use as a
   safety tool for preservation were highlighted. The application of
   machine learning in agriculture, food packaging, food processing, and
   food safety is reviewed. The working principle and methodology, as well
   as the principles of machine learning, were discussed.},
DOI = {10.3390/pr11061720},
Article-Number = {1720},
EISSN = {2227-9717},
ResearcherID-Numbers = {Kumar Pandey, Dr. Vinay/ISU-9986-2023
   Pandey, Vinay/ISU-9986-2023
   Dash, Kshirod/AEQ-2949-2022
   Kovács, Béla/ABH-3324-2022
   Srivastava, Shivangi/KYP-5548-2024
   Shaikh, Ayaz Mukarram/ITU-8330-2023},
ORCID-Numbers = {Kumar Pandey, Dr. Vinay/0000-0001-9818-5805
   Dash, Kshirod Kumar/0000-0002-3875-1286
   Kovacs, Bela/0000-0002-6439-4753
   Shaikh, Ayaz Mukarram/0000-0002-0410-5286},
Unique-ID = {WOS:001015789300001},
}

@article{ WOS:000952240100002,
Author = {Latif, Sarmad Dashti and Ahmed, Ali Najah},
Title = {A review of deep learning and machine learning techniques for
   hydrological inflow forecasting},
Journal = {ENVIRONMENT DEVELOPMENT AND SUSTAINABILITY},
Year = {2023},
Volume = {25},
Number = {11},
Pages = {12189-12216},
Month = {NOV},
Abstract = {Conventional machine learning models have been widely used for reservoir
   inflow and rainfall prediction. Nowadays, researchers focus on a new
   computing architecture in the area of AI, namely, deep learning for
   hydrological forecasting parameters. This review paper tends to
   broadcast more of the intriguing interest in reservoir inflow prediction
   utilizing deep learning and machine learning algorithms. The AI models
   utilized for different hydrology sectors, as well as the most prevalent
   machine learning techniques, will be explored in this thorough study,
   which divides AI techniques into two primary categories: deep learning
   and machine learning. In this study, we look at the long short-term
   memory deep learning method as well as three traditional machine
   learning algorithms: support vector machine, random forest, and boosted
   regression tree. Under each part, a summary of the findings is provided.
   For convenience of reference, some of the benefits and drawbacks
   discovered through literature reviews have been listed. Finally, future
   recommendations and overall conclusions based on research findings are
   given. This review focuses on papers from high-impact factor periodicals
   published over a 4 years period beginning in 2018 onwards.},
DOI = {10.1007/s10668-023-03131-1},
EarlyAccessDate = {MAR 2023},
ISSN = {1387-585X},
EISSN = {1573-2975},
ResearcherID-Numbers = {Latif, Sarmad/ABD-4755-2020
   AHMED, Ali/J-9456-2017},
ORCID-Numbers = {Latif, Sarmad Dashti/0000-0002-0417-3545
   },
Unique-ID = {WOS:000952240100002},
}

@article{ WOS:000917312100001,
Author = {Joshi, Payal B.},
Title = {Navigating with chemometrics and machine learning in chemistry},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2023},
Volume = {56},
Number = {9},
Pages = {9089-9114},
Month = {SEP},
Abstract = {Chemometrics and machine learning are artificial intelligence-based
   methods stirring a transformative change in chemistry. Organic
   synthesis, drug discovery and analytical techniques are incorporating
   machine learning techniques at an accelerated pace. However,
   machine-assisted chemistry faces challenges while solving critical
   problems in chemistry due to complex relationships in data sets. Even
   with increasing publishing volumes on machine learning, its application
   in areas of chemistry is not a straightforward endeavour. A particular
   concern in applying machine learning in chemistry is data availability
   and reproducibility. The present review article discusses the various
   chemometric methods, expert systems, and machine learning techniques
   developed for solving problems of organic synthesis and drug discovery
   with selected examples. Further, a concise discussion on chemometrics
   and ML deployed in analytical techniques such as, spectroscopy,
   microscopy and chromatography are presented. Finally, the review
   reflects the challenges, opportunities and future perspectives on
   machine learning and automation in chemistry. The review concludes by
   pondering on some tough questions on applying machine learning and their
   possibility of navigation in the different terrains of chemistry.},
DOI = {10.1007/s10462-023-10391-w},
EarlyAccessDate = {JAN 2023},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Joshi, Payal/F-8102-2015
   },
ORCID-Numbers = {Joshi, Payal/0000-0003-4132-5576},
Unique-ID = {WOS:000917312100001},
}

@article{ WOS:000978338900001,
Author = {Zhao, Yuhong and Liu, Ruirui and Liu, Zhansheng and Liu, Liang and Wang,
   Jingjing and Liu, Wenxiang},
Title = {A Review of Macroscopic Carbon Emission Prediction Model Based on
   Machine Learning},
Journal = {SUSTAINABILITY},
Year = {2023},
Volume = {15},
Number = {8},
Month = {APR},
Abstract = {Under the background of global warming and the energy crisis, the
   Chinese government has set the goal of carbon peaking and carbon
   neutralization. With the rapid development of machine learning, some
   advanced machine learning algorithms have also been applied to the
   control and prediction of carbon emissions due to their high efficiency
   and accuracy. In this paper, the current situation of machine learning
   applied to carbon emission prediction is studied in detail by means of
   paper retrieval. It was found that machine learning has become a hot
   topic in the field of carbon emission prediction models, and the main
   carbon emission prediction models are mainly based on back propagation
   neural networks, support vector machines, long short-term memory neural
   networks, random forests and extreme learning machines. By describing
   the characteristics of these five types of carbon emission prediction
   models and conducting a comparative analysis, we determined the
   applicable characteristics of each model, and based on this, future
   research ideas for carbon emission prediction models based on machine
   learning are proposed.},
DOI = {10.3390/su15086876},
Article-Number = {6876},
EISSN = {2071-1050},
ResearcherID-Numbers = {wen, jing/HDM-2205-2022
   },
ORCID-Numbers = {Liu, Zhansheng/0000-0002-4251-5702},
Unique-ID = {WOS:000978338900001},
}

@article{ WOS:001089938100001,
Author = {Khoei, Tala Talaei and Kaabouch, Naima},
Title = {Machine Learning: Models, Challenges, and Research Directions},
Journal = {FUTURE INTERNET},
Year = {2023},
Volume = {15},
Number = {10},
Month = {OCT},
Abstract = {Machine learning techniques have emerged as a transformative force,
   revolutionizing various application domains, particularly cybersecurity.
   The development of optimal machine learning applications requires the
   integration of multiple processes, such as data pre-processing, model
   selection, and parameter optimization. While existing surveys have shed
   light on these techniques, they have mainly focused on specific
   application domains. A notable gap that exists in current studies is the
   lack of a comprehensive overview of machine learning architecture and
   its essential phases in the cybersecurity field. To address this gap,
   this survey provides a holistic review of current studies in machine
   learning, covering techniques applicable to any domain. Models are
   classified into four categories: supervised, semi-supervised,
   unsupervised, and reinforcement learning. Each of these categories and
   their models are described. In addition, the survey discusses the
   current progress related to data pre-processing and hyperparameter
   tuning techniques. Moreover, this survey identifies and reviews the
   research gaps and key challenges that the cybersecurity field faces. By
   analyzing these gaps, we propose some promising research directions for
   the future. Ultimately, this survey aims to serve as a valuable resource
   for researchers interested in learning about machine learning, providing
   them with insights to foster innovation and progress across diverse
   application domains.},
DOI = {10.3390/fi15100332},
Article-Number = {332},
ISSN = {1999-5903},
ResearcherID-Numbers = {TalaeiKhoei, Tala/KLY-8206-2024
   },
ORCID-Numbers = {, Tala/0000-0002-7630-9034},
Unique-ID = {WOS:001089938100001},
}

@article{ WOS:001136730400009,
Author = {Gao, Lei and Guan, Ling},
Title = {Interpretability of Machine Learning: Recent Advances and Future
   Prospects},
Journal = {IEEE MULTIMEDIA},
Year = {2023},
Volume = {30},
Number = {4},
Pages = {105-118},
Month = {OCT},
Abstract = {The proliferation of machine learning (ML) has drawn unprecedented
   interest in the study of various multimedia contents such as text,
   image, audio, and video, among others. Consequently, understanding and
   learning ML-based representations have taken center stage in knowledge
   discovery in intelligent multimedia research and applications.
   Nevertheless, the black-box nature of contemporary ML, especially in
   deep neural networks, has posed a primary challenge for ML-based
   representation learning. To address this black-box problem, studies on
   the interpretability of ML have attracted tremendous interest in recent
   years. This article presents a survey on recent advances in and future
   prospects for the interpretability of ML, with several application
   examples pertinent to multimedia computing, including text-image
   cross-modal representation learning, face recognition, and the
   recognition of objects. It is evidently shown that the study of the
   interpretability of ML promises an important research direction, one
   that is worth further investment in.},
DOI = {10.1109/MMUL.2023.3272513},
ISSN = {1070-986X},
EISSN = {1941-0166},
ORCID-Numbers = {Gao, Lei/0000-0001-5583-713X},
Unique-ID = {WOS:001136730400009},
}

@article{ WOS:001043801800001,
Author = {Topuz, Beyza and Alp, Nese Cakici},
Title = {Machine learning in architecture},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2023},
Volume = {154},
Month = {OCT},
Abstract = {This paper explores the utilisation of machine learning in architecture,
   focusing on the addressed problems and commonly employed programming
   languages, software, platforms, libraries, and algorithms. Eight major
   academic search and publishing platforms were systematically reviewed,
   covering the period from 2007 to 2022, resulting in the selection of 60
   relevant articles from a pool of 175. The articles were categorised
   based on their thematic focus, primarily centring around Computer-Aided
   Design (CAD), Computer-Aided Engineering (CAE), and Computer-Aided
   Manufacturing (CAM). By evaluating the current state of machine learning
   in architecture, this study provides valuable insights into its usage
   and identifies potential areas for future research. This paper
   contributes to a comprehensive understanding of the evolving landscape
   of machine learning in the field by investigating subfields within
   architecture and the specific tools used to tackle architectural
   challenges.},
DOI = {10.1016/j.autcon.2023.105012},
EarlyAccessDate = {JUL 2023},
Article-Number = {105012},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Çakıcı Alp, Neşe/HRA-2178-2023},
ORCID-Numbers = {CAKICI ALP, NESE/0000-0002-7626-9212
   },
Unique-ID = {WOS:001043801800001},
}

@article{ WOS:001061075500001,
Author = {Kang, Edward B.},
Title = {Ground truth tracings (GTT): On the epistemic limits of machine learning},
Journal = {BIG DATA \& SOCIETY},
Year = {2023},
Volume = {10},
Number = {1},
Month = {JAN},
Abstract = {There is a gap in existing critical scholarship that engages with the
   ways in which current ``machine listening{''} or voice
   analytics/biometric systems intersect with the technical specificities
   of machine learning. This article examines the sociotechnical assemblage
   of machine learning techniques, practices, and cultures that underlie
   these technologies. After engaging with various practitioners working in
   companies that develop machine listening systems, ranging from CEOs,
   machine learning engineers, data scientists, and business analysts,
   among others, I bring attention to the centrality of ``learnability{''}
   as a malleable conceptual framework that bends according to various
   ``ground-truthing{''} practices in formalizing certain listening-based
   prediction tasks for machine learning. In response, I introduce a
   process I call Ground Truth Tracings to examine the various ontological
   translations that occur in training a machine to ``learn to listen.{''}
   Ultimately, by further examining this notion of learnability through the
   aperture of power, I take insights acquired through my fieldwork in the
   machine listening industry and propose a strategically reductive
   heuristic through which the epistemological and ethical soundness of
   machine learning, writ large, can be contemplated.},
DOI = {10.1177/20539517221146122},
Article-Number = {20539517221146122},
ISSN = {2053-9517},
ResearcherID-Numbers = {Kang, Edward (Byungkwon)/AAZ-3291-2021},
ORCID-Numbers = {Kang, Byungkwon/0000-0003-4919-1832
   },
Unique-ID = {WOS:001061075500001},
}

@article{ WOS:001026855700002,
Author = {Xiao, Tailong and Zhai, Xinliang and Wu, Xiaoyan and Fan, Jianping and
   Zeng, Guihua},
Title = {Practical advantage of quantum machine learning in ghost imaging},
Journal = {COMMUNICATIONS PHYSICS},
Year = {2023},
Volume = {6},
Number = {1},
Month = {JUL 10},
Abstract = {Quantum computation can provide practical applications where quantum
   algorithms can outperform classical ones. The authors focus on
   experimental ghost imaging using a mixture of classical and quantum
   machine learning (simulated on a classical computer) to process the
   experimental data and reconstruct/classify the image, finding that
   quantum machine learning techniques allow for a better reconstruction
   compared to standard methods.
   Demonstrating the practical advantage of quantum computation remains a
   long-standing challenge whereas quantum machine learning becomes a
   promising application that can be resorted to. In this work, we
   investigate the practical advantage of quantum machine learning in ghost
   imaging by overcoming the limitations of classical methods in blind
   object identification and imaging. We propose two hybrid
   quantum-classical machine learning algorithms and a physical-inspired
   patch strategy to allow distributed quantum learning with parallel
   variational circuits. In light of the algorithm, we conduct experiments
   for imaging-free object identification and blind ghost imaging under
   different physical sampling rates. We further quantitatively analyze the
   advantage through the lens of information geometry and generalization
   capability. The numerical results showcase that quantum machine learning
   can restore high-quality images but classical machine learning fails.
   The advantage of identification rate are up to 10\% via fair comparison
   with the classical machine learning methods. Our work explores a
   physics-related application capable of practical quantum advantage,
   which highlights the prospect of quantum computation in the machine
   learning field.},
DOI = {10.1038/s42005-023-01290-1},
Article-Number = {171},
ISSN = {2399-3650},
ResearcherID-Numbers = {Zeng, Guihua/LZH-9953-2025
   Xiao, Tailong/AEF-5510-2022},
Unique-ID = {WOS:001026855700002},
}

@article{ WOS:000626358900001,
Author = {Huellermeier, Eyke and Waegeman, Willem},
Title = {Aleatoric and epistemic uncertainty in machine learning: an introduction
   to concepts and methods},
Journal = {MACHINE LEARNING},
Year = {2021},
Volume = {110},
Number = {3},
Pages = {457-506},
Month = {MAR},
Abstract = {The notion of uncertainty is of major importance in machine learning and
   constitutes a key element of machine learning methodology. In line with
   the statistical tradition, uncertainty has long been perceived as almost
   synonymous with standard probability and probabilistic predictions. Yet,
   due to the steadily increasing relevance of machine learning for
   practical applications and related issues such as safety requirements,
   new problems and challenges have recently been identified by machine
   learning scholars, and these problems may call for new methodological
   developments. In particular, this includes the importance of
   distinguishing between (at least) two different types of uncertainty,
   often referred to as aleatoric and epistemic. In this paper, we provide
   an introduction to the topic of uncertainty in machine learning as well
   as an overview of attempts so far at handling uncertainty in general and
   formalizing this distinction in particular.},
DOI = {10.1007/s10994-021-05946-3},
EarlyAccessDate = {MAR 2021},
ISSN = {0885-6125},
EISSN = {1573-0565},
Unique-ID = {WOS:000626358900001},
}

@article{ WOS:000842577300001,
Author = {Mosqueira-Rey, Eduardo and Hernandez-Pereira, Elena and Alonso-Rios,
   David and Bobes-Bascaran, Jose and Fernandez-Leal, Angel},
Title = {Human-in-the-loop machine learning: a state of the art},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2023},
Volume = {56},
Number = {4},
Pages = {3005-3054},
Month = {APR},
Abstract = {Researchers are defining new types of interactions between humans and
   machine learning algorithms generically called human-in-the-loop machine
   learning. Depending on who is in control of the learning process, we can
   identify: active learning, in which the system remains in control;
   interactive machine learning, in which there is a closer interaction
   between users and learning systems; and machine teaching, where human
   domain experts have control over the learning process. Aside from
   control, humans can also be involved in the learning process in other
   ways. In curriculum learning human domain experts try to impose some
   structure on the examples presented to improve the learning; in
   explainable AI the focus is on the ability of the model to explain to
   humans why a given solution was chosen. This collaboration between AI
   models and humans should not be limited only to the learning process; if
   we go further, we can see other terms that arise such as Usable and
   Useful AI. In this paper we review the state of the art of the
   techniques involved in the new forms of relationship between humans and
   ML algorithms. Our contribution is not merely listing the different
   approaches, but to provide definitions clarifying confusing, varied and
   sometimes contradictory terms; to elucidate and determine the boundaries
   between the different methods; and to correlate all the techniques
   searching for the connections and influences between them.},
DOI = {10.1007/s10462-022-10246-w},
EarlyAccessDate = {AUG 2022},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Bobes-Bascaran, Jose/AAM-4591-2021
   Rey, Eduardo/B-1959-2013
   Alonso Ríos, David/H-9793-2014
   Mosqueira Rey, Eduardo/B-1959-2013
   Alonso Rios, David/H-9793-2014
   Bobes-Bascarán, José/AAM-4591-2021
   Hernandez-Pereira, Elena/A-7682-2015
   Pereira, Elena/A-7682-2015
   Leal, Ángel/AAB-6095-2019},
ORCID-Numbers = {Bobes-Bascaran, Jose/0000-0002-7317-3039
   Mosqueira Rey, Eduardo/0000-0002-4894-1067
   Alonso Rios, David/0000-0003-2147-3010
   Hernandez-Pereira, Elena/0000-0001-8666-4075
   },
Unique-ID = {WOS:000842577300001},
}

@article{ WOS:000742179000001,
Author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
Title = {Machine Learning Testing: Survey, Landscapes and Horizons},
Journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
Year = {2022},
Volume = {48},
Number = {1},
Pages = {1-36},
Month = {JAN 1},
Abstract = {This paper provides a comprehensive survey of techniques for testing
   machine learning systems; Machine Learning Testing (ML testing)
   research. It covers 144 papers on testing properties (e.g., correctness,
   robustness, and fairness), testing components (e.g., the data, learning
   program, and framework), testing workflow (e.g., test generation and
   test evaluation), and application scenarios (e.g., autonomous driving,
   machine translation). The paper also analyses trends concerning
   datasets, research trends, and research focus, concluding with research
   challenges and promising research directions in ML testing.},
DOI = {10.1109/TSE.2019.2962027},
ISSN = {0098-5589},
EISSN = {1939-3520},
ResearcherID-Numbers = {Liu, Yang/D-2306-2013
   Zhang, Lejie/ACD-9278-2022},
ORCID-Numbers = {Zhang, Jie/0000-0003-0481-7264
   Liu, Yang/0000-0001-7300-9215
   },
Unique-ID = {WOS:000742179000001},
}

@article{ WOS:000743249300001,
Author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and
   Semenova, Lesia and Zhong, Chudi},
Title = {Interpretable machine learning: Fundamental principles and 10 grand
   challenges},
Journal = {STATISTICS SURVEYS},
Year = {2022},
Volume = {16},
Pages = {1-85},
Abstract = {Interpretability in machine learning (ML) is crucial for high stakes
   decisions and troubleshooting. In this work, we provide fundamental
   principles for interpretable ML, and dispel common misunderstandings
   that dilute the importance of this crucial topic. We also identify 10
   technical challenge areas in interpretable machine learning and provide
   history and background on each problem. Some of these problems are
   classically important, and some are recent problems that have arisen in
   the last few years. These problems are: (1) Optimizing sparse logical
   models such as decision trees; (2) Optimization of scoring systems; (3)
   Placing constraints into generalized additive models to encourage
   sparsity and better interpretability; (4) Modern case-based reasoning,
   including neural networks and matching for causal inference; (5)
   Complete supervised disentanglement of neural networks; (6) Complete or
   even partial unsupervised disentanglement of neural networks; (7)
   Dimensionality reduction for data visualization; (8) Machine learning
   models that can incorporate physics and other generative or causal
   constraints; (9) Characterization of the ``Rashomon set{''} of good
   models; and (10) Interpretable reinforcement learning. This survey is
   suitable as a starting point for statisticians and computer scientists
   interested in working in interpretable machine learning.},
DOI = {10.1214/21-SS133},
ISSN = {1935-7516},
ResearcherID-Numbers = {Chen, Chaofan/AED-3261-2022
   },
ORCID-Numbers = {Chen, Chaofan/0000-0002-9250-5887},
Unique-ID = {WOS:000743249300001},
}

@article{ WOS:001099973100001,
Author = {Han, Henry and Wu, Yi and Wang, Jiacun and Han, Ashley},
Title = {Interpretable machine learning assessment},
Journal = {NEUROCOMPUTING},
Year = {2023},
Volume = {561},
Month = {DEC 7},
Abstract = {With the surge of machine learning in AI and data science, there remains
   an urgent need to not only compare the performance of different methods
   across diverse datasets but also to analyze machine learning behaviors
   with sensitivity using an explainable approach. In this study, we
   introduce a uniquely designed diagnostic index: dindex to tackle this
   challenge. This tool integrates classification effectiveness from
   multiple dimensions, delivering a transparent and comprehensive
   assessment that transcends the limitations of traditional evaluation
   methods in classification. We propose two innovative concepts: breakeven
   states and imbalanced points in this study. Integrated with the d-index,
   these concepts afford a more profound understanding of the learning
   behaviors across different machine learning models compared to the
   existing classification metrics. Significantly, the d-index excels as a
   powerful tool, identifying learning singularity problems (LSPs) that
   remain elusive to most current machine learning models and imbalanced
   learning techniques. Furthermore, leveraging the d-index, we unravel the
   mechanisms behind imbalanced point generation in binary and multiclass
   classification. We also put forth a novel technique: identifying a
   priori informative kernels to optimize support vector machine learning,
   ensuring outstanding d-index values with the fewest necessary support
   vectors. Moreover, we address a seldomdiscussed state of overfitting in
   deep learning, where overfitting occurs despite the training and testing
   loss curves exhibiting favorable trends throughout the epochs. To the
   best of our knowledge, this work represents a pioneering stride in the
   realm of explainable machine learning assessments and will inspire
   further studies in this area.},
DOI = {10.1016/j.neucom.2023.126891},
EarlyAccessDate = {OCT 2023},
Article-Number = {126891},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Wang, Jiacun/HSF-1978-2023},
ORCID-Numbers = {Wang, Jiacun/0000-0003-4176-3947
   },
Unique-ID = {WOS:001099973100001},
}

@article{ WOS:000952857700002,
Author = {Grosse, Kathrin and Bieringer, Lukas and Besold, Tarek R. and Biggio,
   Battista and Krombholz, Katharina},
Title = {Machine Learning Security in Industry: A Quantitative Survey},
Journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
Year = {2023},
Volume = {18},
Pages = {1749-1762},
Abstract = {Despite the large body of academic work on machine learning security,
   little is known about the occurrence of attacks on machine learning
   systems in the wild. In this paper, we report on a quantitative study
   with 139 industrial practitioners. We analyze attack occurrence and
   concern and evaluate statistical hypotheses on factors influencing
   threat perception and exposure. Our results shed light on real-world
   attacks on deployed machine learning. On the organizational level, while
   we find no predictors for threat exposure in our sample, the amount of
   implement defenses depends on exposure to threats or expected likelihood
   to become a target. We also provide a detailed analysis of
   practitioners' replies on the relevance of individual machine learning
   attacks, unveiling complex concerns like unreliable decision making,
   business information leakage, and bias introduction into models.
   Finally, we find that on the individual level, prior knowledge about
   machine learning security influences threat perception. Our work paves
   the way for more research about adversarial machine learning in
   practice, but yields also insights for regulation and auditing.},
DOI = {10.1109/TIFS.2023.3251842},
ISSN = {1556-6013},
EISSN = {1556-6021},
ResearcherID-Numbers = {Grosse, Kathrin/JOK-9043-2023
   Krombholz, Katharina/AAI-3222-2021
   BIGGIO, BATTISTA/M-5931-2016
   Biggio, Battista/HCI-4766-2022},
ORCID-Numbers = {Bieringer, Lukas/0009-0004-8172-965X
   Besold, Tarek Richard/0000-0002-8002-0049
   BIGGIO, BATTISTA/0000-0001-7752-509X
   },
Unique-ID = {WOS:000952857700002},
}

@article{ WOS:001070983900001,
Author = {Nam, Wongyung and Jang, Beakcheol},
Title = {A survey on multimodal bidirectional machine learning translation of
   image and natural language processing},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2024},
Volume = {235},
Month = {JAN},
Abstract = {Advances in multimodal machine learning help artificial intelligence to
   resemble human intellect more closely, which perceives the world from
   multiple modalities. We surveyed state-of-the-art research on the
   modalities of bidirectional machine learning translation of image and
   natural language processing (NLP), which address a considerable
   proportion of human life. Recently, with the advances in deep learning
   model architectures and learning methods in the fields of image and NLP,
   considerable progress has been made in multimodal machine learning
   translations that can be built by integrating image and NLP. Our goal is
   to explore and summarize state-of-the-art research on multimodal machine
   learning translation and present a taxonomy for the multimodal
   bidirectional machine learning translation of image and NLP.
   Furthermore, we reviewed the evaluation metrics and compared
   state-of-the-art approaches that influences this field. We believe that
   this survey will become a cornerstone of future research by discussing
   the challenges in multimodal machine learning translation and direction
   of future research based on understanding state-of-the-art research in
   the field.},
DOI = {10.1016/j.eswa.2023.121168},
EarlyAccessDate = {AUG 2023},
Article-Number = {121168},
ISSN = {0957-4174},
EISSN = {1873-6793},
ORCID-Numbers = {Nam, Wongyung/0000-0002-5198-7189},
Unique-ID = {WOS:001070983900001},
}

@article{ WOS:000888210300020,
Author = {Cerezo, M. and Verdon, Guillaume and Huang, Hsin-Yuan and Cincio, Lukasz
   and Coles, Patrick J.},
Title = {Challenges and opportunities in quantum machine learning},
Journal = {NATURE COMPUTATIONAL SCIENCE},
Year = {2022},
Volume = {2},
Number = {9},
Pages = {567-576},
Month = {SEP},
Abstract = {At the intersection of machine learning and quantum computing, quantum
   machine learning has the potential of accelerating data analysis,
   especially for quantum data, with applications for quantum materials,
   biochemistry and high-energy physics. Nevertheless, challenges remain
   regarding the trainability of quantum machine learning models. Here we
   review current methods and applications for quantum machine learning. We
   highlight differences between quantum and classical machine learning,
   with a focus on quantum neural networks and quantum deep learning.
   Finally, we discuss opportunities for quantum advantage with quantum
   machine learning.},
DOI = {10.1038/s43588-022-00311-3},
EISSN = {2662-8457},
ResearcherID-Numbers = {Cerezo, Marco/ABD-9254-2020},
ORCID-Numbers = {Cerezo, Marco/0000-0002-2757-3170
   Huang, Hsin-Yuan/0000-0001-5317-2613
   },
Unique-ID = {WOS:000888210300020},
}

@article{ WOS:001339794500006,
Author = {Zhu, Mengyuan and Wang, Jiawei and Yang, Xiao and Zhang, Yu and Zhang,
   Linyu and Ren, Hongqiang and Wu, Bing and Ye, Lin},
Title = {A review of the application of machine learning in water quality
   evaluation},
Journal = {ECO-ENVIRONMENT \& HEALTH},
Year = {2022},
Volume = {1},
Number = {2},
Pages = {107-116},
Month = {JUN},
Abstract = {With the rapid increase in the volume of data on the aquatic
   environment, machine learning has become an important tool for data
   analysis, classification, and prediction. Unlike traditional models used
   in water-related research, data-driven models based on machine learning
   can efficiently solve more complex nonlinear problems. In water
   environment research, models and conclusions derived from machine
   learning have been applied to the construction, monitoring, simulation,
   evaluation, and optimization of various water treatment and management
   systems. Additionally, machine learning can provide solutions for water
   pollution control, water quality improvement, and watershed ecosystem
   security management. In this review, we describe the cases in which
   machine learning algorithms have been applied to evaluate the water
   quality in different water environments, such as surface water,
   groundwater, drinking water, sewage, and seawater. Furthermore, we
   propose possible future applications of machine learning approaches to
   water environments.},
DOI = {10.1016/j.eehl.2022.06.001},
EISSN = {2772-9850},
ResearcherID-Numbers = {Ye, Lin/F-9709-2013
   hq, ren/AGA-1128-2022
   WANG, Jiawei/GMX-3167-2022
   Wu, Bing/H-8245-2014
   Zhu, Mengyuan/LEN-0284-2024
   LINYU, ZHANG/ADC-3393-2022},
Unique-ID = {WOS:001339794500006},
}

@article{ WOS:000833418600004,
Author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma,
   Tianlong and He, Liang},
Title = {A survey of human-in-the-loop for machine learning},
Journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
Year = {2022},
Volume = {135},
Pages = {364-381},
Month = {OCT},
Abstract = {Machine learning has become the state-of-the-art technique for many
   tasks including computer vision, natural language processing, speech
   processing tasks, etc. However, the unique challenges posed by machine
   learning suggest that incorporating user knowledge into the system can
   be beneficial. The purpose of integrating human domain knowledge is also
   to promote the automation of machine learning. Human-in-the-loop is an
   area that we see as increasingly important in future research due to the
   knowledge learned by machine learning cannot win human domain knowledge.
   Human-in-the-loop aims to train an accurate prediction model with
   minimum cost by integrating human knowledge and experience. Humans can
   provide training data for machine learning applications and directly
   accomplish tasks that are hard for computers in the pipeline with the
   help of machine-based approaches. In this paper, we survey existing
   works on human-in-the-loop from a data perspective and classify them
   into three categories with a progressive relationship: (1) the work of
   improving model performance from data processing, (2) the work of
   improving model performance through interventional model training, and
   (3) the design of the system independent human-in-the-loop. Using the
   above categorization, we summarize the major approaches in the field;
   along with their technical strengths/weaknesses, we have a simple
   classification and discussion in natural language processing, computer
   vision, and others. Besides, we provide some open challenges and
   opportunities. This survey intends to provide a high-level summarization
   for human-in-the-loop and to motivate interested readers to consider
   approaches for designing effective human-in-the-loop solutions.
   Keywords: Human-in-the-loop Machine learning Deep learning Data
   processing Computer vision Natural language processing (C) 2022 Elsevier
   B.V. All rights reserved.},
DOI = {10.1016/j.future.2022.05.014},
EarlyAccessDate = {MAY 2022},
ISSN = {0167-739X},
EISSN = {1872-7115},
ResearcherID-Numbers = {Wu, Xingjiao/AAH-8983-2020
   Xiao, Luwei/JHT-4685-2023
   He, Liang/CAF-0477-2022},
ORCID-Numbers = {Wu, XingJiao/0000-0001-9146-051X
   Xiao, Luwei/0000-0001-7229-2741
   },
Unique-ID = {WOS:000833418600004},
}

@article{ WOS:000849857300001,
Author = {Mai, Haoxin and Le, Tu C. and Chen, Dehong and Winkler, David A. and
   Caruso, Rachel A.},
Title = {Machine Learning for Electrocatalyst and Photocatalyst Design and
   Discovery},
Journal = {CHEMICAL REVIEWS},
Year = {2022},
Volume = {122},
Number = {16},
Pages = {13478-13515},
Month = {AUG 24},
Abstract = {Electrocatalysts and photocatalysts are key to a sustainable future,
   generating clean fuels, reducing the impact of global warming, and
   providing solutions to environmental pollution. Improved processes for
   catalyst design and a better understanding of electro/ photocatalytic
   processes are essential for improving catalyst effectiveness. Recent
   advances in data science and artificial intelligence have great
   potential to accelerate electrocatalysis and photocatalysis research,
   particularly the rapid exploration of large materials chemistry spaces
   through machine learning. Here a comprehensive introduction to, and
   critical review of, machine learning techniques used in electrocatalysis
   and photocatalysis research are provided. Sources of
   electro/photocatalyst data and current approaches to representing these
   materials by mathematical features are described, the most commonly used
   machine learning methods summarized, and the quality and utility of
   electro/photocatalyst models evaluated. Illustrations of how machine
   learning models are applied to novel electro/ photocatalyst discovery
   and used to elucidate electrocatalytic or photocatalytic reaction
   mechanisms are provided. The review offers a guide for materials
   scientists on the selection of machine learning methods for
   electrocatalysis and photocatalysis research. The application of machine
   learning to catalysis science represents a paradigm shift in the way
   advanced, next-generation catalysts will be designed and synthesized.},
DOI = {10.1021/acs.chemrev.2c00061},
ISSN = {0009-2665},
EISSN = {1520-6890},
ResearcherID-Numbers = {Caruso, Rachel/E-2353-2013
   Winkler, Dave/A-3774-2008
   Mai, Haoxin/IVV-8150-2023
   Chen, Dehong/AHH-4317-2022
   Winkler, David/A-3774-2008
   Le, Cam/Y-9868-2019},
ORCID-Numbers = {Winkler, Dave/0000-0002-7301-6076
   Mai, Haoxin/0000-0002-3493-2717
   Le, Tu/0000-0003-3552-8211
   },
Unique-ID = {WOS:000849857300001},
}

@article{ WOS:000770194100002,
Author = {Esterhuizen, Jacques A. and Goldsmith, Bryan R. and Linic, Suljo},
Title = {Interpretable machine learning for knowledge generation in heterogeneous
   catalysis},
Journal = {NATURE CATALYSIS},
Year = {2022},
Volume = {5},
Number = {3},
Pages = {175-184},
Month = {MAR},
Abstract = {Most applications of machine learning in heterogeneous catalysis thus
   far have used black-box models to predict computable physical properties
   (descriptors), such as adsorption or formation energies, that can be
   related to catalytic performance (that is, activity or stability).
   Extracting meaningful physical insights from these black-box models has
   proved challenging, as the internal logic of these black-box models is
   not readily interpretable due to their high degree of complexity.
   Interpretable machine learning methods that merge the predictive
   capacity of black-box models with the physical interpretability of
   physics-based models offer an alternative to black-box models. In this
   Perspective, we discuss the various interpretable machine learning
   methods available to catalysis researchers, highlight the potential of
   interpretable machine learning to accelerate hypothesis formation and
   knowledge generation, and outline critical challenges and opportunities
   for interpretable machine learning in heterogeneous catalysis.},
DOI = {10.1038/s41929-022-00744-z},
EarlyAccessDate = {MAR 2022},
ISSN = {2520-1158},
ResearcherID-Numbers = {Goldsmith, Bryan/K-9502-2014},
ORCID-Numbers = {Esterhuizen, Jacques/0000-0002-2582-3172
   Linic, Suljo/0000-0003-2153-6755
   Goldsmith, Bryan/0000-0003-1264-8018},
Unique-ID = {WOS:000770194100002},
}

@article{ WOS:000899553000001,
Author = {Moein, Mohammad Mohtasham and Saradar, Ashkan and Rahmati, Komeil and
   Mousavinejad, Seyed Hosein Ghasemzadeh and Bristow, James and Aramali,
   Vartenie and Karakouzian, Moses},
Title = {Predictive models for concrete properties using machine learning and
   deep learning approaches: A review},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2023},
Volume = {63},
Number = {A},
Month = {JAN 1},
Abstract = {Concrete is one of the most widely used materials in various civil
   engineering applications. Its global production rate is increasing to
   meet demand. Mechanical properties of concrete are among important
   parameters in designing and evaluating its performance. Over the past
   few decades, machine learning has been used to model real-world
   problems. Machine learning, as a branch of artificial intelligence, is
   gaining popularity in many scientific fields such as robotics,
   statistics, bioinformatics, computer science, and construction
   materials. Machine learning has many advantages over statistical and
   experimental models, such as optimal accuracy, highperformance speed,
   responsiveness in complex environments, and economic cost-effectiveness.
   Recently, more researchers are looking into deep learning, which is a
   group of machine learning algorithms, as a powerful method in matters of
   diagnosis and classification. Hence, this paper provides a review of
   successful ML and DL model applications to predict concrete mechanical
   properties. Several modeling algorithms were reviewed highlighting their
   applications, performance, current knowledge gaps, and suggestions for
   future research. This paper will assist construction material engineers
   and researchers in selecting suitable and accurate techniques that fit
   their applications.},
DOI = {10.1016/j.jobe.2022.105444},
EarlyAccessDate = {OCT 2022},
Article-Number = {105444},
EISSN = {2352-7102},
ResearcherID-Numbers = {Saradar, Ashkan/S-7069-2019
   MOHTASHAM MOEIN, MOHAMMAD/GPP-1664-2022
   Aramali, Vartenie/AGZ-7909-2022
   },
ORCID-Numbers = {Mohtasham Moein, Mohammad/0000-0003-3756-6959
   Aramali, Vartenie/0000-0002-1631-5360
   Rahmati, Komeil/0000-0002-4630-7265
   Saradar, Ashkan/0000-0003-4696-3382},
Unique-ID = {WOS:000899553000001},
}

@article{ WOS:000410555900032,
Author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost,
   Patrick and Wiebe, Nathan and Lloyd, Seth},
Title = {Quantum machine learning},
Journal = {NATURE},
Year = {2017},
Volume = {549},
Number = {7671},
Pages = {195-202},
Month = {SEP 14},
Abstract = {Fuelled by increasing computer power and algorithmic advances, machine
   learning techniques have become powerful tools for finding patterns in
   data. Quantum systems produce atypical patterns that classical systems
   are thought not to produce efficiently, so it is reasonable to postulate
   that quantum computers may outperform classical computers on machine
   learning tasks. The field of quantum machine learning explores how to
   devise and implement quantum software that could enable machine learning
   that is faster than that of classical computers. Recent work has
   produced quantum algorithms that could act as the building blocks of
   machine learning programs, but the hardware and software challenges are
   still considerable.},
DOI = {10.1038/nature23474},
ISSN = {0028-0836},
EISSN = {1476-4687},
ResearcherID-Numbers = {Biamonte, Jacob/U-8622-2019
   Biamonte, Jacob/E-5633-2010},
ORCID-Numbers = {Biamonte, Jacob/0000-0002-0590-3327},
Unique-ID = {WOS:000410555900032},
}

@article{ WOS:001150877600003,
Author = {Lee, Yongjae and Thompson, John R. J. and Kim, Jang Ho and Kim, Woo
   Chang and Fabozzi, Francesco A.},
Title = {An Overview of Machine Learning for Asset Management},
Journal = {JOURNAL OF PORTFOLIO MANAGEMENT},
Year = {2023},
Volume = {49},
Number = {9, SI},
Pages = {31-63},
Month = {SEP},
Abstract = {Machine learning has been widely used in the asset management industry
   to improve operations and make data-driven decisions. This article
   provides an overview of machine learning for asset management by
   presenting various machine learning models in the context of their
   applications, including general classification and regression,
   time-series forecasting, natural language processing, dimension
   reduction, reinforcement learning, data generation, recommendation, and
   clustering. Additionally, it highlights the challenges of implementing
   machine learning in asset management, such as data quality and quantity,
   interpretability, and fairness.},
ISSN = {0095-4918},
EISSN = {2168-8656},
ResearcherID-Numbers = {Kim, Woo/C-2066-2011
   Lee, Yongjae/Q-4430-2018
   Thompson, John/AAM-5588-2021
   KIM, Seung Hyun/T-5133-2017},
Unique-ID = {WOS:001150877600003},
}

@article{ WOS:000917611300001,
Author = {Michaud, Eric J. J. and Liu, Ziming and Tegmark, Max},
Title = {Precision Machine Learning},
Journal = {ENTROPY},
Year = {2023},
Volume = {25},
Number = {1},
Month = {JAN},
Abstract = {We explore unique considerations involved in fitting machine learning
   (ML) models to data with very high precision, as is often required for
   science applications. We empirically compare various function
   approximation methods and study how they scale with increasing
   parameters and data. We find that neural networks (NNs) can often
   outperform classical approximation methods on high-dimensional examples,
   by (we hypothesize) auto-discovering and exploiting modular structures
   therein. However, neural networks trained with common optimizers are
   less powerful for low-dimensional cases, which motivates us to study the
   unique properties of neural network loss landscapes and the
   corresponding optimization challenges that arise in the high precision
   regime. To address the optimization issue in low dimensions, we develop
   training tricks which enable us to train neural networks to extremely
   low loss, close to the limits allowed by numerical precision.},
DOI = {10.3390/e25010175},
Article-Number = {175},
EISSN = {1099-4300},
ORCID-Numbers = {Tegmark, Max/0000-0001-7670-7190},
Unique-ID = {WOS:000917611300001},
}

@article{ WOS:000998914800001,
Author = {Dehkharghanian, Taher and Mu, Youqing and Tizhoosh, Hamid R. and
   Campbell, Clinton J. V.},
Title = {Applied machine learning in hematopathology},
Journal = {INTERNATIONAL JOURNAL OF LABORATORY HEMATOLOGY},
Year = {2023},
Volume = {45},
Number = {2, SI},
Pages = {87-94},
Month = {JUN},
Abstract = {An increasing number of machine learning applications are being
   developed and applied to digital pathology, including hematopathology.
   The goal of these modern computerized tools is often to support
   diagnostic workflows by extracting and summarizing information from
   multiple data sources, including digital images of human tissue.
   Hematopathology is inherently multimodal and can serve as an ideal case
   study for machine learning applications. However, hematopathology also
   poses unique challenges compared to other pathology subspecialities when
   applying machine learning approaches. By modeling the pathologist
   workflow and thinking process, machine learning algorithms may be
   designed to address practical and tangible problems in hematopathology.
   In this article, we discuss the current trends in machine learning in
   hematopathology. We review currently available machine learning enabled
   medical devices supporting hematopathology workflows. We then explore
   current machine learning research trends of the field with a focus on
   bone marrow cytology and histopathology, and how adoption of new machine
   learning tools may be enabled through the transition to digital
   pathology.},
DOI = {10.1111/ijlh.14110},
EarlyAccessDate = {MAY 2023},
ISSN = {1751-5521},
EISSN = {1751-553X},
ResearcherID-Numbers = {Tizhoosh, Hamid/G-1369-2014
   Tizhoosh, Hamid/KIL-6277-2024
   Campbell, Clinton/MDT-0853-2025
   },
ORCID-Numbers = {Tizhoosh, Hamid/0000-0001-5488-601X
   Campbell, Clinton/0000-0002-8896-1134
   Mu, Youqing/0000-0002-4561-5514},
Unique-ID = {WOS:000998914800001},
}

@article{ WOS:001011556200001,
Author = {Giudici, Paolo and Gramegna, Alex and Raffinetti, Emanuela},
Title = {Machine Learning Classification Model Comparison},
Journal = {SOCIO-ECONOMIC PLANNING SCIENCES},
Year = {2023},
Volume = {87},
Number = {B},
Month = {JUN},
Abstract = {Machine learning models are boosting Artificial Intelligence
   applications in many domains, such as automotive, finance and health
   care. This is mainly due to their advantage, in terms of predictive
   accuracy, with respect to classic statistical models. However, machine
   learning models are much less explainable: less transparent, less
   interpretable. This paper proposes to improve machine learning models,
   by proposing a model selection methodology, based on Lorenz Zonoids,
   which allows to compare them in terms of predictive accuracy significant
   gains, leading to a selected model which maintains accuracy while
   improving explainability. We illustrate our proposal by means of
   simulated datasets and of a real credit scoring problem. The analysis of
   the former shows that the proposal improves alternative methods, based
   on the AUROC. The analysis of the latter shows that the proposal leads
   to models made up of two/three relevant variables that measure the
   profitability and the financial leverage of the companies asking for
   credit.},
DOI = {10.1016/j.seps.2023.101560},
EarlyAccessDate = {MAY 2023},
Article-Number = {101560},
ISSN = {0038-0121},
EISSN = {1873-6041},
ResearcherID-Numbers = {Giudici, Paolo/AAA-5126-2021
   Raffinetti, Emanuela/MCJ-7704-2025
   Giudici, Paolo Stefano/AAA-5126-2021
   Raffinetti, Emanuela/D-2536-2018},
ORCID-Numbers = {Giudici, Paolo Stefano/0000-0002-4198-0127
   Raffinetti, Emanuela/0000-0002-0386-7975},
Unique-ID = {WOS:001011556200001},
}

@article{ WOS:001036662400001,
Author = {Komuro, Jin and Kusumoto, Dai and Hashimoto, Hisayuki and Yuasa,
   Shinsuke},
Title = {Machine learning in cardiology: Clinical application and basic research},
Journal = {JOURNAL OF CARDIOLOGY},
Year = {2023},
Volume = {82},
Number = {2},
Pages = {128-133},
Month = {AUG},
Abstract = {Machine learning is a subfield of artificial intelligence. The quality
   and versatility of machine learning have been rapidly improving and
   playing a critical role in many aspects of social life. This trend is
   also observed in the med-ical field. Generally, there are three main
   types of machine learning: supervised, unsupervised, and reinforcement
   learning. Each type of learning is adequately selected for the purpose
   and type of data. In the field of medicine, various types of information
   are collected and used, and research using machine learning is becoming
   increas-ingly relevant. Many clinical studies are conducted using
   electronic health and medical records, including in the cardiovascular
   area. Machine learning has also been applied in basic research. Machine
   learning has been widely used for several types of data analysis, such
   as clustering of microarray analysis and RNA sequence anal-ysis. Machine
   learning is essential for genome and multi-omics analyses. This review
   summarizes the recent ad-vancements in the use of machine learning in
   clinical applications and basic cardiovascular research.\& COPY; 2023
   Japanese College of Cardiology. Published by Elsevier Ltd. All rights
   reserved.},
DOI = {10.1016/j.jjcc.2023.04.020},
EarlyAccessDate = {JUN 2023},
ISSN = {0914-5087},
EISSN = {1876-4738},
ResearcherID-Numbers = {Yuasa, Shinsuke/HMO-8133-2023
   Kusumoto, Dai/LXB-3172-2024},
ORCID-Numbers = {Yuasa, Shinsuke/0000-0001-5593-7552
   },
Unique-ID = {WOS:001036662400001},
}

@article{ WOS:001133095300012,
Author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E.},
Title = {pystacked: Stacking generalization and machine learning in Stata},
Journal = {STATA JOURNAL},
Year = {2023},
Volume = {23},
Number = {4},
Pages = {909-931},
Month = {DEC},
Abstract = {The pystacked command implements stacked generalization (Wolpert, 1992,
   Neural Networks 5: 241-259) for regression and binary classification via
   Python's scikit-learn. Stacking combines multiple supervised machine
   learners-the ``base{''} or ``level-0{''} learners-into one learner. The
   currently supported base learners include regularized regression, random
   forest, gradient boosted trees, support vector machines, and
   feed-forward neural nets (multilayer perceptron). pystacked can also be
   used as a ``regular{''} machine learning program to fit one base learner
   and thus provides an easy-to-use application programming interface for
   scikit-learn's machine learning algorithms.},
DOI = {10.1177/1536867X231212426},
ISSN = {1536-867X},
EISSN = {1536-8734},
ResearcherID-Numbers = {Hansen, Christian/KHE-4355-2024
   },
ORCID-Numbers = {Ahrens, Achim/0000-0002-3201-6395
   Hansen, Christian/0009-0006-8123-1039},
Unique-ID = {WOS:001133095300012},
}

@article{ WOS:000985911200001,
Author = {Stevens, Christophe A. T. and Lyons, Alexander R. M. and Dharmayat, I,
   Kanika and Mahani, Alireza and Ray, Kausik K. and Vallejo-Vaz, Antonio
   J. and Sharabiani, Mansour T. A.},
Title = {Ensemble machine learning methods in screening electronic health
   records: A scoping review},
Journal = {DIGITAL HEALTH},
Year = {2023},
Volume = {9},
Abstract = {BackgroundElectronic health records provide the opportunity to identify
   undiagnosed individuals likely to have a given disease using machine
   learning techniques, and who could then benefit from more medical
   screening and case finding, reducing the number needed to screen with
   convenience and healthcare cost savings. Ensemble machine learning
   models combining multiple prediction estimates into one are often said
   to provide better predictive performances than non-ensemble models. Yet,
   to our knowledge, no literature review summarises the use and
   performances of different types of ensemble machine learning models in
   the context of medical pre-screening. MethodWe aimed to conduct a
   scoping review of the literature reporting the derivation of ensemble
   machine learning models for screening of electronic health records. We
   searched EMBASE and MEDLINE databases across all years applying a formal
   search strategy using terms related to medical screening, electronic
   health records and machine learning. Data were collected, analysed, and
   reported in accordance with the PRISMA scoping review guideline.
   ResultsA total of 3355 articles were retrieved, of which 145 articles
   met our inclusion criteria and were included in this study. Ensemble
   machine learning models were increasingly employed across several
   medical specialties and often outperformed non-ensemble approaches.
   Ensemble machine learning models with complex combination strategies and
   heterogeneous classifiers often outperformed other types of ensemble
   machine learning models but were also less used. Ensemble machine
   learning models methodologies, processing steps and data sources were
   often not clearly described. ConclusionsOur work highlights the
   importance of deriving and comparing the performances of different types
   of ensemble machine learning models when screening electronic health
   records and underscores the need for more comprehensive reporting of
   machine learning methodologies employed in clinical research.},
DOI = {10.1177/20552076231173225},
Article-Number = {20552076231173225},
ISSN = {2055-2076},
ResearcherID-Numbers = {Ray, Kausik/Z-2055-2019
   Vallejo-Vaz, Antonio/AAT-9301-2021
   },
ORCID-Numbers = {Vallejo-Vaz, Antonio J./0000-0001-7954-1253
   STEVENS, CHRISTOPHE A.T./0000-0002-7129-8524
   Ray, Kausik/0000-0003-0508-0954},
Unique-ID = {WOS:000985911200001},
}

@article{ WOS:001124950300001,
Author = {Liu, Risheng and Lin, Zhouchen},
Title = {Bilevel optimization for automated machine learning: a new perspective
   on framework and algorithm},
Journal = {NATIONAL SCIENCE REVIEW},
Year = {2023},
Volume = {11},
Number = {8},
Month = {DEC 13},
Abstract = {Formulating the methodology of machine learning by bilevel optimization
   techniques provides a new perspective to understand and solve automated
   machine learning problems.},
DOI = {10.1093/nsr/nwad292},
EarlyAccessDate = {DEC 2023},
ISSN = {2095-5138},
EISSN = {2053-714X},
ResearcherID-Numbers = {Liu, Risheng/A-8716-2015
   },
ORCID-Numbers = {Liu, Risheng/0000-0002-9554-0565},
Unique-ID = {WOS:001124950300001},
}

@article{ WOS:001015283800001,
Author = {Raghavendran, Krishna Raj and Elragal, Ahmed},
Title = {Low-Code Machine Learning Platforms: A Fastlane to Digitalization},
Journal = {INFORMATICS-BASEL},
Year = {2023},
Volume = {10},
Number = {2},
Month = {JUN},
Abstract = {In the context of developing machine learning models, until and unless
   we have the required data engineering and machine learning development
   competencies as well as the time to train and test different machine
   learning models and tune their hyperparameters, it is worth trying out
   the automatic machine learning features provided by several cloud-based
   and cloud-agnostic platforms. This paper explores the possibility of
   generating automatic machine learning models with low-code experience.
   We developed criteria to compare different machine learning platforms
   for generating automatic machine learning models and presenting their
   results. Thereafter, lessons learned by developing automatic machine
   learning models from a sample dataset across four different machine
   learning platforms were elucidated. We also interviewed machine learning
   experts to conceptualize their domain-specific problems that automatic
   machine learning platforms can address. Results showed that automatic
   machine learning platforms can provide a fast track for organizations
   seeking the digitalization of their businesses. Automatic machine
   learning platforms help produce results, especially for time-constrained
   projects where resources are lacking. The contribution of this paper is
   in the form of a lab experiment in which we demonstrate how low-code
   platforms can provide a viable option to many business cases and,
   henceforth, provide a lane that is faster than the usual hiring and
   training of already scarce data scientists and to analytics projects
   that suffer from overruns.},
DOI = {10.3390/informatics10020050},
Article-Number = {50},
EISSN = {2227-9709},
ResearcherID-Numbers = {Elragal, Ahmed/AGQ-5501-2022
   },
ORCID-Numbers = {Elragal, Ahmed/0000-0003-4250-4752
   Raghavendran, Krishnaraj/0000-0001-7059-7816},
Unique-ID = {WOS:001015283800001},
}

@article{ WOS:001096009300001,
Author = {Swacha, Jakub and Gracel, Michal},
Title = {Machine Learning in Gamification and Gamification in Machine Learning: A
   Systematic Literature Mapping},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {20},
Month = {OCT},
Abstract = {Albeit in different ways, both machine learning and gamification have
   transfigured the user experience of information systems. Although both
   are hot research topics, so far, little attention has been paid to how
   these two technologies converge with each other. This relation is not
   obvious as while it is feasible to enhance gamification with machine
   learning, it is also feasible to support machine learning with
   gamification; moreover, there are applications in which machine learning
   and gamification are combined yet not directly connected. In this study,
   we aim to shed light on the use of both machine learning in gamification
   and gamification in machine learning, as well as the related topics of
   using gamification in machine learning education and machine learning in
   gamification research. By performing a systematic literature mapping, we
   not only identify prior works addressing these respective themes, but
   also analyze how their popularity evolved in time, investigate the areas
   of application reported by prior works, used machine learning techniques
   and software tools, as well as the character of research contribution
   and the character of evaluation results for works that presented them.},
DOI = {10.3390/app132011427},
Article-Number = {11427},
EISSN = {2076-3417},
ResearcherID-Numbers = {Swacha, Jakub/AAC-8790-2020
   },
ORCID-Numbers = {Swacha, Jakub/0000-0002-2214-6989
   Gracel, Michal/0009-0009-3945-9269},
Unique-ID = {WOS:001096009300001},
}

@article{ WOS:001001117700011,
Author = {Huang, Feiqi and Wang, Yunsen},
Title = {Introducing Machine Learning in Auditing Courses},
Journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING},
Year = {2023},
Volume = {20},
Number = {1},
Pages = {195-211},
Abstract = {The advances in machine learning have gained close attention from audit
   practitioners and standard setters. However, fewer than half of
   accounting programs teach predictive analysis, including machine
   learning. To develop students' knowledge and skills of machine learning
   in auditing applications, this study introduces machine learning to the
   accounting curriculum and presents a novel hands-on approach for
   teaching machine learning in auditing courses. The objective is to
   provide students who have no statistics background and programming
   skills with the basic knowledge of machine learning and hands-on
   exercises for predicting auditing tasks. In addition to instruction
   manuals, this study demonstrates an implementation of machine learning
   exercises in a graduate-level course.},
DOI = {10.2308/JETA-2022-017},
ISSN = {1554-1908},
EISSN = {1558-7940},
ORCID-Numbers = {Wang, Yunsen/0000-0001-8360-945X},
Unique-ID = {WOS:001001117700011},
}

@article{ WOS:000760318100006,
Author = {Cui, Peng and Athey, Susan},
Title = {Stable learning establishes some common ground between causal inference
   and machine learning},
Journal = {NATURE MACHINE INTELLIGENCE},
Year = {2022},
Volume = {4},
Number = {2},
Pages = {110-115},
Month = {FEB},
Abstract = {Causal inference has recently attracted substantial attention in the
   machine learning and artificial intelligence community. It is usually
   positioned as a distinct strand of research that can broaden the scope
   of machine learning from predictive modelling to intervention and
   decision-making. In this Perspective, however, we argue that ideas from
   causality can also be used to improve the stronghold of machine
   learning, predictive modelling, if predictive stability, explainability
   and fairness are important. With the aim of bridging the gap between the
   tradition of precise modelling in causal inference and black-box
   approaches from machine learning, stable learning is proposed and
   developed as a source of common ground. This Perspective clarifies a
   source of risk for machine learning models and discusses the benefits of
   bringing causality into learning. We identify the fundamental problems
   addressed by stable learning, as well as the latest progress from both
   causal inference and learning perspectives, and we discuss relationships
   with explainability and fairness problems.
   Machine learning performs well at predictive modelling based on
   statistical correlations, but for high-stakes applications, more robust,
   explainable and fair approaches are required. Cui and Athey discuss the
   benefits of bringing causal inference into machine learning, presenting
   a stable learning approach.},
DOI = {10.1038/s42256-022-00445-z},
EISSN = {2522-5839},
ORCID-Numbers = {Athey, Susan/0000-0001-6934-562X},
Unique-ID = {WOS:000760318100006},
}

@article{ WOS:000819703300001,
Author = {Mahmood, Asif and Irfan, Ahmad and Wang, Jin-Liang},
Title = {Machine Learning for Organic Photovoltaic Polymers: A Minireview},
Journal = {CHINESE JOURNAL OF POLYMER SCIENCE},
Year = {2022},
Volume = {40},
Number = {8, SI},
Pages = {870-876},
Month = {AUG},
Abstract = {Machine learning is a powerful tool that can provide a way to
   revolutionize the material science. Its use for the designing and
   screening of materials for polymer solar cells is also increasing.
   Search of efficient polymeric materials for solar cells is really
   difficult task. Researchers have synthesized and fabricated so many
   materials. Sorting the results and get feedback for further research
   requires an innovative approach. In this minireview, we provides brief
   introduction of machine learning. The importance of machine learning is
   also mentioned, and the application of machine learning for polymeric
   material design is discussed. The key challenges that are hindering the
   wide spread use of machine are discussed. Suggestions are also given to
   improve the use of data science. The predictions using machine learning
   maybe not highly accurate but it definitely better than no prediction at
   all.},
DOI = {10.1007/s10118-022-2782-5},
EarlyAccessDate = {JUN 2022},
ISSN = {0256-7679},
EISSN = {1439-6203},
ResearcherID-Numbers = {Irfan, Ahmad/R-7642-2019
   Mahmood, Asif/S-5579-2019
   wang, jinliang/GRJ-3717-2022},
ORCID-Numbers = {Mahmood, Asif/0000-0001-9412-1011
   },
Unique-ID = {WOS:000819703300001},
}

@article{ WOS:000760354200006,
Author = {Cai, Zhaoxiang and Poulos, Rebecca C. and Liu, Jia and Zhong, Qing},
Title = {Machine learning for multi-omics data integration in cancer},
Journal = {ISCIENCE},
Year = {2022},
Volume = {25},
Number = {2},
Month = {FEB 18},
Abstract = {Multi-omics data analysis is an important aspect of cancer molecular
   biology studies and has led to ground-breaking discoveries. Many efforts
   have been made to develop machine learning methods that automatically
   integrate omics data. Here, we review machine learning tools categorized
   as either general-purpose or task-specific, covering both supervised and
   unsupervised learning for integrative analysis of multi-omics data. We
   benchmark the performance of five machine learning approaches using data
   from the Cancer Cell Line Encyclopedia, reporting accuracy on cancer
   type classification and mean absolute error on drug response prediction,
   and evaluating runtime efficiency. This review provides recommendations
   to researchers regarding suitable machine learning method selection for
   their specific applications. It should also promote the development of
   novel machine learning methodologies for data integration, which will be
   essential for drug discovery, clinical trial design, and personalized
   treatments.},
DOI = {10.1016/j.isci.2022.103798},
EarlyAccessDate = {FEB 2022},
Article-Number = {103798},
EISSN = {2589-0042},
ResearcherID-Numbers = {Zhong, Qing/ABE-2397-2021
   Cai, Zhaoxiang/OLQ-9952-2025
   Poulos, Rebecca/NWH-5891-2025
   },
ORCID-Numbers = {Liu, Jia/0000-0003-4442-6516
   Zhong, Qing/0000-0002-5340-301X
   Cai, Zhaoxiang/0000-0003-3809-0817},
Unique-ID = {WOS:000760354200006},
}

@article{ WOS:001059183900006,
Author = {Zhou, Lei and Song, Yuntong and Ji, Weiqi and Wei, Haiqiao},
Title = {Machine learning for combustion},
Journal = {ENERGY AND AI},
Year = {2022},
Volume = {7},
Month = {JAN},
Abstract = {Combustion science is an interdisciplinary study that involves nonlinear
   physical and chemical phenomena in time and length scales, including
   complex chemical reactions and fluid flows. Combustion widely supplies
   energy for powering vehicles, heating houses, generating electricity,
   cooking food, etc. The key to study combustion is to improve the
   combustion efficiency with minimum emission of pollutants. Machine
   learning facilitates datadriven techniques for handling large amounts of
   combustion data, either obtained through experiments or simulations
   under multiple spatiotemporal scales, thereby finding the hidden
   patterns underlying these data and promoting combustion research. This
   work presents an overview of studies on the applications of machine
   learning in combustion science fields over the past several decades. We
   introduce the fundamentals of machine learning and its usage in aiding
   chemical reactions, combustion modeling, combustion measurement, engine
   performance prediction and optimization, and fuel design. The
   opportunities and limitations of using machine learning in combustion
   studies are also discussed. This paper aims to provide readers with a
   portrait of what and how machine learning can be used in combustion
   research and to inspire researchers in their ongoing studies. Machine
   learning techniques are rapidly advancing in this era of big data, and
   there is high potential for exploring the combination between machine
   learning and combustion research and achieving remarkable results.},
DOI = {10.1016/j.egyai.2021.100128},
Article-Number = {100128},
ISSN = {2666-5468},
ResearcherID-Numbers = {Zhou, Lei/IZQ-5974-2023
   Ji, Weiqi/AAF-4969-2019},
ORCID-Numbers = {Haiqiao, Wei/0000-0003-3665-4228
   },
Unique-ID = {WOS:001059183900006},
}

@article{ WOS:000804596500003,
Author = {Wang, Yankun and Tang, Huiming and Huang, Jinsong and Wen, Tao and Ma,
   Junwei and Zhang, Junrong},
Title = {A comparative study of different machine learning methods for reservoir
   landslide displacement prediction},
Journal = {ENGINEERING GEOLOGY},
Year = {2022},
Volume = {298},
Month = {MAR 5},
Abstract = {ABSTR A C T This paper compares the performance of five popular machine
   learning methods, namely, particle swarm opti-mization-extreme learning
   machine (PSO-ELM), particle swarm optimization-kernel extreme learning
   machine (PSO-KELM), particle swarm optimization-support vector machine
   (PSO-SVM), particle swarm opti-mization-least squares support vector
   machine (PSO-LSSVM), and long short-term memory neural network (LSTM),
   in the prediction of reservoir landslide displacement. The Baishuihe,
   Shuping, and Baijiabao landslides in the Three Gorges reservoir area of
   China were used for case studies. Cumulative displacement was
   decom-posed into trend displacement and periodic displacement by the
   Hodrick-Prescott filter. The double exponential smoothing method and the
   five machine learning methods were used to predict the trend and
   periodic displacement, respectively. The five machine learning methods
   are compared in three aspects: highest single prediction accuracy, mean
   prediction accuracy, and prediction stability. The results show that no
   method per -formed the best for all three aspects in the three landslide
   cases. LSTM and PSO-ELM achieved better single prediction accuracy, but
   worse mean prediction accuracy and stability. PSO-KELM, PSO-LSSVM, and
   PSO-SVM always yielded consistent predictions with slight variations. On
   the whole, PSO-KELM and PSO-LSSVM are recommended for their superior
   mean prediction accuracy and prediction stability.},
DOI = {10.1016/j.enggeo.2022.106544},
EarlyAccessDate = {FEB 2022},
Article-Number = {106544},
ISSN = {0013-7952},
EISSN = {1872-6917},
ResearcherID-Numbers = {张, 俊荣/GLR-6453-2022
   温, 韬/GXV-1294-2022
   MA, JUNWEI/AFY-2415-2022
   Tang, Huiming/GVT-5806-2022
   },
ORCID-Numbers = {Wen, Tao/0000-0002-4588-3586
   Tang, Huiming/0000-0003-4272-8430},
Unique-ID = {WOS:000804596500003},
}

@article{ WOS:000812536000080,
Author = {Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Lyu,
   Lingjuan and Liu, Ji},
Title = {Data Poisoning Attacks on Federated Machine Learning},
Journal = {IEEE INTERNET OF THINGS JOURNAL},
Year = {2022},
Volume = {9},
Number = {13},
Pages = {11365-11375},
Month = {JUL 1},
Abstract = {Federated machine learning which enables resource-constrained node
   devices (e.g., Internet of Things (IoT) devices and smartphones) to
   establish a knowledge-shared model while keeping the raw data local,
   could provide privacy preservation, and economic benefit by designing an
   effective communication protocol. However, this communication protocol
   can be adopted by attackers to launch data poisoning attacks for
   different nodes, which has been shown as a big threat to most machine
   learning models. Therefore, we in this article intend to study the model
   vulnerability of federated machine learning, and even on IoT systems. To
   be specific, we here attempt to attacking a popular federated multitask
   learning framework, which uses a general multitask learning framework to
   handle statistical challenges in the federated learning setting. The
   problem of calculating optimal poisoning attacks on federated multitask
   learning is formulated as a bilevel program, which is adaptive to the
   arbitrary selection of target nodes and source attacking nodes. We then
   propose a novel systems-aware optimization method, called as attack on
   federated learning (AT(2)FL), to efficiently derive the implicit
   gradients for poisoned data, and further attain optimal attack
   strategies in the federated machine learning. This is an earlier work,
   to our knowledge, that explores attacking federated machine learning via
   data poisoning. Finally, experiments on several real-world data sets
   demonstrate that when the attackers directly poison the target nodes or
   indirectly poison the related nodes via using the communication
   protocol, the federated multitask learning model is sensitive to both
   poisoning attacks.},
DOI = {10.1109/JIOT.2021.3128646},
ISSN = {2327-4662},
ResearcherID-Numbers = {Sun, Gan/ABD-6793-2021},
ORCID-Numbers = {Wang, Qiang/0000-0002-2018-1764
   },
Unique-ID = {WOS:000812536000080},
}

@article{ WOS:000851470400007,
Author = {Balasubramanian, Natarajan and Ye, Yang and Xu, Mingtao},
Title = {SUBSTITUTING HUMAN DECISION-MAKING WITH MACHINE LEARNING: IMPLICATIONS
   FOR ORGANIZATIONAL LEARNING},
Journal = {ACADEMY OF MANAGEMENT REVIEW},
Year = {2022},
Volume = {47},
Number = {3},
Pages = {448-465},
Month = {JUL},
Abstract = {The richness of organizational learning relies on the ability of humans
   to develop diverse patterns of action by actively engaging with their
   environments and applying substantive rationality. The substitution of
   human decision-making with machine learning has the potential to alter
   this richness of organizational learning. Though machine learning is
   significantly faster and seemingly unconstrained by human cognitive
   limitations and inflexibility, it is not true sentient learning and
   relies on formal statistical analysis for decision-making. We propose
   that the distinct differences between human learning and machine
   learning risk decreasing the within-organizational diversity in
   organizational routines and the extent of causal, contextual, and
   general knowledge associated with routines. We theorize that these
   changes may affect organizational learning by exacerbating the myopia of
   learning, and highlight some important contingencies that may mute or
   amplify the risk of such myopia.},
DOI = {10.5465/amr.2019.0470},
ISSN = {0363-7425},
EISSN = {1930-3807},
ResearcherID-Numbers = {Xu, Mingtao/LKL-1777-2024
   Balasubramanian, Natarajan/Y-7764-2019
   },
ORCID-Numbers = {Balasubramanian, Natarajan/0000-0002-3513-5090
   Ye, Yang/0000-0003-4756-0403},
Unique-ID = {WOS:000851470400007},
}

@article{ WOS:000863168300001,
Author = {Mohr, Felix and Wever, Marcel},
Title = {Naive automated machine learning},
Journal = {MACHINE LEARNING},
Year = {2023},
Volume = {112},
Number = {4},
Pages = {1131-1170},
Month = {APR},
Abstract = {An essential task of automated machine learning (AutoML) is the problem
   of automatically finding the pipeline with the best generalization
   performance on a given dataset. This problem has been addressed with
   sophisticated black-box optimization techniques such as Bayesian
   optimization, grammar-based genetic algorithms, and tree search
   algorithms. Most of the current approaches are motivated by the
   assumption that optimizing the components of a pipeline in isolation may
   yield sub-optimal results. We present Naive AutoML, an approach that
   precisely realizes such an in-isolation optimization of the different
   components of a pre-defined pipeline scheme. The returned pipeline is
   obtained by just taking the best algorithm of each slot. The isolated
   optimization leads to substantially reduced search spaces, and,
   surprisingly, this approach yields comparable and sometimes even better
   performance than current state-of-the-art optimizers.},
DOI = {10.1007/s10994-022-06200-0},
EarlyAccessDate = {SEP 2022},
ISSN = {0885-6125},
EISSN = {1573-0565},
ResearcherID-Numbers = {Wever, Marcel/AAE-3091-2020
   },
ORCID-Numbers = {Wever, Marcel/0000-0001-9782-6818},
Unique-ID = {WOS:000863168300001},
}

@article{ WOS:000838252900001,
Author = {Zhou, Zhi-Hua},
Title = {Open-environment machine learning},
Journal = {NATIONAL SCIENCE REVIEW},
Year = {2022},
Volume = {9},
Number = {8},
Month = {AUG 17},
Abstract = {Conventional machine learning studies generally assume close-environment
   scenarios where important factors of the learning process hold
   invariant. With the great success of machine learning, nowadays, more
   and more practical tasks, particularly those involving open-environment
   scenarios where important factors are subject to change, called
   open-environment machine learning in this article, are present to the
   community. Evidently, it is a grand challenge for machine learning
   turning from close environment to open environment. It becomes even more
   challenging since, in various big data tasks, data are usually
   accumulated with time, like streams, while it is hard to train the
   machine learning model after collecting all data as in conventional
   studies. This article briefly introduces some advances in this line of
   research, focusing on techniques concerning emerging new classes,
   decremental/incremental features, changing data distributions and varied
   learning objectives, and discusses some theoretical issues.
   This article briefly introduces Open Environment Machine Learning, where
   important factors of the machine learning process are subject to change,
   as occurring in many practical tasks.},
DOI = {10.1093/nsr/nwac123},
EarlyAccessDate = {AUG 2022},
ISSN = {2095-5138},
EISSN = {2053-714X},
ResearcherID-Numbers = {Zhihua, Zhou/B-7290-2009},
Unique-ID = {WOS:000838252900001},
}

@article{ WOS:000852243600006,
Author = {Zhou, Yueying and Huang, Shuo and Xu, Ziming and Wang, Pengpai and Wu,
   Xia and Zhang, Daoqiang},
Title = {Cognitive Workload Recognition Using EEG Signals and Machine Learning: A
   Review},
Journal = {IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS},
Year = {2022},
Volume = {14},
Number = {3},
Pages = {799-818},
Month = {SEP},
Abstract = {Machine learning and its subfield deep learning techniques provide
   opportunities for the development of operator mental state monitoring,
   especially for cognitive workload recognition using electroencephalogram
   (EEG) signals. Although a variety of machine learning methods have been
   proposed for recognizing cognitive workload via EEG recently, there does
   not yet exist a review that covers in-depth the application of machine
   learning methods. To alleviate this gap, in this article, we survey
   cognitive workload and machine learning literature to identify the
   approaches and highlight the primary advances. To be specific, we first
   introduce the concepts of cognitive workload and machine learning. Then,
   we discuss the steps of classical machine learning for cognitive
   workload recognition from the following aspects, i.e., EEG data
   preprocessing, feature extraction and selection, classification method,
   and evaluation methods. Further, we review the commonly used deep
   learning methods for this domain. Finally, we expound on the open
   problem and future outlooks.},
DOI = {10.1109/TCDS.2021.3090217},
ISSN = {2379-8920},
EISSN = {2379-8939},
ResearcherID-Numbers = {Zhang, Daoqiang/D-3754-2011
   Zhou, ying/HZJ-5073-2023},
ORCID-Numbers = {Zhou, Yueying/0000-0003-0971-9428
   Huang, Shuo/0000-0002-3267-1816
   },
Unique-ID = {WOS:000852243600006},
}

@article{ WOS:000816020800006,
Author = {Sullivan, Emily},
Title = {Understanding from Machine Learning Models},
Journal = {BRITISH JOURNAL FOR THE PHILOSOPHY OF SCIENCE},
Year = {2022},
Volume = {73},
Number = {1},
Pages = {109-133},
Month = {MAR 1},
Abstract = {Simple idealized models seem to provide more understanding than opaque,
   complex, and hyper-realistic models. However, an increasing number of
   scientists are going in the opposite direction by utilizing opaque
   machine learning models to make predictions and draw inferences,
   suggesting that scientists are opting for models that have less
   potential for understanding. Are scientists trading understanding for
   some other epistemic or pragmatic good when they choose a machine
   learning model? Or are the assumptions behind why minimal models provide
   understanding misguided? In this article, using the case of deep neural
   networks, I argue that it is not the complexity or black box nature of a
   model that limits how much understanding the model provides. Instead, it
   is a lack of scientific and empirical evidence supporting the link that
   connects a model to the target phenomenon that primarily prohibits
   understanding.},
DOI = {10.1093/bjps/axz035},
ISSN = {0007-0882},
EISSN = {1464-3537},
ResearcherID-Numbers = {Sullivan, Emily/AAA-5220-2021},
ORCID-Numbers = {Sullivan, Emily/0000-0002-2073-5384
   },
Unique-ID = {WOS:000816020800006},
}

@article{ WOS:000819852500009,
Author = {Charilaou, Paris and Battat, Robert},
Title = {Machine learning models and over-fitting considerations},
Journal = {WORLD JOURNAL OF GASTROENTEROLOGY},
Year = {2022},
Volume = {28},
Number = {5},
Pages = {605-607},
Month = {FEB 7},
Abstract = {Machine learning models may outperform traditional statistical
   regression algorithms for predicting clinical outcomes. Proper
   validation of building such models and tuning their underlying
   algorithms is necessary to avoid over-fitting and poor generalizability,
   which smaller datasets can be more prone to. In an effort to educate
   readers interested in artificial intelligence and model-building based
   on machine-learning algorithms, we outline important details on
   cross-validation techniques that can enhance the performance and
   generalizability of such models.},
DOI = {10.3748/wjg.v28.i5.605},
ISSN = {1007-9327},
EISSN = {2219-2840},
ResearcherID-Numbers = {Charilaou, Paris/G-3631-2017
   Battat, Robert/LCE-5324-2024},
Unique-ID = {WOS:000819852500009},
}

@article{ WOS:000602863200001,
Author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
Title = {Machine learning for combinatorial optimization: A methodological tour
   d'horizon},
Journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
Year = {2021},
Volume = {290},
Number = {2},
Pages = {405-421},
Month = {APR 16},
Abstract = {This paper surveys the recent attempts, both from the machine learning
   and operations research communities, at leveraging machine learning to
   solve combinatorial optimization problems. Given the hard nature of
   these problems, state-of-the-art algorithms rely on handcrafted
   heuristics for making decisions that are otherwise too expensive to
   compute or mathematically not well defined. Thus, machine learning looks
   like a natural candidate to make such decisions in a more principled and
   optimized way. We advocate for pushing further the integration of
   machine learning and combinatorial optimization and detail a methodology
   to do so. A main point of the paper is seeing generic optimization
   problems as data points and inquiring what is the relevant distribution
   of problems to use for learning on a given task. (C) 2020 Elsevier B.V.
   All rights reserved.},
DOI = {10.1016/j.ejor.2020.07.063},
ISSN = {0377-2217},
EISSN = {1872-6860},
Unique-ID = {WOS:000602863200001},
}

@article{ WOS:000868715700001,
Author = {Mahadevkar, V, Supriya and Khemani, Bharti and Patil, Shruti and
   Kotecha, Ketan and Vora, Deepali R. and Abraham, Ajith and Gabralla,
   Lubna Abdelkareim},
Title = {A Review on Machine Learning Styles in Computer Vision-Techniques and
   Future Directions},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {107293-107329},
Abstract = {Computer applications have considerably shifted from single data
   processing to machine learning in recent years due to the accessibility
   and availability of massive volumes of data obtained through the
   internet and various sources. Machine learning is automating human
   assistance by training an algorithm on relevant data. Supervised,
   Unsupervised, and Reinforcement Learning are the three fundamental
   categories of machine learning techniques. In this paper, we have
   discussed the different learning styles used in the field of Computer
   vision, Deep Learning, Neural networks, and machine learning. Some of
   the most recent applications of machine learning in computer vision
   include object identification, object classification, and extracting
   usable information from images, graphic documents, and videos. Some
   machine learning techniques frequently include zero-shot learning,
   active learning, contrastive learning, self-supervised learning,
   life-long learning, semi-supervised learning, ensemble learning,
   sequential learning, and multi-view learning used in computer vision
   until now. There is a lack of systematic reviews about all learning
   styles. This paper presents literature analysis of how different machine
   learning styles evolved in the field of Artificial Intelligence (AI) for
   computer vision. This research examines and evaluates machine learning
   applications in computer vision and future forecasting. This paper will
   be helpful for researchers working with learning styles as it gives a
   deep insight into future directions.},
DOI = {10.1109/ACCESS.2022.3209825},
ISSN = {2169-3536},
ResearcherID-Numbers = {Kotecha, Ketan/U-3927-2017
   Khemani, Bharti/GOV-3994-2022
   Patil, Shruti G/AAL-7179-2021
   Kotecha, K/U-3927-2017
   Patil, Shruti/AAL-7179-2021
   GABRALLA, LUBNA/GQQ-6827-2022
   Vora, Deepali/AAB-8430-2019
   Abraham, Ajith/A-1416-2008},
ORCID-Numbers = {Gabralla, Lubna/0000-0002-6910-6861
   Patil, Shruti G/0000-0002-4903-1540
   Kotecha, K/0000-0003-2653-3780
   Vora, Deepali/0000-0003-3969-9800
   },
Unique-ID = {WOS:000868715700001},
}

@article{ WOS:000794033200007,
Author = {van Giffen, Benjamin and Herhausen, Dennis and Fahse, Tobias},
Title = {Overcoming the pitfalls and perils of algorithms: A classification of
   machine learning biases and mitigation methods},
Journal = {JOURNAL OF BUSINESS RESEARCH},
Year = {2022},
Volume = {144},
Pages = {93-106},
Month = {MAY},
Abstract = {Over the last decade, the importance of machine learning increased
   dramatically in business and marketing. However, when machine learning
   is used for decision-making, bias rooted in unrepresentative datasets,
   inade-quate models, weak algorithm designs, or human stereotypes can
   lead to low performance and unfair decisions, resulting in financial,
   social, and reputational losses. This paper offers a systematic,
   interdisciplinary literature review of machine learning biases as well
   as methods to avoid and mitigate these biases. We identified eight
   distinct machine learning biases, summarized these biases in the
   cross-industry standard process for data mining to account for all
   phases of machine learning projects, and outline twenty-four mitigation
   methods. We further contextualize these biases in a real-world case
   study and illustrate adequate mitigation strategies. These insights
   synthesize the literature on machine learning biases in a concise manner
   and point to the importance of human judgment for machine learning
   algorithms.},
DOI = {10.1016/j.jbusres.2022.01.076},
EarlyAccessDate = {FEB 2022},
ISSN = {0148-2963},
EISSN = {1873-7978},
ResearcherID-Numbers = {Herhausen, Dennis/AAY-2492-2021},
ORCID-Numbers = {Herhausen, Dennis/0000-0002-4335-1703
   },
Unique-ID = {WOS:000794033200007},
}

@article{ WOS:000913331400001,
Author = {Fang, Jiheng and Xie, Ming and He, Xingqun and Zhang, Jiming and Hu,
   Jieqiong and Chen, Yongtai and Yang, Youcai and Jin, Qinglin},
Title = {Machine learning accelerates the materials discovery},
Journal = {MATERIALS TODAY COMMUNICATIONS},
Year = {2022},
Volume = {33},
Month = {DEC},
Abstract = {As the big data generated by the development of modern experiments and
   computing technology becomes more and more accessible, the material
   design method based on machine learning (ML) has opened a new paradigm
   for materials science research. With its ability to automatically solve
   complex tasks, machine learning is being used as a new method to help
   discover the relevance of materials, understand materials' properties,
   and accelerate the discovery of materials. This paper first introduces
   the general process of machine learning in materials science. Secondly,
   the applications of machine learning in material properties prediction,
   classification and identification, auxiliary micro-scale
   characterization, phase transformation research and phase diagram
   construction, process optimization, service behavior evaluation,
   accelerating the development of computational simulation technology,
   multi-objective optimization and inverse design of materials are
   reviewed. Finally, we discuss the main challenges and possible solutions
   in machine learning, and predict the potential research directions.},
DOI = {10.1016/j.mtcomm.2022.104900},
EarlyAccessDate = {NOV 2022},
Article-Number = {104900},
EISSN = {2352-4928},
Unique-ID = {WOS:000913331400001},
}

@article{ WOS:000737778100001,
Author = {Brunton, Steven L.},
Title = {Applying machine learning to study fluid mechanics},
Journal = {ACTA MECHANICA SINICA},
Year = {2021},
Volume = {37},
Number = {12},
Pages = {1718-1726},
Month = {DEC},
Abstract = {This paper provides a short overview of how to use machine learning to
   build data-driven models in fluid mechanics. The process of machine
   learning is broken down into five stages: (1) formulating a problem to
   model, (2) collecting and curating training data to inform the model,
   (3) choosing an architecture with which to represent the model, (4)
   designing a loss function to assess the performance of the model, and
   (5) selecting and implementing an optimization algorithm to train the
   model. At each stage, we discuss how prior physical knowledge may be
   embedding into the process, with specific examples from the field of
   fluid mechanics.},
DOI = {10.1007/s10409-021-01143-6},
EarlyAccessDate = {JAN 2022},
ISSN = {0567-7718},
EISSN = {1614-3116},
ResearcherID-Numbers = {Brunton, Steven/AAG-3871-2019},
Unique-ID = {WOS:000737778100001},
}

@article{ WOS:000761920900004,
Author = {Houssein, Essam H. and Abohashima, Zainab and Elhoseny, Mohamed and
   Mohamed, Waleed M.},
Title = {Machine learning in the quantum realm: The state-of-the-art, challenges,
   and future vision},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2022},
Volume = {194},
Month = {MAY 15},
Abstract = {Machine learning has become a ubiquitous and effective technique for
   data processing and classification. Furthermore, due to the superiority
   and progress of quantum computing in many areas (e.g., cryptography,
   machine learning, healthcare), a combination of classical machine
   learning and quantum information processing has established a new field,
   called, quantum machine learning. One of the most frequently used
   applications of quantum computing is machine learning. This paper aims
   to present a comprehensive review of state-of-the-art advances in
   quantum machine learning. Besides, this paper outlines recent works on
   different architectures of quantum deep learning, and illustrates
   classification tasks in the quantum domain as well as encoding methods
   and quantum subroutines. Furthermore, this paper examines how the
   concept of quantum computing enhances classical machine learning. Two
   methods for improving the performance of classical machine learning are
   presented. Finally, this work provides a general review of challenges
   and the future vision of quantum machine learning.},
DOI = {10.1016/j.eswa.2022.116512},
EarlyAccessDate = {JAN 2022},
Article-Number = {116512},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {Houssein, Essam/C-8941-2016
   Halim Houssein, Essam/C-8941-2016
   Elhoseny, Mohamed/Q-5591-2017
   Mohamed, Assoc. prof. Waleed/ACS-6284-2022},
ORCID-Numbers = {Halim Houssein, Essam/0000-0002-8127-7233
   Makram Mohamed, Waleed/0000-0002-2516-5800
   Elhoseny, Mohamed/0000-0001-6347-8368
   },
Unique-ID = {WOS:000761920900004},
}

@article{ WOS:001156335200001,
Author = {Shen, Zhong-Hui and Liu, Han-Xing and Shen, Yang and Hu, Jia-Mian and
   Chen, Long-Qing and Nan, Ce-Wen},
Title = {Machine learning in energy storage materials},
Journal = {INTERDISCIPLINARY MATERIALS},
Year = {2022},
Volume = {1},
Number = {2},
Pages = {175-195},
Month = {APR},
Abstract = {With its extremely strong capability of data analysis, machine learning
   has shown versatile potential in the revolution of the materials
   research paradigm. Here, taking dielectric capacitors and lithium-ion
   batteries as two representative examples, we review substantial advances
   of machine learning in the research and development of energy storage
   materials. First, a thorough discussion of the machine learning
   framework in materials science is presented. Then, we summarize the
   applications of machine learning from three aspects, including
   discovering and designing novel materials, enriching theoretical
   simulations, and assisting experimentation and characterization.
   Finally, a brief outlook is highlighted to spark more insights on the
   innovative implementation of machine learning in materials science.},
DOI = {10.1002/idm2.12020},
ISSN = {2767-4401},
EISSN = {2767-441X},
ResearcherID-Numbers = {Shen, Zhonghui/AAX-7508-2021
   Chen, Long-Qing/I-7536-2012},
ORCID-Numbers = {Chen, Long-Qing/0000-0003-3359-3781},
Unique-ID = {WOS:001156335200001},
}

@article{ WOS:000789003800004,
Author = {Wang, Meng and Fu, Weijie and He, Xiangnan and Hao, Shijie and Wu,
   Xindong},
Title = {A Survey on Large-Scale Machine Learning},
Journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
Year = {2022},
Volume = {34},
Number = {6},
Pages = {2574-2594},
Month = {JUN 1},
Abstract = {Machine learning can provide deep insights into data, allowing machines
   to make high-quality predictions and having been widely used in
   real-world applications, such as text mining, visual classification, and
   recommender systems. However, most sophisticated machine learning
   approaches suffer from huge time costs when operating on large-scale
   data. This issue calls for the need of Large-scale Machine Learning
   (LML), which aims to learn patterns from big data with comparable
   performance efficiently. In this paper, we offer a systematic survey on
   existing LML methods to provide a blueprint for the future developments
   of this area. We first divide these LML methods according to the ways of
   improving the scalability: 1) model simplification on computational
   complexities, 2) optimization approximation on computational efficiency,
   and 3) computation parallelism on computational capabilities. Then we
   categorize the methods in each perspective according to their targeted
   scenarios and introduce representative methods in line with intrinsic
   strategies. Lastly, we analyze their limitations and discuss potential
   directions as well as open issues that are promising to address in the
   future.},
DOI = {10.1109/TKDE.2020.3015777},
ISSN = {1041-4347},
EISSN = {1558-2191},
ResearcherID-Numbers = {Fu, Weijie/AAB-1036-2019
   Wang, Meng/ITR-8699-2023
   He, Xiangnan/G-3986-2011
   Wu, Xindong/AAB-6713-2022},
Unique-ID = {WOS:000789003800004},
}

@article{ WOS:000781177700001,
Author = {Thiyagalingam, Jeyan and Shankar, Mallikarjun and Fox, Geoffrey and Hey,
   Tony},
Title = {Scientific machine learning benchmarks},
Journal = {NATURE REVIEWS PHYSICS},
Year = {2022},
Volume = {4},
Number = {6},
Pages = {413-420},
Month = {JUN},
Abstract = {Finding the most appropriate machine learning algorithm for the analysis
   of any given scientific dataset is currently challenging, but new
   machine learning benchmarks for science are being developed to help.
   Deep learning has transformed the use of machine learning technologies
   for the analysis of large experimental datasets. In science, such
   datasets are typically generated by large-scale experimental facilities,
   and machine learning focuses on the identification of patterns, trends
   and anomalies to extract meaningful scientific insights from the data.
   In upcoming experimental facilities, such as the Extreme Photonics
   Application Centre (EPAC) in the UK or the international Square
   Kilometre Array (SKA), the rate of data generation and the scale of data
   volumes will increasingly require the use of more automated data
   analysis. However, at present, identifying the most appropriate machine
   learning algorithm for the analysis of any given scientific dataset is a
   challenge due to the potential applicability of many different machine
   learning frameworks, computer architectures and machine learning models.
   Historically, for modelling and simulation on high-performance computing
   systems, these issues have been addressed through benchmarking computer
   applications, algorithms and architectures. Extending such a
   benchmarking approach and identifying metrics for the application of
   machine learning methods to open, curated scientific datasets is a new
   challenge for both scientists and computer scientists. Here, we
   introduce the concept of machine learning benchmarks for science and
   review existing approaches. As an example, we describe the SciMLBench
   suite of scientific machine learning benchmarks.},
DOI = {10.1038/s42254-022-00441-7},
EarlyAccessDate = {APR 2022},
EISSN = {2522-5820},
ResearcherID-Numbers = {Shankar, Mallikarjun/N-4400-2015
   },
ORCID-Numbers = {Fox, Geoffrey/0000-0003-1017-1391
   Thiyagalingam, Jeyan/0000-0002-2167-1343
   Hey, Tony/0000-0001-6782-3691},
Unique-ID = {WOS:000781177700001},
}

@article{ WOS:000754182500001,
Author = {Xu, Pengcheng and Chen, Huimin and Li, Minjie and Lu, Wencong},
Title = {New Opportunity: Machine Learning for Polymer Materials Design and
   Discovery},
Journal = {ADVANCED THEORY AND SIMULATIONS},
Year = {2022},
Volume = {5},
Number = {5},
Month = {MAY},
Abstract = {Under the guidance of the material genome initiative (MGI), the use of
   data-driven methods to discover new materials has become an innovation
   of materials science. The polymer materials have been one of the most
   important parts in materials science for the excellent physical and
   chemical properties as well as corresponding complex structures. Machine
   learning, as the core of data-driven methods, has taken an important
   place in polymer materials design and discovery. In this review, the
   authors have introduced the applications of machine learning in the
   design and discovery of polymer materials. The development tendency of
   published papers about machine learning in polymer materials, the
   commonly used algorithms, the polymer descriptors, the workflow of
   machine learning in polymer materials, and recent progresses of machine
   learning in materials are summarized. Then, the detail of how to use
   machine learning to assist design and discovery of polymer materials is
   fully discussed combined with two cases. Finally, the opportunities and
   challenges on the future development prospects of machine learning in
   the field of polymer materials are proposed.},
DOI = {10.1002/adts.202100565},
EarlyAccessDate = {FEB 2022},
Article-Number = {2100565},
EISSN = {2513-0390},
Unique-ID = {WOS:000754182500001},
}

@article{ WOS:000884152000002,
Author = {Eckhardt, Christina M. and Madjarova, Sophia J. and Williams, Riley J.
   and Ollivier, Mattheu and Karlsson, Jon and Pareek, Ayoosh and
   Nwachukwu, Benedict U.},
Title = {Unsupervised machine learning methods and emerging applications in
   healthcare},
Journal = {KNEE SURGERY SPORTS TRAUMATOLOGY ARTHROSCOPY},
Year = {2023},
Volume = {31},
Number = {2},
Pages = {376-381},
Month = {FEB},
Abstract = {Unsupervised machine learning methods are important analytical tools
   that can facilitate the analysis and interpretation of high-dimensional
   data. Unsupervised machine learning methods identify latent patterns and
   hidden structures in high-dimensional data and can help simplify complex
   datasets. This article provides an overview of key unsupervised machine
   learning techniques including K-means clustering, hierarchical
   clustering, principal component analysis, and factor analysis. With a
   deeper understanding of these analytical tools, unsupervised machine
   learning methods can be incorporated into health sciences research to
   identify novel risk factors, improve prevention strategies, and
   facilitate delivery of personalized therapies and targeted patient care.},
DOI = {10.1007/s00167-022-07233-7},
EarlyAccessDate = {NOV 2022},
ISSN = {0942-2056},
EISSN = {1433-7347},
Unique-ID = {WOS:000884152000002},
}

@article{ WOS:000479252200001,
Author = {Schmidt, Jonathan and Marques, Mario R. G. and Botti, Silvana and
   Marques, Miguel A. L.},
Title = {Recent advances and applications of machine learning in solid-state
   materials science},
Journal = {NPJ COMPUTATIONAL MATERIALS},
Year = {2019},
Volume = {5},
Month = {AUG 8},
Abstract = {One of the most exciting tools that have entered the material science
   toolbox in recent years is machine learning. This collection of
   statistical methods has already proved to be capable of considerably
   speeding up both fundamental and applied research. At present, we are
   witnessing an explosion of works that develop and apply machine learning
   to solid-state systems. We provide a comprehensive overview and analysis
   of the most recent research in this topic. As a starting point, we
   introduce machine learning principles, algorithms, descriptors, and
   databases in materials science. We continue with the description of
   different machine learning approaches for the discovery of stable
   materials and the prediction of their crystal structure. Then we discuss
   research in numerous quantitative structure-property relationships and
   various approaches for the replacement of first-principle methods by
   machine learning. We review how active learning and surrogate-based
   optimization can be applied to improve the rational design process and
   related examples of applications. Two major questions are always the
   interpretability of and the physical understanding gained from machine
   learning models. We consider therefore the different facets of
   interpretability and their importance in materials science. Finally, we
   propose solutions and future research paths for various challenges in
   computational materials science.},
DOI = {10.1038/s41524-019-0221-0},
Article-Number = {83},
EISSN = {2057-3960},
ResearcherID-Numbers = {Marques, Miguel/AAO-9914-2021
   Botti, Silvana/NQF-0975-2025
   Botti, Silvana/A-6241-2010
   Marques, Miguel/A-7807-2008},
ORCID-Numbers = {Schmidt, Jonathan/0000-0001-5685-6404
   Marques, Miguel A., L./0000-0003-0170-8222
   Botti, Silvana/0000-0002-4920-2370
   },
Unique-ID = {WOS:000479252200001},
}

@article{ WOS:000761186500001,
Author = {Halbouni, Asmaa and Gunawan, Teddy Surya and Habaebi, Mohamed Hadi and
   Halbouni, Murad and Kartiwi, Mira and Ahmad, Robiah},
Title = {Machine Learning and Deep Learning Approaches for CyberSecurity: A
   Review},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {19572-19585},
Abstract = {The rapid evolution and growth of the internet through the last decades
   led to more concern about cyber-attacks that are continuously increasing
   and changing. As a result, an effective intrusion detection system was
   required to protect data, and the discovery of artificial intelligence's
   sub-fields, machine learning, and deep learning, was one of the most
   successful ways to address this problem. This paper reviewed intrusion
   detection systems and discussed what types of learning algorithms
   machine learning and deep learning are using to protect data from
   malicious behavior. It discusses recent machine learning and deep
   learning work with various network implementations, applications,
   algorithms, learning approaches, and datasets to develop an operational
   intrusion detection system.},
DOI = {10.1109/ACCESS.2022.3151248},
ISSN = {2169-3536},
ResearcherID-Numbers = {Halbouni, Asmaa/HKP-0003-2023
   Habaebi, Mohamed/P-2128-2017
   Habaebi, Mohamed/F-2460-2011
   Gunawan, Teddy/E-9644-2011},
ORCID-Numbers = {Ahmad, Robiah/0000-0001-6461-9564
   Habaebi, Mohamed/0000-0002-2263-0850
   },
Unique-ID = {WOS:000761186500001},
}

@article{ WOS:000870821400025,
Author = {Lavin, Alexander and Gilligan-Lee, Ciaran M. and Visnjic, Alessya and
   Ganju, Siddha and Newman, Dava and Ganguly, Sujoy and Lange, Danny and
   Baydin, Atilim Gunes and Sharma, Amit and Gibson, Adam and Zheng,
   Stephan and Xing, Eric P. and Mattmann, Chris and Parr, James and Gal,
   Yarin},
Title = {Technology readiness levels for machine learning systems},
Journal = {NATURE COMMUNICATIONS},
Year = {2022},
Volume = {13},
Number = {1},
Month = {OCT 20},
Abstract = {The development of machine learning systems has to ensure their
   robustness and reliability. The authors introduce a framework that
   defines a principled process of machine learning system formation, from
   research to production, for various domains and data scenarios.
   The development and deployment of machine learning systems can be
   executed easily with modern tools, but the process is typically rushed
   and means-to-an-end. Lack of diligence can lead to technical debt, scope
   creep and misaligned objectives, model misuse and failures, and
   expensive consequences. Engineering systems, on the other hand, follow
   well-defined processes and testing standards to streamline development
   for high-quality, reliable results. The extreme is spacecraft systems,
   with mission critical measures and robustness throughout the process.
   Drawing on experience in both spacecraft engineering and machine
   learning (research through product across domain areas), we've developed
   a proven systems engineering approach for machine learning and
   artificial intelligence: the Machine Learning Technology Readiness
   Levels framework defines a principled process to ensure robust,
   reliable, and responsible systems while being streamlined for machine
   learning workflows, including key distinctions from traditional software
   engineering, and a lingua franca for people across teams and
   organizations to work collaboratively on machine learning and artificial
   intelligence technologies. Here we describe the framework and elucidate
   with use-cases from physics research to computer vision apps to medical
   diagnostics.},
DOI = {10.1038/s41467-022-33128-9},
Article-Number = {6039},
EISSN = {2041-1723},
ResearcherID-Numbers = {Baydin, Atilim Gunes/M-7029-2014
   },
ORCID-Numbers = {Ganguly, Sujoy/0000-0002-2889-8188
   Lavin, Alexander/0000-0003-3422-7820
   Newman, Dava/0000-0001-6190-348X},
Unique-ID = {WOS:000870821400025},
}

@article{ WOS:000765501200001,
Author = {Chan, Cheuk Hei and Sun, Mingzi and Huang, Bolong},
Title = {Application of machine learning for advanced material prediction and
   design},
Journal = {ECOMAT},
Year = {2022},
Volume = {4},
Number = {4},
Month = {JUL},
Abstract = {In material science, traditional experimental and computational
   approaches require investing enormous time and resources, and the
   experimental conditions limit the experiments. Sometimes, traditional
   approaches may not yield satisfactory results for the desired purpose.
   Therefore, it is essential to develop a new approach to accelerate
   experimental progress and avoid unnecessary wasting of time and
   resources. As a data-driven method, machine learning provides reliable
   and accurate performance to solve problems in material science. This
   review first outlines the fundamental information of machine learning.
   It continues with the research concerning the prediction of various
   properties of materials by machine learning. Then it discusses the
   methods for the discovery of new materials and the prediction of their
   structural information. Finally, we summarize other applications of
   machine learning in material science. This review will be beneficial for
   future application of machine learning in more material science
   research.},
DOI = {10.1002/eom2.12194},
EarlyAccessDate = {MAR 2022},
Article-Number = {e12194},
EISSN = {2567-3173},
ResearcherID-Numbers = {Sun, Mingzi/M-5617-2017
   Huang, Bolong/AAG-5165-2019},
ORCID-Numbers = {Huang, Bolong/0000-0002-2526-2002},
Unique-ID = {WOS:000765501200001},
}

@article{ WOS:000674857200001,
Author = {Belle, Vaishak and Papantonis, Ioannis},
Title = {Principles and Practice of Explainable Machine Learning},
Journal = {FRONTIERS IN BIG DATA},
Year = {2021},
Volume = {4},
Month = {JUL 1},
Abstract = {Artificial intelligence (AI) provides many opportunities to improve
   private and public life. Discovering patterns and structures in large
   troves of data in an automated manner is a core component of data
   science, and currently drives applications in diverse areas such as
   computational biology, law and finance. However, such a highly positive
   impact is coupled with a significant challenge: how do we understand the
   decisions suggested by these systems in order that we can trust them? In
   this report, we focus specifically on data-driven methods-machine
   learning (ML) and pattern recognition models in particular-so as to
   survey and distill the results and observations from the literature. The
   purpose of this report can be especially appreciated by noting that ML
   models are increasingly deployed in a wide range of businesses. However,
   with the increasing prevalence and complexity of methods, business
   stakeholders in the very least have a growing number of concerns about
   the drawbacks of models, data-specific biases, and so on. Analogously,
   data science practitioners are often not aware about approaches emerging
   from the academic literature or may struggle to appreciate the
   differences between different methods, so end up using industry
   standards such as SHAP. Here, we have undertaken a survey to help
   industry practitioners (but also data scientists more broadly)
   understand the field of explainable machine learning better and apply
   the right tools. Our latter sections build a narrative around a putative
   data scientist, and discuss how she might go about explaining her models
   by asking the right questions. From an organization viewpoint, after
   motivating the area broadly, we discuss the main developments, including
   the principles that allow us to study transparent models vs. opaque
   models, as well as model-specific or model-agnostic post-hoc
   explainability approaches. We also briefly reflect on deep learning
   models, and conclude with a discussion about future research directions.},
DOI = {10.3389/fdata.2021.688969},
Article-Number = {688969},
EISSN = {2624-909X},
ORCID-Numbers = {Papantonis, Ioannis/0000-0003-4282-5820},
Unique-ID = {WOS:000674857200001},
}

@article{ WOS:000599821400007,
Author = {Sun, Han and Burton, Henry V. and Huang, Honglan},
Title = {Machine learning applications for building structural design and
   performance assessment: State-of-the-art review},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2021},
Volume = {33},
Month = {JAN},
Abstract = {Machine learning models have been shown to be useful for predicting and
   assessing structural performance, identifying structural condition and
   informing preemptive and recovery decisions by extracting patterns from
   data collected via various sources and media. This paper presents a
   review of the historical development and recent advances in the
   application of machine learning to the area of building structural
   design and performance assessment. To this end, an overview of machine
   learning theory and the most relevant algorithms is provided with the
   goal of identifying problems suitable for machine learning and the
   appropriate models to use. The machine learning applications in building
   structural design and performance assessment are then reviewed in four
   main categories: (1) predicting structural response and performance, (2)
   interpreting experimental data and formulating models to predict
   component-level structural properties, (3) information retrieval using
   images and written text and (4) recognizing patterns in structural
   health monitoring data. The challenges of bringing machine learning into
   structural engineering practice are identified, and future research
   opportunities are discussed.},
DOI = {10.1016/j.jobe.2020.101816},
Article-Number = {101816},
EISSN = {2352-7102},
ResearcherID-Numbers = {Huang, Honglan/GRR-2513-2022},
Unique-ID = {WOS:000599821400007},
}

@article{ WOS:000649545300034,
Author = {Zhang, Liang and Wen, Jin and Li, Yanfei and Chen, Jianli and Ye,
   Yunyang and Fu, Yangyang and Livingood, William},
Title = {A review of machine learning in building load prediction},
Journal = {APPLIED ENERGY},
Year = {2021},
Volume = {285},
Month = {MAR 1},
Abstract = {The surge of machine learning and increasing data accessibility in
   buildings provide great opportunities for applying machine learning to
   building energy system modeling and analysis. Building load prediction
   is one of the most critical components for many building control and
   analytics activities, as well as grid-interactive and energy efficiency
   building operation. While a large number of research papers exist on the
   topic of machine-learning-based building load prediction, a
   comprehensive review from the perspective of machine learning is
   missing. In this paper, we review the application of machine learning
   techniques in building load prediction under the organization and logic
   of the machine learning, which is to perform tasks T using Performance
   measure P and based on learning from Experience E.
   Firstly, we review the applications of building load prediction model
   (task T). Then, we review the modeling algorithms that improve machine
   learning performance and accuracy (performance P). Throughout the
   papers, we also review the literature from the data perspective for
   modeling (experience E), including data engineering from the sensor
   level to data level, pre-processing, feature extraction and selection.
   Finally, we conclude with a discussion of well-studied and relatively
   unexplored fields for future research reference. We also identify the
   gaps in current machine learning application and predict for future
   trends and development.},
DOI = {10.1016/j.apenergy.2021.116452},
EarlyAccessDate = {JAN 2021},
Article-Number = {116452},
ISSN = {0306-2619},
EISSN = {1872-9118},
ResearcherID-Numbers = {Chen, Jianli/KUD-8259-2024
   Wen, Jin/E-6986-2013
   Ye, Yunyang/AAL-6278-2021
   Fu, Yangyang/AAP-8730-2020
   Li, Yanfei/AAE-2746-2020
   },
ORCID-Numbers = {Zhang, Liang/0000-0001-9884-5199
   Wen, Jin/0000-0002-1964-8574
   Fu, Yangyang/0000-0002-9507-1420
   Ye, Yunyang/0000-0001-7726-5769},
Unique-ID = {WOS:000649545300034},
}

@article{ WOS:000606751200009,
Author = {Wang, Pin and Fan, En and Wang, Peng},
Title = {Comparative analysis of image classification algorithms based on
   traditional machine learning and deep learning},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2021},
Volume = {141},
Pages = {61-67},
Month = {JAN},
Abstract = {Image classification is a hot research topic in today's society and an
   important direction in the field of image processing research. SVM is a
   very powerful classification model in machine learning. CNN is a type of
   feedforward neural network that includes convolution calculation and has
   a deep structure. It is one of the representative algorithms of deep
   learning. Taking SVM and CNN as examples, this paper compares and
   analyzes the traditional machine learning and deep learning image
   classification algorithms. This study found that when using a large
   sample mnist dataset, the accuracy of SVM is 0.88 and the accuracy of
   CNN is 0.98; when using a small sample COREL1000 dataset, the accuracy
   of SVM is 0.86 and the accuracy of CNN is 0.83. The experimental results
   in this paper show that traditional machine learning has a better
   solution effect on small sample data sets, and deep learning framework
   has higher recognition accuracy on large sample data sets. (C) 2020
   Published by Elsevier B.V.},
DOI = {10.1016/j.patrec.2020.07.042},
ISSN = {0167-8655},
EISSN = {1872-7344},
Unique-ID = {WOS:000606751200009},
}

@article{ WOS:000663421300008,
Author = {Reel, Parminder S. and Reel, Smarti and Pearson, Ewan and Trucco,
   Emanuele and Jefferson, Emily},
Title = {Using machine learning approaches for multi-omics data analysis: A
   review},
Journal = {BIOTECHNOLOGY ADVANCES},
Year = {2021},
Volume = {49},
Month = {JUL-AUG},
Abstract = {With the development of modern high-throughput omic measurement
   platforms, it has become essential for biomedical studies to undertake
   an integrative (combined) approach to fully utilise these data to gain
   insights into biological systems. Data from various omics sources such
   as genetics, proteomics, and metabolomics can be integrated to unravel
   the intricate working of systems biology using machine learning-based
   predictive algorithms. Machine learning methods offer novel techniques
   to integrate and analyse the various omics data enabling the discovery
   of new biomarkers. These biomarkers have the potential to help in
   accurate disease prediction, patient stratification and delivery of
   precision medicine. This review paper explores different integrative
   machine learning methods which have been used to provide an in-depth
   understanding of biological systems during normal physiological
   functioning and in the presence of a disease. It provides insight and
   recommendations for interdisciplinary professionals who envisage
   employing machine learning skills in multi-omics studies.},
DOI = {10.1016/j.biotechadv.2021.107739},
EarlyAccessDate = {APR 2021},
Article-Number = {107739},
ISSN = {0734-9750},
EISSN = {1873-1899},
ResearcherID-Numbers = {Pearson, Ewan/JFK-7030-2023
   Reel, Parminder Singh/AFK-2737-2022
   },
ORCID-Numbers = {Pearson, Ewan/0000-0001-9237-8585
   Reel, Parminder Singh/0000-0001-7796-1457
   Jefferson, Emily/0000-0003-2992-7582
   Trucco, Emanuele/0000-0002-5055-0794
   Reel, Smarti/0000-0002-6092-9175},
Unique-ID = {WOS:000663421300008},
}

@article{ WOS:000675035000001,
Author = {Hart, Gus L. W. and Mueller, Tim and Toher, Cormac and Curtarolo,
   Stefano},
Title = {Machine learning for alloys},
Journal = {NATURE REVIEWS MATERIALS},
Year = {2021},
Volume = {6},
Number = {8},
Pages = {730-755},
Month = {AUG},
Abstract = {Alloy modelling has a history of machine-learning-like approaches,
   preceding the tide of data-science-inspired work. The dawn of
   computational databases has made the integration of analysis, prediction
   and discovery the key theme in accelerated alloy research. Advances in
   machine-learning methods and enhanced data generation have created a
   fertile ground for computational materials science. Pairing machine
   learning and alloys has proven to be particularly instrumental in
   pushing progress in a wide variety of materials, including metallic
   glasses, high-entropy alloys, shape-memory alloys, magnets, superalloys,
   catalysts and structural materials. This Review examines the present
   state of machine-learning-driven alloy research, discusses the
   approaches and applications in the field and summarizes theoretical
   predictions and experimental validations. We foresee that the
   partnership between machine learning and alloys will lead to the design
   of new and improved systems.
   Machine learning is enabling a metallurgical renaissance. This Review
   discusses recent progress in representations, descriptors and
   interatomic potentials, overviewing metallic glasses, high-entropy
   alloys, superalloys and shape-memory alloys, magnets and catalysts, and
   the prediction of mechanical and thermal properties.},
DOI = {10.1038/s41578-021-00340-w},
EarlyAccessDate = {JUL 2021},
ISSN = {2058-8437},
ResearcherID-Numbers = {Toher, Cormac/D-9920-2011
   Hart, Gus/B-6788-2011
   Mueller, Tim/G-3435-2011
   Curtarolo, Stefano/ABD-7672-2021
   },
ORCID-Numbers = {Hart, Gus L. W./0000-0002-6149-9234
   Mueller, Tim/0000-0001-8284-7747
   Curtarolo, Stefano/0000-0003-0570-8238},
Unique-ID = {WOS:000675035000001},
}

@article{ WOS:000626617900002,
Author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
Title = {A Survey on Data Collection for Machine Learning: A Big Data-AI
   Integration Perspective},
Journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
Year = {2021},
Volume = {33},
Number = {4},
Pages = {1328-1347},
Month = {APR 1},
Abstract = {Data collection is a major bottleneck in machine learning and an active
   research topic in multiple communities. There are largely two reasons
   data collection has recently become a critical issue. First, as machine
   learning is becoming more widely-used, we are seeing new applications
   that do not necessarily have enough labeled data. Second, unlike
   traditional machine learning, deep learning techniques automatically
   generate features, which saves feature engineering costs, but in return
   may require larger amounts of labeled data. Interestingly, recent
   research in data collection comes not only from the machine learning,
   natural language, and computer vision communities, but also from the
   data management community due to the importance of handling large
   amounts of data. In this survey, we perform a comprehensive study of
   data collection from a data management point of view. Data collection
   largely consists of data acquisition, data labeling, and improvement of
   existing data or models. We provide a research landscape of these
   operations, provide guidelines on which technique to use when, and
   identify interesting research challenges. The integration of machine
   learning and data management for data collection is part of a larger
   trend of Big data and Artificial Intelligence (AI) integration and opens
   many opportunities for new research.},
DOI = {10.1109/TKDE.2019.2946162},
ISSN = {1041-4347},
EISSN = {1558-2191},
ResearcherID-Numbers = {Whang, Steven/F-4529-2018},
ORCID-Numbers = {Whang, Steven/0000-0001-6419-931X
   },
Unique-ID = {WOS:000626617900002},
}

@article{ WOS:000658723000005,
Author = {Huang, Hsin-Yuan and Broughton, Michael and Mohseni, Masoud and Babbush,
   Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.},
Title = {Power of data in quantum machine learning},
Journal = {NATURE COMMUNICATIONS},
Year = {2021},
Volume = {12},
Number = {1},
Month = {MAY 11},
Abstract = {The use of quantum computing for machine learning is among the most
   exciting prospective applications of quantum technologies. However,
   machine learning tasks where data is provided can be considerably
   different than commonly studied computational tasks. In this work, we
   show that some problems that are classically hard to compute can be
   easily predicted by classical machines learning from data. Using
   rigorous prediction error bounds as a foundation, we develop a
   methodology for assessing potential quantum advantage in learning tasks.
   The bounds are tight asymptotically and empirically predictive for a
   wide range of learning models. These constructions explain numerical
   results showing that with the help of data, classical machine learning
   models can be competitive with quantum models even if they are tailored
   to quantum problems. We then propose a projected quantum model that
   provides a simple and rigorous quantum speed-up for a learning problem
   in the fault-tolerant regime. For near-term implementations, we
   demonstrate a significant prediction advantage over some classical
   models on engineered data sets designed to demonstrate a maximal quantum
   advantage in one of the largest numerical tests for gate-based quantum
   machine learning to date, up to 30 qubits. Expectations for quantum
   machine learning are high, but there is currently a lack of rigorous
   results on which scenarios would actually exhibit a quantum advantage.
   Here, the authors show how to tell, for a given dataset, whether a
   quantum model would give any prediction advantage over a classical one.},
DOI = {10.1038/s41467-021-22539-9},
Article-Number = {2631},
EISSN = {2041-1723},
ORCID-Numbers = {Boixo, Sergio/0000-0002-1090-7584
   McClean, Jarrod/0000-0002-2809-0509
   Huang, Hsin-Yuan/0000-0001-5317-2613},
Unique-ID = {WOS:000658723000005},
}

@article{ WOS:000644444900006,
Author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi,
   Farhad and Lin, Zihuai},
Title = {When Machine Learning Meets Privacy: A Survey and Outlook},
Journal = {ACM COMPUTING SURVEYS},
Year = {2021},
Volume = {54},
Number = {2},
Month = {APR},
Abstract = {The newly emerged machine learning (e.g., deep learning) methods have
   become a strong driving force to revolutionize a wide range of
   industries, such as smart healthcare, financial technology, and
   surveillance systems. Meanwhile, privacy has emerged as a big concern in
   this machine learning-based artificial intelligence era. It is important
   to note that the problem of privacy preservation in the context of
   machine learning is quite different from that in traditional data
   privacy protection, as machine learning can act as both friend and foe.
   Currently, the work on the preservation of privacy and machine learning
   are still in an infancy stage, as most existing solutions only focus on
   privacy problems during the machine learning process. Therefore, a
   comprehensive study on the privacy preservation problems and machine
   learning is required. This article surveys the state of the art in
   privacy issues and solutions for machine learning. The survey covers
   three categories of interactions between privacy and machine learning:
   (i) private machine learning, (ii) machine learning-aided privacy
   protection, and (iii) machine learning-based privacy attack and
   corresponding protection schemes. The current research progress in each
   category is reviewed and the key challenges are identified. Finally,
   based on our in-depth analysis of the area of privacy and machine
   learning, we point out future research directions in this field.},
DOI = {10.1145/3436755},
Article-Number = {31},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Farokhi, Farhad/M-2683-2018
   LIU, BO/LRT-5000-2024
   Lin, Zihuai/X-7908-2019
   Shaham, Sina/AAA-1530-2021
   Farokhi, Farhad/ABE-3729-2020
   Rahayu, Wenny/ABA-5515-2020
   Ding, Ming/AAW-4395-2021},
ORCID-Numbers = {LIU, BO/0000-0002-3603-6617
   Farokhi, Farhad/0000-0002-5102-7073
   Ding, Ming/0000-0002-3690-0321
   Lin, Zihuai/0000-0002-3299-0411
   },
Unique-ID = {WOS:000644444900006},
}

@article{ WOS:000656549000001,
Author = {Artrith, Nongnuch and Butler, Keith T. and Coudert, Francois-Xavier and
   Han, Seungwu and Isayev, Olexandr and Jain, Anubhav and Walsh, Aron},
Title = {Best practices in machine learning for chemistry comment},
Journal = {NATURE CHEMISTRY},
Year = {2021},
Volume = {13},
Number = {6},
Pages = {505-508},
Month = {JUN},
Abstract = {Statistical tools based on machine learning are becoming integrated into
   chemistry research workflows. We discuss the elements necessary to train
   reliable, repeatable and reproducible models, and recommend a set of
   guidelines for machine learning reports.},
DOI = {10.1038/s41557-021-00716-z},
ISSN = {1755-4330},
EISSN = {1755-4349},
ResearcherID-Numbers = {Artrith, Nong/H-4329-2016
   Coudert, François-Xavier/C-1374-2008
   Walsh, Aron/A-7843-2008
   Jain, Anubhav/JYO-3714-2024
   Isayev, Olexandr/B-7944-2008},
ORCID-Numbers = {Walsh, Aron/0000-0001-5460-7033
   Jain, Anubhav/0000-0001-5893-9967
   Coudert, Francois-Xavier/0000-0001-5318-3910
   Butler, Keith/0000-0001-5432-5597
   Isayev, Olexandr/0000-0001-7581-8497},
Unique-ID = {WOS:000656549000001},
}

@article{ WOS:000628819200010,
Author = {Otchere, Daniel Asante and Ganat, Tarek Omar Arbi and Gholami, Raoof and
   Ridha, Syahrir},
Title = {Application of supervised machine learning paradigms in the prediction
   of petroleum reservoir properties: Comparative analysis of ANN and SVM
   models},
Journal = {JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING},
Year = {2021},
Volume = {200},
Month = {MAY},
Abstract = {The advent of Artificial Intelligence (AI) in the petroleum industry has
   seen an increase in its use in exploration, development, production,
   reservoir engineering and management planning to accelerate decision
   making, reduce cost and time. Supervised machine learning has gained
   much popularity in establishing a relationship between complex
   non-linear datasets. This type of machine learning algorithm has
   showcased its superiority over petroleum engineering regression
   techniques in terms of prediction errors for high dimensional data,
   computational power and memory.
   This review focuses on the most widely used machine learning algorithm
   employed in the petroleum industry, the Artificial Neural Network (ANN)
   with different shallow models used in reservoir characterisation. The
   Support Vector Machine (SVM) and Relevant Vector Machine (RVM) has over
   the years emerged as competitive algorithms where in most cases based on
   this review it outperformed the ANN. This makes it preferable than the
   ANN when there are limited data sets. Finally, hybridisation of multiple
   algorithms methodologies also showed improved performance over
   singularly applied algorithms offering a pathway in improving reservoir
   characterisation based on supervised machine learning as future scope of
   work.},
DOI = {10.1016/j.petrol.2020.108182},
EarlyAccessDate = {MAR 2021},
Article-Number = {108182},
ISSN = {0920-4105},
EISSN = {1873-4715},
ResearcherID-Numbers = {Otchere, Daniel/AAK-8834-2021
   Ganat, Tarek/AIB-2989-2022
   Gholami, Raoof/R-6939-2016},
ORCID-Numbers = {Otchere, Daniel Asante/0000-0001-5720-4539
   Gholami, Raoof/0000-0001-5744-0566
   },
Unique-ID = {WOS:000628819200010},
}

@article{ WOS:000611850000003,
Author = {Mahmood, Asif and Wang, Jin-Liang},
Title = {Machine learning for high performance organic solar cells: current
   scenario and future prospects},
Journal = {ENERGY \& ENVIRONMENTAL SCIENCE},
Year = {2021},
Volume = {14},
Number = {1},
Pages = {90-105},
Month = {JAN 1},
Abstract = {Machine learning (ML) is a field of computer science that uses
   algorithms and techniques for automating solutions to complex problems
   that are hard to program using conventional programming methods. Owing
   to the chemical versatility of organic building blocks, a large number
   of organic semi-conductors have been used for organic solar cells.
   Selecting a suitable organic semi-conductor is like searching for a
   needle in a haystack. Data-driven science, the fourth paradigm of
   science, has the potential to guide experimentalists to discover and
   develop new high-performance materials. The last decade has seen
   impressive progress in materials informatics and data science; however,
   data-driven molecular design of organic solar cell materials is still
   challenging. The data-analysis capability of machine learning methods is
   well known. This review is written about the use of machine learning
   methods for organic solar cell research. In this review, we have
   outlined the basics of machine learning and common procedures for
   applying machine learning. A brief introduction on different classes of
   machine learning algorithms as well as related software and tools is
   provided. Then, the current research status of machine learning in
   organic solar cells is reviewed. We have discussed the challenges in
   anticipating the data driven material design, such as the complexity
   metric of organic solar cells, diversity of chemical structures and
   necessary programming ability. We have also proposed some suggestions
   that can enhance the usefulness of machine learning for organic solar
   cell research enterprises.},
DOI = {10.1039/d0ee02838j},
ISSN = {1754-5692},
EISSN = {1754-5706},
ResearcherID-Numbers = {Mahmood, Asif/S-5579-2019
   },
ORCID-Numbers = {Mahmood, Asif/0000-0001-9412-1011},
Unique-ID = {WOS:000611850000003},
}

@article{ WOS:000799950300002,
Author = {Okoroafor, Esuru Rita and Smith, Connor M. and Ochie, Karen Ifeoma and
   Nwosu, Chinedu Joseph and Gudmundsdottir, Halldora and Aljubran,
   Mohammad (Jabs)},
Title = {Machine learning in subsurface geothermal energy: Two decades in review},
Journal = {GEOTHERMICS},
Year = {2022},
Volume = {102},
Month = {JUN},
Abstract = {This paper reviews the trends in applying machine learning to subsurface
   geothermal resource development. The review is focused on the machine
   learning applications over the past two decades (from 2002 to 2021) to
   determine which machine learning algorithms are being used. In addition,
   the review seeks to determine what types of problems are being addressed
   with machine learning and how machine learning is aiding decisionmaking
   and problem-solving for subsurface aspects of the geothermal industry.
   The study shows that there has been a steady increase in the application
   of machine learning in the geothermal industry over the past 20 years,
   with an exponential increase in machine learning applications from 2018
   to 2021. Several research areas associated with geothermal resource
   development were reviewed, including exploration, drilling, reservoir
   characterization, seismicity, petrophysics, reservoir engineering, and
   production and injection engineering. The study reveals that the field
   of reservoir characterization had the most significant applications of
   machine learning in the geothermal industry. Though machine learning has
   been applied across all the geothermal research areas we investigated,
   this study shows that there are still opportunities to improve and
   expand the adoption of machine learning in exploration, drilling, and
   seismicity. The main challenges that would need to be addressed are
   ensuring researchers have access to data, curating the data to be
   suitable for machine learning, and training geothermal industry students
   and professionals on artificial intelligence related to the energy
   sector.},
DOI = {10.1016/j.geothermics.2022.102401},
EarlyAccessDate = {MAR 2022},
Article-Number = {102401},
ISSN = {0375-6505},
EISSN = {1879-3576},
ResearcherID-Numbers = {Okoroafor, Esuru/HMO-5544-2023
   Aljubran, Mohammad/HSH-0591-2023},
ORCID-Numbers = {Nwosu, Chinedu/0000-0001-6025-1934
   Gudmundsdottir, Halldora/0000-0001-9452-3387
   Okoroafor, Esuru/0000-0002-5887-8508
   Smith, Connor/0000-0002-2743-5747
   },
Unique-ID = {WOS:000799950300002},
}

@article{ WOS:000880854200004,
Author = {Sohani, Ali and Sayyaadi, Hoseyn and Cornaro, Cristina and Shahverdian,
   Mohammad Hassan and Pierro, Marco and Moser, David and Karimi, Nader and
   Doranehgard, Mohammad Hossein and Li, Larry K. B.},
Title = {Using machine learning in photovoltaics to create smarter and cleaner
   energy generation systems: A comprehensive review},
Journal = {JOURNAL OF CLEANER PRODUCTION},
Year = {2022},
Volume = {364},
Month = {SEP 1},
Abstract = {Photovoltaic (PV) technologies are expected to play an increasingly
   important role in future energy production. In parallel, machine
   learning has gained prominence because of a combination of factors such
   as advances in computational hardware, data collection and storage, and
   data-driven algorithms. Against this backdrop, we provide a
   comprehensive review of machine learning techniques applied to PV
   systems. First, conventional methods for modeling PV systems are
   introduced from both electrical and thermal perspectives. Then, the
   application of machine learning to the analysis of PV systems is
   discussed. We focus on reviewing the use of machine learning algorithms
   to predict performance and detect faults, and on discussing how machine
   learning can help humanity to achieve a cleaner environment in the
   worldwide drive towards carbon neutrality. This review also discusses
   the challenges to and future directions of using machine learning to
   analyze PV systems. A key conclusion is that the use of machine learning
   to analyze PV systems is still in its infancy, with many small-scale PV
   technologies, such as building integrated photovoltaic thermal systems
   (BIPV/T), not yet benefiting fully in terms of system efficiency and
   economic viability. The wider application of machine learning to PV
   systems could therefore forge a shorter path towards sustainable energy
   production.},
DOI = {10.1016/j.jclepro.2022.132701},
Article-Number = {132701},
ISSN = {0959-6526},
EISSN = {1879-1786},
ResearcherID-Numbers = {Sohani, Ali/V-1754-2019
   sayyaadi, hoseyn/J-2034-2019
   Cornaro, Cristina/AAR-5613-2020
   Moser, David/F-4590-2010
   Shahverdian, Mohammad/GON-8166-2022
   Sayyaadi, Hoseyn/J-2034-2019},
ORCID-Numbers = {Cornaro, Cristina/0000-0002-7546-1878
   Karimi, Nader/0000-0002-4559-6245
   Moser, David/0000-0002-4895-8862
   Sayyaadi, Hoseyn/0000-0003-1368-1426},
Unique-ID = {WOS:000880854200004},
}

@article{ WOS:000803594800001,
Author = {Jiang, Yuying and Li, Guangming and Ge, Hongyi and Wang, Faye and Li, Li
   and Chen, Xinyu and Lu, Ming and Zhang, Yuan},
Title = {Machine Learning and Application in Terahertz Technology: A Review on
   Achievements and Future Challenges},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {53761-53776},
Abstract = {Terahertz (THz) radiation (0.1 similar to 10 THz) shows great potential
   in agricultural products detection, biomedical, and security inspection
   in recent years. Machine learning methods are widely used to support the
   user demand of higher efficiency and high prediction accuracy. The
   technological and key challenges of machine learning methods are for THz
   spectroscopy and image data preprocessing, reconstruction algorithms,
   and qualitative and quantitative analysis. In this paper, an exhaustive
   review of recent related works of THz detection and imaging techniques
   and machine learning methods are presented. The application of machine
   learning methods combined with THz technology in quality inspection of
   agricultural products, biomedical, security inspection, and materials
   science are highlighted. Challenges of machine learning methods for
   these applications are addressed. The development trend and future
   perspectives of THz technology are also discussed.},
DOI = {10.1109/ACCESS.2022.3174595},
ISSN = {2169-3536},
ResearcherID-Numbers = {Ge, Hongyi/KLE-2481-2024
   },
ORCID-Numbers = {Ge, Hongyi/0000-0002-9969-8861
   Jiang, Yuying/0000-0002-4645-824X
   Lv, Ming/0000-0003-3678-548X},
Unique-ID = {WOS:000803594800001},
}

@article{ WOS:000797748400001,
Author = {Pederson, Ryan and Kalita, Bhupalee and Burke, Kieron},
Title = {Machine learning and density functional theory},
Journal = {NATURE REVIEWS PHYSICS},
Year = {2022},
Volume = {4},
Number = {6},
Pages = {357-358},
Month = {JUN},
Abstract = {Over the past decade machine learning has made significant advances in
   approximating density functionals, but whether this signals the end of
   human-designed functionals remains to be seen. Ryan Pederson, Bhupalee
   Kalita and Kieron Burke discuss the rise of machine learning for
   functional design.},
DOI = {10.1038/s42254-022-00470-2},
EarlyAccessDate = {MAY 2022},
EISSN = {2522-5820},
ORCID-Numbers = {Kalita, Bhupalee/0000-0001-9967-1629},
Unique-ID = {WOS:000797748400001},
}

@article{ WOS:000930523100002,
Author = {Sandeep, M. S. and Tiprak, Koravith and Kaewunruen, Sakdirat and
   Pheinsusom, Phoonsak and Pansuk, Withit},
Title = {Shear strength prediction of reinforced concrete beams using machine
   learning},
Journal = {STRUCTURES},
Year = {2023},
Volume = {47},
Pages = {1196-1211},
Month = {JAN},
Abstract = {Recent years have witnessed a surge in the application of machine
   learning techniques for solving hard to solve structural engineering
   problems. The application of machine learning can replace the use of
   empirical and semiempirical prediction models currently used in practice
   with highly accurate models. This paper provides a detailed discussion
   on the basic terminologies and concepts of commonly used machine
   learning algorithms for solving structural engineering problems. To
   provide confidence to use this method and show the potential of machine
   learning in accurately predicting the results of complex civil
   engineering problems, a comprehensive literature review on the
   application of machine learning in shear strength prediction is also
   presented. The literature review covers the application of different
   machine learning algorithms in predicting the shear strength of
   conventional concrete beams, steel fibre reinforced concrete beams,
   beams reinforced with FRP bars as well as high strength concrete beams.
   Major observations, challenges and future scope in this field are also
   discussed in detail. This article will be a valuable resource for
   individuals who are unfamiliar with machine learning yet aspire to learn
   more about it.},
DOI = {10.1016/j.istruc.2022.11.140},
EarlyAccessDate = {DEC 2022},
ISSN = {2352-0124},
ResearcherID-Numbers = {Kaewunruen, Sakdirat/AAE-2374-2020
   MS, Sandeep/CAC-9992-2022
   Kaewunruen, Sakdirat/A-6793-2008},
ORCID-Numbers = {Pansuk, Withit/0000-0001-8910-2043
   Kaewunruen, Sakdirat/0000-0003-2153-3538},
Unique-ID = {WOS:000930523100002},
}

@article{ WOS:001466774000001,
Author = {Ahsan, Mostofa and Nygard, Kendall E. and Gomes, Rahul and Chowdhury, Md
   Minhaz and Rifat, Nafiz and Connolly, Jayden F.},
Title = {Cybersecurity Threats and Their Mitigation Approaches Using Machine
   Learning-A Review},
Journal = {JOURNAL OF CYBERSECURITY AND PRIVACY},
Year = {2022},
Volume = {2},
Number = {3},
Pages = {527-555},
Month = {JUL 10},
Abstract = {Machine learning is of rising importance in cybersecurity. The primary
   objective of applying machine learning in cybersecurity is to make the
   process of malware detection more actionable, scalable and effective
   than traditional approaches, which require human intervention. The
   cybersecurity domain involves machine learning challenges that require
   efficient methodical and theoretical handling. Several machine learning
   and statistical methods, such as deep learning, support vector machines
   and Bayesian classification, among others, have proven effective in
   mitigating cyber-attacks. The detection of hidden trends and insights
   from network data and building of a corresponding data-driven machine
   learning model to prevent these attacks is vital to design intelligent
   security systems. In this survey, the focus is on the machine learning
   techniques that have been implemented on cybersecurity data to make
   these systems secure. Existing cybersecurity threats and how machine
   learning techniques have been used to mitigate these threats have been
   discussed. The shortcomings of these state-of-the-art models and how
   attack patterns have evolved over the past decade have also been
   presented. Our goal is to assess how effective these machine learning
   techniques are against the ever-increasing threat of malware that
   plagues our online community.},
DOI = {10.3390/jcp2030027},
EISSN = {2624-800X},
ResearcherID-Numbers = {Gomes, Rahul/Z-4475-2019
   Chowdhury, Md Minhaz/ABD-7782-2021},
ORCID-Numbers = {Gomes, Rahul/0000-0002-5377-8196
   },
Unique-ID = {WOS:001466774000001},
}

@article{ WOS:000819919700004,
Author = {van Cranenburgh, Sander and Wang, Shenhao and Vij, Akshay and Pereira,
   Francisco and Walker, Joan},
Title = {Choice modelling in the age of machine learning Discussion paper},
Journal = {JOURNAL OF CHOICE MODELLING},
Year = {2022},
Volume = {42},
Month = {MAR},
Abstract = {Since its inception, the choice modelling field has been dominated by
   theory-driven modelling approaches. Machine learning offers an
   alternative data-driven approach for modelling choice behaviour and is
   increasingly drawing interest in our field. Cross-pollination of machine
   learning models, techniques and practices could help overcome problems
   and limitations encountered in the current theory-driven modelling
   paradigm, such as subjective labour-intensive search processes for model
   selection, and the inability to work with text and image data. However,
   despite the potential benefits of using the advances of machine learning
   to improve choice modelling practices, the choice modelling field has
   been hesitant to embrace machine learning. This discussion paper aims to
   consolidate knowledge on the use of machine learning models, techniques
   and practices for choice modelling, and discuss their potential.
   Thereby, we hope not only to make the case that further integration of
   machine learning in choice modelling is beneficial, but also to further
   facilitate it. To this end, we clarify the similarities and differences
   between the two modelling paradigms; we review the use of machine
   learning for choice modelling; and we explore areas of opportunities for
   embracing machine learning models and techniques to improve our
   practices. To conclude this discussion paper, we put forward a set of
   research questions which must be addressed to better understand if and
   how machine learning can benefit choice modelling.},
DOI = {10.1016/j.jocm.2021.100340},
EarlyAccessDate = {FEB 2022},
Article-Number = {100340},
ISSN = {1755-5345},
ResearcherID-Numbers = {Pereira, Francisco Camara/B-2111-2010
   Vij, Akshay/E-9077-2015
   Wang, Shenhao/ABD-4771-2020
   Pereira, Francisco/B-2111-2010
   },
ORCID-Numbers = {Pereira, Francisco Camara/0000-0001-5457-9909
   Wang, Shenhao/0000-0003-4374-8193
   van Cranenburgh, Sander/0000-0002-0976-3923},
Unique-ID = {WOS:000819919700004},
}

@article{ WOS:000652706400001,
Author = {Kang, Jaeyong and Ullah, Zahid and Gwak, Jeonghwan},
Title = {MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and
   Machine Learning Classifiers},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {6},
Month = {MAR},
Abstract = {Brain tumor classification plays an important role in clinical diagnosis
   and effective treatment. In this work, we propose a method for brain
   tumor classification using an ensemble of deep features and machine
   learning classifiers. In our proposed framework, we adopt the concept of
   transfer learning and uses several pre-trained deep convolutional neural
   networks to extract deep features from brain magnetic resonance (MR)
   images. The extracted deep features are then evaluated by several
   machine learning classifiers. The top three deep features which perform
   well on several machine learning classifiers are selected and
   concatenated as an ensemble of deep features which is then fed into
   several machine learning classifiers to predict the final output. To
   evaluate the different kinds of pre-trained models as a deep feature
   extractor, machine learning classifiers, and the effectiveness of an
   ensemble of deep feature for brain tumor classification, we use three
   different brain magnetic resonance imaging (MRI) datasets that are
   openly accessible from the web. Experimental results demonstrate that an
   ensemble of deep features can help improving performance significantly,
   and in most cases, support vector machine (SVM) with radial basis
   function (RBF) kernel outperforms other machine learning classifiers,
   especially for large datasets.},
DOI = {10.3390/s21062222},
Article-Number = {2222},
EISSN = {1424-8220},
ResearcherID-Numbers = {Ullah, Zahid/AAA-3822-2020},
ORCID-Numbers = {Kang, Jaeyong/0000-0003-1211-2678
   Gwak, Jeonghwan/0000-0002-6237-0141
   Ullah, Zahid/0000-0002-0184-7620},
Unique-ID = {WOS:000652706400001},
}

@article{ WOS:000731150400004,
Author = {Saha, Dhritiman and Manickavasagan, Annamalai},
Title = {Machine learning techniques for analysis of hyperspectral images to
   determine quality of food products: A review},
Journal = {CURRENT RESEARCH IN FOOD SCIENCE},
Year = {2021},
Volume = {4},
Pages = {28-44},
Abstract = {Non-destructive testing techniques have gained importance in monitoring
   food quality over the years. Hyperspectral imaging is one of the
   important non-destructive quality testing techniques which provides both
   spatial and spectral information. Advancement in machine learning
   techniques for rapid analysis with higher classification accuracy have
   improved the potential of using this technique for food applications.
   This paper provides an overview of the application of different machine
   learning techniques in analysis of hyperspectral images for
   determination of food quality. It covers the principle underlying
   hyperspectral imaging, the advantages, and the limitations of each
   machine learning technique. The machine learning techniques exhibited
   rapid analysis of hyperspectral images of food products with high
   accuracy thereby enabling robust classification or regression models.
   The selection of effective wavelengths from the hyperspectral data is of
   paramount importance since it greatly reduces the computational load and
   time which enhances the scope for real time applications. Due to the
   feature learning nature of deep learning, it is one of the most
   promising and powerful techniques for real time applications. However,
   the field of deep learning is relatively new and need further research
   for its full utilization. Similarly, lifelong machine learning paves the
   way for real time HSI applications but needs further research to
   incorporate the seasonal variations in food quality. Further, the
   research gaps in machine learning techniques for hyperspectral image
   analysis, and the prospects are discussed.},
DOI = {10.1016/j.crfs.2021.01.002},
EarlyAccessDate = {FEB 2021},
EISSN = {2665-9271},
ORCID-Numbers = {SAHA, DHRITIMAN/0000-0001-6709-8186},
Unique-ID = {WOS:000731150400004},
}

@article{ WOS:000835498400003,
Author = {Yerlikaya, Fahri Anil and Bahtiyar, Serif},
Title = {Data poisoning attacks against machine learning algorithms},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2022},
Volume = {208},
Month = {DEC 1},
Abstract = {For the past decade, machine learning technology has increasingly become
   popular and it has been contributing to many areas that have the
   potential to influence the society considerably. Generally, machine
   learning is used by various industries to enhance their performances.
   Moreover, machine learning algorithms are used to solve some hard
   problems of systems that may contain very critical information. This
   makes machine learning algorithms a target of adversaries, which is an
   important problem for systems that use such algorithms. Therefore, it is
   significant to determine the performance and the robustness of a machine
   learning algorithm against attacks. In this paper, we analyze
   empirically the robustness and performances of six machine learning
   algorithms against two types of adversarial attacks by using four
   different datasets and three metrics. In our experiments, we analyze the
   robustness of Support Vector Machine, Stochastic Gradient Descent,
   Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest
   Neighbor algorithms to create learning models. We observe their
   performances in spam, botnet, malware, and cancer detection datasets
   when we launch adversarial attacks against these environments. We use
   data poisoning for manipulating training data during adversarial
   attacks, which are random label flipping and distance-based label
   flipping attacks. We analyze the performance of each algorithm for a
   specific dataset by modifying the amount of poisoned data and analyzing
   behaviors of accuracy rate, f1-score, and AUC score. Analyses results
   show that machine learning algorithms have various performance results
   and robustness under different adversarial attacks. Moreover, machine
   learning algorithms are affected differently in each stage of an
   adversarial attacks. Furthermore, the behavior of a machine learning
   algorithm highly depends on the type of the dataset. On the other hand,
   some machine learning algorithms have better robustness and performance
   results against adversarial attacks for almost all datasets.},
DOI = {10.1016/j.eswa.2022.118101},
EarlyAccessDate = {JUL 2022},
Article-Number = {118101},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {Bahtiyar, Serif/K-9773-2017
   Bahtiyar, Şerif/ABA-9144-2020
   },
ORCID-Numbers = {Bahtiyar, Serif/0000-0003-0314-2621
   Yerlikaya, Fahri Anil/0000-0001-9859-3905},
Unique-ID = {WOS:000835498400003},
}

@article{ WOS:000799943600006,
Author = {Khan, Mohammad Monirujjaman and Hossain, Sazzad and Mozumdar, Puezia and
   Akter, Shamima and Ashique, Ratil H.},
Title = {A review on machine learning and deep learning for various antenna
   design applications},
Journal = {HELIYON},
Year = {2022},
Volume = {8},
Number = {4},
Month = {APR},
Abstract = {The next generation of wireless communication networks will rely heavily
   on machine learning and deep learning. In comparison to traditional
   ground-based systems, the development of various communication-based
   applications is projected to increase coverage and spectrum efficiency.
   Machine learning and deep learning can be used to optimize solutions in
   a variety of applications, including antennas. The latter have grown
   popular for obtaining effective solutions due to high computational
   processing, clean data, and large data storage capability. In this
   research, machine learning and deep learning for various antenna design
   applications have been discussed in detail. The general concept of
   machine learning and deep learning is introduced. However, the main
   focus is on various antenna applications, such as millimeter wave,
   body-centric, terahertz, satellite, unmanned aerial vehicle, global
   positioning system, and textiles. The feasibility of antenna
   applications with respect to conventional methods, acceleration of the
   antenna design process, reduced number of simulations, and better
   computational feasibility features are highlighted. Overall, machine
   learning and deep learning provide satisfactory results for antenna
   design.},
DOI = {10.1016/j.heliyon.2022.e09317},
EarlyAccessDate = {APR 2022},
Article-Number = {e09317},
EISSN = {2405-8440},
ResearcherID-Numbers = {Akter Swapna, Shamima/AAA-5921-2022
   Ashique, Ratil/T-7277-2019},
ORCID-Numbers = {khan, Dr Mohammad Monirujjaman/0000-0003-0779-8820
   Hossain, Sazzad/0000-0002-9675-0468
   Ashique, Ratil/0000-0002-6039-3841
   },
Unique-ID = {WOS:000799943600006},
}

@article{ WOS:000880551800002,
Author = {Kuehl, Niklas and Schemmer, Max and Goutier, Marc and Satzger, Gerhard},
Title = {Artificial intelligence and machine learning},
Journal = {ELECTRONIC MARKETS},
Year = {2022},
Volume = {32},
Number = {4},
Pages = {2235-2244},
Month = {DEC},
Abstract = {Within the last decade, the application of ``artificial intelligence{''}
   and ``machine learning{''} has become popular across multiple
   disciplines, especially in information systems. The two terms are still
   used inconsistently in academia and industry-sometimes as synonyms,
   sometimes with different meanings. With this work, we try to clarify the
   relationship between these concepts. We review the relevant literature
   and develop a conceptual framework to specify the role of machine
   learning in building (artificial) intelligent agents. Additionally, we
   propose a consistent typology for AI-based information systems. We
   contribute to a deeper understanding of the nature of both concepts and
   to more terminological clarity and guidance-as a starting point for
   interdisciplinary discussions and future research.},
DOI = {10.1007/s12525-022-00598-0},
EarlyAccessDate = {NOV 2022},
ISSN = {1019-6781},
EISSN = {1422-8890},
ResearcherID-Numbers = {Schemmer, Max/W-9974-2019
   Satzger, Gerhard/AAF-4725-2020
   },
ORCID-Numbers = {Kuhl, Niklas/0000-0001-6750-0876},
Unique-ID = {WOS:000880551800002},
}

@article{ WOS:000818226100001,
Author = {Xu, Chongchong and Liao, Zhicheng and Li, Chaojie and Zhou, Xiaojun and
   Xie, Renyou},
Title = {Review on Interpretable Machine Learning in Smart Grid},
Journal = {ENERGIES},
Year = {2022},
Volume = {15},
Number = {12},
Month = {JUN},
Abstract = {In recent years, machine learning, especially deep learning, has
   developed rapidly and has shown remarkable performance in many tasks of
   the smart grid field. The representation ability of machine learning
   algorithms is greatly improved, but with the increase of model
   complexity, the interpretability of machine learning algorithms is
   worse. The smart grid is a critical infrastructure area, so machine
   learning models involving it must be interpretable in order to increase
   user trust and improve system reliability. Unfortunately, the black-box
   nature of most machine learning models remains unresolved, and many
   decisions of intelligent systems still lack explanation. In this paper,
   we elaborate on the definition, motivations, properties, and
   classification of interpretability. In addition, we review the relevant
   literature addressing interpretability for smart grid applications.
   Finally, we discuss the future research directions of interpretable
   machine learning in the smart grid.},
DOI = {10.3390/en15124427},
Article-Number = {4427},
EISSN = {1996-1073},
ResearcherID-Numbers = {Lee, Promising/GVR-9434-2022
   Zhou, Xiaojun/AAY-8732-2020
   xie, renyou/GQA-8640-2022
   },
ORCID-Numbers = {Xie, Renyou/0000-0003-1947-0435},
Unique-ID = {WOS:000818226100001},
}

@article{ WOS:000762430500001,
Author = {Mueller, Brianna and Kinoshita, Takahiro and Peebles, Alexander and
   Graber, Mark A. and Lee, Sangil},
Title = {Artificial intelligence and machine learning in emergency medicine: a
   narrative review},
Journal = {ACUTE MEDICINE \& SURGERY},
Year = {2022},
Volume = {9},
Number = {1},
Month = {JAN},
Abstract = {Aim: The emergence and evolution of artificial intelligence (AI) has
   generated increasing interest in machine learning applications for
   health care. Specifically, researchers are grasping the potential of
   machine learning solutions to enhance the quality of care in emergency
   medicine.
   Methods: We undertook a narrative review of published works on machine
   learning applications in emergency medicine and provide a synopsis of
   recent developments.
   Results: This review describes fundamental concepts of machine learning
   and presents clinical applications for triage, risk stratification
   specific to disease, medical imaging, and emergency department
   operations. Additionally, we consider how machine learning models could
   contribute to the improvement of causal inference in medicine, and to
   conclude, we discuss barriers to safe implementation of AI.
   Conclusion: We intend that this review serves as an introduction to AI
   and machine learning in emergency medicine.},
DOI = {10.1002/ams2.740},
Article-Number = {e740},
ISSN = {2052-8817},
ORCID-Numbers = {Lee, Sangil/0000-0001-8529-4500},
Unique-ID = {WOS:000762430500001},
}

@article{ WOS:000787825500001,
Author = {Rajulapati, Lokesh and Chinta, Sivadurgaprasad and Shyamala, Bala and
   Rengaswamy, Raghunathan},
Title = {Integration of machine learning and first principles models},
Journal = {AICHE JOURNAL},
Year = {2022},
Volume = {68},
Number = {6},
Month = {JUN},
Abstract = {Model building and parameter estimation are traditional concepts widely
   used in chemical, biological, metallurgical, and manufacturing
   industries. Early modeling methodologies focused on mathematically
   capturing the process knowledge and domain expertise of the modeler. The
   models thus developed are termed first principles models (or white-box
   models). Over time, computational power became cheaper, and massive
   amounts of data became available for modeling. This led to the
   development of cutting edge machine learning models (black-box models)
   and artificial intelligence (AI) techniques. Hybrid models (gray-box
   models) are a combination of first principles and machine learning
   models. The development of hybrid models has captured the attention of
   researchers as this combines the best of both modeling paradigms. Recent
   attention to this field stems from the interest in explainable AI (XAI),
   a critical requirement as AI systems become more pervasive. This work
   aims at identifying and categorizing various hybrid models available in
   the literature that integrate machine-learning models with different
   forms of domain knowledge. Benefits such as enhanced predictive power,
   extrapolation capabilities, and other advantages of combining the two
   approaches are summarized. The goal of this article is to consolidate
   the published corpus in the area of hybrid modeling and develop a
   comprehensive framework to understand the various techniques presented.
   This framework can further be used as the foundation to explore rational
   associations between several models.},
DOI = {10.1002/aic.17715},
EarlyAccessDate = {APR 2022},
Article-Number = {e17715},
ISSN = {0001-1541},
EISSN = {1547-5905},
Unique-ID = {WOS:000787825500001},
}

@article{ WOS:001072225000001,
Author = {Tian, Xiao-lan and Song, Si-wei and Chen, Fang and Qi, Xiu-juan and
   Wang, Yi and Zhang, Qing-hua},
Title = {Machine learning-guided property prediction of energetic materials:
   Recent advances, challenges, and perspectives},
Journal = {ENERGETIC MATERIALS FRONTIERS},
Year = {2022},
Volume = {3},
Number = {3},
Pages = {177-186},
Month = {SEP},
Abstract = {Predicting chemical properties is one of the most important applications
   of machine learning. In recent years, the prediction of the properties
   of energetic materials using machine learning has been receiving more
   attention. This review summarized recent advances in predicting
   energetic compounds' properties (e.g., density, detonation velocity,
   enthalpy of formation, sensitivity, the heat of the explosion, and
   decomposition temperature) using machine learning. Moreover, it
   presented general steps for applying machine learning to the prediction
   of practical chemical properties from the aspects of data, molecular
   representation, algorithms, and general accu-racy. Additionally, it
   raised some controversies specific to machine learning in energetic
   materials and its possible development directions. Machine learning is
   expected to become a new power for driving the development of energetic
   materials soon.},
DOI = {10.1016/j.enmf.2022.07.005},
EISSN = {2666-6472},
ResearcherID-Numbers = {Wang, Yi/AAI-1973-2021
   song, siwei/LEM-4701-2024
   Zhang, Qinghua/K-4474-2013},
ORCID-Numbers = {Wang, Yi/0000-0003-1875-6597
   siwei, song/0000-0003-2528-8728
   },
Unique-ID = {WOS:001072225000001},
}

@article{ WOS:000908283400019,
Author = {Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday and Omidiora,
   Joseph Olamide},
Title = {Exploring teachers' preconceptions of teaching machine learning in high
   school: A preliminary insight from Africa},
Journal = {COMPUTERS AND EDUCATION OPEN},
Year = {2022},
Volume = {3},
Month = {DEC},
Abstract = {The teaching of machine learning is now considered essential and
   relevant in schools globally. Despite the ongoing discourse and
   increased research in the emerging field, teachers' conceptions of
   machine learning remain under-researched. This study aims at filling the
   gap by describing the initial conceptions of teaching machine learning
   by 12 African in-service teachers. We detailed the result of a
   phenomenographic analysis of teachers' pre-conceptions on teaching
   machine learning in K-12 settings. Twelve high school (Grades 10-12)
   computer science teachers in some selected African countries were
   recruited for a semi-structured interview. Five categories emerged from
   the analysis of the semi-structured interviews as follows: supporting
   student technical knowledge, having knowledge of the concept, focusing
   on professional development practices, contextualizing teaching
   resources and tools, and sustainability for development goals. These
   involve the relevance of teaching machine learning, the pedagogical
   approaches, strategies, and sustainability relating to practical
   implementation in schools. The results suggest the need to train
   in-service teachers to use existing tools designed for introducing
   machine learning. The teachers should also be involved in the
   co-designing process of resources considering contextual factors and,
   significantly, the curriculum to integrate machine learning into
   mainstream education. Involving teachers in the development process
   would help contextualize machine learning, contributing to real impact
   and societal changes.},
DOI = {10.1016/j.caeo.2021.100072},
EarlyAccessDate = {JAN 2022},
Article-Number = {100072},
ISSN = {2666-5573},
ResearcherID-Numbers = {Sanusi, Ismaila Temitayo/AAE-6972-2020
   Oyelere, Solomon Sunday/AAE-7541-2020},
ORCID-Numbers = {chamsou, ilias/0009-0009-4074-1421
   Oyelere, Solomon Sunday/0000-0001-9895-6796
   },
Unique-ID = {WOS:000908283400019},
}

@article{ WOS:000833855900006,
Author = {Bowler, Alexander L. and Pound, Michael P. and Watson, Nicholas J.},
Title = {A review of ultrasonic sensing and machine learning methods to monitor
   industrial processes},
Journal = {ULTRASONICS},
Year = {2022},
Volume = {124},
Month = {AUG},
Abstract = {Supervised machine learning techniques are increasingly being combined
   with ultrasonic sensor measurements owing to their strong performance.
   These techniques also offer advantages over calibration procedures of
   more complex fitting, improved generalisation, reduced development time,
   ability for continuous retraining, and the correlation of sensor data to
   important process information. However, their implementation requires
   expertise to extract and select appropriate features from the sensor
   measurements as model inputs, select the type of machine learning
   algorithm to use, and find a suitable set of model hyperparameters. The
   aim of this article is to facilitate implementation of machine learning
   techniques in combination with ultrasonic measurements for in-line and
   online monitoring of industrial processes and other similar
   applications. The article first reviews the use of ultrasonic sensors
   for monitoring processes, before reviewing the combination of ultrasonic
   measurements and machine learning. We include literature from other
   sectors such as structural health monitoring. This review covers feature
   extraction, feature selection, algorithm choice, hyperparameter
   selection, data augmentation, domain adaptation, semi-supervised
   learning and machine learning interpretability. Finally, recommendations
   for applying machine learning to the reviewed processes are made.},
DOI = {10.1016/j.ultras.2022.106776},
EarlyAccessDate = {MAY 2022},
Article-Number = {106776},
ISSN = {0041-624X},
EISSN = {1874-9968},
ORCID-Numbers = {Pound, Michael/0000-0002-5016-1078
   watson, nicholas/0000-0001-5216-4873},
Unique-ID = {WOS:000833855900006},
}

@article{ WOS:000895081000015,
Author = {Siemuri, Akpojoto and Selvan, Kannan and Kuusniemi, Heidi and Valisuo,
   Petri and Elmusrati, Mohammed S.},
Title = {A Systematic Review of Machine Learning Techniques for GNSS Use Cases},
Journal = {IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS},
Year = {2022},
Volume = {58},
Number = {6},
Pages = {5043-5077},
Month = {DEC},
Abstract = {In terms of the availability and accuracy of positioning, navigation,
   and timing (PNT), the traditional Global Navigation Satellite System
   (GNSS) algorithms and models perform well under good signal conditions.
   In order to improve their robustness and performance in less than
   optimal signal environments, many researchers have proposed machine
   learning (ML) based GNSS models (ML models) as early as the 1990s.
   However, no study has been done in a systematic way to analyze the
   extent of the research on the utilization of ML models in GNSS and their
   performance. In this study, we perform a systematic review of studies
   from 2000 to 2021 in the literature that utilizes machine learning
   techniques in GNSS use cases. We assess the performance of the machine
   learning techniques in the existing literature on their application to
   GNSS. Furthermore, the strengths and weaknesses of machine learning
   techniques are summarized. In this paper, we have identified 213
   selected studies and ten categories of machine learning techniques. The
   results prove the acceptable performance of machine learning techniques
   in several GNSS use cases. In most cases, the models using the machine
   learning techniques in these GNSS use cases outperform the traditional
   GNSS models. ML models are promising in their utilization in GNSS.
   However, the application of ML models in the industry is still limited.
   More effort and incentives are needed to facilitate the utilization of
   ML models in the PNT context. Therefore, based on the findings of this
   review, we provide recommendations for researchers and guidelines for
   practitioners.},
DOI = {10.1109/TAES.2022.3219366},
ISSN = {0018-9251},
EISSN = {1557-9603},
ResearcherID-Numbers = {Elmusrati, Mohammed/H-8442-2012
   },
ORCID-Numbers = {Siemuri, Akpojoto/0000-0002-2644-1985
   Elmusrati, Mohammed/0000-0001-9304-6590
   Kuusniemi, Heidi/0000-0002-7551-9531
   Valisuo, Petri/0000-0002-9566-6408},
Unique-ID = {WOS:000895081000015},
}

@article{ WOS:000741323700002,
Author = {Ouadah, Abdelfettah and Zemmouchi-Ghomari, Leila and Salhi, Nedjma},
Title = {Selecting an appropriate supervised machine learning algorithm for
   predictive maintenance},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY},
Year = {2022},
Volume = {119},
Number = {7-8},
Pages = {4277-4301},
Month = {APR},
Abstract = {Predictive maintenance refers to predicting malfunctions using data from
   monitoring equipment and process performance measurements. Machine
   learning algorithms and techniques are often used to analyze equipment
   monitoring data. Machine learning is the process in which a computer can
   work more precisely by collecting and analyzing data. It is often the
   case that machine learning algorithms use supervised learning, in which
   labelled data is used to feed the algorithm. However, there are many
   supervised machine learning algorithms available. Therefore, choosing
   the best-supervised machine learning algorithm to resolve predictive
   maintenance issues is not trivial. This paper aims to increase the
   performance of predictive maintenance and achieve its goals by selecting
   the most suitable supervised machine learning algorithm. Based on the
   most commonly used criteria in research articles, we selected three
   supervised machine learning algorithms from a comparative study: Random
   forest, Decision tree and KNN. We then tested selected algorithms on
   data from real-world and simulation scenarios. Finally, we conducted the
   experiment based on vibration analysis and reliability evaluation. We
   noticed that Random forests and Decision trees obtained slightly the
   same performance. KNN is a better classification algorithm for extensive
   volumes of data; on the contrary, Random forest performs better in the
   case of small datasets.},
DOI = {10.1007/s00170-021-08551-9},
EarlyAccessDate = {JAN 2022},
ISSN = {0268-3768},
EISSN = {1433-3015},
ResearcherID-Numbers = {Zemmouchi-Ghomari, Leila/CAA-2412-2022},
ORCID-Numbers = {ZEMMOUCHI-GHOMARI, LEILA/0000-0002-6754-6062
   Abdelfetah, OUADAH/0000-0001-7427-2007
   },
Unique-ID = {WOS:000741323700002},
}

@article{ WOS:000768294100005,
Author = {Watson, David S.},
Title = {Conceptual challenges for interpretable machine learning},
Journal = {SYNTHESE},
Year = {2022},
Volume = {200},
Number = {1},
Month = {FEB},
Abstract = {As machine learning has gradually entered into ever more sectors of
   public and private life, there has been a growing demand for algorithmic
   explainability. How can we make the predictions of complex statistical
   models more intelligible to end users? A subdiscipline of computer
   science known as interpretable machine learning (IML) has emerged to
   address this urgent question. Numerous influential methods have been
   proposed, from local linear approximations to rule lists and
   counterfactuals. In this article, I highlight three conceptual
   challenges that are largely overlooked by authors in this area. I argue
   that the vast majority of IML algorithms are plagued by (1) ambiguity
   with respect to their true target; (2) a disregard for error rates and
   severe testing; and (3) an emphasis on product over process. Each point
   is developed at length, drawing on relevant debates in epistemology and
   philosophy of science. Examples and counterexamples from IML are
   considered, demonstrating how failure to acknowledge these problems can
   result in counterintuitive and potentially misleading explanations.
   Without greater care for the conceptual foundations of IML, future work
   in this area is doomed to repeat the same mistakes.},
DOI = {10.1007/s11229-022-03485-5},
ISSN = {0039-7857},
EISSN = {1573-0964},
ORCID-Numbers = {Watson, David/0000-0001-9632-2159},
Unique-ID = {WOS:000768294100005},
}

@article{ WOS:000848617400011,
Author = {Chase, Randy J. and Harrison, David R. and Burke, Amanda and Lackmann,
   Gary M. and McGovern, Amy},
Title = {A Machine Learning Tutorial for Operational Meteorology. Part I:
   Traditional Machine Learning},
Journal = {WEATHER AND FORECASTING},
Year = {2022},
Volume = {37},
Number = {8},
Pages = {1509-1529},
Month = {AUG},
Abstract = {Recently, the use of machine learning in meteorology has increased
   greatly. While many machine learning methods are not new, university
   classes on machine learning are largely unavailable to meteorology
   students and are not required to become a meteorologist. The lack of
   formal instruction has contributed to perception that machine learning
   methods are ``black boxes{''} and thus end-users are hesitant to apply
   the machine learning methods in their everyday workflow. To reduce the
   opaqueness of machine learning methods and lower hesitancy toward
   machine learning in meteorology, this paper provides a survey of some of
   the most common machine learning methods. A familiar meteorological
   example is used to contextualize the machine learning methods while also
   discussing machine learning topics using plain language. The following
   machine learning methods are demonstrated: linear regression, logistic
   regression, decision trees, random forest, gradient boosted decision
   trees, naive Bayes, and support vector machines. Beyond discussing the
   different methods, the paper also contains discussions on the general
   machine learning process as well as best practices to enable readers to
   apply machine learning to their own datasets. Furthermore, all code (in
   the form of Jupyter notebooks and Google Colaboratory notebooks) used to
   make the examples in the paper is provided in an effort to catalyze the
   use of machine learning in meteorology.},
DOI = {10.1175/WAF-D-22-0070.1},
ISSN = {0882-8156},
EISSN = {1520-0434},
ResearcherID-Numbers = {McGovern, Amy/AAC-8132-2022
   Lackmann, Gary/AAG-9745-2019
   Chase, Randy/ABE-3180-2021
   },
ORCID-Numbers = {Lackmann, Gary/0000-0001-9069-1228
   Burke, Amanda/0000-0001-6703-6006
   Chase, Randy/0000-0002-2606-7612
   McGovern, Amy/0000-0001-6675-7119},
Unique-ID = {WOS:000848617400011},
}

@article{ WOS:000855096700002,
Author = {He, GaoYuan and Zhao, YongXiang and Yan, ChuLiang},
Title = {Application of tabular data synthesis using generative adversarial
   networks on machine learning-based multiaxial fatigue life prediction},
Journal = {INTERNATIONAL JOURNAL OF PRESSURE VESSELS AND PIPING},
Year = {2022},
Volume = {199},
Month = {OCT},
Abstract = {Machine learning has gradually developed into a new and effective scheme
   for fatigue life prediction. The novelty of this work is the proposal
   and verification of using virtual synthetic multiaxial fatigue data as
   input of machine learning models. First, the data generated by tabular
   generative adversarial networks are applied to machine learning models
   for life prediction. Then based on equivalent stress (strain)
   amplitude-life relationship curve, a multiaxial fatigue data generation
   evaluation metric is proposed. Finally, the effect of the generated
   sample size on the predictions of machine learning models is
   investigated. The method is demonstrated on 5 multiaxial fatigue data
   sets. The results indicate the synthetic data help machine learning
   models arrive at good life prediction ability. Using this method will
   help expand the application of machine learning-based multiaxial fatigue
   life prediction.},
DOI = {10.1016/j.ijpvp.2022.104779},
EarlyAccessDate = {AUG 2022},
Article-Number = {104779},
ISSN = {0308-0161},
EISSN = {1879-3541},
ResearcherID-Numbers = {Zhao, YongXiang/JKI-9544-2023},
ORCID-Numbers = {He, GaoYuan/0000-0002-8191-5311
   },
Unique-ID = {WOS:000855096700002},
}

@article{ WOS:000838619300001,
Author = {Maheshwari, Danyal and Garcia-Zapirain, Begonya and Sierra-Sosa, Daniel},
Title = {Quantum Machine Learning Applications in the Biomedical Domain: A
   Systematic Review},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {80463-80484},
Abstract = {Quantum technologies have become powerful tools for a wide range of
   application disciplines, which tend to range from chemistry to
   agriculture, natural language processing, and healthcare due to
   exponentially growing computational power and advancement in machine
   learning algorithms. Furthermore, the processing of classical data and
   machine learning algorithms in the quantum domain has given rise to an
   emerging field like quantum machine learning. Recently, quantum machine
   learning has become quite a challenging field in the case of healthcare
   applications. As a result, quantum machine learning has become a common
   and effective technique for data processing and classification across a
   wide range of domains. Consequently, quantum machine learning is the
   most commonly used application of quantum computing. The main objective
   of this work is to present a brief overview of current state-of-the-art
   published articles between 2013 and 2021 to identify, analyze, and
   classify the different QML algorithms and applications in the biomedical
   field. Furthermore, the approach adheres to the requirements for
   conducting systematic literature review techniques such as research
   questions and quality metrics of the articles. Initially, we discovered
   3149 articles, excluded the 2847 papers, and read the 121 full papers.
   Therefore, this research compiled 30 articles that comply with the
   quantum machine learning models and quantum circuits using biomedical
   data. Eventually, this article provides a broad overview of quantum
   machine learning limitations and future prospects.},
DOI = {10.1109/ACCESS.2022.3195044},
ISSN = {2169-3536},
ResearcherID-Numbers = {Garcia-Zapirain, Begona/L-5619-2014
   Garcia-Zapirain, Begonya/L-5619-2014
   Maheshwari, Danyal/AAC-4398-2019
   Sierra-Sosa, Daniel/AAP-4610-2020},
ORCID-Numbers = {Garcia-Zapirain, Begonya/0000-0002-9356-1186
   Maheshwari, Danyal/0000-0001-7544-7817
   Sierra-Sosa, Daniel/0000-0003-1326-0867},
Unique-ID = {WOS:000838619300001},
}

@article{ WOS:000856097600001,
Author = {Duong, Trung Q. and Ansere, James Adu and Narottama, Bhaskara and
   Sharma, Vishal and Dobre, Octavia A. and Shin, Hyundong},
Title = {Quantum-Inspired Machine Learning for 6G: Fundamentals, Security,
   Resource Allocations, Challenges, and Future Research Directions},
Journal = {IEEE OPEN JOURNAL OF VEHICULAR TECHNOLOGY},
Year = {2022},
Volume = {3},
Pages = {375-387},
Abstract = {Quantum computing is envisaged as an evolving paradigm for solving
   computationally complex optimization problems with a large-number
   factorization and exhaustive search. Recently, there has been a
   proliferating growth of the size of multi-dimensional datasets, the
   input-output space dimensionality, and data structures. Hence, the
   conventional machine learning approaches in data training and processing
   have exhibited their limited computing capabilities to support the
   sixth-generation (6G) networks with highly dynamic applications and
   services. In this regard, the fast developing quantum computing with
   machine learning for 6G networks is investigated. Quantum machine
   learning algorithm can significantly enhance the processing efficiency
   and exponentially computational speed-up for effective quantum data
   representation and superposition framework, highly capable of
   guaranteeing high data storage and secured communications. We present
   the state-of-the-art in quantum computing and provide a comprehensive
   overview of its potential, via machine learning approaches. Furthermore,
   we introduce quantum-inspired machine learning applications for 6G
   networks in terms of resource allocation and network security,
   considering their enabling technologies and potential challenges.
   Finally, some dominating research issues and future research directions
   for the quantum-inspired machine learning in 6G networks are elaborated.},
DOI = {10.1109/OJVT.2022.3202876},
EISSN = {2644-1330},
ResearcherID-Numbers = {Dobre, Octavia A./B-9689-2019
   Shin, Hyundong/A-3223-2011
   Duong, Trung Q./AAI-6708-2020
   Sharma, Vishal/E-5834-2017
   Duong, Trung Q./I-1291-2013
   Dobre, Octavia/B-1435-2008},
ORCID-Numbers = {Dobre, Octavia A./0000-0001-8528-0512
   Sharma, Vishal/0000-0001-7470-6506
   Shin, Hyundong/0000-0003-3364-8084
   Duong, Trung Q./0000-0002-4703-4836
   },
Unique-ID = {WOS:000856097600001},
}

@article{ WOS:000913916400001,
Author = {Pandey, Ashutosh Kumar and Park, Jungsu and Ko, Jeun and Joo, Hwan-Hong
   and Raj, Tirath and Singh, Lalit Kumar and Singh, Noopur and Kim,
   Sang-Hyoun},
Title = {Machine learning in fermentative biohydrogen production: Advantages,
   challenges, and applications},
Journal = {BIORESOURCE TECHNOLOGY},
Year = {2023},
Volume = {370},
Month = {FEB},
Abstract = {Hydrogen can be produced in an environmentally friendly manner through
   biological processes using a variety of organic waste and biomass as
   feedstock. However, the complexity of biological processes limits their
   predictability and reliability, which hinders the scale-up and
   dissemination. This article reviews contemporary research and
   perspectives on the application of machine learning in biohydrogen
   production technology. Several machine learning algorithems have
   recently been implemented for modeling the nonlinear and complex
   relationships among operational and performance parameters in
   biohydrogen production as well as predicting the process performance and
   microbial population dynamics. Reinforced machine learning methods
   exhibited precise state prediction and retrieved the underlying kinetics
   effectively. Machine-learning based prediction was also improved by
   using microbial sequencing data as input parameters. Further research on
   machine learning could be instrumental in designing a process control
   tool to maintain reliable hydrogen production performance and identify
   connection between the process performance and the microbial population.},
DOI = {10.1016/j.biortech.2022.128502},
EarlyAccessDate = {DEC 2022},
Article-Number = {128502},
ISSN = {0960-8524},
EISSN = {1873-2976},
ResearcherID-Numbers = {singh, lalit/JHT-4573-2023
   RAJ, TIRATH/HGA-1238-2022
   Singh, Lalit Kumar/Q-7133-2017
   Singh, Noopur/LKM-4042-2024
   Pandey, Ashutosh Kumar/AAZ-2240-2020
   Raj, Tirath/HGA-1238-2022
   Singh, Lalit/JTD-2028-2023
   Kim, Sang-Hyoun/P-6837-2019
   Kim, Sang-Hyoun/G-9114-2011},
ORCID-Numbers = {Singh, Lalit Kumar/0000-0001-9374-2123
   Pandey, Ashutosh Kumar/0000-0001-5013-6561
   Raj, Tirath/0000-0003-2950-0458
   Singh, Noopur/0000-0003-4305-3375
   Kim, Sang-Hyoun/0000-0003-3835-4077},
Unique-ID = {WOS:000913916400001},
}

@article{ WOS:000862449600001,
Author = {Black, Jason E. and Kueper, Jacqueline K. and Williamson, Tyler S.},
Title = {An introduction to machine learning for classification and prediction},
Journal = {FAMILY PRACTICE},
Year = {2022},
Month = {2022 OCT 1},
Abstract = {Classification and prediction tasks are common in health research. With
   the increasing availability of vast health data repositories (e.g.
   electronic medical record databases) and advances in computing power,
   traditional statistical approaches are being augmented or replaced with
   machine learning (ML) approaches to classify and predict health
   outcomes. ML describes the automated process of identifying
   ({''}learning{''}) patterns in data to perform tasks. Developing an ML
   model includes selecting between many ML models (e.g. decision trees,
   support vector machines, neural networks); model specifications such as
   hyperparameter tuning; and evaluation of model performance. This process
   is conducted repeatedly to find the model and corresponding
   specifications that optimize some measure of model performance. ML
   models can make more accurate classifications and predictions than their
   statistical counterparts and confer greater flexibility when modelling
   unstructured data or interactions between covariates; however, many ML
   models require larger sample sizes to achieve good classification or
   predictive performance and have been criticized as ``black box{''} for
   their poor transparency and interpretability. ML holds potential in
   family medicine for risk profiling of patients' disease risk and
   clinical decision support to present additional information at times of
   uncertainty or high demand. In the future, ML approaches are positioned
   to become commonplace in family medicine. As such, it is important to
   understand the objectives that can be addressed using ML approaches and
   the associated techniques and limitations. This article provides a brief
   introduction into the use of ML approaches for classification and
   prediction tasks in family medicine.},
DOI = {10.1093/fampra/cmac104},
EarlyAccessDate = {OCT 2022},
ISSN = {0263-2136},
EISSN = {1460-2229},
ORCID-Numbers = {Black, Jason Edward/0000-0002-3762-3231
   Williamson, Tyler/0000-0001-5029-2345},
Unique-ID = {WOS:000862449600001},
}

@article{ WOS:000847846800001,
Author = {Kumar, Yogesh and Kaul, Surabhi and Hu, Yu-Chen},
Title = {Machine learning for energy-resource allocation, workflow scheduling and
   live migration in cloud computing: State-of-the-art survey},
Journal = {SUSTAINABLE COMPUTING-INFORMATICS \& SYSTEMS},
Year = {2022},
Volume = {36},
Month = {DEC},
Abstract = {Machine learning and artificial intelligence techniques have been proven
   helpful when pragmatic to a wide range of complex problems and areas
   such as energy optimization, workflow scheduling, video gaming, and
   cloud computing. When machine learning and cloud computing algorithms
   are combined, they help achieve better outcomes by providing the
   improved performance of cloud data centers compared to solutions
   currently employed by various researchers. It is also helpful for
   migrating the virtual machines based on the current traffic condition
   and fluctuation due to network congestion and bandwidth availability.
   The survey aims to present the improvement in dynamic load allocation,
   task scheduling, energy optimization, live migration, mobile cloud
   computing, and security on the cloud using machine learning
   classification. Machine learning algorithms are prevailing analytical
   approaches that allow machines to identify patterns and simplify the
   human learning process. The flow of the paper consists of an
   introduction part, motivation, and background study, including a
   framework for cloud-machine learning integration, best practices of
   introducing machine learning in cloud computing, and the objective of
   the work. The paper also highlights the machine learning-based cloud
   services and the role of artificial intelligence in different cloud
   computing platforms. This comprehensive study provides mindfulness and
   valuable facilities to the researchers by giving thorough studies about
   various machine learning algorithms and their applicability in cloud
   computing.},
DOI = {10.1016/j.suscom.2022.100780},
EarlyAccessDate = {AUG 2022},
Article-Number = {100780},
ISSN = {2210-5379},
EISSN = {2210-5387},
ResearcherID-Numbers = {KUMAR, YOGESH/GXN-1220-2022},
Unique-ID = {WOS:000847846800001},
}

@article{ WOS:000747379200007,
Author = {Merkin, Alexander and Krishnamurthi, Rita and Medvedev, Oleg N.},
Title = {Machine learning, artificial intelligence and the prediction of dementia},
Journal = {CURRENT OPINION IN PSYCHIATRY},
Year = {2022},
Volume = {35},
Number = {2},
Pages = {123-129},
Month = {MAR},
Abstract = {Purpose of review Artificial intelligence and its division machine
   learning are emerging technologies that are increasingly applied in
   medicine. Artificial intelligence facilitates automatization of
   analytical modelling and contributes to prediction, diagnostics and
   treatment of diseases. This article presents an overview of the
   application of artificial intelligence in dementia research. Recent
   findings Machine learning and its branch Deep Learning are widely used
   in research to support in diagnosis and prediction of dementia. Deep
   Learning models in certain tasks often result in better accuracy of
   detection and prediction of dementia than traditional machine learning
   methods, but they are more costly in terms of run times and hardware
   requirements. Both machine learning and Deep Learning models have their
   own strengths and limitations. Currently, there are few datasets with
   limited data available to train machine learning models. There are very
   few commercial applications of machine learning in medical practice to
   date, mostly represented by mobile applications, which include
   questionnaires and psychometric assessments with limited machine
   learning data processing. Application of machine learning technologies
   in detection and prediction of dementia may provide an advantage to
   psychiatry and neurology by promoting a better understanding of the
   nature of the disease and more accurate evidence-based processes that
   are reproducible and standardized.},
DOI = {10.1097/YCO.0000000000000768},
ISSN = {0951-7367},
EISSN = {1473-6578},
ResearcherID-Numbers = {Medvedev, Oleg/AAO-8339-2020
   Medvedev, Oleg N./AAO-8339-2020
   Krishnamurthi, Rita/AAU-7841-2021},
ORCID-Numbers = {Medvedev, Oleg N./0000-0002-2167-5002
   },
Unique-ID = {WOS:000747379200007},
}

@article{ WOS:000795191700002,
Author = {Basnet, Ram B. and Johnson, Clayton and Doleck, Tenzin},
Title = {Dropout prediction in Moocs using deep learning and machine learning},
Journal = {EDUCATION AND INFORMATION TECHNOLOGIES},
Year = {2022},
Volume = {27},
Number = {8},
Pages = {11499-11513},
Month = {SEP},
Abstract = {The nature of teaching and learning has evolved over the years,
   especially as technology has evolved. Innovative application of
   educational analytics has gained momentum. Indeed, predictive analytics
   have become increasingly salient in education. Considering the
   prevalence of learner-system interaction data and the potential value of
   such data, it is not surprising that significant scholarly attention has
   been directed at understanding ways of drawing insights from educational
   data. Although prior literature on educational big data recognizes the
   utility of deep learning and machine learning methods, little research
   examines both deep learning and machine learning together, and the
   differences in predictive performance have been relatively understudied.
   This paper aims to present a comprehensive comparison of predictive
   performance using deep learning and machine learning. Specifically, we
   use educational big data in the context of predicting dropout in MOOCs.
   We find that machine learning classifiers can predict equally well as
   deep learning classifiers. This research advances our understanding of
   the use of deep learning and machine learning in optimizing dropout
   prediction performance models.},
DOI = {10.1007/s10639-022-11068-7},
EarlyAccessDate = {MAY 2022},
ISSN = {1360-2357},
EISSN = {1573-7608},
Unique-ID = {WOS:000795191700002},
}

@article{ WOS:000887333600001,
Author = {Qin, Yifan and Wu, Jinlong and Xiao, Wen and Wang, Kun and Huang, Anbing
   and Liu, Bowen and Yu, Jingxuan and Li, Chuhao and Yu, Fengyu and Ren,
   Zhanbing},
Title = {Machine Learning Models for Data-Driven Prediction of Diabetes by
   Lifestyle Type},
Journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH},
Year = {2022},
Volume = {19},
Number = {22},
Month = {NOV},
Abstract = {The prevalence of diabetes has been increasing in recent years, and
   previous research has found that machine-learning models are good
   diabetes prediction tools. The purpose of this study was to compare the
   efficacy of five different machine-learning models for diabetes
   prediction using lifestyle data from the National Health and Nutrition
   Examination Survey (NHANES) database. The 1999-2020 NHANES database
   yielded data on 17,833 individuals data based on demographic
   characteristics and lifestyle-related variables. To screen training data
   for machine models, the Akaike Information Criterion (AIC) forward
   propagation algorithm was utilized. For predicting diabetes, five
   machine-learning models (CATBoost, XGBoost, Random Forest (RF), Logistic
   Regression (LR), and Support Vector Machine (SVM)) were developed. Model
   performance was evaluated using accuracy, sensitivity, specificity,
   precision, F1 score, and receiver operating characteristic (ROC) curve.
   Among the five machine-learning models, the dietary intake levels of
   energy, carbohydrate, and fat, contributed the most to the prediction of
   diabetes patients. In terms of model performance, CATBoost ranks higher
   than RF, LG, XGBoost, and SVM. The best-performing machine-learning
   model among the five is CATBoost, which achieves an accuracy of 82.1\%
   and an AUC of 0.83. Machine-learning models based on NHANES data can
   assist medical institutions in identifying diabetes patients.},
DOI = {10.3390/ijerph192215027},
Article-Number = {15027},
EISSN = {1660-4601},
ResearcherID-Numbers = {Ren, Zhanbing/AAL-8932-2020
   Liu, Bowen/AAH-7725-2021
   Wu, Jinlong/X-3225-2018
   Kun, Wang/KVA-4884-2024},
ORCID-Numbers = {Wu, Jinlong/0000-0002-0694-3144
   Ren, Zhanbing/0000-0002-2999-9239
   Li, Chuhao/0009-0007-6687-413X
   },
Unique-ID = {WOS:000887333600001},
}

@article{ WOS:000874639800001,
Author = {Ishioka, Sora and Fujiwara, Aya and Nakanowatari, Sunao and Takahashi,
   Lauren and Taniike, Toshiaki and Takahashi, Keisuke},
Title = {Designing Catalyst Descriptors for Machine Learning in Oxidative
   Coupling of Methane},
Journal = {ACS CATALYSIS},
Year = {2022},
Volume = {12},
Number = {19},
Pages = {11541-11546},
Month = {OCT 7},
Abstract = {Catalysts descriptors for representing catalytic activities have been
   challenging in regard to machine learning. Machine learning and catalyst
   big data generated from high-throughput experiments are combined to
   explore the catalyst descriptors. Catalyst descriptors are designed
   using the physical quantities from the periodic table in the oxidative
   coupling of methane (OCM) reaction. Machine learning unveils the five
   key physical quantities representing ethylene/ethane selectivity (C2s)
   in the OCM reaction, where machine learning predicted three catalysts to
   have high C2s values. Experiments confirm that the proposed three
   catalysts have high C2s values in the OCM reaction. Hence, the physical
   quantities can be used as alternative descriptors for designing
   heterogeneous catalysts.},
DOI = {10.1021/acscatal.2c03142},
ISSN = {2155-5435},
ResearcherID-Numbers = {Takahashi, Keisuke/Q-3874-2019
   Takahashi, Lauren/ABF-1017-2021},
ORCID-Numbers = {Takahashi, Lauren/0000-0001-9922-8889
   Takahashi, Keisuke/0000-0002-9328-1694
   },
Unique-ID = {WOS:000874639800001},
}

@article{ WOS:000792068500001,
Author = {Fardouly, Jasmine and Crosby, Ross D. and Sukunesan, Suku},
Title = {Potential benefits and limitations of machine learning in the field of
   eating disorders: current research and future directions},
Journal = {JOURNAL OF EATING DISORDERS},
Year = {2022},
Volume = {10},
Number = {1},
Month = {MAY 8},
Abstract = {Plain English Summary Machine learning models are computer algorithms
   that learn from data to reach an optimal solution for a problem. These
   algorithms provide exciting potential for the accurate, accessible, and
   cost-effective early identification, prevention, and treatment of eating
   disorders, but this potential is just beginning to be explored. Research
   to date has mainly used machine learning to predict women's eating
   disorder status with relatively high levels of accuracy from responses
   to validated surveys, social media posts, or neuroimaging data. These
   studies show potential for the use of machine learning in the field, but
   we are far from using these methods in practice. Useful avenues for
   future research include the use of machine learning to personalise
   prevention and treatment options, provide ecological momentary
   interventions via smartphones, and to aid clinicians with their
   treatment fidelity and effectiveness. More research is needed with large
   samples of diverse participants to ensure that machine learning models
   are accurate, unbiased, and generalisable to all people with eating
   disorders. There are limitations and ethical considerations with using
   these methods in practice. If accurate and generalisable machine
   learning models can be created in the field of eating disorders, it
   could improve the way we identify, prevent, and treat these debilitating
   disorders.
   Advances in machine learning and digital data provide vast potential for
   mental health predictions. However, research using machine learning in
   the field of eating disorders is just beginning to emerge. This paper
   provides a narrative review of existing research and explores potential
   benefits, limitations, and ethical considerations of using machine
   learning to aid in the detection, prevention, and treatment of eating
   disorders. Current research primarily uses machine learning to predict
   eating disorder status from females' responses to validated surveys,
   social media posts, or neuroimaging data often with relatively high
   levels of accuracy. This early work provides evidence for the potential
   of machine learning to improve current eating disorder screening
   methods. However, the ability of these algorithms to generalise to other
   samples or be used on a mass scale is only beginning to be explored. One
   key benefit of machine learning over traditional statistical methods is
   the ability of machine learning to simultaneously examine large numbers
   (100s to 1000s) of multimodal predictors and their complex non-linear
   interactions, but few studies have explored this potential in the field
   of eating disorders. Machine learning is also being used to develop
   chatbots to provide psychoeducation and coping skills training around
   body image and eating disorders, with implications for early
   intervention. The use of machine learning to personalise treatment
   options, provide ecological momentary interventions, and aid the work of
   clinicians is also discussed. Machine learning provides vast potential
   for the accurate, rapid, and cost-effective detection, prevention, and
   treatment of eating disorders. More research is needed with large
   samples of diverse participants to ensure that machine learning models
   are accurate, unbiased, and generalisable to all people with eating
   disorders. There are important limitations and ethical considerations
   with utilising machine learning methods in practice. Thus, rather than a
   magical solution, machine learning should be seen as an important tool
   to aid the work of researchers, and eventually clinicians, in the early
   identification, prevention, and treatment of eating disorders.},
DOI = {10.1186/s40337-022-00581-2},
Article-Number = {66},
ISSN = {2050-2974},
ResearcherID-Numbers = {Forman, Evan/I-1042-2012
   sinnappan, Sukunesan/AFQ-7313-2022
   Fardouly, Jasmine/F-5430-2014},
ORCID-Numbers = {Fardouly, Jasmine/0000-0001-5648-6757},
Unique-ID = {WOS:000792068500001},
}

@article{ WOS:000795188600001,
Author = {Cong, Zicun and Luo, Xuan and Pei, Jian and Zhu, Feida and Zhang, Yong},
Title = {Data pricing in machine learning pipelines},
Journal = {KNOWLEDGE AND INFORMATION SYSTEMS},
Year = {2022},
Volume = {64},
Number = {6},
Pages = {1417-1455},
Month = {JUN},
Abstract = {Machine learning is disruptive. At the same time, machine learning can
   only succeed by collaboration among many parties in multiple steps
   naturally as pipelines in an eco-system, such as collecting data for
   possible machine learning applications, collaboratively training models
   by multiple parties and delivering machine learning services to end
   users. Data are critical and penetrating in the whole machine learning
   pipelines. As machine learning pipelines involve many parties and, in
   order to be successful, have to form a constructive and dynamic
   eco-system, marketplaces and data pricing are fundamental in connecting
   and facilitating those many parties. In this article, we survey the
   principles and the latest research development of data pricing in
   machine learning pipelines. We start with a brief review of data
   marketplaces and pricing desiderata. Then, we focus on pricing in three
   important steps in machine learning pipelines. To understand pricing in
   the step of training data collection, we review pricing raw data sets
   and data labels. We also investigate pricing in the step of
   collaborative training of machine learning models and overview pricing
   machine learning models for end users in the step of machine learning
   deployment. We also discuss a series of possible future directions.},
DOI = {10.1007/s10115-022-01679-4},
EarlyAccessDate = {MAY 2022},
ISSN = {0219-1377},
EISSN = {0219-3116},
ResearcherID-Numbers = {ZHU, Feida/E-8579-2012
   Luo, Xuan/JXM-5678-2024
   Zhu, Feida/E-8579-2012},
ORCID-Numbers = {Pei, Jian/0000-0002-2200-8711
   Luo, Xuan/0000-0003-0849-1396
   Zhang, Yong/0000-0002-0238-0719
   Zhu, Feida/0000-0001-6077-4356},
Unique-ID = {WOS:000795188600001},
}

@article{ WOS:000993711000001,
Author = {Ren, Yi-Shuai and Ma, Chao-Qun and Kong, Xiao-Lin and Baltas,
   Konstantinos and Zureigat, Qasim},
Title = {Past, present, and future of the application of machine learning in
   cryptocurrency research},
Journal = {RESEARCH IN INTERNATIONAL BUSINESS AND FINANCE},
Year = {2022},
Volume = {63},
Month = {DEC},
Abstract = {Cryptocurrency has captured the interest of financial scholars and
   become a major research topic in blockchain. In cryptocurrency research,
   the use of machine learning algorithms is enabled by the presence of
   many types of data and abundant resources. However, there is currently
   no comprehensive review on cryptocurrencies using machine learning.
   Therefore, we collect papers on cryptocurrency-related using machine
   learning in the web of science database, and summarize these papers
   according to the algorithm, and draw the following conclusions: (1) The
   application of machine learning for cryptocurrencies research is
   increasing year over year; (2) Predicting cryptocurrency price trends
   and income fluctuations is the most relevant research topic; (3) The
   machine learning algorithm utilized in cryptocurrency research is not
   unique, and the practise of combining multiple machine learning
   approaches has emerged; (4) Concerns such as overfitting and
   interpretability still persist with machine learning methods. Finally,
   we suggest future research directions.},
DOI = {10.1016/j.ribaf.2022.101799},
Article-Number = {101799},
ISSN = {0275-5319},
EISSN = {1878-3384},
ResearcherID-Numbers = {Ren, Yi-Shuai/AAB-9519-2019},
Unique-ID = {WOS:000993711000001},
}

@article{ WOS:000741129900001,
Author = {Kaptan, Shreyas and Vattulainen, Ilpo},
Title = {Machine learning in the analysis of biomolecular simulations},
Journal = {ADVANCES IN PHYSICS-X},
Year = {2022},
Volume = {7},
Number = {1},
Month = {DEC 31},
Abstract = {Machine learning has rapidly become a key method for the analysis and
   organization of large-scale data in all scientific disciplines. In life
   sciences, the use of machine learning techniques is a particularly
   appealing idea since the enormous capacity of computational
   infrastructures generates terabytes of data through millisecond
   simulations of atomistic and molecular-scale biomolecular systems. Due
   to this explosion of data, the automation, reproducibility, and
   objectivity provided by machine learning methods are highly desirable
   features in the analysis of complex systems. In this review, we focus on
   the use of machine learning in biomolecular simulations. We discuss the
   main categories of machine learning tasks, such as dimensionality
   reduction, clustering, regression, and classification used in the
   analysis of simulation data. We then introduce the most popular classes
   of techniques involved in these tasks for the purpose of enhanced
   sampling, coordinate discovery, and structure prediction. Whenever
   possible, we explain the scope and limitations of machine learning
   approaches, and we discuss examples of applications of these techniques.},
DOI = {10.1080/23746149.2021.2006080},
ISSN = {2374-6149},
ResearcherID-Numbers = {Vattulainen, Ilpo/G-4224-2014
   },
ORCID-Numbers = {Vattulainen, Ilpo/0000-0001-7408-3214
   Kaptan, Shreyas/0000-0003-1265-4557},
Unique-ID = {WOS:000741129900001},
}

@article{ WOS:000839022000001,
Author = {Di Salvo, Cristina},
Title = {Improving Results of Existing Groundwater Numerical Models Using Machine
   Learning Techniques: A Review},
Journal = {WATER},
Year = {2022},
Volume = {14},
Number = {15},
Month = {AUG},
Abstract = {This paper presents a review of papers specifically focused on the use
   of both numerical and machine learning methods for groundwater level
   modelling. In the reviewed papers, machine learning models (also called
   data-driven models) are used to improve the prediction or speed process
   of existing numerical modelling. When long runtimes inhibit the use of
   numerical models, machine learning models can be a valid alternative,
   capable of reducing the time for model development and calibration
   without sacrificing accuracy of detail in groundwater level forecasting.
   The results of this review highlight that machine learning models do not
   offer a complete representation of the physical system, such as flux
   estimates or total water balance and, thus, cannot be used to substitute
   numerical models in large study areas; however, they are affordable
   tools to improve predictions at specific observation wells. Numerical
   and machine learning models can be successfully used as complementary to
   each other as a powerful groundwater management tool. The machine
   learning techniques can be used to improve calibration of numerical
   models, whereas results of numerical models allow us to understand the
   physical system and select proper input variables for machine learning
   models. Machine learning models can be integrated in decision-making
   processes when rapid and effective solutions for groundwater management
   need to be considered. Finally, machine learning models are
   computationally efficient tools to correct head error prediction of
   numerical models.},
DOI = {10.3390/w14152307},
Article-Number = {2307},
EISSN = {2073-4441},
ResearcherID-Numbers = {DISALVO, CRISTINA/ABB-1287-2020},
ORCID-Numbers = {DISALVO, CRISTINA/0000-0002-5119-7400
   },
Unique-ID = {WOS:000839022000001},
}

@article{ WOS:000844738900003,
Author = {Rodriguez-Perez, Raquel and Miljkovic, Filip and Bajorath, Juergen},
Title = {Machine Learning in Chemoinformatics and Medicinal Chemistry},
Journal = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE},
Year = {2022},
Volume = {5},
Pages = {43-65},
Abstract = {In chemoinformatics and medicinal chemistry, machine learning has
   evolved into an important approach. In recent years, increasing
   computational resources and new deep learning algorithms have put
   machine learning onto a new level, addressing previously unmet
   challenges in pharmaceutical research. In silico approaches for compound
   activity predictions, de novo design, and reaction modeling have been
   further advanced by new algorithmic developments and the emergence of
   big data in the field. Herein, novel applications of machine learning
   and deep learning in chemoinformatics and medicinal chemistry are
   reviewed. Opportunities and challenges for new methods and applications
   are discussed, placing emphasis on proper baseline comparisons, robust
   validation methodologies, and new applicability domains.},
DOI = {10.1146/annurev-biodatasci-122120-124216},
ISSN = {2574-3414},
ResearcherID-Numbers = {Rodríguez-Pérez, Raquel/C-7483-2019},
Unique-ID = {WOS:000844738900003},
}

@article{ WOS:000867398400004,
Author = {Aloraini, Fatimah and Javed, Amir and Rana, Omer and Burnap, Pete},
Title = {Adversarial machine learning in IoT from an insider point of view},
Journal = {JOURNAL OF INFORMATION SECURITY AND APPLICATIONS},
Year = {2022},
Volume = {70},
Month = {NOV},
Abstract = {With the rapid progress and significant successes in various
   applications, machine learning has been considered a crucial component
   in the Internet of Things ecosystem. However, machine learning models
   have recently been vulnerable to carefully crafted perturbations,
   so-called adversarial attacks. A capable insider adversary can subvert
   the machine learning model at either the training or testing phase,
   causing them to behave differently. The vulnerability of machine
   learning to adversarial attacks becomes one of the significant risks.
   Therefore, there is a need to secure machine learning models enabling
   the safe adoption in malicious insider cases. This paper reviews and
   organizes the body of knowledge in adversarial attacks and defense
   presented in IoT literature from an insider adversary point of view. We
   proposed a taxonomy of adversarial methods against machine learning
   models that an insider can exploit. Under the taxonomy, we discuss how
   these methods can be applied in real-life IoT applications. Finally, we
   explore defensive methods against adversarial attacks. We believe this
   can draw a comprehensive overview of the scattered research works to
   raise awareness of the existing insider threats landscape and encourages
   others to safeguard machine learning models against insider threats in
   the IoT ecosystem.},
DOI = {10.1016/j.jisa.2022.103341},
EarlyAccessDate = {OCT 2022},
Article-Number = {103341},
ISSN = {2214-2126},
EISSN = {2214-2134},
ResearcherID-Numbers = {Rana, Omer/E-4314-2015
   Burnap, Pete/HTN-3478-2023
   Rana, Omer/AAP-8523-2020},
ORCID-Numbers = {Rana, Omer/0000-0003-3597-2646
   Aloraini, Fatimah/0000-0001-5494-0661
   },
Unique-ID = {WOS:000867398400004},
}

@article{ WOS:000765072200001,
Author = {Wong, Lauren J. and Michaels, Alan J.},
Title = {Transfer Learning for Radio Frequency Machine Learning: A Taxonomy and
   Survey},
Journal = {SENSORS},
Year = {2022},
Volume = {22},
Number = {4},
Month = {FEB},
Abstract = {Transfer learning is a pervasive technology in computer vision and
   natural language processing fields, yielding exponential performance
   improvements by leveraging prior knowledge gained from data with
   different distributions. However, while recent works seek to mature
   machine learning and deep learning techniques in applications related to
   wireless communications, a field loosely termed radio frequency machine
   learning, few have demonstrated the use of transfer learning techniques
   for yielding performance gains, improved generalization, or to address
   concerns of training data costs. With modifications to existing transfer
   learning taxonomies constructed to support transfer learning in other
   modalities, this paper presents a tailored taxonomy for radio frequency
   applications, yielding a consistent framework that can be used to
   compare and contrast existing and future works. This work offers such a
   taxonomy, discusses the small body of existing works in transfer
   learning for radio frequency machine learning, and outlines directions
   where future research is needed to mature the field.},
DOI = {10.3390/s22041416},
Article-Number = {1416},
EISSN = {1424-8220},
ORCID-Numbers = {Wong, Lauren/0000-0001-5896-0176},
Unique-ID = {WOS:000765072200001},
}

@article{ WOS:000827662800001,
Author = {Borch, Christian and Hee Min, Bo},
Title = {Toward a sociology of machine learning explainability: Human-machine
   interaction in deep neural network-based automated trading},
Journal = {BIG DATA \& SOCIETY},
Year = {2022},
Volume = {9},
Number = {2},
Month = {JUL},
Abstract = {Machine learning systems are making considerable inroads in society
   owing to their ability to recognize and predict patterns. However, the
   decision-making logic of some widely used machine learning models, such
   as deep neural networks, is characterized by opacity, thereby rendering
   them exceedingly difficult for humans to understand and explain and, as
   a result, potentially risky to use. Considering the importance of
   addressing this opacity, this paper calls for research that studies
   empirically and theoretically how machine learning experts and users
   seek to attain machine learning explainability. Focusing on automated
   trading, we take steps in this direction by analyzing a trading firm's
   quest for explaining its deep neural network system's actionable
   predictions. We demonstrate that this explainability effort involves a
   particular form of human-machine interaction that contains both
   anthropomorphic and technomorphic elements. We discuss this attempt to
   attain machine learning explainability in light of reflections on
   cross-species companionship and consider it an example of human-machine
   companionship.},
DOI = {10.1177/20539517221111361},
Article-Number = {20539517221111361},
ISSN = {2053-9517},
ORCID-Numbers = {Borch, Christian/0000-0001-8217-5880},
Unique-ID = {WOS:000827662800001},
}

@article{ WOS:000597401000025,
Author = {Wang, Haojie and Zhang, Limin and Yin, Kesheng and Luo, Hongyu and Li,
   Jinhui},
Title = {Landslide identification using machine learning},
Journal = {GEOSCIENCE FRONTIERS},
Year = {2021},
Volume = {12},
Number = {1},
Pages = {351-364},
Month = {JAN},
Abstract = {Landslide identification is critical for risk assessment and mitigation.
   This paper proposes a novel machine-learning and deep-learning method to
   identify natural-terrain landslides using integrated geodatabases.
   First, landslide-related data are compiled, including topographic data,
   geological data and rainfall-related data. Then, three integrated
   geodatabases are established; namely, Recent Landslide Database (RecLD),
   Relict Landslide Database (RelLD) and Joint Landslide Database (JLD).
   After that, five machine learning and deep learning algorithms,
   including logistic regression (LR), support vector machine (SVM), random
   forest (RF), boosting methods and convolutional neural network (CNN),
   are utilized and evaluated on each database. A case study in Lantau,
   Hong Kong, is conducted to demonstrate the application of the proposed
   method. From the results of the case study, CNN achieves an
   identification accuracy of 92.5\% on RecLD, and outperforms other
   algorithms due to its strengths in feature extraction and multi
   dimensional data processing. Boosting methods come second in terms of
   accuracy, followed by RF, LR and SVM. By using machine learning and deep
   learning techniques, the proposed landslide identification method shows
   outstanding robustness and great potential in tackling the landslide
   identification problem.},
DOI = {10.1016/j.gsf.2020.02.012},
ISSN = {1674-9871},
ResearcherID-Numbers = {Wang, Hesheng/P-3192-2015
   Zhang, Li-Min/G-9891-2011
   LI, Jinhui/HDN-1258-2022
   Luo, Hongyu/LVR-7328-2024
   Yin, Khin/ABA-7368-2020},
ORCID-Numbers = {Yin, Kesheng/0000-0003-1350-4497
   Luo, Hongyu/0000-0002-2986-9418
   },
Unique-ID = {WOS:000597401000025},
}

@article{ WOS:000864033400001,
Author = {Audus, Debra J. and McDannald, Austin and DeCost, Brian},
Title = {Leveraging Theory for Enhanced Machine Learning},
Journal = {ACS MACRO LETTERS},
Year = {2022},
Month = {2022 AUG 26},
Abstract = {The application of machine learning to the materials domain has
   traditionally struggled with two major challenges: a lack of large,
   curated data sets and the need to understand the physics behind the
   machine-learning prediction. The former problem is particularly acute in
   the polymers domain. Here we aim to simultaneously tackle these
   challenges through the incorporation of scientific knowledge, thus,
   providing improved predictions for smaller data sets, both under
   interpolation and extrapolation, and a degree of explainability. We
   focus on imperfect theories, as they are often readily available and
   easier to interpret. Using a system of a polymer in different solvent
   qualities, we explore numerous methods for incorporating theory into
   machine learning using different machine-learning models, including
   Gaussian process regression. Ultimately, we find that encoding the
   functional form of the theory performs best followed by an encoding of
   the numeric values of the theory.},
DOI = {10.1021/acsmacrolett.2c00369},
EarlyAccessDate = {AUG 2022},
EISSN = {2161-1653},
ORCID-Numbers = {DeCost, Brian/0000-0002-3459-5888
   Audus, Debra/0000-0002-5937-7721},
Unique-ID = {WOS:000864033400001},
}

@article{ WOS:000771334100001,
Author = {De Iaco, Sandra and Hristopulos, Dionissios T. and Lin, Guang},
Title = {Special Issue: Geostatistics and Machine Learning},
Journal = {MATHEMATICAL GEOSCIENCES},
Year = {2022},
Volume = {54},
Number = {3, SI},
Pages = {459-465},
Month = {APR},
Abstract = {Recent years have seen a steady growth in the number of papers that
   apply machine learning methods to problems in the earth sciences.
   Although they have different origins, machine learning and geostatistics
   share concepts and methods. For example, the kriging formalism can be
   cast in the machine learning framework of Gaussian process regression.
   Machine learning, with its focus on algorithms and ability to seek,
   identify, and exploit hidden structures in big data sets, is providing
   new tools for exploration and prediction in the earth sciences.
   Geostatistics, on the other hand, offers interpretable models of spatial
   (and spatiotemporal) dependence. This special issue on Geostatistics and
   Machine Learning aims to investigate applications of machine learning
   methods as well as hybrid approaches combining machine learning and
   geostatistics which advance our understanding and predictive ability of
   spatial processes.},
DOI = {10.1007/s11004-022-09998-6},
EarlyAccessDate = {MAR 2022},
ISSN = {1874-8961},
EISSN = {1874-8953},
ResearcherID-Numbers = {Lin, Guang/ABB-2145-2021
   Hristopulos, Dionisis/Q-4386-2019
   De Iaco, Sandra/AAJ-3432-2020
   },
ORCID-Numbers = {HRISTOPULOS, DIONISSIOS/0000-0002-5189-5612
   Lin, Guang/0000-0002-0976-1987
   De Iaco, Sandra/0000-0003-1820-2068},
Unique-ID = {WOS:000771334100001},
}

@article{ WOS:000907756500001,
Author = {Su, Yu-Sheng and Lin, Yu-Da and Liu, Tai-Quan},
Title = {Applying machine learning technologies to explore students' learning
   features and performance prediction},
Journal = {FRONTIERS IN NEUROSCIENCE},
Year = {2022},
Volume = {16},
Month = {DEC 22},
Abstract = {To understand students' learning behaviors, this study uses machine
   learning technologies to analyze the data of interactive learning
   environments, and then predicts students' learning outcomes. This study
   adopted a variety of machine learning classification methods, quizzes,
   and programming system logs, found that students' learning
   characteristics were correlated with their learning performance when
   they encountered similar programming practice. In this study, we used
   random forest (RF), support vector machine (SVM), logistic regression
   (LR), and neural network (NN) algorithms to predict whether students
   would submit on time for the course. Among them, the NN algorithm showed
   the best prediction results. Education-related data can be predicted by
   machine learning techniques, and different machine learning models with
   different hyperparameters can be used to obtain better results.},
DOI = {10.3389/fnins.2022.1018005},
Article-Number = {1018005},
EISSN = {1662-453X},
ResearcherID-Numbers = {Su, Yu-Sheng/ABD-3489-2020
   },
ORCID-Numbers = {Su, Yu-Sheng/0009-0005-3601-1208},
Unique-ID = {WOS:000907756500001},
}

@article{ WOS:000910903500001,
Author = {Kaur, Manpreet and Rattan, Dhavleesh},
Title = {A systematic literature review on the use of machine learning in code
   clone research},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2023},
Volume = {47},
Month = {FEB},
Abstract = {Context: Research related to code clones includes detection of clones in
   software systems, analysis, visualization and management of clones.
   Detection of semantic clones and management of clones have attracted use
   of machine learning techniques in code clone related research.Objective:
   The aim of this study is to report the extent of machine learning usage
   in code clone related research areas.Method: The paper uses a systematic
   review method to report the use of machine learning in research related
   to code clones. The study considers a comprehensive set of 57 articles
   published in leading conferences, workshops and journals.Results: Code
   clone related research using machine learning techniques is classified
   into different categories. Machine learning and deep learning algorithms
   used in the code clone research are reported. The datasets, features
   used to train machine learning models and metrics used to evaluate
   machine learning algorithms are reported. The comparative results of
   various machine learning algorithms presented in primary studies are
   reported.Conclusion: The research will help to identify the status of
   using machine learning in different code clone related research areas.
   We identify the need of more empirical studies to assess the benefits of
   machine learning in code clone research and give recommendations for
   future research.(c) 2022 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.cosrev.2022.100528},
EarlyAccessDate = {DEC 2022},
Article-Number = {100528},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {kaur, manpreet/KLZ-4413-2024},
ORCID-Numbers = {Rattan, Dhavleesh/0000-0002-6295-5078
   Kaur, Manpreet/0000-0002-6880-0480
   },
Unique-ID = {WOS:000910903500001},
}

@article{ WOS:000912711800001,
Author = {Wei, Yi and Sekiya, Yuji},
Title = {Sufficiency of Ensemble Machine Learning Methods for Phishing Websites
   Detection},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {124103-124113},
Abstract = {Phishing is a kind of worldwide spread cybercrime that uses disguised
   websites to trick users into downloading malware or providing personally
   sensitive information to attackers. With the rapid development of
   artificial intelligence, more and more researchers in the cybersecurity
   field utilize machine learning and deep learning algorithms to classify
   phishing websites. In order to compare the performances of various
   machine learning and deep learning methods, several experiments are
   conducted in this study. According to the experimental results, ensemble
   machine learning algorithms stand out among other candidates in both
   detection accuracy and computational consumption. Furthermore, the
   ensemble architectures still provide impressive capability when the
   amount of features decreases sharply in the dataset. Subsequently, the
   paper discusses the factors why ensemble machine learning methods are
   more suitable for the binary phishing classification challenge in
   up-date training and real-time detecting environment, which reflects the
   sufficiency of ensemble machine learning methods in anti-phishing
   techniques.},
DOI = {10.1109/ACCESS.2022.3224781},
ISSN = {2169-3536},
ORCID-Numbers = {Sekiya, Yuji/0009-0006-6287-9606},
Unique-ID = {WOS:000912711800001},
}

@article{ WOS:000901956500001,
Author = {Omar, Mohamed and Sayed, Ehab and Abdalmagid, Mohamed and Bilgin, Berker
   and Bakr, Mohamed H. and Emadi, Ali},
Title = {Review of Machine Learning Applications to the Modeling and Design
   Optimization of Switched Reluctance Motors},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {130444-130468},
Abstract = {This work presents a comprehensive review of the developments in using
   Machine Learning (ML)-based algorithms for the modeling and design
   optimization of switched reluctance motors (SRMs). We reviewed Machine
   Learning-based numerical and analytical approaches used in modeling
   SRMs. We showed the difference between the supervised, unsupervised and
   reinforcement learning algorithms. More focus is placed on supervised
   learning algorithms as they are the most used algorithms in this area.
   The supervised learning algorithms studied in this work include the
   feedforward neural networks, recurrent neural networks, support vector
   machines, extreme learning machines, and Bayesian networks. This work
   also discusses several essential aspects of the considered machine
   learning algorithms, such as core concept, structure, and computational
   time. It also surveys sample data acquisition methods and data size.
   Finally, comparisons between the different considered ML-based
   algorithms are conducted in terms of electric motor type, dataset inputs
   and outputs, and algorithm's structure and accuracy to provide a summary
   overview of the ML-based algorithms for SRMs modeling and design.},
DOI = {10.1109/ACCESS.2022.3229043},
ISSN = {2169-3536},
ResearcherID-Numbers = {Bilgin, Berker/U-2346-2019
   emadi, ali/AAL-3732-2020
   },
ORCID-Numbers = {Bilgin, Berker/0000-0001-5103-0072
   Omar, Mohamed/0000-0002-9823-5224},
Unique-ID = {WOS:000901956500001},
}

@article{ WOS:000431395700001,
Author = {Bottou, Leon and Curtis, Frank E. and Nocedal, Jorge},
Title = {Optimization Methods for Large-Scale Machine Learning},
Journal = {SIAM REVIEW},
Year = {2018},
Volume = {60},
Number = {2},
Pages = {223-311},
Month = {JUN},
Abstract = {This paper provides a review and commentary on the past, present, and
   future of numerical optimization algorithms in the context of machine
   learning applications. Through case studies on text classification and
   the training of deep neural networks, we discuss how optimization
   problems arise in machine learning and what makes them challenging. A
   major theme of our study is that large-scale machine learning represents
   a distinctive setting in which the stochastic gradient (SG) method has
   traditionally played a central role while conventional gradient-based
   nonlinear optimization techniques typically falter. Based on this
   viewpoint, we present a comprehensive theory of a straightforward, yet
   versatile SG algorithm, discuss its practical behavior, and highlight
   opportunities for designing algorithms with improved performance. This
   leads to a discussion about the next generation of optimization methods
   for large-scale machine learning, including an investigation of two main
   streams of research on techniques that diminish noise in the stochastic
   directions and methods that make use of second-order derivative
   approximations.},
DOI = {10.1137/16M1080173},
ISSN = {0036-1445},
EISSN = {1095-7200},
ResearcherID-Numbers = {Nocedal, Jorge/B-7255-2009
   Curtis, Frank/P-9235-2019},
ORCID-Numbers = {Curtis, Frank E./0000-0001-7214-9187
   },
Unique-ID = {WOS:000431395700001},
}

@article{ WOS:000517757300010,
Author = {Choi, Rene Y. and Coyner, Aaron S. and Kalpathy-Cramer, Jayashree and
   Chiang, Michael F. and Campbell, J. Peter},
Title = {Introduction to Machine Learning, Neural Networks, and Deep Learning},
Journal = {TRANSLATIONAL VISION SCIENCE \& TECHNOLOGY},
Year = {2020},
Volume = {9},
Number = {2},
Month = {JAN},
Abstract = {Purpose: To present an overview of current machine learning methods and
   their use in medical research, focusing on select machine learning
   techniques, best practices, and deep learning.
   Methods: A systematic literature search in PubMed was performed for
   articles pertinent to the topic of artificial intelligence methods used
   in medicine with an emphasis on ophthalmology.
   Results: A review of machine learning and deep learning methodology for
   the audience without an extensive technical computer programming
   background.
   Conclusions: Artificial intelligence has a promising future in medicine;
   however, many challenges remain.
   Translational Relevance: The aim of this review article is to provide
   the nontechnical readers a layman's explanation of the machine learning
   methods being used in medicine today. The goal is to provide the reader
   a better understanding of the potential and challenges of artificial
   intelligence within the field of medicine.},
DOI = {10.1167/tvst.9.2.14},
Article-Number = {14},
ISSN = {2164-2591},
ResearcherID-Numbers = {Coyner, Aaron/W-5592-2019
   },
ORCID-Numbers = {Coyner, Aaron/0000-0003-3261-1909
   Chiang, Michael/0000-0002-8172-7636
   Kalpathy-Cramer, Jayashree/0000-0001-8906-9618},
Unique-ID = {WOS:000517757300010},
}

@article{ WOS:000556899700018,
Author = {Merghadi, Abdelaziz and Yunus, Ali P. and Dou, Jie and Whiteley, Jim and
   Binh ThaiPham and Dieu Tien Bui and Avtar, Ram and Abderrahmane,
   Boumezbeur},
Title = {Machine learning methods for landslide susceptibility studies: A
   comparative overview of algorithm performance},
Journal = {EARTH-SCIENCE REVIEWS},
Year = {2020},
Volume = {207},
Month = {AUG},
Abstract = {Landslides are one of the catastrophic natural hazards that occur in
   mountainous areas, leading to loss of life, damage to properties, and
   economic disruption. Landslide susceptibility models prepared in a
   Geographic Information System (GIS) integrated environment can be key
   for formulating disaster prevention measures and mitigating future risk.
   The accuracy and precision of susceptibility models is evolving rapidly
   from opinion-driven models and statistical learning toward increased use
   of machine learning techniques. Critical reviews on opinion-driven
   models and statistical learning in landslide susceptibility mapping have
   been published, but an overview of current machine learning models for
   landslide susceptibility studies, including background information on
   their operation, implementation, and performance is currently lacking.
   Here, we present an overview of the most popular machine learning
   techniques available for landslide susceptibility studies. We find that
   only a handful of researchers use machine learning techniques in
   landslide susceptibility mapping studies. Therefore, we present the
   architecture of various Machine Learning (ML) algorithms in plain
   language, so as to be understandable to a broad range of geoscientists.
   Furthermore, a comprehensive study comparing the performance of various
   ML algorithms is absent from the current literature, making an
   assessment of comparative performance and predictive capabilities
   difficult. We therefore undertake an extensive analysis and comparison
   between different ML techniques using a case study from Algeria. We
   summarize and discuss the algorithm's accuracies, advantages and
   limitations using a range of evaluation criteria. We note that
   tree-based ensemble algorithms achieve excellent results compared to
   other machine learning algorithms and that the Random Forest algorithm
   offers robust performance for accurate landslide susceptibility mapping
   with only a small number of adjustments required before training the
   model.},
DOI = {10.1016/j.earscirev.2020.103225},
Article-Number = {103225},
ISSN = {0012-8252},
EISSN = {1872-6828},
ResearcherID-Numbers = {Whiteley, Jim/AAA-4166-2020
   Avtar, Ram/C-7394-2012
   Dou, Jie/K-2809-2013
   ALI, YUNUS/O-7015-2019
   Merghadi, Abdelaziz/O-3574-2017
   PHAM, BINH THAI/H-8316-2018
   Tien Bui, Dieu/LGZ-9302-2024
   },
ORCID-Numbers = {Whiteley, Jim/0000-0001-5254-4817
   Pulpadan, YunusAli/0000-0001-6852-145X
   Merghadi, Abdelaziz/0000-0002-1041-6865
   Pulpadan, Yunus Ali/0000-0001-9545-0714
   THAI PHAM, BINH/0000-0001-9707-840X},
Unique-ID = {WOS:000556899700018},
}

@article{ WOS:000546615200005,
Author = {Mangalathu, Sujith and Hwang, Seong-Hoon and Jeon, Jong-Su},
Title = {Failure mode and effects analysis of RC members based on
   machine-learning-based SHapley Additive exPlanations (SHAP) approach},
Journal = {ENGINEERING STRUCTURES},
Year = {2020},
Volume = {219},
Month = {SEP 15},
Abstract = {Machine learning approaches can establish the complex and non-linear
   relationship among input and response variables for the seismic damage
   assessment of structures. However, lack of explainability of complex
   machine learning models prevents their use in such assessment. This
   paper uses extensive experimental databases to suggest random forest
   machine learning models for failure mode predictions of reinforced
   concrete columns and shear walls, employs the recently developed SHapley
   Additive exPlanations approach to rank input variables for
   identification of failure modes, and explains why the machine learning
   model predicts a specific failure mode for a given sample or experiment.
   A random forest model established provides an accuracy of 84\% and 86\%
   for unknown data of columns and shear walls, respectively. The geometric
   variables and reinforcement indices are critical parameters that
   influence failure modes. The study also reveals that existing strategies
   of failure mode identification based solely on geometric features are
   not enough to properly identify failure modes.},
DOI = {10.1016/j.engstruct.2020.110927},
Article-Number = {110927},
ISSN = {0141-0296},
EISSN = {1873-7323},
ResearcherID-Numbers = {Jeon, Jong-Su/R-4622-2019
   Hwang, Seong-Hoon/O-3523-2015
   Mangalathu, Sujith/I-3020-2017},
ORCID-Numbers = {Jeon, Jong-Su/0000-0001-6657-7265
   Hwang, Seong-Hoon/0000-0002-6098-5214
   Mangalathu, Sujith/0000-0001-8435-3919},
Unique-ID = {WOS:000546615200005},
}

@article{ WOS:000566732800001,
Author = {Jiang, Tammy and Gradus, Jaimie L. and Rosellini, Anthony J.},
Title = {Supervised Machine Learning: A Brief Primer},
Journal = {BEHAVIOR THERAPY},
Year = {2020},
Volume = {51},
Number = {5},
Pages = {675-687},
Month = {SEP},
Abstract = {Machine learning is increasingly used in mental health research and has
   the potential to advance our understanding of how to characterize,
   predict, and treat mental disorders and associated adverse health
   outcomes (e.g., suicidal behavior). Machine learning offers new tools to
   overcome challenges for which traditional statistical methods are not
   well-suited. This paper provides an overview of machine learning with a
   specific focus on supervised learning (i.e., methods that are designed
   to predict or classify an outcome of interest). Several common
   supervised learning methods are described, along with applied examples
   from the published literature. We also provide an overview of supervised
   learning model building, validation, and performance evaluation.
   Finally, challenges in creating robust and generalizable machine
   learning algorithms are discussed.},
ISSN = {0005-7894},
EISSN = {1878-1888},
ORCID-Numbers = {Rosellini, Anthony/0000-0002-8385-8621},
Unique-ID = {WOS:000566732800001},
}

@article{ WOS:000548811800021,
Author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
Title = {A Survey of Optimization Methods From a Machine Learning Perspective},
Journal = {IEEE TRANSACTIONS ON CYBERNETICS},
Year = {2020},
Volume = {50},
Number = {8},
Pages = {3668-3681},
Month = {AUG.},
Abstract = {Machine learning develops rapidly, which has made many theoretical
   breakthroughs and is widely applied in various fields. Optimization, as
   an important part of machine learning, has attracted much attention of
   researchers. With the exponential growth of data amount and the increase
   of model complexity, optimization methods in machine learning face more
   and more challenges. A lot of work on solving optimization problems or
   improving optimization methods in machine learning has been proposed
   successively. The systematic retrospect and summary of the optimization
   methods from the perspective of machine learning are of great
   significance, which can offer guidance for both developments of
   optimization and machine learning research. In this article, we first
   describe the optimization problems in machine learning. Then, we
   introduce the principles and progresses of commonly used optimization
   methods. Finally, we explore and give some challenges and open problems
   for the optimization in machine learning.},
DOI = {10.1109/TCYB.2019.2950779},
ISSN = {2168-2267},
EISSN = {2168-2275},
ORCID-Numbers = {Sun, Shiliang/0000-0001-7069-3752},
Unique-ID = {WOS:000548811800021},
}

@article{ WOS:000525389000018,
Author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke,
   Jochen},
Title = {Explainable Machine Learning for Scientific Insights and Discoveries},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {42200-42216},
Abstract = {Machine learning methods have been remarkably successful for a wide
   range of application areas in the extraction of essential information
   from data. An exciting and relatively recent development is the uptake
   of machine learning in the natural sciences, where the major goal is to
   obtain novel scientific insights and discoveries from observational or
   simulated data. A prerequisite for obtaining a scientific outcome is
   domain knowledge, which is needed to gain explainability, but also to
   enhance scientific consistency. In this article, we review explainable
   machine learning in view of applications in the natural sciences and
   discuss three core elements that we identified as relevant in this
   context: transparency, interpretability, and explainability. With
   respect to these core elements, we provide a survey of recent scientific
   works that incorporate machine learning and the way that explainable
   machine learning is used in combination with domain knowledge from the
   application areas.},
DOI = {10.1109/ACCESS.2020.2976199},
ISSN = {2169-3536},
ResearcherID-Numbers = {Duarte, Marco/G-6906-2012
   Roscher, Ribana/N-2238-2014
   },
ORCID-Numbers = {Roscher, Ribana/0000-0003-0094-6210
   Bohn, Bastian/0000-0001-5663-7527
   Garcke, Jochen/0000-0002-8334-3695},
Unique-ID = {WOS:000525389000018},
}

@article{ WOS:000582585700009,
Author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and
   Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
Title = {A Survey on Distributed Machine Learning},
Journal = {ACM COMPUTING SURVEYS},
Year = {2020},
Volume = {53},
Number = {2},
Month = {APR},
Abstract = {The demand for artificial intelligence has grown significantly over the
   past decade, and this growth has been fueled by advances in machine
   learning techniques and the ability to leverage hardware acceleration.
   However, to increase the quality of predictions and render machine
   learning solutions feasible for more complex applications, a substantial
   amount of training data is required. Although small machine learning
   models can be trained with modest amounts of data, the input for
   training larger models such as neural networks grows exponentially with
   the number of parameters. Since the demand for processing training data
   has outpaced the increase in computation power of computing machinery,
   there is a need for distributing the machine learning workload across
   multiple machines, and turning the centralized into a distributed
   system. These distributed systems present new challenges: first and
   foremost, the efficient parallelization of the training process and the
   creation of a coherent model. This article provides an extensive
   overview of the current state-of-the-art in the field by outlining the
   challenges and opportunities of distributed machine learning over
   conventional (centralized) machine learning, discussing the techniques
   used for distributed machine learning, and providing an overview of the
   systems that are available.},
DOI = {10.1145/3377454},
Article-Number = {30},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Rellermeyer, Jan/U-4316-2019
   },
ORCID-Numbers = {Rellermeyer, Jan/0000-0003-3791-7114
   Verbelen, Tim/0000-0003-2731-7262},
Unique-ID = {WOS:000582585700009},
}

@article{ WOS:000509755200009,
Author = {Meng, Tong and Jing, Xuyang and Yan, Zheng and Pedrycz, Witold},
Title = {A survey on machine learning for data fusion},
Journal = {INFORMATION FUSION},
Year = {2020},
Volume = {57},
Pages = {115-129},
Month = {MAY},
Abstract = {Data fusion is a prevalent way to deal with imperfect raw data for
   capturing reliable, valuable and accurate information. Comparing with a
   range of classical probabilistic data fusion techniques, machine
   learning method that automatically learns from past experiences without
   explicitly programming, remarkably renovates fusion techniques by
   offering the strong ability of computing and predicting. Nevertheless,
   the literature still lacks a thorough review of the recent advances of
   machine learning for data fusion. Therefore, it is beneficial to review
   and summarize the state of the art in order to gain a deep insight on
   how machine learning can benefit and optimize data fusion. In this
   paper, we provide a comprehensive survey on data fusion methods based on
   machine learning. We first offer a detailed introduction to the
   background of data fusion and machine learning in terms of definitions,
   applications, architectures, processes, and typical techniques. Then, we
   propose a number of requirements and employ them as criteria to review
   and evaluate the performance of existing fusion methods based on machine
   learning. Through the literature review, analysis and comparison, we
   finally come up with a number of open issues and propose future research
   directions in this field.},
DOI = {10.1016/j.inffus.2019.12.001},
ISSN = {1566-2535},
EISSN = {1872-6305},
ResearcherID-Numbers = {Zheng, Yan/HTP-6363-2023
   jing, xuyang/S-3183-2019
   Pedrycz, Witold/HJZ-2779-2023
   Yan, Zheng/AEV-7247-2022},
ORCID-Numbers = {Yan, Zheng/0000-0002-9697-2108},
Unique-ID = {WOS:000509755200009},
}

@article{ WOS:000537804900016,
Author = {Waring, Jonathan and Lindvall, Charlotta and Umeton, Renato},
Title = {Automated machine learning: Review of the state-of-the-art and
   opportunities for healthcare},
Journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE},
Year = {2020},
Volume = {104},
Month = {APR},
Abstract = {Objective: This work aims to provide a review of the existing literature
   in the field of automated machine learning (AutoML) to help healthcare
   professionals better utilize machine learning models ``off-the-shelf{''}
   with limited data science expertise. We also identify the potential
   opportunities and barriers to using AutoML in healthcare, as well as
   existing applications of AutoML in healthcare.
   Methods: Published papers, accompanied with code, describing work in the
   field of AutoML from both a computer science perspective or a biomedical
   informatics perspective were reviewed. We also provide a short summary
   of a series of AutoML challenges hosted by ChaLearn.
   Results: A review of 101 papers in the field of AutoML revealed that
   these automated techniques can match or improve upon expert human
   performance in certain machine learning tasks, often in a shorter amount
   of time. The main limitation of AutoML at this point is the ability to
   get these systems to work efficiently on a large scale, i.e. beyond
   small- and medium-size retrospective datasets.
   Discussion: The utilization of machine learning techniques has the
   demonstrated potential to improve health outcomes, cut healthcare costs,
   and advance clinical research. However, most hospitals are not currently
   deploying machine learning solutions. One reason for this is that health
   care professionals often lack the machine learning expertise that is
   necessary to build a successful model, deploy it in production, and
   integrate it with the clinical workflow. In order to make machine
   learning techniques easier to apply and to reduce the demand for human
   experts, automated machine learning (AutoML) has emerged as a growing
   field that seeks to automatically select, compose, and parametrize
   machine learning models, so as to achieve optimal performance on a given
   task and/or dataset.
   Conclusion: While there have already been some use cases of AutoML in
   the healthcare field, more work needs to be done in order for there to
   be widespread adoption of AutoML in healthcare.},
DOI = {10.1016/j.artmed.2020.101822},
Article-Number = {101822},
ISSN = {0933-3657},
EISSN = {1873-2860},
ORCID-Numbers = {Umeton, Renato/0000-0002-5561-6932},
Unique-ID = {WOS:000537804900016},
}

@article{ WOS:000523319000015,
Author = {Goecks, Jeremy and Jalili, Vahid and Heiser, Laura M. and Gray, Joe W.},
Title = {How Machine Learning Will Transform Biomedicine},
Journal = {CELL},
Year = {2020},
Volume = {181},
Number = {1},
Pages = {92-101},
Month = {APR 2},
Abstract = {This Perspective explores the application of machine learning toward
   improved diagnosis and treatment. We outline a vision for how machine
   learning can transform three broad areas of biomedicine: clinical
   diagnostics, precision treatments, and health monitoring, where the goal
   is to maintain health through a range of diseases and the normal aging
   process. For each area, early instances of successful machine learning
   applications are discussed, as well as opportunities and challenges for
   machine learning. When these challenges are met, machine learning
   promises a future of rigorous, outcomes-based medicine with detection,
   diagnosis, and treatment strategies that are continuously adapted to
   individual and environmental differences.},
DOI = {10.1016/j.cell.2020.03.022},
ISSN = {0092-8674},
EISSN = {1097-4172},
ResearcherID-Numbers = {Jalili, Vahid/KFS-7095-2024
   Gray, Joe/AAX-9549-2020
   },
ORCID-Numbers = {Jalili, Vahid/0000-0003-4986-2157
   Goecks, Jeremy/0000-0002-4583-5226},
Unique-ID = {WOS:000523319000015},
}

@article{ WOS:000520831800008,
Author = {Helm, J. Matthew and Swiergosz, Andrew M. and Haeberle, Heather S. and
   Karnuta, Jaret M. and Schaffer, Jonathan L. and Krebs, Viktor E. and
   Spitzer, I, Andrew and Ramkumar, Prem N.},
Title = {Machine Learning and Artificial Intelligence: Definitions, Applications,
   and Future Directions},
Journal = {CURRENT REVIEWS IN MUSCULOSKELETAL MEDICINE},
Year = {2020},
Volume = {13},
Number = {1},
Pages = {69-76},
Month = {FEB},
Abstract = {Purpose of Review With the unprecedented advancement of data aggregation
   and deep learning algorithms, artificial intelligence (AI) and machine
   learning (ML) are poised to transform the practice of medicine. The
   field of orthopedics, in particular, is uniquely suited to harness the
   power of big data, and in doing so provide critical insight into
   elevating the many facets of care provided by orthopedic surgeons. The
   purpose of this review is to critically evaluate the recent and novel
   literature regarding ML in the field of orthopedics and to address its
   potential impact on the future of musculoskeletal care. Recent Findings
   Recent literature demonstrates that the incorporation of ML into
   orthopedics has the potential to elevate patient care through
   alternative patient-specific payment models, rapidly analyze imaging
   modalities, and remotely monitor patients. Just as the business of
   medicine was once considered outside the domain of the orthopedic
   surgeon, we report evidence that demonstrates these emerging
   applications of AI warrant ownership, leverage, and application by the
   orthopedic surgeon to better serve their patients and deliver optimal,
   value-based care.},
DOI = {10.1007/s12178-020-09600-8},
ISSN = {1935-973X},
EISSN = {1935-9748},
ResearcherID-Numbers = {Haeberle, Heather/KOM-7583-2024
   },
ORCID-Numbers = {Haeberle, Heather/0000-0001-8341-8389},
Unique-ID = {WOS:000520831800008},
}

@article{ WOS:000577150900001,
Author = {Liu, Yue and Guo, Biru and Zou, Xinxin and Li, Yajie and Shi, Siqi},
Title = {Machine learning assisted materials design and discovery for
   rechargeable batteries},
Journal = {ENERGY STORAGE MATERIALS},
Year = {2020},
Volume = {31},
Pages = {434-450},
Month = {OCT},
Abstract = {Machine learning plays an important role in accelerating the discovery
   and design process for novel electrochemical energy storage materials.
   This review aims to provide the state-of-the-art and prospects of
   machine learning for the design of rechargeable battery materials. After
   illustrating the key concepts of machine learning and basic procedures
   for applying machine learning in rechargeable battery materials science,
   we focus on how to obtain the most important features from the specific
   physical, chemical and/or other properties of material by using wrapper
   feature selection method, embedded feature selection method, and the
   combination of these two methods. And then, the applications of machine
   learning in rechargeable battery materials design and discovery are
   reviewed, including the property prediction for liquid electrolytes,
   solid electrolytes, electrode materials, and the discovery of novel
   rechargeable battery materials through component prediction and
   structure prediction. More importantly, we discuss the key challenges
   related to machine learning in rechargeable battery materials science,
   including the contradiction between high dimension and small sample, the
   conflict between the complexity and accuracy of machine learning models,
   and the inconsistency between learning results and domain expert
   knowledge. In response to these challenges, we propose possible
   countermeasures and forecast potential directions of future research.
   This review is expected to shed light on machine learning in
   rechargeable battery materials design and property optimization.},
DOI = {10.1016/j.ensm.2020.06.033},
ISSN = {2405-8297},
EISSN = {2405-8289},
ResearcherID-Numbers = {Shi, Siqi/E-1245-2011},
Unique-ID = {WOS:000577150900001},
}

@article{ WOS:000892292800005,
Author = {Chen, Xiaolu and Weng, Tongfeng and Li, Chunzi and Yang, Huijie},
Title = {Equivalence of machine learning models in modeling chaos},
Journal = {CHAOS SOLITONS \& FRACTALS},
Year = {2022},
Volume = {165},
Number = {2},
Month = {DEC},
Abstract = {Recent advances have demonstrated that machine learning models are
   effective methods for predicting chaotic systems. Although short-term
   chaos prediction can be successfully realized by seemingly different
   machine learning models, an intriguing question of their correlation is
   still unknown. Here, we focus on three commonly used machine learning
   models that are reservoir computing, long-short term memory networks,
   and deep belief networks, respectively. We find that these selected
   models present almost identical long-term statistical properties as that
   of a learned chaotic system. Specifically, we show that these machine
   learning models have the same correlation dimension and recurrence time.
   Furthermore, by sharing a common signal, we realize synchronization,
   cascading synchronization, and coupled synchronization among machine
   learning models. Our findings reveal the equivalence of machine learning
   models in characterizing and modeling chaotic systems.},
DOI = {10.1016/j.chaos.2022.112831},
EarlyAccessDate = {NOV 2022},
Article-Number = {112831},
ISSN = {0960-0779},
EISSN = {1873-2887},
ResearcherID-Numbers = {weng, tongfeng/ABB-2363-2021
   Li, Chunzi/LSJ-2473-2024
   Yang, Huijie/D-6458-2013},
ORCID-Numbers = {chen, xiaolu/0000-0002-6342-8747
   },
Unique-ID = {WOS:000892292800005},
}

@article{ WOS:000895564800001,
Author = {Chen, Long and Wang, Jiangtao and Guo, Bin and Chen, Liming},
Title = {Human-in-the-loop machine learning with applications for population
   health},
Journal = {CCF TRANSACTIONS ON PERVASIVE COMPUTING AND INTERACTION},
Year = {2023},
Volume = {5},
Number = {1},
Pages = {1-12},
Month = {MAR},
Abstract = {Though technical advance of artificial intelligence and machine learning
   has enabled many promising intelligent systems, many computing tasks are
   still not able to be fully accomplished by machine intelligence.
   Motivated by the complementary nature of human and machine intelligence,
   an emerging trend is to involve humans in the loop of machine learning
   and decision-making. In this paper, we provide a macro-micro review of
   human-in-the-loop machine learning. We first describe major machine
   learning challenges which can be addressed by human intervention in the
   loop. Then we examine closely the latest research and findings of
   introducing humans into each step of the lifecycle of machine learning.
   Next, a case study of our recent application study in human-in-the-loop
   machine learning for population health is introduced. Finally, we
   analyze current research gaps and point out future research directions.},
DOI = {10.1007/s42486-022-00115-4},
EarlyAccessDate = {DEC 2022},
ISSN = {2524-521X},
EISSN = {2524-5228},
ResearcherID-Numbers = {Wang, Jiangtao/K-3422-2019},
ORCID-Numbers = {Wang, Jiangtao/0000-0002-8704-502X
   },
Unique-ID = {WOS:000895564800001},
}

@article{ WOS:000873821800023,
Author = {Oprea, Alina and Singhal, Anoop and Vassilev, Apostol},
Title = {Poisoning Attacks Against Machine Learning: Can Machine Learning Be
   Trustworthy?},
Journal = {COMPUTER},
Year = {2022},
Volume = {55},
Number = {11},
Pages = {94-99},
Month = {NOV},
Abstract = {Many practical applications benefit from machine learning and artificial
   intelligence technologies, but their security needs to be studied in
   more depth. We discuss the risk of poisoning attacks against the
   training stage of machine learning and challenges of defending against
   them.},
DOI = {10.1109/MC.2022.3190787},
ISSN = {0018-9162},
EISSN = {1558-0814},
ORCID-Numbers = {Vassilev, Apostol/0000-0002-9081-3042},
Unique-ID = {WOS:000873821800023},
}

@article{ WOS:000605460600001,
Author = {Gambella, Claudio and Ghaddar, Bissan and Naoum-Sawaya, Joe},
Title = {Optimization problems for machine learning: A survey},
Journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
Year = {2021},
Volume = {290},
Number = {3},
Pages = {807-828},
Month = {MAY 1},
Abstract = {This paper surveys the machine learning literature and presents in an
   optimization framework several commonly used machine learning
   approaches. Particularly, mathematical optimization models are presented
   for regression, classification, clustering, deep learning, and
   adversarial learning, as well as new emerging applications in machine
   teaching, empirical model learning, and Bayesian network structure
   learning. Such models can benefit from the advancement of numerical
   optimization techniques which have already played a distinctive role in
   several machine learning settings. The strengths and the shortcomings of
   these models are discussed and potential research directions and open
   problems are highlighted. (C) 2020 Elsevier B.V. Allrights reserved.},
DOI = {10.1016/j.ejor.2020.08.045},
EarlyAccessDate = {JAN 2021},
ISSN = {0377-2217},
EISSN = {1872-6860},
ResearcherID-Numbers = {Naoum-sawaya, Joe/ACV-0142-2022
   Ghaddar, Bissan/IAP-8652-2023
   Gambella, Claudio/AAB-6746-2019},
ORCID-Numbers = {Naoum-Sawaya, Joe/0000-0002-4908-225X
   Ghaddar, Bissan/0000-0003-4695-200X
   },
Unique-ID = {WOS:000605460600001},
}

@article{ WOS:000643029400001,
Author = {Fang, Haokun and Qian, Quan},
Title = {Privacy Preserving Machine Learning with Homomorphic Encryption and
   Federated Learning},
Journal = {FUTURE INTERNET},
Year = {2021},
Volume = {13},
Number = {4},
Month = {APR},
Abstract = {Privacy protection has been an important concern with the great success
   of machine learning. In this paper, it proposes a multi-party privacy
   preserving machine learning framework, named PFMLP, based on partially
   homomorphic encryption and federated learning. The core idea is all
   learning parties just transmitting the encrypted gradients by
   homomorphic encryption. From experiments, the model trained by PFMLP has
   almost the same accuracy, and the deviation is less than 1\%.
   Considering the computational overhead of homomorphic encryption, we use
   an improved Paillier algorithm which can speed up the training by
   25-28\%. Moreover, comparisons on encryption key length, the learning
   network structure, number of learning clients, etc. are also discussed
   in detail in the paper.},
DOI = {10.3390/fi13040094},
Article-Number = {94},
ISSN = {1999-5903},
ORCID-Numbers = {Qian, Quan/0000-0002-3020-005X},
Unique-ID = {WOS:000643029400001},
}

@article{ WOS:000744050600002,
Author = {Zoeller, Marc-Andre and Huber, Marco F.},
Title = {Benchmark and Survey of Automated Machine Learning Frameworks},
Journal = {JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH},
Year = {2021},
Volume = {70},
Pages = {409-472},
Abstract = {Machine learning (ML) has become a vital part in many aspects of our
   daily life. However, building well performing machine learning
   applications requires highly specialized data scientists and domain
   experts. Automated machine learning (AutoML) aims to reduce the demand
   for data scientists by enabling domain experts to build machine learning
   applications automatically without extensive knowledge of statistics and
   machine learning. This paper is a combination of a survey on current
   AutoML methods and a benchmark of popular AutoML frameworks on real data
   sets. Driven by the selected frameworks for evaluation, we summarize and
   review important AutoML techniques and methods concerning every step in
   building an ML pipeline. The selected AutoML frameworks are evaluated on
   137 data sets from established AutoML benchmark suites.},
ISSN = {1076-9757},
EISSN = {1943-5037},
ResearcherID-Numbers = {Huber, Marco/H-7866-2019},
ORCID-Numbers = {Zoller, Marc-Andre/0000-0001-8705-9862
   },
Unique-ID = {WOS:000744050600002},
}

@article{ WOS:000652961400002,
Author = {Kwekha-Rashid, Ameer Sardar and Abduljabbar, Heamn N. and Alhayani,
   Bilal},
Title = {Coronavirus disease (COVID-19) cases analysis using machine-learning
   applications},
Journal = {APPLIED NANOSCIENCE},
Year = {2021},
Month = {2021 MAY 21},
Abstract = {Today world thinks about coronavirus disease that which means all even
   this pandemic disease is not unique. The purpose of this study is to
   detect the role of machine-learning applications and algorithms in
   investigating and various purposes that deals with COVID-19. Review of
   the studies that had been published during 2020 and were related to this
   topic by seeking in Science Direct, Springer, Hindawi, and MDPI using
   COVID-19, machine learning, supervised learning, and unsupervised
   learning as keywords. The total articles obtained were 16,306 overall
   but after limitation; only 14 researches of these articles were included
   in this study. Our findings show that machine learning can produce an
   important role in COVID-19 investigations, prediction, and
   discrimination. In conclusion, machine learning can be involved in the
   health provider programs and plans to assess and triage the COVID-19
   cases. Supervised learning showed better results than other Unsupervised
   learning algorithms by having 92.9\% testing accuracy. In the future
   recurrent supervised learning can be utilized for superior accuracy.},
DOI = {10.1007/s13204-021-01868-7},
EarlyAccessDate = {MAY 2021},
ISSN = {2190-5509},
EISSN = {2190-5517},
ResearcherID-Numbers = {Abduljabbar, Heamn/ABC-6523-2021
   OZTURK, Bilal/AAM-1142-2020
   Kwekha Rashid, Ameer/AAQ-9112-2021
   },
ORCID-Numbers = {abduljabbar, heamn/0000-0002-9042-2957
   Kwekha-Rashid, Ameer/0000-0003-1422-1878},
Unique-ID = {WOS:000652961400002},
}

@article{ WOS:000719871700005,
Author = {Dobbelaere, Maarten R. and Plehiers, Pieter P. and van de Vijver, Ruben
   and V. Stevens, Christian and Van Geem, Kevin M.},
Title = {Machine Learning in Chemical Engineering: Strengths, Weaknesses,
   Opportunities, and Threats},
Journal = {ENGINEERING},
Year = {2021},
Volume = {7},
Number = {9},
Pages = {1201-1211},
Month = {SEP},
Abstract = {Chemical engineers rely on models for design, research, and daily
   decision-making, often with potentially large financial and safety
   implications. Previous efforts a few decades ago to combine artificial
   intelligence and chemical engineering for modeling were unable to
   fulfill the expectations. In the last five years, the increasing
   availability of data and computational resources has led to a resurgence
   in machine learning-based research. Many recent efforts have facilitated
   the roll-out of machine learning techniques in the research field by
   developing large databases, benchmarks, and representations for chemical
   applications and new machine learning frameworks. Machine learning has
   significant advantages over traditional modeling techniques, including
   flexibility, accuracy, and execution speed. These strengths also come
   with weaknesses, such as the lack of interpretability of these black-box
   models. The greatest opportunities involve using machine learning in
   time-limited applications such as real-time optimization and planning
   that require high accuracy and that can build on models with a
   self-learning ability to recognize patterns, learn from data, and become
   more intelligent over time. The greatest threat in artificial
   intelligence research today is inappropriate use because most chemical
   engineers have had limited training in computer science and data
   analysis. Nevertheless, machine learning will definitely become a
   trustworthy element in the modeling toolbox of chemical engineers. (C)
   2021 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy
   of Engineering and Higher Education Press Limited Company.},
DOI = {10.1016/j.eng.2021.03.019},
EarlyAccessDate = {NOV 2021},
ISSN = {2095-8099},
EISSN = {2096-0026},
ResearcherID-Numbers = {Van Geem, Kevin/J-3294-2014
   Stevens, Christian/X-2792-2019
   Van de Vijver, Ruben/C-2454-2018
   Dobbelaere, Maarten/ABD-4954-2021},
ORCID-Numbers = {Van Geem, Kevin/0000-0003-4191-4960
   Dobbelaere, Maarten/0000-0002-8977-8569
   },
Unique-ID = {WOS:000719871700005},
}

@article{ WOS:000662774900003,
Author = {Xu, Yayin and Zhou, Ying and Sekula, Przemyslaw and Ding, Lieyun},
Title = {Machine learning in construction: From shallow to deep learning},
Journal = {DEVELOPMENTS IN THE BUILT ENVIRONMENT},
Year = {2021},
Volume = {6},
Month = {MAY},
Abstract = {The development of artificial intelligence technology is currently
   bringing about new opportunities in construction. Machine learning is a
   major area of interest within the field of artificial intelligence,
   playing a pivotal role in the process of making construction
   ``smart{''}. The application of machine learning in construction has the
   potential to open up an array of opportunities such as site supervision,
   automatic detection, and intelligent maintenance. However, the
   implementation of machine learning faces a range of challenges due to
   the difficulties in acquiring labeled data, especially when applied in a
   highly complex construction site environment. This paper reviews the
   history of machine learning development from shallow to deep learning
   and its applications in construction. The strengths and weaknesses of
   machine learning technology in construction have been analyzed in order
   to foresee the future direction of machine learning applications in this
   sphere. Furthermore, this paper presents suggestions which may benefit
   researchers in terms of combining specific knowledge domains in
   construction with machine learning algorithms so as to develop dedicated
   deep network models for the industry.},
DOI = {10.1016/j.dibe.2021.100045},
EarlyAccessDate = {MAR 2021},
Article-Number = {100045},
EISSN = {2666-1659},
ResearcherID-Numbers = {Sekula, Przemyslaw/AAL-3896-2020
   Sekula, Przemyslaw/U-1413-2018
   Zhou, Ying/ABB-3897-2020},
ORCID-Numbers = {Zhou, Ying/0000-0002-3661-9265
   Sekula, Przemyslaw/0000-0002-4599-1077
   },
Unique-ID = {WOS:000662774900003},
}

@article{ WOS:000632782800001,
Author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and
   Ranganath, Rajesh and Foschini, Luca and Ghassemi, Marzyeh},
Title = {Reproducibility in machine learning for health research: Still a ways to
   go},
Journal = {SCIENCE TRANSLATIONAL MEDICINE},
Year = {2021},
Volume = {13},
Number = {586},
Month = {MAR 24},
Abstract = {Machine learning for health must be reproducible to ensure reliable
   clinical use. We evaluated 511 scientific papers across several machine
   learning subfields and found that machine learning for health compared
   poorly to other areas regarding reproducibility metrics, such as dataset
   and code accessibility. We propose recommendations to address this
   problem.},
DOI = {10.1126/scitranslmed.abb1655},
Article-Number = {eabb1655},
ISSN = {1946-6234},
EISSN = {1946-6242},
ResearcherID-Numbers = {Corradi, Antonio/L-7480-2015
   McDermott, Matthew/AAW-9648-2020
   },
ORCID-Numbers = {Ghassemi, Marzyeh/0000-0001-6349-7251
   Foschini, Luca/0000-0003-1409-3570
   McDermott, Matthew/0000-0001-6048-9707},
Unique-ID = {WOS:000632782800001},
}

@incollection{ WOS:000652490700019,
Author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
Editor = {Levi, M and Rosenblum, NL},
Title = {Machine Learning for Social Science: An Agnostic Approach},
Booktitle = {ANNUAL REVIEW OF POLITICAL SCIENCE, VOL 24, 2021},
Series = {Annual Review of Political Science},
Year = {2021},
Volume = {24},
Pages = {395-419},
Abstract = {Social scientists are now in an era of data abundance, and machine
   learning tools are increasingly used to extract meaning from data sets
   both massive and small. We explain how the inclusion of machine learning
   in the social sciences requires us to rethink not only applications of
   machine learning methods but also best practices in the social sciences.
   In contrast to the traditional tasks for machine learning in computer
   science and statistics, when machine learning is applied to social
   scientific data, it is used to discover new concepts, measure the
   prevalence of those concepts, assess causal effects, and make
   predictions. The abundance of data and resources facilitates the move
   away from a deductive social science to a more sequential, interactive,
   and ultimately inductive approach to inference. We explain how an
   agnostic approach to machine learning methods focused on the social
   science tasks facilitates progress across a wide range of questions.},
DOI = {10.1146/annurev-polisci-053119-015921},
ISSN = {1094-2939},
EISSN = {1545-1577},
ResearcherID-Numbers = {Stewart, Brandon/ADW-8389-2022},
Unique-ID = {WOS:000652490700019},
}

@article{ WOS:000655346100001,
Author = {Xu, Tianfang and Liang, Feng},
Title = {Machine learning for hydrologic sciences: An introductory overview},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-WATER},
Year = {2021},
Volume = {8},
Number = {5},
Month = {SEP},
Abstract = {The hydrologic community has experienced a surge in interest in machine
   learning in recent years. This interest is primarily driven by rapidly
   growing hydrologic data repositories, as well as success of machine
   learning in various academic and commercial applications, now possible
   due to increasing accessibility to enabling hardware and software. This
   overview is intended for readers new to the field of machine learning.
   It provides a non-technical introduction, placed within a historical
   context, to commonly used machine learning algorithms and deep learning
   architectures. Applications in hydrologic sciences are summarized next,
   with a focus on recent studies. They include the detection of patterns
   and events such as land use change, approximation of hydrologic
   variables and processes such as rainfall-runoff modeling, and mining
   relationships among variables for identifying controlling factors. The
   use of machine learning is also discussed in the context of integrated
   with process-based modeling for parameterization, surrogate modeling,
   and bias correction. Finally, the article highlights challenges of
   extrapolating robustness, physical interpretability, and small sample
   size in hydrologic applications.},
DOI = {10.1002/wat2.1533},
EarlyAccessDate = {MAY 2021},
Article-Number = {e1533},
ISSN = {2049-1948},
ResearcherID-Numbers = {Liang, Fenghua/HHM-3798-2022},
Unique-ID = {WOS:000655346100001},
}

@article{ WOS:000953243000001,
Author = {Vokinger, Kerstin N. and Feuerriegel, Stefan and Kesselheim, Aaron S.},
Title = {Mitigating bias in machine learning for medicine},
Journal = {COMMUNICATIONS MEDICINE},
Year = {2021},
Volume = {1},
Number = {1},
Month = {AUG 23},
Abstract = {Several sources of bias can affect the performance of machine learning
   systems used in medicine and potentially impact clinical care. Here, we
   discuss solutions to mitigate bias across the different development
   steps of machine learning-based systems for medical applications.},
DOI = {10.1038/s43856-021-00028-w},
Article-Number = {25},
ISSN = {2730-664X},
ResearcherID-Numbers = {Feuerriegel, Stefan/ABD-6599-2021
   Kesselheim, Aaron/R-6793-2017
   },
ORCID-Numbers = {Vokinger, Kerstin Noelle/0000-0002-6997-7384},
Unique-ID = {WOS:000953243000001},
}

@article{ WOS:000677963600002,
Author = {Walsh, Ian and Fishman, Dmytro and Garcia-Gasulla, Dario and Titma,
   Tiina and Pollastri, Gianluca and Harrow, Jennifer and Psomopoulos,
   Fotis E. and Tosatto, Silvio C. E. and ELIXIR Machine Learning Focus Grp},
Title = {DOME: recommendations for supervised machine learning validation in
   biology},
Journal = {NATURE METHODS},
Year = {2021},
Volume = {18},
Number = {10},
Pages = {1122-1127},
Month = {OCT},
Abstract = {DOME is a set of community-wide recommendations for reporting supervised
   machine learning-based analyses applied to biological studies. Broad
   adoption of these recommendations will help improve machine learning
   assessment and reproducibility.},
DOI = {10.1038/s41592-021-01205-4},
EarlyAccessDate = {JUL 2021},
ISSN = {1548-7091},
EISSN = {1548-7105},
ResearcherID-Numbers = {Tosatto, Silvio/B-2840-2009
   Dias Silva, Marcelo Jose/JDD-3305-2023
   Walsh, Ian/HCH-9412-2022
   Capriotti, Emidio/D-9318-2011
   Navarro, Arcadi/F-1592-2011
   Garcia-Gasulla, Dario/A-7601-2011
   Casadio, Rita/K-4814-2015
   Fernandez Gonzalez, Jose Maria/N-5920-2014
   Psomopoulos, Fotis/E-9995-2013
   Titma, Tiina/Q-6971-2019
   Zambelli, Federico/B-8198-2017
   Lenaerts, Tom/B-6376-2008
   Titma, Tiina/I-9206-2014},
ORCID-Numbers = {Harrow, Jennifer/0000-0003-0338-3070
   Spiwok, Vojtech/0000-0001-8108-2033
   Dias Silva, Marcelo Jose/0000-0002-8988-5532
   DOMINGUEZ DEL ANGEL, Victoria/0000-0002-5514-6651
   Martelli, Pier Luigi/0000-0002-0274-5669
   Walsh, Ian/0000-0003-3994-5522
   Capriotti, Emidio/0000-0002-2323-0963
   Navarro, Arcadi/0000-0003-2162-8246
   Garcia-Gasulla, Dario/0000-0001-6732-5641
   SAVOJARDO, CASTRENSE/0000-0002-7359-0633
   Casadio, Rita/0000-0002-7462-7039
   O Broin, Pilib/0000-0002-6702-8564
   Fernandez Gonzalez, Jose Maria/0000-0002-4806-5140
   Dimopoulos, Alexandros/0000-0002-4602-2040
   Pollastri, Gianluca/0000-0002-5825-4949
   Del Conte, Alessio/0000-0002-8052-3519
   Salgado, David/0000-0002-5905-3591
   Zambelli, Federico/0000-0003-3487-4331
   Tangaro, Marco Antonio/0000-0003-3923-2266
   Lenaerts, Tom/0000-0003-3645-1455
   Titma, Tiina/0000-0002-4935-8914},
Unique-ID = {WOS:000677963600002},
}

@article{ WOS:000684854500006,
Author = {Ghannam, Ryan B. and Techtmann, Stephen M.},
Title = {Machine learning applications in microbial ecology, human microbiome
   studies, and environmental monitoring},
Journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL},
Year = {2021},
Volume = {19},
Pages = {1092-1107},
Abstract = {Advances in nucleic acid sequencing technology have enabled expansion of
   our ability to profile microbial diversity. These large datasets of
   taxonomic and functional diversity are key to better understanding
   microbial ecology. Machine learning has proven to be a useful approach
   for analyzing microbial commu-nity data and making predictions about
   outcomes including human and environmental health. Machine learning
   applied to microbial community profiles has been used to predict disease
   states in human health, environmental quality and presence of
   contamination in the environment, and as trace evidence in forensics.
   Machine learning has appeal as a powerful tool that can provide deep
   insights into microbial communities and identify patterns in microbial
   community data. However, often machine learning models can be used as
   black boxes to predict a specific outcome, with little understanding of
   how the models arrived at predictions. Complex machine learning
   algorithms often may value higher accuracy and per-formance at the
   sacrifice of interpretability. In order to leverage machine learning
   into more translational research related to the microbiome and
   strengthen our ability to extract meaningful biological informa-tion, it
   is important for models to be interpretable. Here we review current
   trends in machine learning applications in microbial ecology as well as
   some of the important challenges and opportunities for more broad
   application of machine learning to understanding microbial communities.
   (C) 2021 The Authors. Published by Elsevier B.V. on behalf of Research
   Network of Computational and Structural Biotechnology.},
DOI = {10.1016/j.csbj.2021.01.028},
EarlyAccessDate = {FEB 2021},
ISSN = {2001-0370},
ORCID-Numbers = {Techtmann, Stephen/0000-0002-8702-9329
   Ghannam, Ryan/0000-0003-4403-9751},
Unique-ID = {WOS:000684854500006},
}

@article{ WOS:000620625100026,
Author = {Kim, Daekyum and Kim, Sang-Hun and Kim, Taekyoung and Kang, Brian
   Byunghyun and Lee, Minhyuk and Park, Wookeun and Ku, Subyeong and Kim,
   DongWook and Kwon, Junghan and Lee, Hochang and Bae, Joonbum and Park,
   Yong-Lae and Cho, Kyu-Jin and Jo, Sungho},
Title = {Review of machine learning methods in soft robotics},
Journal = {PLOS ONE},
Year = {2021},
Volume = {16},
Number = {2},
Month = {FEB 18},
Abstract = {Soft robots have been extensively researched due to their flexible,
   deformable, and adaptive characteristics. However, compared to rigid
   robots, soft robots have issues in modeling, calibration, and control in
   that the innate characteristics of the soft materials can cause complex
   behaviors due to non-linearity and hysteresis. To overcome these
   limitations, recent studies have applied various approaches based on
   machine learning. This paper presents existing machine learning
   techniques in the soft robotic fields and categorizes the implementation
   of machine learning approaches in different soft robotic applications,
   which include soft sensors, soft actuators, and applications such as
   soft wearable robots. An analysis of the trends of different machine
   learning approaches with respect to different types of soft robot
   applications is presented; in addition to the current limitations in the
   research field, followed by a summary of the existing machine learning
   methods for soft robots.},
DOI = {10.1371/journal.pone.0246102},
Article-Number = {e0246102},
EISSN = {1932-6203},
ResearcherID-Numbers = {park, wookeun/JDV-6609-2023
   Ku, Subyeong/KAM-0096-2024
   Kim, Daekyum/KSM-1135-2024
   Bae, Joonbum/A-5717-2012
   Kim, Taekyoung/ISU-7728-2023},
ORCID-Numbers = {Kim, Taekyoung/0000-0003-3739-7403
   Ku, Subyeong/0000-0001-5184-3073
   DongWook, Kim/0000-0002-8809-4114
   Kim, Sang-Hun/0000-0003-0971-9292
   Kim, Daekyum/0000-0001-7777-8388
   },
Unique-ID = {WOS:000620625100026},
}

@article{ WOS:000649692900007,
Author = {Zhao, Shili and Zhang, Song and Liu, Jincun and Wang, He and Zhu, Jia
   and Li, Daoliang and Zhao, Ran},
Title = {Application of machine learning in intelligent fish aquaculture: A
   review},
Journal = {AQUACULTURE},
Year = {2021},
Volume = {540},
Month = {JUL 15},
Abstract = {Among the background of developments in automation and intelligence,
   machine learning technology has been extensively applied in aquaculture
   in recent years, providing a new opportunity for the realization of
   digital fishery farming. In the present paper, the machine learning
   algorithms and techniques adopted in intelligent fish aquaculture in the
   past five years are expounded, and the application of machine learning
   in aquaculture is explored in detail, including the information
   evaluation of fish biomass, the identification and classification of
   fish, behavioral analysis and prediction of water quality parameters.
   Further, the application of machine learning algorithms in aquaculture
   is outlined, and the results are analyzed. Finally, several current
   problems in aquaculture are highlighted, and the development trend is
   considered.},
DOI = {10.1016/j.aquaculture.2021.736724},
EarlyAccessDate = {APR 2021},
Article-Number = {736724},
ISSN = {0044-8486},
EISSN = {1873-5622},
Unique-ID = {WOS:000649692900007},
}

@article{ WOS:001343358300003,
Author = {Meshram, Vishal and Patil, Kailas and Meshram, Vidula and Hanchate,
   Dinesh and Ramkteke, S. D.},
Title = {Machine learning in agriculture domain: A state-of-art survey},
Journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES},
Year = {2021},
Volume = {1},
Month = {DEC},
Abstract = {Food is considered as a basic need of human being which can be satisfied
   through farming. Agriculture not only fulfills humans' basic needs, but
   also considered as source of employment worldwide. Agriculture is
   considered as a backbone of economy and source of employment in the
   developing countries like India. Agriculture contributes 15.4\% in the
   GDP of India. Agriculture activities are broadly categorized into three
   major areas: pre-harvesting, harvesting and post harvesting. Advancement
   in area of machine learning has helped improving gains in agriculture.
   Machine learning is the current technology which is benefiting farmers
   to minimize the losses in the farming by providing rich recommendations
   and insights about the crops. This paper presents an extensive survey of
   latest machine learning application in agriculture to alleviate the
   problems in the three areas of pre-harvesting, harvesting and
   post-harvesting. Application of machine learning in agriculture allows
   more efficient and precise farming with less human manpower with high
   quality production.},
DOI = {10.1016/j.ailsci.2021.100010},
Article-Number = {100010},
EISSN = {2667-3185},
ResearcherID-Numbers = {Meshram, Vishal/HOH-8961-2023
   Patil, Kailas/ABI-6917-2020
   Patil, Kailas/E-3458-2016
   Meshram, Vidula/ABE-8613-2021},
ORCID-Numbers = {Patil, Kailas/0000-0002-1046-9860
   Meshram, Vidula/0000-0003-3489-0366},
Unique-ID = {WOS:001343358300003},
}

@article{ WOS:000605576700001,
Author = {Liu, Zhaocheng and Zhu, Dayu and Raju, Lakshmi and Cai, Wenshan},
Title = {Tackling Photonic Inverse Design with Machine Learning},
Journal = {ADVANCED SCIENCE},
Year = {2021},
Volume = {8},
Number = {5},
Month = {MAR},
Abstract = {Machine learning, as a study of algorithms that automate prediction and
   decision-making based on complex data, has become one of the most
   effective tools in the study of artificial intelligence. In recent
   years, scientific communities have been gradually merging data-driven
   approaches with research, enabling dramatic progress in revealing
   underlying mechanisms, predicting essential properties, and discovering
   unconventional phenomena. It is becoming an indispensable tool in the
   fields of, for instance, quantum physics, organic chemistry, and medical
   imaging. Very recently, machine learning has been adopted in the
   research of photonics and optics as an alternative approach to address
   the inverse design problem. In this report, the fast advances of
   machine-learning-enabled photonic design strategies in the past few
   years are summarized. In particular, deep learning methods, a subset of
   machine learning algorithms, dealing with intractable high
   degrees-of-freedom structure design are focused upon.},
DOI = {10.1002/advs.202002923},
EarlyAccessDate = {JAN 2021},
Article-Number = {2002923},
EISSN = {2198-3844},
ResearcherID-Numbers = {Liu, Zhaocheng/T-8674-2019
   Raju, Lakshmi/KVB-4070-2024
   Zhu, Dayu/AFV-6450-2022
   },
ORCID-Numbers = {Raju, Lakshmi/0000-0003-2450-8856
   Cai, Wenshan/0000-0002-6367-3857},
Unique-ID = {WOS:000605576700001},
}

@article{ WOS:000493720200024,
Author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl,
   Reza and Yu, Bin},
Title = {Definitions, methods, and applications in interpretable machine learning},
Journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA},
Year = {2019},
Volume = {116},
Number = {44},
Pages = {22071-22080},
Month = {OCT 29},
Abstract = {Machine-learning models have demonstrated great success in learning
   complex patterns that enable them to make predictions about unobserved
   data. In addition to using models for prediction, the ability to
   interpret what a model has learned is receiving an increasing amount of
   attention. However, this increased focus has led to considerable
   confusion about the notion of interpretability. In particular, it is
   unclear how the wide array of proposed interpretation methods are
   related and what common concepts can be used to evaluate them. We aim to
   address these concerns by defining interpretability in the context of
   machine learning and introducing the predictive, descriptive, relevant
   (PDR) framework for discussing interpretations. The PDR framework
   provides 3 overarching desiderata for evaluation: predictive accuracy,
   descriptive accuracy, and relevancy, with relevancy judged relative to a
   human audience. Moreover, to help manage the deluge of interpretation
   methods, we introduce a categorization of existing techniques into
   model-based and post hoc categories, with subgroups including sparsity,
   modularity, and simulatability. To demonstrate how practitioners can use
   the PDR framework to evaluate and understand interpretations, we provide
   numerous real-world examples. These examples highlight the often
   underappreciated role played by human audiences in discussions of
   interpretability. Finally, based on our framework, we discuss
   limitations of existing methods and directions for future work. We hope
   that this work will provide a common vocabulary that will make it easier
   for both practitioners and researchers to discuss and choose from the
   full range of interpretation methods.},
DOI = {10.1073/pnas.1900654116},
ISSN = {0027-8424},
EISSN = {1091-6490},
ResearcherID-Numbers = {Abbasi, Reza/ABX-3544-2022
   },
ORCID-Numbers = {Abbasi Asl, Reza/0000-0001-7824-4628},
Unique-ID = {WOS:000493720200024},
}

@article{ WOS:000688449200019,
Author = {Tang, Fengxiao and Mao, Bomin and Kato, Nei and Gui, Guan},
Title = {Comprehensive Survey on Machine Learning in Vehicular Network:
   Technology, Applications and Challenges},
Journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS},
Year = {2021},
Volume = {23},
Number = {3},
Pages = {2027-2057},
Abstract = {Towards future intelligent vehicular network, the machine learning as
   the promising artificial intelligence tool is widely researched to
   intelligentize communication and networking functions. In this paper, we
   provide a comprehensive survey on various machine learning techniques
   applied to both communication and network parts in vehicular network. To
   benefit reading, we first give a preliminary on communication
   technologies and machine learning technologies in vehicular network.
   Then, we detailedly describe the challenges of conventional techniques
   in vehicular network and corresponding machine learning based solutions.
   Finally, we present several open issues and emphasize potential
   directions that are worthy of research for the future intelligent
   vehicular network.},
DOI = {10.1109/COMST.2021.3089688},
EISSN = {1553-877X},
ResearcherID-Numbers = {Gui, Guan/AAG-3593-2019
   Mao, Bomin/AAG-8851-2019
   Tang, Fengxiao/AFN-7960-2022
   KATO, NEI/T-5892-2019},
ORCID-Numbers = {Tang, Fengxiao/0000-0003-2414-4802
   Mao, Bomin/0000-0001-7780-5972
   },
Unique-ID = {WOS:000688449200019},
}

@article{ WOS:000702995500001,
Author = {Gao, Chaochao and Min, Xin and Fang, Minghao and Tao, Tianyi and Zheng,
   Xiaohong and Liu, Yangai and Wu, Xiaowen and Huang, Zhaohui},
Title = {Innovative Materials Science via Machine Learning},
Journal = {ADVANCED FUNCTIONAL MATERIALS},
Year = {2022},
Volume = {32},
Number = {1},
Month = {JAN},
Abstract = {Nowadays, the research on materials science is rapidly entering a phase
   of data-driven age. Machine learning, one of the most powerful
   data-driven methods, have been being applied to materials discovery and
   performances prediction with undoubtedly tremendous application
   foreground. Herein, the challenges and current progress of machine
   learning are summarized in materials science, the design strategies are
   classified and highlighted, and possible perspectives are proposed for
   the future development. It is hoped this review can provide important
   scientific guidance for innovating materials science and technology via
   machine learning in the future.},
DOI = {10.1002/adfm.202108044},
EarlyAccessDate = {OCT 2021},
Article-Number = {2108044},
ISSN = {1616-301X},
EISSN = {1616-3028},
ResearcherID-Numbers = {Fang, Minghao/M-9048-2013
   Wu, Xiaowen/E-9952-2013
   Min, Xin/L-8050-2019
   gao, chaochao/AAE-9674-2019
   郑, 晓洪/AAM-8258-2021},
ORCID-Numbers = {Wu, Xiaowen/0000-0002-5611-0626
   },
Unique-ID = {WOS:000702995500001},
}

@article{ WOS:000660500300002,
Author = {Cuoco, Elena and Powell, Jade and Cavaglia, Marco and Ackley, Kendall
   and Bejger, Michal and Chatterjee, Chayan and Coughlin, Michael and
   Coughlin, Scott and Easter, Paul and Essick, Reed and Gabbard, Hunter
   and Gebhard, Timothy and Ghosh, Shaon and Haegel, Leila and Iess,
   Alberto and Keitel, David and Marka, Zsuzsa and Marka, Szabolcs and
   Morawski, Filip and Nguyen, Tri and Ormiston, Rich and Puerrer, Michael
   and Razzano, Massimiliano and Staats, Kai and Vajente, Gabriele and
   Williams, Daniel},
Title = {Enhancing gravitational-wave science with machine learning},
Journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {2},
Number = {1},
Month = {MAR},
Abstract = {Machine learning has emerged as a popular and powerful approach for
   solving problems in astrophysics. We review applications of machine
   learning techniques for the analysis of ground-based gravitational-wave
   (GW) detector data. Examples include techniques for improving the
   sensitivity of Advanced Laser Interferometer GW Observatory and Advanced
   Virgo GW searches, methods for fast measurements of the astrophysical
   parameters of GW sources, and algorithms for reduction and
   characterization of non-astrophysical detector noise. These applications
   demonstrate how machine learning techniques may be harnessed to enhance
   the science that is possible with current and future GW detectors.},
DOI = {10.1088/2632-2153/abb93a},
Article-Number = {011002},
EISSN = {2632-2153},
ResearcherID-Numbers = {Cavaglià, Marco/AAW-6540-2021
   Keitel, David/AAA-1579-2019
   Bejger, Magdalena/W-7298-2018
   Morawski, Filip/AHB-7548-2022
   Marka, Szabolcs/ABE-7493-2021
   Poole, Michael/K-6707-2013
   Williams, Daniel/ABA-6070-2022
   Iess, Alberto/GWD-0844-2022
   Cuoco, Elena/I-8789-2012
   Vahlbruch, Henning/HSD-4655-2023},
ORCID-Numbers = {Gebhard, Timothy/0000-0001-9310-8579
   Easter, Paul/0000-0003-2212-9051
   Iess, Alberto/0000-0001-9658-6752
   Keitel, David/0000-0002-2824-626X
   Cavaglia, Marco/0000-0002-3835-6729
   Chatterjee, Chayan/0000-0001-8700-3455
   Williams, Daniel/0000-0003-3772-198X
   Ackley, Kendall/0000-0002-8648-0767
   Bejger, Michal/0000-0002-4991-8213
   Cuoco, Elena/0000-0002-6528-3449
   RAZZANO, MASSIMILIANO/0000-0003-4825-1629
   Powell, Jade/0000-0002-1357-4164
   Marka, Szabolcs/0000-0002-3957-1324
   },
Unique-ID = {WOS:000660500300002},
}

@article{ WOS:000921883400006,
Author = {Butlin, Patrick},
Title = {Machine Learning, Functions and Goals},
Journal = {CROATIAN JOURNAL OF PHILOSOPHY},
Year = {2022},
Volume = {22},
Number = {66},
Pages = {351-370},
Note = {1st Annual Kathy Wilkes Memorial Conference, Inter-Univ Ctr, Dubrovnik,
   CROATIA, APR 29, 2022},
Abstract = {Machine learning researchers distinguish between reinforcement learning
   and supervised learning and refer to reinforcement learning systems as
   ``agents{''}. This paper vindicates the claim that systems trained by
   reinforcement learning are agents while those trained by supervised
   learning are not. Systems of both kinds satisfy Dretske's criteria for
   agency, because they both learn to produce outputs selectively in
   response to inputs. However, reinforcement learning is sensitive to the
   instrumental value of outputs, giving rise to systems which exploit the
   effects of outputs on subsequent inputs to achieve good performance over
   episodes of interaction with their environments. Supervised learning
   systems, in contrast, merely learn to produce better outputs in response
   to individual inputs.},
DOI = {10.52685/cjp.22.66.5},
ISSN = {1333-1108},
EISSN = {1847-6139},
Unique-ID = {WOS:000921883400006},
}

@article{ WOS:000663500200010,
Author = {Balaji, T. K. and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree},
Title = {Machine learning algorithms for social media analysis: A survey},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2021},
Volume = {40},
Month = {MAY},
Abstract = {Social Media (SM) are the most widespread and rapid data generation
   applications on the Internet increase the study of these data. However,
   the efficient processing of such massive data is challenging, so we
   require a system that learns from these data, like machine learning.
   Machine learning methods make the systems to learn itself. Many papers
   are published on SM using machine learning approaches over the past few
   decades. In this paper, we provide a comprehensive survey of multiple
   applications of SM analysis using robust machine learning algorithms.
   Initially, we discuss a summary of machine learning algorithms, which
   are used in SM analysis. After that, we provide a detailed survey of
   machine learning approaches to SM analysis. Furthermore, we summarize
   the challenges and benefits of Machine Learning usages in SM analysis.
   Finally, we presented open issues and consequences in SM analysis for
   further research.
   (c) 2021 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.cosrev.2021.100395},
EarlyAccessDate = {MAR 2021},
Article-Number = {100395},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {T K, BALAJI/G-4070-2018
   Annavarapu, Chandra/AAO-5684-2020
   K, BALAJI/G-4070-2018
   Bablani, Annushree/AAQ-3515-2020},
ORCID-Numbers = {T K, BALAJI/0000-0003-4448-2948
   },
Unique-ID = {WOS:000663500200010},
}

@article{ WOS:000751704800103,
Author = {Hensel, Felix and Moor, Michael and Rieck, Bastian},
Title = {A Survey of Topological Machine Learning Methods},
Journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE},
Year = {2021},
Volume = {4},
Abstract = {The last decade saw an enormous boost in the field of computational
   topology: methods and concepts from algebraic and differential topology,
   formerly confined to the realm of pure mathematics, have demonstrated
   their utility in numerous areas such as computational biology
   personalised medicine, and time-dependent data analysis, to name a few.
   The newly-emerging domain comprising topology-based techniques is often
   referred to as topological data analysis (TDA). Next to their
   applications in the aforementioned areas, TDA methods have also proven
   to be effective in supporting, enhancing, and augmenting both classical
   machine learning and deep learning models. In this paper, we review the
   state of the art of a nascent field we refer to as ``topological machine
   learning,{''} i.e., the successful symbiosis of topology-based methods
   and machine learning algorithms, such as deep neural networks. We
   identify common threads, current applications, and future challenges.},
DOI = {10.3389/frai.2021.681108},
Article-Number = {681108},
EISSN = {2624-8212},
ResearcherID-Numbers = {Rieck, Bastian/J-7507-2019
   },
ORCID-Numbers = {Moor, Michael/0000-0003-4911-6437
   Grossenbacher-Rieck, Bastian/0000-0003-4335-0302},
Unique-ID = {WOS:000751704800103},
}

@article{ WOS:000517788300001,
Author = {Badillo, Solveig and Banfai, Balazs and Birzele, Fabian and Davydov,
   Iakov I. and Hutchinson, Lucy and Kam-Thong, Tony and Siebourg-Polster,
   Juliane and Steiert, Bernhard and Zhang, Jitao David},
Title = {An Introduction to Machine Learning},
Journal = {CLINICAL PHARMACOLOGY \& THERAPEUTICS},
Year = {2020},
Volume = {107},
Number = {4},
Pages = {871-885},
Month = {APR},
Abstract = {In the last few years, machine learning (ML) and artificial intelligence
   have seen a new wave of publicity fueled by the huge and ever-increasing
   amount of data and computational power as well as the discovery of
   improved learning algorithms. However, the idea of a computer learning
   some abstract concept from data and applying them to yet unseen
   situations is not new and has been around at least since the 1950s. Many
   of these basic principles are very familiar to the pharmacometrics and
   clinical pharmacology community. In this paper, we want to introduce the
   foundational ideas of ML to this community such that readers obtain the
   essential tools they need to understand publications on the topic.
   Although we will not go into the very details and theoretical
   background, we aim to point readers to relevant literature and put
   applications of ML in molecular biology as well as the fields of
   pharmacometrics and clinical pharmacology into perspective.},
DOI = {10.1002/cpt.1796},
EarlyAccessDate = {MAR 2020},
ISSN = {0009-9236},
EISSN = {1532-6535},
ResearcherID-Numbers = {Zhang, Xinyuan/GZA-6167-2022
   Banfai, Balazs/H-3464-2012
   },
ORCID-Numbers = {Steiert, Bernhard/0000-0002-1398-1624
   Banfai, Balazs/0000-0003-0422-7977
   Hutchinson, Lucy/0000-0002-3845-5460
   Zhang, Jitao David/0000-0002-3085-0909
   Siebourg, Juliane/0000-0002-1759-3223},
Unique-ID = {WOS:000517788300001},
}

@article{ WOS:000719226000003,
Author = {Mangalathu, Sujith and Karthikeyan, Karthika and Feng, De-Cheng and
   Jeon, Jong-Su},
Title = {Machine-learning interpretability techniques for seismic performance
   assessment of infrastructure systems},
Journal = {ENGINEERING STRUCTURES},
Year = {2022},
Volume = {250},
Month = {JAN 1},
Abstract = {Machine-learning has recently gained considerable attention in the
   earthquake engineering community, as it can map the complex relationship
   between the expected damage and the input parameters. It is often
   necessary to understand the reasons for the behavior and predictions of
   the machine-learning model. This paper addresses this issue through
   interpretable machine-learning approaches such as partial dependence
   plots, accumulated local effects, and Shapely additive explanations. The
   evaluation of these approaches is carried out (1) at a component level
   by analyzing the shear strength predictions by a machine-learning model
   and (2) at a regional level through the machine-learning model for the
   regional damage assessment of bridges in California. The comparison
   helps to identify (1) the proper implementation of these approaches for
   the efficient use of machine-learning models and (2) key influential
   variables and thresholds that govern the prediction of the
   machine-learning models.},
DOI = {10.1016/j.engstruct.2021.112883},
EarlyAccessDate = {NOV 2021},
Article-Number = {112883},
ISSN = {0141-0296},
EISSN = {1873-7323},
ResearcherID-Numbers = {Feng, De-Cheng/AAF-1399-2019
   Jeon, Jong-Su/R-4622-2019
   Mangalathu, Sujith/I-3020-2017
   },
ORCID-Numbers = {Jeon, Jong-Su/0000-0001-6657-7265
   Karthikeyan, Karthika/0000-0002-0078-5907},
Unique-ID = {WOS:000719226000003},
}

@article{ WOS:000656859400017,
Author = {Mowbray, Max and Savage, Thomas and Wu, Chufan and Song, Ziqi and Cho,
   Bovinille Anye and Del Rio-Chanona, Ehecatl A. and Zhang, Dongda},
Title = {Machine learning for biochemical engineering: A review},
Journal = {BIOCHEMICAL ENGINEERING JOURNAL},
Year = {2021},
Volume = {172},
Month = {AUG},
Abstract = {The field of machine learning is comprised of techniques, which have
   proven powerful approaches to knowledge discovery and construction of
   `digital twins' in the highly dimensional, nonlinear and stochastic
   domains common to biochemical engineering. We review the use of machine
   learning within biochemical engineering over the last 20 years. The most
   prevalent machine learning methods are demystified, and their impact
   across individual biochemical engineering subfields is outlined. In
   doing so we provide insights into the true benefits of each technique,
   and obstacles for their wider deployment. Finally, core challenges into
   the application of machine learning in biochemical engineering are
   thoroughly discussed, and further insight into adoption of innovative
   hybrid modelling and transfer learning strategies for development of new
   digital biotechnologies is provided.},
DOI = {10.1016/j.bej.2021.108054},
EarlyAccessDate = {MAY 2021},
Article-Number = {108054},
ISSN = {1369-703X},
EISSN = {1873-295X},
ORCID-Numbers = {Savage, Thomas/0000-0001-8715-8369
   Anye Cho, Bovinille/0000-0001-9803-0520
   Mowbray, Max/0000-0003-1398-0469},
Unique-ID = {WOS:000656859400017},
}

@article{ WOS:000656789000001,
Author = {Garg, Arunim and Mago, Vijay},
Title = {Role of machine learning in medical research: A survey},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2021},
Volume = {40},
Month = {MAY},
Abstract = {Machine learning is one of the essential and effective tools in
   analyzing highly complex medical data. With vast amounts of medical data
   being generated, there is an urgent need to effectively use this data to
   benefit the medical and health care sectors all across the world. This
   survey paper presents a systematic literature review for the
   investigation of various machine learning techniques used for numerous
   medical applications which are published in highly reputable venues in
   recent years. Considering only the recent work, we are able to survey
   the current machine learning and deep learning models that are being
   used for medical data. This literature review identifies a clear shift
   of artificial intelligence techniques used in the medical domain, with
   deep learning methods taking precedence over machine learning methods.
   (C) 2021 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.cosrev.2021.100370},
EarlyAccessDate = {FEB 2021},
Article-Number = {100370},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {Mago, Vijay/AAC-6646-2019
   },
ORCID-Numbers = {Mago, Vijay/0000-0002-9741-3463},
Unique-ID = {WOS:000656789000001},
}

@article{ WOS:000664988000001,
Author = {Zhang, Kaiyi and Wang, Jianwu and Liu, Tianyi and Luo, Yifei and Loh,
   Xian Jun and Chen, Xiaodong},
Title = {Machine Learning-Reinforced Noninvasive Biosensors for Healthcare},
Journal = {ADVANCED HEALTHCARE MATERIALS},
Year = {2021},
Volume = {10},
Number = {17, SI},
Month = {SEP},
Abstract = {The emergence and development of noninvasive biosensors largely
   facilitate the collection of physiological signals and the processing of
   health-related data. The utilization of appropriate machine learning
   algorithms improves the accuracy and efficiency of biosensors. Machine
   learning-reinforced biosensors are started to use in clinical practice,
   health monitoring, and food safety, bringing a digital revolution in
   healthcare. Herein, the recent advances in machine learning-reinforced
   noninvasive biosensors applied in healthcare are summarized. First,
   different types of noninvasive biosensors and physiological signals
   collected are categorized and summarized. Then machine learning
   algorithms adopted in subsequent data processing are introduced and
   their practical applications in biosensors are reviewed. Finally, the
   challenges faced by machine learning-reinforced biosensors are raised,
   including data privacy and adaptive learning capability, and their
   prospects in real-time monitoring, out-of-clinic diagnosis, and onsite
   food safety detection are proposed.},
DOI = {10.1002/adhm.202100734},
EarlyAccessDate = {JUN 2021},
Article-Number = {2100734},
ISSN = {2192-2640},
EISSN = {2192-2659},
ResearcherID-Numbers = {Loh, Xian/H-6260-2013
   Loh, Xian Jun/H-6260-2013
   Luo, Yifei/AAU-4334-2021
   Chen, Xiaodong/A-4537-2009
   wang, jianwu/KBB-3546-2024},
ORCID-Numbers = {Loh, Xian Jun/0000-0001-8118-6502
   Chen, Xiaodong/0000-0002-3312-1664
   Luo, Yifei/0000-0002-4454-6318
   },
Unique-ID = {WOS:000664988000001},
}

@article{ WOS:000624582400019,
Author = {Luan, Hui and Tsai, Chin-Chung},
Title = {A Review of Using Machine Learning Approaches for Precision Education},
Journal = {EDUCATIONAL TECHNOLOGY \& SOCIETY},
Year = {2021},
Volume = {24},
Number = {1},
Pages = {250-266},
Month = {JAN},
Abstract = {In recent years, in the field of education, there has been a clear
   progressive trend toward precision education. As a rapidly evolving AI
   technique, machine learning is viewed as an important means to realize
   it. In this paper, we systematically review 40 empirical studies
   regarding machine-learning-based precision education. The results showed
   that the majority of studies focused on the prediction of learning
   performance or dropouts, and were carried out in online or blended
   learning environments among university students majoring in computer
   science or STEM, whereas the data sources were divergent. The commonly
   used machine learning algorithms, evaluation methods, and validation
   approaches are presented. The emerging issues and future directions are
   discussed accordingly.},
ISSN = {1176-3647},
EISSN = {1436-4522},
ResearcherID-Numbers = {Tsai, Chin-Chung/E-5902-2010},
ORCID-Numbers = {Tsai, Chin-Chung/0000-0001-7744-9971},
Unique-ID = {WOS:000624582400019},
}

@article{ WOS:000709474600010,
Author = {Goodswen, Stephen J. and Barratt, Joel L. N. and Kennedy, Paul J. and
   Kaufer, Alexa and Calarco, Larissa and Ellis, John T.},
Title = {Machine learning and applications in microbiology},
Journal = {FEMS MICROBIOLOGY REVIEWS},
Year = {2021},
Volume = {45},
Number = {5},
Month = {SEP},
Abstract = {To understand the intricacies of microorganisms at the molecular level
   requires making sense of copious volumes of data such that it may now be
   humanly impossible to detect insightful data patterns without an
   artificial intelligence application called machine learning. Applying
   machine learning to address biological problems is expected to grow at
   an unprecedented rate, yet it is perceived by the uninitiated as a
   mysterious and daunting entity entrusted to the domain of mathematicians
   and computer scientists. The aim of this review is to identify key
   points required to start the journey of becoming an effective machine
   learning practitioner. These key points are further reinforced with an
   evaluation of how machine learning has been applied so far in a broad
   scope of real-life microbiology examples. This includes predicting drug
   targets or vaccine candidates, diagnosing microorganisms causing
   infectious diseases, classifying drug resistance against antimicrobial
   medicines, predicting disease outbreaks and exploring microbial
   interactions. Our hope is to inspire microbiologists and other related
   researchers to join the emerging machine learning revolution.},
DOI = {10.1093/femsre/fuab015},
EarlyAccessDate = {MAR 2021},
Article-Number = {fuab015},
ISSN = {0168-6445},
EISSN = {1574-6976},
ResearcherID-Numbers = {Kennedy, Paul/M-8450-2014
   Ellis, John/L-6988-2016},
ORCID-Numbers = {Goodswen, Stephen/0000-0001-6184-3157
   Kennedy, Paul/0000-0001-7837-3171
   Kaufer, Alexa/0000-0002-2682-1187
   Barratt, Joel/0000-0001-8711-2408
   Ellis, John/0000-0001-7328-4831},
Unique-ID = {WOS:000709474600010},
}

@article{ WOS:000646865900001,
Author = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and
   Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and
   Mueller, Klaus-Robert},
Title = {Towards CRISP-ML(Q): A Machine Learning Process Model with Quality
   Assurance Methodology},
Journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION},
Year = {2021},
Volume = {3},
Number = {2},
Pages = {392-413},
Month = {JUN},
Abstract = {Machine learning is an established and frequently used technique in
   industry and academia, but a standard process model to improve success
   and efficiency of machine learning applications is still missing.
   Project organizations and machine learning practitioners face manifold
   challenges and risks when developing machine learning applications and
   have a need for guidance to meet business expectations. This paper
   therefore proposes a process model for the development of machine
   learning applications, covering six phases from defining the scope to
   maintaining the deployed machine learning application. Business and data
   understanding are executed simultaneously in the first phase, as both
   have considerable impact on the feasibility of the project. The next
   phases are comprised of data preparation, modeling, evaluation, and
   deployment. Special focus is applied to the last phase, as a model
   running in changing real-time environments requires close monitoring and
   maintenance to reduce the risk of performance degradation over time.
   With each task of the process, this work proposes quality assurance
   methodology that is suitable to address challenges in machine learning
   development that are identified in the form of risks. The methodology is
   drawn from practical experience and scientific literature, and has
   proven to be general and stable. The process model expands on CRISP-DM,
   a data mining process model that enjoys strong industry support, but
   fails to address machine learning specific tasks. The presented work
   proposes an industry- and application-neutral process model tailored for
   machine learning applications with a focus on technical tasks for
   quality assurance.},
DOI = {10.3390/make3020020},
EISSN = {2504-4990},
ResearcherID-Numbers = {Mueller, Klaus-Robert/C-3196-2013
   Peters, Steven/J-8422-2014
   },
ORCID-Numbers = {Studer, Stefan/0000-0003-1598-6899
   Peters, Steven/0000-0003-3131-1664
   Mueller, Klaus-Robert/0000-0002-3861-7685},
Unique-ID = {WOS:000646865900001},
}

@article{ WOS:000696667700006,
Author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
Title = {How does Machine Learning Change Software Development Practices?},
Journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
Year = {2021},
Volume = {47},
Number = {9},
Pages = {1857-1871},
Month = {SEPT 1},
Abstract = {Adding an ability for a system to learn inherently adds uncertainty into
   the system. Given the rising popularity of incorporating machine
   learning into systems, we wondered how the addition alters software
   development practices. We performed a mixture of qualitative and
   quantitative studies with 14 interviewees and 342 survey respondents
   from 26 countries across four continents to elicit significant
   differences between the development of machine learning systems and the
   development of non-machine-learning systems. Our study uncovers
   significant differences in various aspects of software engineering
   (e.g., requirements, design, testing, and process) and work
   characteristics (e.g., skill variety, problem solving and task
   identity). Based on our findings, we highlight future research
   directions and provide recommendations for practitioners.},
DOI = {10.1109/TSE.2019.2937083},
ISSN = {0098-5589},
EISSN = {1939-3520},
ResearcherID-Numbers = {Lo, David/A-2493-2012
   LO, David/A-2493-2012
   Xia, Xin/AAD-6217-2022},
ORCID-Numbers = {Lo, David/0000-0002-4367-7201
   },
Unique-ID = {WOS:000696667700006},
}

@article{ WOS:000628641300001,
Author = {Roy, Priya and Chowdhury, Chandreyee},
Title = {A Survey of Machine Learning Techniques for Indoor Localization and
   Navigation Systems},
Journal = {JOURNAL OF INTELLIGENT \& ROBOTIC SYSTEMS},
Year = {2021},
Volume = {101},
Number = {3},
Month = {MAR 4},
Abstract = {In the recent past, we have witnessed the adoption of different machine
   learning techniques for indoor positioning applications using WiFi,
   Bluetooth and other technologies. The techniques range from
   heuristically derived hand-crafted feature-based traditional machine
   learning algorithms, feature selection algorithms to the hierarchically
   self-evolving feature-based Deep Learning algorithms. The transient and
   chaotic nature of the WiFi/Bluetooth fingerprint data along with
   different signal sensitivity of different device configurations presents
   numerous challenges that influence the performance of the indoor
   localization system in the wild. This article is intended to offer a
   comprehensive state-of-the-art survey on machine learning techniques
   that have recently been adopted for localization purposes. Hence, we
   review the applicability of machine learning techniques in this domain
   along with basic localization principles, applications, and the
   underlying problems and challenges associated with the existing systems.
   We also articulate the recent advances and state-of-the-art machine
   learning techniques to visualize the possible future directions in the
   research field of indoor localization.},
DOI = {10.1007/s10846-021-01327-z},
Article-Number = {63},
ISSN = {0921-0296},
EISSN = {1573-0409},
ResearcherID-Numbers = {Roy Karmakar, Priya/GLS-5145-2022},
ORCID-Numbers = {Roy, Priya/0000-0001-8790-0017
   },
Unique-ID = {WOS:000628641300001},
}

@article{ WOS:000701765000004,
Author = {Fan, Weiying and Chen, Yao and Li, Jiaqiang and Sun, Yue and Feng, Jian
   and Hassanin, Hany and Sareh, Pooya},
Title = {Machine learning applied to the design and inspection of reinforced
   concrete bridges: Resilient methods and emerging applications},
Journal = {STRUCTURES},
Year = {2021},
Volume = {33},
Pages = {3954-3963},
Month = {OCT},
Abstract = {Machine learning is one of the key pillars of industry 4.0 that has
   enabled rapid technological advancement through establishing complex
   connections among heterogeneous and highly complex engineering data
   automatically. Once the machine learning model is trained appropriately,
   it becomes able to effectively predict and make decisions. The
   technology is rapidly evolving and has found numerous applications in
   various branches of engineering due to its preponderance. This study is
   focused on exploring the recent advances of machine learning and its
   applications in reinforced concrete bridges. It covers a range of
   different machine learning techniques exploited in structural design,
   construction quality management, bridge engineering, and the inspection
   of reinforced concrete bridges. This review demonstrated that machine
   learning algorithms have established new research directions in bridge
   engineering, in particular for applications such as the form-finding of
   innovative long-span structures, structural reinforcement, and
   structural optimization.},
DOI = {10.1016/j.istruc.2021.06.110},
EarlyAccessDate = {JUL 2021},
ISSN = {2352-0124},
ResearcherID-Numbers = {CAI, Jianguo/HHZ-1309-2022
   Li, Jiaqiang/AAK-4903-2020
   Sun, Yue/B-1373-2013
   Sareh, Pooya/AGM-7498-2022
   Hassanin, Hany/C-3656-2012
   Chen, Yao/E-1154-2019
   },
ORCID-Numbers = {Feng, Jian/0000-0003-1508-0384
   Hassanin, Hany/0000-0002-8646-0520
   Chen, Yao/0000-0003-0924-8945
   Sareh, Pooya/0000-0003-1836-2598},
Unique-ID = {WOS:000701765000004},
}

@article{ WOS:000723910100001,
Author = {Rahmani, Amir Masoud and Yousefpoor, Efat and Yousefpoor, Mohammad
   Sadegh and Mehmood, Zahid and Haider, Amir and Hosseinzadeh, Mehdi and
   Ali Naqvi, Rizwan},
Title = {Machine Learning (ML) in Medicine: Review, Applications, and Challenges},
Journal = {MATHEMATICS},
Year = {2021},
Volume = {9},
Number = {22},
Month = {NOV},
Abstract = {Today, artificial intelligence (AI) and machine learning (ML) have
   dramatically advanced in various industries, especially medicine. AI
   describes computational programs that mimic and simulate human
   intelligence, for example, a person's behavior in solving problems or
   his ability for learning. Furthermore, ML is a subset of artificial
   intelligence. It extracts patterns from raw data automatically. The
   purpose of this paper is to help researchers gain a proper understanding
   of machine learning and its applications in healthcare. In this paper,
   we first present a classification of machine learning-based schemes in
   healthcare. According to our proposed taxonomy, machine learning-based
   schemes in healthcare are categorized based on data pre-processing
   methods (data cleaning methods, data reduction methods), learning
   methods (unsupervised learning, supervised learning, semi-supervised
   learning, and reinforcement learning), evaluation methods
   (simulation-based evaluation and practical implementation-based
   evaluation in real environment) and applications (diagnosis, treatment).
   According to our proposed classification, we review some studies
   presented in machine learning applications for healthcare. We believe
   that this review paper helps researchers to familiarize themselves with
   the newest research on ML applications in medicine, recognize their
   challenges and limitations in this area, and identify future research
   directions.},
DOI = {10.3390/math9222970},
Article-Number = {2970},
EISSN = {2227-7390},
ResearcherID-Numbers = {Yousefpoor, Efat/AIA-5998-2022
   Haider, Amir/AAY-8339-2020
   Mehmood, Dr. Zahid/S-1709-2018
   Mehmood, Zahid/S-1709-2018
   Rahmani, Amir Masoud/K-2702-2013
   Yousefpoor, Mohammad Sadegh/R-8864-2019
   Naqvi, Rizwan/AAW-9242-2020
   Hosseinzadeh, Mehdi/GWV-3822-2022},
ORCID-Numbers = {Mehmood, Dr. Zahid/0000-0003-4888-2594
   Haider, Amir/0000-0002-1534-061X
   Naqvi, Rizwan Ali/0000-0002-7473-8441
   Yousefpoor, Mohammad Sadegh/0000-0002-9012-6138
   , mehdi/0000-0003-1088-4551
   Hosseinzadeh, Mehdi/0000-0003-3040-1801
   },
Unique-ID = {WOS:000723910100001},
}

@article{ WOS:000731472800001,
Author = {Mainali, Shraddha and Darsie, Marin E. and Smetana, Keaton S.},
Title = {Machine Learning in Action: Stroke Diagnosis and Outcome Prediction},
Journal = {FRONTIERS IN NEUROLOGY},
Year = {2021},
Volume = {12},
Month = {DEC 6},
Abstract = {The application of machine learning has rapidly evolved in medicine over
   the past decade. In stroke, commercially available machine learning
   algorithms have already been incorporated into clinical application for
   rapid diagnosis. The creation and advancement of deep learning
   techniques have greatly improved clinical utilization of machine
   learning tools and new algorithms continue to emerge with improved
   accuracy in stroke diagnosis and outcome prediction. Although
   imaging-based feature recognition and segmentation have significantly
   facilitated rapid stroke diagnosis and triaging, stroke prognostication
   is dependent on a multitude of patient specific as well as clinical
   factors and hence accurate outcome prediction remains challenging.
   Despite its vital role in stroke diagnosis and prognostication, it is
   important to recognize that machine learning output is only as good as
   the input data and the appropriateness of algorithm applied to any
   specific data set. Additionally, many studies on machine learning tend
   to be limited by small sample size and hence concerted efforts to
   collate data could improve evaluation of future machine learning tools
   in stroke. In the present state, machine learning technology serves as a
   helpful and efficient tool for rapid clinical decision making while
   oversight from clinical experts is still required to address specific
   aspects not accounted for in an automated algorithm. This article
   provides an overview of machine learning technology and a tabulated
   review of pertinent machine learning studies related to stroke diagnosis
   and outcome prediction.},
DOI = {10.3389/fneur.2021.734345},
Article-Number = {734345},
ISSN = {1664-2295},
ResearcherID-Numbers = {Smetana, Keaton/ABA-6412-2020
   Mainali, Shraddha/AAA-4670-2021},
ORCID-Numbers = {Darsie, Marin/0000-0002-4890-7870
   Smetana, Keaton/0000-0002-4680-1116
   },
Unique-ID = {WOS:000731472800001},
}

@article{ WOS:000684547900025,
Author = {Beroza, Gregory C. and Segou, Margarita and Mostafa Mousavi, S.},
Title = {Machine learning and earthquake forecasting-next steps},
Journal = {NATURE COMMUNICATIONS},
Year = {2021},
Volume = {12},
Number = {1},
Month = {AUG 6},
Abstract = {A new generation of earthquake catalogs developed through supervised
   machine-learning illuminates earthquake activity with unprecedented
   detail. Application of unsupervised machine learning to analyze the more
   complete expression of seismicity in these catalogs may be the fastest
   route to improving earthquake forecasting.},
DOI = {10.1038/s41467-021-24952-6},
Article-Number = {4761},
EISSN = {2041-1723},
ResearcherID-Numbers = {Mousavi, Seyed/O-5713-2018
   },
ORCID-Numbers = {Segou, Margarita/0000-0001-8119-4019},
Unique-ID = {WOS:000684547900025},
}

@article{ WOS:000709466800001,
Author = {Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and Hoehndorf,
   Robert},
Title = {Semantic similarity and machine learning with ontologies},
Journal = {BRIEFINGS IN BIOINFORMATICS},
Year = {2021},
Volume = {22},
Number = {4},
Month = {JUL},
Abstract = {Ontologies have long been employed in the life sciences to formally
   represent and reason over domain knowledge and they are employed in
   almost every major biological database. Recently, ontologies are
   increasingly being used to provide background knowledge in
   similarity-based analysis and machine learning models. The methods
   employed to combine ontologies and machine learning are still novel and
   actively being developed. We provide an overview over the methods that
   use ontologies to compute similarity and incorporate them in machine
   learning methods; in particular, we outline how semantic similarity
   measures and ontology embeddings can exploit the background knowledge in
   ontologies and how ontologies can provide constraints that improve
   machine learning models. The methods and experiments we describe are
   available as a set of executable notebooks, and we also provide a set of
   slides and additional resources at
   https://github.com/bio-ontology-research-group/machine-learning-with-ont
   ologies.},
DOI = {10.1093/bib/bbaa199},
Article-Number = {bbaa199},
ISSN = {1467-5463},
EISSN = {1477-4054},
ResearcherID-Numbers = {Kulmanov, Maxat/AAG-5628-2021
   Smaili, Fatima/AAA-7996-2020
   Hoehndorf, Robert/H-6127-2019
   Gao, Xin/D-5487-2013
   },
ORCID-Numbers = {Kulmanov, Maxat/0000-0003-1710-1820
   Gao, Xin/0000-0002-7108-3574
   Hoehndorf, Robert/0000-0001-8149-5890
   Smaili, Fatima Zohra/0000-0001-6439-0659},
Unique-ID = {WOS:000709466800001},
}

@article{ WOS:000659549200030,
Author = {Chen, Haihua and Chen, Jiangping and Ding, Junhua},
Title = {Data Evaluation and Enhancement for Quality Improvement of Machine
   Learning},
Journal = {IEEE TRANSACTIONS ON RELIABILITY},
Year = {2021},
Volume = {70},
Number = {2},
Pages = {831-847},
Month = {JUN},
Abstract = {Poor data quality has a direct impact on the performance of the machine
   learning system that is built on the data. As a demonstrated effective
   approach for data quality improvement, transfer learning has been widely
   used to improve machine learning quality. However, the ``quality
   improvement{''} brought by transfer learning was rarely rigorously
   validated, and some of the quality improvement results were misleading.
   This article first exposed the hidden quality problem in the datasets
   used to build a machine learning system for normalizing medical concepts
   in social media text. The system was claimed to have achieved the best
   performance compared to existing work on a machine learning task.
   However, the results of our experiments showed that the ``best
   performance{''} was due to the poor quality of the datasets and the
   defective validation process. To address the data quality issue and
   build a high-performance medical concept normalization system, we
   developed a transfer-learning-based strategy for data quality
   enhancement and system performance improvement. The results of the
   experiments showed a strong correlation between the quality of the
   datasets and the performance of the machine learning system. The results
   also demonstrated that a rigorous evaluation of data quality is
   necessary for guiding the quality improvement of machine learning.
   Therefore, we propose a data quality evaluation framework that includes
   the quality criteria and their corresponding evaluation approaches. The
   data validation process, the performance improvement strategy, and the
   data quality evaluation framework discussed in this article can be used
   for machine learning researchers and practitioners to build
   high-performance machine learning systems. The code and datasets used in
   this research are available in GitHub
   (https://github.com/haihua0913/dataEvaluationML).},
DOI = {10.1109/TR.2021.3070863},
ISSN = {0018-9529},
EISSN = {1558-1721},
ORCID-Numbers = {Chen, Haihua/0000-0002-7088-9752},
Unique-ID = {WOS:000659549200030},
}

@article{ WOS:000649132600001,
Author = {Chantry, Matthew and Christensen, Hannah and Dueben, Peter and Palmer,
   Tim},
Title = {Opportunities and challenges for machine learning in weather and climate
   modelling: hard, medium and soft AI},
Journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL
   AND ENGINEERING SCIENCES},
Year = {2021},
Volume = {379},
Number = {2194},
Month = {APR 5},
Abstract = {In September 2019, a workshop was held to highlight the growing area of
   applying machine learning techniques to improve weather and climate
   prediction. In this introductory piece, we outline the motivations,
   opportunities and challenges ahead in this exciting avenue of research.
   This article is part of the theme issue `Machine learning for weather
   and climate modelling'.},
DOI = {10.1098/rsta.2020.0083},
Article-Number = {20200083},
ISSN = {1364-503X},
EISSN = {1471-2962},
ResearcherID-Numbers = {Christensen, Hannah/L-1180-2016
   },
ORCID-Numbers = {Dueben, Peter/0000-0002-4610-3326},
Unique-ID = {WOS:000649132600001},
}

@article{ WOS:000627674800001,
Author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, Celine and Gromowski,
   Mark and Schmid, Ute},
Title = {Beneficial and harmful explanatory machine learning},
Journal = {MACHINE LEARNING},
Year = {2021},
Volume = {110},
Number = {4},
Pages = {695-721},
Month = {APR},
Abstract = {Given the recent successes of Deep Learning in AI there has been
   increased interest in the role and need for explanations in machine
   learned theories. A distinct notion in this context is that of Michie's
   definition of ultra-strong machine learning (USML). USML is demonstrated
   by a measurable increase in human performance of a task following
   provision to the human of a symbolic machine learned theory for task
   performance. A recent paper demonstrates the beneficial effect of a
   machine learned logic theory for a classification task, yet no existing
   work to our knowledge has examined the potential harmfulness of
   machine's involvement for human comprehension during learning. This
   paper investigates the explanatory effects of a machine learned theory
   in the context of simple two person games and proposes a framework for
   identifying the harmfulness of machine explanations based on the
   Cognitive Science literature. The approach involves a cognitive window
   consisting of two quantifiable bounds and it is supported by empirical
   evidence collected from human trials. Our quantitative and qualitative
   results indicate that human learning aided by a symbolic machine learned
   theory which satisfies a cognitive window has achieved significantly
   higher performance than human self learning. Results also demonstrate
   that human learning aided by a symbolic machine learned theory that
   fails to satisfy this window leads to significantly worse performance
   than unaided human learning.},
DOI = {10.1007/s10994-020-05941-0},
EarlyAccessDate = {MAR 2021},
ISSN = {0885-6125},
EISSN = {1573-0565},
ResearcherID-Numbers = {Ai, Lun/ITT-4718-2023},
ORCID-Numbers = {Schmid, Ute/0000-0002-1301-0326
   Ai, Lun/0000-0003-2731-482X
   },
Unique-ID = {WOS:000627674800001},
}

@article{ WOS:000690882400007,
Author = {Ramadhan, Raden A. A. and Heatubun, Yosca R. J. and Tan, Sek F. and Lee,
   Hyun-Jin},
Title = {Comparison of physical and machine learning models for estimating solar
   irradiance and photovoltaic power},
Journal = {RENEWABLE ENERGY},
Year = {2021},
Volume = {178},
Pages = {1006-1019},
Month = {NOV},
Abstract = {Conventional models to estimate solar irradiance and photovoltaic power
   rely on physics and use empirical correlations to handle regional
   climate and complex physics. Recently, machine learning emerges as an
   advanced statistical tool to construct more accurate correlations
   between inputs and outputs. Although machine learning has been applied
   for modeling solar irradiance and power, no study has reported the
   accuracy improvement by machine learning compared to conventional
   physical models. Hence, this study aims to compare the accuracies of
   physical and machine learning models at each step of solar power
   modeling, i.e., modeling of global horizontal irradiance, direct normal
   irradiance, global tilted irradiance, and photovoltaic power. Comparison
   results demonstrated that machine learning models generally outperform
   physical models when input parameters are appropriately selected.
   Machine learning models more significantly reduced the mean bias
   difference (MBD) than the root mean square difference (RMSD). For global
   horizontal irradiance and photovoltaic power, machine learning models
   led to substantially unbiased estimations with 0.96\% and 0.03\% of MBD,
   respectively. Among machine learning algorithms, long short-term memory
   and gated recurrent unit were more recommendable. However, the physical
   model for solar power estimation was more efficient to reduce RMSD
   because of their ability to consider constant parameters as input. (C)
   2021 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.renene.2021.06.079},
EarlyAccessDate = {JUL 2021},
ISSN = {0960-1481},
EISSN = {1879-0682},
ORCID-Numbers = {Lee, Hyunjin/0000-0002-2112-354X},
Unique-ID = {WOS:000690882400007},
}

@article{ WOS:000660500300003,
Author = {Guan, Wen and Perdue, Gabriel and Pesah, Arthur and Schuld, Maria and
   Terashi, Koji and Vallecorsa, Sofia and Vlimant, Jean-Roch},
Title = {Quantum machine learning in high energy physics},
Journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {2},
Number = {1},
Month = {MAR},
Abstract = {Machine learning has been used in high energy physics (HEP) for a long
   time, primarily at the analysis level with supervised classification.
   Quantum computing was postulated in the early 1980s as way to perform
   computations that would not be tractable with a classical computer. With
   the advent of noisy intermediate-scale quantum computing devices, more
   quantum algorithms are being developed with the aim at exploiting the
   capacity of the hardware for machine learning applications. An
   interesting question is whether there are ways to apply quantum machine
   learning to HEP. This paper reviews the first generation of ideas that
   use quantum machine learning on problems in HEP and provide an outlook
   on future applications.},
DOI = {10.1088/2632-2153/abc17d},
Article-Number = {011003},
EISSN = {2632-2153},
ResearcherID-Numbers = {Terashi, Koji/ITW-2370-2023
   },
ORCID-Numbers = {Terashi, Koji/0000-0001-6520-8070
   Perdue, Gabriel/0000-0001-6785-8720
   Vallecorsa, Sofia/0000-0002-7003-5765
   Pesah, Arthur/0000-0002-5759-6314},
Unique-ID = {WOS:000660500300003},
}

@article{ WOS:000702918200001,
Author = {Qian, Xin and Yang, Ronggui},
Title = {Machine learning for predicting thermal transport properties of solids},
Journal = {MATERIALS SCIENCE \& ENGINEERING R-REPORTS},
Year = {2021},
Volume = {146},
Month = {OCT},
Abstract = {Quantitative descriptions of the structure-thermal property correlation
   have always been a challenging bottleneck in designing functional
   materials with superb thermal properties. In the past decade, the
   first-principlesbased modeling of phonon properties using density
   functional theory and the Boltzmann transport equation has become a
   common practice for predicting the thermal conductivity of new
   materials. However, firstprinciples calculations of thermal properties
   are too costly for high-throughput material screening and multiscale
   structural design. First-principles calculations also face several
   fundamental challenges in modeling thermal transport properties, for
   example, of crystalline materials with defects, of amorphous materials,
   and for materials at high temperatures. In the past five years or so,
   machine learning started to play a role in solving the aforementioned
   challenges. This review provides a comprehensive summary and discussion
   on the state-of-theart, future opportunities, and the remaining
   challenges in implementing machine learning techniques for studying
   thermal conductivity. After a brief introduction to the working
   principles of machine learning algorithms and descriptors for
   characterizing material structures, recent research using machine
   learning to study nanoscale thermal transport is discussed. Three major
   applications of machine learning techniques for predicting thermal
   properties are discussed. First, machine learning is applied to solve
   the challenges in modeling phonon transport of crystals with defects, in
   amorphous materials, and at high temperatures. In particular, machine
   learning is used to build high-fidelity interatomic potentials to bridge
   the gap between first-principles calculations and empirical molecular
   dynamics simulations. Second, machine learning can be used to study the
   correlation between thermal conductivity and other relevant properties
   for the high-throughput screening of functional materials. Finally,
   machine learning is a powerful tool for structural design to achieve
   target thermal conductance or thermal conductivity. This review
   concludes with a summary and outlook for future directions for
   implementing machine learning in thermal sciences.},
DOI = {10.1016/j.mser.2021.100642},
EarlyAccessDate = {SEP 2021},
Article-Number = {100642},
ISSN = {0927-796X},
EISSN = {1879-212X},
ResearcherID-Numbers = {Yang, Ronggui/H-1278-2011
   },
ORCID-Numbers = {YANG, RONGGUI/0000-0002-3602-6945
   Qian, Xin/0000-0002-3198-2014},
Unique-ID = {WOS:000702918200001},
}

@article{ WOS:000637712400007,
Author = {Wang, Zeyu and Liu, Jian and Zhang, Yuanxin and Yuan, Hongping and
   Zhang, Ruixue and Srinivasan, Ravi S.},
Title = {Practical issues in implementing machine-learning models for building
   energy efficiency: Moving beyond obstacles},
Journal = {RENEWABLE \& SUSTAINABLE ENERGY REVIEWS},
Year = {2021},
Volume = {143},
Month = {JUN},
Abstract = {Implementing machine-learning models in real applications is crucial to
   achieving intelligent building control and high energy efficiency. Over
   the past few decades, numerous studies have attempted to explore the
   application of machine-learning models to building energy efficiency.
   However, these studies have focused on analyzing the technical
   feasibility and superiority of machine learning algorithms for fitting
   building energyrelated data and have not considered methods of
   implementing machine learning technology in building energy efficiency
   applications. Therefore, this review aims to summarize the current
   practical issues involved in applying machine-learning models to
   building energy efficiency by systematically analyzing existing research
   findings and limitations. The paper first reviews the application status
   of machine learning-based building energy efficiency research by
   analyzing the model implementation process and summarizing the main uses
   of the technology in the overall building energy management life cycle.
   The paper then elaborates on the causes of, influences on, and potential
   solutions for practical issues found in the implementation and promotion
   of machine learning-based building energy efficiency measures. Finally,
   this paper discusses valuable future machine learning-based building
   energy efficiency research directions with regard to technology
   opportunity discovery, data governance, feature engineering,
   generalizability test, technology diffusion, and knowledge sharing. This
   paper will provide building researchers and practitioners with a better
   understanding of the current application statuses of and potential
   research directions for machine learning models in building energy
   efficiency.},
DOI = {10.1016/j.rser.2021.110929},
EarlyAccessDate = {MAR 2021},
Article-Number = {110929},
ISSN = {1364-0321},
EISSN = {1879-0690},
ResearcherID-Numbers = {Srinivasan, Ravi/C-6245-2019
   Zhang, Yuanxin/IQU-9607-2023
   Srinivasan, Ravi/F-1202-2014
   Yuan, Hongping/G-7441-2014
   Wang, Zeyu/MIP-4095-2025},
ORCID-Numbers = {Zhang, Yuanxin/0000-0002-2046-8116
   Srinivasan, Ravi/0000-0002-0461-5729
   },
Unique-ID = {WOS:000637712400007},
}

@article{ WOS:000687473600006,
Author = {Mihaljevic, Bojan and Bielza, Concha and Larranaga, Pedro},
Title = {Bayesian networks for interpretable machine learning and optimization},
Journal = {NEUROCOMPUTING},
Year = {2021},
Volume = {456},
Pages = {648-665},
Month = {OCT 7},
Abstract = {As artificial intelligence is being increasingly used for high-stakes
   applications, it is becoming more and more important that the models
   used be interpretable. Bayesian networks offer a paradigm for
   inter-pretable artificial intelligence that is based on probability
   theory. They provide a semantics that enables a compact, declarative
   representation of a joint probability distribution over the variables of
   a domain by leveraging the conditional independencies among them. The
   representation consists of a directed acyclic graph that encodes the
   conditional independencies among the variables and a set of parameters
   that encodes conditional distributions. This representation has provided
   a basis for the development of algo-rithms for probabilistic reasoning
   (inference) and for learning probability distributions from data.
   Bayesian networks are used for a wide range of tasks in machine
   learning, including clustering, super -vised classification,
   multi-dimensional supervised classification, anomaly detection, and
   temporal mod-eling. They also provide a basis for estimation of
   distribution algorithms, a class of evolutionary algorithms for
   heuristic optimization. We illustrate the use of Bayesian networks for
   interpretable machine learning and optimization by presenting
   applications in neuroscience, the industry, and bioin-formatics,
   covering a wide range of machine learning and optimization tasks. (c)
   2021 Published by Elsevier B.V.},
DOI = {10.1016/j.neucom.2021.01.138},
EarlyAccessDate = {AUG 2021},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Bielza, Concha/F-9277-2013
   Larranaga, Pedro/F-9293-2013
   MIHALJEVIC, BOJAN/X-6153-2018
   Mihaljevic, Bojan/X-6153-2018},
ORCID-Numbers = {Bielza, Concha/0000-0001-7109-2668
   Larranaga, Pedro/0000-0003-0652-9872
   MIHALJEVIC, BOJAN/0000-0002-1656-6135
   },
Unique-ID = {WOS:000687473600006},
}

@article{ WOS:000607931400002,
Author = {May, Mike},
Title = {Eight ways machine learning is assisting medicine},
Journal = {NATURE MEDICINE},
Year = {2021},
Volume = {27},
Number = {1},
Pages = {2-3},
Month = {JAN},
Abstract = {There has been a lot of hype around the applications of machine learning
   in medicine. But how is machine learning actually helping
   bench-to-bedside scientists and clinicians do their jobs?},
DOI = {10.1038/s41591-020-01197-2},
ISSN = {1078-8956},
EISSN = {1546-170X},
Unique-ID = {WOS:000607931400002},
}

@article{ WOS:000623811400031,
Author = {Osarogiagbon, Augustine Uhunoma and Khan, Faisal and Venkatesan,
   Ramachandran and Gillard, Paul},
Title = {Review and analysis of supervised machine learning algorithms for
   hazardous events in drilling operations},
Journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION},
Year = {2021},
Volume = {147},
Pages = {367-384},
Month = {MAR},
Abstract = {Results of bibliometric analysis and a detailed review are reported on
   the use of supervised machine learning to study hazardous drilling
   events. The bibliometric analysis attempts to answer pertinent questions
   related to progress in the use of supervised machine learning for
   hazardous events due to drilling fluid density/mud weight. The analysis
   indicates artificial neural network as the most popular algorithm among
   researchers. Also, deep learning, random forest and support vector
   machine have gained momentum in recent use.
   A critical review of literature on hazardous events and supervised
   machine learning algorithms are reported. This review was done to
   observe how the algorithms were used, their relative successes,
   limitations, as well as input parameters which aided in detection or
   estimation by the machine learning algorithms. An introduction to deep
   learning and a review of literature on the use of deep learning with
   respect to operations involving drilling parameters is presented. The
   review on deep learning and drilling parameters covered the following
   operations: lithology identification, drilling rig state determination,
   generating logging/other drilling parameters and detecting abnormality
   in data.
   The study highlights need of publicly accessible large database with
   data from different oilfields for development of machine learning
   algorithms. These algorithms could be used globally for the enhancement
   of machine learning for new fields or fields with limited data. The
   availability of such large database would aid researchers in improving
   or customizing deep learning algorithms in line with the unique needs of
   drilling activities. (C) 2020 Institution of Chemical Engineers.
   Published by Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.psep.2020.09.038},
ISSN = {0957-5820},
EISSN = {1744-3598},
ResearcherID-Numbers = {Khan, Faisal/AAO-6293-2020},
ORCID-Numbers = {Khan, Faisal/0000-0002-5638-4299
   Osarogiagbon, Augustine Uhunoma/0000-0002-9104-1507
   Venkatesan, Ramachandran/0000-0002-0337-5532
   },
Unique-ID = {WOS:000623811400031},
}

@article{ WOS:000700671700001,
Author = {Nikparvar, Behnam and Thill, Jean-Claude},
Title = {Machine Learning of Spatial Data},
Journal = {ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION},
Year = {2021},
Volume = {10},
Number = {9},
Month = {SEP},
Abstract = {Properties of spatially explicit data are often ignored or inadequately
   handled in machine learning for spatial domains of application. At the
   same time, resources that would identify these properties and
   investigate their influence and methods to handle them in machine
   learning applications are lagging behind. In this survey of the
   literature, we seek to identify and discuss spatial properties of data
   that influence the performance of machine learning. We review some of
   the best practices in handling such properties in spatial domains and
   discuss their advantages and disadvantages. We recognize two broad
   strands in this literature. In the first, the properties of spatial data
   are developed in the spatial observation matrix without amending the
   substance of the learning algorithm; in the other, spatial data
   properties are handled in the learning algorithm itself. While the
   latter have been far less explored, we argue that they offer the most
   promising prospects for the future of spatial machine learning.},
DOI = {10.3390/ijgi10090600},
Article-Number = {600},
EISSN = {2220-9964},
ResearcherID-Numbers = {nikparvar, behnam/ITT-3139-2023
   },
ORCID-Numbers = {nikparvar, behnam/0000-0002-7828-9356
   Thill, Jean-Claude/0000-0002-6651-8123},
Unique-ID = {WOS:000700671700001},
}

@article{ WOS:000425056700007,
Author = {Maxwell, Aaron E. and Warner, Timothy A. and Fang, Fang},
Title = {Implementation of machine-learning classification in remote sensing: an
   applied review},
Journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING},
Year = {2018},
Volume = {39},
Number = {9},
Pages = {2784-2817},
Abstract = {Machine learning offers the potential for effective and efficient
   classification of remotely sensed imagery. The strengths of machine
   learning include the capacity to handle data of high dimensionality and
   to map classes with very complex characteristics. Nevertheless,
   implementing a machine-learning classification is not straightforward,
   and the literature provides conflicting advice regarding many key
   issues. This article therefore provides an overview of machine learning
   from an applied perspective. We focus on the relatively mature methods
   of support vector machines, single decision trees (DTs), Random Forests,
   boosted DTs, artificial neural networks, and k-nearest neighbours
   (k-NN). Issues considered include the choice of algorithm, training data
   requirements, user-defined parameter selection and optimization, feature
   space impacts and reduction, and computational costs. We illustrate
   these issues through applying machine-learning classification to two
   publically available remotely sensed data sets.},
DOI = {10.1080/01431161.2018.1433343},
ISSN = {0143-1161},
EISSN = {1366-5901},
ResearcherID-Numbers = {Fang, Fang/AAA-4980-2019
   Warner, Timothy/F-1483-2010
   Maxwell, Aaron/AAN-8401-2021},
ORCID-Numbers = {Maxwell, Aaron/0000-0002-4412-5599
   Warner, Timothy/0000-0002-0414-9748
   },
Unique-ID = {WOS:000425056700007},
}

@article{ WOS:000432490900005,
Author = {Cai, Jie and Luo, Jiawei and Wang, Shulin and Yang, Sheng},
Title = {Feature selection in machine learning: A new perspective},
Journal = {NEUROCOMPUTING},
Year = {2018},
Volume = {300},
Pages = {70-79},
Month = {JUL 26},
Abstract = {High-dimensional data analysis is a challenge for researchers and
   engineers in the fields of machine learning and data mining. Feature
   selection provides an effective way to solve this problem by removing
   irrelevant and redundant data, which can reduce computation time,
   improve learning accuracy, and facilitate a better understanding for the
   learning model or data. In this study, we discuss several frequentlyused
   evaluation measures for feature selection, and then survey supervised,
   unsupervised, and semisupervised feature selection methods, which are
   widely applied in machine learning problems, such as classification and
   clustering. Lastly, future challenges about feature selection are
   discussed.},
DOI = {10.1016/j.neucom.2017.11.077},
ISSN = {0925-2312},
EISSN = {1872-8286},
Unique-ID = {WOS:000432490900005},
}

@article{ WOS:000645724900001,
Author = {Auslander, Noam and Gussow, Ayal B. and Koonin, Eugene V.},
Title = {Incorporating Machine Learning into Established Bioinformatics
   Frameworks},
Journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES},
Year = {2021},
Volume = {22},
Number = {6},
Month = {MAR},
Abstract = {The exponential growth of biomedical data in recent years has urged the
   application of numerous machine learning techniques to address emerging
   problems in biology and clinical research. By enabling the automatic
   feature extraction, selection, and generation of predictive models,
   these methods can be used to efficiently study complex biological
   systems. Machine learning techniques are frequently integrated with
   bioinformatic methods, as well as curated databases and biological
   networks, to enhance training and validation, identify the best
   interpretable features, and enable feature and model investigation.
   Here, we review recently developed methods that incorporate machine
   learning within the same framework with techniques from molecular
   evolution, protein structure analysis, systems biology, and disease
   genomics. We outline the challenges posed for machine learning, and, in
   particular, deep learning in biomedicine, and suggest unique
   opportunities for machine learning techniques integrated with
   established bioinformatics approaches to overcome some of these
   challenges.},
DOI = {10.3390/ijms22062903},
Article-Number = {2903},
ISSN = {1661-6596},
EISSN = {1422-0067},
ResearcherID-Numbers = {Auslander, Noam/ABX-6673-2022
   },
ORCID-Numbers = {Koonin, Eugene/0000-0003-3943-8299
   Ausalnder, Noam/0000-0002-1923-8735},
Unique-ID = {WOS:000645724900001},
}

@article{ WOS:000643700200002,
Author = {Kliegr, Tomas and Bahnik, Stepan and Fuernkranz, Johannes},
Title = {A review of possible effects of cognitive biases on interpretation of
   rule-based machine learning models},
Journal = {ARTIFICIAL INTELLIGENCE},
Year = {2021},
Volume = {295},
Month = {JUN},
Abstract = {While the interpretability of machine learning models is often equated
   with their mere syntactic comprehensibility, we think that
   interpretability goes beyond that, and that human interpretability
   should also be investigated from the point of view of cognitive science.
   The goal of this paper is to discuss to what extent cognitive biases may
   affect human understanding of interpretable machine learning models, in
   particular of logical rules discovered from data. Twenty cognitive
   biases are covered, as are possible debiasing techniques that can be
   adopted by designers of machine learning algorithms and software. Our
   review transfers results obtained in cognitive psychology to the domain
   of machine learning, aiming to bridge the current gap between these two
   areas. It needs to be followed by empirical studies specifically focused
   on the machine learning domain. (C) 2021 The Authors. Published by
   Elsevier B.V.},
DOI = {10.1016/j.artint.2021.103458},
EarlyAccessDate = {FEB 2021},
Article-Number = {103458},
ISSN = {0004-3702},
EISSN = {1872-7921},
ResearcherID-Numbers = {Kliegr, Tomáš/AAH-5577-2020
   Fürnkranz, Johannes/AAH-2585-2019},
ORCID-Numbers = {Bahnik, Stepan/0000-0002-0579-6808
   Furnkranz, Johannes/0000-0002-1207-0159
   },
Unique-ID = {WOS:000643700200002},
}

@article{ WOS:000624645700001,
Author = {Park, Hochong and Son, Joo-Hiuk},
Title = {Machine Learning Techniques for THz Imaging and Time-Domain Spectroscopy},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {4},
Month = {FEB},
Abstract = {Terahertz imaging and time-domain spectroscopy have been widely used to
   characterize the properties of test samples in various biomedical and
   engineering fields. Many of these tasks require the analysis of acquired
   terahertz signals to extract embedded information, which can be achieved
   using machine learning. Recently, machine learning techniques have
   developed rapidly, and many new learning models and learning algorithms
   have been investigated. Therefore, combined with state-of-the-art
   machine learning techniques, terahertz applications can be performed
   with high performance that cannot be achieved using modeling techniques
   that precede the machine learning era. In this review, we introduce the
   concept of machine learning and basic machine learning techniques and
   examine the methods for performance evaluation. We then summarize
   representative examples of terahertz imaging and time-domain
   spectroscopy that are conducted using machine learning.},
DOI = {10.3390/s21041186},
Article-Number = {1186},
EISSN = {1424-8220},
Unique-ID = {WOS:000624645700001},
}

@article{ WOS:000663460400001,
Author = {Bhamare, Dnyandip K. and Saikia, Pranaynil and Rathod, Manish K. and
   Rakshit, Dibakar and Banerjee, Jyotirmay},
Title = {A machine learning and deep learning based approach to predict the
   thermal performance of phase change material integrated building
   envelope},
Journal = {BUILDING AND ENVIRONMENT},
Year = {2021},
Volume = {199},
Month = {JUL 15},
Abstract = {This study aims to develop a machine learning and deep learning-based
   model for thermal performance prediction of PCM integrated roof
   building. Performance prediction is carried out using the newly proposed
   MKR index. Five machine learning and one deep learning technique are
   explored in order to predict the thermal performance of PCM integrated
   roof considering variations in thermophysical properties of PCM. Total
   500 data points are generated using numerical simulations considering
   variations in thermophysical properties of PCM. The five machine
   learning models used in this study are Random forest regression, Extra
   trees regression, Gradient boosting regression, Extreme Gradient
   boosting regression, and Catboost regression. The results indicate that
   Gradient boosting regression is the best-performing model compared to
   other machine learning models. An artificial neural network is used as a
   deep learning approach for predicting the MKR index. The ANN-based model
   performed best among all five machine learning models and proved its
   efficacy in training, testing, and sensitivity analysis with the
   independent dataset.},
DOI = {10.1016/j.buildenv.2021.107927},
EarlyAccessDate = {MAY 2021},
Article-Number = {107927},
ISSN = {0360-1323},
EISSN = {1873-684X},
ResearcherID-Numbers = {Rathod, Manish/Y-9545-2019
   Saikia, Pranaynil/OBO-9048-2025
   Banerjee, Jyotirmay/NDT-1472-2025},
ORCID-Numbers = {Rakshit, Dr Dibakar/0000-0002-8469-5924
   Saikia, Pranaynil/0000-0001-6565-5954
   },
Unique-ID = {WOS:000663460400001},
}

@article{ WOS:000697377500048,
Author = {Sonabend, Raphael and Kiraly, Franz J. and Bender, Andreas and Bischl,
   Bernd and Lang, Michel},
Title = {mlr3proba: an R package for machine learning in survival analysis},
Journal = {BIOINFORMATICS},
Year = {2021},
Volume = {37},
Number = {17},
Pages = {2789-2791},
Month = {SEP 1},
Abstract = {As machine learning has become increasingly popular over the last few
   decades, so too has the number of machine-learning interfaces for
   implementing these models. Whilst many R libraries exist for machine
   learning, very few offer extended support for survival analysis. This is
   problematic considering its importance in fields like medicine,
   bioinformatics, economics, engineering and more. mlr3proba provides a
   comprehensive machine-learning interface for survival analysis and
   connects with mlr3's general model tuning and benchmarking facilities to
   provide a systematic infrastructure for survival modelling and
   evaluation.},
DOI = {10.1093/bioinformatics/btab039},
EarlyAccessDate = {FEB 2021},
ISSN = {1367-4803},
EISSN = {1367-4811},
ResearcherID-Numbers = {Bender, Andreas/C-6942-2009
   Kiraly, Franz/JGD-2076-2023},
ORCID-Numbers = {Sonabend, Raphael/0000-0001-9225-4654
   Lang, Michel/0000-0001-9754-0393
   Kiraly, Franz/0000-0002-9254-793X
   },
Unique-ID = {WOS:000697377500048},
}

@article{ WOS:000626579600078,
Author = {Liaqat, Sidrah and Dashtipour, Kia and Arshad, Kamran and Assaleh,
   Khaled and Ramzan, Naeem},
Title = {A Hybrid Posture Detection Framework: Integrating Machine Learning and
   Deep Neural Networks},
Journal = {IEEE SENSORS JOURNAL},
Year = {2021},
Volume = {21},
Number = {7},
Pages = {9515-9522},
Month = {APR 1},
Abstract = {The posture detection received lots of attention in the fields of human
   sensing and artificial intelligence. Posture detection can be used for
   the monitoring health status of elderly remotely by identifying their
   postures such as standing, sitting and walking. Most of the current
   studies used traditional machine learning classifiers to identify the
   posture. However, these methods do not perform well to detect the
   postures accurately. Therefore, in this study, we proposed a novel
   hybrid approach based on machine learning classifiers (i. e., support
   vector machine (SVM), logistic regression (KNN), decision tree, Naive
   Bayes, random forest, Linear discrete analysis and Quadratic discrete
   analysis) and deep learning classifiers (i. e., 1D-convolutional neural
   network (1D-CNN), 2D-convolutional neural network (2D-CNN), LSTM and
   bidirectional LSTM) to identify posture detection. The proposed hybrid
   approach uses prediction of machine learning (ML) and deep learning (DL)
   to improve the performance of ML and DL algorithms. The experimental
   results on widely benchmark dataset are shown and results achieved an
   accuracy of more than 98\%.},
DOI = {10.1109/JSEN.2021.3055898},
ISSN = {1530-437X},
EISSN = {1558-1748},
ResearcherID-Numbers = {Dashtipour, Kia/AAX-9489-2020
   },
ORCID-Numbers = {Arshad, Kamran/0000-0002-4447-8335
   Dashtipour, Kia/0000-0002-9651-6487
   Ramzan, Naeem/0000-0002-5088-1462
   Assaleh, Khaled/0000-0002-0942-0453},
Unique-ID = {WOS:000626579600078},
}

@article{ WOS:000533911600040,
Author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
Title = {Machine Learning in Python: Main Developments and Technology Trends in
   Data Science, Machine Learning, and Artificial Intelligence},
Journal = {INFORMATION},
Year = {2020},
Volume = {11},
Number = {4},
Month = {APR},
Abstract = {Smarter applications are making better use of the insights gleaned from
   data, having an impact on every industry and research discipline. At the
   core of this revolution lies the tools and the methods that are driving
   it, from processing the massive piles of data generated each day to
   learning from and taking useful action. Deep neural networks, along with
   advancements in classical machine learning and scalable general-purpose
   graphics processing unit (GPU) computing, have become critical
   components of artificial intelligence, enabling many of these astounding
   breakthroughs and lowering the barrier to adoption. Python continues to
   be the most preferred language for scientific computing, data science,
   and machine learning, boosting both performance and productivity by
   enabling the use of low-level libraries and clean high-level APIs. This
   survey offers insight into the field of machine learning with Python,
   taking a tour through important topics to identify some of the core
   hardware and software paradigms that have enabled it. We cover
   widely-used libraries and concepts, collected together for holistic
   comparison, with the goal of educating the reader and driving the field
   of Python machine learning forward.},
DOI = {10.3390/info11040193},
Article-Number = {193},
EISSN = {2078-2489},
ResearcherID-Numbers = {Raschka, Sebastian/AAG-4178-2020},
ORCID-Numbers = {Raschka, Sebastian/0000-0001-6989-4493
   },
Unique-ID = {WOS:000533911600040},
}

@article{ WOS:000528284900001,
Author = {Kuwajima, Hiroshi and Yasuoka, Hirotoshi and Nakae, Toshihiro},
Title = {Engineering problems in machine learning systems},
Journal = {MACHINE LEARNING},
Year = {2020},
Volume = {109},
Number = {5},
Pages = {1103-1126},
Month = {MAY},
Abstract = {Fatal accidents are a major issue hindering the wide acceptance of
   safety-critical systems that employ machine learning and deep learning
   models, such as automated driving vehicles. In order to use machine
   learning in a safety-critical system, it is necessary to demonstrate the
   safety and security of the system through engineering processes.
   However, thus far, no such widely accepted engineering concepts or
   frameworks have been established for these systems. The key to using a
   machine learning model in a deductively engineered system is decomposing
   the data-driven training of machine learning models into requirement,
   design, and verification, particularly for machine learning models used
   in safety-critical systems. Simultaneously, open problems and relevant
   technical fields are not organized in a manner that enables researchers
   to select a theme and work on it. In this study, we identify, classify,
   and explore the open problems in engineering (safety-critical) machine
   learning systems-that is, in terms of requirement, design, and
   verification of machine learning models and systems-as well as discuss
   related works and research directions, using automated driving vehicles
   as an example. Our results show that machine learning models are
   characterized by a lack of requirements specification, lack of design
   specification, lack of interpretability, and lack of robustness. We also
   perform a gap analysis on a conventional system quality standard SQuaRE
   with the characteristics of machine learning models to study quality
   models for machine learning systems. We find that a lack of requirements
   specification and lack of robustness have the greatest impact on
   conventional quality models.},
DOI = {10.1007/s10994-020-05872-w},
EarlyAccessDate = {APR 2020},
ISSN = {0885-6125},
EISSN = {1573-0565},
ORCID-Numbers = {Kuwajima, Hiroshi/0000-0003-0731-8057},
Unique-ID = {WOS:000528284900001},
}

@article{ WOS:000632089400001,
Author = {Li, Yanbin and Lei, Gang and Bramerdorfer, Gerd and Peng, Sheng and Sun,
   Xiaodong and Zhu, Jianguo},
Title = {Machine Learning for Design Optimization of Electromagnetic Devices:
   Recent Developments and Future Directions},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2021},
Volume = {11},
Number = {4},
Month = {FEB},
Abstract = {This paper reviews the recent developments of design optimization
   methods for electromagnetic devices, with a focus on machine learning
   methods. First, the recent advances in multi-objective,
   multidisciplinary, multilevel, topology, fuzzy, and robust design
   optimization of electromagnetic devices are overviewed. Second, a review
   is presented to the performance prediction and design optimization of
   electromagnetic devices based on the machine learning algorithms,
   including artificial neural network, support vector machine, extreme
   learning machine, random forest, and deep learning. Last, to meet modern
   requirements of high manufacturing/production quality and lifetime
   reliability, several promising topics, including the application of
   cloud services and digital twin, are discussed as future directions for
   design optimization of electromagnetic devices.},
DOI = {10.3390/app11041627},
Article-Number = {1627},
EISSN = {2076-3417},
ResearcherID-Numbers = {Zhu, Jianguo/AAG-7918-2019
   Bramerdorfer, Gerd/V-5750-2019
   },
ORCID-Numbers = {Bramerdorfer, Gerd/0000-0002-0987-7306
   Zhu, Jianguo/0000-0002-9763-4047
   Sun, Xiaodong/0000-0002-9451-3311},
Unique-ID = {WOS:000632089400001},
}

@article{ WOS:000704508900007,
Author = {Lalehzarian, Simon P. and Gowd, Anirudh K. and Liu, Joseph N.},
Title = {Machine learning in orthopaedic surgery},
Journal = {WORLD JOURNAL OF ORTHOPEDICS},
Year = {2021},
Volume = {12},
Number = {9},
Pages = {685-699},
Month = {SEP 18},
Abstract = {Artificial intelligence and machine learning in orthopaedic surgery has
   gained mass interest over the last decade or so. In prior studies,
   researchers have demonstrated that machine learning in orthopaedics can
   be used for different applications such as fracture detection, bone
   tumor diagnosis, detecting hip implant mechanical loosening, and grading
   osteoarthritis. As time goes on, the utility of artificial intelligence
   and machine learning algorithms, such as deep learning, continues to
   grow and expand in orthopaedic surgery. The purpose of this review is to
   provide an understanding of the concepts of machine learning and a
   background of current and future orthopaedic applications of machine
   learning in risk assessment, outcomes assessment, imaging, and basic
   science fields. In most cases, machine learning has proven to be just as
   effective, if not more effective, than prior methods such as logistic
   regression in assessment and prediction. With the help of deep learning
   algorithms, such as artificial neural networks and convolutional neural
   networks, artificial intelligence in orthopaedics has been able to
   improve diagnostic accuracy and speed, flag the most critical and urgent
   patients for immediate attention, reduce the amount of human error,
   reduce the strain on medical professionals, and improve care. Because
   machine learning has shown diagnostic and prognostic uses in orthopaedic
   surgery, physicians should continue to research these techniques and be
   trained to use these methods effectively in order to improve orthopaedic
   treatment.},
DOI = {10.5312/wjo.v12.i9.685},
ISSN = {2218-5836},
ResearcherID-Numbers = {Liu`, Joseph/AAI-8201-2020},
ORCID-Numbers = {Liu`, Joseph/0000-0002-3801-8885},
Unique-ID = {WOS:000704508900007},
}

@article{ WOS:000718837900001,
Author = {Alchieri, Leonardo and Badalotti, Davide and Bonardi, Pietro and Bianco,
   Simone},
Title = {An introduction to quantum machine learning: from quantum logic to
   quantum deep learning},
Journal = {QUANTUM MACHINE INTELLIGENCE},
Year = {2021},
Volume = {3},
Number = {2},
Month = {DEC},
Abstract = {The aim of this work is to give an introduction for a non-practical
   reader to the growing field of quantum machine learning, which is a
   recent discipline that combines the research areas of machine learning
   and quantum computing. This work presents the most notable scientific
   literature about quantum machine learning, starting from the basics of
   quantum logic to some specific elements and algorithms of quantum
   computing (such as QRAM, Grover and HHL), in order to allow a better
   understanding of latest quantum machine learning techniques. The main
   aspects of quantum machine learning are then covered, with detailed
   descriptions of some notable algorithms, such as quantum natural
   gradient and quantum support vector machines, up to the most recent
   quantum deep learning techniques, such as quantum neural networks.},
DOI = {10.1007/s42484-021-00056-8},
Article-Number = {28},
ISSN = {2524-4906},
EISSN = {2524-4914},
ResearcherID-Numbers = {Alchieri, Leonardo/LWI-8827-2024
   Bianco, Simone/T-1224-2019
   Badalotti, Davide/LSJ-0671-2024},
ORCID-Numbers = {Badalotti, Davide/0000-0002-0944-9598
   Bianco, Simone/0000-0002-7070-1545
   Alchieri, Leonardo/0000-0003-3088-6132
   },
Unique-ID = {WOS:000718837900001},
}

@article{ WOS:000739121700001,
Author = {Nevin, Josh W. and Nallaperuma, Sam and Shevchenko, Nikita A. and Li,
   Xiang and Faruk, Md. Saifuddin and Savory, Seb J.},
Title = {Machine learning for optical fiber communication systems: An
   introduction and overview},
Journal = {APL PHOTONICS},
Year = {2021},
Volume = {6},
Number = {12},
Month = {DEC 1},
Abstract = {Optical networks generate a vast amount of diagnostic, control, and
   performance monitoring data. When information is extracted from these
   data, reconfigurable network elements and reconfigurable transceivers
   allow the network to adapt not only to changes in the physical
   infrastructure but also to changing traffic conditions. Machine learning
   is emerging as a disruptive technology for extracting useful information
   from these raw data to enable enhanced planning, monitoring, and dynamic
   control. We provide a survey of the recent literature and highlight
   numerous promising avenues for machine learning applied to optical
   networks, including explainable machine learning, digital twins, and
   approaches in which we embed our knowledge into machine learning such as
   physics-informed machine learning for the physical layer and graph-based
   machine learning for the networking layer.},
DOI = {10.1063/5.0070838},
Article-Number = {121101},
ISSN = {2378-0967},
ResearcherID-Numbers = {Faruk, Md. Saifuddin/G-3764-2011},
ORCID-Numbers = {Savory, Seb/0000-0002-6803-718X
   Shevchenko, Mykyta/0000-0001-7094-1322
   },
Unique-ID = {WOS:000739121700001},
}

@article{ WOS:000703568200007,
Author = {Harrison, James H. Jr Jr and Gilbertson, John R. and Hanna, Matthew G.
   and Olson, Niels H. and Seheult, Jansen N. and Sorace, James M. and
   Stram, Michelle N.},
Title = {Introduction to Artificial Intelligence and Machine Learning for
   Pathology},
Journal = {ARCHIVES OF PATHOLOGY \& LABORATORY MEDICINE},
Year = {2021},
Volume = {145},
Number = {10},
Pages = {1228-1254},
Month = {OCT},
Abstract = {center dot Context.-Recent developments in machine learning have
   stimulated intense interest in software that may augment or replace
   human experts. Machine learning may impact pathology practice by
   offering new capabilities in analysis, interpretation, and outcomes
   prediction using images and other data. The principles of operation and
   management of machine learning systems are unfamiliar to pathologists,
   who anticipate a need for additional education to be effective as expert
   users and managers of the new tools. Objective.-To provide a background
   on machine learning for practicing pathologists, including an overview
   of algorithms, model development, and performance evaluation; to examine
   the current status of machine learning in pathology and consider
   possible roles and requirements for pathologists in local deployment and
   management of machine learning systems; and to highlight existing
   challenges and gaps in deployment methodology and regulation. Data
   Sources.-Sources include the biomedical and engineering literature,
   white papers from professional organizations, government reports,
   electronic resources, and authors' experience in machine learning.
   References were chosen when possible for accessibility to practicing
   pathologists without specialized training in mathematics, statistics, or
   software development. Conclusions.-Machine learning offers an array of
   techniques that in recent published results show substantial promise.
   Data suggest that human experts working with machine learning tools
   outperform humans or machines separately, but the optimal form for this
   combination in pathology has not been established. Significant questions
   related to the generalizability of machine learning systems, local site
   verification, and performance monitoring remain to be resolved before a
   consensus on best practices and a regulatory environment can be
   established. (Arch Pathol Lab Med. 2021;145:1228-1254 ; doi:
   10.5858/arpa.2020-0541-CP)},
DOI = {10.5858/arpa.2020-0541-CP)},
ISSN = {0003-9985},
EISSN = {1543-2165},
ResearcherID-Numbers = {Stram, Michelle/CAJ-2174-2022
   Seheult, Jansen/AAB-6713-2020
   },
ORCID-Numbers = {Olson, Niels/0000-0003-2079-9641
   Seheult, Jansen/0000-0002-6850-7495},
Unique-ID = {WOS:000703568200007},
}

@incollection{ WOS:000713670600026,
Author = {Donti, Priya L. and Kolter, J. Zico},
Editor = {Gadgil, A and Tomich, TP},
Title = {Machine Learning for Sustainable Energy Systems},
Booktitle = {ANNUAL REVIEW OF ENVIRONMENT AND RESOURCES, VOL 46, 2021},
Series = {Annual Review of Environment and Resources},
Year = {2021},
Volume = {46},
Pages = {719-747},
Abstract = {In recent years, machine learning has proven to be a powerful tool for
   deriving insights from data. In this review, we describe ways in which
   machine learning has been leveraged to facilitate the development and
   operation of sustainable energy systems. We first provide a taxonomy of
   machine learning paradigms and techniques, along with a discussion of
   their strengths and limitations. We then provide an overview of existing
   research using machine learning for sustainable energy production,
   delivery, and storage. Finally, we identify gaps in this literature,
   propose future research directions, and discuss important considerations
   for deployment.},
DOI = {10.1146/annurev-environ-020220-061831},
ISSN = {1543-5938},
ISBN = {978-0-8243-2346-2},
ORCID-Numbers = {Donti, Priya/0000-0002-8503-7464},
Unique-ID = {WOS:000713670600026},
}

@article{ WOS:000701828000003,
Author = {Gundersen, Odd Erik and Shamsaliei, Saeid and Isdahl, Richard Juul},
Title = {Do machine learning platforms provide out-of-the-box reproducibility?},
Journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
Year = {2022},
Volume = {126},
Pages = {34-47},
Month = {JAN},
Abstract = {Science is experiencing an ongoing reproducibility crisis. In light of
   this crisis, our objective is to investigate whether machine learning
   platforms provide out-of-the-box reproducibility. Our method is twofold:
   First, we survey machine learning platforms for whether they provide
   features that simplify making experiments reproducible out-of-the-box.
   Second, we conduct the exact same experiment on four different machine
   learning platforms, and by this varying the processing unit and
   ancillary software only. The survey shows that no machine learning
   platform supports the feature set described by the proposed framework
   while the experiment reveals statstically significant difference in
   results when the exact same experiment is conducted on different machine
   learning platforms. The surveyed machine learning platforms do not on
   their own enable users to achieve the full reproducibility potential of
   their research. Also, the machine learning platforms with most users
   provide less functionality for achieving it. Furthermore, results differ
   when executing the same experiment on the different platforms. Wrong
   conclusions can be inferred at the at 95\% confidence level. Hence, we
   conclude that machine learning platforms do not provide reproducibility
   out-of-the-box and that results generated from one machine learning
   platform alone cannot be fully trusted. (C) 2021 The Author(s).
   Published by Elsevier B.V.},
DOI = {10.1016/j.future.2021.06.014},
EarlyAccessDate = {AUG 2021},
ISSN = {0167-739X},
EISSN = {1872-7115},
ResearcherID-Numbers = {Gundersen, Odd Erik/IUQ-6430-2023},
ORCID-Numbers = {Gundersen, Odd Erik/0000-0002-9754-5941
   },
Unique-ID = {WOS:000701828000003},
}

@article{ WOS:000676750300001,
Author = {Dumakor-Dupey, Nelson K. and Arya, Sampurna},
Title = {Machine Learning-A Review of Applications in Mineral Resource Estimation},
Journal = {ENERGIES},
Year = {2021},
Volume = {14},
Number = {14},
Month = {JUL},
Abstract = {Mineral resource estimation involves the determination of the grade and
   tonnage of a mineral deposit based on its geological characteristics
   using various estimation methods. Conventional estimation methods, such
   as geometric and geostatistical techniques, remain the most widely used
   methods for resource estimation. However, recent advances in computer
   algorithms have allowed researchers to explore the potential of machine
   learning techniques in mineral resource estimation. This study presents
   a comprehensive review of papers that have employed machine learning to
   estimate mineral resources. The review covers popular machine learning
   techniques and their implementation and limitations. Papers that
   performed a comparative analysis of both conventional and machine
   learning techniques were also considered. The literature shows that the
   machine learning models can accommodate several geological parameters
   and effectively approximate complex nonlinear relationships among them,
   exhibiting superior performance over the conventional techniques.},
DOI = {10.3390/en14144079},
Article-Number = {4079},
EISSN = {1996-1073},
ResearcherID-Numbers = {Dumakor-Dupey, Nelson/AAT-9328-2020
   Arya, Sampurna/T-4776-2019},
ORCID-Numbers = {Dumakor-Dupey, Nelson/0000-0001-7385-2173
   Arya, Sampurna/0000-0002-6796-3974},
Unique-ID = {WOS:000676750300001},
}

@article{ WOS:000611115200001,
Author = {Vaccaro, Lorenzo and Sansonetti, Giuseppe and Micarelli, Alessandro},
Title = {An Empirical Review of Automated Machine Learning},
Journal = {COMPUTERS},
Year = {2021},
Volume = {10},
Number = {1},
Month = {JAN},
Abstract = {In recent years, Automated Machine Learning (AutoML) has become
   increasingly important in Computer Science due to the valuable potential
   it offers. This is testified by the high number of works published in
   the academic field and the significant efforts made in the industrial
   sector. However, some problems still need to be resolved. In this paper,
   we review some Machine Learning (ML) models and methods proposed in the
   literature to analyze their strengths and weaknesses. Then, we propose
   their use-alone or in combination with other approaches-to provide
   possible valid AutoML solutions. We analyze those solutions from a
   theoretical point of view and evaluate them empirically on three Atari
   games from the Arcade Learning Environment. Our goal is to identify
   what, we believe, could be some promising ways to create truly effective
   AutoML frameworks, therefore able to replace the human expert as much as
   possible, thereby making easier the process of applying ML approaches to
   typical problems of specific domains. We hope that the findings of our
   study will provide useful insights for future research work in AutoML.},
DOI = {10.3390/computers10010011},
Article-Number = {11},
ISSN = {2073-431X},
ORCID-Numbers = {Sansonetti, Giuseppe/0000-0003-4953-1390
   micarelli, alessandro/0000-0002-0495-5272},
Unique-ID = {WOS:000611115200001},
}

@article{ WOS:000685591400001,
Author = {Boudreaux, Edwin D. and Rundensteiner, Elke and Liu, Feifan and Wang, Bo
   and Larkin, Celine and Agu, Emmanuel and Ghosh, Samiran and Semeter,
   Joshua and Simon, Gregory and Davis-Martin, Rachel E.},
Title = {Applying Machine Learning Approaches to Suicide Prediction Using
   Healthcare Data: Overview and Future Directions},
Journal = {FRONTIERS IN PSYCHIATRY},
Year = {2021},
Volume = {12},
Month = {AUG 3},
Abstract = {Objective: Early identification of individuals who are at risk for
   suicide is crucial in supporting suicide prevention. Machine learning is
   emerging as a promising approach to support this objective. Machine
   learning is broadly defined as a set of mathematical models and
   computational algorithms designed to automatically learn complex
   patterns between predictors and outcomes from example data, without
   being explicitly programmed to do so. The model's performance
   continuously improves over time by learning from newly available data.
   Method: This concept paper explores how machine learning approaches
   applied to healthcare data obtained from electronic health records,
   including billing and claims data, can advance our ability to accurately
   predict future suicidal behavior.
   Results: We provide a general overview of machine learning concepts,
   summarize exemplar studies, describe continued challenges, and propose
   innovative research directions.
   Conclusion: Machine learning has potential for improving estimation of
   suicide risk, yet important challenges and opportunities remain. Further
   research can focus on incorporating evolving methods for addressing data
   imbalances, understanding factors that affect generalizability across
   samples and healthcare systems, expanding the richness of the data,
   leveraging newer machine learning approaches, and developing automatic
   learning systems.},
DOI = {10.3389/fpsyt.2021.707916},
Article-Number = {707916},
ISSN = {1664-0640},
ResearcherID-Numbers = {Larkin, Celine/HOC-7834-2023
   Liu, Feifan/D-2478-2012},
ORCID-Numbers = {Larkin, Celine/0000-0003-2961-0585
   },
Unique-ID = {WOS:000685591400001},
}

@article{ WOS:000611043000001,
Author = {Jacobucci, Ross and Littlefield, Andrew K. and Millner, Alexander J. and
   Kleiman, Evan M. and Steinley, Douglas},
Title = {Evidence of Inflated Prediction Performance: A Commentary on Machine
   Learning and Suicide Research},
Journal = {CLINICAL PSYCHOLOGICAL SCIENCE},
Year = {2021},
Volume = {9},
Number = {1},
Pages = {129-134},
Month = {JAN},
Abstract = {The use of machine learning is increasing in clinical psychology, yet it
   is unclear whether these approaches enhance the prediction of clinical
   outcomes. Several studies show that machine-learning algorithms
   outperform traditional linear models. However, many studies that have
   found such an advantage use the same algorithm, random forests with the
   optimism-corrected bootstrap, for internal validation. Through both a
   simulation and empirical example, we demonstrate that the pairing of
   nonlinear, flexible machine-learning approaches, such as random forests
   with the optimism-corrected bootstrap, provide highly inflated
   prediction estimates. We find no advantage for properly validated
   machine-learning models over linear models.},
DOI = {10.1177/2167702620954216},
EarlyAccessDate = {JAN 2021},
ISSN = {2167-7026},
EISSN = {2167-7034},
ResearcherID-Numbers = {Kleiman, Evan/H-3908-2019
   },
ORCID-Numbers = {Littlefield, Andrew/0000-0002-4725-1152
   Jacobucci, Ross/0000-0001-7818-7424
   Millner, Alexander/0000-0001-6092-2857},
Unique-ID = {WOS:000611043000001},
}

@article{ WOS:000565731100014,
Author = {Futoma, Joseph and Simons, Morgan and Panch, Trishan and Doshi-Velez,
   Finale and Celi, Leo Anthony},
Title = {The myth of generalisability in clinical research and machine learning
   in health care},
Journal = {LANCET DIGITAL HEALTH},
Year = {2020},
Volume = {2},
Number = {9},
Pages = {E489-E492},
Month = {SEP},
Abstract = {An emphasis on overly broad notions of generalisability as it pertains
   to applications of machine learning in health care can overlook
   situations in which machine learning might provide clinical utility. We
   believe that this narrow focus on generalisability should be replaced
   with wider considerations for the ultimate goal of building machine
   learning systems that are useful at the bedside.},
EISSN = {2589-7500},
Unique-ID = {WOS:000565731100014},
}

@article{ WOS:000537106200097,
Author = {Merenda, Massimo and Porcaro, Carlo and Iero, Demetrio},
Title = {Edge Machine Learning for AI-Enabled IoT Devices: A Review},
Journal = {SENSORS},
Year = {2020},
Volume = {20},
Number = {9},
Month = {MAY},
Abstract = {In a few years, the world will be populated by billions of connected
   devices that will be placed in our homes, cities, vehicles, and
   industries. Devices with limited resources will interact with the
   surrounding environment and users. Many of these devices will be based
   on machine learning models to decode meaning and behavior behind
   sensors' data, to implement accurate predictions and make decisions. The
   bottleneck will be the high level of connected things that could congest
   the network. Hence, the need to incorporate intelligence on end devices
   using machine learning algorithms. Deploying machine learning on such
   edge devices improves the network congestion by allowing computations to
   be performed close to the data sources. The aim of this work is to
   provide a review of the main techniques that guarantee the execution of
   machine learning models on hardware with low performances in the
   Internet of Things paradigm, paving the way to the Internet of Conscious
   Things. In this work, a detailed review on models, architecture, and
   requirements on solutions that implement edge machine learning on
   Internet of Things devices is presented, with the main goal to define
   the state of the art and envisioning development requirements.
   Furthermore, an example of edge machine learning implementation on a
   microcontroller will be provided, commonly regarded as the machine
   learning ``Hello World{''}.},
DOI = {10.3390/s20092533},
Article-Number = {2533},
EISSN = {1424-8220},
ResearcherID-Numbers = {Iero, Demetrio/AAO-1064-2020
   Merenda, Massimo/K-7061-2013},
ORCID-Numbers = {Merenda, Massimo/0000-0003-3668-8014},
Unique-ID = {WOS:000537106200097},
}

@article{ WOS:000542963900016,
Author = {Kato, Nei and Mao, Bomin and Tang, Fengxiao and Kawamoto, Yuichi and
   Liu, Jiajia},
Title = {Ten Challenges in Advancing Machine Learning Technologies toward 6G},
Journal = {IEEE WIRELESS COMMUNICATIONS},
Year = {2020},
Volume = {27},
Number = {3},
Pages = {96-103},
Month = {JUN},
Abstract = {As the 5G standard is being completed, academia and industry have begun
   to consider a more developed cellular communication technique, 6G, which
   is expected to achieve high data rates up to 1 Tb/s and broad frequency
   bands of 100 GHz to 3 THz. Besides the significant upgrade of the key
   communication metrics, Artificial Intelligence (AI) has been envisioned
   by many researchers as the most important feature of 6G, since the
   state-of-the-art machine learning technique has been adopted as the top
   solution in many extremely complex scenarios. Network intelligentization
   will be the new trend to address the challenges of exponentially
   increasing number of connected heterogeneous devices. However, compared
   with the application of machine learning in other fields, such as
   computer games, current research on intelligent networking still has a
   long way to go to realize the automatically- configured cellular
   communication systems. Various problems in terms of communication
   system, machine learning architectures, and computation efficiency
   should be addressed for the full use of this technique in 6G. In this
   paper, we analyze machine learning techniques and introduce 10 most
   critical challenges in advancing the intelligent 6G system.},
DOI = {10.1109/MWC.001.1900476},
ISSN = {1536-1284},
EISSN = {1558-0687},
ResearcherID-Numbers = {Tang, Fengxiao/AFN-7960-2022
   KATO, NEI/T-5892-2019
   Mao, Bomin/AAG-8851-2019
   LIU, JIAJIA/HMD-9871-2023
   Kawamoto, Yuichi/GPC-7068-2022
   },
ORCID-Numbers = {Mao, Bomin/0000-0001-7780-5972
   Tang, Fengxiao/0000-0003-2414-4802
   LIU, JIAJIA/0000-0003-4273-8866},
Unique-ID = {WOS:000542963900016},
}

@article{ WOS:000594889300001,
Author = {Patel, Lauv and Shukla, Tripti and Huang, Xiuzhen and Ussery, David W.
   and Wang, Shanzhi},
Title = {Machine Learning Methods in Drug Discovery},
Journal = {MOLECULES},
Year = {2020},
Volume = {25},
Number = {22},
Month = {NOV},
Abstract = {The advancements of information technology and related processing
   techniques have created a fertile base for progress in many scientific
   fields and industries. In the fields of drug discovery and development,
   machine learning techniques have been used for the development of novel
   drug candidates. The methods for designing drug targets and novel drug
   discovery now routinely combine machine learning and deep learning
   algorithms to enhance the efficiency, efficacy, and quality of developed
   outputs. The generation and incorporation of big data, through
   technologies such as high-throughput screening and high through-put
   computational analysis of databases used for both lead and target
   discovery, has increased the reliability of the machine learning and
   deep learning incorporated techniques. The use of these virtual
   screening and encompassing online information has also been highlighted
   in developing lead synthesis pathways. In this review, machine learning
   and deep learning algorithms utilized in drug discovery and associated
   techniques will be discussed. The applications that produce promising
   results and methods will be reviewed.},
DOI = {10.3390/molecules25225277},
Article-Number = {5277},
EISSN = {1420-3049},
ResearcherID-Numbers = {Ussery, David/J-2026-2019
   Wang, Shanzhi/AAA-3595-2021},
ORCID-Numbers = {Wang, Shanzhi/0000-0002-7068-0756
   Ussery, David/0000-0003-3632-5512
   },
Unique-ID = {WOS:000594889300001},
}

@article{ WOS:000551576900008,
Author = {Liebal, Ulf W. and Phan, An N. T. and Sudhakar, Malvika and Raman,
   Karthik and Blank, Lars M.},
Title = {Machine Learning Applications for Mass Spectrometry-Based Metabolomics},
Journal = {METABOLITES},
Year = {2020},
Volume = {10},
Number = {6},
Month = {JUN},
Abstract = {The metabolome of an organism depends on environmental factors and
   intracellular regulation and provides information about the
   physiological conditions. Metabolomics helps to understand disease
   progression in clinical settings or estimate metabolite overproduction
   for metabolic engineering. The most popular analytical metabolomics
   platform is mass spectrometry (MS). However, MS metabolome data analysis
   is complicated, since metabolites interact nonlinearly, and the data
   structures themselves are complex. Machine learning methods have become
   immensely popular for statistical analysis due to the inherent nonlinear
   data representation and the ability to process large and heterogeneous
   data rapidly. In this review, we address recent developments in using
   machine learning for processing MS spectra and show how machine learning
   generates new biological insights. In particular, supervised machine
   learning has great potential in metabolomics research because of the
   ability to supply quantitative predictions. We review here commonly used
   tools, such as random forest, support vector machines, artificial neural
   networks, and genetic algorithms. During processing steps, the
   supervised machine learning methods help peak picking, normalization,
   and missing data imputation. For knowledge-driven analysis, machine
   learning contributes to biomarker detection, classification and
   regression, biochemical pathway identification, and carbon flux
   determination. Of important relevance is the combination of different
   omics data to identify the contributions of the various regulatory
   levels. Our overview of the recent publications also highlights that
   data quality determines analysis quality, but also adds to the challenge
   of choosing the right model for the data. Machine learning methods
   applied to MS-based metabolomics ease data analysis and can support
   clinical decisions, guide metabolic engineering, and stimulate
   fundamental biological discoveries.},
DOI = {10.3390/metabo10060243},
Article-Number = {243},
EISSN = {2218-1989},
ResearcherID-Numbers = {Blank, Lars/JFA-3308-2023
   Raman, Karthik/A-6459-2011
   Blank, Lars M./A-6761-2012
   Raman, Karthik/Q-5278-2019},
ORCID-Numbers = {THUY AN, PHAN NGUYEN/0000-0002-1438-416X
   Liebal, Ulf/0000-0001-5172-7339
   Raman, Karthik/0000-0002-9311-7093
   Sudhakar, Malvika/0000-0002-2442-4305
   Blank, Lars M./0000-0003-0961-4976
   },
Unique-ID = {WOS:000551576900008},
}

@incollection{ WOS:000590407100004,
Author = {Morgan, Dane and Jacobs, Ryan},
Editor = {Clarke, DR},
Title = {Opportunities and Challenges for Machine Learning in Materials Science},
Booktitle = {ANNUAL REVIEW OF MATERIALS RESEARCH, VOL 50, 2020},
Series = {Annual Review of Materials Research},
Year = {2020},
Volume = {50},
Pages = {71-103},
Abstract = {Advances in machine learning have impacted myriad areas of materials
   science, such as the discovery of novel materials and the improvement
   ofmolecular simulations, with likely many more important developments to
   come. Given the rapid changes in this field, it is challenging to
   understand both the breadth of opportunities and the best practices for
   their use. In this review, we address aspects of both problems by
   providing an overview of the areas in which machine learning has
   recently had significant impact in materials science, and then we
   provide amore detailed discussion on determining the accuracy and domain
   of applicability of some common types of machine learning models.
   Finally, we discuss some opportunities and challenges for the materials
   community to fully utilize the capabilities of machine learning.},
DOI = {10.1146/annurev-matsci-070218-010015},
ISSN = {1531-7331},
EISSN = {1545-4118},
ISBN = {978-0-8243-1750-8},
ResearcherID-Numbers = {Morgan, Dane/B-7972-2008},
Unique-ID = {WOS:000590407100004},
}

@article{ WOS:000596015500004,
Author = {Seo, Hyunseok and Khuzani, Masoud Badiei and Vasudevan, Varun and Huang,
   Charles and Ren, Hongyi and Xiao, Ruoxiu and Jia, Xiao and Xing, Lei},
Title = {Machine learning techniques for biomedical image segmentation: An
   overview of technical aspects and introduction to state-of-art
   applications},
Journal = {MEDICAL PHYSICS},
Year = {2020},
Volume = {47},
Number = {5},
Pages = {E148-E167},
Month = {JUN},
Abstract = {In recent years, significant progress has been made in developing more
   accurate and efficient machine learning algorithms for segmentation of
   medical and natural images. In this review article, we highlight the
   imperative role of machine learning algorithms in enabling efficient and
   accurate segmentation in the field of medical imaging. We specifically
   focus on several key studies pertaining to the application of machine
   learning methods to biomedical image segmentation. We review classical
   machine learning algorithms such as Markov random fields, k-means
   clustering, random forest, etc. Although such classical learning models
   are often less accurate compared to the deep-learning techniques, they
   are often more sample efficient and have a less complex structure. We
   also review different deep-learning architectures, such as the
   artificial neural networks (ANNs), the convolutional neural networks
   (CNNs), and the recurrent neural networks (RNNs), and present the
   segmentation results attained by those learning models that were
   published in the past 3 yr. We highlight the successes and limitations
   of each machine learning paradigm. In addition, we discuss several
   challenges related to the training of different machine learning models,
   and we present some heuristics to address those challenges.},
DOI = {10.1002/mp.13649},
ISSN = {0094-2405},
EISSN = {2473-4209},
ResearcherID-Numbers = {Vasudevan, Varun/IRZ-5213-2023},
ORCID-Numbers = {Xing, Lei/0000-0003-2536-5359
   },
Unique-ID = {WOS:000596015500004},
}

@article{ WOS:000546550100020,
Author = {Dastile, Xolani and Celik, Turgay and Potsane, Moshe},
Title = {Statistical and machine learning models in credit scoring: A systematic
   literature survey},
Journal = {APPLIED SOFT COMPUTING},
Year = {2020},
Volume = {91},
Month = {JUN},
Abstract = {In practice, as a well-known statistical method, the logistic regression
   model is used to evaluate the credit-worthiness of borrowers due to its
   simplicity and transparency in predictions. However, in literature,
   sophisticated machine learning models can be found that can replace the
   logistic regression model. Despite the advances and applications of
   machine learning models in credit scoring, there are still two major
   issues: the incapability of some of the machine learning models to
   explain predictions; and the issue of imbalanced datasets. As such,
   there is a need for a thorough survey of recent literature in credit
   scoring. This article employs a systematic literature survey approach to
   systematically review statistical and machine learning models in credit
   scoring, to identify limitations in literature, to propose a guiding
   machine learning framework, and to point to emerging directions. This
   literature survey is based on 74 primary studies, such as journal and
   conference articles, that were published between 2010 and 2018.
   According to the meta-analysis of this literature survey, we found that
   in general, an ensemble of classifiers performs better than single
   classifiers. Although deep learning models have not been applied
   extensively in credit scoring literature, they show promising results.
   (C) 2020 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.asoc.2020.106263},
Article-Number = {106263},
ISSN = {1568-4946},
EISSN = {1872-9681},
ResearcherID-Numbers = {Celik, Turgay/Q-9713-2018},
ORCID-Numbers = {Celik, Turgay/0000-0001-6925-6010
   },
Unique-ID = {WOS:000546550100020},
}

@article{ WOS:000643857400011,
Author = {Konstantinov, V, Andrei and Utkin, V, Lev},
Title = {Interpretable machine learning with an ensemble of gradient boosting
   machines},
Journal = {KNOWLEDGE-BASED SYSTEMS},
Year = {2021},
Volume = {222},
Month = {JUN 21},
Abstract = {A method for the local and global interpretation of a black-box model on
   the basis of the well-known generalized additive models is proposed. It
   can be viewed as an extension or a modification of the algorithm using
   the neural additive model. The method is based on using an ensemble of
   gradient boosting machines (GBMs) such that each GBM is learned on a
   single feature and produces a shape function of the feature. The
   ensemble is composed as a weighted sum of separate GBMs resulting a
   weighted sum of shape functions which form the generalized additive
   model. GBMs are built in parallel using randomized decision trees of
   depth 1, which provide a very simple architecture. Weights of GBMs as
   well as features are computed in each iteration of boosting by using the
   Lasso method and then updated by means of a specific smoothing
   procedure. In contrast to the neural additive model, the method provides
   weights of features in the explicit form, and it is simply trained. A
   lot of numerical experiments with an algorithm implementing the proposed
   method on synthetic and real datasets demonstrate its efficiency and
   properties for local and global interpretation. (C) 2021 Elsevier B.V.
   All rights reserved.},
DOI = {10.1016/j.knosys.2021.106993},
EarlyAccessDate = {MAR 2021},
Article-Number = {106993},
ISSN = {0950-7051},
EISSN = {1872-7409},
ResearcherID-Numbers = {Konstantinov, Andrei/U-5940-2019},
ORCID-Numbers = {Konstantinov, Andrei/0000-0002-1542-6480
   },
Unique-ID = {WOS:000643857400011},
}

@article{ WOS:000503751600001,
Author = {Uddin, Shahadat and Khan, Arif and Hossain, Md Ekramul and Moni,
   Mohammad Ali},
Title = {Comparing different supervised machine learning algorithms for disease
   prediction},
Journal = {BMC MEDICAL INFORMATICS AND DECISION MAKING},
Year = {2019},
Volume = {19},
Number = {1},
Month = {DEC 21},
Abstract = {Background Supervised machine learning algorithms have been a dominant
   method in the data mining field. Disease prediction using health data
   has recently shown a potential application area for these methods. This
   study ai7ms to identify the key trends among different types of
   supervised machine learning algorithms, and their performance and usage
   for disease risk prediction. Methods In this study, extensive research
   efforts were made to identify those studies that applied more than one
   supervised machine learning algorithm on single disease prediction. Two
   databases (i.e., Scopus and PubMed) were searched for different types of
   search items. Thus, we selected 48 articles in total for the comparison
   among variants supervised machine learning algorithms for disease
   prediction. Results We found that the Support Vector Machine (SVM)
   algorithm is applied most frequently (in 29 studies) followed by the
   Naive Bayes algorithm (in 23 studies). However, the Random Forest (RF)
   algorithm showed superior accuracy comparatively. Of the 17 studies
   where it was applied, RF showed the highest accuracy in 9 of them, i.e.,
   53\%. This was followed by SVM which topped in 41\% of the studies it
   was considered. Conclusion This study provides a wide overview of the
   relative performance of different variants of supervised machine
   learning algorithms for disease prediction. This important information
   of relative performance can be used to aid researchers in the selection
   of an appropriate supervised machine learning algorithm for their
   studies.},
DOI = {10.1186/s12911-019-1004-8},
Article-Number = {281},
EISSN = {1472-6947},
ResearcherID-Numbers = {Moni, Mohammad/ABG-3606-2020
   Uddin, Shahadat/E-1963-2011},
ORCID-Numbers = {Moni, Mohammad Ali/0000-0003-0756-1006
   Khan, Arif/0000-0002-5793-933X
   Uddin, Shahadat/0000-0003-0091-6919},
Unique-ID = {WOS:000503751600001},
}

@article{ WOS:000466380000034,
Author = {Ngiam, Kee Yuan and Khor, Ing Wei},
Title = {Big data and machine learning algorithms for health-care delivery},
Journal = {LANCET ONCOLOGY},
Year = {2019},
Volume = {20},
Number = {5},
Pages = {E262-E273},
Month = {MAY},
Abstract = {Analysis of big data by machine learning offers considerable advantages
   for assimilation and evaluation of large amounts of complex health-care
   data. However, to effectively use machine learning tools in health care,
   several limitations must be addressed and key issues considered, such as
   its clinical implementation and ethics in health-care delivery.
   Advantages of machine learning include flexibility and scalability
   compared with traditional biostatistical methods, which makes it
   deployable for many tasks, such as risk stratification, diagnosis and
   classification, and survival predictions. Another advantage of machine
   learning algorithms is the ability to analyse diverse data types (eg,
   demographic data, laboratory findings, imaging data, and doctors'
   free-text notes) and incorporate them into predictions for disease risk,
   diagnosis, prognosis, and appropriate treatments. Despite these
   advantages, the application of machine learning in health-care delivery
   also presents unique challenges that require data preprocessing, model
   training, and refinement of the system with respect to the actual
   clinical problem. Also crucial are ethical considerations, which include
   medico-legal implications, doctors' understanding of machine learning
   tools, and data privacy and security. In this Review, we discuss some of
   the benefits and challenges of big data and machine learning in health
   care.},
DOI = {10.1016/S1470-2045(19)30149-4},
ISSN = {1470-2045},
EISSN = {1474-5488},
ResearcherID-Numbers = {Ngiam, Kee/S-5501-2017},
Unique-ID = {WOS:000466380000034},
}

@article{ WOS:000517852500004,
Author = {Lee, In and Shin, Yong Jae},
Title = {Machine learning for enterprises: Applications, algorithm selection, and
   challenges},
Journal = {BUSINESS HORIZONS},
Year = {2020},
Volume = {63},
Number = {2, SI},
Pages = {157-170},
Month = {MAR-APR},
Abstract = {Machine learning holds great promise for lowering product and service
   costs, speeding up business processes, and serving customers better. It
   is recognized as one of the most important application areas in this era
   of unprecedented technological development, and its adoption is gaining
   momentum across almost all industries. In view of this, we offer a brief
   discussion of categories of machine learning and then present three
   types of machine-learning usage at enterprises. We then discuss the
   trade-off between the accuracy and interpretability of machine-learning
   algorithms, a crucial consideration in selecting the right algorithm for
   the task at hand. We next outline three cases of machine-learning
   development in financial services. Finally, we discuss challenges all
   managers must confront in deploying machine-learning applications. (C)
   2019 Kelley School of Business, Indiana University. Published by
   Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.bushor.2019.10.005},
ISSN = {0007-6813},
EISSN = {1873-6068},
ResearcherID-Numbers = {SHIN, YONG/AAJ-2870-2020},
Unique-ID = {WOS:000517852500004},
}

@article{ WOS:000540393400001,
Author = {Aykol, Muratahan and Herring, Patrick and Anapolsky, Abraham},
Title = {Machine learning for continuous innovation in battery technologies},
Journal = {NATURE REVIEWS MATERIALS},
Year = {2020},
Volume = {5},
Number = {10},
Pages = {725-727},
Month = {OCT},
Abstract = {Batteries, as complex materials systems, pose unique challenges for the
   application of machine learning. Although a shift to data-driven,
   machine learning-based battery research has started, new initiatives in
   academia and industry are needed to fully exploit its potential.},
DOI = {10.1038/s41578-020-0216-y},
EarlyAccessDate = {JUN 2020},
ISSN = {2058-8437},
ORCID-Numbers = {Herring, Patrick/0000-0001-9922-1169
   Aykol, Muratahan/0000-0001-6433-7217},
Unique-ID = {WOS:000540393400001},
}

@article{ WOS:000506166100099,
Author = {Spinner, Thilo and Schlegel, Udo and Schaefer, Hanna and El-Assady,
   Mennatallah},
Title = {explAIner: A Visual Analytics Framework for Interactive and Explainable
   Machine Learning},
Journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS},
Year = {2020},
Volume = {26},
Number = {1},
Pages = {1064-1074},
Month = {JAN},
Abstract = {We propose a framework for interactive and explainable machine learning
   that enables users to (1) understand machine learning models; (2)
   diagnose model limitations using different explainable AI methods; as
   well as (3) refine and optimize the models. Our framework combines an
   iterative XAI pipeline with eight global monitoring and steering
   mechanisms, including quality monitoring, provenance tracking, model
   comparison, and trust building. To operationalize the framework, we
   present explAIner, a visual analytics system for interactive and
   explainable machine learning that instantiates all phases of the
   suggested pipeline within the commonly used TensorBoard environment. We
   performed a user-study with nine participants across different expertise
   levels to examine their perception of our workflow and to collect
   suggestions to fill the gap between our system and framework. The
   evaluation confirms that our tightly integrated system leads to an
   informed machine learning process while disclosing opportunities for
   further extensions.},
DOI = {10.1109/TVCG.2019.2934629},
ISSN = {1077-2626},
EISSN = {1941-0506},
ResearcherID-Numbers = {Schlegel, Udo/AAI-9385-2021
   Hauptmann, Hanna/R-3492-2016},
ORCID-Numbers = {El-Assady, Mennatallah/0000-0001-8526-2613
   Spinner, Thilo/0000-0002-1168-1804
   Hauptmann, Hanna/0000-0002-6840-5341},
Unique-ID = {WOS:000506166100099},
}

@article{ WOS:000554567500001,
Author = {Liu, Kaijun and Xu, Shengwei and Xu, Guoai and Zhang, Miao and Sun,
   Dawei and Liu, Haifeng},
Title = {A Review of Android Malware Detection Approaches Based on Machine
   Learning},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {124579-124607},
Abstract = {Android applications are developing rapidly across the mobile ecosystem,
   but Android malware is also emerging in an endless stream. Many
   researchers have studied the problem of Android malware detection and
   have put forward theories and methods from different perspectives.
   Existing research suggests that machine learning is an effective and
   promising way to detect Android malware. Notwithstanding, there exist
   reviews that have surveyed different issues related to Android malware
   detection based on machine learning. We believe our work complements the
   previous reviews by surveying a wider range of aspects of the topic.
   This paper presents a comprehensive survey of Android malware detection
   approaches based on machine learning. We briefly introduce some
   background on Android applications, including the Android system
   architecture, security mechanisms, and classification of Android
   malware. Then, taking machine learning as the focus, we analyze and
   summarize the research status from key perspectives such as sample
   acquisition, data preprocessing, feature selection, machine learning
   models, algorithms, and the evaluation of detection effectiveness.
   Finally, we assess the future prospects for research into Android
   malware detection based on machine learning. This review will help
   academics gain a full picture of Android malware detection based on
   machine learning. It could then serve as a basis for subsequent
   researchers to start new work and help to guide research in the field
   more generally.},
DOI = {10.1109/ACCESS.2020.3006143},
ISSN = {2169-3536},
ResearcherID-Numbers = {Xu, Shengwei/LLL-2053-2024},
Unique-ID = {WOS:000554567500001},
}

@article{ WOS:001483708400001,
Author = {Liang, Yiji and Dai, Canwen and Wang, Jingwei and Zhang, Guoqing and To,
   Suet and Zhao, Zejia},
Title = {Typical applications and perspectives of machine learning for advanced
   precision machining: A comprehensive review},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2025},
Volume = {283},
Month = {JUL 15},
Abstract = {Advanced precision machining technologies, such as micro/ultraprecision
   mechanical machining and atomic and close-to-atomic scale manufacturing,
   are critical to high-value industries like aerospace and defense.
   However, extreme precision requirements and nonlinear dynamics pose
   significant challenges for accurate modeling, as traditional methods
   often struggle to capture intricate interactions and inherent
   variability. Machine learning emerges as a transformative solution,
   enabling data-driven modeling with unprecedented accuracy. This paper
   provides a comprehensive overview of the significant advancements and
   typical applications of machine learning in advanced precision
   machining, focusing on model architectures and methodologies to guide
   industrial implementation. For instance, this paper presents various
   examples, such as the application of LSTM networks in predicting tool
   life by capturing temporal dependencies in force signals, which
   illustrates how machine learning models are tailored to address specific
   challenges in precision machining. However, industrial adoption of
   machine learning remains hindered by limited datasets and computational
   constraints. This paper offers forward-looking recommendations to
   address these issues, integrating machine learning into precision
   machining within the framework of Industry 5.0 and providing robust
   support for the further promotion and application of machine learning in
   actual production environments. Furthermore, this research establishes a
   robust framework for recognizing similarities in machine learning
   applications across diverse machining domains, facilitating transfer
   learning among various advanced precision machining processes. By
   bridging the gap between theoretical models and industrial scalability,
   this review highlights the transformative role of machine learning in
   advanced precision machining toward intelligent, sustainable production,
   ultimately supporting highperformance component manufacturing.},
DOI = {10.1016/j.eswa.2025.127770},
EarlyAccessDate = {APR 2025},
Article-Number = {127770},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {To, Sandy/M-2815-2015
   zhang, guoqing/GXG-4800-2022
   },
ORCID-Numbers = {Zhao, Zejia/0000-0001-8640-541X},
Unique-ID = {WOS:001483708400001},
}

@article{ WOS:000669776700006,
Author = {Chin, Keene and Hellebrekers, Tess and Majidi, Carmel},
Title = {Machine Learning for Soft Robotic Sensing and Control},
Journal = {ADVANCED INTELLIGENT SYSTEMS},
Year = {2020},
Volume = {2},
Number = {6, SI},
Month = {JUN},
Abstract = {Herein, the progress of machine learning methods in the field of soft
   robotics, specifically in the applications of sensing and control, is
   outlined. Data-driven methods such as machine learning are especially
   suited to systems with governing functions that are unknown, impractical
   or impossible to represent analytically, or computationally intractable
   to integrate into real-world solutions. Function approximation with
   careful formulation of the machine learning architecture enables the
   encoding of dynamic behavior and nonlinearities, with the added
   potential to address hysteresis and nonstationary behavior. Supervised
   learning and reinforcement learning in simulation and on a wide variety
   of physical robotic systems have shown promising results for the use of
   empirical data-driven methods as a solution to contemporary soft
   robotics problems.},
DOI = {10.1002/aisy.201900171},
Article-Number = {1900171},
EISSN = {2640-4567},
ORCID-Numbers = {Majidi, Carmel/0000-0002-6469-9645},
Unique-ID = {WOS:000669776700006},
}

@article{ WOS:000670264800013,
Author = {Du, Peijun and Bai, Xuyu and Tan, Kun and Xue, Zhaohui and Samat, Alim
   and Xia, Junshi and Li, Erzhu and Su, Hongjun and Liu, Wei},
Title = {Advances of Four Machine Learning Methods for Spatial Data Handling: a
   Review},
Journal = {JOURNAL OF GEOVISUALIZATION AND SPATIAL ANALYSIS},
Year = {2020},
Volume = {4},
Number = {1},
Month = {JUN},
Abstract = {Most machine learning tasks can be categorized into classification or
   regression problems. Regression and classification models are normally
   used to extract useful geographic information from observed or measured
   spatial data, such as land cover classification, spatial interpolation,
   and quantitative parameter retrieval. This paper reviews the progress of
   four advanced machine learning methods for spatial data handling,
   namely, support vector machine (SVM)-based kernel learning,
   semi-supervised and active learning, ensemble learning, and deep
   learning. These four machine learning modes are representative because
   they improve learning performances from different views, for example,
   feature space transform and decision function (SVM), optimized uses of
   samples (semi-supervised and active learning), and enhanced learning
   models and capabilities (ensemble learning and deep learning). For
   spatial data handling via machine learning that can be improved by the
   four machine learning models, three key elements are learning
   algorithms, training samples, and input features. To apply machine
   learning methods to spatial data handling successfully, a four-level
   strategy is suggested: experimenting and evaluating the applicability,
   extending the algorithms by embedding spatial properties, optimizing the
   parameters for better performance, and enhancing the algorithm by
   multiple means. Firstly, the advances of SVM are reviewed to demonstrate
   the merits of novel machine learning methods for spatial data, running
   the line from direct use and comparison with traditional classifiers,
   and then targeted improvements to address multiple class problems, to
   optimize parameters of SVM, and to use spatial and spectral features. To
   overcome the limits of small-size training samples, semi-supervised
   learning and active learning methods are then utilized to deal with
   insufficient labeled samples, showing the potential of learning from
   small-size training samples. Furthermore, considering the poor
   generalization capacity and instability of machine learning algorithms,
   ensemble learning is introduced to integrate the advantages of multiple
   learners and to enhance the generalization capacity. The typical
   research lines, including the combination of multiple classifiers,
   advanced ensemble classifiers, and spatial interpolation, are presented.
   Finally, deep learning, one of the most popular branches of machine
   learning, is reviewed with specific examples for scene classification
   and urban structural type recognition from high-resolution remote
   sensing images. By this review, it can be concluded that machine
   learning methods are very effective for spatial data handling and have
   wide application potential in the big data era.},
DOI = {10.1007/s41651-020-00048-5},
Article-Number = {13},
ISSN = {2509-8810},
EISSN = {2509-8829},
ResearcherID-Numbers = {Su, Hongjun/E-4859-2013
   Xia, Junshi/ABD-4116-2020
   Samat, Alim/H-5309-2019
   Xue, Zhaohui/K-3413-2014
   Tan, Kun/AAA-6254-2019},
ORCID-Numbers = {Xia, Junshi/0000-0002-5586-6536
   Tan, Kun/0000-0001-6353-0146
   Xue, Zhaohui/0000-0001-6253-2967
   Xue, Zhaohui/0000-0002-1672-317X
   Samat, Alim/0000-0002-9091-6033
   },
Unique-ID = {WOS:000670264800013},
}

@article{ WOS:000512357000002,
Author = {Yu, Chunling and Jiang, Jingchao},
Title = {A Perspective on Using Machine Learning in 3D Bioprinting},
Journal = {INTERNATIONAL JOURNAL OF BIOPRINTING},
Year = {2020},
Volume = {6},
Number = {1},
Abstract = {Recently, three-dimensional (3D) printing technologies have been widely
   applied in industry and our daily lives. The term 3D bioprinting has
   been coined to describe 3D printing at the biomedical level. Machine
   learning is currently becoming increasingly active and has been used to
   improve 3D printing processes, such as process optimization, dimensional
   accuracy analysis, manufacturing defect detection, and material property
   prediction. However, few studies have been found to use machine learning
   in 3D bioprinting processes. In this paper, related machine learning
   methods used in 3D printing are briefly reviewed and a perspective on
   how machine learning can also benefit 3D bioprinting is discussed. We
   believe that machine learning can significantly affect the future
   development of 3D bioprinting and hope this paper can inspire some ideas
   on how machine learning can be used to improve 3D bioprinting.},
DOI = {10.18063/ijb.v6i1.253},
Article-Number = {253},
ISSN = {2424-7723},
EISSN = {2424-8002},
ResearcherID-Numbers = {Jiang, Jingchao/R-1303-2019},
ORCID-Numbers = {Jiang, Jingchao/0000-0002-0446-3454},
Unique-ID = {WOS:000512357000002},
}

@article{ WOS:000591283600001,
Author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco},
Title = {Machine learning for landslides prevention: a survey},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2021},
Volume = {33},
Number = {17, SI},
Pages = {10881-10907},
Month = {SEP},
Abstract = {Landslides are one of the most critical categories of natural disasters
   worldwide and induce severely destructive outcomes to human life and the
   overall economic system. To reduce its negative effects, landslides
   prevention has become an urgent task, which includes investigating
   landslide-related information and predicting potential landslides.
   Machine learning is a state-of-the-art analytics tool that has been
   widely used in landslides prevention. This paper presents a
   comprehensive survey of relevant research on machine learning applied in
   landslides prevention, mainly focusing on (1) landslides detection based
   on images, (2) landslides susceptibility assessment, and (3) the
   development of landslide warning systems. Moreover, this paper discusses
   the current challenges and potential opportunities in the application of
   machine learning algorithms for landslides prevention.},
DOI = {10.1007/s00521-020-05529-8},
EarlyAccessDate = {NOV 2020},
ISSN = {0941-0643},
EISSN = {1433-3058},
ResearcherID-Numbers = {Mei, Gang/C-9124-2016
   Piccialli, Francesco/ABC-2457-2020},
ORCID-Numbers = {Mei, Gang/0000-0003-0026-5423
   ma, mazhengjing/0000-0003-0044-945X
   },
Unique-ID = {WOS:000591283600001},
}

@article{ WOS:000573738100002,
Author = {Zhang, Jie and Petersen, Soren D. and Radivojevic, Tijana and Ramirez,
   Andres and Perez-Manriquez, Andres and Abeliuk, Eduardo and Sanchez,
   Benjamin J. and Costello, Zak and Chen, Yu and Fero, Michael J. and
   Martin, Hector Garcia and Nielsen, Jens and Keasling, Jay D. and Jensen,
   Michael K.},
Title = {Combining mechanistic and machine learning models for predictive
   engineering and optimization of tryptophan metabolism},
Journal = {NATURE COMMUNICATIONS},
Year = {2020},
Volume = {11},
Number = {1},
Month = {SEP 25},
Abstract = {Through advanced mechanistic modeling and the generation of large
   high-quality datasets, machine learning is becoming an integral part of
   understanding and engineering living systems. Here we show that
   mechanistic and machine learning models can be combined to enable
   accurate genotype-to-phenotype predictions. We use a genome-scale model
   to pinpoint engineering targets, efficient library construction of
   metabolic pathway designs, and high-throughput biosensor-enabled
   screening for training diverse machine learning algorithms. From a
   single data-generation cycle, this enables successful forward
   engineering of complex aromatic amino acid metabolism in yeast, with the
   best machine learning-guided design recommendations improving tryptophan
   titer and productivity by up to 74 and 43\%, respectively, compared to
   the best designs used for algorithm training. Thus, this study
   highlights the power of combining mechanistic and machine learning
   models to effectively direct metabolic engineering efforts. In metabolic
   engineering, mechanistic models require prior metabolism knowledge of
   the chassis strain, whereas machine learning models need ample training
   data. Here, the authors combine the mechanistic and machine learning
   models to improve prediction performance of tryptophan metabolism in
   baker's yeast.},
DOI = {10.1038/s41467-020-17910-1},
Article-Number = {4880},
EISSN = {2041-1723},
ResearcherID-Numbers = {Sánchez, Benjamín/O-6859-2019
   Martin, Hector/B-5357-2009
   Keasling, Jay/JBJ-7336-2023
   Chen, Yu/AAL-3329-2020
   Nielsen, Jens/ABH-7527-2020},
ORCID-Numbers = {Perez-Manriquez, Andres/0000-0003-3803-951X
   Ramirez Neilson, Juan Andres/0000-0001-6273-5211
   },
Unique-ID = {WOS:000573738100002},
}

@article{ WOS:000609037300001,
Author = {Bhangale, Kishor Barasu and Mohanaprasad, K.},
Title = {A review on speech processing using machine learning paradigm},
Journal = {INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY},
Year = {2021},
Volume = {24},
Number = {2, SI},
Pages = {367-388},
Month = {JUN},
Abstract = {Speech processing plays a crucial role in many signal processing
   applications, while the last decade has bought gigantic evolution based
   on machine learning prototype. Speech processing has a close
   relationship with computer linguistics, human-machine interaction,
   natural language processing, and psycholinguistics. This review article
   majorly discusses the feature extraction techniques and machine learning
   classifiers employed in speech processing and recognition activities.
   The performance of several machine learning techniques is validated for
   speech emotion recognition application on Berlin EmoDB database.
   Further, it gives the broad application areas and challenges in machine
   learning for speech processing.},
DOI = {10.1007/s10772-021-09808-0},
EarlyAccessDate = {JAN 2021},
ISSN = {1381-2416},
EISSN = {1572-8110},
ResearcherID-Numbers = {Kothandaraman, Mohanaprasad/O-8250-2019
   Bhangale, Kishor/LDE-7427-2024
   },
ORCID-Numbers = {Bhangale, Kishor/0000-0002-0453-7492
   Kothandaraman, Mohanaprasad/0000-0003-3938-7495},
Unique-ID = {WOS:000609037300001},
}

@article{ WOS:000668245900001,
Author = {Monteiro, Jose Pedro and Ramos, Diogo and Carneiro, Davide and Duarte,
   Francisco and Fernandes, Joao M. and Novais, Paulo},
Title = {Meta-learning and the new challenges of machine learning},
Journal = {INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS},
Year = {2021},
Volume = {36},
Number = {11},
Pages = {6240-6272},
Month = {NOV},
Abstract = {In the last years, organizations and companies in general have found the
   true potential value of collecting and using data for supporting
   decision-making. As a consequence, data are being collected at an
   unprecedented rate. This poses several challenges, including, for
   example, regarding the storage and processing of these data. Machine
   Learning (ML) is also not an exception, in the sense that algorithms
   must now deal with novel challenges, such as learn from streaming data
   or deal with concept drift. ML engineers also have a harder task when it
   comes to selecting the most appropriate model, given the wealth of
   algorithms and possible configurations that exist nowadays. At the same
   time, training time is a stronger restriction as the computational
   complexity of the training model increases. In this paper we propose a
   framework for dealing with these challenges, based on meta-learning.
   Specifically, we tackle two well-defined problems: automatic algorithm
   selection and continuous algorithm updates that do not require the
   retraining of the whole algorithm to adapt to new data. Results show
   that the proposed framework can contribute to ameliorate the identified
   issues.},
DOI = {10.1002/int.22549},
EarlyAccessDate = {JUN 2021},
ISSN = {0884-8173},
EISSN = {1098-111X},
ResearcherID-Numbers = {Novais, Paulo/M-4053-2013
   Duarte, Francisco J./B-6197-2009
   Fernandes, João/B-3942-2013
   Carneiro, Davide/A-3490-2014
   },
ORCID-Numbers = {Novais, Paulo/0000-0002-3549-0754
   Monteiro, Jose Pedro/0000-0003-3977-4786
   Duarte, Francisco J./0000-0001-9270-2226
   Carneiro, Davide/0000-0002-6650-0388
   Fernandes, Joao M./0000-0003-1174-1966},
Unique-ID = {WOS:000668245900001},
}

@article{ WOS:000594402900006,
Author = {Deng, Changyu and Ji, Xunbi and Rainey, Colton and Zhang, Jianyu and Lu,
   Wei},
Title = {Integrating Machine Learning with Human Knowledge},
Journal = {ISCIENCE},
Year = {2020},
Volume = {23},
Number = {11},
Month = {NOV 20},
Abstract = {Machine learning has been heavily researched and widely used in many
   disciplines. However, achieving high accuracy requires a large amount of
   data that is sometimes difficult, expensive, or impractical to obtain.
   Integrating human knowledge into machine learning can significantly
   reduce data requirement, increase reliability and robustness of machine
   learning, and build explainable machine learning systems. This allows
   leveraging the vast amount of human knowledge and capability of machine
   learning to achieve functions and performance not available before and
   will facilitate the interaction between human beings and machine
   learning systems, making machine learning decisions understandable to
   humans. This paper gives an overview of the knowledge and its
   representations that can be integrated into machine learning and the
   methodology. We cover the fundamentals, current status, and recent
   progress of the methods, with a focus on popular and new topics. The
   perspectives on future directions are also discussed.},
DOI = {10.1016/j.isci.2020.101656},
Article-Number = {101656},
EISSN = {2589-0042},
ResearcherID-Numbers = {Zhang, Jianyu/GLS-9446-2022
   Deng, Changyu/AAI-9746-2021
   Ji, Xunbi/HTR-4958-2023
   Lu, Wei/C-2776-2008
   },
ORCID-Numbers = {Rainey, Colton/0000-0003-0014-0823
   Zhang, Jianyu/0000-0002-0318-8112
   Deng, Changyu/0000-0001-8339-4014},
Unique-ID = {WOS:000594402900006},
}

@incollection{ WOS:000524457700003,
Author = {Wiemken, Timothy L. and Kelley, Robert R.},
Editor = {Fielding, JE},
Title = {Machine Learning in Epidemiology and Health Outcomes Research},
Booktitle = {ANNUAL REVIEW OF PUBLIC HEALTH, VOL 41},
Series = {Annual Review of Public Health},
Year = {2020},
Volume = {41},
Pages = {21-36},
Abstract = {Machine learning approaches to modeling of epidemiologic data are
   becoming increasingly more prevalent in the literature. These methods
   have the potential to improve our understanding of health and
   opportunities for intervention, far beyond our past capabilities. This
   article provides a walkthrough for creating supervised machine learning
   models with current examples from the literature. From identifying an
   appropriate sample and selecting features through training, testing, and
   assessing performance, the end-to-end approach to machine learning can
   be a daunting task. We take the reader through each step in the process
   and discuss novel concepts in the area of machine learning, including
   identifying treatment effects and explaining the output from machine
   learning models.},
DOI = {10.1146/annurev-publhealth-040119-094437},
ISSN = {0163-7525},
ISBN = {978-0-8243-2741-5},
ORCID-Numbers = {Kelley, Rob Robert/0009-0002-8472-100X},
Unique-ID = {WOS:000524457700003},
}

@article{ WOS:000649908600001,
Author = {Caiafa, Cesar F. and Sun, Zhe and Tanaka, Toshihisa and Marti-Puig, Pere
   and Sole-Casals, Jordi},
Title = {Machine Learning Methods with Noisy, Incomplete or Small Datasets},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2021},
Volume = {11},
Number = {9},
Month = {MAY},
Abstract = {In this article, we present a collection of fifteen novel contributions
   on machine learning methods with low-quality or imperfect datasets,
   which were accepted for publication in the special issue ``Machine
   Learning Methods with Noisy, Incomplete or Small Datasets{''}, Applied
   Sciences (ISSN 2076-3417). These papers provide a variety of novel
   approaches to real-world machine learning problems where available
   datasets suffer from imperfections such as missing values, noise or
   artefacts. Contributions in applied sciences include medical
   applications, epidemic management tools, methodological work, and
   industrial applications, among others. We believe that this special
   issue will bring new ideas for solving this challenging problem, and
   will provide clear examples of application in real-world scenarios.},
DOI = {10.3390/app11094132},
Article-Number = {4132},
EISSN = {2076-3417},
ResearcherID-Numbers = {Caiafa, Cesar/A-6045-2010
   Solé-Casals, Jordi/B-7754-2008
   Marti-Puig, Pere/I-2797-2015
   Sole-Casals, Jordi/B-7754-2008
   zhe, sun/AFS-5711-2022},
ORCID-Numbers = {Marti-Puig, Pere/0000-0001-6582-4551
   Sole-Casals, Jordi/0000-0002-6534-1979
   Caiafa, Cesar F./0000-0001-5437-6095
   zhe, sun/0000-0002-6531-0769
   },
Unique-ID = {WOS:000649908600001},
}

@incollection{ WOS:000677831600018,
Author = {Chen, Irene Y. and Joshi, Shalmali and Ghassemi, Marzyeh and Ranganath,
   Rajesh},
Editor = {Altman, RB},
Title = {Probabilistic Machine Learning for Healthcare},
Booktitle = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE, VOL 4},
Series = {Annual Review of Biomedical Data Science},
Year = {2021},
Volume = {4},
Pages = {393-415},
Abstract = {Machine learning can be used to make sense of healthcare data.
   Probabilistic machine learning models help provide a complete picture of
   observed data in healthcare. In this review, we examine how
   probabilistic machine learning can advance healthcare. We consider
   challenges in the predictive model building pipeline where probabilistic
   models can be beneficial, including calibration and missing data. Beyond
   predictive models, we also investigate the utility of probabilistic
   machine learning models in phenotyping, in generative models for
   clinical use cases, and in reinforcement learning.},
DOI = {10.1146/annurev-biodatasci-092820-033938},
ISSN = {2574-3414},
Unique-ID = {WOS:000677831600018},
}

@article{ WOS:000709064000001,
Author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed,
   Alexander},
Title = {A Literature Review of Using Machine Learning in Software Development
   Life Cycle Stages},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {140896-140920},
Abstract = {The software engineering community is rapidly adopting machine learning
   for transitioning modern-day software towards highly intelligent and
   self-learning systems. However, the software engineering community is
   still discovering new ways how machine learning can offer help for
   various software development life cycle stages. In this article, we
   present a study on the use of machine learning across various software
   development life cycle stages. The overall aim of this article is to
   investigate the relationship between software development life cycle
   stages, and machine learning tools, techniques, and types. We attempt a
   holistic investigation in part to answer the question of whether machine
   learning favors certain stages and/or certain techniques.},
DOI = {10.1109/ACCESS.2021.3119746},
ISSN = {2169-3536},
ResearcherID-Numbers = {Mayr-Dorn, Christoph/ABH-3543-2020
   Egyed, Alexander/E-2632-2017
   Mashkoor, Atif/AAK-6747-2020},
ORCID-Numbers = {Shafiq, Saad/0000-0002-5901-1420
   },
Unique-ID = {WOS:000709064000001},
}

@article{ WOS:000704764100004,
Author = {Chen, Qiuan and Wei, Haipeng and Rashid, Muhammad and Cai, Zhiqiang},
Title = {Kernel extreme learning machine based hierarchical machine learning for
   multi-type and concurrent fault diagnosis},
Journal = {MEASUREMENT},
Year = {2021},
Volume = {184},
Month = {NOV},
Abstract = {The detection and identification of faults in rotary machines are of
   great significance to the mechanical equipment reliability especially
   the gearbox. Traditional machine learning algorithms suffer from low
   diagnosis accuracy of faults that have multiple types and exist
   concurrently. A novel machine learning method called hierarchical
   machine learning (HML) was proposed in this study to improve the faults
   diagnosis accuracy. The proposed algorithm consists of two layers. The
   first layer comprises a traditional machine learning model to identify
   the faults with distinguishable features and filter out these faults
   with indistinguishable features. The second layer model recognizes the
   faults filtered out by the first layer. In order to verify the
   effectiveness of the proposed method, the gearbox simulation experiment
   is carried out in the study. The simulation results validate that the
   proposed method outperforms other algorithms under an identical measure.},
DOI = {10.1016/j.measurement.2021.109923},
EarlyAccessDate = {JUL 2021},
Article-Number = {109923},
ISSN = {0263-2241},
EISSN = {1873-412X},
ResearcherID-Numbers = {Cai, Zhiqiang/GLS-3354-2022
   Muhammad, Raashid/ODI-9785-2025},
ORCID-Numbers = {Cai, Zhiqiang/0000-0002-7380-8110
   },
Unique-ID = {WOS:000704764100004},
}

@article{ WOS:000742888800012,
Author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick},
Title = {Data Acquisition for Improving Machine Learning Models},
Journal = {PROCEEDINGS OF THE VLDB ENDOWMENT},
Year = {2021},
Volume = {14},
Number = {10},
Pages = {1832-1844},
Month = {JUN},
Abstract = {The vast advances in Machine Learning (ML) over the last ten years have
   been powered by the availability of suitably prepared data for training
   purposes. The future of ML-enabled enterprise hinges on data. As such,
   there is already a vibrant market offering data annotation services to
   tailor sophisticated ML models.
   In this paper, inspired by the recent vision of online data markets and
   associated market designs, we present research on the practical problem
   of obtaining data in order to improve the accuracy of ML models. We
   consider an environment in which consumers query for data to enhance the
   accuracy of their models and data providers who possess data make them
   available for training purposes. We first formalize this interaction
   process laying out the suitable framework and associated parameters for
   data exchange. We then propose two data acquisition strategies that
   consider a trade-off between exploration during which we obtain data to
   learn about the distribution of a provider's data and exploitation
   during which we optimize our data inquiries utilizing the gained
   knowledge. In the first strategy, Estimation and Allocation (EA), we
   utilize queries to estimate the utilities of various predicates while
   learning about the distribution of the provider's data; then we proceed
   to the allocation stage in which we utilize those learned utility
   estimates to inform our data acquisition decisions. The second
   algorithmic proposal, named Sequential Predicate Selection (SPS),
   utilizes a sampling strategy to explore the distribution of the
   provider's data, adaptively investing more resources to parts of the
   data space that are statistically more promising to improve overall
   model accuracy.
   We present a detailed experimental evaluation of our proposals utilizing
   a variety of ML models and associated real data sets exploring all
   applicable parameters of interest. Our results demonstrate the relative
   benefits of the proposed algorithms. Depending on the models trained and
   the associated learning tasks we identify trade-offs and highlight the
   relative benefits of each algorithm to further optimize model accuracy.},
DOI = {10.14778/3467861.3467872},
ISSN = {2150-8097},
ResearcherID-Numbers = {Li, Yifan/AAO-3483-2020
   Yu, Hui/JED-7628-2023},
ORCID-Numbers = {Yu, Xiaohui/0000-0001-8170-2327
   },
Unique-ID = {WOS:000742888800012},
}

@article{ WOS:000732946500010,
Author = {Wang, Xin (Shane) and Ryoo, Jun Hyun (Joseph) and Bendle, Neil and
   Kopalle, Praveen K.},
Title = {The role of machine learning analytics and metrics in retailing research},
Journal = {JOURNAL OF RETAILING},
Year = {2021},
Volume = {97},
Number = {4},
Pages = {658-675},
Month = {DEC},
Abstract = {This research presents the use of machine learning analytics and metrics
   in the retailing context. We first discuss what is machine learning and
   explain the field's origins. We then demonstrate the strengths of
   machine learning methods using an online retailing dataset, noting key
   areas of divergence from the traditional explanatory approach to data
   analysis. We then provide a review of the current state of machine
   learning in top-level retailing and marketing research, integrating
   ideas for future research and showcasing potential applications for
   practitioners. We propose that the explanatory and machine learning
   approaches need not be mutually exclusive. Particularly, we discuss four
   key areas in the general scientific research process that can benefit
   from machine learning: data exploration/theory building, variable
   creation, estimation, and predicting an outcome metric. Due to the
   customer-facing nature of retailing, we anticipate several challenges
   researchers and practitioners might face in the adoption and
   implementation of machine learning, such as ethical prediction and
   customer privacy issues. Overall, our belief is that machine learning
   can enhance customer experience and, accordingly, we advance
   opportunities for future research. (c) 2020 New York University.
   Published by Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jretai.2020.12.001},
EarlyAccessDate = {DEC 2021},
ISSN = {0022-4359},
EISSN = {1873-3271},
ResearcherID-Numbers = {Bendle, Neil/AAZ-6717-2021
   },
ORCID-Numbers = {Ryoo, Jun Hyun (Joseph)/0000-0002-9434-1724
   Bendle, Neil/0000-0002-8557-5983},
Unique-ID = {WOS:000732946500010},
}

@article{ WOS:000554897200004,
Author = {Wei, Jing and Chu, Xuan and Sun, Xiang-Yu and Xu, Kun and Deng,
   Hui-Xiong and Chen, Jigen and Wei, Zhongming and Lei, Ming},
Title = {Machine learning in materials science},
Journal = {INFOMAT},
Year = {2019},
Volume = {1},
Number = {3},
Pages = {338-358},
Month = {SEP},
Abstract = {Traditional methods of discovering new materials, such as the empirical
   trial and error method and the density functional theory (DFT)-based
   method, are unable to keep pace with the development of materials
   science today due to their long development cycles, low efficiency, and
   high costs. Accordingly, due to its low computational cost and short
   development cycle, machine learning is coupled with powerful data
   processing and high prediction performance and is being widely used in
   material detection, material analysis, and material design. In this
   article, we discuss the basic operational procedures in analyzing
   material properties via machine learning, summarize recent applications
   of machine learning algorithms to several mature fields in materials
   science, and discuss the improvements that are required for wide-ranging
   application.},
DOI = {10.1002/inf2.12028},
EISSN = {2567-3165},
ResearcherID-Numbers = {Wei, Jing/AGO-9658-2022
   jigen, chen/AAP-2221-2020
   Lei, Ming/D-8847-2013},
ORCID-Numbers = {Wei, Jing/0000-0003-2991-0123
   wei, zhong ming/0000-0002-6237-0993
   },
Unique-ID = {WOS:000554897200004},
}

@article{ WOS:000477857700021,
Author = {Yang, Kevin K. and Wu, Zachary and Arnold, Frances H.},
Title = {Machine-learning-guided directed evolution for protein engineering},
Journal = {NATURE METHODS},
Year = {2019},
Volume = {16},
Number = {8},
Pages = {687-694},
Month = {AUG},
Abstract = {Protein engineering through machine-learning-guided directed evolution
   enables the optimization of protein functions. Machine-learning
   approaches predict how sequence maps to function in a data-driven manner
   without requiring a detailed model of the underlying physics or
   biological pathways. Such methods accelerate directed evolution by
   learning from the properties of characterized variants and using that
   information to select sequences that are likely to exhibit improved
   properties. Here we introduce the steps required to build
   machine-learning sequence-function models and to use those models to
   guide engineering, making recommendations at each stage. This review
   covers basic concepts relevant to the use of machine learning for
   protein engineering, as well as the current literature and applications
   of this engineering paradigm. We illustrate the process with two case
   studies. Finally, we look to future opportunities for machine learning
   to enable the discovery of unknown protein functions and uncover the
   relationship between protein sequence and function.},
DOI = {10.1038/s41592-019-0496-6},
ISSN = {1548-7091},
EISSN = {1548-7105},
ORCID-Numbers = {Wu, Zachary/0000-0003-2429-9812},
Unique-ID = {WOS:000477857700021},
}

@article{ WOS:000645896700001,
Author = {Chellappa, Rama and Theodoridis, Sergios and van Schaik, Andre},
Title = {Advances in Machine Learning and Deep Neural Networks},
Journal = {PROCEEDINGS OF THE IEEE},
Year = {2021},
Volume = {109},
Number = {5},
Pages = {607-611},
Month = {MAY},
Abstract = {We are currently experiencing the dawn of what is known as the fourth
   industrial revolution. At the center of this historical happening, as
   one of the key enabling technologies, lies a discipline that deals with
   data and whose goal is to extract information and related knowledge that
   is hidden in it, in order to make predictions and, subsequently, take
   decisions. Machine learning (ML) is the name that is used as an umbrella
   to cover a wide range of theories, methods, algorithms, and
   architectures that are used to this end.},
DOI = {10.1109/JPROC.2021.3072172},
ISSN = {0018-9219},
EISSN = {1558-2256},
ResearcherID-Numbers = {Chellappa, Rama/B-6573-2012},
ORCID-Numbers = {van Schaik, Andre/0000-0001-6140-017X
   Theodoridis, Sergios/0000-0001-5040-161X
   },
Unique-ID = {WOS:000645896700001},
}

@article{ WOS:000625545300001,
Author = {Chen, James Ming},
Title = {An Introduction to Machine Learning for Panel Data},
Journal = {INTERNATIONAL ADVANCES IN ECONOMIC RESEARCH},
Year = {2021},
Volume = {27},
Number = {1},
Pages = {1-16},
Month = {FEB},
Abstract = {Machine learning has dramatically expanded the range of tools for
   evaluating economic panel data. This paper applies a variety of
   machine-learning methods to the Boston housing dataset, an iconic
   proving ground for machine learning. Though machine learning often lacks
   the overt interpretability of linear regression, methods based on
   decision trees score the relative importance of dataset features. In
   addition to addressing the theoretical tradeoff between bias and
   variance, this paper discusses practices rarely followed in traditional
   economics: the splitting of data into training, validation, and test
   sets; the scaling of data; and the preference for retaining all data.
   The choice between traditional and machine-learning methods hinges on
   practical rather than mathematical considerations. In settings
   emphasizing interpretative clarity through the scale and sign of
   regression coefficients, machine learning may best play an ancillary
   role. Wherever predictive accuracy is paramount, however, or where
   heteroskedasticity or high dimensionality might impair the clarity of
   linear methods, machine learning can deliver superior results.},
DOI = {10.1007/s11294-021-09815-6},
EarlyAccessDate = {MAR 2021},
ISSN = {1083-0898},
EISSN = {1573-966X},
ResearcherID-Numbers = {Chen, James/AAN-6668-2021
   },
ORCID-Numbers = {Chen, James/0000-0001-9824-174X},
Unique-ID = {WOS:000625545300001},
}

@article{ WOS:000538049100005,
Author = {Bikmukhametov, Timur and Jaschke, Johannes},
Title = {Combining machine learning and process engineering physics towards
   enhanced accuracy and explainability of data-driven models},
Journal = {COMPUTERS \& CHEMICAL ENGINEERING},
Year = {2020},
Volume = {138},
Month = {JUL 12},
Abstract = {Machine learning models are often considered as black-box solutions
   which is one of the main reasons why they are still not widely used in
   operation of process engineering systems. One approach to overcome this
   problem is to combine machine learning with first principles models of a
   process engineering system. In this work, we investigate different
   methods of combining machine learning with first principles and test
   them on a case study of multiphase flowrate estimation in a petroleum
   production system. However, the methods can be applied to any process
   engineering system. The results show that by adding physics-based models
   to machine learning, it is possible not only to improve the performance
   of the purely black-box machine learning models, but also to make them
   more transparent and interpretable. We also propose a step-by-step
   procedure for selecting a method for combining physics and machine
   learning depending on the process engineering system conditions. (C)
   2020 The Authors. Published by Elsevier Ltd.},
DOI = {10.1016/j.compchemeng.2020.106834},
Article-Number = {106834},
ISSN = {0098-1354},
EISSN = {1873-4375},
ResearcherID-Numbers = {Jäschke, Johannes/N-9405-2015},
Unique-ID = {WOS:000538049100005},
}

@article{ WOS:000496269400199,
Author = {Liu, Hongyu and Lang, Bo},
Title = {Machine Learning and Deep Learning Methods for Intrusion Detection
   Systems: A Survey},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2019},
Volume = {9},
Number = {20},
Month = {OCT},
Abstract = {Networks play important roles in modern life, and cyber security has
   become a vital research area. An intrusion detection system (IDS) which
   is an important cyber security technique, monitors the state of software
   and hardware running in the network. Despite decades of development,
   existing IDSs still face challenges in improving the detection accuracy,
   reducing the false alarm rate and detecting unknown attacks. To solve
   the above problems, many researchers have focused on developing IDSs
   that capitalize on machine learning methods. Machine learning methods
   can automatically discover the essential differences between normal data
   and abnormal data with high accuracy. In addition, machine learning
   methods have strong generalizability, so they are also able to detect
   unknown attacks. Deep learning is a branch of machine learning, whose
   performance is remarkable and has become a research hotspot. This survey
   proposes a taxonomy of IDS that takes data objects as the main dimension
   to classify and summarize machine learning-based and deep learning-based
   IDS literature. We believe that this type of taxonomy framework is fit
   for cyber security researchers. The survey first clarifies the concept
   and taxonomy of IDSs. Then, the machine learning algorithms frequently
   used in IDSs, metrics, and benchmark datasets are introduced. Next,
   combined with the representative literature, we take the proposed
   taxonomic system as a baseline and explain how to solve key IDS issues
   with machine learning and deep learning techniques. Finally, challenges
   and future developments are discussed by reviewing recent representative
   studies.},
DOI = {10.3390/app9204396},
Article-Number = {4396},
EISSN = {2076-3417},
ResearcherID-Numbers = {Lang, Bo/AAA-7966-2022},
Unique-ID = {WOS:000496269400199},
}

@article{ WOS:000459730200026,
Author = {Mishra, Preeti and Varadharajan, Vijay and Tupakula, Uday and Pilli,
   Emmanuel S.},
Title = {A Detailed Investigation and Analysis of Using Machine Learning
   Techniques for Intrusion Detection},
Journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS},
Year = {2019},
Volume = {21},
Number = {1},
Pages = {686-728},
Abstract = {Intrusion detection is one of the important security problems in todays
   cyber world. A significant number of techniques have been developed
   which are based on machine learning approaches. However, they are not
   very successful in identifying all types of intrusions. In this paper, a
   detailed investigation and analysis of various machine learning
   techniques have been carried out for finding the cause of problems
   associated with various machine learning techniques in detecting
   intrusive activities. Attack classification and mapping of the attack
   features is provided corresponding to each attack. Issues which are
   related to detecting low-frequency attacks using network attack dataset
   are also discussed and viable methods are suggested for improvement.
   Machine learning techniques have been analyzed and compared in terms of
   their detection capability for detecting the various category of
   attacks. Limitations associated with each category of them are also
   discussed. Various data mining tools for machine learning have also been
   included in the paper. At the end, future directions are provided for
   attack detection using machine learning techniques.},
DOI = {10.1109/COMST.2018.2847722},
EISSN = {1553-877X},
ResearcherID-Numbers = {Pilli, Emmanuel/H-7984-2017},
ORCID-Numbers = {TUPAKULA, Udaya/0000-0001-5048-9797
   },
Unique-ID = {WOS:000459730200026},
}

@article{ WOS:000474586200010,
Author = {Karpatne, Anuj and Ebert-Uphoff, Imme and Ravela, Sai and Babaie, Hassan
   Ali and Kumar, Vipin},
Title = {Machine Learning for the Geosciences: Challenges and Opportunities},
Journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
Year = {2019},
Volume = {31},
Number = {8},
Pages = {1544-1554},
Month = {AUG},
Abstract = {Geosciences is a field of great societal relevance that requires
   solutions to several urgent problems facing our humanity and the planet.
   As geosciences enters the era of big data, machine learning (ML)-that
   has been widely successful in commercial domains-offers immense
   potential to contribute to problems in geosciences. However, geoscience
   applications introduce novel challenges for ML due to combinations of
   geoscience properties encountered in every problem, requiring novel
   research in machine learning. This article introduces researchers in the
   machine learning (ML) community to these challenges offered by
   geoscience problems and the opportunities that exist for advancing both
   machine learning and geosciences. We first highlight typical sources of
   geoscience data and describe their common properties. We then describe
   some of the common categories of geoscience problems where machine
   learning can play a role, discussing the challenges faced by existing ML
   methods and opportunities for novel ML research. We conclude by
   discussing some of the cross-cutting research themes in machine learning
   that are applicable across several geoscience problems, and the
   importance of a deep collaboration between machine learning and
   geosciences for synergistic advancements in both disciplines.},
DOI = {10.1109/TKDE.2018.2861006},
ISSN = {1041-4347},
EISSN = {1558-2191},
ResearcherID-Numbers = {Ebert-Uphoff, Imme/Y-3389-2019
   Kumar, Vipin/KMY-0355-2024
   },
ORCID-Numbers = {Ebert-Uphoff, Imme/0000-0001-6470-1947},
Unique-ID = {WOS:000474586200010},
}

@article{ WOS:000530830800100,
Author = {Xue, Mingfu and Yuan, Chengxiang and Wu, Heyi and Zhang, Yushu and Liu,
   Weiqiang},
Title = {Machine Learning Security: Threats, Countermeasures, and Evaluations},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {74720-74742},
Abstract = {Machine learning has been pervasively used in a wide range of
   applications due to its technical breakthroughs in recent years. It has
   demonstrated significant success in dealing with various complex
   problems, and shows capabilities close to humans or even beyond humans.
   However, recent studies show that machine learning models are vulnerable
   to various attacks, which will compromise the security of the models
   themselves and the application systems. Moreover, such attacks are
   stealthy due to the unexplained nature of the deep learning models. In
   this survey, we systematically analyze the security issues of machine
   learning, focusing on existing attacks on machine learning systems,
   corresponding defenses or secure learning techniques, and security
   evaluation methods. Instead of focusing on one stage or one type of
   attack, this paper covers all the aspects of machine learning security
   from the training phase to the test phase. First, the machine learning
   model in the presence of adversaries is presented, and the reasons why
   machine learning can be attacked are analyzed. Then, the machine
   learning security-related issues are classified into five categories:
   training set poisoning; backdoors in the training set; adversarial
   example attacks; model theft; recovery of sensitive training data. The
   threat models, attack approaches, and defense techniques are analyzed
   systematically. To demonstrate that these threats are real concerns in
   the physical world, we also reviewed the attacks in real-world
   conditions. Several suggestions on security evaluations of machine
   learning systems are also provided. Last, future directions for machine
   learning security are also presented.},
DOI = {10.1109/ACCESS.2020.2987435},
ISSN = {2169-3536},
ORCID-Numbers = {Xue, Mingfu/0000-0003-2408-503X
   Liu, Weiqiang/0000-0001-8398-8648},
Unique-ID = {WOS:000530830800100},
}

@article{ WOS:000601374000001,
Author = {Meza Ramirez, Carlos A. and Greenop, Michael and Ashton, Lorna and
   Rehman, Ihtesham Ur},
Title = {Applications of machine learning in spectroscopy},
Journal = {APPLIED SPECTROSCOPY REVIEWS},
Year = {2021},
Volume = {56},
Number = {8-10},
Pages = {733-763},
Month = {NOV 26},
Abstract = {The way to analyze data in spectroscopy has changed substantially. At
   the same time, data science has evolved to the point where spectroscopy
   can find space to be housed, adapted and be functional. The integration
   of the two sciences has introduced a knowledge gap between data
   scientists who know about advanced machine learning techniques and
   spectroscopists who have a solid background in chemometrics. To reach a
   symbiosis, the knowledge gap requires bridging. This review article
   focuses on introducing data science subjects to non-specialist
   spectroscopists, or those unfamiliar with the subject. The article will
   explain concepts that are covered in machine learning, such as
   supervised learning, unsupervised learning, deep learning, and most
   importantly, the difference between machine learning and artificial
   intelligence. This article also includes examples of published
   spectroscopy research, in which some of the concepts explained here are
   applied. Machine learning together with spectroscopy can provide a
   useful, fast, and efficient tool to analyze samples of interest both for
   industrial and research purposes.},
DOI = {10.1080/05704928.2020.1859525},
EarlyAccessDate = {DEC 2020},
ISSN = {0570-4928},
EISSN = {1520-569X},
ResearcherID-Numbers = {Greenop, Michael/KDN-2412-2024
   },
ORCID-Numbers = {Rehman, Ihtesham/0000-0003-2502-7608
   Greenop, Dr. Michael/0000-0003-4894-7466
   Meza, Carlos/0000-0003-4588-3743},
Unique-ID = {WOS:000601374000001},
}

@article{ WOS:000543560600001,
Author = {Kumar, Prashant and Hati, Ananda Shankar},
Title = {Review on Machine Learning Algorithm Based Fault Detection in Induction
   Motors},
Journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING},
Year = {2021},
Volume = {28},
Number = {3},
Pages = {1929-1940},
Month = {MAY},
Abstract = {Fault detection prior to their occurrence or complete shut-down in
   induction motor is essential for the industries. The fault detection
   based on condition monitoring techniques and application of machine
   learning have tremendous potential. The power of machine learning can be
   harnessed and optimally used for fault detection. The faults especially
   in induction motor needs to be addressed at a proper time for avoiding
   losses. Machine learning algorithm applications in the domain of fault
   detection provides a reliable and effective solution for preventive
   maintenance. This paper presents a review of the machine learning
   algorithm applications in fault detection in induction motors. This
   paper also presents the future prospects and challenges for an efficient
   machine learning based fault detection systems.},
DOI = {10.1007/s11831-020-09446-w},
EarlyAccessDate = {JUN 2020},
ISSN = {1134-3060},
EISSN = {1886-1784},
ResearcherID-Numbers = {KUMAR, PRASHANT/AAZ-9714-2020
   Hati, Ananda/ABC-1258-2020},
ORCID-Numbers = {Hati, Ananda Shankar/0000-0003-1414-3398
   Kumar, Prashant/0000-0001-6353-6302
   },
Unique-ID = {WOS:000543560600001},
}

@article{ WOS:000568991800003,
Author = {Reig, Beatriu and Heacock, Laura and Geras, Krzysztof J. and Moy, Linda},
Title = {Machine learning in breast MRI},
Journal = {JOURNAL OF MAGNETIC RESONANCE IMAGING},
Year = {2020},
Volume = {52},
Number = {4},
Pages = {998-1018},
Month = {OCT},
Abstract = {Machine-learning techniques have led to remarkable advances in data
   extraction and analysis of medical imaging. Applications of machine
   learning to breast MRI continue to expand rapidly as increasingly
   accurate 3D breast and lesion segmentation allows the combination of
   radiologist-level interpretation (eg, BI-RADS lexicon), data from
   advanced multiparametric imaging techniques, and patient-level data such
   as genetic risk markers. Advances in breast MRI feature extraction have
   led to rapid dataset analysis, which offers promise in large pooled
   multiinstitutional data analysis. The object of this review is to
   provide an overview of machine-learning and deep-learning techniques for
   breast MRI, including supervised and unsupervised methods, anatomic
   breast segmentation, and lesion segmentation. Finally, it explores the
   role of machine learning, current limitations, and future applications
   to texture analysis, radiomics, and radiogenomics. Technical Efficacy
   Stage:2 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging
   2020;52:998-1018.},
DOI = {10.1002/jmri.26852},
ISSN = {1053-1807},
EISSN = {1522-2586},
ResearcherID-Numbers = {Moy, Linda/U-8018-2019
   },
ORCID-Numbers = {Moy, Linda/0000-0001-9564-9360
   Reig, Beatriu/0000-0002-6577-8601},
Unique-ID = {WOS:000568991800003},
}

@article{ WOS:000544436100001,
Author = {Shamshirband, Shahaboddin and Mosavi, Amir and Rabczuk, Timon and
   Nabipour, Narjes and Chau, Kwok-wing},
Title = {Prediction of significant wave height; comparison between nested grid
   numerical model, and machine learning models of artificial neural
   networks, extreme learning and support vector machines},
Journal = {ENGINEERING APPLICATIONS OF COMPUTATIONAL FLUID MECHANICS},
Year = {2020},
Volume = {14},
Number = {1},
Pages = {805-817},
Month = {JAN 1},
Abstract = {Estimation of wave height is essential for several coastal engineering
   applications. This study advances a nested grid numerical model and
   compare its efficiency with three machine learning (ML) methods of
   artificial neural networks (ANN), extreme learning machines (ELM) and
   support vector regression (SVR) for wave height modeling. The models are
   trained by surface wind data. The results demonstrate that all the
   models generally provide sound predictions. Due to the high level of
   variability in the bathymetry of the study area, implementation of the
   nested grid with different Whitecapping coefficient is a suitable
   approach to improve the efficiency of the numerical models. Performance
   on the ML models do not differ remarkably even though the ELM model
   slightly outperforms the other models.},
DOI = {10.1080/19942060.2020.1773932},
ISSN = {1994-2060},
EISSN = {1997-003X},
ResearcherID-Numbers = {Chau, Kwok-wing/E-5235-2011
   Mosavi, Amir/I-7440-2018
   Rabczuk, Timon/A-3067-2009
   S.Band, Shahab/ABI-7388-2020
   },
ORCID-Numbers = {/0000-0003-4842-0613},
Unique-ID = {WOS:000544436100001},
}

@article{ WOS:000638379600001,
Author = {Choi, Hyejeong and Park, Sejin},
Title = {A Survey of Machine Learning-Based System Performance Optimization
   Techniques},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2021},
Volume = {11},
Number = {7},
Month = {APR},
Abstract = {Recently, the machine learning research trend expands to the system
   performance optimization field, where it has still been proposed by
   researchers based on their intuitions and heuristics. Compared to
   conventional major machine learning research areas such as image or
   speech recognition, machine learning-based system performance
   optimization fields are at the beginning stage. However, recent papers
   show that this approach is promising and has significant potential. This
   paper reviews 11 machine learning-based system performance optimization
   approaches from nine recent papers based on well-known machine learning
   models such as perceptron, LSTM, and RNN. This survey provides a
   detailed design and summarizes model, input, output, and prediction
   method of each approach. This paper covers various system performance
   areas from the data structure to essential system components of a
   computer system such as index structure, branch predictor, sort, and
   cache management. The result shows that machine learning-based system
   performance optimization has an important potential for future research.
   We expect that this paper shows a wide range of applicability of machine
   learning technology and provides a new perspective for system
   performance optimization.},
DOI = {10.3390/app11073235},
Article-Number = {3235},
EISSN = {2076-3417},
ResearcherID-Numbers = {Park, Sejin/KPY-3053-2024},
ORCID-Numbers = {Park, Sejin/0000-0001-5050-3093
   Choi, Hyejeong/0000-0002-2824-6382
   },
Unique-ID = {WOS:000638379600001},
}

@article{ WOS:000551254000005,
Author = {Hugle, Maria and Omoumi, Patrick and van Laar, Jacob M. and Boedecker,
   Joschka and Hugle, Thomas},
Title = {Applied machine learning and artificial intelligence in rheumatology},
Journal = {RHEUMATOLOGY ADVANCES IN PRACTICE},
Year = {2020},
Volume = {4},
Number = {1},
Abstract = {Machine learning as a field of artificial intelligence is increasingly
   applied in medicine to assist patients and physicians. Growing datasets
   provide a sound basis with which to apply machine learning methods that
   learn from previous experiences. This review explains the basics of
   machine learning and its subfields of supervised learning, unsupervised
   learning, reinforcement learning and deep learning. We provide an
   overview of current machine learning applications in rheumatology,
   mainly supervised learning methods for e-diagnosis, disease detection
   and medical image analysis. In the future, machine learning will be
   likely to assist rheumatologists in predicting the course of the disease
   and identifying important disease factors. Even more interestingly,
   machine learning will probably be able to make treatment propositions
   and estimate their expected benefit (e.g. by reinforcement learning).
   Thus, in future, shared decision-making will not only include the
   patient's opinion and the rheumatologist's empirical and evidence-based
   experience, but it will also be influenced by machine-learned evidence.},
DOI = {10.1093/rap/rkaa005},
Article-Number = {rkaa005},
EISSN = {2514-1775},
ResearcherID-Numbers = {Boedecker, Joschka/A-8144-2010
   Omoumi, Patrick/AAE-5721-2020
   },
ORCID-Numbers = {Boedecker, Joschka/0000-0002-3486-7345
   Hugle, Thomas/0000-0002-3276-9581
   Kalweit, Maria/0000-0003-4581-8810
   Omoumi, Patrick/0000-0002-3251-3666},
Unique-ID = {WOS:000551254000005},
}

@article{ WOS:000600294400001,
Author = {Khan, Tariq M. and Robles-Kelly, Antonio},
Title = {Machine Learning: Quantum vs Classical},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {219275-219294},
Abstract = {Encouraged by growing computing power and algorithmic development,
   machine learning technologies have become powerful tools for a wide
   variety of application areas, spanning from agriculture to chemistry and
   natural language processing. The use of quantum systems to process
   classical data using machine learning algorithms has given rise to an
   emerging research area, i.e. quantum machine learning. Despite its
   origins in the processing of classical data, quantum machine learning
   also explores the use of quantum phenomena for learning systems, the use
   of quantum computers for learning on quantum data and how machine
   learning algorithms and software can be formulated and implemented on
   quantum computers. Quantum machine learning can have a transformational
   effect on computer science. It may speed up the processing of
   information well beyond the existing classical speeds. Recent work has
   seen the development of quantum algorithms that could serve as
   foundations for machine learning applications. Despite its great
   promise, there are still significant hardware and software challenges
   that need to be resolved before quantum machine learning becomes
   practical. In this paper, we present an overview of quantum machine
   learning in the light of classical approaches. Departing from
   foundational concepts of machine learning and quantum computing, we
   discuss various technical contributions, strengths and similarities of
   the research work in this domain. We also elaborate upon the recent
   progress of different quantum machine learning approaches, their
   complexity, and applications in various fields such as physics,
   chemistry and natural language processing.},
DOI = {10.1109/ACCESS.2020.3041719},
ISSN = {2169-3536},
ResearcherID-Numbers = {Khan, Muhammad/L-7462-2016},
ORCID-Numbers = {Khan, Tariq Mahmood/0000-0002-7477-1591
   },
Unique-ID = {WOS:000600294400001},
}

@article{ WOS:000601128500001,
Author = {Lo Vercio, Lucas and Amador, Kimberly and Bannister, Jordan J. and
   Crites, Sebastian and Gutierrez, Alejandro and MacDonald, M. Ethan and
   Moore, Jasmine and Mouches, Pauline and Rajashekar, Deepthi and
   Schimert, Serena and Subbanna, Nagesh and Tuladhar, Anup and Wang,
   Nanjia and Wilms, Matthias and Winder, Anthony and Forkert, Nils D.},
Title = {Supervised machine learning tools: a tutorial for clinicians},
Journal = {JOURNAL OF NEURAL ENGINEERING},
Year = {2020},
Volume = {17},
Number = {6},
Month = {DEC},
Abstract = {In an increasingly data-driven world, artificial intelligence is
   expected to be a key tool for converting big data into tangible benefits
   and the healthcare domain is no exception to this. Machine learning aims
   to identify complex patterns in multi-dimensional data and use these
   uncovered patterns to classify new unseen cases or make data-driven
   predictions. In recent years, deep neural networks have shown to be
   capable of producing results that considerably exceed those of
   conventional machine learning methods for various classification and
   regression tasks. In this paper, we provide an accessible tutorial of
   the most important supervised machine learning concepts and methods,
   including deep learning, which are potentially the most relevant for the
   medical domain. We aim to take some of the mystery out of machine
   learning and depict how machine learning models can be useful for
   medical applications. Finally, this tutorial provides a few practical
   suggestions for how to properly design a machine learning model for a
   generic medical problem.},
DOI = {10.1088/1741-2552/abbff2},
Article-Number = {062001},
ISSN = {1741-2560},
EISSN = {1741-2552},
ResearcherID-Numbers = {Winder, Anthony/HKE-8188-2023
   Forkert, Nils/K-6273-2012
   Lo Vercio, Lucas/AAR-4408-2021
   Forkert, Nils Daniel/K-6273-2012
   mouches, pauline/AFT-3154-2022
   Tuladhar, Anup/AAL-7139-2020},
ORCID-Numbers = {Moore, Jasmine/0000-0001-9333-2730
   Lo Vercio, Lucas/0000-0002-5465-6801
   MacDonald, M Ethan/0000-0001-5421-3536
   Mouches, Pauline/0000-0002-0304-5584
   Gutierrez, Alejandro/0000-0003-1224-7169
   Rajashekar, Deepthi/0000-0003-2009-764X
   Forkert, Nils Daniel/0000-0003-2556-3224
   Tuladhar, Anup/0000-0002-3942-2732
   Winder, Anthony/0000-0002-9650-5526
   Amador, Kimberly/0000-0003-3956-1895
   },
Unique-ID = {WOS:000601128500001},
}

@article{ WOS:000530832200187,
Author = {Janny Ariza-Garzon, Miller and Arroyo, Javier and Caparrini, Antonio and
   Segovia-Vargas, Maria-Jesus},
Title = {Explainability of a Machine Learning Granting Scoring Model in
   Peer-to-Peer Lending},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {64873-64890},
Abstract = {Peer-to-peer (P2P) lending demands effective and explainable credit risk
   models. Typical machine learning algorithms offer high prediction
   performance, but most of them lack explanatory power. However, this
   deficiency can be solved with the help of the explainability tools
   proposed in the last few years, such as the SHAP values. In this work,
   we assess the well-known logistic regression model and several machine
   learning algorithms for granting scoring in P2P lending. The comparison
   reveals that the machine learning alternative is superior in terms of
   not only classification performance but also explainability. More
   precisely, the SHAP values reveal that machine learning algorithms can
   reflect dispersion, nonlinearity and structural breaks in the
   relationships between each feature and the target variable. Our results
   demonstrate that is possible to have machine learning credit scoring
   models be both accurate and transparent. Such models provide the trust
   that the industry, regulators and end-users demand in P2P lending and
   may lead to a wider adoption of machine learning in this and other risk
   assessment applications where explainability is required.},
DOI = {10.1109/ACCESS.2020.2984412},
ISSN = {2169-3536},
ResearcherID-Numbers = {SEGOVIA-VARGAS, MARIA-JESUS/I-3637-2017
   López, Antonio/ABC-9659-2020
   Arroyo, Javier/K-3353-2014},
ORCID-Numbers = {SEGOVIA-VARGAS, MARIA-JESUS/0000-0002-6578-8017
   ARIZA, MILLER/0000-0002-0370-9539
   Caparrini Lopez, Antonio Javier/0000-0002-5411-6397
   Arroyo, Javier/0000-0001-6127-7538},
Unique-ID = {WOS:000530832200187},
}

@article{ WOS:000626551700002,
Author = {Garbulowski, Mateusz and Diamanti, Klev and Smolinska, Karolina and
   Baltzer, Nicholas and Stoll, Patricia and Bornelov, Susanne and Ohrn,
   Aleksander and Feuk, Lars and Komorowski, Jan},
Title = {R.ROSETTA: an interpretable machine learning framework},
Journal = {BMC BIOINFORMATICS},
Year = {2021},
Volume = {22},
Number = {1},
Month = {MAR 6},
Abstract = {BackgroundMachine learning involves strategies and algorithms that may
   assist bioinformatics analyses in terms of data mining and knowledge
   discovery. In several applications, viz. in Life Sciences, it is often
   more important to understand how a prediction was obtained rather than
   knowing what prediction was made. To this end so-called interpretable
   machine learning has been recently advocated. In this study, we
   implemented an interpretable machine learning package based on the rough
   set theory. An important aim of our work was provision of statistical
   properties of the models and their components.ResultsWe present the
   R.ROSETTA package, which is an R wrapper of ROSETTA framework. The
   original ROSETTA functions have been improved and adapted to the R
   programming environment. The package allows for building and analyzing
   non-linear interpretable machine learning models. R.ROSETTA gathers
   combinatorial statistics via rule-based modelling for accessible and
   transparent results, well-suited for adoption within the greater
   scientific community. The package also provides statistics and
   visualization tools that facilitate minimization of analysis bias and
   noise. The R.ROSETTA package is freely available at
   https://github.com/komorowskilab/R.ROSETTA. To illustrate the usage of
   the package, we applied it to a transcriptome dataset from an autism
   case-control study. Our tool provided hypotheses for potential
   co-predictive mechanisms among features that discerned phenotype
   classes. These co-predictors represented neurodevelopmental and
   autism-related genes.ConclusionsR.ROSETTA provides new insights for
   interpretable machine learning analyses and knowledge-based systems. We
   demonstrated that our package facilitated detection of dependencies for
   autism-related genes. Although the sample application of R.ROSETTA
   illustrates transcriptome data analysis, the package can be used to
   analyze any data organized in decision tables.},
DOI = {10.1186/s12859-021-04049-z},
Article-Number = {110},
ISSN = {1471-2105},
ResearcherID-Numbers = {Bornelöv, Susanne/I-8225-2019
   Komorowski, Jan/M-2667-2013
   Diamanti, Klev/B-3352-2014},
ORCID-Numbers = {Bornelov, Susanne/0000-0001-9276-9981
   Komorowski, Jan/0000-0002-0766-8789
   Diamanti, Klev/0000-0002-4922-8415},
Unique-ID = {WOS:000626551700002},
}

@article{ WOS:000546625200003,
Author = {Goldberg, Simon B. and Flemotomos, Nikolaos and Martinez, Victor R. and
   Tanana, Michael J. and Kuo, Patty B. and Pace, Brian T. and Villatte,
   Jennifer L. and Georgiou, Panayiotis G. and Van Epps, Jake and Imel, Zac
   E. and Narayanan, Shrikanth S. and Atkins, David C.},
Title = {Machine Learning and Natural Language Processing in Psychotherapy
   Research: Alliance as Example Use Case},
Journal = {JOURNAL OF COUNSELING PSYCHOLOGY},
Year = {2020},
Volume = {67},
Number = {4, SI},
Pages = {438-448},
Month = {JUL},
Abstract = {Artificial intelligence generally and machine learning specifically have
   become deeply woven into the lives and technologies of modern life.
   Machine learning is dramatically changing scientific research and
   industry and may also hold promise for addressing limitations
   encountered in mental health care and psychotherapy. The current paper
   introduces machine learning and natural language processing as related
   methodologies that may prove valuable for automating the assessment of
   meaningful aspects of treatment. Prediction of therapeutic alliance from
   session recordings is used as a case in point. Recordings from 1,235
   sessions of 386 clients seen by 40 therapists at a university counseling
   center were processed using automatic speech recognition software.
   Machine learning algorithms learned associations between client ratings
   of therapeutic alliance exclusively from session linguistic content.
   Using a portion of the data to train the model, machine learning
   algorithms modestly predicted alliance ratings from session content in
   an independent test set (Spearman's rho =15, p <.001). These results
   highlight the potential to harness natural language processing and
   machine learning to predict a key psychotherapy process variable that is
   relatively distal from linguistic content. Six practical suggestions for
   conducting psychotherapy research using machine learning are presented
   along with several directions for future research. Questions of
   dissemination and implementation may be particularly important to
   explore as machine learning improves in its ability to automate
   assessment of psychotherapy process and outcome.},
DOI = {10.1037/cou0000382},
ISSN = {0022-0167},
EISSN = {1939-2168},
ResearcherID-Numbers = {Villatte, Jennifer/M-8758-2019
   Georgiou, Panayiotis/E-8387-2018
   Narayanan, Shrikanth/D-5676-2012
   Goldberg, Simon/AAQ-5173-2021},
ORCID-Numbers = {Flemotomos, Nikolaos/0000-0001-5227-0246
   Goldberg, Simon B./0000-0002-6888-0126
   imel, zachary/0000-0001-9645-7184
   },
Unique-ID = {WOS:000546625200003},
}

@article{ WOS:000545164100001,
Author = {Shouval, Roni and Fein, Joshua A. and Savani, Bipin and Mohty, Mohamad
   and Nagler, Arnon},
Title = {Machine learning and artificial intelligence in haematology},
Journal = {BRITISH JOURNAL OF HAEMATOLOGY},
Year = {2021},
Volume = {192},
Number = {2},
Pages = {239-250},
Month = {JAN},
Abstract = {Digitalization of the medical record and integration of genomic methods
   into clinical practice have resulted in an unprecedented wealth of data.
   Machine learning is a subdomain of artificial intelligence that attempts
   to computationally extract meaningful insights from complex data
   structures. Applications of machine learning in haematological scenarios
   are steadily increasing. However, basic concepts are often unfamiliar to
   clinicians and investigators. The purpose of this review is to provide
   readers with tools to interpret and critically appraise machine learning
   literature. We begin with the elucidation of standard terminology and
   then review examples in haematology. Guidelines for designing and
   evaluating machine-learning studies are provided. Finally, we discuss
   limitations of the machine-learning approach.},
DOI = {10.1111/bjh.16915},
EarlyAccessDate = {JUN 2020},
ISSN = {0007-1048},
EISSN = {1365-2141},
ResearcherID-Numbers = {Mohty, Mohamad/LSL-2073-2024
   Fein, Joshua/GXZ-7089-2022
   Savani, Bipin/LZH-1365-2025},
ORCID-Numbers = {Mohty, Mohamad/0000-0002-7264-808X
   Fein, Joshua/0000-0001-5441-1516
   Shouval, Roni/0000-0001-9827-8032
   },
Unique-ID = {WOS:000545164100001},
}

@article{ WOS:000595502700002,
Author = {Nakaura, Takeshi and Higaki, Toru and Awai, Kazuo and Ikeda, Osamu and
   Yamashita, Yasuyuki},
Title = {A primer for understanding radiology articles about machine learning and
   deep learning},
Journal = {DIAGNOSTIC AND INTERVENTIONAL IMAGING},
Year = {2020},
Volume = {101},
Number = {12},
Pages = {765-770},
Month = {DEC},
Abstract = {The application of machine learning and deep learning in the field of
   imaging is rapidly growing. Although the principles of machine and deep
   learning are unfamiliar to the majority of clinicians, the basics are
   not so complicated. One of the major issues is that commentaries written
   by experts are difficult to understand, and are not primarily written
   for clinicians. The purpose of this article was to describe the
   different concepts behind machine learning, radiomics, and deep learning
   to make clinicians more familiar with these techniques. (C) 2020 Societe
   francaise de radiologie. Published by Elsevier Masson SAS. All rights
   reserved.},
DOI = {10.1016/j.diii.2020.10.001},
ISSN = {2211-5684},
ORCID-Numbers = {Nakaura, Takeshi/0000-0002-9010-0341
   Higaki, Toru/0000-0003-0631-7271},
Unique-ID = {WOS:000595502700002},
}

@article{ WOS:000544758900009,
Author = {Radakovich, Nathan and Nagy, Matthew and Nazha, Aziz},
Title = {Machine learning in haematological malignancies},
Journal = {LANCET HAEMATOLOGY},
Year = {2020},
Volume = {7},
Number = {7},
Pages = {E541-E550},
Month = {JUL},
Abstract = {Machine learning is a branch of computer science and statistics that
   generates predictive or descriptive models by learning from training
   data rather than by being rigidly programmed. It has attracted
   substantial attention for its many applications in medicine, both as a
   catalyst for research and as a means of improving clinical care across
   the cycle of diagnosis, prognosis, and treatment of disease. These
   applications include the management of haematological malignancy, in
   which machine learning has created inroads in pathology, radiology,
   genomics, and the analysis of electronic health record data. As
   computational power becomes cheaper and the tools for implementing
   machine learning become increasingly democratised, it is likely to
   become increasingly integrated into the research and practice landscape
   of haematology. As such, machine learning merits understanding and
   attention from researchers and clinicians alike. This narrative Review
   describes important concepts in machine learning for unfamiliar readers,
   details machine learning?s current applications in haematological
   malignancy, and summarises important concepts for clinicians to be aware
   of when appraising research that uses machine learning.},
ISSN = {2352-3026},
Unique-ID = {WOS:000544758900009},
}

@article{ WOS:000518473500027,
Author = {Gennatas, Efstathios D. and Friedman, Jerome H. and Ungar, Lyle H. and
   Pirracchio, Romain and Eaton, Eric and Reichmann, Lara G. and Interian,
   Yannet and Luna, Jose Marcio and Simone, II, Charles B. and Auerbach,
   Andrew and Delgado, Elier and van der Laan, Mark J. and Solberg, Timothy
   D. and Valdes, Gilmer},
Title = {Expert-augmented machine learning},
Journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA},
Year = {2020},
Volume = {117},
Number = {9},
Pages = {4571-4577},
Month = {MAR 3},
Abstract = {Machine learning is proving invaluable across disciplines. However, its
   success is often limited by the quality and quantity of available data,
   while its adoption is limited by the level of trust afforded by given
   models. Human vs. machine performance is commonly compared empirically
   to decide whether a certain task should be performed by a computer or an
   expert. In reality, the optimal learning strategy may involve combining
   the complementary strengths of humans and machines. Here, we present
   expert-augmented machine learning (EAML), an automated method that
   guides the extraction of expert knowledge and its integration into
   machine-learned models. We used a large dataset of intensive-care
   patient data to derive 126 decision rules that predict hospital
   mortality. Using an online platform, we asked 15 clinicians to assess
   the relative risk of the subpopulation defined by each rule compared to
   the total sample. We compared the clinician-assessed risk to the
   empirical risk and found that, while clinicians agreed with the data in
   most cases, there were notable exceptions where they overestimated or
   underestimated the true risk. Studying the rules with greatest
   disagreement, we identified problems with the training data, including
   one miscoded variable and one hidden confounder. Filtering the rules
   based on the extent of disagreement between clinician-assessed risk and
   empirical risk, we improved performance on out-of-sample data and were
   able to train with less data. EAML provides a platform for automated
   creation of problem-specific priors, which help build robust and
   dependable machine-learning models in critical applications.},
DOI = {10.1073/pnas.1906831117},
ISSN = {0027-8424},
ResearcherID-Numbers = {Schuler, Alejandro/JOK-9202-2023
   Luna, Jose/ABG-1296-2020
   },
ORCID-Numbers = {Solberg, Timothy/0000-0001-8829-7774
   Friedman, Jerome/0000-0001-5968-8901
   Gennatas, Efstathios/0000-0001-9280-3609
   Luna, Jose/0000-0002-5513-022X},
Unique-ID = {WOS:000518473500027},
}

@article{ WOS:000568280500001,
Author = {Chang, Michael and Canseco, Jose A. and Nicholson, Kristen J. and Patel,
   Neil and Vaccaro, Alexander R.},
Title = {The Role of Machine Learning in Spine Surgery: The Future Is Now},
Journal = {FRONTIERS IN SURGERY},
Year = {2020},
Volume = {7},
Month = {AUG 21},
Abstract = {The recent influx of machine learning centered investigations in the
   spine surgery literature has led to increased enthusiasm as to the
   prospect of using artificial intelligence to create clinical decision
   support tools, optimize postoperative outcomes, and improve technologies
   used in the operating room. However, the methodology underlying machine
   learning in spine research is often overlooked as the subject matter is
   quite novel and may be foreign to practicing spine surgeons. Improper
   application of machine learning is a significant bioethics challenge,
   given the potential consequences of over- or underestimating the results
   of such studies for clinical decision-making processes. Proper peer
   review of these publications requires a baseline familiarity of the
   language associated with machine learning, and how it differs from
   classical statistical analyses. This narrative review first introduces
   the overall field of machine learning and its role in artificial
   intelligence, and defines basic terminology. In addition, common
   modalities for applying machine learning, including classification and
   regression decision trees, support vector machines, and artificial
   neural networks are examined in the context of examples gathered from
   the spine literature. Lastly, the ethical challenges associated with
   adapting machine learning for research related to patient care, as well
   as future perspectives on the potential use of machine learning in spine
   surgery, are discussed specifically.},
DOI = {10.3389/fsurg.2020.00054},
Article-Number = {54},
ISSN = {2296-875X},
ResearcherID-Numbers = {Chang, Michael/JXN-0401-2024
   Canseco, Jose/AAT-1667-2020},
Unique-ID = {WOS:000568280500001},
}

@article{ WOS:000888566100001,
Author = {{[}Anonymous]},
Title = {Moving towards reproducible machine learning},
Journal = {NATURE COMPUTATIONAL SCIENCE},
Year = {2021},
Volume = {1},
Number = {10},
Pages = {629-630},
Month = {OCT},
Abstract = {We provide some recommendations on how to report machine learning-based
   research in order to improve transparency and reproducibility.},
DOI = {10.1038/s43588-021-00152-6},
EISSN = {2662-8457},
Unique-ID = {WOS:000888566100001},
}

@article{ WOS:000762389600001,
Author = {Heuer, Hendrik and Jarke, Juliane and Breiter, Andreas},
Title = {Machine learning in tutorials - Universal applicability, underinformed
   application, and other misconceptions},
Journal = {BIG DATA \& SOCIETY},
Year = {2021},
Volume = {8},
Number = {1},
Month = {JAN},
Abstract = {Machine learning has become a key component of contemporary information
   systems. Unlike prior information systems explicitly programmed in
   formal languages, ML systems infer rules from data. This paper shows
   what this difference means for the critical analysis of socio-technical
   systems based on machine learning. To provide a foundation for future
   critical analysis of machine learning-based systems, we engage with how
   the term is framed and constructed in self-education resources. For
   this, we analyze machine learning tutorials, an important information
   source for self-learners and a key tool for the formation of the
   practices of the machine learning community. Our analysis identifies
   canonical examples of machine learning as well as important
   misconceptions and problematic framings. Our results show that machine
   learning is presented as being universally applicable and that the
   application of machine learning without special expertise is actively
   encouraged. Explanations of machine learning algorithms are missing or
   strongly limited. Meanwhile, the importance of data is vastly
   understated. This has implications for the manifestation of (new) social
   inequalities through machine learning-based systems.},
DOI = {10.1177/20539517211017593},
Article-Number = {20539517211017593},
ISSN = {2053-9517},
ResearcherID-Numbers = {Jarke, Juliane/AAQ-1519-2021
   Breiter, Andreas/P-4859-2016
   },
ORCID-Numbers = {Breiter, Andreas/0000-0002-0577-8685
   Jarke, Juliane/0000-0001-8349-2298
   Heuer, Hendrik/0000-0003-1919-9016},
Unique-ID = {WOS:000762389600001},
}

@article{ WOS:000726375800010,
Author = {Zhang, Shuwen and Su, Qiang and Chen, Qin},
Title = {Application of Machine Learning in Animal Disease Analysis and
   Prediction},
Journal = {CURRENT BIOINFORMATICS},
Year = {2021},
Volume = {16},
Number = {7},
Pages = {972-982},
Abstract = {Major animal diseases pose a great threat to animal husbandry and human
   beings. With the deepening of globalization and the abundance of data
   resources, the prediction and analysis of animal diseases by using big
   data are becoming more and more important. The focus of machine learning
   is to make computers how to learn from data and use the learned
   experience to analyze and predict. Firstly, this paper introduces the
   animal epidemic situation and machine learning. Then it briefly
   introduces the application of machine learning in animal disease
   analysis and prediction. Machine learning is mainly divided into
   supervised learning and unsupervised learning. Supervised learning
   includes support vector machines, naive bayes, decision trees, random
   forests, logistic regression, artificial neural networks, deep learning,
   and AdaBoost. Unsupervised learning has maximum expectation algorithm,
   principal component analysis hierarchical clustering algorithm and
   maxent. Through the discussion of this paper, people have a clearer
   concept of machine learning and an understanding of its application
   prospect in animal diseases.},
DOI = {10.2174/1574893615999200728195613},
ISSN = {1574-8936},
EISSN = {2212-392X},
ResearcherID-Numbers = {Su, Qiang/H-3616-2017},
ORCID-Numbers = {Zhang, Shuwen/0000-0002-6895-8608
   Su, Qiang/0000-0003-0755-5802},
Unique-ID = {WOS:000726375800010},
}

@article{ WOS:000503916400004,
Author = {Borstelmann, Stephen M.},
Title = {Machine Learning Principles for Radiology Investigators},
Journal = {ACADEMIC RADIOLOGY},
Year = {2020},
Volume = {27},
Number = {1, SI},
Pages = {13-25},
Month = {JAN},
Abstract = {Artificial intelligence and deep learning are areas of high interest for
   radiology investigators at present. However, the field of machine
   learning encompasses multiple statistics-based techniques useful for
   investigators, which may be complementary to deep learning approaches.
   After a refresher in basic statistical concepts, relevant considerations
   for machine learning practitioners are reviewed: regression,
   classification, decision boundaries, and bias-variance tradeoff.
   Regularization, ground truth, and populations are discussed along with
   compute and data management principles. Advanced statistical machine
   learning techniques including bootstrapping, bagging, boosting, decision
   trees, random forest, XGboost, and support vector machines are reviewed
   along with relevant examples from the radiology literature.},
DOI = {10.1016/j.acra.2019.07.030},
ISSN = {1076-6332},
EISSN = {1878-4046},
ResearcherID-Numbers = {Borstelmann, stephen/JMP-4574-2023
   },
ORCID-Numbers = {, Stephen/0000-0002-4806-5183},
Unique-ID = {WOS:000503916400004},
}

@article{ WOS:000590917300001,
Author = {Webb, Mary E. and Fluck, Andrew and Magenheim, Johannes and Malyn-Smith,
   Joyce and Waters, Juliet and Deschenes, Michelle and Zagami, Jason},
Title = {Machine learning for human learners: opportunities, issues, tensions and
   threats},
Journal = {ETR\&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT},
Year = {2021},
Volume = {69},
Number = {4, SI},
Pages = {2109-2130},
Month = {AUG},
Abstract = {Machine learning systems are infiltrating our lives and are beginning to
   become important in our education systems. This article, developed from
   a synthesis and analysis of previous research, examines the implications
   of recent developments in machine learning for human learners and
   learning. In this article we first compare deep learning in computers
   and humans to examine their similarities and differences. Deep learning
   is identified as a sub-set of machine learning, which is itself a
   component of artificial intelligence. Deep learning often depends on
   backwards propagation in weighted neural networks, so is
   non-deterministic-the system adapts and changes through practical
   experience or training. This adaptive behaviour predicates the need for
   explainability and accountability in such systems. Accountability is the
   reverse of explainability. Explainability flows through the system from
   inputs to output (decision) whereas accountability flows backwards, from
   a decision to the person taking responsibility for it. Both
   explainability and accountability should be incorporated in machine
   learning system design from the outset to meet social, ethical and
   legislative requirements. For students to be able to understand the
   nature of the systems that may be supporting their own learning as well
   as to act as responsible citizens in contemplating the ethical issues
   that machine learning raises, they need to understand key aspects of
   machine learning systems and have opportunities to adapt and create such
   systems. Therefore, some changes are needed to school curricula. The
   article concludes with recommendations about machine learning for
   teachers, students, policymakers, developers and researchers.},
DOI = {10.1007/s11423-020-09858-2},
EarlyAccessDate = {NOV 2020},
ISSN = {1042-1629},
EISSN = {1556-6501},
ResearcherID-Numbers = {Fluck, Andrew/L-1874-2019
   Zagami, Jason/HJZ-0517-2023
   },
ORCID-Numbers = {Malyn-Smith, Joyce/0009-0000-5092-8008
   Webb, Mary/0000-0002-4409-5940
   Deschenes, Michelle/0000-0002-8804-5160
   Zagami, Jason/0000-0003-2003-9546},
Unique-ID = {WOS:000590917300001},
}

@article{ WOS:000519572500022,
Author = {Jacobs, Ryan and Mayeshiba, Tam and Afflerbach, Ben and Miles, Luke and
   Williams, Max and Turner, Matthew and Finkel, Raphael and Morgan, Dane},
Title = {The Materials Simulation Toolkit for Machine learning (MAST-ML): An
   automated open source toolkit to accelerate data-driven materials
   research},
Journal = {COMPUTATIONAL MATERIALS SCIENCE},
Year = {2020},
Volume = {176},
Month = {APR 15},
Abstract = {As data science and machine learning methods are taking on an
   increasingly important role in the materials research community, there
   is a need for the development of machine learning software tools that
   are easy to use (even for nonexperts with no programming ability),
   provide flexible access to the most important algorithms, and codify
   best practices of machine learning model development and evaluation.
   Here, we introduce the Materials Simulation Toolkit for Machine Learning
   (MAST-ML), an open source Python-based software package designed to
   broaden and accelerate the use of machine learning in materials science
   research. MAST-ML provides predefined routines for many input setup,
   model fitting, and post-analysis tasks, as well as a simple structure
   for executing a multi-step machine learning model workflow. In this
   paper, we describe how MAST-ML is used to streamline and accelerate the
   execution of machine learning problems. We walk through how to acquire
   and run MAST-ML, demonstrate how to execute different components of a
   supervised machine learning workflow via a customized input file, and
   showcase a number of features and analyses conducted automatically
   during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by
   showcasing examples of recent materials informatics studies which used
   MAST-ML to formulate and evaluate various machine learning models for an
   array of materials applications. Finally, we lay out a vision of how
   MAST-ML, together with complementary software packages and emerging
   cyberinfrastructure, can advance the rapidly growing field of materials
   informatics, with a focus on producing machine learning models easily,
   reproducibly, and in a manner that facilitates model evolution and
   improvement in the future.},
DOI = {10.1016/j.commatsci.2020.109544},
Article-Number = {109544},
ISSN = {0927-0256},
EISSN = {1879-0801},
ResearcherID-Numbers = {Morgan, Dane/B-7972-2008},
ORCID-Numbers = {Jacobs, Ryan/0000-0003-2229-6730
   Morgan, Dane/0000-0002-4911-0046
   },
Unique-ID = {WOS:000519572500022},
}

@article{ WOS:000592624200002,
Author = {Weinan, E.},
Title = {Machine Learning and Computational Mathematics},
Journal = {COMMUNICATIONS IN COMPUTATIONAL PHYSICS},
Year = {2020},
Volume = {28},
Number = {5},
Pages = {1639-1670},
Month = {NOV},
Abstract = {Neural network-based machine learning is capable of approximating
   functions in very high dimension with unprecedented efficiency and
   accuracy. This has opened up many exciting new possibilities, not just
   in traditional areas of artificial intelligence, but also in scientific
   computing and computational science. At the same time, machine learning
   has also acquired the reputation of being a set of ``black box{''} type
   of tricks, without fundamental principles. This has been a real obstacle
   for making further progress in machine learning.
   In this article, we try to address the following two very important
   questions: (1) How machine learning has already impacted and will
   further impact computational mathematics, scientific computing and
   computational science? (2) How computational mathematics, particularly
   numerical analysis, can impact machine learning? We describe some of the
   most important progress that has been made on these issues. Our hope is
   to put things into a perspective that will help to integrate machine
   learning with computational mathematics.},
DOI = {10.4208/cicp.OA-2020-0185},
ISSN = {1815-2406},
EISSN = {1991-7120},
Unique-ID = {WOS:000592624200002},
}

@article{ WOS:000572456900001,
Author = {Cho, Soohyun and Vasarhelyi, Miklos A. and Sun, Ting (Sophia) and Zhang,
   Chanyuan (Abigail)},
Title = {Learning from Machine Learning in Accounting and Assurance},
Journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING},
Year = {2020},
Volume = {17},
Number = {1},
Pages = {1-10},
Month = {SPR},
Abstract = {Machine learning is a subset of artificial intelligence, and it is a
   computational method that learns patterns from large and complex data.
   The learning processes enable us to make predictions for future events.
   In the accounting and assurance profession, machine learning is
   gradually being applied to various tasks like reviewing source
   documents, analyzing business transactions or activities, and assessing
   risks. In academic research, machine learning has been used to make
   predictions of fraud, bankruptcy, material misstatements, and accounting
   estimates. More importantly, machine learning is generating awareness
   about the inductive reasoning methodology, which has long been
   undervalued in the mainstream of academic research in accounting and
   auditing. The use of machine learning in accounting/auditing research
   and practice is also raising concerns about its potential bias and
   ethical implications. Therefore, this editorial aims to call the
   readers' attention to these issues and encourage scholars to perform
   research in this domain.},
DOI = {10.2308/jeta-10718},
ISSN = {1554-1908},
EISSN = {1558-7940},
Unique-ID = {WOS:000572456900001},
}

@article{ WOS:000522553100014,
Author = {Li, Peng},
Title = {Research on radar signal recognition based on automatic machine learning},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2020},
Volume = {32},
Number = {7, SI},
Pages = {1959-1969},
Month = {APR},
Abstract = {With the advancement of machine learning and radar technology, machine
   learning is becoming more and more widely used in the field of radar.
   Radar scanning, signal acquisition and processing, one-dimensional range
   image, radar SAR, ISAR image recognition, radar tracking and guidance
   are all integrated into machine learning technology, but machine
   learning technology relies heavily on human machine learning experts for
   radar signal recognition. In order to realize the automation of radar
   signal recognition by machine learning, this paper proposes an automatic
   machine learning AUTO-SKLEARN system and applies it to radar radiation
   source signals. Identification: Firstly, this paper briefly introduces
   the classification of traditional machine learning algorithms and the
   types of algorithms specifically included in each type of algorithm. On
   this basis, the machine learning Bayesian algorithm is introduced.
   Secondly, the automatic machine learning AUTO based on Bayesian
   algorithm is proposed. -SKLEARN system, elaborates the process of
   AUTO-SKLEARN system in solving automatic selection algorithm and
   hyperparameter optimization, including meta-learning and its program
   implementation and automatic model integration construction. Finally,
   this paper introduces the process of automatic machine learning applied
   to radar emitter signal recognition. Through data simulation and
   experiment, the effect of traditional machine learning k-means algorithm
   and automatic machine learning AUTO-SKLEARN system in radar signal
   recognition is compared, which shows that automatic machine learning is
   feasible for radar signal recognition. The automatic machine learning
   AUTO-SKLEARN system can significantly improve the accuracy of the radar
   emitter signal recognition process, and the scheme is more reliable in
   signal recognition stability.},
DOI = {10.1007/s00521-019-04494-1},
ISSN = {0941-0643},
EISSN = {1433-3058},
Unique-ID = {WOS:000522553100014},
}

@article{ WOS:000579583300001,
Author = {Dunn, Corey and Moustafa, Nour and Turnbull, Benjamin},
Title = {Robustness Evaluations of Sustainable Machine Learning Models against
   Data Poisoning Attacks in the Internet of Things},
Journal = {SUSTAINABILITY},
Year = {2020},
Volume = {12},
Number = {16},
Month = {AUG},
Abstract = {With the increasing popularity of the Internet of Things (IoT)
   platforms, the cyber security of these platforms is a highly active area
   of research. One key technology underpinning smart IoT systems is
   machine learning, which classifies and predicts events from large-scale
   data in IoT networks. Machine learning is susceptible to cyber attacks,
   particularly data poisoning attacks that inject false data when training
   machine learning models. Data poisoning attacks degrade the performances
   of machine learning models. It is an ongoing research challenge to
   develop trustworthy machine learning models resilient and sustainable
   against data poisoning attacks in IoT networks. We studied the effects
   of data poisoning attacks on machine learning models, including the
   gradient boosting machine, random forest, naive Bayes, and feed-forward
   deep learning, to determine the levels to which the models should be
   trusted and said to be reliable in real-world IoT settings. In the
   training phase, a label modification function is developed to manipulate
   legitimate input classes. The function is employed at data poisoning
   rates of 5\%, 10\%, 20\%, and 30\% that allow the comparison of the
   poisoned models and display their performance degradations. The machine
   learning models have been evaluated using the ToN\_IoT and UNSW NB-15
   datasets, as they include a wide variety of recent legitimate and attack
   vectors. The experimental results revealed that the models' performances
   will be degraded, in terms of accuracy and detection rates, if the
   number of the trained normal observations is not significantly larger
   than the poisoned data. At the rate of data poisoning of 30\% or greater
   on input data, machine learning performances are significantly degraded.},
DOI = {10.3390/su12166434},
Article-Number = {6434},
EISSN = {2071-1050},
ResearcherID-Numbers = {Moustafa, Nour/Z-1160-2018
   Turnbull, Benjamin/IZE-8140-2023},
ORCID-Numbers = {Turnbull, Benjamin/0000-0003-0440-5032
   Moustafa, Nour/0000-0001-6127-9349
   },
Unique-ID = {WOS:000579583300001},
}

@article{ WOS:000722548300001,
Author = {Cui, Zhongmin},
Title = {Machine Learning and Small Data},
Journal = {EDUCATIONAL MEASUREMENT-ISSUES AND PRACTICE},
Year = {2021},
Volume = {40},
Number = {4},
Pages = {8-12},
Month = {DEC},
Abstract = {Commonly used machine learning applications seem to relate to big data.
   This article provides a gentle review of machine learning and shows why
   machine learning can be applied to small data too. An example of
   applying machine learning to screen irregularity reports is presented.
   In the example, the support vector machine and multinomial naive Bayes
   methods were used and compared. The performance of machine learning was
   compared to human experts in terms of flagging records to be excluded
   from equating. The application of machine learning seemed to be
   successful, although the data only consisted of a couple of thousand
   records. Recommendations in using machine learning are provided.},
DOI = {10.1111/emip.12472},
EarlyAccessDate = {NOV 2021},
ISSN = {0731-1745},
EISSN = {1745-3992},
ORCID-Numbers = {Cui, Zhongmin/0000-0003-2426-6762
   chamsou, ilias/0009-0009-4074-1421},
Unique-ID = {WOS:000722548300001},
}

@article{ WOS:000518547500020,
Author = {Bi, Qifang and Goodman, Katherine E. and Kaminsky, Joshua and Lessler,
   Justin},
Title = {What is Machine Learning? A Primer for the Epidemiologist},
Journal = {AMERICAN JOURNAL OF EPIDEMIOLOGY},
Year = {2019},
Volume = {188},
Number = {12},
Pages = {2222-2239},
Month = {DEC},
Abstract = {Machine learning is a branch of computer science that has the potential
   to transform epidemiologic sciences. Amid a growing focus on ``Big
   Data,{''} it offers epidemiologists new tools to tackle problems for
   which classical methods are not well-suited. In order to critically
   evaluate the value of integrating machine learning algorithms and
   existing methods, however, it is essential to address language and
   technical barriers between the two fields that can make it difficult for
   epidemiologists to read and assess machine learning studies. Here, we
   provide an overview of the concepts and terminology used in machine
   learning literature, which encompasses a diverse set of tools with goals
   ranging from prediction to classification to clustering. We provide a
   brief introduction to 5 common machine learning algorithms and 4
   ensemble-based approaches. We then summarize epidemiologic applications
   of machine learning techniques in the published literature. We recommend
   approaches to incorporate machine learning in epidemiologic research and
   discuss opportunities and challenges for integrating machine learning
   and existing epidemiologic research methods.},
DOI = {10.1093/aje/kwz189},
ISSN = {0002-9262},
EISSN = {1476-6256},
Unique-ID = {WOS:000518547500020},
}

@article{ WOS:000660871100001,
Author = {Vanchurin, Vitaly},
Title = {Toward a theory of machine learning},
Journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {2},
Number = {3},
Month = {SEP},
Abstract = {We define a neural network as a septuple consisting of (1) a state
   vector, (2) an input projection, (3) an output projection, (4) a weight
   matrix, (5) a bias vector, (6) an activation map and (7) a loss
   function. We argue that the loss function can be imposed either on the
   boundary (i.e. input and/or output neurons) or in the bulk (i.e. hidden
   neurons) for both supervised and unsupervised systems. We apply the
   principle of maximum entropy to derive a canonical ensemble of the state
   vectors subject to a constraint imposed on the bulk loss function by a
   Lagrange multiplier (or an inverse temperature parameter). We show that
   in an equilibrium the canonical partition function must be a product of
   two factors: a function of the temperature, and a function of the bias
   vector and weight matrix. Consequently, the total Shannon entropy
   consists of two terms which represent, respectively, a thermodynamic
   entropy and a complexity of the neural network. We derive the first and
   second laws of learning: during learning the total entropy must decrease
   until the system reaches an equilibrium (i.e. the second law), and the
   increment in the loss function must be proportional to the increment in
   the thermodynamic entropy plus the increment in the complexity (i.e. the
   first law). We calculate the entropy destruction to show that the
   efficiency of learning is given by the Laplacian of the total free
   energy, which is to be maximized in an optimal neural architecture, and
   explain why the optimization condition is better satisfied in a deep
   network with a large number of hidden layers. The key properties of the
   model are verified numerically by training a supervised feedforward
   neural network using the stochastic gradient descent method. We also
   discuss a possibility that the entire Universe at its most fundamental
   level is a neural network.},
DOI = {10.1088/2632-2153/abe6d7},
Article-Number = {035012},
EISSN = {2632-2153},
ORCID-Numbers = {Vanchurin, Vitaly/0000-0002-9496-9212},
Unique-ID = {WOS:000660871100001},
}

@article{ WOS:000563451300001,
Author = {Wang, Yan and Jia, Peng and Liu, Luping and Huang, Cheng and Liu,
   Zhonglin},
Title = {A systematic review of fuzzing based on machine learning techniques},
Journal = {PLOS ONE},
Year = {2020},
Volume = {15},
Number = {8},
Month = {AUG 18},
Abstract = {Security vulnerabilities play a vital role in network security system.
   Fuzzing technology is widely used as a vulnerability discovery
   technology to reduce damage in advance. However, traditional fuzz
   testing faces many challenges, such as how to mutate input seed files,
   how to increase code coverage, and how to bypass the format verification
   effectively. Therefore machine learning techniques have been introduced
   as a new method into fuzz testing to alleviate these challenges. This
   paper reviews the research progress of using machine learning techniques
   for fuzz testing in recent years, analyzes how machine learning improves
   the fuzzing process and results, and sheds light on future work in
   fuzzing. Firstly, this paper discusses the reasons why machine learning
   techniques can be used for fuzzing scenarios and identifies five
   different stages in which machine learning has been used. Then this
   paper systematically studies machine learning-based fuzzing models from
   five dimensions of selection of machine learning algorithms,
   pre-processing methods, datasets, evaluation metrics, and
   hyperparameters setting. Secondly, this paper assesses the performance
   of the machine learning techniques in existing research for fuzz
   testing. The results of the evaluation prove that machine learning
   techniques have an acceptable capability of prediction for fuzzing.
   Finally, the capability of discovering vulnerabilities both traditional
   fuzzers and machine learning-based fuzzers is analyzed. The results
   depict that the introduction of machine learning techniques can improve
   the performance of fuzzing. We hope to provide researchers with a
   systematic and more in-depth understanding of fuzzing based on machine
   learning techniques and provide some references for this field through
   analysis and summarization of multiple dimensions.},
DOI = {10.1371/journal.pone.0237749},
Article-Number = {e0237749},
ISSN = {1932-6203},
ResearcherID-Numbers = {Zhonglin, Liu/ABP-1204-2022
   LIU, LUPING/T-7934-2019},
ORCID-Numbers = {Liu, Zhonglin/0000-0002-3239-5647
   },
Unique-ID = {WOS:000563451300001},
}

@article{ WOS:000600895900004,
Author = {Sun, Shanwen and Wang, Chunyu and Ding, Hui and Zou, Quan},
Title = {Machine learning and its applications in plant molecular studies},
Journal = {BRIEFINGS IN FUNCTIONAL GENOMICS},
Year = {2020},
Volume = {19},
Number = {1},
Pages = {40-48},
Month = {JAN},
Abstract = {The advent of high-throughput genomic technologies has resulted in the
   accumulation of massive amounts of genomic information. However,
   biologists are challenged with how to effectively analyze these data.
   Machine learning can provide tools for better and more efficient data
   analysis. Unfortunately, because many plant biologists are unfamiliar
   with machine learning, its application in plant molecular studies has
   been restricted to a few species and a limited set of algorithms. Thus,
   in this study, we provide the basic steps for developing machine
   learning frameworks and present a comprehensive overview of machine
   learning algorithms and various evaluation metrics. Furthermore, we
   introduce sources of important curated plant genomic data and R packages
   to enable plant biologists to easily and quickly apply appropriate
   machine learning algorithms in their research. Finally, we discuss
   current applications of machine learning algorithms for identifying
   various genes related to resistance to biotic and abiotic stress. Broad
   application of machine learning and the accumulation of plant sequencing
   data will advance plant molecular studies.},
DOI = {10.1093/bfgp/elz036},
ISSN = {2041-2649},
EISSN = {2041-2657},
ResearcherID-Numbers = {Zou, Quan/A-7801-2015
   Wang, Chunyu/AAN-5766-2020
   },
ORCID-Numbers = {Wang, Chunyu/0000-0002-2965-9920
   Sun, Shanwen/0000-0003-4358-8636},
Unique-ID = {WOS:000600895900004},
}

@article{ WOS:000577397300001,
Author = {Rashid, Mamoon and Singh, Harjeet and Goyal, Vishal},
Title = {The use of machine learning and deep learning algorithms in functional
   magnetic resonance imaging-A systematic review},
Journal = {EXPERT SYSTEMS},
Year = {2020},
Volume = {37},
Number = {6, SI},
Month = {DEC},
Abstract = {Functional Magnetic Resonance Imaging (fMRI) is presently one of the
   most popular techniques for analysing the dynamic states in brain images
   using various kinds of algorithms. From the last decade, there is an
   exponential rise in the use of the machine and deep learning algorithms
   of artificial intelligence for analysing fMRI data. However, it is a big
   challenge for every researcher to choose a suitable machine or deep
   learning algorithm for analysing fMRI data due to the availability of a
   large number of algorithms in the literature. It takes much time for
   each researcher to know about the various approaches and algorithms
   which are in use for fMRI data. This paper provides a review in a
   systematic manner for the present literature of fMRI data that makes use
   of the machine and deep learning algorithms. The major goals of this
   review paper are to (a) identify machine learning and deep learning
   research trends for the implementation of fMRI; (b) identify usage of
   Machine Learning Algorithms and deep learning in fMRI, and (c) help new
   researchers based on fMRI to put their new findings appropriately in
   existing domain of fMRI research. The results of this systematic review
   identified various fMRI studies and classified them based on fMRI types,
   mental diseases, use of machine learning and deep learning algorithms.
   The authors have provided the studies with the best performance of
   machine learning and deep learning algorithms used in fMRI. The authors
   believe that this systematic review will help incoming researchers on
   fMRI in their future works.},
DOI = {10.1111/exsy.12644},
EarlyAccessDate = {OCT 2020},
Article-Number = {e12644},
ISSN = {0266-4720},
EISSN = {1468-0394},
ResearcherID-Numbers = {Goyal, Vishal/I-5538-2015
   Rashid, Mamoon/AAB-7135-2020
   },
ORCID-Numbers = {Rashid, Dr. Mamoon/0000-0002-8302-4571
   Goyal, Vishal/0000-0003-2269-7606},
Unique-ID = {WOS:000577397300001},
}

@article{ WOS:000437004000009,
Author = {Camacho, Diogo M. and Collins, Katherine M. and Powers, Rani K. and
   Costello, James C. and Collins, James J.},
Title = {Next-Generation Machine Learning for Biological Networks},
Journal = {CELL},
Year = {2018},
Volume = {173},
Number = {7},
Pages = {1581-1592},
Month = {JUN 14},
Abstract = {Machine learning, a collection of data-analytical techniques aimed at
   building predictive models from multi-dimensional datasets, is becoming
   integral to modern biological research. By enabling one to generate
   models that learn from large datasets and make predictions on likely
   outcomes, machine learning can be used to study complex cellular systems
   such as biological networks. Here, we provide a primer on machine
   learning for life scientists, including an introduction to deep
   learning. We discuss opportunities and challenges at the intersection of
   machine learning and network biology, which could impact disease
   biology, drug discovery, microbiome research, and synthetic biology.},
DOI = {10.1016/j.cell.2018.05.015},
ISSN = {0092-8674},
EISSN = {1097-4172},
ResearcherID-Numbers = {Collins, J.J./E-5574-2014
   Camacho, Diogo/A-5623-2010
   },
ORCID-Numbers = {Camacho, Diogo/0000-0002-1971-3524
   Costello, James/0000-0003-3158-9682
   Powers, Rani/0000-0001-9501-2589},
Unique-ID = {WOS:000437004000009},
}

@article{ WOS:000641958000001,
Author = {Nahavandi, Saeid},
Title = {Applications for Machine Learning},
Journal = {IEEE SYSTEMS MAN AND CYBERNETICS MAGAZINE},
Year = {2021},
Volume = {7},
Number = {2},
Pages = {3},
Month = {APR},
Abstract = {In this issue of IEEE Systems, Man, and Cybernetics Magazine, four
   articles are presented that relate to the fascinating topic of machine
   learning and its application for real-world systems.},
DOI = {10.1109/MSMC.2021.3058718},
ISSN = {2380-1298},
EISSN = {2333-942X},
ResearcherID-Numbers = {Nahavandi, Saeid/AAE-5536-2022},
Unique-ID = {WOS:000641958000001},
}

@article{ WOS:000691881100022,
Author = {Alridha, Ahmed and Wahbi, Fadhil Abdalhasan and Kadhim, Mazin Kareem},
Title = {Training analysis of optimization models in machine learning},
Journal = {INTERNATIONAL JOURNAL OF NONLINEAR ANALYSIS AND APPLICATIONS},
Year = {2021},
Volume = {12},
Number = {2},
Pages = {1453-1461},
Abstract = {Machine learning is fast evolving, with numerous theoretical advances
   and applications in a variety of domains. In reality, most machine
   learning algorithms are based on optimization issues. This interaction
   is also explored in the special topic on machine learning and
   large-scale optimization. Furthermore, machine learning optimization
   issues have several unique characteristics that are rarely seen in other
   optimization contexts. Aside from that, the notions of classical
   optimization vs machine learning will be discussed. Finally, this study
   will give an outline of these peculiar aspects of machine learning
   optimization.},
DOI = {10.22075/ijnaa.2021.5261},
ISSN = {2008-6822},
ResearcherID-Numbers = {ALRIDHA, Ahmed/GXG-0692-2022
   },
ORCID-Numbers = {Hasan ALRIDHA, Ahmed/0000-0002-2192-5251
   Marjan, Mazen/0000-0002-5283-8716},
Unique-ID = {WOS:000691881100022},
}

@article{ WOS:000453280300005,
Author = {Komura, Daisuke and Ishikawa, Shumpei},
Title = {Machine Learning Methods for Histopathological Image Analysis},
Journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL},
Year = {2018},
Volume = {16},
Pages = {34-42},
Abstract = {Abundant accumulation of digital histopathological images has led to the
   increased demand for their analysis, such as computer-aided diagnosis
   using machine learning techniques. However, digital pathological images
   and related tasks have some issues to be considered. In this
   mini-review, we introduce the application of digital pathological image
   analysis using machine learning algorithms, address some problems
   specific to suchanalysis, and propose possible solutions. (C) 2018
   Komura, Ishikawa. Published by Elsevier B.V. on behalf of the Research
   Network of Computational and Structural Biotechnology.},
DOI = {10.1016/j.csbj.2018.01.001},
ISSN = {2001-0370},
ResearcherID-Numbers = {Ishikawa, Shumpei/NBW-9562-2025
   Komura, Daisuke/AAF-2481-2019},
ORCID-Numbers = {Komura, Daisuke/0000-0002-0038-728X
   },
Unique-ID = {WOS:000453280300005},
}

@article{ WOS:000559782300004,
Author = {Gao, Kaifeng and Mei, Gang and Piccialli, Francesco and Cuomo, Salvatore
   and Tu, Jingzhi and Huo, Zenan},
Title = {Julia language in machine learning: Algorithms, applications, and open
   issues},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2020},
Volume = {37},
Month = {AUG},
Abstract = {Machine learning is driving development across many fields in science
   and engineering. A simple and efficient programming language could
   accelerate applications of machine learning in various fields.
   Currently, the programming languages most commonly used to develop
   machine learning algorithms include Python, MATLAB, and C/C ++. However,
   none of these languages well balance both efficiency and simplicity. The
   Julia language is a fast, easy-to-use, and open-source programming
   language that was originally designed for high-performance computing,
   which can well balance the efficiency and simplicity. This paper
   summarizes the related research work and developments in the
   applications of the Julia language in machine learning. It first surveys
   the popular machine learning algorithms that are developed in the Julia
   language. Then, it investigates applications of the machine learning
   algorithms implemented with the Julia language. Finally, it discusses
   the open issues and the potential future directions that arise in the
   use of the Julia language in machine learning. (c) 2020 The Authors.
   Published by Elsevier Inc.},
DOI = {10.1016/j.cosrev.2020.100254},
Article-Number = {100254},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {Piccialli, Francesco/ABC-2457-2020
   Mei, Gang/C-9124-2016
   Cuomo, Salvatore/Q-1365-2016
   },
ORCID-Numbers = {Huo, Zenan/0000-0003-3750-1709
   Mei, Gang/0000-0003-0026-5423
   Cuomo, Salvatore/0000-0003-4128-2588
   Kaifeng, Gao/0000-0003-0181-6602},
Unique-ID = {WOS:000559782300004},
}

@article{ WOS:000489358200007,
Author = {Garcia-Martin, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and
   Grahn, Hakan},
Title = {Estimation of energy consumption in machine learning},
Journal = {JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING},
Year = {2019},
Volume = {134},
Pages = {75-88},
Month = {DEC},
Abstract = {Energy consumption has been widely studied in the computer architecture
   field for decades. While the adoption of energy as a metric in machine
   learning is emerging, the majority of research is still primarily
   focused on obtaining high levels of accuracy without any computational
   constraint. We believe that one of the reasons for this lack of interest
   is due to their lack of familiarity with approaches to evaluate energy
   consumption. To address this challenge, we present a review of the
   different approaches to estimate energy consumption in general and
   machine learning applications in particular. Our goal is to provide
   useful guidelines to the machine learning community giving them the
   fundamental knowledge to use and build specific energy estimation
   methods for machine learning algorithms. We also present the latest
   software tools that give energy estimation values, together with two use
   cases that enhance the study of energy consumption in machine learning.
   (C) 2019 The Authors. Published by Elsevier Inc.},
DOI = {10.1016/j.jpdc.2019.07.007},
ISSN = {0743-7315},
EISSN = {1096-0848},
ResearcherID-Numbers = {Grahn, Håkan/G-9720-2011
   },
ORCID-Numbers = {RODRIGUES, CREFEDA/0000-0001-7401-6265
   Grahn, Hakan/0000-0001-9947-1088},
Unique-ID = {WOS:000489358200007},
}

@article{ WOS:000595025900001,
Author = {Saini, Shivani and Khosla, P. K. and Kaur, Manjit and Singh, Gurmohan},
Title = {Quantum Driven Machine Learning},
Journal = {INTERNATIONAL JOURNAL OF THEORETICAL PHYSICS},
Year = {2020},
Volume = {59},
Number = {12},
Pages = {4013-4024},
Month = {DEC},
Abstract = {Quantum computing is proving to be very beneficial for solving complex
   machine learning problems. Quantum computers are inherently excellent in
   handling and manipulating vectors and matrix operations. The ever
   increasing size of data has started creating bottlenecks for classical
   machine learning systems. Quantum computers are emerging as potential
   solutions to tackle big data related problems. This paper presents a
   quantum machine learning model based on quantum support vector machine
   (QSVM) algorithm to solve a classification problem. The quantum machine
   learning model is practically implemented on quantum simulators and
   real-time superconducting quantum processors. The performance of quantum
   machine learning model is computed in terms of processing speed and
   accuracy and compared against its classical counterpart. The breast
   cancer dataset is used for the classification problem. The results are
   indicative that quantum computers offer quantum speed-up.},
DOI = {10.1007/s10773-020-04656-1},
EarlyAccessDate = {DEC 2020},
ISSN = {0020-7748},
EISSN = {1572-9575},
ResearcherID-Numbers = {Kaur, Manjit/MBF-0295-2025
   Singh, Gurmohan/AFG-3279-2022
   Saini, Shivani/M-8258-2017
   Khosla, Praveen/OBN-6991-2025
   Singh, Gurmohan/F-7425-2017},
ORCID-Numbers = {Kaur, Manjit/0000-0002-2406-7694
   KHOSLA, PRAVEEN/0000-0002-0509-2018
   Kaur, Manjit/0000-0002-3440-2767
   Singh, Gurmohan/0000-0003-2955-9505},
Unique-ID = {WOS:000595025900001},
}

@article{ WOS:000848787000001,
Author = {Gilpin, William and Huang, Yitong and Forger, Daniel B.},
Title = {Learning dynamics from large biological data sets: Machine learning
   meets systems biology},
Journal = {CURRENT OPINION IN SYSTEMS BIOLOGY},
Year = {2020},
Volume = {22},
Pages = {1-7},
Month = {AUG},
Abstract = {In the past few decades, mathematical models based on dynamical systems
   theory have provided new insight into diverse biological systems. In
   this review, we ask whether the recent success of machine learning
   techniques for large-scale biological data analysis provides a
   complementary or competing approach to more traditional modeling
   approaches. Recent applications of machine learning to the problem of
   learning biological dynamics in diverse systems range from neuroscience
   to animal behavior. We compare the underlying mechanisms and limitations
   of traditional dynamical models with those of machine learning models.
   We highlight the unique role that traditional modeling has played in
   providing predictive insights into biological systems, and we propose
   several avenues for bridging traditional dynamical systems theory with
   large-scale analysis enabled by machine learning.},
DOI = {10.1016/j.coisb.2020.07.009},
ISSN = {2452-3100},
ResearcherID-Numbers = {Forger, Daniel/C-5552-2015
   Gilpin, W/KII-8157-2024
   Huang, Yitong/AAY-4095-2021
   },
ORCID-Numbers = {Huang, Yitong/0000-0002-5200-8077},
Unique-ID = {WOS:000848787000001},
}

@article{ WOS:000468930900005,
Author = {Hao, Jiangang and Ho, Tin Kam},
Title = {Machine Learning Made Easy: A Review of Scikit-learn Package in Python
   Programming Language},
Journal = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS},
Year = {2019},
Volume = {44},
Number = {3},
Pages = {348-361},
Month = {JUN},
Abstract = {Machine learning is a popular topic in data analysis and modeling. Many
   different machine learning algorithms have been developed and
   implemented in a variety of programming languages over the past 20
   years. In this article, we first provide an overview of machine learning
   and clarify its difference from statistical inference. Then, we review
   Scikit-learn, a machine learning package in the Python programming
   language that is widely used in data science. The Scikit-learn package
   includes implementations of a comprehensive list of machine learning
   methods under unified data and modeling procedure conventions, making it
   a convenient toolkit for educational and behavior statisticians.},
DOI = {10.3102/1076998619832248},
ISSN = {1076-9986},
EISSN = {1935-1054},
ResearcherID-Numbers = {Hao, Jiangang/G-3954-2011},
ORCID-Numbers = {Hao, Jiangang/0000-0003-0502-7571
   },
Unique-ID = {WOS:000468930900005},
}

@article{ WOS:000543431000001,
Author = {Clement, Conrad L. and Kauwe, Steven K. and Sparks, Taylor D.},
Title = {Benchmark AFLOW Data Sets for Machine Learning},
Journal = {INTEGRATING MATERIALS AND MANUFACTURING INNOVATION},
Year = {2020},
Volume = {9},
Number = {2},
Pages = {153-156},
Month = {JUN},
Abstract = {Materials informatics is increasingly finding ways to exploit machine
   learning algorithms. Techniques such as decision trees, ensemble
   methods, support vector machines, and a variety of neural network
   architectures are used to predict likely material characteristics and
   property values. Supplemented with laboratory synthesis, applications of
   machine learning to compound discovery and characterization represent
   one of the most promising research directions in materials informatics.
   A shortcoming of this trend, in its current form, is a lack of
   standardized materials data sets on which to train, validate, and test
   model effectiveness. Applied machine learning research depends on
   benchmark data to make sense of its results. Fixed, predetermined data
   sets allow for rigorous model assessment and comparison. Machine
   learning publications that do not refer to benchmarks are often hard to
   contextualize and reproduce. In this data descriptor article, we present
   a collection of data sets of different material properties taken from
   the AFLOW database. We describe them, the procedures that generated
   them, and their use as potential benchmarks. We provide a compressed ZIP
   file containing the data sets and a GitHub repository of associated
   Python code. Finally, we discuss opportunities for future work
   incorporating the data sets and creating similar benchmark collections.},
DOI = {10.1007/s40192-020-00174-4},
ISSN = {2193-9764},
EISSN = {2193-9772},
ResearcherID-Numbers = {Sparks, Taylor/I-4927-2019},
ORCID-Numbers = {Sparks, Taylor/0000-0001-8020-7711},
Unique-ID = {WOS:000543431000001},
}

@article{ WOS:000591678300009,
Author = {Maehashi, Kohei and Shintani, Mototsugu},
Title = {Macroeconomic forecasting using factor models and machine learning: an
   application to Japan},
Journal = {JOURNAL OF THE JAPANESE AND INTERNATIONAL ECONOMIES},
Year = {2020},
Volume = {58},
Month = {DEC},
Abstract = {We perform a thorough comparative analysis of factor models and machine
   learning to forecast Japanese macroeconomic time series. Our main
   results can be summarized as follows. First, in many instances, factor
   models and machine learning perform better than the conventional AR
   model. Second, predictions made by machine learning methods perform
   particularly well for medium to long forecast horizons. Third, the
   success of machine learning mainly comes from the nonlinearity and
   interaction of variables, which suggests the importance of nonlinear
   structure in predicting the Japanese macroeconomic series. Fourth, the
   composite forecast of factor models and machine learning performs better
   than factor models or machine learning alone; and machine learning
   methods applied to common factors are found to be useful in the
   composite forecast.},
DOI = {10.1016/j.jjie.2020.101104},
Article-Number = {101104},
ISSN = {0889-1583},
EISSN = {1095-8681},
Unique-ID = {WOS:000591678300009},
}

@article{ WOS:000895964000001,
Author = {Lu, Mei and Li, Fanzhang},
Title = {Survey on Lie Group Machine Learning},
Journal = {BIG DATA MINING AND ANALYTICS},
Year = {2020},
Volume = {3},
Number = {4},
Pages = {235-258},
Month = {DEC},
Abstract = {Lie group machine learning is recognized as the theoretical basis of
   brain intelligence, brain learning, higher machine learning, and higher
   artificial intelligence. Sample sets of Lie group matrices are widely
   available in practical applications. Lie group learning is a vibrant
   field of increasing importance and extraordinary potential and thus
   needs to be developed further. This study aims to provide a
   comprehensive survey on recent advances in Lie group machine learning.
   We introduce Lie group machine learning techniques in three major
   categories: supervised Lie group machine learning, semisupervised Lie
   group machine learning, and unsupervised Lie group machine learning. In
   addition, we introduce the special application of Lie group machine
   learning in image processing. This work covers the following techniques:
   Lie group machine learning model, Lie group subspace orbit generation
   learning, symplectic group learning, quantum group learning, Lie group
   fiber bundle learning, Lie group cover learning, Lie group deep
   structure learning, Lie group semisupervised learning, Lie group kernel
   learning, tensor learning, frame bundle connection learning, spectral
   estimation learning, Finsler geometric learning, homology boundary
   learning, category representation learning, and neuromorphic synergy
   learning. Overall, this survey aims to provide an insightful overview of
   state-of-the-art development in the field of Lie group machine learning.
   It will enable researchers to comprehensively understand the state of
   the field, identify the most appropriate tools for particular
   applications, and identify directions for future research.},
DOI = {10.26599/BDMA.2020.9020011},
EISSN = {2096-0654},
ResearcherID-Numbers = {Li, Zhang/C-4794-2014},
Unique-ID = {WOS:000895964000001},
}

@article{ WOS:000456754100058,
Author = {Rehman, Tanzeel U. and Mahmud, Md Sultan and Chang, Young K. and Jin,
   Jian and Shin, Jaemyung},
Title = {Current and future applications of statistical machine learning
   algorithms for agricultural machine vision systems},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2019},
Volume = {156},
Pages = {585-605},
Month = {JAN},
Abstract = {With being rapid increasing population in worldwide, the need for
   satisfactory level of crop production with decreased amount of
   agricultural lands. Machine vision would ensure the increase of crop
   production by using an automated, non-destructive and cost-effective
   technique. In last few years, remarkable results have been achieved in
   different sectors of agriculture. These achievements are integrated with
   machine learning techniques on machine vision approach that cope with
   colour, shape, texture and spectral analysis from the image of objects.
   Despite having many applications of different machine learning
   techniques, this review only described the statistical machine learning
   technologies with machine vision systems in agriculture due to broad
   area of machine learning applications. Two types of statistical machine
   learning techniques such as supervised and unsupervised learning have
   been utilized for agriculture. This paper comprehensively surveyed
   current application of statistical machine learning techniques in
   machine vision systems, analyses each technique potential for specific
   application and represents an overview of instructive examples in
   different agricultural areas. Suggestions of specific statistical
   machine learning technique for specific purpose and limitations of each
   technique are also given. Future trends of statistical machine learning
   technology applications are discussed.},
DOI = {10.1016/j.compag.2018.12.006},
ISSN = {0168-1699},
EISSN = {1872-7107},
ResearcherID-Numbers = {Mahmud, Md Sultan/AAO-8823-2021},
ORCID-Numbers = {Mahmud, Md Sultan/0000-0002-8141-105X
   Rehman, Tanzeel/0000-0002-9322-223X
   Chang, Young/0000-0003-2752-1474
   },
Unique-ID = {WOS:000456754100058},
}

@article{ WOS:000491213400062,
Author = {Demolli, Halil and Dokuz, Ahmet Sakir and Ecemis, Alper and Gokcek,
   Murat},
Title = {Wind power forecasting based on daily wind speed data using machine
   learning algorithms},
Journal = {ENERGY CONVERSION AND MANAGEMENT},
Year = {2019},
Volume = {198},
Month = {OCT 15},
Abstract = {Wind energy is a significant and eligible source that has the potential
   for producing energy in a continuous and sustainable manner among
   renewable energy sources. However, wind energy has several challenges,
   such as initial investment costs, the stationary property of wind
   plants, and the difficulty in finding wind-efficient energy areas. In
   this study, long-term wind power forecasting was performed based on
   daily wind speed data using five machine learning algorithms. We
   proposed a method based on machine learning algorithms to forecast wind
   power values efficiently. We conducted several case studies to reveal
   performances of machine learning algorithms. The results showed that
   machine learning algorithms could be used for forecasting long-term wind
   power values with respect to historical wind speed data. Furthermore,
   the results showed that machine learning-based models could be applied
   to a location different from model-trained locations. This study
   demonstrated that machine learning algorithms could be successfully used
   before the establishment of wind plants in an unknown geographical
   location whether it is logical by using the model of a base location.},
DOI = {10.1016/j.enconman.2019.111823},
Article-Number = {111823},
ISSN = {0196-8904},
EISSN = {1879-2227},
ResearcherID-Numbers = {Demolli, Halil/AAE-1799-2020
   Gokcek, Murat/M-6787-2019
   },
ORCID-Numbers = {Gokcek, Murat/0000-0002-7951-4236
   Demolli, Halil/0000-0001-6474-3549},
Unique-ID = {WOS:000491213400062},
}

@article{ WOS:000464886100002,
Author = {Xu, Chunming and Jackson, Scott A.},
Title = {Machine learning and complex biological data},
Journal = {GENOME BIOLOGY},
Year = {2019},
Volume = {20},
Month = {APR 16},
Abstract = {Machine learning has demonstrated potential in analyzing large, complex
   biological data. In practice, however, biological information is
   required in addition to machine learning for successful application.},
DOI = {10.1186/s13059-019-1689-0},
Article-Number = {76},
ISSN = {1474-760X},
ORCID-Numbers = {jackson, scott/0000-0002-3172-1607
   Xu, Chunming/0000-0002-0986-0993},
Unique-ID = {WOS:000464886100002},
}

@article{ WOS:000484866500001,
Author = {Abbasi, Bardia and Goldenholz, Daniel M.},
Title = {Machine learning applications in epilepsy},
Journal = {EPILEPSIA},
Year = {2019},
Volume = {60},
Number = {10},
Pages = {2037-2047},
Month = {OCT},
Abstract = {Machine learning leverages statistical and computer science principles
   to develop algorithms capable of improving performance through
   interpretation of data rather than through explicit instructions.
   Alongside widespread use in image recognition, language processing, and
   data mining, machine learning techniques have received increasing
   attention in medical applications, ranging from automated imaging
   analysis to disease forecasting. This review examines the parallel
   progress made in epilepsy, highlighting applications in automated
   seizure detection from electroencephalography (EEG), video, and kinetic
   data, automated imaging analysis and pre-surgical planning, prediction
   of medication response, and prediction of medical and surgical outcomes
   using a wide variety of data sources. A brief overview of commonly used
   machine learning approaches, as well as challenges in further
   application of machine learning techniques in epilepsy, is also
   presented. With increasing computational capabilities, availability of
   effective machine learning algorithms, and accumulation of larger
   datasets, clinicians and researchers will increasingly benefit from
   familiarity with these techniques and the significant progress already
   made in their application in epilepsy.},
DOI = {10.1111/epi.16333},
EarlyAccessDate = {SEP 2019},
ISSN = {0013-9580},
EISSN = {1528-1167},
ResearcherID-Numbers = {Goldenholz, Daniel/AEN-6620-2022
   },
ORCID-Numbers = {Goldenholz, Daniel/0000-0002-8370-2758},
Unique-ID = {WOS:000484866500001},
}

@article{ WOS:000398133500010,
Author = {Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and
   Kline, Timothy L.},
Title = {Machine Learning for Medical Imaging1},
Journal = {RADIOGRAPHICS},
Year = {2017},
Volume = {37},
Number = {2},
Pages = {505-515},
Month = {MAR-APR},
Abstract = {Machine learning is a technique for recognizing patterns that can be
   applied to medical images. Although it is a powerful tool that can help
   in rendering medical diagnoses, it can be misapplied. Machine learning
   typically begins with the machine learning algorithm system computing
   the image features that are believed to be of importance in making the
   prediction or diagnosis of interest. The machine learning algorithm
   system then identifies the best combination of these image features for
   classifying the image or computing some metric for the given image
   region. There are several methods that can be used, each with different
   strengths and weaknesses. There are open-source versions of most of
   these machine learning methods that make them easy to try and apply to
   images. Several metrics for measuring the performance of an algorithm
   exist; however, one must be aware of the possible associated pitfalls
   that can result in misleading metrics. More recently, deep learning has
   started to be used; this method has the benefit that it does not require
   image feature identification and calculation as a first step; rather,
   features are identified as part of the learning process. Machine
   learning has been used in medical imaging and will have a greater
   influence in the future. Those working in medical imaging must be aware
   of how machine learning works.},
DOI = {10.1148/rg.2017160130},
ISSN = {0271-5333},
ORCID-Numbers = {AKKUS, Zeynettin/0000-0003-3920-1515},
Unique-ID = {WOS:000398133500010},
}

@article{ WOS:000473818300119,
Author = {Dada, Emmanuel Gbenga and Bassi, Joseph Stephen and Chiroma, Haruna and
   Abdulhamid, Shafi'i Muhammad and Adetunmbi, Adebayo Olusola and Ajibuwa,
   Opeyemi Emmanuel},
Title = {Machine learning for email spam filtering: review, approaches and open
   research problems},
Journal = {HELIYON},
Year = {2019},
Volume = {5},
Number = {6},
Month = {JUN},
Abstract = {The upsurge in the volume of unwanted emails called spam has created an
   intense need for the development of more dependable and robust antispam
   fillers. Machine learning methods of recent are being used to
   successfully detect and filter spam emails. We present a systematic
   review of some of the popular machine learning based email spam
   filtering approaches. Our review covers survey of the important
   concepts, attempts, efficiency, and the research trend in spam
   filtering. The preliminary discussion in the study background examines
   the applications of machine learning techniques to the email spam
   filtering process of the leading internet service providers (ISPs) like
   Gmail, Yahoo and Outlook emails spam fillers. Discussion on general
   email spam filtering process, and the various efforts by different
   researchers in combating spam through the use machine learning
   techniques was done. Our review compares the strengths and drawbacks of
   existing machine learning approaches and the open research problems in
   spam filtering. We recommended deep leaning and deep adversarial
   learning as the future techniques that can effectively handle the menace
   of spam emails.},
DOI = {10.1016/j.heliyon.2019.e01802},
Article-Number = {e01802},
EISSN = {2405-8440},
ResearcherID-Numbers = {Abdulhamid, Shafi'i/J-4288-2016
   Ajibuwa, Opeyemi/HTR-8877-2023
   Adetunmbi, Adebayo/ABA-4705-2020
   Haruna, Ph.D/O-2934-2013
   Joseph, Stephen/AAW-1183-2020
   ABDULHAMID, Shafi'i Muhammad/J-4288-2016
   DADA, EMMANUEL/AAV-2728-2021},
ORCID-Numbers = {Ajibuwa, Opeyemi/0009-0007-5956-0925
   DADA, EMMANUEL GBENGA/0000-0002-1132-5447
   Joseph, Stephen/0000-0001-5050-0132
   ABDULHAMID, Shafi'i Muhammad/0000-0001-9196-9447
   },
Unique-ID = {WOS:000473818300119},
}

@article{ WOS:000542536700007,
Author = {Benton, William C.},
Title = {Machine Learning Systems and Intelligent Applications},
Journal = {IEEE SOFTWARE},
Year = {2020},
Volume = {37},
Number = {4},
Pages = {43-49},
Month = {JUL-AUG},
Abstract = {Machine learning techniques are useful in a wide range of contexts, but
   techniques alone are insufficient to solve real business problems. We
   introduce the intelligent applications concept, which characterizes the
   structure and responsibilities of contemporary machine learning systems.},
DOI = {10.1109/MS.2020.2985224},
ISSN = {0740-7459},
EISSN = {1937-4194},
ORCID-Numbers = {Benton, William/0000-0001-6295-5521},
Unique-ID = {WOS:000542536700007},
}

@article{ WOS:000485253300001,
Author = {Rashidi, Hooman H. and Tran, Nam K. and Betts, Elham Vali and Howell,
   Lydia P. and Green, Ralph},
Title = {Artificial Intelligence and Machine Learning in Pathology: The Present
   Landscape of Supervised Methods},
Journal = {ACADEMIC PATHOLOGY},
Year = {2019},
Volume = {6},
Month = {SEP 3},
Abstract = {Increased interest in the opportunities provided by artificial
   intelligence and machine learning has spawned a new field of health-care
   research. The new tools under development are targeting many aspects of
   medical practice, including changes to the practice of pathology and
   laboratory medicine. Optimal design in these powerful tools requires
   cross-disciplinary literacy, including basic knowledge and understanding
   of critical concepts that have traditionally been unfamiliar to
   pathologists and laboratorians. This review provides definitions and
   basic knowledge of machine learning categories (supervised,
   unsupervised, and reinforcement learning), introduces the underlying
   concept of the bias-variance trade-off as an important foundation in
   supervised machine learning, and discusses approaches to the supervised
   machine learning study design along with an overview and description of
   common supervised machine learning algorithms (linear regression,
   logistic regression, Naive Bayes, k-nearest neighbor, support vector
   machine, random forest, convolutional neural networks).},
DOI = {10.1177/2374289519873088},
Article-Number = {2374289519873088},
ISSN = {2374-2895},
ResearcherID-Numbers = {Tran, Nam/OEN-4952-2025
   },
ORCID-Numbers = {Tran, Nam/0000-0003-1565-0025},
Unique-ID = {WOS:000485253300001},
}

@article{ WOS:000470026000001,
Author = {Gong, Zhiqiang and Zhong, Ping and Hu, Weidong},
Title = {Diversity in Machine Learning},
Journal = {IEEE ACCESS},
Year = {2019},
Volume = {7},
Pages = {64323-64350},
Abstract = {Machine learning methods have achieved good performance and been widely
   applied in various real-world applications. They can learn the model
   adaptively and be better fit for special requirements of different
   tasks. Generally, a good machine learning system is composed of
   plentiful training data, a good model training process, and an accurate
   inference. Many factors can affect the performance of the machine
   learning process, among which the diversity of the machine learning
   process is an important one. The diversity can help each procedure to
   guarantee a totally good machine learning: diversity of the training
   data ensures that the training data can provide more discriminative
   information for the model, diversity of the learned model (diversity in
   parameters of each model or diversity among different base models) makes
   each parameter/model capture unique or complement information and the
   diversity in inference can provide multiple choices each of which
   corresponds to a specific plausible local optimal result. Even though
   diversity plays an important role in the machine learning process, there
   is no systematical analysis of the diversification in the machine
   learning system. In this paper, we systematically summarize the methods
   to make data diversification, model diversification, and inference
   diversification in the machine learning process. In addition, the
   typical applications where the diversity technology improved the machine
   learning performance have been surveyed including the remote sensing
   imaging tasks, machine translation, camera relocalization, image
   segmentation, object detection, topic modeling, and others. Finally, we
   discuss some challenges of the diversity technology in machine learning
   and point out some directions in future work. Our analysis provides a
   deeper understanding of the diversity technology in machine learning
   tasks and hence can help design and learn more effective models for
   real-world applications.},
DOI = {10.1109/ACCESS.2019.2917620},
ISSN = {2169-3536},
Unique-ID = {WOS:000470026000001},
}

@article{ WOS:000497715600006,
Author = {Pitropakis, Nikolaos and Panaousis, Emmanouil and Giannetsos, Thanassis
   and Anastasiadis, Eleftherios and Loukas, George},
Title = {A taxonomy and survey of attacks against machine learning},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2019},
Volume = {34},
Month = {NOV},
Abstract = {The majority of machine learning methodologies operate with the
   assumption that their environment is benign. However, this assumption
   does not always hold, as it is often advantageous to adversaries to
   maliciously modify the training (poisoning attacks) or test data
   (evasion attacks). Such attacks can be catastrophic given the growth and
   the penetration of machine learning applications in society. Therefore,
   there is a need to secure machine learning enabling the safe adoption of
   it in adversarial cases, such as spam filtering, malware detection, and
   biometric recognition. This paper presents a taxonomy and survey of
   attacks against systems that use machine learning. It organizes the body
   of knowledge in adversarial machine learning so as to identify the
   aspects where researchers from different fields can contribute to. The
   taxonomy identifies attacks which share key characteristics and as such
   can potentially be addressed by the same defence approaches. Thus, the
   proposed taxonomy makes it easier to understand the existing attack
   landscape towards developing defence mechanisms, which are not
   investigated in this survey. The taxonomy is also leveraged to identify
   open problems that can lead to new research areas within the field of
   adversarial machine learning. (C) 2019 Elsevier Inc. All rights
   reserved.},
DOI = {10.1016/j.cosrev.2019.100199},
Article-Number = {100199},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {Panaousis, Emmanouil/A-1444-2016
   Pitropakis, Nikolaos/ACW-7211-2022
   Loukas, George/AAC-4291-2021},
ORCID-Numbers = {Panaousis, Manos/0000-0001-7306-4062
   Pitropakis, Nikolaos/0000-0002-3392-9970
   Loukas, George/0000-0003-3559-5182
   },
Unique-ID = {WOS:000497715600006},
}

@article{ WOS:000465199900003,
Author = {Chen, Po-Hsuan Cameron and Liu, Yun and Peng, Lily},
Title = {How to develop machine learning models for healthcare},
Journal = {NATURE MATERIALS},
Year = {2019},
Volume = {18},
Number = {5},
Pages = {410-414},
Month = {MAY},
Abstract = {Rapid progress in machine learning is enabling opportunities for
   improved clinical decision support. Importantly, however, developing,
   validating and implementing machine learning models for healthcare
   entail some particular considerations to increase the chances of
   eventually improving patient care.},
DOI = {10.1038/s41563-019-0345-0},
ISSN = {1476-1122},
EISSN = {1476-4660},
ORCID-Numbers = {Chen, Po-Hsuan Cameron/0000-0002-0083-4991
   Liu, Yun/0000-0003-4079-8275},
Unique-ID = {WOS:000465199900003},
}

@article{ WOS:000447085500015,
Author = {Zhao, Rui and Yan, Ruqiang and Chen, Zhenghua and Mao, Kezhi and Wang,
   Peng and Gao, Robert X.},
Title = {Deep learning and its applications to machine health monitoring},
Journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING},
Year = {2019},
Volume = {115},
Pages = {213-237},
Month = {JAN 15},
Abstract = {Since 2006, deep learning (DL) has become a rapidly growing research
   direction, redefining state-of-the-art performances in a wide range of
   areas such as object recognition, image segmentation, speech recognition
   and machine translation. In modern manufacturing systems, data-driven
   machine health monitoring is gaining in popularity due to the widespread
   deployment of low-cost sensors and their connection to the Internet.
   Meanwhile, deep learning provides useful tools for processing and
   analyzing these big machinery data. The main purpose of this paper is to
   review and summarize the emerging research work of deep learning on
   machine health monitoring. After the brief introduction of deep learning
   techniques, the applications of deep learning in machine health
   monitoring systems are reviewed mainly from the following aspects:
   Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and
   its variants including Deep Belief Network (DBN) and Deep Boltzmann
   Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural
   Networks (RNN). In addition, an experimental study on the performances
   of these approaches has been conducted, in which the data and code have
   been online. Finally, some new trends of DL-based machine health
   monitoring methods are discussed. (C) 2018 Elsevier Ltd. All rights
   reserved.},
DOI = {10.1016/j.ymssp.2018.05.050},
ISSN = {0888-3270},
EISSN = {1096-1216},
ResearcherID-Numbers = {Chen, Zhenghua/I-1937-2018
   zhao, rui/AAD-1562-2020
   Gao, Robert/O-9339-2014
   Mao, Kezhi/A-5025-2011
   Yan, Ruqiang/A-9776-2012},
ORCID-Numbers = {Mao, Kezhi/0000-0002-9191-8604
   Yan, Ruqiang/0000-0002-1250-4084
   },
Unique-ID = {WOS:000447085500015},
}

@article{ WOS:000472453300009,
Author = {Al-Sahaf, Harith and Bi, Ying and Chen, Qi and Lensen, Andrew and Mei,
   Yi and Sun, Yanan and Tran, Binh and Xue, Bing and Zhang, Mengjie},
Title = {A survey on evolutionary machine learning},
Journal = {JOURNAL OF THE ROYAL SOCIETY OF NEW ZEALAND},
Year = {2019},
Volume = {49},
Number = {2, SI},
Pages = {205-228},
Abstract = {Artificial intelligence (AI) emphasises the creation of intelligent
   machines/systems that function like humans. AI has been applied to many
   real-world applications. Machine learning is a branch of AI based on the
   idea that systems can learn from data, identify hidden patterns, and
   make decisions with little/minimal human intervention. Evolutionary
   computation is an umbrella of population-based intelligent/learning
   algorithms inspired by nature, where New Zealand has a good
   international reputation. This paper provides a review on evolutionary
   machine learning, i.e. evolutionary computation techniques for major
   machine learning tasks such as classification, regression and
   clustering, and emerging topics including combinatorial optimisation,
   computer vision, deep learning, transfer learning, and ensemble
   learning. The paper also provides a brief review of evolutionary
   learning applications, such as supply chain and manufacturing for
   milk/dairy, wine and seafood industries, which are important to New
   Zealand. Finally, the paper presents current issues with future
   perspectives in evolutionary machine learning.},
DOI = {10.1080/03036758.2019.1609052},
ISSN = {0303-6758},
EISSN = {1175-8899},
ResearcherID-Numbers = {Tran, Binh/AAL-4815-2021
   XUE, Bing/JOZ-6681-2023
   Al-Sahaf, Harith/I-2749-2019
   Bi, Ying/HCI-1773-2022
   Mei, Yi/D-1451-2019
   Lensen, Andrew/J-3207-2019
   Zhang, Mengjie/JPK-5619-2023
   Chen, Qi/T-3322-2019},
ORCID-Numbers = {Chen, Qi/0000-0001-9367-4757
   Xue, Bing/0000-0002-4865-8026
   Al-Sahaf, Harith/0000-0003-4633-6135
   Mei, Yi/0000-0003-0682-1363
   Tran, Binh N./0000-0002-2445-1231
   Bi, Ying/0000-0003-2758-6067
   SUN, YANAN/0000-0001-6374-1429
   zhang, mengjie/0000-0003-4463-9538
   Lensen, Andrew/0000-0003-1269-4751
   },
Unique-ID = {WOS:000472453300009},
}

@article{ WOS:000463615200004,
Author = {Bleidorn, Wiebke and Hopwood, Christopher James},
Title = {Using Machine Learning to Advance Personality Assessment and Theory},
Journal = {PERSONALITY AND SOCIAL PSYCHOLOGY REVIEW},
Year = {2019},
Volume = {23},
Number = {2},
Pages = {190-203},
Month = {MAY},
Abstract = {Machine learning has led to important advances in society. One of the
   most exciting applications of machine learning in psychological science
   has been the development of assessment tools that can powerfully predict
   human behavior and personality traits. Thus far, machine learning
   approaches to personality assessment have focused on the associations
   between social media and other digital records with established
   personality measures. The goal of this article is to expand the
   potential of machine learning approaches to personality assessment by
   embedding it in a more comprehensive construct validation framework. We
   review recent applications of machine learning to personality
   assessment, place machine learning research in the broader context of
   fundamental principles of construct validation, and provide
   recommendations for how to use machine learning to advance our
   understanding of personality.},
DOI = {10.1177/1088868318772990},
ISSN = {1088-8683},
EISSN = {1532-7957},
ResearcherID-Numbers = {Bleidorn, Wiebke/H-2433-2013
   },
ORCID-Numbers = {Hopwood, Christopher/0000-0001-6645-8645},
Unique-ID = {WOS:000463615200004},
}

@article{ WOS:000473429200010,
Author = {Doupe, Patrick and Faghmous, James and Basu, Sanjay},
Title = {Machine Learning for Health Services Researchers},
Journal = {VALUE IN HEALTH},
Year = {2019},
Volume = {22},
Number = {7},
Pages = {808-815},
Month = {JUL},
Abstract = {Background: Machine learning is increasingly used to predict healthcare
   outcomes, including cost, utilization, and quality.
   Objective: We provide a high-level overview of machine learning for
   healthcare outcomes researchers and decision makers.
   Methods: We introduce key concepts for understanding the application of
   machine learning methods to healthcare outcomes research. We first
   describe current standards to rigorously learn an estimator, which is an
   algorithm developed through machine learning to predict a particular
   outcome. We include steps for data preparation, estimator family
   selection, parameter learning, regularization, and evaluation. We then
   compare 3 of the most common machine learning methods: (1) decision tree
   methods that can be useful for identifying how different subpopulations
   experience different risks for an outcome; (2) deep learning methods
   that can identify complex nonlinear patterns or interactions between
   variables predictive of an outcome; and (3) ensemble methods that can
   improve predictive performance by combining multiple machine learning
   methods.
   Results: We demonstrate the application of common machine methods to a
   simulated insurance claims dataset. We specifically include statistical
   code in R and Python for the development and evaluation of estimators
   for predicting which patients are at heightened risk for hospitalization
   from ambulatory care-sensitive conditions.
   Conclusions: Outcomes researchers should be aware of key standards for
   rigorously evaluating an estimator developed through machine learning
   approaches. Although multiple methods use machine learning concepts,
   different approaches are best suited for different research problems.},
DOI = {10.1016/j.jval.2019.02.012},
ISSN = {1098-3015},
EISSN = {1524-4733},
Unique-ID = {WOS:000473429200010},
}

@article{ WOS:000502191300012,
Author = {Khosla, Meenakshi and Jamison, Keith and Ngo, Gia H. and Kuceyeski, Amy
   and Sabuncu, Mert R.},
Title = {Machine learning in resting-state fMRI analysis},
Journal = {MAGNETIC RESONANCE IMAGING},
Year = {2019},
Volume = {64},
Number = {SI},
Pages = {101-121},
Month = {DEC},
Abstract = {Machine learning techniques have gained prominence for the analysis of
   resting-state functional Magnetic Resonance Imaging (rs-fMRI) data.
   Here, we present an overview of various unsupervised and supervised
   machine learning applications to rs-fMRI. We offer a methodical taxonomy
   of machine learning methods in restingstate fMRI. We identify three
   major divisions of unsupervised learning methods with regard to their
   applications to rs-fMRI, based on whether they discover principal modes
   of variation across space, time or population. Next, we survey the
   algorithms and rs-fMRI feature representations that have driven the
   success of supervised subjectlevel predictions. The goal is to provide a
   high-level overview of the burgeoning field of rs-fMRI from the
   perspective of machine learning applications.},
DOI = {10.1016/j.mri.2019.05.031},
ISSN = {0730-725X},
EISSN = {1873-5894},
ResearcherID-Numbers = {Ngo, Gia H/AGQ-7253-2022
   Kuceyeski, Amy/AGI-9682-2022
   Khosla, Meenakshi/JRW-4066-2023
   Sabuncu, Mert/ABE-2284-2021},
ORCID-Numbers = {Jamison, Keith/0000-0001-7139-6661
   Kuceyeski, Amy/0000-0002-5050-8342
   Sabuncu, Mert/0000-0002-7068-719X
   },
Unique-ID = {WOS:000502191300012},
}

@article{ WOS:000355286600032,
Author = {Ghahramani, Zoubin},
Title = {Probabilistic machine learning and artificial intelligence},
Journal = {NATURE},
Year = {2015},
Volume = {521},
Number = {7553},
Pages = {452-459},
Month = {MAY 28},
Abstract = {How can a machine learn from experience? Probabilistic modelling
   provides a framework for understanding what learning is, and has
   therefore emerged as one of the principal theoretical and practical
   approaches for designing machines that learn from data acquired through
   experience. The probabilistic framework, which describes how to
   represent and manipulate uncertainty about models and predictions, has a
   central role in scientific data analysis, machine learning, robotics,
   cognitive science and artificial intelligence. This Review provides an
   introduction to this framework, and discusses some of the
   state-of-the-art advances in the field, namely, probabilistic
   programming, Bayesian optimization, data compression and automatic model
   discovery.},
DOI = {10.1038/nature14541},
ISSN = {0028-0836},
EISSN = {1476-4687},
Unique-ID = {WOS:000355286600032},
}

@article{ WOS:000501336600003,
Author = {Huang, Jui-Chan and Ko, Kuo-Min and Shu, Ming-Hung and Hsu, Bi-Min},
Title = {Application and comparison of several machine learning algorithms and
   their integration models in regression problems},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2020},
Volume = {32},
Number = {10, SI},
Pages = {5461-5469},
Month = {MAY},
Abstract = {With the rapid development of machine learning technology, as a
   regression problem that helps people to find the law from the massive
   data to achieve the prediction effect, more and more people pay
   attention. Data prediction has become an important part of people's
   daily life. Currently, the technology is widely used in many fields such
   as weather forecasting, medical diagnosis and financial forecasting.
   Therefore, the research of machine learning algorithms in regression
   problems is a research hotspot in the field of machine learning in
   recent years. However, real-world regression problems often have very
   complex internal and external factors, and various machine learning
   algorithms have different effects on scalability and predictive
   performance. In order to better study the application effect of machine
   learning algorithm in regression problem, this paper mainly adopts three
   common machine learning algorithms: BP neural network, extreme learning
   machine and support vector machine. Then, by comparing the effects of
   the single model and integrated model of these machine learning
   algorithms in the application of regression problems, the advantages and
   disadvantages of each machine learning algorithm are studied. Finally,
   the performance of each machine learning algorithm in regression
   prediction is verified by simulation experiments on four different data
   sets. The results show that the research on several machine learning
   algorithms and their integration models has certain feasibility and
   rationality.},
DOI = {10.1007/s00521-019-04644-5},
EarlyAccessDate = {NOV 2019},
ISSN = {0941-0643},
EISSN = {1433-3058},
ResearcherID-Numbers = {Shu, Ming-Hung/AAT-7702-2021
   Huang, Yi-June/HWS-6510-2023},
Unique-ID = {WOS:000501336600003},
}

@article{ WOS:000502169900112,
Author = {Masood, Hassan and Toe, Cui Ying and Teoh, Wey Yang and Sethu,
   Vidhyasaharan and Amal, Rose},
Title = {Machine Learning for Accelerated Discovery of Solar Photocatalysts},
Journal = {ACS CATALYSIS},
Year = {2019},
Volume = {9},
Number = {12},
Pages = {11774-11787},
Month = {DEC},
Abstract = {Robust screening of materials on the basis of
   structure-property-activity relationships to discover active
   photocatalysts is a highly sought out aspect of photocatalysis research.
   Recent advancements in machine learning offer considerable opportunities
   to evolve photocatalysts discovery practices. Machine learning has
   largely facilitated various areas of science and engineering, including
   heterogeneous catalysis, but adaptation of it in photocatalysis research
   is still at an elementary stage. The scarcity of consistent training
   data is a major bottleneck, and we foresee the integration of
   photo-catalysis domain knowledge in mainstream machine learning
   protocols as a viable solution. Here, we present a holistic framework
   incorporating machine learning and domain knowledge to set directions
   toward accelerated discovery of solar photocatalysts. This Perspective
   begins with a discussion on domain knowledge available in photocatalysis
   which could potentially be leveraged to liaise with machine learning
   methods. Subsequently, we present prevalent machine learning practices
   in heterogeneous catalysis tailored to assist discovery of
   photocatalysts in a purely data-driven fashion. Lastly, we conceptualize
   various strategies for complementing data-driven machine learning with
   photocatalysis domain knowledge. The strategies involve the following:
   (i) integration of theoretical and prior empirical knowledge during the
   training of machine learning models; (ii) embedding the knowledge in
   feature space; and (iii) utilizing existing material databases to
   constrain machine learning predictions. The aforementioned human-in-loop
   framework (leveraging both human and machine intelligence) could
   possibly mitigate the lack of interpretability and reliability
   associated with data-driven machine learning and reinforce complex model
   architectures irrespective of data scarcity. The concept could also
   offer substantial benefits to photocatalysis informatics by promoting a
   paradigm shift away from the Edisonian approach.},
DOI = {10.1021/acscatal.9b02531},
ISSN = {2155-5435},
ResearcherID-Numbers = {Toe, Cui/X-9226-2019
   Masood, Hassan/E-3612-2013
   Sethu, Vidhyasaharan/B-5197-2013
   Teoh, Wey Yang/ABK-6069-2022
   Amal, Rose/D-4749-2011
   },
ORCID-Numbers = {Teoh, Wey Yang/0000-0002-4400-4578
   Masood, Hassan/0000-0001-9390-169X
   Amal, Rose/0000-0001-9561-4918
   Sethu, Vidhyasaharan/0000-0001-8492-1787
   Toe, Cui Ying/0000-0002-8480-5994},
Unique-ID = {WOS:000502169900112},
}

@article{ WOS:000471357900035,
Author = {Zhang, Zhenwei and Sejdic, Ervin},
Title = {Radiological images and machine learning: Trends, perspectives, and
   prospects},
Journal = {COMPUTERS IN BIOLOGY AND MEDICINE},
Year = {2019},
Volume = {108},
Pages = {354-370},
Month = {MAY},
Abstract = {The application of machine learning to radiological images is an
   increasingly active research area that is expected to grow in the next
   five to ten years. Recent advances in machine learning have the
   potential to recognize and classify complex patterns from different
   radiological imaging modalities such as x-rays, computed tomography,
   magnetic resonance imaging and positron emission tomography imaging. In
   many applications, machine learning based systems have shown comparable
   performance to human decision-making. The applications of machine
   learning are the key ingredients of future clinical decision making and
   monitoring systems. This review covers the fundamental concepts behind
   various machine learning techniques and their applications in several
   radiological imaging areas, such as medical image segmentation, brain
   function studies and neurological disease diagnosis, as well as
   computer-aided systems, image registration, and content-based image
   retrieval systems. Synchronistically, we will briefly discuss current
   challenges and future directions regarding the application of machine
   learning in radiological imaging. By giving insight on how take
   advantage of machine learning powered applications, we expect that
   clinicians can prevent and diagnose diseases more accurately and
   efficiently.},
DOI = {10.1016/j.compbiomed.2019.02.017},
ISSN = {0010-4825},
EISSN = {1879-0534},
ResearcherID-Numbers = {ZHANG, ZHENWEI/AAW-7973-2021},
ORCID-Numbers = {Zhang, Zhenwei/0000-0001-7215-0858
   },
Unique-ID = {WOS:000471357900035},
}

@article{ WOS:000503827500061,
Author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J. P. C. and
   Hassan, Suhaidi and Korotaev, Valery V.},
Title = {Machine learning in the Internet of Things: Designed techniques for
   smart cities},
Journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
Year = {2019},
Volume = {100},
Pages = {826-843},
Month = {NOV},
Abstract = {Machine learning is one of the emerging technologies that has grabbed
   the attention of academicians and industrialists, and is expected to
   evolve in the near future. Machine learning techniques are anticipated
   to provide pervasive connections for wireless nodes. In fact, machine
   learning paves the way for the Internet of Things (IoT)-a network that
   supports communications among various devices without human
   interactions. Machine learning techniques are being utilized in several
   fields such as healthcare, smart grids, vehicular communications, and so
   on. In this paper, we study different IoT-based machine learning
   mechanisms that are used in the mentioned fields among others. In
   addition, the lessons learned are reported and the assessments are
   explored viewing the basic aim machine learning techniques are expected
   to play in IoT networks. (C) 2019 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.future.2019.04.017},
ISSN = {0167-739X},
EISSN = {1872-7115},
ResearcherID-Numbers = {Rodrigues, Joel/A-8103-2013
   Hassan, Suhaidi/F-4000-2010
   Mohsen, Guizani/D-9103-2018
   Ud Din, Ikram/AAK-4524-2020
   Korotaev, Valery/F-6180-2014},
ORCID-Numbers = {Hassan, Suhaidi/0000-0002-5094-4929
   Rodrigues, Joel/0000-0001-8657-3800
   Ud Din, Ikram/0000-0001-8896-547X
   },
Unique-ID = {WOS:000503827500061},
}

@article{ WOS:000478780200033,
Author = {Jollans, Lee and Boyle, Rory and Artiges, Eric and Banaschewski, Tobias
   and Desrivieres, Sylvane and Grigis, Antoine and Martinot, Jean-Luc and
   Paus, Tomas and Smolka, Michael N. and Walter, Henrik and Schumann,
   Gunter and Garavan, Hugh and Whelan, Robert},
Title = {Quantifying performance of machine learning methods for neuroimaging
   data},
Journal = {NEUROIMAGE},
Year = {2019},
Volume = {199},
Pages = {351-365},
Month = {OCT 1},
Abstract = {Machine learning is increasingly being applied to neuroimaging data.
   However, most machine learning algorithms have not been designed to
   accommodate neuroimaging data, which typically has many more data points
   than subjects, in addition to multicollinearity and low signal-to-noise.
   Consequently, the relative efficacy of different machine learning
   regression algorithms for different types of neuroimaging data are not
   known. Here, we sought to quantify the performance of a variety of
   machine learning algorithms for use with neuroimaging data with various
   sample sizes, feature set sizes, and predictor effect sizes. The
   contribution of additional machine learning techniques - embedded
   feature selection and bootstrap aggregation (bagging) - to model
   performance was also quantified. Five machine learning regression
   methods - Gaussian Process Regression, Multiple Kernel Learning, Kernel
   Ridge Regression, the Elastic Net and Random Forest, were examined with
   both real and simulated MRI data, and in comparison to standard multiple
   regression. The different machine learning regression algorithms
   produced varying results, which depended on sample size, feature set
   size, and predictor effect size. When the effect size was large, the
   Elastic Net, Kernel Ridge Regression and Gaussian Process Regression
   performed well at most sample sizes and feature set sizes. However, when
   the effect size was small, only the Elastic Net made accurate
   predictions, but this was limited to analyses with sample sizes greater
   than 400. Random Forest also produced a moderate performance for small
   effect sizes, but could do so across all sample sizes. Machine learning
   techniques also improved prediction accuracy for multiple regression.
   These data provide empirical evidence for the differential performance
   of various machines on neuroimaging data, which are dependent on number
   of sample size, features and effect size.},
DOI = {10.1016/j.neuroimage.2019.05.082},
ISSN = {1053-8119},
EISSN = {1095-9572},
ResearcherID-Numbers = {Smolka, Michael/B-4865-2011
   Boyle, Rory/AAP-5432-2021
   Schumann, Gunter/H-1013-2013
   Walter, Henrik/O-2612-2013
   Artiges, Eric/C-2815-2019
   Whelan, Robert/E-5365-2010
   Martinot, Jean-Luc/N-3704-2018
   Poustka, Luise/D-9299-2017
   Desrivieres, Sylvane/B-7399-2011
   Artiges, Eric/AAT-7219-2021
   Banaschewski, Tobias/ABE-5985-2020
   },
ORCID-Numbers = {Schumann, Gunter/0000-0003-4905-5523
   Smolka, Michael/0000-0001-5398-5569
   Banaschewski, Tobias/0000-0003-4595-1144
   Boyle, Rory/0000-0003-0787-6892
   Artiges, Eric/0000-0003-4461-7646
   Walter, Henrik/0000-0002-9403-6121
   Desrivieres, Sylvane/0000-0002-9120-7060
   Jollans, Lee/0000-0003-4254-5159},
Unique-ID = {WOS:000478780200033},
}

@article{ WOS:000512985500004,
Author = {Kleesiek, Jens and Murray, Jacob M. and Strack, Christian and Kaissis,
   Georgios and Braren, Rickmer},
Title = {A primer on machine learning},
Journal = {RADIOLOGE},
Year = {2020},
Volume = {60},
Number = {1, SI},
Pages = {24-31},
Month = {JAN},
Abstract = {Background The methods of machine learning and artificial intelligence
   are slowly but surely being introduced in everyday medical practice. In
   the future, they will support us in diagnosis and therapy and thus
   improve treatment for the benefit of the individual patient. It is
   therefore important to deal with this topic and to develop a basic
   understanding of it. Objectives This article gives an overview of the
   exciting and dynamic field of machine learning and serves as an
   introduction to some methods primarily from the realm of supervised
   learning. In addition to definitions and simple examples, limitations
   are discussed. Conclusions The basic principles behind the methods are
   simple. Nevertheless, due to their high dimensional nature, the factors
   influencing the results are often difficult or impossible to understand
   by humans. In order to build confidence in the new technologies and to
   guarantee their safe application, we need explainable algorithms and
   prospective effectiveness studies.},
DOI = {10.1007/s00117-019-00616-x},
ISSN = {0033-832X},
EISSN = {1432-2102},
ResearcherID-Numbers = {Braren, Rickmer/U-3254-2018},
Unique-ID = {WOS:000512985500004},
}

@article{ WOS:000425074100016,
Author = {Portugal, Ivens and Alencar, Paulo and Cowan, Donald},
Title = {The use of machine learning algorithms in recommender systems: A
   systematic review},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2018},
Volume = {97},
Pages = {205-227},
Month = {MAY 1},
Abstract = {Recommender systems use algorithms to provide users with product or
   service recommendations. Recently, these systems have been using machine
   learning algorithms from the field of artificial intelligence. However,
   choosing a suitable machine learning algorithm for a recommender system
   is difficult because of the number of algorithms described in the
   literature. Researchers and practitioners developing recommender systems
   are left with little information about the current approaches in
   algorithm usage. Moreover, the development of recommender systems using
   machine learning algorithms often faces problems and raises questions
   that must be resolved. This paper presents a systematic review of the
   literature that analyzes the use of machine learning algorithms in
   recommender systems and identifies new research opportunities. The goals
   of this study are to (i) identify trends in the use or research of
   machine learning algorithms in recommender systems; (ii) identify open
   questions in the use or research of machine learning algorithms; and
   (iii) assist new researchers to position new research activity in this
   domain appropriately. The results of this study identify existing
   classes of recommender systems, characterize adopted machine learning
   approaches, discuss the use of big data technologies, identify types of
   machine learning algorithms and their application domains, and analyzes
   both main and alternative performance metrics. (C) 2017 Elsevier Ltd.
   All rights reserved.},
DOI = {10.1016/j.eswa.2017.12.020},
ISSN = {0957-4174},
EISSN = {1873-6793},
Unique-ID = {WOS:000425074100016},
}

@article{ WOS:000471293100006,
Author = {Handa, Anand and Sharma, Ashu and Shukla, Sandeep K.},
Title = {Machine learning in cybersecurity: A review},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY},
Year = {2019},
Volume = {9},
Number = {4},
Month = {JUL},
Abstract = {Machine learning technology has become mainstream in a large number of
   domains, and cybersecurity applications of machine learning techniques
   are plenty. Examples include malware analysis, especially for zero-day
   malware detection, threat analysis, anomaly based intrusion detection of
   prevalent attacks on critical infrastructures, and many others. Due to
   the ineffectiveness of signature-based methods in detecting zero day
   attacks or even slight variants of known attacks, machine learning-based
   detection is being used by researchers in many cybersecurity products.
   In this review, we discuss several areas of cybersecurity where machine
   learning is used as a tool. We also provide a few glimpses of
   adversarial attacks on machine learning algorithms to manipulate
   training and test data of classifiers, to render such tools ineffective.
   This article is categorized under: Application Areas > Science and
   Technology Technologies > Machine Learning Technologies > Classification
   Application Areas > Data Mining Software Tools},
DOI = {10.1002/widm.1306},
Article-Number = {e1306},
ISSN = {1942-4787},
EISSN = {1942-4795},
ResearcherID-Numbers = {SHUKLA, SANDEEP/T-6430-2019},
ORCID-Numbers = {Sharma, Ashu/0000-0002-0695-7749
   },
Unique-ID = {WOS:000471293100006},
}

@article{ WOS:000488315100001,
Author = {Komura, Daisuke and Ishikawa, Shumpei},
Title = {Machine learning approaches for pathologic diagnosis},
Journal = {VIRCHOWS ARCHIV},
Year = {2019},
Volume = {475},
Number = {2},
Pages = {131-138},
Month = {AUG},
Abstract = {Machine learning techniques, especially deep learning techniques such as
   convolutional neural networks, have been successfully applied to general
   image recognitions since their overwhelming performance at the 2012
   ImageNet Large Scale Visual Recognition Challenge. Recently, such
   techniques have also been applied to various medical, including
   histopathological, images to assist the process of medical diagnosis. In
   some cases, deep learning-based algorithms have already outperformed
   experienced pathologists for recognition of histopathological images.
   However, pathological images differ from general images in some aspects,
   and thus, machine learning of histopathological images requires
   specialized learning methods. Moreover, many pathologists are skeptical
   about the ability of deep learning technology to accurately recognize
   histopathological images because what the learned neural network
   recognizes is often indecipherable to humans. In this review, we first
   introduce various applications incorporating machine learning developed
   to assist the process of pathologic diagnosis, and then describe machine
   learning problems related to histopathological image analysis, and
   review potential ways to solve these problems.},
DOI = {10.1007/s00428-019-02594-w},
ISSN = {0945-6317},
EISSN = {1432-2307},
ResearcherID-Numbers = {Ishikawa, Shumpei/NBW-9562-2025
   Komura, Daisuke/AAF-2481-2019},
ORCID-Numbers = {Komura, Daisuke/0000-0002-0038-728X
   },
Unique-ID = {WOS:000488315100001},
}

@article{ WOS:000439847600006,
Author = {Shameer, Khader and Johnson, Kipp W. and Glicksberg, Benjamin S. and
   Dudley, Joel T. and Sengupta, Partho P.},
Title = {Machine learning in cardiovascular medicine: are we there yet?},
Journal = {HEART},
Year = {2018},
Volume = {104},
Number = {14},
Pages = {1156-1164},
Month = {JUL},
Abstract = {Artificial intelligence (AI) broadly refers to analytical algorithms
   that iteratively learn from data, allowing computers to find hidden
   insights without being explicitly programmed where to look. These
   include a family of operations encompassing several terms like machine
   learning, cognitive learning, deep learning and reinforcement
   learning-based methods that can be used to integrate and interpret
   complex biomedical and healthcare data in scenarios where traditional
   statistical methods may not be able to perform. In this review article,
   we discuss the basics of machine learning algorithms and what potential
   data sources exist; evaluate the need for machine learning; and examine
   the potential limitations and challenges of implementing machine in the
   context of cardiovascular medicine. The most promising avenues for AI in
   medicine are the development of automated risk prediction algorithms
   which can be used to guide clinical care; use of unsupervised learning
   techniques to more precisely phenotype complex disease; and the
   implementation of reinforcement learning algorithms to intelligently
   augment healthcare providers. The utility of a machine learning-based
   predictive model will depend on factors including data heterogeneity,
   data depth, data breadth, nature of modelling task, choice of machine
   learning and feature selection algorithms, and orthogonal evidence. A
   critical understanding of the strength and limitations of various
   methods and tasks amenable to machine learning is vital. By leveraging
   the growing corpus of big data in medicine, we detail pathways by which
   machine learning may facilitate optimal development of patient-specific
   models for improving diagnoses, intervention and outcome in
   cardiovascular medicine.},
DOI = {10.1136/heartjnl-2017-311198},
ISSN = {1355-6037},
EISSN = {1468-201X},
ResearcherID-Numbers = {Khader, Shameer/J-2564-2016
   Sengupta, Partho/AAG-9508-2020
   Glicksberg, Benjamin/I-9500-2019
   Johnson, Kipp/I-1260-2019
   Dudley, Joel/E-9283-2010
   },
ORCID-Numbers = {Johnson, Kipp/0000-0002-5102-741X
   Glicksberg, Benjamin/0000-0003-4515-8090},
Unique-ID = {WOS:000439847600006},
}

@article{ WOS:000481489500005,
Author = {Rasekhschaffe, Keywan Christian and Jones, Robert C.},
Title = {Machine Learning for Stock Selection},
Journal = {FINANCIAL ANALYSTS JOURNAL},
Year = {2019},
Volume = {75},
Number = {3},
Pages = {70-88},
Abstract = {Machine learning is an increasingly important and controversial topic in
   quantitative finance. A lively debate persists as to whether machine
   learning techniques can be practical investment tools. Although machine
   learning algorithms can uncover subtle, contextual. and nonlinear
   relationships, overfitting poses a major challenge when one is trying to
   extract signals from noisy historical data. We describe some of the
   basic concepts of machine learning and provide a simple example of how
   investors can use machine learning techniques to forecast the
   cross-section of stock returns while limiting the risk of overfitting.},
DOI = {10.1080/0015198X.2019.1596678},
ISSN = {0015-198X},
EISSN = {1938-3312},
ORCID-Numbers = {Rasekhschaffe, Keywan/0000-0002-5690-1620},
Unique-ID = {WOS:000481489500005},
}

@article{ WOS:000484486600010,
Author = {Senanayake, Sameera and White, Nicole and Graves, Nicholas and Healy,
   Helen and Baboolal, Keshwar and Kularatna, Sanjeewa},
Title = {Machine learning in predicting graft failure following kidney
   transplantation: A systematic review of published predictive models},
Journal = {INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS},
Year = {2019},
Volume = {130},
Month = {OCT},
Abstract = {Introduction: Machine learning has been increasingly used to develop
   predictive models to diagnose different disease conditions. The
   heterogeneity of the kidney transplant population makes predicting graft
   outcomes extremely challenging. Several kidney graft outcome prediction
   models have been developed using machine learning, and are available in
   the literature. However, a systematic review of machine learning based
   prediction methods applied to kidney transplant has not been done to
   date. The main aim of our study was to perform an in-depth systematic
   analysis of different machine learning methods used to predict graft
   outcomes among kidney transplant patients, and assess their usefulness
   as an aid to decision-making.
   Methods: A systemic review of machine learning methods used to predict
   graft outcomes among kidney transplant patients was carried out using a
   search of the Medline, the Cumulative Index to Nursing and Allied Health
   Literature, EMBASE, PsycINFO and Cochrane databases.
   Results: A total of 295 articles were identified and extracted. Of
   these, 18 ma the inclusion criteria. Most of the studies were published
   in the United States after 2010. The population size used to develop the
   models varied from 80 to 92,844, and the number of features in the
   models ranged from 6 to 71. The most common machine learning methods
   used were artificial neural networks, decision trees and Bayesian belief
   networks. Most of the machine learning based predictive models predicted
   graft failure with high sensitivity and specificity. Only one machine
   learning based prediction model had modelled time-to-event (survival)
   information. Seven studies compared the predictive performance of
   machine learning models with traditional regression methods and the
   performance of machine learning methods was found to be mixed, when
   compared with traditional regression methods.
   Conclusion: There was a wide variation in the size of the study
   population and the input variables used. However, the prediction
   accuracy provided mixed results when machine learning and traditional
   predictive methods are compared. Based on reported gains in predictive
   performance, machine learning has the potential to improve kidney
   transplant outcome prediction and aid medical decision making},
DOI = {10.1016/j.ijmedinf.2019.103957},
Article-Number = {103957},
ISSN = {1386-5056},
EISSN = {1872-8243},
ResearcherID-Numbers = {White, Nicole/AGO-2409-2022
   Graves, Nicholas/A-3052-2011
   Baboolal, Keshwar/B-8243-2012
   Senanayake, Sameera/K-2966-2019
   Healy, Helen/G-2686-2010
   Baboolal, Kesh/B-8243-2012},
ORCID-Numbers = {Graves, Nicholas/0000-0002-5559-3267
   Healy, Helen/0000-0003-4342-5300
   Senanayake, Sameera/0000-0002-5606-2046
   White, Nicole/0000-0002-9292-0773
   Kularatna, Sanjeewa/0000-0001-5650-154X
   Baboolal, Kesh/0000-0002-3688-9353},
Unique-ID = {WOS:000484486600010},
}

@article{ WOS:000493335100001,
Author = {Iwasaki, Yuma and Sawada, Ryohto and Stanev, Valentin and Ishida,
   Masahiko and Kirihara, Akihiro and Omori, Yasutomo and Someya, Hiroko
   and Takeuchi, Ichiro and Saitoh, Eiji and Yorozu, Shinichi},
Title = {Identification of advanced spin-driven thermoelectric materials via
   interpretable machine learning},
Journal = {NPJ COMPUTATIONAL MATERIALS},
Year = {2019},
Volume = {5},
Month = {OCT 30},
Abstract = {Machine learning is becoming a valuable tool for scientific discovery.
   Particularly attractive is the application of machine learning methods
   to the field of materials development, which enables innovations by
   discovering new and better functional materials. To apply machine
   learning to actual materials development, close collaboration between
   scientists and machine learning tools is necessary. However, such
   collaboration has been so far impeded by the black box nature of many
   machine learning algorithms. It is often difficult for scientists to
   interpret the data-driven models from the viewpoint of material science
   and physics. Here, we demonstrate the development of spin-driven
   thermoelectric materials with anomalous Nernst effect by using an
   interpretable machine learning method called factorized asymptotic
   Bayesian inference hierarchical mixture of experts (FAB/HMEs). Based on
   prior knowledge of material science and physics, we were able to extract
   from the interpretable machine learning some surprising correlations and
   new knowledge about spin-driven thermoelectric materials. Guided by
   this, we carried out an actual material synthesis that led to the
   identification of a novel spin-driven thermoelectric material. This
   material shows the largest thermopower to date.},
DOI = {10.1038/s41524-019-0241-9},
Article-Number = {103},
EISSN = {2057-3960},
ResearcherID-Numbers = {Saitoh, Eiji/C-1004-2011},
ORCID-Numbers = {Iwasaki, Yuma/0000-0002-7117-277X
   },
Unique-ID = {WOS:000493335100001},
}

@article{ WOS:000417528000001,
Author = {Chicco, Davide},
Title = {Ten quick tips for machine learning in computational biology},
Journal = {BIODATA MINING},
Year = {2017},
Volume = {10},
Month = {DEC 8},
Abstract = {Machine learning has become a pivotal tool for many projects in
   computational biology, bioinformatics, and health informatics.
   Nevertheless, beginners and biomedical researchers often do not have
   enough experience to run a data mining project effectively, and
   therefore can follow incorrect practices, that may lead to common
   mistakes or over-optimistic results. With this review, we present ten
   quick tips to take advantage of machine learning in any computational
   biology context, by avoiding some common errors that we observed
   hundreds of times in multiple bioinformatics projects. We believe our
   ten suggestions can strongly help any machine learning practitioner to
   carry on a successful project in computational biology and related
   sciences.},
DOI = {10.1186/s13040-017-0155-3},
Article-Number = {35},
ISSN = {1756-0381},
ResearcherID-Numbers = {Chicco, Davide/C-7058-2017},
ORCID-Numbers = {Chicco, Davide/0000-0001-9655-7142},
Unique-ID = {WOS:000417528000001},
}

@article{ WOS:000462868200006,
Author = {Uribe, Carlos F. and Mathotaarachchi, Sulantha and Gaudet, Vincent and
   Smith, Kenneth C. and Rosa-Neto, Pedro and Benard, Francois and Black,
   Sandra E. and Zukotynski, Katherine},
Title = {Machine Learning in Nuclear Medicine: Part 1-Introduction},
Journal = {JOURNAL OF NUCLEAR MEDICINE},
Year = {2019},
Volume = {60},
Number = {4},
Pages = {451-458},
Month = {APR 1},
Abstract = {Learning Objectives: On successful completion of this activity,
   participants should be able to (1) provide an introduction to machine
   learning, neural networks, and deep learning; (2) discuss common machine
   learning algorithms with illustrative examples and figures; and (3)
   compare machine learning algorithms and provide guidance on selection
   for a given application.},
DOI = {10.2967/jnumed.118.223495},
ISSN = {0161-5505},
EISSN = {1535-5667},
ResearcherID-Numbers = {Benard, Francois/M-7720-2015
   Black, Sandra/C-7294-2011
   Rosa-Neto, Pedro/F-5077-2015
   Zaidi, Habib/I-4669-2017
   ROSA-NETO, PEDRO/F-5077-2015},
ORCID-Numbers = {Benard, Francois/0000-0001-7995-3581
   Gaudet, Vincent/0000-0002-5534-0825
   ROSA-NETO, PEDRO/0000-0001-9116-1376},
Unique-ID = {WOS:000462868200006},
}

@article{ WOS:000454602800004,
Author = {Sturm, Bob L. and Ben-Tal, Oded and Monaghan, Una and Collins, Nick and
   Herremans, Dorien and Chew, Elaine and Hadjeres, Gaetan and Deruty,
   Emmanuel and Pachet, Francois},
Title = {Machine learning research that matters for music creation: A case study},
Journal = {JOURNAL OF NEW MUSIC RESEARCH},
Year = {2019},
Volume = {48},
Number = {1},
Pages = {36-55},
Month = {JAN 1},
Abstract = {Research applying machine learning to music modelling and generation
   typically proposes model architectures, training methods and datasets,
   and gauges system performance using quantitative measures like sequence
   likelihoods and/or qualitative listening tests. Rarely does such work
   explicitly question and analyse its usefulness for and impact on
   real-world practitioners, and then build on those outcomes to inform the
   development and application of machine learning. This article attempts
   to do these things for machine learning applied to music creation.
   Together with practitioners, we develop and use several applications of
   machine learning for music creation, and present a public concert of the
   results. We reflect on the entire experience to arrive at several ways
   of advancing these and similar applications of machine learning to music
   creation.},
DOI = {10.1080/09298215.2018.1515233},
ISSN = {0929-8215},
EISSN = {1744-5027},
ResearcherID-Numbers = {Hadjeres, Gaëtan/ABF-3378-2020
   Sturm, Bob/C-2613-2013
   Herremans, Dorien/G-9599-2018
   Monaghan, Una/KLC-4994-2024
   },
ORCID-Numbers = {Ben-Tal, Oded/0000-0003-2719-8152
   Chew, Elaine/0000-0002-8342-1024
   Herremans, Dorien/0000-0001-8607-1640
   Monaghan, Una/0000-0003-2389-5700},
Unique-ID = {WOS:000454602800004},
}

@article{ WOS:000489702000038,
Author = {Lee, Keon Myung and Yoo, Jaesoo and Kim, Sang-Wook and Lee, Jee-Hyong
   and Hong, Jiman},
Title = {Autonomic machine learning platform},
Journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT},
Year = {2019},
Volume = {49},
Pages = {491-501},
Month = {DEC},
Abstract = {Acquiring information properly through machine learning requires
   familiarity with the available algorithms and understanding how they
   work and how to address the given problem in the best possible way.
   However, even for machine-learning experts in specific industrial
   fields, in order to predict and acquire information properly in
   different industrial fields, it is necessary to attempt several
   instances of trial and error to succeed with the application of machine
   learning. For non-experts, it is much more difficult to make accurate
   predictions through machine learning.
   In this paper, we propose an autonomic machine learning platform which
   provides the decision factors to be made during the developing of
   machine learning applications. In the proposed autonomic machine
   learning platform, machine learning processes are automated based on the
   specification of autonomic levels. This autonomic machine learning
   platform can be used to derive a high-quality learning result by
   minimizing experts' interventions and reducing the number of design
   selections that require expert knowledge and intuition. We also
   demonstrate that the proposed autonomic machine learning platform is
   suitable for smart cities which typically require considerable amounts
   of security sensitive information.},
DOI = {10.1016/j.ijinfomgt.2019.07.003},
ISSN = {0268-4012},
EISSN = {1873-4707},
ORCID-Numbers = {YOO, JAESOO/0000-0001-9926-9947},
Unique-ID = {WOS:000489702000038},
}

@article{ WOS:000514113100007,
Author = {Kumeno, Fumihiro},
Title = {Sofware engneering challenges for machine learning applications: A
   literature review},
Journal = {INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS},
Year = {2019},
Volume = {13},
Number = {4},
Pages = {463-476},
Abstract = {Machine learning techniques, especially deep learning, have achieved
   remarkable breakthroughs over the past decade. At present, machine
   learning applications are deployed in many fields. However, the outcomes
   of software engineering researches are not always easily utilized in the
   development and deployment of machine learning applications. The main
   reason for this difficulty is the many differences between machine
   learning applications and traditional information systems. Machine
   learning techniques are evolving rapidly, but face inherent technical
   and non-technical challenges that complicate their lifecycle activities.
   This review paper attempts to clarify the software engineering
   challenges for machine learning applications that either exist or
   potentially exist by conducting a systematic literature collection and
   by mapping the identified challenge topics to knowledge areas defined by
   the Software Engineering Body of Knowledge (Swebok).},
DOI = {10.3233/IDT-190160},
ISSN = {1872-4981},
EISSN = {1875-8843},
Unique-ID = {WOS:000514113100007},
}

@article{ WOS:000504871100008,
Author = {Khan, Omar and Badhiwala, Jetan H. and Wilson, Jamie R. F. and Jiang,
   Fan and Martin, Allan R. and Fehlings, Michael G.},
Title = {Predictive Modeling of Outcomes After Traumatic and Nontraumatic Spinal
   Cord Injury Using Machine Learning: Review of Current Progress and
   Future Directions},
Journal = {NEUROSPINE},
Year = {2019},
Volume = {16},
Number = {4},
Pages = {678-685},
Month = {DEC},
Abstract = {Machine learning represents a promising frontier in epidemiological
   research on spine surgery. It consists of a series of algorithms that
   determines relationships between data. Machine learning maintains
   numerous advantages over conventional regression techniques, such as a
   reduced requirement for a priori knowledge on predictors and better
   ability to manage large datasets. Current studies have made extensive
   strides in employing machine learning to a greater capacity in spinal
   cord injury (SCI). Analyses using machine learning algorithms have been
   done on both traumatic SCI and nontraumatic SCI, the latter of which
   typically represents degenerative spine disease resulting in spinal cord
   compression, such as degenerative cervical myelopathy. This article is a
   literature review of current studies published in traumatic and
   nontraumatic SCI that employ machine learning for the prediction of a
   host of outcomes. The studies described utilize machine learning in a
   variety of capacities, including imaging analysis and prediction in
   large epidemiological data sets. We discuss the performance of these
   machine learning-based clinical prognostic models relative to
   conventional statistical prediction models. Finally, we detail the
   future steps needed for machine learning to become a more common
   modality for statistical analysis in SCI.},
DOI = {10.14245/ns.1938390.195},
ISSN = {2586-6583},
EISSN = {2586-6591},
ORCID-Numbers = {Badhiwala, Jetan/0000-0002-2528-1684
   Fehlings, Michael G./0000-0002-5722-6364},
Unique-ID = {WOS:000504871100008},
}

@article{ WOS:000448233900010,
Author = {Kim, Dong-Hyeon and Kim, Thomas J. Y. and Wang, Xinlin and Kim, Mincheol
   and Quan, Ying-Jun and Oh, Jin Woo and Min, Soo-Hong and Kim, Hyungjung
   and Bhandari, Binayak and Yang, Insoon and Ahn, Sung-Hoon},
Title = {Smart Machining Process Using Machine Learning: A Review and Perspective
   on Machining Industry},
Journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN
   TECHNOLOGY},
Year = {2018},
Volume = {5},
Number = {4},
Pages = {555-568},
Month = {AUG},
Abstract = {The Fourth Industrial Revolution incorporates the digital revolution
   into the physical world, creating a new direction in a number of fields,
   including artificial intelligence, quantum computing, nanotechnology,
   biotechnology, robotics, 3D printing, autonomous vehicles, and the
   Internet of Things. The artificial intelligence field has encountered a
   turning point mainly due to advancements in machine learning, which
   allows machines to learn, improve, and perform a specific task through
   data without being explicitly programmed. Machine learning can be
   utilized with machining processes to improve product quality levels and
   productivity rates, to monitor the health of systems, and to optimize
   design and process parameters. This is known as smart machining,
   referring to a new machining paradigm in which machine tools are fully
   connected through a cyber-physical system. This paper reviews and
   summarizes machining processes using machine learning algorithms and
   suggests a perspective on the machining industry.},
DOI = {10.1007/s40684-018-0057-y},
ISSN = {2288-6206},
EISSN = {2198-0810},
ResearcherID-Numbers = {Bhandari, Binayak/AAE-5605-2019
   Ahn, Sung-Hoon/GLS-7239-2022
   Quan, Yingjun/GLS-7192-2022
   Bhandari, Binayak/GXF-8648-2022
   Yang, Insoon/KPA-1530-2024
   Kim, Hyungjung/AAA-9375-2020},
ORCID-Numbers = {Ahn, Sung-Hoon/0000-0002-1548-2394
   Bhandari, Binayak/0000-0002-4046-2379
   Quan, Yingjun/0000-0001-7354-4736
   Kim, Mincheol/0000-0001-8469-123X
   Kim, Hyungjung/0000-0003-0895-6886
   },
Unique-ID = {WOS:000448233900010},
}

@article{ WOS:000424709000002,
Author = {Stilgoe, Jack},
Title = {Machine learning, social learning and the governance of self-driving
   cars},
Journal = {SOCIAL STUDIES OF SCIENCE},
Year = {2018},
Volume = {48},
Number = {1},
Pages = {25-56},
Month = {FEB},
Abstract = {Self-driving cars, a quintessentially smart' technology, are not born
   smart. The algorithms that control their movements are learning as the
   technology emerges. Self-driving cars represent a high-stakes test of
   the powers of machine learning, as well as a test case for social
   learning in technology governance. Society is learning about the
   technology while the technology learns about society. Understanding and
   governing the politics of this technology means asking Who is learning,
   what are they learning and how are they learning?' Focusing on the
   successes and failures of social learning around the much-publicized
   crash of a Tesla Model S in 2016, I argue that trajectories and
   rhetorics of machine learning in transport pose a substantial governance
   challenge. Self-driving' or autonomous' cars are misnamed. As with other
   technologies, they are shaped by assumptions about social needs,
   solvable problems, and economic opportunities. Governing these
   technologies in the public interest means improving social learning by
   constructively engaging with the contingencies of machine learning.},
DOI = {10.1177/0306312717741687},
ISSN = {0306-3127},
EISSN = {1460-3659},
Unique-ID = {WOS:000424709000002},
}

@article{ WOS:000499323200025,
Author = {Deist, Timo M. and Patti, Andrew and Wang, Zhaoqi and Krane, David and
   Sorenson, Taylor and Craft, David},
Title = {Simulation-assisted machine learning},
Journal = {BIOINFORMATICS},
Year = {2019},
Volume = {35},
Number = {20},
Pages = {4072-4080},
Month = {OCT 15},
Abstract = {Motivation: In a predictive modeling setting, if sufficient details of
   the system behavior are known, one can build and use a simulation for
   making predictions. When sufficient system details are not known, one
   typically turns to machine learning, which builds a black-box model of
   the system using a large dataset of input sample features and outputs.
   We consider a setting which is between these two extremes: some details
   of the system mechanics are known but not enough for creating
   simulations that can be used to make high quality predictions. In this
   context we propose using approximate simulations to build a kernel for
   use in kernelized machine learning methods, such as support vector
   machines. The results of multiple simulations (under various uncertainty
   scenarios) are used to compute similarity measures between every pair of
   samples: sample pairs are given a high similarity score if they behave
   similarly under a wide range of simulation parameters. These similarity
   values, rather than the original high dimensional feature data, are used
   to build the kernel.
   Results: We demonstrate and explore the simulation-based kernel
   (SimKern) concept using four synthetic complex systems-three
   biologically inspired models and one network flow optimization model. We
   show that, when the number of training samples is small compared to the
   number of features, the SimKern approach dominates over
   no-prior-knowledge methods. This approach should be applicable in all
   disciplines where predictive models are sought and informative yet
   approximate simulations are available.},
DOI = {10.1093/bioinformatics/btz199},
ISSN = {1367-4803},
EISSN = {1367-4811},
ResearcherID-Numbers = {Krane, Dale/M-5461-2019},
ORCID-Numbers = {Craft, David/0000-0003-3093-718X
   },
Unique-ID = {WOS:000499323200025},
}

@article{ WOS:000710554200001,
Author = {Mengoni, Riccardo and Di Pierro, Alessandra},
Title = {Kernel methods in Quantum Machine Learning},
Journal = {QUANTUM MACHINE INTELLIGENCE},
Year = {2019},
Volume = {1},
Number = {3-4},
Pages = {65-71},
Month = {DEC},
Abstract = {Quantum Machine Learning has established itself as one of the most
   promising applications of quantum computers and Noisy Intermediate Scale
   Quantum (NISQ) devices. In this paper, we review the latest developments
   regarding the usage of quantum computing for a particular class of
   machine learning algorithms known as kernel methods.},
DOI = {10.1007/s42484-019-00007-4},
ISSN = {2524-4906},
EISSN = {2524-4914},
ResearcherID-Numbers = {DI PIERRO, Alessandra/LVR-0942-2024
   Di Pierro, Alessandra/LVR-0942-2024},
ORCID-Numbers = {Mengoni, Riccardo/0000-0003-2717-7298
   DI PIERRO, Alessandra/0000-0003-4173-7941
   },
Unique-ID = {WOS:000710554200001},
}

@article{ WOS:000483104800010,
Author = {Hasan, Ali and Moin, Sana and Karim, Ahmad and Shamshirband, Shahaboddin},
Title = {Machine Learning-Based Sentiment Analysis for Twitter Accounts},
Journal = {MATHEMATICAL AND COMPUTATIONAL APPLICATIONS},
Year = {2018},
Volume = {23},
Number = {1},
Month = {MAR},
Abstract = {Growth in the area of opinion mining and sentiment analysis has been
   rapid and aims to explore the opinions or text present on different
   platforms of social media through machine-learning techniques with
   sentiment, subjectivity analysis or polarity calculations. Despite the
   use of various machine-learning techniques and tools for sentiment
   analysis during elections, there is a dire need for a state-of-the-art
   approach. To deal with these challenges, the contribution of this paper
   includes the adoption of a hybrid approach that involves a sentiment
   analyzer that includes machine learning. Moreover, this paper also
   provides a comparison of techniques of sentiment analysis in the
   analysis of political views by applying supervised machine-learning
   algorithms such as Naive Bayes and support vector machines (SVM).},
DOI = {10.3390/mca23010011},
Article-Number = {11},
ISSN = {1300-686X},
EISSN = {2297-8747},
ResearcherID-Numbers = {S.Band, Shahab/ABI-7388-2020},
ORCID-Numbers = {Hasan, Ali/0000-0001-5711-1164
   Shamshirband, Shahaboddin/0000-0002-6605-498X
   },
Unique-ID = {WOS:000483104800010},
}

@article{ WOS:000391480800001,
Author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and
   Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, D.
   B. and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and
   Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar,
   Ameet},
Title = {MLlib: Machine Learning in Apache Spark},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2016},
Volume = {17},
Abstract = {Apache Spark is a popular open-source platform for large-scale data
   processing that is well-suited for iterative machine learning tasks. In
   this paper we present MLlib, Spark's open-source distributed machine
   learning library. MLlib provides effcient functionality fo wide range of
   learning settings and includes several underlying statistical,
   optimization, and linear algebra primitives. Shipped with Spark, MLlib
   supports several languages and provides a high-level API that leverages
   Spark's rich ecosystem to simplify the development of end-to-end machine
   learning pipelines. MLlib has experienced a rapid growth due to its
   vibrant open-source community of over 140 contributors, and includes
   extensive documentation to support further growth and to let users
   quickly get up to speed.},
Article-Number = {34},
ISSN = {1532-4435},
ResearcherID-Numbers = {Xin, Doris/MAH-4837-2025},
Unique-ID = {WOS:000391480800001},
}

@article{ WOS:000391065600020,
Author = {Luo, Wei and Phung, Dinh and Tran, Truyen and Gupta, Sunil and Rana,
   Santu and Karmakar, Chandan and Shilton, Alistair and Yearwood, John and
   Dimitrova, Nevenka and Ho, Tu Bao and Venkatesh, Svetha and Berk,
   Michael},
Title = {Guidelines for Developing and Reporting Machine Learning Predictive
   Models in Biomedical Research: A Multidisciplinary View},
Journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
Year = {2016},
Volume = {18},
Number = {12},
Month = {DEC},
Abstract = {Background: As more and more researchers are turning to big data for new
   opportunities of biomedical discoveries, machine learning models, as the
   backbone of big data analysis, are mentioned more often in biomedical
   journals. However, owing to the inherent complexity of machine learning
   methods, they are prone to misuse. Because of the flexibility in
   specifying machine learning models, the results are often insufficiently
   reported in research articles, hindering reliable assessment of model
   validity and consistent interpretation of model outputs.
   Objective: To attain a set of guidelines on the use of machine learning
   predictive models within clinical settings to make sure the models are
   correctly applied and sufficiently reported so that true discoveries can
   be distinguished from random coincidence.
   Methods: A multidisciplinary panel of machine learning experts,
   clinicians, and traditional statisticians were interviewed, using an
   iterative process in accordance with the Delphi method.
   Results: The process produced a set of guidelines that consists of (1) a
   list of reporting items to be included in a research article and (2) a
   set of practical sequential steps for developing predictive models.
   Conclusions: A set of guidelines was generated to enable correct
   application of machine learning models and consistent reporting of model
   specifications and results in biomedical research. We believe that such
   guidelines will accelerate the adoption of big data analysis,
   particularly with machine learning methods, in the biomedical research
   community.},
DOI = {10.2196/jmir.5870},
Article-Number = {e323},
ISSN = {1438-8871},
ResearcherID-Numbers = {Luo, Wei/A-6043-2011
   Rana, Santu/R-2992-2019
   Yearwood, John/HPB-5213-2023
   Berk, Michael/M-7891-2013
   Kumar, Chandan/HJB-3782-2022
   Berk, Michael/AGH-9427-2022
   Phung, Dinh/D-1328-2012
   },
ORCID-Numbers = {Luo, Wei/0000-0002-4711-7543
   Tran, Truyen/0000-0001-6531-8907
   Karmakar, Chandan/0000-0003-1814-0856
   Berk, Michael/0000-0002-5554-6946
   gupta, sunil/0000-0002-4669-9940
   Phung, Dinh/0000-0002-9977-8247
   Yearwood, John/0000-0002-7562-6767
   Rana, Santu/0000-0003-2247-850X
   Venkatesh, Svetha/0000-0001-8675-6631},
Unique-ID = {WOS:000391065600020},
}

@article{ WOS:000433269000002,
Author = {Guo, Yabin and Wang, Jiangyu and Chen, Huanxin and Li, Guannan and Liu,
   Jiangyan and Xu, Chengliang and Huang, Ronggeng and Huang, Yao},
Title = {Machine learning-based thermal response time ahead energy demand
   prediction for building heating systems},
Journal = {APPLIED ENERGY},
Year = {2018},
Volume = {221},
Pages = {16-27},
Month = {JUL 1},
Note = {9th International Conference on Applied Energy (ICAE), Cardiff, ENGLAND,
   AUG 21-24, 2017},
Abstract = {Energy demand prediction of building heating is conducive to optimal
   control, fault detection and diagnosis and building intelligentization.
   In this study, energy demand prediction models are developed through
   machine learning methods, including extreme learning machine, multiple
   linear regression, support vector regression and backpropagation neural
   network. Seven different meteorological parameters, operating
   parameters, time and indoor temperature parameters are used as feature
   variables of the model. Correlation analysis method is utilized to
   optimize the feature sets. Moreover, this paper proposes a strategy for
   obtaining the thermal response time of building, which is used as the
   time ahead of prediction models. The prediction performances of extreme
   learning machine models with various hidden layer nodes are analyzed and
   contrasted. Actual data of building heating using a ground source heat
   pump system are collected and used to test the performances of the
   models. Results show that the thermal response time of the building is
   approximately 40 min. Four feature sets are obtained, and the
   performances of the models with feature set 4 are better. For different
   machine learning methods, the performances of extreme learning machine
   models are better than others. In addition, the optimal number of hidden
   layer nodes is 11 for the extreme learning machine model with feature
   set 4.},
DOI = {10.1016/j.apenergy.2018.03.125},
ISSN = {0306-2619},
EISSN = {1872-9118},
ResearcherID-Numbers = {Li, Guannan/GXN-2627-2022
   },
ORCID-Numbers = {guo, ya bin/0000-0002-4350-1396
   Liu, Jiangyan/0000-0002-4553-4124},
Unique-ID = {WOS:000433269000002},
}

@article{ WOS:000425839800026,
Author = {Lakhani, Paras and Prater, Adam B. and Hutson, R. Kent and Andriole,
   Kathy P. and Dreyer, Keith J. and Morey, Jose and Prevedello, Luciano M.
   and Clark, Toshi J. and Geis, J. Raymond and Itri, Jason N. and Hawkins,
   C. Matthew},
Title = {Machine Learning in Radiology: Applications Beyond Image Interpretation},
Journal = {JOURNAL OF THE AMERICAN COLLEGE OF RADIOLOGY},
Year = {2018},
Volume = {15},
Number = {2},
Pages = {350-359},
Month = {FEB},
Abstract = {Much attention has been given to machine learning and its perceived
   impact in radiology, particularly in light of recent success with image
   classification in international competitions. However, machine learning
   is likely to impact radiology outside of image interpretation long
   before a fully functional ``machine radiologist{''} is implemented in
   practice. Here, we describe an overview of machine learning, its
   application to radiology and other domains, and many cases of use that
   do not involve image interpretation. We hope that better understanding
   of these potential applications will help radiology practices prepare
   for the future and realize performance improvement and efficiency gains.},
DOI = {10.1016/j.jacr.2017.09.044},
ISSN = {1546-1440},
EISSN = {1558-349X},
ResearcherID-Numbers = {Andriole, Katherine/AAM-3166-2020
   Prevedello, Luciano/G-8650-2014
   Hawkins, Matt/HPC-2860-2023},
ORCID-Numbers = {Prevedello, Luciano/0000-0002-6768-6452
   },
Unique-ID = {WOS:000425839800026},
}

@article{ WOS:000416496500008,
Author = {Zhang, Lu and Tan, Jianjun and Han, Dan and Zhu, Hao},
Title = {From machine learning to deep learning: progress in machine intelligence
   for rational drug discovery},
Journal = {DRUG DISCOVERY TODAY},
Year = {2017},
Volume = {22},
Number = {11},
Pages = {1680-1685},
Month = {NOV},
Abstract = {Machine intelligence, which is normally presented as artificial
   intelligence, refers to the intelligence exhibited by computers. In the
   history of rational drug discovery, various machine intelligence
   approaches have been applied to guide traditional experiments, which are
   expensive and time-consuming. Over the past several decades,
   machine-learning tools, such as quantitative structure activity
   relationship (QSAR) modeling, were developed that can identify potential
   biological active molecules from millions of candidate compounds quickly
   and cheaply. However, when drug discovery moved into the era of `big'
   data, machine learning approaches evolved into deep learning approaches,
   which are a more powerful and efficient way to deal with the massive
   amounts of data generated from modern drug discovery approaches. Here,
   we summarize the history of machine learning and provide insight into
   recently developed deep learning approaches and their applications in
   rational drug discovery. We suggest that this evolution of machine
   intelligence now provides a guide for early-stage drug design and
   discovery in the current big data era.},
DOI = {10.1016/j.drudis.2017.08.010},
ISSN = {1359-6446},
EISSN = {1878-5832},
ResearcherID-Numbers = {Tan, Jianjun/E-5867-2011
   zh, H/HIK-0903-2022
   },
ORCID-Numbers = {Tan, Jianjun/0000-0003-3573-0807
   Zhu, Hao/0000-0002-3559-6129},
Unique-ID = {WOS:000416496500008},
}

@article{ WOS:001190989000001,
Author = {Dehghani, Mohammad and Yazdanparast, Zahra},
Title = {From distributed machine to distributed deep learning: a comprehensive
   survey},
Journal = {JOURNAL OF BIG DATA},
Year = {2023},
Volume = {10},
Number = {1},
Month = {OCT 13},
Abstract = {Artificial intelligence has made remarkable progress in handling complex
   tasks, thanks to advances in hardware acceleration and machine learning
   algorithms. However, to acquire more accurate outcomes and solve more
   complex issues, algorithms should be trained with more data. Processing
   this huge amount of data could be time-consuming and require a great
   deal of computation. To address these issues, distributed machine
   learning has been proposed, which involves distributing the data and
   algorithm across several machines. There has been considerable effort
   put into developing distributed machine learning algorithms, and
   different methods have been proposed so far. We divide these algorithms
   in classification and clustering (traditional machine learning), deep
   learning and deep reinforcement learning groups. Distributed deep
   learning has gained more attention in recent years and most of the
   studies have focused on this approach. Therefore, we mostly concentrate
   on this category. Based on the investigation of the mentioned
   algorithms, we highlighted the limitations that should be addressed in
   future research.},
DOI = {10.1186/s40537-023-00829-x},
Article-Number = {158},
EISSN = {2196-1115},
Unique-ID = {WOS:001190989000001},
}

@article{ WOS:000442625700004,
Author = {Cuperlovic-Culf, Miroslava},
Title = {Machine Learning Methods for Analysis of Metabolic Data and Metabolic
   Pathway Modeling},
Journal = {METABOLITES},
Year = {2018},
Volume = {8},
Number = {1},
Month = {MAR},
Abstract = {Machine learning uses experimental data to optimize clustering or
   classification of samples or features, or to develop, augment or verify
   models that can be used to predict behavior or properties of systems. It
   is expected that machine learning will help provide actionable knowledge
   from a variety of big data including metabolomics data, as well as
   results of metabolism models. A variety of machine learning methods has
   been applied in bioinformatics and metabolism analyses including
   self-organizing maps, support vector machines, the kernel machine,
   Bayesian networks or fuzzy logic. To a lesser extent, machine learning
   has also been utilized to take advantage of the increasing availability
   of genomics and metabolomics data for the optimization of metabolic
   network models and their analysis. In this context, machine learning has
   aided the development of metabolic networks, the calculation of
   parameters for stoichiometric and kinetic models, as well as the
   analysis of major features in the model for the optimal application of
   bioreactors. Examples of this very interesting, albeit highly complex,
   application of machine learning for metabolism modeling will be the
   primary focus of this review presenting several different types of
   applications for model optimization, parameter determination or system
   analysis using models, as well as the utilization of several different
   types of machine learning technologies.},
DOI = {10.3390/metabo8010004},
Article-Number = {4},
EISSN = {2218-1989},
ResearcherID-Numbers = {Cuperlovic-Culf, Miroslava/C-7876-2016},
Unique-ID = {WOS:000442625700004},
}

@article{ WOS:000376943000001,
Author = {Qiu, Junfei and Wu, Qihui and Ding, Guoru and Xu, Yuhua and Feng, Shuo},
Title = {A survey of machine learning for big data processing},
Journal = {EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING},
Year = {2016},
Month = {MAY 28},
Abstract = {There is no doubt that big data are now rapidly expanding in all science
   and engineering domains. While the potential of these massive data is
   undoubtedly significant, fully making sense of them requires new ways of
   thinking and novel learning techniques to address the various
   challenges. In this paper, we present a literature survey of the latest
   advances in researches on machine learning for big data processing.
   First, we review the machine learning techniques and highlight some
   promising learning methods in recent studies, such as representation
   learning, deep learning, distributed and parallel learning, transfer
   learning, active learning, and kernel-based learning. Next, we focus on
   the analysis and discussions about the challenges and possible solutions
   of machine learning for big data. Following that, we investigate the
   close connections of machine learning with signal processing techniques
   for big data processing. Finally, we outline several open issues and
   research trends.},
DOI = {10.1186/s13634-016-0355-x},
Article-Number = {67},
ISSN = {1687-6180},
ResearcherID-Numbers = {Qiu, Junfei/AAF-4659-2019
   wu, qihui/KCL-7173-2024
   Xu, Yuhua/G-5310-2010},
ORCID-Numbers = {, Guoru/0000-0003-1780-2547
   },
Unique-ID = {WOS:000376943000001},
}

@article{ WOS:000479003300133,
Author = {Padmanabhan, Meghana and Yuan, Pengyu and Chada, Govind and Hien Van
   Nguyen},
Title = {Physician-Friendly Machine Learning: A Case Study with Cardiovascular
   Disease Risk Prediction},
Journal = {JOURNAL OF CLINICAL MEDICINE},
Year = {2019},
Volume = {8},
Number = {7},
Month = {JUL},
Abstract = {Machine learning is often perceived as a sophisticated technology
   accessible only by highly trained experts. This prevents many physicians
   and biologists from using this tool in their research. The goal of this
   paper is to eliminate this out-dated perception. We argue that the
   recent development of auto machine learning techniques enables
   biomedical researchers to quickly build competitive machine learning
   classifiers without requiring in-depth knowledge about the underlying
   algorithms. We study the case of predicting the risk of cardiovascular
   diseases. To support our claim, we compare auto machine learning
   techniques against a graduate student using several important metrics,
   including the total amounts of time required for building machine
   learning models and the final classification accuracies on unseen test
   datasets. In particular, the graduate student manually builds multiple
   machine learning classifiers and tunes their parameters for one month
   using scikit-learn library, which is a popular machine learning library
   to obtain ones that perform best on two given, publicly available
   datasets. We run an auto machine learning library called auto-sklearn on
   the same datasets. Our experiments find that automatic machine learning
   takes 1 h to produce classifiers that perform better than the ones built
   by the graduate student in one month. More importantly, building this
   classifier only requires a few lines of standard code. Our findings are
   expected to change the way physicians see machine learning and encourage
   wide adoption of Artificial Intelligence (AI) techniques in clinical
   domains.},
DOI = {10.3390/jcm8071050},
Article-Number = {1050},
EISSN = {2077-0383},
ResearcherID-Numbers = {Yuan, Pengyu/JYP-6611-2024
   },
ORCID-Numbers = {Yuan, Pengyu/0000-0002-9589-411X
   Nguyen Van, Hien/0000-0001-7280-2182},
Unique-ID = {WOS:000479003300133},
}

@article{ WOS:000430565600012,
Author = {Awan, Saqib Ejaz and Sohel, Ferdous and Sanfilippo, Frank Mario and
   Bennamoun, Mohammed and Dwivedi, Girish},
Title = {Machine learning in heart failure: ready for prime time},
Journal = {CURRENT OPINION IN CARDIOLOGY},
Year = {2018},
Volume = {33},
Number = {2},
Pages = {190-195},
Month = {MAR},
Abstract = {Purpose of review
   The aim of this review is to present an up-to-date overview of the
   application of machine learning methods in heart failure including
   diagnosis, classification, readmissions and medication adherence.
   Recent findings
   Recent studies have shown that the application of machine learning
   techniques may have the potential to improve heart failure outcomes and
   management, including cost savings by improving existing diagnostic and
   treatment support systems. Recently developed deep learning methods are
   expected to yield even better performance than traditional machine
   learning techniques in performing complex tasks by learning the
   intricate patterns hidden in big medical data.
   Summary
   The review summarizes the recent developments in the application of
   machine and deep learning methods in heart failure management.},
DOI = {10.1097/HCO.0000000000000491},
ISSN = {0268-4705},
EISSN = {1531-7080},
ResearcherID-Numbers = {Sohel, Ferdous/C-2428-2013
   Dwivedi, Girish/AAF-2726-2021
   Bennamoun, Mohammed/C-2789-2013
   Sanfilippo, Frank/H-9334-2013
   },
ORCID-Numbers = {Dwivedi, Girish/0000-0003-0717-740X
   Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257
   Sanfilippo, Frank/0000-0003-3639-0787
   Awan, Saqib Ejaz/0000-0002-5759-6832},
Unique-ID = {WOS:000430565600012},
}

@article{ WOS:000427315000007,
Author = {Chang, Spencer and Cohen, Timothy and Ostdiek, Bryan},
Title = {What is the machine learning?},
Journal = {PHYSICAL REVIEW D},
Year = {2018},
Volume = {97},
Number = {5},
Month = {MAR 13},
Abstract = {Applications of machine learning tools to problems of physical interest
   are often criticized for producing sensitivity at the expense of
   transparency. To address this concern, we explore a data planing
   procedure for identifying combinations of variables-aided by physical
   intuition-that can discriminate signal from background. Weights are
   introduced to smooth away the features in a given variable(s). New
   networks are then trained on this modified data. Observed decreases in
   sensitivity diagnose the variable's discriminating power. Planing also
   allows the investigation of the linear versus nonlinear nature of the
   boundaries between signal and background. We demonstrate the efficacy of
   this approach using a toy example, followed by an application to an
   idealized heavy resonance scenario at the Large Hadron Collider. By
   unpacking the information being utilized by these algorithms, this
   method puts in context what it means for a machine to learn.},
DOI = {10.1103/PhysRevD.97.056009},
Article-Number = {056009},
ISSN = {2470-0010},
EISSN = {2470-0029},
ORCID-Numbers = {Ostdiek, Bryan/0000-0002-0376-6461
   Cohen, Timothy/0000-0002-7040-3038
   Chang, Spencer/0000-0002-2727-6723},
Unique-ID = {WOS:000427315000007},
}

@article{ WOS:000435911300004,
Author = {Tamke, Martin and Nicholas, Paul and Zwierzycki, Mateusz},
Title = {Machine learning for architectural design: Practices and infrastructure},
Journal = {INTERNATIONAL JOURNAL OF ARCHITECTURAL COMPUTING},
Year = {2018},
Volume = {16},
Number = {2, SI},
Pages = {123-143},
Month = {JUN},
Abstract = {In this article, we propose that new architectural design practices
   might be based on machine learning approaches to better leverage
   data-rich environments and workflows. Through reference to recent
   architectural research, we describe how the application of machine
   learning can occur throughout the design and fabrication process, to
   develop varied relations between design, performance and learning. The
   impact of machine learning on architectural practices with
   performance-based design and fabrication is assessed in two cases by the
   authors. We then summarise what we perceive as current limits to a more
   widespread application and conclude by providing an outlook and
   direction for future research for machine learning in architectural
   design practice.},
DOI = {10.1177/1478077118778580},
ISSN = {1478-0771},
EISSN = {2048-3988},
ORCID-Numbers = {Tamke, Martin/0000-0003-1209-4967
   Nicholas, Paul/0000-0003-2420-8351},
Unique-ID = {WOS:000435911300004},
}

@article{ WOS:000478765300004,
Author = {Weigand, Alois},
Title = {Machine learning in empirical asset pricing},
Journal = {FINANCIAL MARKETS AND PORTFOLIO MANAGEMENT},
Year = {2019},
Volume = {33},
Number = {1},
Pages = {93-104},
Month = {MAR},
Abstract = {The tremendous speedup in computing in recent years, the low data
   storage costs of today, the availability of ``big data{''} as well as
   the broad range of free open-source software, have created a renaissance
   in the application of machine learning techniques in science. However,
   this new wave of research is not limited to computer science or software
   engineering anymore. Among others, machine learning tools are now used
   in financial problem settings as well. Therefore, this paper mentions a
   specific definition of machine learning in an asset pricing context and
   elaborates on the usefulness of machine learning in this context. Most
   importantly, the literature review gives the reader a theoretical
   overview of the most recent academic studies in empirical asset pricing
   that employ machine learning techniques. Overall, the paper concludes
   that machine learning can offer benefits for future research. However,
   researchers should be critical about these methodologies as machine
   learning has its pitfalls and is relatively new to asset pricing.},
DOI = {10.1007/s11408-019-00326-3},
ISSN = {1934-4554},
EISSN = {2373-8529},
ORCID-Numbers = {Weigand, Alois/0000-0003-4679-3619},
Unique-ID = {WOS:000478765300004},
}

@article{ WOS:000510726600002,
Author = {Chase, Hunter and Freitag, James},
Title = {MODEL THEORY AND MACHINE LEARNING},
Journal = {BULLETIN OF SYMBOLIC LOGIC},
Year = {2019},
Volume = {25},
Number = {3},
Pages = {319-332},
Month = {SEP},
Abstract = {About 25 years ago, it came to light that a single combinatorial
   property determines both an important dividing line in model theory
   (NIP) and machine learning (PAC-learnability). The following years saw a
   fruitful exchange of ideas between PAC-learning and the model theory of
   NIP structures. In this article, we point out a new and similar
   connection between model theory and machine learning, this time
   developing a correspondence between stability and learnability in
   various settings of online learning. In particular, this gives many new
   examples of mathematically interesting classes which are learnable in
   the online setting.},
DOI = {10.1017/bsl.2018.71},
ISSN = {1079-8986},
EISSN = {1943-5894},
ORCID-Numbers = {Freitag, James/0000-0002-7457-3122},
Unique-ID = {WOS:000510726600002},
}

@article{ WOS:000430730700007,
Author = {Elnaggar, Rana and Chakrabarty, Krishnendu},
Title = {Machine Learning for Hardware Security: Opportunities and Risks},
Journal = {JOURNAL OF ELECTRONIC TESTING-THEORY AND APPLICATIONS},
Year = {2018},
Volume = {34},
Number = {2},
Pages = {183-201},
Month = {APR},
Abstract = {Recently, machine learning algorithms have been utilized by system
   defenders and attackers to secure and attack hardware, respectively. In
   this work, we investigate the impact of machine learning on hardware
   security. We explore the defense and attack mechanisms for hardware that
   are based on machine learning. Moreover, we identify suitable machine
   learning algorithms for each category of hardware security problems.
   Finally, we highlight some important aspects related to the application
   of machine learning to hardware security problems and show how the
   practice of applying machine learning to hardware security problems has
   changed over the past decade.},
DOI = {10.1007/s10836-018-5726-9},
ISSN = {0923-8174},
EISSN = {1573-0727},
ResearcherID-Numbers = {Chakrabarty, Krishnendu/J-6086-2012
   },
ORCID-Numbers = {Chakrabarty, Krishnendu/0000-0003-4475-6435},
Unique-ID = {WOS:000430730700007},
}

@article{ WOS:000472240500010,
Author = {Akbilgic, Oguz and Davis, Robert L.},
Title = {The Promise of Machine Learning: When Will it be Delivered?},
Journal = {JOURNAL OF CARDIAC FAILURE},
Year = {2019},
Volume = {25},
Number = {6},
Pages = {484-485},
Month = {JUN},
Abstract = {Background: The real-life applications of machine learning clinical
   decision making is currently lagging behind its promise. One of the
   critics on machine learning is that it doesn't outperform more
   traditional statistical approaches in every problem.
   Methods and Results: Authors of ``Predictive Abilities of Machine
   Learning Techniques May Be Limited by Dataset Characteristics: Insights
   From the UNOS Database{''} presented in the current issue of the Journal
   of Cardiac Failure that machine learning approaches do not provide
   significantly higher performance when compared to more traditional
   statistical approaches in predicting mortality following heart
   transplant. In this brief report, we provide an insight on the possible
   reasons for why machine learning methods do not outperform more
   traditional approaches for every problem and every dataset.
   Conclusions: Most of the performance-focused critics on machine learning
   are because the bar is set unfairly too high for machine learning. In
   most cases, machine learning methods provides at least as good results
   as traditional statistical methods do. It is normal for machine learning
   models to provide similar performance with linear models if the actual
   underlying input-outcome relationship is linear. Moreover, machine
   learning methods outperforms linear statistical models when the
   underlying input-output relationship is not linear and if the dataset is
   large enough and include predictors capturing that nonlinear
   relationship.},
DOI = {10.1016/j.cardfail.2019.04.006},
ISSN = {1071-9164},
EISSN = {1532-8414},
ResearcherID-Numbers = {akbilgic, oguz/F-9407-2013},
ORCID-Numbers = {akbilgic, oguz/0000-0003-0313-9254},
Unique-ID = {WOS:000472240500010},
}

@article{ WOS:000895924400002,
Author = {Yu, Ning and Li, Zhihua and Yu, Zeng},
Title = {Survey on Encoding Schemes for Genomic Data Representation and Feature
   Learning-From Signal Processing to Machine Learning},
Journal = {BIG DATA MINING AND ANALYTICS},
Year = {2018},
Volume = {1},
Number = {3},
Pages = {191-210},
Month = {SEP},
Abstract = {Data-driven machine learning, especially deep learning technology, is
   becoming an important tool for handling big data issues in
   bioinformatics. In machine learning, DNA sequences are often converted
   to numerical values for data representation and feature learning in
   various applications. Similar conversion occurs in Genomic Signal
   Processing (GSP), where genome sequences are transformed into numerical
   sequences for signal extraction and recognition. This kind of conversion
   is also called encoding scheme. The diverse encoding schemes can greatly
   affect the performance of GSP applications and machine learning models.
   This paper aims to collect, analyze, discuss, and summarize the existing
   encoding schemes of genome sequence particularly in GSP as well as other
   genome analysis applications to provide a comprehensive reference for
   the genomic data representation and feature learning in machine
   learning.},
DOI = {10.26599/BDMA.2018.9020018},
EISSN = {2096-0654},
ResearcherID-Numbers = {Li, Zh/GLR-2952-2022},
Unique-ID = {WOS:000895924400002},
}

@article{ WOS:000405098300001,
Author = {Weinan, E.},
Title = {A Proposal on Machine Learning via Dynamical Systems},
Journal = {COMMUNICATIONS IN MATHEMATICS AND STATISTICS},
Year = {2017},
Volume = {5},
Number = {1},
Pages = {1-11},
Month = {MAR},
Abstract = {We discuss the idea of using continuous dynamical systems to model
   general high-dimensional nonlinear functions used in machine learning.
   We also discuss the connection with deep learning.},
DOI = {10.1007/s40304-017-0103-z},
ISSN = {2194-6701},
EISSN = {2194-671X},
Unique-ID = {WOS:000405098300001},
}

@article{ WOS:000454710400002,
Author = {Majaj, Najib J. and Pelli, Denis G.},
Title = {Deep learning-Using machine learning to study biological vision},
Journal = {JOURNAL OF VISION},
Year = {2018},
Volume = {18},
Number = {13},
Month = {DEC},
Abstract = {Many vision science studies employ machine learning, especially the
   version called ``deep learning.{''} Neuroscientists use machine learning
   to decode neural responses. Perception scientists try to understand how
   living organisms recognize objects. To them, deep neural networks offer
   benchmark accuracies for recognition of learned stimuli. Originally
   machine learning was inspired by the brain. Today, machine learning is
   used as a statistical tool to decode brain activity. Tomorrow, deep
   neural networks might become our best model of brain function. This
   brief overview of the use of machine learning in biological vision
   touches on its strengths, weaknesses, milestones, controversies, and
   current directions. Here, we hope to help vision scientists assess what
   role machine learning should play in their research.},
DOI = {10.1167/18.13.2},
Article-Number = {2},
ISSN = {1534-7362},
Unique-ID = {WOS:000454710400002},
}

@article{ WOS:000348105800001,
Author = {Abraham, Alexandre and Pedregosa, Fabian and Eickenberg, Michael and
   Gervais, Philippe and Mueller, Andreas and Kossaifi, Jean and Gramfort,
   Alexandre and Thirion, Bertrand and Varoquaux, Gael},
Title = {Machine learning for neuroirnaging with scikit-learn},
Journal = {FRONTIERS IN NEUROINFORMATICS},
Year = {2014},
Volume = {8},
Month = {FEB 21},
Abstract = {Statistical machine learning methods are increasingly used for
   neuroimaging data analysis. Their main virtue is their ability to model
   high-dimensional datasets, e.g., multivariate analysis of activation
   images or resting-state time series. Supervised learning is typically
   used in decoding or encoding settings to relate brain images to
   behavioral or clinical observations, while unsupervised learning can
   uncover hidden structures in sets of images (e.g., resting state
   functional MRI) or find sub-populations in large cohorts. By considering
   different functional neuroimaging applications, we illustrate how
   scikit-learn, a Python machine learning library, can be used to perform
   some key analysis steps. Scikit-learn contains a very large set of
   statistical learning algorithms, both supervised and unsupervised, and
   its application to neuroimaging data provides a versatile tool to study
   the brain.},
DOI = {10.3389/fninf.2014.00014},
Article-Number = {14},
EISSN = {1662-5196},
ResearcherID-Numbers = {Pedregosa, Fabian/U-3477-2019
   Varoquaux, Gael/ABQ-5456-2022
   Kossaifi, Jean/AAW-8519-2021},
ORCID-Numbers = {Kossaifi, Jean/0000-0002-4445-3429
   },
Unique-ID = {WOS:000348105800001},
}

@article{ WOS:000731150400024,
Author = {Zhu, Lili and Spachos, Petros and Pensini, Erica and Plataniotis,
   Konstantinos N.},
Title = {Deep learning and machine vision for food processing: A survey},
Journal = {CURRENT RESEARCH IN FOOD SCIENCE},
Year = {2021},
Volume = {4},
Pages = {233-249},
Abstract = {The quality and safety of food is an important issue to the whole
   society, since it is at the basis of human health, social development
   and stability. Ensuring food quality and safety is a complex process,
   and all stages of food processing must be considered, from cultivating,
   harvesting and storage to preparation and consumption. However, these
   processes are often labour-intensive. Nowadays, the development of
   machine vision can greatly assist researchers and industries in
   improving the efficiency of food processing. As a result, machine vision
   has been widely used in all aspects of food processing. At the same
   time, image processing is an important component of machine vision.
   Image processing can take advantage of machine learning and deep
   learning models to effectively identify the type and quality of food.
   Subsequently, follow-up design in the machine vision system can address
   tasks such as food grading, detecting locations of defective spots or
   foreign objects, and removing impurities. In this paper, we provide an
   overview on the traditional machine learning and deep learning methods,
   as well as the machine vision techniques that can be applied to the
   field of food processing. We present the current approaches and
   challenges, and the future trends.},
DOI = {10.1016/j.crfs.2021.03.009},
EarlyAccessDate = {APR 2021},
EISSN = {2665-9271},
ResearcherID-Numbers = {Zhu, Li/JTT-9093-2023},
Unique-ID = {WOS:000731150400024},
}

@article{ WOS:000397585800014,
Author = {Kohli, Marc and Prevedello, Luciano M. and Filice, Ross W. and Geis, J.
   Raymond},
Title = {Implementing Machine Learning in Radiology Practice and Research},
Journal = {AMERICAN JOURNAL OF ROENTGENOLOGY},
Year = {2017},
Volume = {208},
Number = {4},
Pages = {754-760},
Month = {APR},
Abstract = {OBJECTIVE. The purposes of this article are to describe concepts that
   radiologists should understand to evaluate machine learning projects,
   including common algorithms, supervised as opposed to unsupervised
   techniques, statistical pitfalls, and data considerations for training
   and evaluation, and to briefly describe ethical dilemmas and legal risk.
   CONCLUSION. Machine learning includes a broad class of computer programs
   that improve with experience. The complexity of creating, training, and
   monitoring machine learning indicates that the success of the algorithms
   will require radiologist involvement for years to come, leading to
   engagement rather than replacement.},
DOI = {10.2214/AJR.16.17224},
ISSN = {0361-803X},
EISSN = {1546-3141},
ResearcherID-Numbers = {Prevedello, Luciano/G-8650-2014
   },
ORCID-Numbers = {Prevedello, Luciano/0000-0002-6768-6452},
Unique-ID = {WOS:000397585800014},
}

@incollection{ WOS:000514284700008,
Author = {Lin, Eugene and Tsai, Shih-Jen},
Editor = {Kim, YK},
Title = {Machine Learning in Neural Networks},
Booktitle = {FRONTIERS IN PSYCHIATRY: ARTIFCIAL INTELLIGENCE, PRECISION MEDICINE, AND
   OTHER PARADIGM SHIFTS},
Series = {Advances in Experimental Medicine and Biology},
Year = {2019},
Volume = {1192},
Pages = {127-137},
Abstract = {Evidence now suggests that precision psychiatry is becoming a
   cornerstone of medical practices by providing the patient of psychiatric
   disorders with the right medication at the right dose at the right time.
   In light of recent advances in neuroimaging and multi-omics, more and
   more biomarkers associated with psychiatric diseases and treatment
   responses are being discovered in precision psychiatry applications by
   leveraging machine learning and neural network approaches. In this
   article, we focus on the most recent developments for research in
   precision psychiatry using machine learning, deep learning, and neural
   network algorithms, together with neuroimaging and multi-omics data.
   First, we describe different machine learning approaches that are
   employed to assess prediction for diagnosis, prognosis, and treatment in
   various precision psychiatry studies. We also survey probable biomarkers
   that have been identified to be involved in psychiatric diseases and
   treatment responses. Furthermore, we summarize the limitations with
   respect to the mentioned precision psychiatry studies. Finally, we
   address a discussion of future directions and challenges.},
DOI = {10.1007/978-981-32-9721-0\_7},
ISSN = {0065-2598},
EISSN = {2214-8019},
ISBN = {978-981-32-9721-0; 978-981-32-9720-3},
ResearcherID-Numbers = {Tsai, Shih-Jen/AAK-7944-2020},
Unique-ID = {WOS:000514284700008},
}

@article{ WOS:000439628500001,
Author = {Fiebrink, Rebecca and Gillies, Marco},
Title = {Introduction to the Special Issue on Human-Centered Machine Learning},
Journal = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS},
Year = {2018},
Volume = {8},
Number = {2, SI},
Month = {JUL},
Abstract = {Machine learning is one of the most important and successful techniques
   in contemporary computer science. Although it can be applied to myriad
   problems of human interest, research in machine learning is often framed
   in an impersonal way, as merely algorithms being applied to model data.
   However, this viewpoint hides considerable human work of tuning the
   algorithms, gathering the data, deciding what should be modeled in the
   first place, and using the outcomes of machine learning in the real
   world. Examining machine learning from a human-centered perspective
   includes explicitly recognizing human work, as well as reframing machine
   learning workflows based on situated human working practices, and
   exploring the co-adaptation of humans and intelligent systems. A
   human-centered understanding of machine learning in human contexts can
   lead not only to more usable machine learning tools, but to new ways of
   understanding what machine learning is good for and how to make it more
   useful. This special issue brings together nine articles that present
   different ways to frame machine learning in a human context. They
   represent very different application areas (from medicine to audio) and
   methodologies (including machine learning methods, human-computer
   interaction methods, and hybrids), but they all explore the human
   contexts in which machine learning is used. This introduction summarizes
   the articles in this issue and draws out some common themes.},
DOI = {10.1145/3205942},
Article-Number = {7},
ISSN = {2160-6455},
EISSN = {2160-6463},
ResearcherID-Numbers = {Fiebrink, Rebecca/A-8317-2017
   Fiebrink, Rebecca/AAU-6856-2021},
ORCID-Numbers = {Fiebrink, Rebecca/0000-0002-7609-2234
   Gillies, Marco/0000-0002-3100-9230
   },
Unique-ID = {WOS:000439628500001},
}

@article{ WOS:000793273600005,
Author = {Zhong, Xianping and Ban, Heng},
Title = {Crack fault diagnosis of rotating machine in nuclear power plant based
   on ensemble learning},
Journal = {ANNALS OF NUCLEAR ENERGY},
Year = {2022},
Volume = {168},
Month = {APR},
Abstract = {Crack faults in rotating machines can cause machine shutdown or
   scrapping, endangering the normal operation and safety of nuclear power
   plants. Intelligent diagnostic techniques based on machine learn-ing
   have the potential to diagnose crack faults. However, problems such as
   scarcity of field fault data and high noise of plant measurements pose
   challenges to the application of machine learning. This study pro -poses
   an ensemble learning approach to mitigate the negative impacts of the
   problems. Ensemble learn-ing is a strategy for combining multiple
   machine learning models into a composite model. The basic idea of
   ensemble learning is that even if one model makes a mistake, other
   models can correct it. Case studies based on bearing and gear system
   fault experiments show that the proposed ensemble learning models have
   better diagnostic results than the single model in the presence of noise
   and small data. (c) 2021 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.anucene.2021.108909},
Article-Number = {108909},
ISSN = {0306-4549},
EISSN = {1873-2100},
Unique-ID = {WOS:000793273600005},
}

@article{ WOS:000383849400001,
Author = {Dunjko, Vedran and Taylor, Jacob M. and Briegel, Hans J.},
Title = {Quantum-Enhanced Machine Learning},
Journal = {PHYSICAL REVIEW LETTERS},
Year = {2016},
Volume = {117},
Number = {13},
Month = {SEP 20},
Abstract = {The emerging field of quantum machine learning has the potential to
   substantially aid in the problems and scope of artificial intelligence.
   This is only enhanced by recent successes in the field of classical
   machine learning. In this work we propose an approach for the systematic
   treatment of machine learning, from the perspective of quantum
   information. Our approach is general and covers all three main branches
   of machine learning: supervised, unsupervised, and reinforcement
   learning. While quantum improvements in supervised and unsupervised
   learning have been reported, reinforcement learning has received much
   less attention. Within our approach, we tackle the problem of quantum
   enhancements in reinforcement learning as well, and propose a systematic
   scheme for providing improvements. As an example, we show that quadratic
   improvements in learning efficiency, and exponential improvements in
   performance over limited time periods, can be obtained for a broad class
   of learning problems.},
DOI = {10.1103/PhysRevLett.117.130501},
Article-Number = {130501},
ISSN = {0031-9007},
EISSN = {1079-7114},
ResearcherID-Numbers = {Taylor, Jacob/O-1670-2019
   },
ORCID-Numbers = {Dunjko, Vedran/0000-0002-2632-7955},
Unique-ID = {WOS:000383849400001},
}

@article{ WOS:000353722300005,
Author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco},
Title = {An introduction to quantum machine learning},
Journal = {CONTEMPORARY PHYSICS},
Year = {2015},
Volume = {56},
Number = {2},
Pages = {172-185},
Month = {APR 3},
Abstract = {Machine learning algorithms learn a desired input-output relation from
   examples in order to interpret new inputs. This is important for tasks
   such as image and speech recognition or strategy optimisation, with
   growing applications in the IT industry. In the last couple of years,
   researchers investigated if quantum computing can help to improve
   classical machine learning algorithms. Ideas range from running
   computationally costly algorithms or their subroutines efficiently on a
   quantum computer to the translation of stochastic methods into the
   language of quantum theory. This contribution gives a systematic
   overview of the emerging field of quantum machine learning. It presents
   the approaches as well as technical details in an accessible way, and
   discusses the potential of a future theory of quantum learning.},
DOI = {10.1080/00107514.2014.964942},
ISSN = {0010-7514},
EISSN = {1366-5812},
ResearcherID-Numbers = {Sinayskiy, Ilya/N-5892-2013
   Petruccione, Francesco/E-6068-2010
   },
ORCID-Numbers = {Sinayskiy, Ilya/0000-0002-3040-0051
   Petruccione, Francesco/0000-0002-8604-0913},
Unique-ID = {WOS:000353722300005},
}

@article{ WOS:000447430700018,
Author = {Luo, Gang},
Title = {A review of automatic selection methods for machine learning algorithms
   and hyper-parameter values},
Journal = {NETWORK MODELING AND ANALYSIS IN HEALTH INFORMATICS AND BIOINFORMATICS},
Year = {2016},
Volume = {5},
Number = {1},
Month = {DEC},
Abstract = {Machine learning studies automatic algorithms that improve themselves
   through experience. It is widely used for analyzing and extracting value
   from large biomedical data sets, or ``big biomedical data,'' advancing
   biomedical research, and improving healthcare. Before a machine learning
   model is trained, the user of a machine learning software tool typically
   must manually select a machine learning algorithm and set one or more
   model parameters termed hyper-parameters. The algorithm and
   hyper-parameter values used can greatly impact the resulting model's
   performance, but their selection requires special expertise as well as
   many labor-intensive manual iterations. To make machine learning
   accessible to layman users with limited computing expertise, computer
   science researchers have proposed various automatic selection methods
   for algorithms and/or hyper-parameter values for a given supervised
   machine learning problem. This paper reviews these methods, identifies
   several of their limitations in the big biomedical data environment, and
   provides preliminary thoughts on how to address these limitations. These
   findings establish a foundation for future research on automatically
   selecting algorithms and hyper-parameter values for analyzing big
   biomedical data.},
DOI = {10.1007/s13721-016-0125-6},
Article-Number = {18},
ISSN = {2192-6662},
EISSN = {2192-6670},
Unique-ID = {WOS:000447430700018},
}

@article{ WOS:000413244600057,
Author = {Malbasa, Vuk and Zheng, Ce and Chen, Po-Chen and Popovic, Tomo and
   Kezunovic, Mladen},
Title = {Voltage Stability Prediction Using Active Machine Learning},
Journal = {IEEE TRANSACTIONS ON SMART GRID},
Year = {2017},
Volume = {8},
Number = {6},
Pages = {3117-3124},
Month = {NOV},
Abstract = {An active machine learning technique for monitoring the voltage
   stability in transmission systems is presented. It has been shown that
   machine learning algorithms may be used to supplement the traditional
   simulation approach, but they suffer from the difficulties of online
   machine learning model update and offline training data preparation. We
   propose an active learning solution to enhance existing machine learning
   applications by actively interacting with the online prediction and
   offline training process. The technique identifies operating points
   where machine learning predictions based on power system measurements
   contradict with actual system conditions. By creating the training set
   around the identified operating points, it is possible to improve the
   capability of machine learning tools to predict future power system
   states. The technique also accelerates the offline training process by
   reducing the amount of simulations on a detailed power system model
   around operating points where correct predictions are made. Experiments
   show a significant advantage in relation to the training time,
   prediction time, and number of measurements that need to be queried to
   achieve high prediction accuracy.},
DOI = {10.1109/TSG.2017.2693394},
ISSN = {1949-3053},
EISSN = {1949-3061},
ResearcherID-Numbers = {ZHENG, CE/P-6534-2018
   Popovic, Tomo/V-1550-2017},
ORCID-Numbers = {Chen, Po-Chen/0000-0002-1459-5821
   Popovic, Tomo/0000-0001-5245-3691},
Unique-ID = {WOS:000413244600057},
}

@article{ WOS:000382418300017,
Author = {de Bruijne, Marleen},
Title = {Machine learning approaches in medical image analysis: From detection to
   diagnosis},
Journal = {MEDICAL IMAGE ANALYSIS},
Year = {2016},
Volume = {33},
Number = {SI},
Pages = {94-97},
Month = {OCT},
Abstract = {Machine learning approaches are increasingly successful in image-based
   diagnosis, disease prognosis, and risk assessment. This paper highlights
   new research directions and discusses three main challenges related to
   machine learning in medical imaging: coping with variation in imaging
   protocols, learning from weak labels, and interpretation and evaluation
   of results. (C) 2016 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.media.2016.06.032},
ISSN = {1361-8415},
EISSN = {1361-8423},
ResearcherID-Numbers = {de Bruijne, Marleen/G-2662-2010},
ORCID-Numbers = {de Bruijne, Marleen/0000-0002-6328-902X},
Unique-ID = {WOS:000382418300017},
}

@article{ WOS:000213214100001,
Author = {Kotsiantis, S. B.},
Title = {Supervised Machine Learning: A Review of Classification Techniques},
Journal = {INFORMATICA-JOURNAL OF COMPUTING AND INFORMATICS},
Year = {2007},
Volume = {31},
Number = {3},
Pages = {249-268},
Month = {OCT},
Abstract = {Supervised machine learning is the search for algorithms that reason
   from externally supplied instances to produce general hypotheses, which
   then make predictions about future instances. In other words, the goal
   of supervised learning is to build a concise model of the distribution
   of class labels in terms of predictor features. The resulting classifier
   is then used to assign class labels to the testing instances where the
   values of the predictor features are known, but the value of the class
   label is unknown. This paper describes various supervised machine
   learning classification techniques. Of course, a single article cannot
   be a complete review of all supervised machine learning classification
   algorithms (also known induction classification algorithms), yet we hope
   that the references cited will cover the major theoretical issues,
   guiding the researcher in interesting research directions and suggesting
   possible bias combinations that have yet to be explored.},
ResearcherID-Numbers = {Kotsiantis, Sotiris/A-8308-2016},
Unique-ID = {WOS:000213214100001},
}

@article{ WOS:000388629300030,
Author = {Joung, Jingon},
Title = {Machine Learning-Based Antenna Selection in Wireless Communications},
Journal = {IEEE COMMUNICATIONS LETTERS},
Year = {2016},
Volume = {20},
Number = {11},
Pages = {2241-2244},
Month = {NOV},
Abstract = {This letter is the first attempt to conflate a machine learning
   technique with wireless communications. Through interpreting the antenna
   selection (AS) in wireless communications (i.e., an optimization-driven
   decision) to multiclass-classification learning (i.e., data-driven
   prediction), and through comparing the learning-based AS using k-nearest
   neighbors and support vector machine algorithms with conventional
   optimization-driven AS methods in terms of communications performance,
   computational complexity, and feedback overhead, we provide insight into
   the potential of fusion of machine learning and wireless communications.},
DOI = {10.1109/LCOMM.2016.2594776},
ISSN = {1089-7798},
EISSN = {1558-2558},
ResearcherID-Numbers = {Joung, Jingon/AAR-9398-2020},
ORCID-Numbers = {Joung, Jingon/0000-0002-9551-1123},
Unique-ID = {WOS:000388629300030},
}

@article{ WOS:000412192800056,
Author = {Wilson, Zachary T. and Sahinidis, Nikolaos V.},
Title = {The ALAMO approach to machine learning},
Journal = {COMPUTERS \& CHEMICAL ENGINEERING},
Year = {2017},
Volume = {106},
Pages = {785-795},
Month = {NOV 2},
Abstract = {ALAMO is a computational methodology for learning algebraic functions
   from data. Given a data set, the approach begins by building a
   low-complexity, linear model composed of explicit non-linear
   transformations of the independent variables. Linear combinations of
   these non-linear transformations allow a linear model to better
   approximate complex behavior observed in real processes. The model is
   refined, as additional data are obtained in an adaptive fashion through
   error maximization sampling using derivative-free optimization. Models
   built using ALAMO can enforce constraints on the response variables to
   incorporate first-principles knowledge. The ability of ALAMO to generate
   simple and accurate models for a number of reaction problems is
   demonstrated. The error maximization sampling is compared with Latin
   hypercube designs to demonstrate its sampling efficiency. ALAMO's
   constrained regression methodology is used to further refine
   concentration models, resulting in models that perform better on
   validation data and satisfy upper and lower bounds placed on model
   outputs. (C) 2017 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.compchemeng.2017.02.010},
ISSN = {0098-1354},
EISSN = {1873-4375},
ResearcherID-Numbers = {Sahinidis, Nikolaos/L-7951-2016
   Sahinidis, Nikolaos/ITV-8956-2023},
ORCID-Numbers = {Sahinidis, Nikolaos/0000-0003-2087-9131
   },
Unique-ID = {WOS:000412192800056},
}

@article{ WOS:000412881200013,
Author = {Comert, Z. and Kocamaz, A. F.},
Title = {Comparison of Machine Learning Techniques for Fetal Heart Rate
   Classification},
Journal = {ACTA PHYSICA POLONICA A},
Year = {2017},
Volume = {132},
Number = {3},
Pages = {451-454},
Month = {SEP},
Note = {3rd International Conference on Computational and Experimental Science
   and Engineering (ICCESEN), Antalya, TURKEY, OCT 19-24, 2016},
Abstract = {Cardiotocography is a monitoring technique providing important and vital
   information on fetal status during antepartum and intrapartum periods.
   The advances in modern obstetric practice allowed many robust and
   reliable machine learning techniques to be utilized in classifying fetal
   heart rate signals. The role of machine learning approaches in
   diagnosing diseases is becoming increasingly essential and intertwined.
   The main aim of the present study is to determine the most efficient
   machine learning technique to classify fetal heart rate signals.
   Therefore, the research has been focused on the widely used and
   practical machine learning techniques, such as artificial neural
   network, support vector machine, extreme learning machine, radial basis
   function network, and random forest. In a comparative way, fetal heart
   rate signals were classified as normal or hypoxic using the
   aforementioned machine learning techniques. The performance metrics
   derived from confusion matrix were used to measure classifiers' success.
   According to experimental results, although all machine learning
   techniques produced satisfactory results, artificial neural network
   yielded the rather well results with the sensitivity of 99.73\% and
   specificity of 97.94\%. The study results show that the artificial
   neural network was superior to other algorithms.},
DOI = {10.12693/APhysPolA.132.451},
ISSN = {0587-4246},
EISSN = {1898-794X},
ResearcherID-Numbers = {Comert, Zafer/F-1940-2016
   Cömert, Zafer/F-1940-2016
   Kocamaz, Adnan/C-2820-2014},
ORCID-Numbers = {Kocamaz, Adnan Fatih/0000-0002-7729-8322
   Comert, Zafer/0000-0001-5256-7648
   },
Unique-ID = {WOS:000412881200013},
}

@article{ WOS:000489304600003,
Author = {Peter, Timm J. and Nelles, Oliver},
Title = {Fast and simple dataset selection for machine learning},
Journal = {AT-AUTOMATISIERUNGSTECHNIK},
Year = {2019},
Volume = {67},
Number = {10},
Pages = {833-842},
Month = {OCT},
Abstract = {The task of data reduction is discussed and a novel selection approach
   which allows to control the optimal point distribution of the selected
   data subset is proposed. The proposed approach utilizes the estimation
   of probability density functions (pdfs). Due to its structure, the new
   method is capable of selecting a subset either by approximating the pdf
   of the original dataset or by approximating an arbitrary, desired target
   pdf. The new strategy evaluates the estimated pdfs solely on the
   selected data points, resulting in a simple and efficient algorithm with
   low computational and memory demand. The performance of the new approach
   is investigated for two different scenarios. For representative subset
   selection of a dataset, the new approach is compared to a recently
   proposed, more complex method and shows comparable results. For the
   demonstration of the capability of matching a target pdf, a uniform
   distribution is chosen as an example. Here the new method is compared to
   strategies for space-filling design of experiments and shows convincing
   results.},
DOI = {10.1515/auto-2019-0010},
ISSN = {0178-2312},
ResearcherID-Numbers = {Nelles, Oliver/C-3608-2015},
Unique-ID = {WOS:000489304600003},
}

@article{ WOS:000383095900019,
Author = {Louridas, Panos and Ebert, Christof},
Title = {Machine Learning},
Journal = {IEEE SOFTWARE},
Year = {2016},
Volume = {33},
Number = {5},
Pages = {110-115},
Month = {SEP-OCT},
Abstract = {Machine learning is the major success factor in the ongoing digital
   transformation across industries. Startups and behemoths alike announce
   new products that will learn to perform tasks that previously only
   humans could do, and perform those tasks better, faster, and more
   intelligently. But how do they do it? What does it mean for IT
   developers and software engineers? Here, Panos Louridas and I present a
   brief overview of machine-learning technologies, with a concrete case
   study from code analysis. I look forward to hearing from both readers
   and prospective column authors.},
DOI = {10.1109/MS.2016.114},
ISSN = {0740-7459},
EISSN = {1937-4194},
ResearcherID-Numbers = {Ebert, Christof/JXM-5500-2024},
Unique-ID = {WOS:000383095900019},
}

@article{ WOS:000371639700001,
Author = {Biehl, Michael and Hammer, Barbara and Villmann, Thomas},
Title = {Prototype-based models in machine learning},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-COGNITIVE SCIENCE},
Year = {2016},
Volume = {7},
Number = {2},
Pages = {92-111},
Month = {MAR-APR},
Abstract = {An overview is given of prototype-based models in machine learning. In
   this framework, observations, i.e., data, are stored in terms of typical
   representatives. Together with a suitable measure of similarity, the
   systems can be employed in the context of unsupervised and supervised
   analysis of potentially high-dimensional, complex datasets. We discuss
   basic schemes of competitive vector quantization as well as the
   so-called neural gas approach and Kohonen's topology-preserving
   self-organizing map. Supervised learning in prototype systems is
   exemplified in terms of learning vector quantization. Most frequently,
   the familiar Euclidean distance serves as a dissimilarity measure. We
   present extensions of the framework to nonstandard measures and give an
   introduction to the use of adaptive distances in relevance learning. (C)
   2016 Wiley Periodicals, Inc.},
DOI = {10.1002/wcs.1378},
ISSN = {1939-5078},
EISSN = {1939-5086},
ResearcherID-Numbers = {Biehl, Michael/B-5105-2009
   Hammer, Barbara/E-8624-2010
   },
ORCID-Numbers = {Biehl, Michael/0000-0001-5148-4568
   Hammer, Barbara/0000-0002-0935-5591},
Unique-ID = {WOS:000371639700001},
}

@article{ WOS:000393223300013,
Author = {Benuwa, Ben-Bright and Zhan, Yongzhao and Ghansah, Benjamin and Wornyo,
   Dickson Keddy and Kataka, Frank Banaseka},
Title = {A Review of Deep Machine Learning},
Journal = {INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH IN AFRICA},
Year = {2016},
Volume = {24},
Pages = {124-136},
Abstract = {The rapid increase of information and accessibility in recent years has
   activated a paradigm shift in algorithm design for artificial
   intelligence. Recently, Deep Learning (a surrogate of Machine Learning)
   have won several contests in pattern recognition and machine learning.
   This review comprehensively summarises relevant studies, much of it from
   prior state-of-the-art techniques. This paper also discusses the
   motivations and principles regarding learning algorithms for deep
   architectures.},
DOI = {10.4028/www.scientific.net/JERA.24.124},
ISSN = {1663-3571},
EISSN = {1663-4144},
ResearcherID-Numbers = {GHANSAH, BENJAMIN/MFK-1007-2025
   Benuwa, Ben-Bright/J-6198-2019
   },
ORCID-Numbers = {Ghansah, Benjamin/0000-0002-1599-6301
   Benuwa, Ben-Bright/0000-0002-3085-706X
   Banaseka, Frank Kataka/0000-0003-4888-9921},
Unique-ID = {WOS:000393223300013},
}

@article{ WOS:000637534200002,
Author = {Cui, Fuwei and Cui, Qian and Song, Yongduan},
Title = {A Survey on Learning-Based Approaches for Modeling and Classification of
   Human-Machine Dialog Systems},
Journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
Year = {2021},
Volume = {32},
Number = {4},
Pages = {1418-1432},
Month = {APR},
Abstract = {With the rapid development from traditional machine learning (ML) to
   deep learning (DL) and reinforcement learning (RL), dialog system
   equipped with learning mechanism has become the most effective solution
   to address human-machine interaction problems. The purpose of this
   article is to provide a comprehensive survey on learning-based
   human-machine dialog systems with a focus on the various dialog models.
   More specifically, we first introduce the fundamental process of
   establishing a dialog model. Second, we examine the features and
   classifications of the system dialog model, expound some representative
   models, and also compare the advantages and disadvantages of different
   dialog models. Third, we comb the commonly used database and evaluation
   metrics of the dialog model. Furthermore, the evaluation metrics of
   these dialog models are analyzed in detail. Finally, we briefly analyze
   the existing issues and point out the potential future direction on the
   human-machine dialog systems.},
DOI = {10.1109/TNNLS.2020.2985588},
ISSN = {2162-237X},
EISSN = {2162-2388},
ResearcherID-Numbers = {Song, Yongduan/AAI-7645-2021},
ORCID-Numbers = {Cui, Fuwei/0000-0001-6800-3791
   Zhao, kai/0000-0003-0656-1901
   },
Unique-ID = {WOS:000637534200002},
}

@article{ WOS:000556008200001,
Author = {Wang, Lidong and Alexander, Cheryl Ann},
Title = {Machine Learning in Big Data},
Journal = {INTERNATIONAL JOURNAL OF MATHEMATICAL ENGINEERING AND MANAGEMENT
   SCIENCES},
Year = {2016},
Volume = {1},
Number = {2},
Pages = {52-61},
Month = {SEP},
Abstract = {Machine learning is an artificial intelligence method of discovering
   knowledge for making intelligent decisions. Big Data has great impacts
   on scientific discoveries and value creation. This paper introduces
   methods in machine learning, main technologies in Big Data, and some
   applications of machine learning in Big Data. Challenges of machine
   learning applications in Big Data are discussed. Some new methods and
   technology progress of machine learning in Big Data are also presented.},
DOI = {10.33889/IJMEMS.2016.1.2-006},
ISSN = {2455-7749},
Unique-ID = {WOS:000556008200001},
}

@article{ WOS:001136314900001,
Author = {Pyun, Kyung Rok and Kwon, Kangkyu and Yoo, Myung Jin and Kim, Kyun Kyu
   and Gong, Dohyeon and Yeo, Woon-Hong and Han, Seungyong and Ko, Seung
   Hwan},
Title = {Machine-learned wearable sensors for real-time hand-motion recognition:
   toward practical applications},
Journal = {NATIONAL SCIENCE REVIEW},
Year = {2024},
Volume = {11},
Number = {2},
Month = {JAN 9},
Abstract = {Soft electromechanical sensors have led to a new paradigm of electronic
   devices for novel motion-based wearable applications in our daily lives.
   However, the vast amount of random and unidentified signals generated by
   complex body motions has hindered the precise recognition and practical
   application of this technology. Recent advancements in
   artificial-intelligence technology have enabled significant strides in
   extracting features from massive and intricate data sets, thereby
   presenting a breakthrough in utilizing wearable sensors for practical
   applications. Beyond traditional machine-learning techniques for
   classifying simple gestures, advanced machine-learning algorithms have
   been developed to handle more complex and nuanced motion-based tasks
   with restricted training data sets. Machine-learning techniques have
   improved the ability to perceive, and thus machine-learned wearable soft
   sensors have enabled accurate and rapid human-gesture recognition,
   providing real-time feedback to users. This forms a crucial component of
   future wearable electronics, contributing to a robust human-machine
   interface. In this review, we provide a comprehensive summary covering
   materials, structures and machine-learning algorithms for hand-gesture
   recognition and possible practical applications through machine-learned
   wearable electromechanical sensors.
   This review provides a thorough overview of the current research in
   machine-learned wearable sensors for real-time hand motion recognition,
   highlighting current challenges and future directions toward practical
   applications in reality.},
DOI = {10.1093/nsr/nwad298},
EarlyAccessDate = {JAN 2024},
ISSN = {2095-5138},
EISSN = {2053-714X},
ResearcherID-Numbers = {Ko, Seung Hwan/B-5448-2008
   Yeo, Woon-Hong/G-6430-2011
   Kim, Kyun Kyu/GNP-5638-2022},
ORCID-Numbers = {Pyun, Kyung Rok/0009-0005-4938-5516
   Ko, Seung Hwan/0000-0002-7477-0820
   },
Unique-ID = {WOS:001136314900001},
}

@article{ WOS:000392585400001,
Author = {Greenwald, Hal S. and Oertel, Carsten K.},
Title = {Future Directions in Machine Learning},
Journal = {FRONTIERS IN ROBOTICS AND AI},
Year = {2017},
Volume = {3},
Month = {JAN 24},
Abstract = {Current machine learning (ML) algorithms identify statistical
   regularities in complex data sets and are regularly used across a range
   of application domains, but they lack the robustness and
   generalizability associated with human learning. If ML techniques could
   enable computers to learn from fewer examples, transfer knowledge
   between tasks, and adapt to changing contexts and environments, the
   results would have very broad scientific and societal impacts. Increased
   processing and memory resources have enabled larger, more capable
   learning models, but there is growing recognition that even greater
   computing resources would not be sufficient to yield algorithms capable
   of learning from a few examples and generalizing beyond initial training
   sets. This paper presents perspectives on feature selection,
   representation schemes and interpretability, transfer learning,
   continuous learning, and learning and adaptation in time-varying
   contexts and environments, five key areas that are essential for
   advancing ML capabilities. Appropriate learning tasks that require these
   capabilities can demonstrate the strengths of novel ML approaches that
   could address these challenges.},
DOI = {10.3389/frobt.2016.00079},
Article-Number = {79},
ISSN = {2296-9144},
Unique-ID = {WOS:000392585400001},
}

@article{ WOS:001217367400004,
Author = {Chen, Wuxing and Yang, Kaixiang and Yu, Zhiwen and Shi, Yifan and Chen,
   C. L. Philip},
Title = {A survey on imbalanced learning: latest research, applications and
   future directions},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2024},
Volume = {57},
Number = {6},
Month = {MAY 9},
Abstract = {Imbalanced learning constitutes one of the most formidable challenges
   within data mining and machine learning. Despite continuous research
   advancement over the past decades, learning from data with an imbalanced
   class distribution remains a compelling research area. Imbalanced class
   distributions commonly constrain the practical utility of machine
   learning and even deep learning models in tangible applications.
   Numerous recent studies have made substantial progress in the field of
   imbalanced learning, deepening our understanding of its nature while
   concurrently unearthing new challenges. Given the field's rapid
   evolution, this paper aims to encapsulate the recent breakthroughs in
   imbalanced learning by providing an in-depth review of extant strategies
   to confront this issue. Unlike most surveys that primarily address
   classification tasks in machine learning, we also delve into techniques
   addressing regression tasks and facets of deep long-tail learning.
   Furthermore, we explore real-world applications of imbalanced learning,
   devising a broad spectrum of research applications from management
   science to engineering, and lastly, discuss newly-emerging issues and
   challenges necessitating further exploration in the realm of imbalanced
   learning.},
DOI = {10.1007/s10462-024-10759-6},
Article-Number = {137},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Chen, C. L. Philip/O-2657-2016
   Yang, Kaixiang/AGZ-1602-2022},
Unique-ID = {WOS:001217367400004},
}

@article{ WOS:000346856600043,
Author = {Malhotra, Ruchika},
Title = {A systematic review of machine learning techniques for software fault
   prediction},
Journal = {APPLIED SOFT COMPUTING},
Year = {2015},
Volume = {27},
Pages = {504-518},
Month = {FEB},
Abstract = {Background: Software fault prediction is the process of developing
   models that can be used by the software practitioners in the early
   phases of software development life cycle for detecting faulty
   constructs such as modules or classes. There are various machine
   learning techniques used in the past for predicting faults. Method: In
   this study we perform a systematic review of studies from January 1991
   to October 2013 in the literature that use the machine learning
   techniques for software fault prediction. We assess the performance
   capability of the machine learning techniques in existing research for
   software fault prediction. We also compare the performance of the
   machine learning techniques with the statistical techniques and other
   machine learning techniques. Further the strengths and weaknesses of
   machine learning techniques are summarized.
   Results: In this paper we have identified 64 primary studies and seven
   categories of the machine learning techniques. The results prove the
   prediction capability of the machine learning techniques for classifying
   module/class as fault prone or not fault prone. The models using the
   machine learning techniques for estimating software fault proneness
   outperform the traditional statistical models.
   Conclusion: Based on the results obtained from the systematic review, we
   conclude that the machine learning techniques have the ability for
   predicting software fault proneness and can be used by software
   practitioners and researchers. However, the application of the machine
   learning techniques in software fault prediction is still limited and
   more number of studies should be carried out in order to obtain well
   formed and generalizable results. We provide future guidelines to
   practitioners and researchers based on the results obtained in this
   work. (C) 2014 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.asoc.2014.11.023},
ISSN = {1568-4946},
EISSN = {1872-9681},
ResearcherID-Numbers = {Malhotra, Ruchika/ABC-3126-2020},
Unique-ID = {WOS:000346856600043},
}

@article{ WOS:000369369100004,
Author = {Donnat, Philippe and Marti, Gautier and Very, Philippe},
Title = {Toward a generic representation of random variables for machine learning},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2016},
Volume = {70},
Pages = {24-31},
Month = {JAN 15},
Abstract = {This paper presents a pre-processing and a distance which improve the
   performance of machine learning algorithms working on independent and
   identically distributed stochastic processes. We introduce a novel non
   parametric approach to represent random variables which splits apart
   dependency and distribution without losing any information. We also
   propound an associated metric leveraging this representation and its
   statistical estimate. Besides experiments on synthetic datasets, the
   benefits of our contribution is illustrated through the example of
   clustering financial time series, for instance prices from the credit
   default swaps market. Results are available on the website
   http://www.datagrapple.com and an IPython Notebook tutorial is available
   at http://www.datagrapple.com/Tech for reproducible research. (C) 2015
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.patrec.2015.11.004},
ISSN = {0167-8655},
EISSN = {1872-7344},
Unique-ID = {WOS:000369369100004},
}

@article{ WOS:000697475900018,
Author = {Qian, Yufeng},
Title = {Exploration of machine algorithms based on deep learning model and
   feature extraction},
Journal = {MATHEMATICAL BIOSCIENCES AND ENGINEERING},
Year = {2021},
Volume = {18},
Number = {6},
Pages = {7602-7618},
Abstract = {The study expects to solve the problems of insufficient labeling, high
   input dimension, and inconsistent task input distribution in traditional
   lifelong machine learning. A new deep learning model is proposed by
   combining feature representation with a deep learning algorithm. First,
   based on the theoretical basis of the deep learning model and feature
   extraction. The study analyzes several representative machine learning
   algorithms, and compares the performance of the optimized deep learning
   model with other algorithms in a practical application. By explaining
   the machine learning system, the study introduces two typical algorithms
   in machine learning, namely ELLA (Efficient lifelong learning algorithm)
   and HLLA (Hierarchical lifelong learning algorithm). Second, the flow of
   the genetic algorithm is described, and combined with mutual information
   feature extraction in a machine algorithm, to form a composite algorithm
   HLLA (Hierarchical lifelong learning algorithm). Finally, the deep
   learning model is optimized and a deep learning model based on the HLLA
   algorithm is constructed. When K = 1200, the classification error rate
   reaches 0.63\%, which reflects the excellent performance of the
   unsupervised database algorithm based on this model. Adding the feature
   model to the updating iteration process of lifelong learning deepens the
   knowledge base ability of lifelong machine learning, which is of great
   value to reduce the number of labels required for subsequent model
   learning and improve the efficiency of lifelong learning.},
DOI = {10.3934/mbe.2021376},
ISSN = {1547-1063},
EISSN = {1551-0018},
ORCID-Numbers = {Yufeng, Qian/0000-0002-4003-143X},
Unique-ID = {WOS:000697475900018},
}

@article{ WOS:000351430600002,
Author = {Cai, X. -D. and Wu, D. and Su, Z. -E. and Chen, M. -C. and Wang, X. -L.
   and Li, Li and Liu, N. -L. and Lu, C. -Y. and Pan, J. -W.},
Title = {Entanglement-Based Machine Learning on a Quantum Computer},
Journal = {PHYSICAL REVIEW LETTERS},
Year = {2015},
Volume = {114},
Number = {11},
Month = {MAR 19},
Abstract = {Machine learning, a branch of artificial intelligence, learns from
   previous experience to optimize performance, which is ubiquitous in
   various fields such as computer sciences, financial analysis, robotics,
   and bioinformatics. A challenge is that machine learning with the
   rapidly growing ``big data{''} could become intractable for classical
   computers. Recently, quantum machine learning algorithms {[}Lloyd,
   Mohseni, and Rebentrost, arXiv. 1307.0411] were proposed which could
   offer an exponential speedup over classical algorithms. Here, we report
   the first experimental entanglement-based classification of two-, four-,
   and eight-dimensional vectors to different clusters using a small-scale
   photonic quantum computer, which are then used to implement supervised
   and unsupervised machine learning. The results demonstrate the working
   principle of using quantum computers to manipulate and classify
   high-dimensional vectors, the core mathematical routine in machine
   learning. The method can, in principle, be scaled to larger numbers of
   qubits, and may provide a new route to accelerate machine learning.},
DOI = {10.1103/PhysRevLett.114.110504},
Article-Number = {110504},
ISSN = {0031-9007},
EISSN = {1079-7114},
ResearcherID-Numbers = {Lu, Chao-Yang/D-8481-2011
   Lu, Chao-Yang/JAZ-0892-2023
   Wu, Dian/AGX-9828-2022
   Chen, Ming-Cheng/AAN-9647-2020
   Cai, Xin-Dong/J-9496-2013
   Wang, Xi-Lin/E-2562-2011
   Pan, Jian-Wei/A-2332-2010},
ORCID-Numbers = {Wang, Xi-Lin/0000-0002-3990-6454
   Lu, Chao-Yang/0000-0002-8227-9177
   Wu, Dian/0000-0002-2220-2071
   Su, Zu-En/0009-0005-4026-7169
   Pan, Jian-Wei/0000-0002-6100-5142},
Unique-ID = {WOS:000351430600002},
}

@article{ WOS:000256504400007,
Author = {Hofmann, Thomas and Schoelkopf, Bernhard and Smola, Alexander J.},
Title = {Kernel methods in machine learning},
Journal = {ANNALS OF STATISTICS},
Year = {2008},
Volume = {36},
Number = {3},
Pages = {1171-1220},
Month = {JUN},
Abstract = {We review machine learning methods employing positive definite kernels.
   These methods formulate learning and estimation problems in a
   reproducing kernel Hilbert space (RKHS) of functions defined on the data
   domain, expanded in terms of a kernel. Working in linear spaces of
   function has the benefit of facilitating the construction and analysis
   of learning algorithms while at the same time allowing large classes of
   functions. The latter include nonlinear functions as well as functions
   defined on nonvectorial data.
   We cover a wide range of methods, ranging from binary classifiers to
   sophisticated methods for estimation with structured data.},
DOI = {10.1214/009053607000000677},
ISSN = {0090-5364},
ResearcherID-Numbers = {Schölkopf, Bernhard/A-7570-2013
   Scholkopf, Bernhard/A-7570-2013},
ORCID-Numbers = {Scholkopf, Bernhard/0000-0002-8177-0925},
Unique-ID = {WOS:000256504400007},
}

@article{ WOS:001361255100001,
Author = {Rafiei, Mohammad H. and Gauthier, Lynne V. and Adeli, Hojjat and Takabi,
   Daniel},
Title = {Self-Supervised Learning for Near-Wild Cognitive Workload Estimation},
Journal = {JOURNAL OF MEDICAL SYSTEMS},
Year = {2024},
Volume = {48},
Number = {1},
Month = {NOV 22},
Abstract = {Feedback on cognitive workload may reduce decision-making mistakes.
   Machine learning-based models can produce feedback from physiological
   data such as electroencephalography (EEG) and electrocardiography (ECG).
   Supervised machine learning requires large training data sets that are
   (1) relevant and decontaminated and (2) carefully labeled for accurate
   approximation, a costly and tedious procedure. Commercial
   over-the-counter devices are low-cost resolutions for the real-time
   collection of physiological modalities. However, they produce
   significant artifacts when employed outside of laboratory settings,
   compromising machine learning accuracies. Additionally, the
   physiological modalities that most successfully machine-approximate
   cognitive workload in everyday settings are unknown. To address these
   challenges, a first-ever hybrid implementation of feature selection and
   self-supervised machine learning techniques is introduced. This model is
   employed on data collected outside controlled laboratory settings to (1)
   identify relevant physiological modalities to machine approximate six
   levels of cognitive-physical workloads from a seven-modality repository
   and (2) postulate limited labeling experiments and machine approximate
   mental-physical workloads using self-supervised learning techniques.},
DOI = {10.1007/s10916-024-02122-7},
Article-Number = {107},
ISSN = {0148-5598},
EISSN = {1573-689X},
ResearcherID-Numbers = {Takabi, Daniel/JVO-5941-2024
   adeli, hojjat/D-1430-2010},
Unique-ID = {WOS:001361255100001},
}

@article{ WOS:000965299600001,
Author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo
   and Li, Yuan and Liu, Xu and He, Bingsheng},
Title = {A Survey on Federated Learning Systems: Vision, Hype and Reality for
   Data Privacy and Protection},
Journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
Year = {2023},
Volume = {35},
Number = {4},
Pages = {3347-3366},
Month = {APR 1},
Abstract = {As data privacy increasingly becomes a critical societal concern,
   federated learning has been a hot research topic in enabling the
   collaborative training of machine learning models among different
   organizations under the privacy restrictions. As researchers try to
   support more machine learning models with different privacy-preserving
   approaches, there is a requirement in developing systems and
   infrastructures to ease the development of various federated learning
   algorithms. Similar to deep learning systems such as PyTorch and
   TensorFlow that boost the development of deep learning, federated
   learning systems (FLSs) are equivalently important, and face challenges
   from various aspects such as effectiveness, efficiency, and privacy. In
   this survey, we conduct a comprehensive review on federated learning
   systems. To understand the key design system components and guide future
   research, we introduce the definition of federated learning systems and
   analyze the system components. Moreover, we provide a thorough
   categorization for federated learning systems according to six different
   aspects, including data distribution, machine learning model, privacy
   mechanism, communication architecture, scale of federation and
   motivation of federation. The categorization can help the design of
   federated learning systems as shown in our case studies. By
   systematically summarizing the existing federated learning systems, we
   present the design factors, case studies, and future research
   opportunities.},
DOI = {10.1109/TKDE.2021.3124599},
ISSN = {1041-4347},
EISSN = {1558-2191},
ResearcherID-Numbers = {Wang, Naibo/GVS-6596-2022
   Wen, Zeyi/A-6240-2019
   Li, Qinbin/JVN-4491-2024
   Wu, Zhaomin/HCI-0541-2022
   },
ORCID-Numbers = {Wen, Zeyi/0000-0003-3370-6053
   Li, Yuan/0000-0002-3178-4018
   Wang, Naibo/0000-0001-5660-5250
   Wu, Zhaomin/0000-0002-6463-0031
   Li, Qinbin/0000-0002-6539-6443},
Unique-ID = {WOS:000965299600001},
}

@article{ WOS:001180702200015,
Author = {Qu, Youyang and Yuan, Xin and Ding, Ming and Ni, Wei and Rakotoarivelo,
   Thierry and Smith, David},
Title = {Learn to Unlearn: Insights Into Machine Unlearning},
Journal = {COMPUTER},
Year = {2024},
Volume = {57},
Number = {3},
Pages = {79-90},
Month = {MAR},
Abstract = {This article presents a comprehensive review of recent machine
   unlearning techniques, verification mechanisms, and potential attacks.
   We highlight emerging challenges and prospective research directions,
   aiming to provide valuable resources for integrating privacy, equity,
   and resilience into machine learning systems and help them ``learn to
   unlearn.{''}},
DOI = {10.1109/MC.2023.3333319},
ISSN = {0018-9162},
EISSN = {1558-0814},
ResearcherID-Numbers = {Ding, Ming/AAW-4395-2021
   Qu, Youyang/JAX-6792-2023
   },
ORCID-Numbers = {Smith, David/0000-0002-8552-1301},
Unique-ID = {WOS:001180702200015},
}

@article{ WOS:000394061800034,
Author = {Sun, Kai and Zhang, Jiangshe and Zhang, Chunxia and Hu, Junying},
Title = {Generalized extreme learning machine autoencoder and a new deep neural
   network},
Journal = {NEUROCOMPUTING},
Year = {2017},
Volume = {230},
Pages = {374-381},
Month = {MAR 22},
Abstract = {Extreme learning machine (ELM) is an efficient learning algorithm of
   training single layer feed-forward neural networks (SLFNs). With the
   development of unsupervised learning in recent years, integrating ELM
   with autoencoder has become a new perspective for extracting feature
   using unlabeled data. In this paper, we propose a new variant of extreme
   learning machine autoencoder (ELM-AE) called generalized extreme
   learning machine autoencoder (GELM-AE) which adds the manifold
   regularization to the objective of ELM-AE. Some experiments carried out
   on real-world data sets show that GELM-AE outperforms some
   state-of-the-art unsupervised learning algorithms, including k-means,
   laplacian embedding (LE), spectral clustering (SC) and ELM-AE.
   Furthermore, we also propose a new deep neural network called multilayer
   generalized extreme learning machine autoencoder (ML-GELM) by stacking
   several GELM-AE to detect more abstract representations. The experiments
   results show that ML-GELM outperforms ELM and many other deep models,
   such as multilayer ELM autoencoder (ML-ELM), deep belief network (DBN)
   and stacked autoencoder (SAE). Due to the utilization of ELM, ML-GELM is
   also faster than DBN and SAE.},
DOI = {10.1016/j.neucom.2016.12.027},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Zhang, Chunxia/B-3713-2015},
Unique-ID = {WOS:000394061800034},
}

@article{ WOS:000398821300014,
Author = {Ding, Shifei and Zhang, Nan and Zhang, Jian and Xu, Xinzheng and Shi,
   Zhongzhi},
Title = {Unsupervised extreme learning machine with representational features},
Journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS},
Year = {2017},
Volume = {8},
Number = {2},
Pages = {587-595},
Month = {APR},
Abstract = {Extreme learning machine (ELM) is not only an effective classifier but
   also a useful cluster. Unsupervised extreme learning machine (US-ELM)
   gives favorable performance compared to state-of-the-art clustering
   algorithms. Extreme learning machine as an auto encoder (ELM-AE) can
   obtain principal components which represent original samples. The
   proposed unsupervised extreme learning machine based on embedded
   features of ELM-AE (US-EF-ELM) algorithm applies ELM-AE to US-ELM.
   US-EF-ELM regards embedded features of ELM-AE as the outputs of US-ELM
   hidden layer, and uses US-ELM to obtain the embedded matrix of US-ELM.
   US-EF-ELM can handle the multi-cluster clustering. The learning
   capability and computational efficiency of US-EF-ELM are as same as
   US-ELM. By experiments on UCI data sets, we compared US-EF-ELM k-means
   algorithm with k-means algorithm, spectral clustering algorithm, and
   US-ELM k-means algorithm in accuracy and efficiency.},
DOI = {10.1007/s13042-015-0351-8},
ISSN = {1868-8071},
EISSN = {1868-808X},
ResearcherID-Numbers = {Shi, Z/ISB-4324-2023
   },
ORCID-Numbers = {Zhang, Nan/0000-0001-9620-5665},
Unique-ID = {WOS:000398821300014},
}

@article{ WOS:000425723300001,
Author = {Ma, Jun and Wu, Jiande and Wang, Xiaodong},
Title = {Fault diagnosis method based on wavelet packet-energy entropy and fuzzy
   kernel extreme learning machine},
Journal = {ADVANCES IN MECHANICAL ENGINEERING},
Year = {2018},
Volume = {10},
Number = {1},
Month = {JAN 12},
Abstract = {Aiming at connatural limitations of extreme learning machine in
   practice, a new fault diagnosis method based on wavelet packet-energy
   entropy and fuzzy kernel extreme learning machine is proposed. On one
   hand, the presented method can extract the more efficient features using
   the wavelet packet-energy entropy method, and on the other hand, the
   sample fuzzy membership degree matrix U, weight matrix W which is used
   to describe the sample imbalance, and the kernel function are introduced
   to construct the fuzzy kernel extreme learning machine model with high
   accuracy and reliability. The experimental results of rolling bearing
   and check valve are obtained and analyzed in MATLAB 2010b. The results
   show that the proposed fuzzy kernel extreme learning machine method can
   obtain fairly or slightly better classification performance than the
   traditional extreme learning machine, kernel extreme learning machine,
   back propagation, support vector machine, and fuzzy support vector
   machine.},
DOI = {10.1177/1687814017751446},
Article-Number = {1687814017751446},
ISSN = {1687-8140},
ResearcherID-Numbers = {Wu, Jiande/AEE-8510-2022
   Wang, Xiao-Dong/A-9822-2008
   },
ORCID-Numbers = {Ma, Jun/0000-0002-7531-7833
   Wang, Xiao-Dong/0000-0002-4533-6734},
Unique-ID = {WOS:000425723300001},
}

@article{ WOS:000480422500020,
Author = {Khan, Uzair and Khan, Khalid and Hasssan, Fadzil and Siddiqui, Anam and
   Afaq, Muhammad},
Title = {Towards Achieving Machine Comprehension Using Deep Learning on Non-GPU
   Machines},
Journal = {ENGINEERING TECHNOLOGY \& APPLIED SCIENCE RESEARCH},
Year = {2019},
Volume = {9},
Number = {4},
Pages = {4423-4427},
Month = {AUG},
Abstract = {Long efforts have been made to enable machines to understand human
   language. Nowadays such activities fall under the broad umbrella of
   machine comprehension. The results are optimistic due to the recent
   advancements in the field of machine learning. Deep learning promises to
   bring even better results but requires expensive and resource hungry
   hardware. In this paper, we demonstrate the use of deep learning in the
   context of machine comprehension by using non-GPU machines. Our results
   suggest that the good algorithm insight and detailed understanding of
   the dataset can help in getting meaningful results through deep learning
   even on non-GPU machines.},
ISSN = {2241-4487},
EISSN = {1792-8036},
ResearcherID-Numbers = {Khan, Kamran/JFJ-9362-2023},
Unique-ID = {WOS:000480422500020},
}

@article{ WOS:000861328200001,
Author = {Mienye, Ibomoiye Domor and Sun, Yanxia},
Title = {A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and
   Prospects},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {99129-99149},
Abstract = {Ensemble learning techniques have achieved state-of-the-art performance
   in diverse machine learning applications by combining the predictions
   from two or more base models. This paper presents a concise overview of
   ensemble learning, covering the three main ensemble methods: bagging,
   boosting, and stacking, their early development to the recent
   state-of-the-art algorithms. The study focuses on the widely used
   ensemble algorithms, including random forest, adaptive boosting
   (AdaBoost), gradient boosting, extreme gradient boosting (XGBoost),
   light gradient boosting machine (LightGBM), and categorical boosting
   (CatBoost). An attempt is made to concisely cover their mathematical and
   algorithmic representations, which is lacking in the existing literature
   and would be beneficial to machine learning researchers and
   practitioners.},
DOI = {10.1109/ACCESS.2022.3207287},
ISSN = {2169-3536},
ResearcherID-Numbers = {Sun, Yanxia/JRW-1277-2023
   Mienye, Ibomoiye/AAN-5976-2021},
ORCID-Numbers = {Mienye, Ibomoiye Domor/0000-0001-9353-8894
   Sun, Yanxia/0000-0002-3455-9625
   },
Unique-ID = {WOS:000861328200001},
}

@article{ WOS:000312959700016,
Author = {Bishop, Christopher M.},
Title = {Model-based machine learning},
Journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL
   AND ENGINEERING SCIENCES},
Year = {2013},
Volume = {371},
Number = {1984},
Month = {FEB 13},
Abstract = {Several decades of research in the field of machine learning have
   resulted in a multitude of different algorithms for solving a broad
   range of problems. To tackle a new application, a researcher typically
   tries to map their problem onto one of these existing methods, often
   influenced by their familiarity with specific algorithms and by the
   availability of corresponding software implementations. In this study,
   we describe an alternative methodology for applying machine learning, in
   which a bespoke solution is formulated for each new application. The
   solution is expressed through a compact modelling language, and the
   corresponding custom machine learning code is then generated
   automatically. This model-based approach offers several major
   advantages, including the opportunity to create highly tailored models
   for specific scenarios, as well as rapid prototyping and comparison of a
   range of alternative models. Furthermore, newcomers to the field of
   machine learning do not have to learn about the huge range of
   traditional methods, but instead can focus their attention on
   understanding a single modelling environment. In this study, we show how
   probabilistic graphical models, coupled with efficient inference
   algorithms, provide a very flexible foundation for model-based machine
   learning, and we outline a large-scale commercial application of this
   framework involving tens of millions of users. We also describe the
   concept of probabilistic programming as a powerful software environment
   for model-based machine learning, and we discuss a specific
   probabilistic programming language called Infer.NET, which has been
   widely used in practical applications.},
DOI = {10.1098/rsta.2012.0222},
Article-Number = {UNSP 20120222},
ISSN = {1364-503X},
EISSN = {1471-2962},
Unique-ID = {WOS:000312959700016},
}

@article{ WOS:000401624200005,
Author = {Zhang, Nan and Ding, Shifei},
Title = {Unsupervised and semi-supervised extreme learning machine with wavelet
   kernel for high dimensional data},
Journal = {MEMETIC COMPUTING},
Year = {2017},
Volume = {9},
Number = {2},
Pages = {129-139},
Month = {JUN},
Abstract = {Extreme learning machine (ELM) not only is an effective classifier in
   supervised learning, but also can be applied on unsupervised learning
   and semi-supervised learning. The model structure of unsupervised
   extreme learning machine (US-ELM) and semi-supervised extreme learning
   machine (SS-ELM) are same as ELM, the difference between them is the
   cost function. We introduce kernel function to US-ELM and propose
   unsupervised extreme learning machine with kernel (US-KELM). And SS-KELM
   has been proposed. Wavelet analysis has the characteristics of
   multivariate interpolation and sparse change, and Wavelet kernel
   functions have been widely used in support vector machine. Therefore, to
   realize a combination of the wavelet kernel function, US-ELM, and
   SS-ELM, unsupervised extreme learning machine with wavelet kernel
   function (US-WKELM) and semi-supervised extreme learning machine with
   wavelet kernel function (SS-WKELM) are proposed in this paper. The
   experimental results show the feasibility and validity of US-WKELM and
   SS-WKELM in clustering and classification.},
DOI = {10.1007/s12293-016-0198-x},
ISSN = {1865-9284},
EISSN = {1865-9292},
ORCID-Numbers = {Zhang, Nan/0000-0001-9620-5665},
Unique-ID = {WOS:000401624200005},
}

@article{ WOS:000620462600001,
Author = {Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and
   Gao, Yuan},
Title = {A survey on federated learning},
Journal = {KNOWLEDGE-BASED SYSTEMS},
Year = {2021},
Volume = {216},
Month = {MAR 15},
Abstract = {Federated learning is a set-up in which multiple clients collaborate to
   solve machine learning problems, which is under the coordination of a
   central aggregator. This setting also allows the training data
   decentralized to ensure the data privacy of each device. Federated
   learning adheres to two major ideas: local computing and model
   transmission, which reduces some systematic privacy risks and costs
   brought by traditional centralized machine learning methods. The
   original data of the client is stored locally and cannot be exchanged or
   migrated. With the application of federated learning, each device uses
   local data for local training, then uploads the model to the server for
   aggregation, and finally the server sends the model update to the
   participants to achieve the learning goal. To provide a comprehensive
   survey and facilitate the potential research of this area, we
   systematically introduce the existing works of federated learning from
   five aspects: data partitioning, privacy mechanism, machine learning
   model, communication architecture and systems heterogeneity. Then, we
   sort out the current challenges and future research directions of
   federated learning. Finally, we summarize the characteristics of
   existing federated learning, and analyze the current practical
   application of federated learning. (C) 2021 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.knosys.2021.106775},
EarlyAccessDate = {FEB 2021},
Article-Number = {106775},
ISSN = {0950-7051},
EISSN = {1872-7409},
ResearcherID-Numbers = {Gao, Yuan/AAP-7481-2021
   },
ORCID-Numbers = {Gao, Yuan/0000-0002-2990-9205},
Unique-ID = {WOS:000620462600001},
}

@article{ WOS:000435287000002,
Author = {Sagi, Omer and Rokach, Lior},
Title = {Ensemble learning: A survey},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY},
Year = {2018},
Volume = {8},
Number = {4},
Month = {JUL-AUG},
Abstract = {Ensemble methods are considered the state-of-the art solution for many
   machine learning challenges. Such methods improve the predictive
   performance of a single model by training multiple models and combining
   their predictions. This paper introduce the concept of ensemble
   learning, reviews traditional, novel and state-of-the-art ensemble
   methods and discusses current challenges and trends in the field. This
   article is categorized under: Algorithmic Development > Model Combining
   Technologies > Machine Learning Technologies > Classification},
DOI = {10.1002/widm.1249},
Article-Number = {e1249},
ISSN = {1942-4787},
EISSN = {1942-4795},
ResearcherID-Numbers = {Rokach, Lior/F-8247-2010},
Unique-ID = {WOS:000435287000002},
}

@article{ WOS:000327394500015,
Author = {Theiler, James},
Title = {Matched-Pair Machine Learning},
Journal = {TECHNOMETRICS},
Year = {2013},
Volume = {55},
Number = {4},
Pages = {536-547},
Month = {NOV},
Abstract = {Following an analogous distinction in statistical hypothesis testing and
   motivated by chemical plume detection in hyperspectral imagery, we
   investigate machine-learning algorithms where the training set is
   comprised of matched pairs. We find that even conventional classifiers
   exhibit improved performance when the input data have a matched-pair
   structure, and we develop an example of a ``dipole{''} algorithm to
   directly exploit this structured input. In some scenarios, matched pairs
   can be generated from independent samples, with the effect of not only
   doubling the nominal size of the training set, but of providing the
   matched-pair structure that leads to better learning. The creation of
   matched pairs from a dataset of interest also permits a kind of
   transductive learning, which is found for the plume detection problem to
   exhibit improved performance. Supplementary materials for this article
   are available online.},
DOI = {10.1080/00401706.2013.838191},
ISSN = {0040-1706},
EISSN = {1537-2723},
Unique-ID = {WOS:000327394500015},
}

@article{ WOS:000209646300007,
Author = {Agneeswaran, Vijay Srinivas and Tonpay, Pranay and Tiwary, Jayati},
Title = {PARADIGMS FOR REALIZING MACHINE LEARNING ALGORITHMS},
Journal = {BIG DATA},
Year = {2013},
Volume = {1},
Number = {4},
Pages = {BD207-BD214},
Month = {DEC},
Abstract = {The article explains the three generations of machine learning
   algorithms-with all three trying to operate on big data. The first
   generation tools are SAS, SPSS, etc., while second generation
   realizations include Mahout and RapidMiner (that work over Hadoop), and
   the third generation paradigms include Spark and GraphLab, among others.
   The essence of the article is that for a number of machine learning
   algorithms, it is important to look beyond the Hadoop's Map- Reduce
   paradigm in order to make them work on big data. A number of promising
   contenders have emerged in the third generation that can be exploited to
   realize deep analytics on big data.},
DOI = {10.1089/big.2013.0006},
ISSN = {2167-6461},
EISSN = {2167-647X},
ResearcherID-Numbers = {Agneeswaran, Vijay Srinivas/LWH-7627-2024
   },
ORCID-Numbers = {Agneeswaran, Vijay Srinivas/0009-0009-9891-7645},
Unique-ID = {WOS:000209646300007},
}

@article{ WOS:001068816800051,
Author = {Tian, Jinkai and Sun, Xiaoyu and Du, Yuxuan and Zhao, Shanshan and Liu,
   Qing and Zhang, Kaining and Yi, Wei and Huang, Wanrong and Wang, Chaoyue
   and Wu, Xingyao and Hsieh, Min-Hsiu and Liu, Tongliang and Yang, Wenjing
   and Tao, Dacheng},
Title = {Recent Advances for Quantum Neural Networks in Generative Learning},
Journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
Year = {2023},
Volume = {45},
Number = {10},
Pages = {12321-12340},
Month = {OCT},
Abstract = {Quantum computers are next-generation devices that hold promise to
   perform calculations beyond the reach of classical computers. A leading
   method towards achieving this goal is through quantum machine learning,
   especially quantum generative learning. Due to the intrinsic
   probabilistic nature of quantum mechanics, it is reasonable to postulate
   that quantum generative learning models (QGLMs) may surpass their
   classical counterparts. As such, QGLMs are receiving growing attention
   from the quantum physics and computer science communities, where various
   QGLMs that can be efficiently implemented on near-term quantum machines
   with potential computational advantages are proposed. In this paper, we
   review the current progress of QGLMs from the perspective of machine
   learning. Particularly, we interpret these QGLMs, covering quantum
   circuit Born machines, quantum generative adversarial networks, quantum
   Boltzmann machines, and quantum variational autoencoders, as the quantum
   extension of classical generative learning models. In this context, we
   explore their intrinsic relations and their fundamental differences. We
   further summarize the potential applications of QGLMs in both
   conventional machine learning tasks and quantum physics. Last, we
   discuss the challenges and further research directions for QGLMs.},
DOI = {10.1109/TPAMI.2023.3272029},
ISSN = {0162-8828},
EISSN = {1939-3539},
ResearcherID-Numbers = {zhang, kaining/AAU-5317-2020
   Tao, Dacheng/A-5449-2012
   Zhao, Shanshan/AFM-7084-2022
   Du, Yuxuan/LOS-6085-2024
   Sun, Xiaoyu/CAG-6501-2022
   Wang, Chaoyue/IWE-0324-2023
   Liu, qing/KPB-2272-2024
   Liu, Tongliang/AAA-1506-2021},
ORCID-Numbers = {Liu, Qing/0000-0002-1576-1975
   Zhao, Shanshan/0000-0003-0682-8645
   DU, YUXUAN/0000-0002-1193-9756
   SUN, Xiaoyu/0009-0005-1249-142X
   Wu, Xingyao/0009-0008-2018-0367
   Huang, Wanrong/0000-0001-5778-9055
   Wang, Chaoyue/0000-0002-9002-1029
   },
Unique-ID = {WOS:001068816800051},
}

@article{ WOS:000645896700002,
Author = {Schoelkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke,
   Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
Title = {Toward Causal Representation Learning},
Journal = {PROCEEDINGS OF THE IEEE},
Year = {2021},
Volume = {109},
Number = {5},
Pages = {612-634},
Month = {MAY},
Abstract = {The two fields of machine learning and graphical causality arose and are
   developed separately. However, there is, now, cross-pollination and
   increasing interest in both fields to benefit from the advances of the
   other. In this article, we review fundamental concepts of causal
   inference and relate them to crucial open problems of machine learning,
   including transfer and generalization, thereby assaying how causality
   can contribute to modern machine learning research. This also applies in
   the opposite direction: we note that most work in causality starts from
   the premise that the causal variables are given. A central problem for
   AI and causality is, thus, causal representation learning, that is, the
   discovery of high-level causal variables from low-level observations.
   Finally, we delineate some implications of causality for machine
   learning and propose key research areas at the intersection of both
   communities.},
DOI = {10.1109/JPROC.2021.3058954},
ISSN = {0018-9219},
EISSN = {1558-2256},
ResearcherID-Numbers = {Schölkopf, Bernhard/A-7570-2013
   Locatello, Francesco/GQY-6025-2022
   Scholkopf, Bernhard/A-7570-2013},
ORCID-Numbers = {Bauer, Stefan/0000-0003-1712-060X
   Bengio, Yoshua/0000-0002-9322-3515
   Locatello, Francesco/0000-0002-4850-0683
   Scholkopf, Bernhard/0000-0002-8177-0925},
Unique-ID = {WOS:000645896700002},
}

@article{ WOS:000660868300001,
Author = {Novikov, Ivan S. and Gubaev, Konstantin and Podryabinkin, V, Evgeny and
   Shapeev, V, Alexander},
Title = {The MLIP package: moment tensor potentials with MPI and active learning},
Journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {2},
Number = {2},
Month = {JUN},
Abstract = {The subject of this paper is the technology (the `how') of constructing
   machine-learning interatomic potentials, rather than science (the `what'
   and `why') of atomistic simulations using machine-learning potentials.
   Namely, we illustrate how to construct moment tensor potentials using
   active learning as implemented in the MLIP package, focusing on the
   efficient ways to automatically sample configurations for the training
   set, how expanding the training set changes the error of predictions,
   how to set up ab initio calculations in a cost-effective manner, etc.
   The MLIP package (short for Machine-Learning Interatomic Potentials) is
   available at https://mlip.skoltech.ru/download/.},
DOI = {10.1088/2632-2153/abc9fe},
Article-Number = {025002},
EISSN = {2632-2153},
ResearcherID-Numbers = {Novikov, Ivan/MTE-8759-2025
   Podryabinkin, Evgeny/GZK-2538-2022},
ORCID-Numbers = {Novikov, Ivan/0000-0003-3146-6455
   Gubaev, Konstantin/0000-0003-2612-8515
   Shapeev, Alexander/0000-0002-7497-5594
   },
Unique-ID = {WOS:000660868300001},
}

@article{ WOS:000314529000003,
Author = {Gould, Stephen},
Title = {DARWIN: A Framework for Machine Learning and Computer Vision Research
   and Development},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2012},
Volume = {13},
Pages = {3533-3537},
Month = {DEC},
Abstract = {We present an open-source platform-independent C++ framework for machine
   learning and computer vision research. The framework includes a wide
   range of standard machine learning and graphical models algorithms as
   well as reference implementations for many machine learning and computer
   vision applications. The framework contains Matlab wrappers for core
   components of the library and an experimental graphical user interface
   for developing and visualizing machine learning data flows.},
ISSN = {1532-4435},
Unique-ID = {WOS:000314529000003},
}

@article{ WOS:000518368000010,
Author = {Lussier, Felix and Thibault, Vincent and Charron, Benjamin and Wallace,
   Gregory Q. and Masson, Jean-Francois},
Title = {Deep learning and artificial intelligence methods for Raman and
   surface-enhanced Raman scattering},
Journal = {TRAC-TRENDS IN ANALYTICAL CHEMISTRY},
Year = {2020},
Volume = {124},
Month = {MAR},
Abstract = {Machine learning is shaping up our lives in many ways. In analytical
   sciences, machine learning provides an unprecedented opportunity to
   extract information from complex or big datasets in chromatography, mass
   spectrometry, NMR, and spectroscopy, among others. This is especially
   the case in Raman and surface-enhanced Raman scattering (SERS)
   techniques where vibrational spectra of complex chemical mixtures are
   acquired as large datasets for the analysis or imaging of chemical
   systems. The classical linear methods of processing the information no
   longer suffice and thus machine learning methods for extracting the
   chemical information from Raman and SERS experiments have been
   implemented recently. In this review, we will provide a brief overview
   of the most common machine learning techniques employed in Raman, a
   guideline for new users to implement machine learning in their data
   analysis process, and an overview of modern applications of machine
   learning in Raman and SERS. (C) 2019 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.trac.2019.115796},
Article-Number = {115796},
ISSN = {0165-9936},
EISSN = {1879-3142},
ResearcherID-Numbers = {Masson, Jean-Francois/C-4893-2008
   },
ORCID-Numbers = {Lussier, Felix/0000-0002-9499-7993
   Charron, Benjamin/0000-0002-4127-6981
   Wallace, Gregory/0000-0003-0702-3734
   Thibault, Vincent/0000-0001-8816-8163},
Unique-ID = {WOS:000518368000010},
}

@article{ WOS:000302331900002,
Author = {Brodley, Carla E. and Rebbapragada, Umaa and Small, Kevin and Wallace,
   Byron C.},
Title = {Challenges and Opportunities in Applied Machine Learning},
Journal = {AI MAGAZINE},
Year = {2012},
Volume = {33},
Number = {1},
Pages = {11-24},
Month = {SPR},
Abstract = {Machine-learning research is often conducted in vitro, divorced from
   motivating practical applications. A researcher might develop a new
   method for the general task of classification, then assess its utility
   by comparing its performance (for example, accuracy or AUC) to that of
   existing classification models on publicly available data sets. In terms
   of advancing machine learning as an academic discipline, this approach
   has thus far proven quite fruitful. However, it is our view that the
   most interesting open problems in machine learning are those that arise
   during its application to real-world problems. We illustrate this point
   by reviewing two of our interdisciplinary collaborations, both of which
   have posed unique machine-learning problems, providing fertile ground
   for novel research.},
DOI = {10.1609/aimag.v33i1.2367},
ISSN = {0738-4602},
Unique-ID = {WOS:000302331900002},
}

@article{ WOS:000647800900004,
Author = {Zhang, Xiao-Cheng and Gong, Jian-Guo and Xuan, Fu-Zhen},
Title = {A deep learning based life prediction method for components under creep,
   fatigue and creep-fatigue conditions},
Journal = {INTERNATIONAL JOURNAL OF FATIGUE},
Year = {2021},
Volume = {148},
Month = {JUL},
Abstract = {Deep learning is a particular kind of machine learning, which achieves
   great power and flexibility by a nested hierarchy of concepts. A general
   life prediction method for components under creep, fatigue and
   creep-fatigue conditions is proposed. Fatigue, creep and creep-fatigue
   data of a typical austenitic stainless steel (i.e., 316) are integrated.
   Conventional machine learning models (e.g., support vector machine,
   random forest, Gaussian process regression, shallow neural network) and
   deep learning model (e.g., deep neural network) are applied for life
   predictions. Results show that deep learning model exhibits better
   prediction accuracy and generalization ability than conventional machine
   learning model.},
DOI = {10.1016/j.ijfatigue.2021.106236},
EarlyAccessDate = {MAR 2021},
Article-Number = {106236},
ISSN = {0142-1123},
EISSN = {1879-3452},
ResearcherID-Numbers = {Gong, Jian-Guo/HKW-5536-2023
   xuan, fu-zhen/L-5683-2016},
Unique-ID = {WOS:000647800900004},
}

@article{ WOS:000252222600001,
Author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.},
Title = {Machine learning: a review of classification and combining techniques},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2006},
Volume = {26},
Number = {3},
Pages = {159-190},
Month = {NOV},
Abstract = {Supervised classification is one of the tasks most frequently carried
   out by so-called Intelligent Systems. Thus, a large number of techniques
   have been developed based on Artificial Intelligence (Logic-based
   techniques, Perceptron-based techniques) and Statistics (Bayesian
   Networks, Instance-based techniques). The goal of supervised learning is
   to build a concise model of the distribution of class labels in terms of
   predictor features. The resulting classifier is then used to assign
   class labels to the testing instances where the values of the predictor
   features are known, but the value of the class label is unknown. This
   paper describes various classification algorithms and the recent attempt
   for improving classification accuracy-ensembles of classifiers.},
DOI = {10.1007/s10462-007-9052-3},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Kotsiantis, Sotiris/A-8308-2016
   kotsiantis, sotiris/C-5640-2009},
ORCID-Numbers = {kotsiantis, sotiris/0000-0002-2247-3082},
Unique-ID = {WOS:000252222600001},
}

@article{ WOS:000283941100019,
Author = {Lavesson, Niklas},
Title = {Learning Machine Learning: A Case Study},
Journal = {IEEE TRANSACTIONS ON EDUCATION},
Year = {2010},
Volume = {53},
Number = {4},
Pages = {672-676},
Month = {NOV},
Abstract = {This correspondence reports on a case study conducted in the
   Master's-level Machine Learning (ML) course at Blekinge Institute of
   Technology, Sweden. The students participated in a self-assessment test
   and a diagnostic test of prerequisite subjects, and their results on
   these tests are correlated with their achievement of the course's
   learning objectives.},
DOI = {10.1109/TE.2009.2038992},
ISSN = {0018-9359},
ResearcherID-Numbers = {Lavesson, Niklas/B-3313-2013},
ORCID-Numbers = {Lavesson, Niklas/0000-0002-0535-1761
   },
Unique-ID = {WOS:000283941100019},
}

@article{ WOS:000474499500001,
Author = {Zuo, Renguang and Xiong, Yihui and Wang, Jian and Carranza, Emmanuel
   John M.},
Title = {Deep learning and its application in geochemical mapping},
Journal = {EARTH-SCIENCE REVIEWS},
Year = {2019},
Volume = {192},
Pages = {1-14},
Month = {MAY},
Abstract = {Machine learning algorithms have been applied widely in the fields of
   natural science, social science and engineering. It can be expected that
   machine learning approaches especially deep learning algorithms will
   help geoscientists to discover mineral deposits through processing of
   various geoscience datasets. This study reviews the state-of-the-art
   application of deep learning algorithms for processing geochemical
   exploration data and mining the geochemical patterns. Deep learning
   algorithms can deal with complex and nonlinear problems and, therefore,
   can enhance the identification of geochemical anomalies and the
   recognition of hidden patterns. Applied geochemistry needs more
   applications of machine learning and/or deep learning algorithms.},
DOI = {10.1016/j.earscirev.2019.02.023},
ISSN = {0012-8252},
EISSN = {1872-6828},
ResearcherID-Numbers = {Zuo, Renguang/AAW-4922-2021
   Wang, Jian/ABH-2334-2021
   Carranza, Emmanuel John/D-3837-2009},
Unique-ID = {WOS:000474499500001},
}

@article{ WOS:000267732400003,
Author = {Stumpf, Simone and Rajaram, Vidya and Li, Lida and Wong, Weng-Keen and
   Burnett, Margaret and Dietterich, Thomas and Sullivan, Erin and
   Herlocker, Jonathan},
Title = {Interacting meaningfully with machine learning systems: Three
   experiments},
Journal = {INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES},
Year = {2009},
Volume = {67},
Number = {8},
Pages = {639-662},
Month = {AUG},
Abstract = {Although machine learning is becoming commonly used in today's software,
   there has been little research into how end users might interact with
   machine learning systems, beyond communicating simple ``right/wrong{''}
   judgments. If the users themselves could work hand-in-hand with machine
   learning systems, the users' understanding and trust of the system could
   improve and the accuracy of learning systems could be improved as well.
   We conducted three experiments to understand the potential for rich
   interactions between users and machine learning systems. The first
   experiment was a think-aloud study that investigated users' willingness
   to interact with machine learning reasoning, and what kinds of feedback
   users might give to machine learning systems. We then investigated the
   viability of introducing such feedback into machine learning systems,
   specifically, how to incorporate some of these types of user feedback
   into machine learning systems, and what their impact was on the accuracy
   of the system. Taken together, the results of our experiments show that
   supporting rich interactions between users and machine learning systems
   is feasible for both user and machine. This shows the potential of rich
   human-computer collaboration via on-the-spot interactions as a promising
   direction for machine learning systems and users to collaboratively
   share intelligence. Published by Elsevier Ltd.},
DOI = {10.1016/j.ijhcs.2009.03.004},
ISSN = {1071-5819},
EISSN = {1095-9300},
ORCID-Numbers = {Stumpf, Simone/0000-0001-6482-1973},
Unique-ID = {WOS:000267732400003},
}

@article{ WOS:000258760000002,
Author = {Dietterich, Thomas G. and Domingos, Pedro and Getoor, Lise and
   Muggleton, Stephen and Tadepalli, Prasad},
Title = {Structured machine learning: the next ten years},
Journal = {MACHINE LEARNING},
Year = {2008},
Volume = {73},
Number = {1},
Pages = {3-23},
Month = {OCT},
Note = {Joint Conference of the 17th International Conference on Inductive Logic
   Programming/24th International Conference on Machine Learning,
   Corvallis, OR, JUN 19-21, 2007},
Abstract = {The field of inductive logic programming (ILP) has made steady progress,
   since the first ILP workshop in 1991, based on a balance of developments
   in theory, implementations and applications. More recently there has
   been an increased emphasis on Probabilistic ILP and the related fields
   of Statistical Relational Learning (SRL) and Structured Prediction. The
   goal of the current paper is to consider these emerging trends and chart
   out the strategic directions and open problems for the broader area of
   structured machine learning for the next 10 years.},
DOI = {10.1007/s10994-008-5079-1},
ISSN = {0885-6125},
EISSN = {1573-0565},
ORCID-Numbers = {Tadepalli, Prasad/0000-0003-2736-3912},
Unique-ID = {WOS:000258760000002},
}

@incollection{ WOS:000448519100007,
Author = {Guest, Dan and Cranmer, Kyle and Whiteson, Daniel},
Editor = {Holstein, BR},
Title = {Deep Learning and Its Application to LHC Physics},
Booktitle = {ANNUAL REVIEW OF NUCLEAR AND PARTICLE SCIENCE, VOL 68},
Series = {Annual Review of Nuclear and Particle Science},
Year = {2018},
Volume = {68},
Pages = {161-181},
Abstract = {Machine learning has played an important role in the analysis of
   high-energy physics data for decades. The emergence of deep learning in
   2012 allowed for machine learning tools which could adeptly handle
   higher-dimensional and more complex problems than previously feasible.
   This review is aimed at the reader who is familiar with high-energy
   physics but not machine learning. The connections between machine
   learning and high-energy physics data analysis arc explored, followed by
   an introduction to the core concepts of neural networks, examples of the
   key results demonstrating the power of deep learning for analysis of
   LIIC data, and discussion of future prospects and concerns.},
DOI = {10.1146/annurev-nucl-101917-021019},
ISSN = {0163-8998},
EISSN = {1545-4134},
ISBN = {978-0-8243-1568-9},
ResearcherID-Numbers = {Ratz, Michael/AAW-6840-2021},
Unique-ID = {WOS:000448519100007},
}

@article{ WOS:000241792300002,
Author = {Friedman, Jerome H.},
Title = {Recent advances in predictive (machine) learning},
Journal = {JOURNAL OF CLASSIFICATION},
Year = {2006},
Volume = {23},
Number = {2},
Pages = {175-197},
Month = {SEP},
Note = {13th International Meeting of the Psychometric-Society/68th Annual
   Meeting of the Psychometric-Society, Cagliari, ITALY, JUL09, 2003},
Organization = {Psychometr Soc},
Abstract = {Prediction involves estimating the unknown value of an attribute of a
   system under study given the values of other measured attributes. In
   prediction (machine) learning the prediction rule is derived from data
   consisting of previously solved cases. Most methods for predictive
   learning were originated many years ago at the dawn of the computer age.
   Recently two new techniques have emerged that have revitalized the
   field. These are support vector machines and boosted decision trees.
   This paper provides an introduction to these two new methods tracing
   their respective ancestral roots to standard kernel methods and ordinary
   decision trees.},
DOI = {10.1007/s00357-006-0012-4},
ISSN = {0176-4268},
EISSN = {1432-1343},
Unique-ID = {WOS:000241792300002},
}

@article{ WOS:000245388800005,
Author = {Bennett, Kristin P. and Parrado-Hernandez, Emilio},
Title = {The interplay of optimization and machine learning research},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2006},
Volume = {7},
Pages = {1265-1281},
Month = {JUL},
Abstract = {The fields of machine learning and mathematical programming are
   increasingly intertwined. Optimization problems lie at the heart of most
   machine learning approaches. The Special Topic on Machine Learning and
   Large Scale Optimization examines this interplay. Machine learning
   researchers have embraced the advances in mathematical programming
   allowing new types of models to be pursued. The special topic includes
   models using quadratic, linear, second-order cone, semi-definite, and
   semi-infinite programs. We observe that the qualities of good
   optimization algorithms from the machine learning and optimization
   perspectives can be quite different. Mathematical programming puts a
   premium on accuracy, speed, and robustness. Since generalization is the
   bottom line in machine learning and training is normally done off-line,
   accuracy and small speed improvements are of little concern in machine
   learning. Machine learning prefers simpler algorithms that work in
   reasonable computational time for specific classes of problems. Reducing
   machine learning problems to well-explored mathematical programming
   classes with robust general purpose optimization codes allows machine
   learning researchers to rapidly develop new techniques. In turn, machine
   learning presents new challenges to mathematical programming. The
   special issue include papers from two primary themes: novel machine
   learning models and novel optimization approaches for existing models.
   Many papers blend both themes, making small changes in the underlying
   core mathematical program that enable the develop of effective new
   algorithms.},
ISSN = {1532-4435},
ResearcherID-Numbers = {PARRADO-HERNANDEZ, EMILIO/ABH-2027-2020},
ORCID-Numbers = {PARRADO-HERNANDEZ, EMILIO/0000-0003-2146-2135
   },
Unique-ID = {WOS:000245388800005},
}

@article{ WOS:000231025700002,
Author = {Pham, D. T. and Afify, A. A.},
Title = {Machine-learning techniques and their applications in manufacturing},
Journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF
   ENGINEERING MANUFACTURE},
Year = {2005},
Volume = {219},
Number = {5},
Pages = {395-412},
Month = {MAY},
Abstract = {Machine learning is concerned with enabling computer programs
   automatically to improve their performance at some tasks through
   experience. Manufacturing is an area where the application of machine
   learning can be very fruitful. However, little has been published about
   the use of machine-learning techniques in the manufacturing domain. This
   paper evaluates several machine-learning techniques and examines
   applications in which they have been successfully deployed. Special
   attention is given to inductive learning, which is among the most mature
   of the machine-learning approaches currently available. Current trends
   and recent developments in machine-learning research are also discussed.
   The paper concludes with a summary of some of the key research issues in
   machine learning.},
DOI = {10.1243/095440505X32274},
ISSN = {0954-4054},
EISSN = {2041-2975},
ResearcherID-Numbers = {Pham, Duc/H-1516-2011
   Afify, Ashraf/AFQ-4076-2022},
ORCID-Numbers = {Pham, Duc/0000-0003-3148-2404
   },
Unique-ID = {WOS:000231025700002},
}

@article{ WOS:000629093200003,
Author = {Kim, Young-Min and Shin, Seung-Jun and Cho, Hae-Won},
Title = {Predictive Modeling for Machining Power Based on Multi-source Transfer
   Learning in Metal Cutting},
Journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN
   TECHNOLOGY},
Year = {2022},
Volume = {9},
Number = {1},
Pages = {107-125},
Month = {JAN},
Abstract = {Energy efficiency has become crucial in the metal cutting industry.
   Machining power has therefore become an important metric because it
   directly affects the energy consumed during the operation of a machine
   tool. Attempts to predict machining power using machine learning have
   relied on the training datasets processed from actual machining data to
   derive the numerical relationship between process parameters and
   machining power. However, real fields hardly provide training datasets
   because of the difficulties in data collection; consequently,
   traditional learning approaches are ineffective in such data-scarce or
   -absent environment. This paper proposes a transfer learning approach
   for the predictive modeling of machining power. The proposed approach
   creates machining power prediction models by transferring the knowledge
   acquired from prior machining to the target machining context where
   machining power data are absent. The proposed approach performs domain
   adaptation by adding workpiece material properties to the original
   feature space for accommodating different machining power patterns
   dependent on the types of workpiece materials. A case study demonstrates
   that the training datasets obtained from the fabrication of steel and
   aluminum materials can be successfully used to create the
   power-predictive models that anticipate machining power for titanium
   material.},
DOI = {10.1007/s40684-021-00327-6},
EarlyAccessDate = {MAR 2021},
ISSN = {2288-6206},
EISSN = {2198-0810},
ResearcherID-Numbers = {Youngmin, Kim/JGE-2412-2023},
ORCID-Numbers = {KIM, Young-Min/0000-0002-6914-901X
   },
Unique-ID = {WOS:000629093200003},
}

@article{ WOS:000341593600009,
Author = {Huang, Guang-Bin},
Title = {An Insight into Extreme Learning Machines: Random Neurons, Random
   Features and Kernels},
Journal = {COGNITIVE COMPUTATION},
Year = {2014},
Volume = {6},
Number = {3, SI},
Pages = {376-390},
Month = {SEP},
Abstract = {Extreme learning machines (ELMs) basically give answers to two
   fundamental learning problems: (1) Can fundamentals of learning (i.e.,
   feature learning, clustering, regression and classification) be made
   without tuning hidden neurons (including biological neurons) even when
   the output shapes and function modeling of these neurons are unknown?
   (2) Does there exist unified framework for feedforward neural networks
   and feature space methods? ELMs that have built some tangible links
   between machine learning techniques and biological learning mechanisms
   have recently attracted increasing attention of researchers in
   widespread research areas. This paper provides an insight into ELMs in
   three aspects, viz: random neurons, random features and kernels. This
   paper also shows that in theory ELMs (with the same kernels) tend to
   outperform support vector machine and its variants in both regression
   and classification applications with much easier implementation.},
DOI = {10.1007/s12559-014-9255-2},
ISSN = {1866-9956},
EISSN = {1866-9964},
ResearcherID-Numbers = {Huang, Guang-Bin/A-5035-2011
   Huang, Guang-Bin/JZE-2974-2024},
ORCID-Numbers = {Huang, Guang-Bin/0000-0002-2480-4965},
Unique-ID = {WOS:000341593600009},
}

@article{ WOS:000697350100008,
Author = {Shafee, Ahmed and Awaad, Tasneem A.},
Title = {Privacy attacks against deep learning models and their countermeasures},
Journal = {JOURNAL OF SYSTEMS ARCHITECTURE},
Year = {2021},
Volume = {114},
Month = {MAR},
Abstract = {Keywords: Adversarial machine learning Convolutional neural network Deep
   neural network Machine learning},
DOI = {10.1016/j.sysarc.2020.101940},
EarlyAccessDate = {FEB 2021},
Article-Number = {101940},
ISSN = {1383-7621},
EISSN = {1873-6165},
ResearcherID-Numbers = {Awaad, Tasneem/AGZ-7136-2022
   },
ORCID-Numbers = {Shafee, Ahmed/0000-0002-4119-9525
   Awaad, Tasneem/0000-0003-4372-8600},
Unique-ID = {WOS:000697350100008},
}

@article{ WOS:000623001500001,
Author = {Lamata, Lucas},
Title = {Quantum Reinforcement Learning with Quantum Photonics},
Journal = {PHOTONICS},
Year = {2021},
Volume = {8},
Number = {2},
Month = {FEB},
Abstract = {Quantum machine learning has emerged as a promising paradigm that could
   accelerate machine learning calculations. Inside this field, quantum
   reinforcement learning aims at designing and building quantum agents
   that may exchange information with their environment and adapt to it,
   with the aim of achieving some goal. Different quantum platforms have
   been considered for quantum machine learning and specifically for
   quantum reinforcement learning. Here, we review the field of quantum
   reinforcement learning and its implementation with quantum photonics.
   This quantum technology may enhance quantum computation and
   communication, as well as machine learning, via the fruitful marriage
   between these previously unrelated fields.},
DOI = {10.3390/photonics8020033},
Article-Number = {33},
EISSN = {2304-6732},
ResearcherID-Numbers = {Lamata, Lucas/CAG-6488-2022},
ORCID-Numbers = {Lamata, Lucas/0000-0002-9504-8685},
Unique-ID = {WOS:000623001500001},
}

@article{ WOS:000544215800001,
Author = {Afzal, A. L. and Nair, Nikhitha K. and Asharaf, S.},
Title = {Deep kernel learning in extreme learning machines},
Journal = {PATTERN ANALYSIS AND APPLICATIONS},
Year = {2021},
Volume = {24},
Number = {1},
Pages = {11-19},
Month = {FEB},
Abstract = {Emergence of extreme learning machine as a breakneck learning algorithm
   has marked its prominence in solitary hidden layer feed-forward
   networks. Kernel-based extreme learning machine (KELM) reflected its
   efficiency in diverse applications where feature mapping functions of
   hidden nodes are concealed from users. The conventional KELM algorithms
   involve only solitary layer of kernels, thereby emulating shallow
   learning architectures for its feature transformation. Trend in
   migrating shallow-based learning models into deep learning architectures
   opens up a new outlook for machine learning domains. This paper attempts
   to bestow deep kernel learning approach in a conventional shallow
   architecture. The emerging arc-cosine kernels possess the potential to
   mimic the prevailing deep layered frameworks to a greater extent. Unlike
   other kernels such as linear, polynomial and Gaussian, arc-cosine
   kernels have a recursive nature by itself and have the potential to
   express multilayer computation in learning models. This paper explores
   the possibility of building a new deep kernel machine with extreme
   learning machine and multilayer arc-cosine kernels. This framework
   outperforms conventional KELM and deep support vector machine in terms
   of training time and accuracy.},
DOI = {10.1007/s10044-020-00891-8},
EarlyAccessDate = {JUN 2020},
ISSN = {1433-7541},
EISSN = {1433-755X},
ORCID-Numbers = {NAIR, NIKHITHA/0009-0009-4102-511X},
Unique-ID = {WOS:000544215800001},
}

@article{ WOS:000494800100001,
Author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos},
Title = {Learning Moore machines from input-output traces},
Journal = {INTERNATIONAL JOURNAL ON SOFTWARE TOOLS FOR TECHNOLOGY TRANSFER},
Year = {2021},
Volume = {23},
Number = {1, SI},
Pages = {1-29},
Month = {FEB},
Abstract = {The problem of learning automata from example traces (but no equivalence
   or membership queries) is fundamental in automata learning theory and
   practice. In this paper, we study this problem for finite-state machines
   with inputs and outputs, and in particular for Moore machines. We
   develop three algorithms for solving this problem: (1) the PTAP
   algorithm, which transforms a set of input-output traces into an
   incomplete Moore machine and then completes the machine with self-loops;
   (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for
   automata learning to learn a product of automata encoding a Moore
   machine; and (3) the MooreMI algorithm, which directly learns a Moore
   machine using PTAP extended with state merging. We prove that MooreMI
   has the fundamental identification in the limit property. We compare the
   algorithms experimentally in terms of the size of the learned machine
   and several notions of accuracy, introduced in this paper. We also carry
   out a performance comparison against two existing tools (LearnLib and
   flexfringe). Finally, we compare with OSTIA, an algorithm that learns a
   more general class of transducers and find that OSTIA generally does not
   learn a Moore machine, even when fed with a characteristic sample.},
DOI = {10.1007/s10009-019-00544-0},
EarlyAccessDate = {NOV 2019},
ISSN = {1433-2779},
EISSN = {1433-2787},
ORCID-Numbers = {Basagiannis, Stylianos/0000-0002-4513-0541},
Unique-ID = {WOS:000494800100001},
}

@article{ WOS:000424176900011,
Author = {Afzal, A. L. and Asharaf, S.},
Title = {Deep multiple multilayer kernel learning in core vector machines},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2018},
Volume = {96},
Pages = {149-156},
Month = {APR 15},
Abstract = {Over the last few years, we have been witnessing a dramatic progress of
   deep learning in many real world applications. Deep learning concepts
   have been originated in the area of neural network and show a quantum
   leap in effective feature learning techniques such as auto-encoders,
   convolutional neural networks, recurrent neural networks etc. In the
   case of kernel machines, there are several attempts to model learning
   machines that mimic deep neural networks. In this direction, Multilayer
   Kernel Machines (MKMs) was an attempt to build a kernel machine
   architecture with multiple layers of feature extraction. It composed of
   many layers of kernel PCA based feature extraction units with support
   vector machine having arc-cosine kernel as the final classifier. The
   other approaches like Multiple Kernel Learning (MKL) and deep core
   vector machines solve the fixed kernel computation problem and
   scalability aspects of MKMs respectively. In addition to this, there are
   lot of avenues where the use of unsupervised MKL with both single and
   multilayer kernels in the multilayer feature extraction framework have
   to be evaluated. In this context, this paper attempts to build a
   scalable deep kernel machines with multiple layers of feature
   extraction. Each kernel PCA based feature extraction layer in this
   framework is modeled by the combination of both single and multilayer
   kernels in an unsupervised manner. Core vector machine with arc-cosine
   kernel is used as the final layer classifier which ensure the
   scalability in this model. The major contribution of this paper is a
   novel effort to build a deep structured kernel machine architecture
   similar to deep neural network architecture for classification. It opens
   up an extendable research avenue for researchers in deep learning based
   intelligent system leveraging the principles of kernel theory.
   Experiments show that the proposed method consistently improves the
   generalization performances of existing deep core vector machine. (C)
   2017 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.eswa.2017.11.058},
ISSN = {0957-4174},
EISSN = {1873-6793},
ORCID-Numbers = {A.L., AFZAL/0000-0001-5356-1918},
Unique-ID = {WOS:000424176900011},
}

@article{ WOS:000927333400001,
Author = {Akkem, Yaganteeswarudu and Biswas, Saroj Kumar and Varanasi, Aruna},
Title = {Smart farming using artificial intelligence: A review},
Journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
Year = {2023},
Volume = {120},
Month = {APR},
Abstract = {Smart farming with artificial intelligence provides an efficient
   solution to today's agricultural sustainability challenges. Machine
   learning, Deep learning, and time series analysis are essential in smart
   farming. Crop selection, crop yield prediction, soil compatibility
   classification, water management, and many other processes are involved
   in agriculture. Machine learning algorithms are used for crop selection
   and management, Deep learning techniques are used for crop selection and
   forecasting crop production, and time series analysis is used for demand
   forecasting of crops, commodity price prediction, and crop yield
   production forecasting. Crops are chosen using machine learning
   algorithms and deep learning algorithms based on soil, soil
   compatibility classification, and other factors. In the agriculture
   industry, this article offers a thorough review of machine learning and
   deep learning techniques. Crop data sets can be used to classify soil
   fertility, crop selection, and many other aspects using machine learning
   algorithms. Deep learning algorithms can be applied to farming data to
   do time series analysis and crop selection. Because there is more need
   for food due to the growing population, crop production forecasting is
   one of the crucial tasks. Therefore, future crop production must be
   predicted in order to overcome food insufficiency. In this article,
   several time series algorithms were reviewed. Suggesting appropriate
   crop recommendations using machine and deep learning by estimating crop
   yield by using time series analysis will reduce food insufficiency in
   the future.},
DOI = {10.1016/j.engappai.2023.105899},
EarlyAccessDate = {JAN 2023},
Article-Number = {105899},
ISSN = {0952-1976},
EISSN = {1873-6769},
ResearcherID-Numbers = {Varanasi, Dr. Aruna/MDT-6342-2025
   Akkem, Yaganteeswarudu/JHT-7322-2023
   },
ORCID-Numbers = {Akkem, Yaganteeswarudu/0000-0002-5637-7064},
Unique-ID = {WOS:000927333400001},
}

@article{ WOS:001242376600007,
Author = {Kim, Dohyung and Min, JinKi and Ko, Seung Hwan},
Title = {Recent Developments and Future Directions of Wearable Skin Biosignal
   Sensors},
Journal = {ADVANCED SENSOR RESEARCH},
Year = {2024},
Volume = {3},
Number = {2},
Month = {FEB},
Abstract = {This review article explores the transformative advancements in wearable
   biosignal sensors powered by machine learning, focusing on four notable
   biosignals: electrocardiogram (ECG), electromyogram (EMG),
   electroencephalogram (EEG), and photoplethysmogram (PPG). The
   integration of machine learning with these biosignals has led to
   remarkable breakthroughs in various medical monitoring and human-machine
   interface applications. For ECG, machine learning enables automated
   heartbeat classification and accurate disease detection, improving
   cardiac healthcare with early diagnosis and personalized interventions.
   EMG technology, combined with machine learning, facilitates real-time
   prediction and classification of human motions, revolutionizing
   applications in sports medicine, rehabilitation, prosthetics, and
   virtual reality interfaces. EEG analysis powered by machine learning
   goes beyond traditional clinical applications, enabling brain activity
   understanding in psychology, neurology, and human-computer interaction,
   and holds promise in brain-computer interfaces. PPG, augmented with
   machine learning, has shown exceptional progress in diagnosing and
   monitoring cardiovascular and respiratory disorders, offering
   non-invasive and accurate healthcare solutions. These integrated
   technologies, powered by machine learning, open new avenues for medical
   monitoring and human-machine interaction, shaping the future of
   healthcare.
   The convergence of wearable biosignal sensors and machine learning paves
   the way for significant advancements in healthcare, enabling early
   medical diagnosis and personalized health monitoring. This review
   article provides an overview of recent transformative advancements in
   wearable biosignal sensors powered by machine learning, focusing on four
   notable biosignals: electrocardiogram, electromyogram,
   electroencephalogram, and photoplethysmogram. image},
DOI = {10.1002/adsr.202300118},
EarlyAccessDate = {OCT 2023},
Article-Number = {2300118},
ISSN = {2751-1219},
ResearcherID-Numbers = {Ko, Seung Hwan/B-5448-2008},
ORCID-Numbers = {Ko, Seung Hwan/0000-0002-7477-0820},
Unique-ID = {WOS:001242376600007},
}

@article{ WOS:000705073600019,
Author = {Karmaker (Santu), Shubhra Kanti and Hassan, Md Mahadi and Smith, Micah
   J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan},
Title = {AutoML to Date and Beyond: Challenges and Opportunities},
Journal = {ACM COMPUTING SURVEYS},
Year = {2021},
Volume = {54},
Number = {8},
Month = {NOV},
Abstract = {As big data becomes ubiquitous across domains, and more and more
   stakeholders aspire to make the most of their data, demand for machine
   learning tools has spurred researchers to explore the possibilities of
   automated machine learning (AutoML). AutoML tools aim to make machine
   learning accessible for non-machine learning experts (domain experts),
   to improve the efficiency of machine learning, and to accelerate machine
   learning research. But although automation and efficiency are among
   AutoML's main selling points, the process still requires human
   involvement at a number of vital steps, including understanding the
   attributes of domain-specific data, defining prediction problems,
   creating a suitable training dataset, and selecting a promising machine
   learning technique. These steps often require a prolonged back-and-forth
   that makes this process inefficient for domain experts and data
   scientists alike and keeps so-called AutoML systems from being truly
   automatic. In this review article, we introduce a new classification
   system for AutoML systems, using a seven-tiered schematic to distinguish
   these systems based on their level of autonomy. We begin by describing
   what an end-to-end machine learning pipeline actually looks like, and
   which subtasks of the machine learning pipeline have been automated so
   far. We highlight those subtasks that are still done manually-generally
   by a data scientist-and explain how this limits domain experts' access
   to machine learning. Next, we introduce our novel level-based taxonomy
   for AutoML systems and define each level according to the scope of
   automation support provided. Finally, we lay out a roadmap for the
   future, pinpointing the research required to further automate the
   end-to-end machine learning pipeline and discussing important challenges
   that stand in the way of this ambitious goal.},
DOI = {10.1145/3470918},
Article-Number = {175},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Hassan, MD Mahadi/KII-8658-2024},
ORCID-Numbers = {KARMAKER SANTU, SHUBHRA KANTI/0000-0001-5744-6925
   },
Unique-ID = {WOS:000705073600019},
}

@article{ WOS:000751250600001,
Author = {Bahri, Maroua and Salutari, Flavia and Putina, Andrian and Sozio, Mauro},
Title = {AutoML: state of the art with a focus on anomaly detection, challenges,
   and research directions},
Journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS},
Year = {2022},
Volume = {14},
Number = {2, SI},
Pages = {113-126},
Month = {AUG},
Abstract = {The last decade has witnessed the explosion of machine learning research
   studies with the inception of several algorithms proposed and
   successfully adopted in different application domains. However, the
   performance of multiple machine learning algorithms is very sensitive to
   multiple ingredients (e.g., hyper-parameters tuning and data cleaning)
   where a significant human effort is required to achieve good results.
   Thus, building well-performing machine learning algorithms requires
   domain knowledge and highly specialized data scientists. Automated
   machine learning (autoML) aims to make easier and more accessible the
   use of machine learning algorithms for researchers with varying levels
   of expertise. Besides, research effort to date has mainly been devoted
   to autoML for supervised learning, and only a few research proposals
   have been provided for the unsupervised learning. In this paper, we
   present an overview of the autoML field with a particular emphasis on
   the automated methods and strategies that have been proposed for
   unsupervised anomaly detection.},
DOI = {10.1007/s41060-022-00309-0},
EarlyAccessDate = {FEB 2022},
ISSN = {2364-415X},
EISSN = {2364-4168},
ORCID-Numbers = {Bahri, Maroua/0000-0002-7420-7464},
Unique-ID = {WOS:000751250600001},
}

@article{ WOS:000703968500001,
Author = {Davis, Jenny L. and Williams, Apryl and Yang, Michael W.},
Title = {Algorithmic reparation},
Journal = {BIG DATA \& SOCIETY},
Year = {2021},
Volume = {8},
Number = {2},
Month = {JUL},
Abstract = {Machine learning algorithms pervade contemporary society. They are
   integral to social institutions, inform processes of governance, and
   animate the mundane technologies of daily life. Consistently, the
   outcomes of machine learning reflect, reproduce, and amplify structural
   inequalities. The field of fair machine learning has emerged in
   response, developing mathematical techniques that increase fairness
   based on anti-classification, classification parity, and calibration
   standards. In practice, these computational correctives invariably fall
   short, operating from an algorithmic idealism that does not, and cannot,
   address systemic, Intersectional stratifications. Taking present fair
   machine learning methods as our point of departure, we suggest instead
   the notion and practice of algorithmic reparation. Rooted in theories of
   Intersectionality, reparative algorithms name, unmask, and undo
   allocative and representational harms as they materialize (American
   English sp) in sociotechnical form. We propose algorithmic reparation as
   a foundation for building, evaluating, adjusting, and when necessary,
   omitting and eradicating machine learning systems.},
DOI = {10.1177/20539517211044808},
Article-Number = {20539517211044808},
ISSN = {2053-9517},
ResearcherID-Numbers = {Davis, Jenny L/P-1898-2017
   Davis, Jenny/P-1898-2017},
ORCID-Numbers = {Williams, Apryl/0000-0003-4896-9366
   Davis, Jenny L/0000-0003-0952-5842
   },
Unique-ID = {WOS:000703968500001},
}

@article{ WOS:000452640000033,
Author = {Ming, Yao and Qu, Huamin and Bertini, Enrico},
Title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules},
Journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS},
Year = {2019},
Volume = {25},
Number = {1},
Pages = {342-352},
Month = {JAN},
Abstract = {With the growing adoption of machine learning techniques, there is a
   surge of research interest towards making machine learning systems more
   transparent and interpretable. Various visualizations have been
   developed to help model developers understand, diagnose, and refine
   machine learning models. However, a large number of potential but
   neglected users are the domain experts with little knowledge of machine
   learning but are expected to work with machine learning systems. In this
   paper, we present an interactive visualization technique to help users
   with little expertise in machine learning to understand, explore and
   validate predictive models. By viewing the model as a black box, we
   extract a standardized rule-based knowledge representation from its
   input-output behavior. Then, we design RuleMatrix, a matrix-based
   visualization of rules to help users navigate and verify the rules and
   the black-box model. We evaluate the effectiveness of RuleMatrix via two
   use cases and a usability study.},
DOI = {10.1109/TVCG.2018.2864812},
ISSN = {1077-2626},
EISSN = {1941-0506},
ResearcherID-Numbers = {Bertini, Enrico/ABG-1278-2020
   },
ORCID-Numbers = {Bertini, Enrico/0000-0002-9932-0551},
Unique-ID = {WOS:000452640000033},
}

@article{ WOS:000498849000042,
Author = {Thomas, Philip S. and da Silva, Bruno Castro and Barto, Andrew G. and
   Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
Title = {Preventing undesirable behavior of intelligent machines},
Journal = {SCIENCE},
Year = {2019},
Volume = {366},
Number = {6468},
Pages = {999+},
Month = {NOV 22},
Abstract = {Intelligent machines using machine learning algorithms are ubiquitous,
   ranging from simple data analysis and pattern recognition tools to
   complex systems that achieve superhuman performance on various tasks.
   Ensuring that they do not exhibit undesirable behavior-that they do not,
   for example, cause harm to humans-is therefore a pressing problem. We
   propose a general and flexible framework for designing machine learning
   algorithms. This framework simplifies the problem of specifying and
   regulating undesirable behavior. To show the viability of this
   framework, we used it to create machine learning algorithms that
   precluded the dangerous behavior caused by standard machine learning
   algorithms in our experiments. Our framework for designing machine
   learning algorithms simplifies the safe and responsible application of
   machine learning.},
DOI = {10.1126/science.aag3311},
ISSN = {0036-8075},
EISSN = {1095-9203},
ResearcherID-Numbers = {Castro da Silva, Bruno/C-5334-2015
   },
ORCID-Numbers = {Brunskill, Emma/0000-0002-3971-7127
   Castro da Silva, Bruno/0000-0002-3708-5728
   Brun, Yuriy/0000-0003-3027-7986},
Unique-ID = {WOS:000498849000042},
}

@article{ WOS:000572786300004,
Author = {Diaz-Vico, David and Prada, Jesus and Omari, Adil and Dorronsoro, Jose},
Title = {Deep support vector neural networks},
Journal = {INTEGRATED COMPUTER-AIDED ENGINEERING},
Year = {2020},
Volume = {27},
Number = {4},
Pages = {389-402},
Abstract = {Kernel based Support Vector Machines, SVM, one of the most popular
   machine learning models, usually achieve top performances in two-class
   classification and regression problems. However, their training cost is
   at least quadratic on sample size, making them thus unsuitable for large
   sample problems. However, Deep Neural Networks (DNNs), with a cost
   linear on sample size, are able to solve big data problems relatively
   easily. In this work we propose to combine the advanced representations
   that DNNs can achieve in their last hidden layers with the hinge and
   epsilon insensitive losses that are used in two-class SVM classification
   and regression. We can thus have much better scalability while achieving
   performances comparable to those of SVMs. Moreover, we will also show
   that the resulting Deep SVM models are competitive with standard DNNs in
   two-class classification problems but have an edge in regression ones.},
DOI = {10.3233/ICA-200635},
ISSN = {1069-2509},
EISSN = {1875-8835},
ResearcherID-Numbers = {Díaz-Vico, David/ADN-0697-2022
   Dorronsoro, Jose/F-4210-2012
   },
ORCID-Numbers = {Diaz-Vico, David/0000-0002-4002-5312},
Unique-ID = {WOS:000572786300004},
}

@article{ WOS:000569592200001,
Author = {Albahli, Saleh and Alhassan, Fatimah and Albattah, Waleed and Khan,
   Rehan Ullah},
Title = {Handwritten Digit Recognition: Hyperparameters-Based Analysis},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {17},
Month = {SEP},
Abstract = {Neural networks have several useful applications in machine learning.
   However, benefiting from the neural-network architecture can be tricky
   in some instances due to the large number of parameters that can
   influence performance. In general, given a particular dataset, a data
   scientist cannot do much to improve the efficiency of the model.
   However, by tuning certain hyperparameters, the model's accuracy and
   time of execution can be improved. Hence, it is of utmost importance to
   select the optimal values of hyperparameters. Choosing the optimal
   values of hyperparameters requires experience and mastery of the machine
   learning paradigm. In this paper, neural network-based architectures are
   tested based on altering the values of hyperparameters for
   handwritten-based digit recognition. Various neural network-based models
   are used to analyze different aspects of the same, primarily accuracy
   based on hyperparameter values. The extensive experimentation setup in
   this article should, therefore, provide the most accurate and
   time-efficient solution models. Such an evaluation will help in
   selecting the optimized values of hyperparameters for similar tasks.},
DOI = {10.3390/app10175988},
Article-Number = {5988},
EISSN = {2076-3417},
ResearcherID-Numbers = {khan, khairullah/F-7148-2011
   },
ORCID-Numbers = {Albattah, Waleed/0000-0003-0292-7304
   Khan, Rehan Ullah/0000-0003-3323-2732
   Albahli, Saleh/0000-0001-6317-4313},
Unique-ID = {WOS:000569592200001},
}

@article{ WOS:000345135700005,
Author = {Xie, Xijiong and Sun, Shiliang},
Title = {Multi-view Laplacian twin support vector machines},
Journal = {APPLIED INTELLIGENCE},
Year = {2014},
Volume = {41},
Number = {4},
Pages = {1059-1068},
Month = {DEC},
Abstract = {Twin support vector machines are a recently proposed learning method for
   pattern classification. They learn two hyperplanes rather than one as in
   usual support vector machines and often bring performance improvements.
   Semi-supervised learning has attracted great attention in machine
   learning in the last decade. Laplacian support vector machines and
   Laplacian twin support vector machines have been proposed in the
   semi-supervised learning framework. In this paper, inspired by the
   recent success of multi-view learning we propose multi-view Laplacian
   twin support vector machines, whose dual optimization problems are
   quadratic programming problems. We further extend them to kernel
   multi-view Laplacian twin support vector machines. Experimental results
   demonstrate that our proposed methods are effective.},
DOI = {10.1007/s10489-014-0563-8},
ISSN = {0924-669X},
EISSN = {1573-7497},
ResearcherID-Numbers = {Xie, Xijiong/JDM-5573-2023},
Unique-ID = {WOS:000345135700005},
}

@article{ WOS:000481413400025,
Author = {Hoch, Jeffrey C.},
Title = {If machines can learn, who needs scientists?},
Journal = {JOURNAL OF MAGNETIC RESONANCE},
Year = {2019},
Volume = {306},
Pages = {162-166},
Month = {SEP},
Abstract = {Machine learning has been used in NMR in for decades, but recent
   developments signal explosive growth is on the horizon. An obstacle to
   the application of machine learning in NMR is the relative paucity of
   available training data, despite the existence of numerous public NMR
   data repositories. Other challenges include the problem of interpreting
   the results of a machine learning algorithm, and incorporating machine
   learning into hypothesis-driven research. This perspective imagines the
   potential of machine learning in NMR and speculates on possible
   approaches to the hurdles. (C) 2019 Published by Elsevier Inc.},
DOI = {10.1016/j.jmr.2019.07.044},
ISSN = {1090-7807},
EISSN = {1096-0856},
ORCID-Numbers = {Hoch, Jeffrey/0000-0002-9230-2019},
Unique-ID = {WOS:000481413400025},
}

@article{ WOS:000461166500028,
Author = {Linh Le and Xie, Ying},
Title = {Deep embedding kernel},
Journal = {NEUROCOMPUTING},
Year = {2019},
Volume = {339},
Pages = {292-302},
Month = {APR 28},
Abstract = {In this paper, we propose a novel supervised learning method that is
   called Deep Embedding Kernel (DEK). DEK combines the advantages of deep
   learning and kernel methods in a unified framework. More specifically,
   DEK is a learnable kernel represented by a newly designed deep
   architecture. Compared with predefined kernels, this kernel can be
   explicitly trained to map data to an optimized high-level feature space
   where data may have favorable features toward the application. Compared
   with typical deep learning using SoftMax or logistic regression as the
   top layer, DEK is expected to be more generalizable to new data.
   Experimental results show that DEK has superior performance than typical
   machine learning methods in identity detection and classification, and
   transfer learning, on different types of data including images,
   sequences, and regularly structured data. (C) 2019 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.neucom.2019.02.037},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Le, Linh/IAR-6039-2023},
ORCID-Numbers = {Le, Linh/0000-0002-0087-3448
   },
Unique-ID = {WOS:000461166500028},
}

@article{ WOS:000467010400067,
Author = {Guo, Liang and Lei, Yaguo and Xing, Saibo and Yan, Tao and Li, Naipeng},
Title = {Deep Convolutional Transfer Learning Network: A New Method for
   Intelligent Fault Diagnosis of Machines With Unlabeled Data},
Journal = {IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS},
Year = {2019},
Volume = {66},
Number = {9},
Pages = {7316-7325},
Month = {SEP},
Abstract = {The success of intelligent fault diagnosis of machines relies on the
   following two conditions: 1) labeled data with fault information are
   available; and 2) the training and testing data are drawn from the same
   probability distribution. However, for some machines, it is difficult to
   obtain massive labeled data. Moreover, even though labeled data can be
   obtained from some machines, the intelligent fault diagnosis method
   trained with such labeled data possibly fails in classifying unlabeled
   data acquired from the other machines due to data distribution
   discrepancy. These problems limit the successful applications of
   intelligent fault diagnosis of machines with unlabeled data. As a
   potential tool, transfer learning adapts a model trained in a source
   domain to its application in a target domain. Based on the transfer
   learning, we propose a new intelligent method named deep convolutional
   transfer learning network (DCTLN). A DCTLN consists of two modules:
   condition recognition and domain adaptation. The condition recognition
   module is constructed by a one-dimensional (1-D) convolutional neural
   network (CNN) to automatically learn features and recognize health
   conditions of machines. The domain adaptation module facilitates the 1-D
   CNN to learn domain-invariant features by maximizing domain recognition
   errors and minimizing the probability distribution distance. The
   effectiveness of the proposed method is verified using six transfer
   fault diagnosis experiments.},
DOI = {10.1109/TIE.2018.2877090},
ISSN = {0278-0046},
EISSN = {1557-9948},
ResearcherID-Numbers = {Xing, Saibo/Y-8132-2019
   guo, Liang/HMO-8017-2023
   Li, Naipeng/AFF-2789-2022
   Yan, Tao/AAF-9181-2021
   Lei, Yaguo/N-4891-2014},
ORCID-Numbers = {Yan, Tao/0000-0002-3328-2118
   Li, Naipeng/0000-0003-0678-8161
   Guo, Liang/0000-0001-5338-4958
   },
Unique-ID = {WOS:000467010400067},
}

@article{ WOS:000720251600001,
Author = {Li, Yanan and Ren, Xuebin and Zhao, Fangyuan and Yang, Shusen},
Title = {A Zeroth-Order Adaptive Learning Rate Method to Reduce Cost of
   Hyperparameter Tuning for Deep Learning},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2021},
Volume = {11},
Number = {21},
Month = {NOV},
Abstract = {Due to powerful data representation ability, deep learning has
   dramatically improved the state-of-the-art in many practical
   applications. However, the utility highly depends on fine-tuning of
   hyper-parameters, including learning rate, batch size, and network
   initialization. Although many first-order adaptive methods (e.g., Adam,
   Adagrad) have been proposed to adjust learning rate based on gradients,
   they are susceptible to the initial learning rate and network
   architecture. Therefore, the main challenge of using deep learning in
   practice is how to reduce the cost of tuning hyper-parameters. To
   address this, we propose a heuristic zeroth-order learning rate method,
   Adacomp, which adaptively adjusts the learning rate based only on values
   of the loss function. The main idea is that Adacomp penalizes large
   learning rates to ensure the convergence and compensates small learning
   rates to accelerate the training process. Therefore, Adacomp is robust
   to the initial learning rate. Extensive experiments, including
   comparison to six typically adaptive methods (Momentum, Adagrad,
   RMSprop, Adadelta, Adam, and Adamax) on several benchmark datasets for
   image classification tasks (MNIST, KMNIST, Fashion-MNIST, CIFAR-10, and
   CIFAR-100), were conducted. Experimental results show that Adacomp is
   not only robust to the initial learning rate but also to the network
   architecture, network initialization, and batch size.},
DOI = {10.3390/app112110184},
Article-Number = {10184},
EISSN = {2076-3417},
ResearcherID-Numbers = {Ren, Xuebin/AAM-7582-2021
   Li, Yanan/ABL-1507-2022
   },
ORCID-Numbers = {Yang, Shusen/0000-0002-4495-6237
   , Li/0000-0002-6321-2567},
Unique-ID = {WOS:000720251600001},
}

@article{ WOS:000697551500015,
Author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine},
Title = {Deep tree-ensembles for multi-output prediction},
Journal = {PATTERN RECOGNITION},
Year = {2022},
Volume = {121},
Month = {JAN},
Abstract = {Recently, deep neural networks have expanded the state-of-art in various
   scientific fields and provided solutions to long standing problems
   across multiple application domains. Nevertheless, they also suf-fer
   from weaknesses since their optimal performance depends on massive
   amounts of training data and the tuning of an extended number of
   parameters. As a countermeasure, some deep-forest methods have been
   recently proposed, as efficient and low-scale solutions. Despite that,
   these approaches simply em-ploy label classification probabilities as
   induced features and primarily focus on traditional classification and
   regression tasks, leaving multi-output prediction under-explored.
   Moreover, recent work has demon-strated that tree-embeddings are highly
   representative, especially in structured output prediction. In this
   direction, we propose a novel deep tree-ensemble (DTE) model, where
   every layer enriches the origi-nal feature set with a representation
   learning component based on tree-embeddings. In this paper, we
   specifically focus on two structured output prediction tasks, namely
   multi-label classification and multi-target regression. We conducted
   experiments using multiple benchmark datasets and the obtained results
   confirm that our method provides superior results to state-of-the-art
   methods in both tasks. (c) 2021 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.patcog.2021.108211},
EarlyAccessDate = {AUG 2021},
Article-Number = {108211},
ISSN = {0031-3203},
EISSN = {1873-5142},
Unique-ID = {WOS:000697551500015},
}

@article{ WOS:001118122400001,
Author = {Kim, Hye-Jin and Gong, Eun-Jeong and Bang, Chang-Seok},
Title = {Application of Machine Learning Based on Structured Medical Data in
   Gastroenterology},
Journal = {BIOMIMETICS},
Year = {2023},
Volume = {8},
Number = {7},
Month = {NOV},
Abstract = {The era of big data has led to the necessity of artificial intelligence
   models to effectively handle the vast amount of clinical data available.
   These data have become indispensable resources for machine learning.
   Among the artificial intelligence models, deep learning has gained
   prominence and is widely used for analyzing unstructured data. Despite
   the recent advancement in deep learning, traditional machine learning
   models still hold significant potential for enhancing healthcare
   efficiency, especially for structured data. In the field of medicine,
   machine learning models have been applied to predict diagnoses and
   prognoses for various diseases. However, the adoption of machine
   learning models in gastroenterology has been relatively limited compared
   to traditional statistical models or deep learning approaches. This
   narrative review provides an overview of the current status of machine
   learning adoption in gastroenterology and discusses future directions.
   Additionally, it briefly summarizes recent advances in large language
   models.},
DOI = {10.3390/biomimetics8070512},
Article-Number = {512},
EISSN = {2313-7673},
ResearcherID-Numbers = {Bang, Chang SEOK/I-9689-2019
   },
ORCID-Numbers = {Gong, Eun Jeong/0000-0003-3996-3472
   Bang, Chang Seok/0000-0003-4908-5431},
Unique-ID = {WOS:001118122400001},
}

@article{ WOS:000429536600008,
Author = {von Lilienfeld, O. Anatole},
Title = {Quantum Machine Learning in Chemical Compound Space},
Journal = {ANGEWANDTE CHEMIE-INTERNATIONAL EDITION},
Year = {2018},
Volume = {57},
Number = {16, SI},
Pages = {4164-4169},
Month = {APR 9},
Abstract = {Rather than numerically solving the computationally demanding equations
   of quantum or statistical mechanics, machine learning methods can infer
   approximate solutions by interpolating previously acquired property data
   sets of molecules and materials. The case is made for quantum machine
   learning: An inductive molecular modeling approach which can be applied
   to quantum chemistry problems.},
DOI = {10.1002/anie.201709686},
ISSN = {1433-7851},
EISSN = {1521-3773},
ResearcherID-Numbers = {von Lilienfeld, O./D-8529-2011},
Unique-ID = {WOS:000429536600008},
}

@article{ WOS:000458997900018,
Author = {Sultana, Nasrin and Chilamkurti, Naveen and Peng, Wei and Alhadad, Rabei},
Title = {Survey on SDN based network intrusion detection system using machine
   learning approaches},
Journal = {PEER-TO-PEER NETWORKING AND APPLICATIONS},
Year = {2019},
Volume = {12},
Number = {2, SI},
Pages = {493-501},
Month = {MAR},
Abstract = {Software Defined Networking Technology (SDN) provides a prospect to
   effectively detect and monitor network security problems ascribing to
   the emergence of the programmable features. Recently, Machine Learning
   (ML) approaches have been implemented in the SDN-based Network Intrusion
   Detection Systems (NIDS) to protect computer networks and to overcome
   network security issues. A stream of advanced machine learning
   approaches - the deep learning technology (DL) commences to emerge in
   the SDN context. In this survey, we reviewed various recent works on
   machine learning (ML) methods that leverage SDN to implement NIDS. More
   specifically, we evaluated the techniques of deep learning in developing
   SDN-based NIDS. In the meantime, in this survey, we covered tools that
   can be used to develop NIDS models in SDN environment. This survey is
   concluded with a discussion of ongoing challenges in implementing NIDS
   using ML/DL and future works.},
DOI = {10.1007/s12083-017-0630-0},
ISSN = {1936-6442},
EISSN = {1936-6450},
ResearcherID-Numbers = {Chilamkurti, Naveen/S-9636-2019
   Peng, Wei/OEO-9234-2025},
ORCID-Numbers = {Peng, Wei/0000-0002-0868-0974
   },
Unique-ID = {WOS:000458997900018},
}

@article{ WOS:000602672200005,
Author = {Kushwaha, Shashi and Bahl, Shashi and Bagha, Ashok Kumar and Parmar,
   Kulwinder Singh and Javaid, Mohd and Haleem, Abid and Singh, Ravi Pratap},
Title = {Significant Applications of Machine Learning for COVID-19 Pandemic},
Journal = {JOURNAL OF INDUSTRIAL INTEGRATION AND MANAGEMENT-INNOVATION AND
   ENTREPRENEURSHIP},
Year = {2020},
Volume = {5},
Number = {4},
Pages = {453-479},
Month = {DEC},
Abstract = {Machine learning is an innovative approach that has extensive
   applications in prediction. This technique needs to be applied for the
   COVID-19 pandemic to identify patients at high risk, their death rate,
   and other abnormalities. It can be used to understand the nature of this
   virus and further predict the upcoming issues. This literature-based
   review is done by searching the relevant papers on machine learning for
   COVID-19 from the databases of SCOPUS, Academia, Google Scholar, PubMed,
   and ResearchGate. This research attempts to discuss the significance of
   machine learning in resolving the COVID-19 pandemic crisis. This paper
   studied how machine learning algorithms and methods can be employed to
   fight the COVID-19 virus and the pandemic. It further discusses the
   primary machine learning methods that are helpful during the COVID-19
   pandemic. We further identified and discussed algorithms used in machine
   learning and their significant applications. Machine learning is a
   useful technique, and this can be witnessed in various areas to identify
   the existing drugs, which also seems advantageous for the treatment of
   COVID-19 patients. This learning algorithm creates interferences out of
   unlabeled input datasets, which can be applied to analyze the unlabeled
   data as an input resource for COVID-19. It provides accurate and useful
   features rather than a traditional explicitly calculation-based method.
   Further, this technique is beneficial to predict the risk in healthcare
   during this COVID-19 crisis. Machine learning also analyses the risk
   factors as per age, social habits, location, and climate.},
DOI = {10.1142/S2424862220500268},
ISSN = {2424-8622},
EISSN = {2424-8630},
ResearcherID-Numbers = {HALEEM, ABID/G-4761-2012
   Haleem, Abid/G-4761-2012
   Javaid, Mohd/AAD-7090-2022
   Parmar, Kulwinder/B-1583-2014
   Kumar, Ashok/Y-1298-2019
   Bahl, Shashi/AAP-8412-2021
   SINGH, RAVI/P-9486-2019
   Parmar, Kulwinder Singh/B-1583-2014
   },
ORCID-Numbers = {Haleem, Abid/0000-0002-3487-0229
   , Mohd Javaid/0009-0006-1734-6392
   Bagha, Ashok Kumar/0000-0003-1455-9115
   Parmar, Kulwinder Singh/0000-0002-7589-7364
   Bahl, Shashi/0000-0001-9294-8226},
Unique-ID = {WOS:000602672200005},
}

@article{ WOS:000504871100004,
Author = {Schwartz, John T. and Gao, Michael and Geng, Eric A. and Mody, Kush S.
   and Mikhail, Christopher M. and Cho, Samuel K.},
Title = {Applications of Machine Learning Using Electronic Medical Records in
   Spine Surgery},
Journal = {NEUROSPINE},
Year = {2019},
Volume = {16},
Number = {4},
Pages = {643-653},
Month = {DEC},
Abstract = {Developments in machine learning in recent years have precipitated a
   surge in research on the applications of artificial intelligence within
   medicine. Machine learning algorithms are beginning to impact medicine
   broadly, and the field of spine surgery is no exception. Electronic
   medical records are a key source of medical data that can be leveraged
   for the creation of clinically valuable machine learning algorithms.
   This review examines the current state of machine learning using
   electronic medical records as it applies to spine surgery. Studies
   across the electronic medical record data domains of imaging, text, and
   structured data are reviewed. Discussed applications include clinical
   prognostication, preoperative planning, diagnostics, and dynamic
   clinical assistance, among others. The limitations and future challenges
   for machine learning research using electronic medical records are also
   discussed.},
DOI = {10.14245/ns.1938386.193},
ISSN = {2586-6583},
EISSN = {2586-6591},
ORCID-Numbers = {Schwartz, John/0000-0003-3682-5605
   Cho, Samuel/0000-0001-7511-2486},
Unique-ID = {WOS:000504871100004},
}

@article{ WOS:000778251600007,
Author = {Bublin, Mugdim},
Title = {Event Detection for Distributed Acoustic Sensing: Combining
   Knowledge-Based, Classical Machine Learning, and Deep Learning
   Approaches},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {22},
Month = {NOV},
Abstract = {Distributed Acoustic Sensing (DAS) is a promising new technology for
   pipeline monitoring and protection. However, a big challenge is
   distinguishing between relevant events, like intrusion by an excavator
   near the pipeline, and interference, like land machines. This paper
   investigates whether it is possible to achieve adequate detection
   accuracy with classic machine learning algorithms using simulations and
   real system implementation. Then, we compare classical machine learning
   with a deep learning approach and analyze the advantages and
   disadvantages of both approaches. Although acceptable performance can be
   achieved with both approaches, preliminary results show that deep
   learning is the more promising approach, eliminating the need for
   laborious feature extraction and offering a six times lower event
   detection delay and twelve times lower execution time. However, we
   achieved the best results by combining deep learning with the
   knowledge-based and classical machine learning approaches. At the end of
   this manuscript, we propose general guidelines for efficient system
   design combining knowledge-based, classical machine learning, and deep
   learning approaches.},
DOI = {10.3390/s21227527},
Article-Number = {7527},
EISSN = {1424-8220},
Unique-ID = {WOS:000778251600007},
}

@article{ WOS:000429088600004,
Author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin},
Title = {POINTS OF SIGNIFICANCE Statistics versus machine learning},
Journal = {NATURE METHODS},
Year = {2018},
Volume = {15},
Number = {4},
Pages = {232-233},
Month = {APR},
Abstract = {Statistics draws population inferences from a sample, and machine
   learning finds generalizable predictive patterns.},
DOI = {10.1038/nmeth.4642},
ISSN = {1548-7091},
EISSN = {1548-7105},
ResearcherID-Numbers = {Bzdok, Danilo/AEW-0113-2022},
Unique-ID = {WOS:000429088600004},
}

@article{ WOS:000453925000013,
Author = {Lotan, Eyal and Lotan, Eyal and Jain, Rajan and Razavian, Narges and
   Fatterpekar, Girish M. and Lui, Yvonne W.},
Title = {State of the Art: Machine Learning Applications in Glioma Imaging},
Journal = {AMERICAN JOURNAL OF ROENTGENOLOGY},
Year = {2019},
Volume = {212},
Number = {1},
Pages = {26-37},
Month = {JAN},
Abstract = {OBJECTIVE. Machine learning has recently gained considerable attention
   because of promising results for a wide range of radiology applications.
   Here we review recent work using machine learning in brain tumor
   imaging, specifically segmentation and MRI radiomics of gliomas.
   CONCLUSION. We discuss available resources, state`of`the`art
   segmentation methods, and machine learning radiomics for glioma. We
   highlight the challenges of these techniques as well as the future
   potential in clinical diagnostics, prognostics, and decision making.},
DOI = {10.2214/AJR.18.20218},
ISSN = {0361-803X},
EISSN = {1546-3141},
ResearcherID-Numbers = {Lotan, Eyal/AAV-9830-2021
   Jain, Rahul/AAY-7671-2021},
ORCID-Numbers = {Jain, Rajan/0000-0002-4879-0457
   Razavian, Narges/0000-0002-9922-6370
   },
Unique-ID = {WOS:000453925000013},
}

@article{ WOS:000619114800012,
Author = {Crowson, Matthew G. and Ranisau, Jonathan and Eskander, Antoine and
   Babier, Aaron and Xu, Bin and Kahmke, Russel R. and Chen, Joseph M. and
   Chan, Timothy C. Y.},
Title = {A Contemporary Review of Machine Learning in Otolaryngology-Head and
   Neck Surgery},
Journal = {LARYNGOSCOPE},
Year = {2020},
Volume = {130},
Number = {1},
Pages = {45-51},
Month = {JAN},
Abstract = {One of the key challenges with big data is leveraging the complex
   network of information to yield useful clinical insights. The confluence
   of massive amounts of health data and a desire to make inferences and
   insights on these data has produced a substantial amount of interest in
   machine-learning analytic methods. There has been a drastic increase in
   the otolaryngology literature volume describing novel applications of
   machine learning within the past 5 years. In this timely contemporary
   review, we provide an overview of popular machine-learning techniques,
   and review recent machine-learning applications in otolaryngology-head
   and neck surgery including neurotology, head and neck oncology,
   laryngology, and rhinology. Investigators have realized significant
   success in validated models with model sensitivities and specificities
   approaching 100\%. Challenges remain in the implementation of
   machine-learning algorithms. This may be in part the unfamiliarity of
   these techniques to clinician leaders on the front lines of patient
   care. Spreading awareness and confidence in machine learning will follow
   with further validation and proof-of-value analyses that demonstrate
   model performance superiority over established methods. We are poised to
   see a greater influx of machine-learning applications to clinical
   problems in otolaryngology-head and neck surgery, and it is prudent for
   providers to understand the potential benefits and limitations of these
   technologies.},
DOI = {10.1002/lary.27850},
ISSN = {0023-852X},
EISSN = {1531-4995},
ResearcherID-Numbers = {bin, bin/KFB-5507-2024
   Babier, Aaron/AAD-6004-2019
   Crowson, Matthew/L-7356-2013
   Crowson, Matthew/X-6751-2019
   },
ORCID-Numbers = {Xu, Bin/0000-0003-4638-9835
   Crowson, Matthew/0000-0001-9950-0985
   Babier, Aaron/0000-0002-5949-2500},
Unique-ID = {WOS:000619114800012},
}

@incollection{ WOS:000433057100004,
Author = {Dwyer, Dominic B. and Falkai, Peter and Koutsouleris, Nikolaos},
Editor = {Widiger, T and Cannon, TD},
Title = {Machine Learning Approaches for Clinical Psychology and Psychiatry},
Booktitle = {ANNUAL REVIEW OF CLINICAL PSYCHOLOGY, VOL 14},
Series = {Annual Review of Clinical Psychology},
Year = {2018},
Volume = {14},
Pages = {91-118},
Abstract = {Machine learning approaches for clinical psychology and psychiatry
   explicitly focus on learning statistical functions from multidimensional
   data sets to make generalizable predictions about individuals. The goal
   of this review is to provide an accessible understanding of why this
   approach is important for future practice given its potential to augment
   decisions associated with the diagnosis, prognosis, and treatment of
   people suffering from mental illness using clinical and biological data.
   To this end, the limitations of current statistical paradigms in mental
   health research are critiqued, and an introduction is provided to
   critical machine learning methods used in clinical studies. A selective
   literature review is then presented aiming to reinforce the usefulness
   of machine learning methods and provide evidence of their potential. In
   the context of promising initial results, the current limitations of
   machine learning approaches are addressed, and considerations for future
   clinical translation are outlined.},
DOI = {10.1146/annurev-clinpsy-032816045037},
ISSN = {1548-5943},
ResearcherID-Numbers = {Koutsouleris, Nikolaos/MHR-0254-2025
   Dwyer, Dominic/Z-1225-2018
   Falkai, Peter/E-3273-2017},
ORCID-Numbers = {Dwyer, Dominic/0000-0003-3949-5867
   Falkai, Peter/0000-0003-2873-8667},
Unique-ID = {WOS:000433057100004},
}

@article{ WOS:000560159100002,
Author = {Li, Yue and Zeng, Yijie and Qing, Yuanyuan and Huang, Guang-Bin},
Title = {Learning local discriminative representations via extreme learning
   machine for machine fault diagnosis},
Journal = {NEUROCOMPUTING},
Year = {2020},
Volume = {409},
Pages = {275-285},
Month = {OCT 7},
Abstract = {Recently, learning data representations have been investigated to reduce
   the dependences of human intervention and improve the performance of
   machine fault diagnosis. However, most of the representation learning
   methods are computationally intensive due to complex training
   procedures. Extreme learning machine is well-known for its fast training
   speed and strong generalization ability. It also has been applied to
   learn data representations for clustering and classification tasks. In
   this paper, a local discriminant preserving extreme learning machine
   autoencoder (LDELM-AE) is proposed to learn data representations with
   the local geometry and local discriminant exploited from the input data.
   Specifically, LDELM-AE utilizes two graphs to enhance the within-class
   compactness and between-class separability, respectively. Furthermore,
   the hierarchical representations can be obtained by stacking several
   LDELM-AEs. On several benchmark datasets, the proposed method
   demonstrates better classification accuracies than the state-of-the-art
   methods. Moreover, the proposed method has been used to diagnostic the
   rotary machine faults and achieves the diagnostic accuracy of 99.96\%,
   which proves the proposed method is an efficient tool to diagnose
   machine faults. (C) 2020 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.neucom.2020.05.021},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Huang, Guang-Bin/A-5035-2011},
Unique-ID = {WOS:000560159100002},
}

@article{ WOS:000354655800008,
Author = {Libbrecht, Maxwell W. and Noble, William Stafford},
Title = {Machine learning applications in genetics and genomics},
Journal = {NATURE REVIEWS GENETICS},
Year = {2015},
Volume = {16},
Number = {6},
Pages = {321-332},
Month = {JUN},
Abstract = {The field of machine learning, which aims to develop computer algorithms
   that improve with experience, holds promise to enable computers to
   assist humans in the analysis of large, complex data sets. Here, we
   provide an overview of machine learning applications for the analysis of
   genome sequencing data sets, including the annotation of sequence
   elements and epigenetic, proteomic or metabolomic data. We present
   considerations and recurrent challenges in the application of
   supervised, semi-supervised and unsupervised machine learning methods,
   as well as of generative and discriminative modelling approaches. We
   provide general guidelines to assist in the selection of these machine
   learning methods and their practical application for the analysis of
   genetic and genomic data sets.},
DOI = {10.1038/nrg3920},
ISSN = {1471-0056},
EISSN = {1471-0064},
ResearcherID-Numbers = {Noble, William/AEP-1001-2022},
ORCID-Numbers = {Libbrecht, Maxwell/0000-0003-2502-0262
   },
Unique-ID = {WOS:000354655800008},
}

@article{ WOS:001006490400001,
Author = {Li, Yucong and Zhang, Ruofei and Yan, Xiyun and Fan, Kelong},
Title = {Machine learning facilitating the rational design of nanozymes},
Journal = {JOURNAL OF MATERIALS CHEMISTRY B},
Year = {2023},
Volume = {11},
Number = {28},
Pages = {6466-6477},
Month = {JUL 19},
Abstract = {As a component substitute for natural enzymes, nanozymes have the
   advantages of easy synthesis, convenient modification, low cost, and
   high stability, and are widely used in many fields. However, their
   application is seriously restricted by the difficulty of rapidly
   creating high-performance nanozymes. The use of machine learning
   techniques to guide the rational design of nanozymes holds great promise
   to overcome this difficulty. In this review, we introduce the recent
   progress of machine learning in assisting the design of nanozymes.
   Particular attention is given to the successful strategies of machine
   learning in predicting the activity, selectivity, catalytic mechanisms,
   optimal structures and other features of nanozymes. The typical
   procedures and approaches for conducting machine learning in the study
   of nanozymes are also highlighted. Moreover, we discuss in detail the
   difficulties of machine learning methods in dealing with the redundant
   and chaotic nanozyme data and provide an outlook on the future
   application of machine learning in the nanozyme field. We hope that this
   review will serve as a useful handbook for researchers in related fields
   and promote the utilization of machine learning in nanozyme rational
   design and related topics.},
DOI = {10.1039/d3tb00842h},
EarlyAccessDate = {MAY 2023},
ISSN = {2050-750X},
EISSN = {2050-7518},
ResearcherID-Numbers = {Zhang, Ruofei/HKW-6653-2023
   Yan, Xiyun/HCH-9174-2022
   Fan, Kelong/E-4997-2013},
ORCID-Numbers = {Zhang, Ruofei/0000-0002-4186-4951
   Fan, Kelong/0000-0001-6285-1933},
Unique-ID = {WOS:001006490400001},
}

@article{ WOS:000269069200037,
Author = {Lykourentzou, Ioanna and Giannoukos, Ioannis and Nikolopoulos, Vassilis
   and Mpardis, George and Loumos, Vassili},
Title = {Dropout prediction in e-learning courses through the combination of
   machine learning techniques},
Journal = {COMPUTERS \& EDUCATION},
Year = {2009},
Volume = {53},
Number = {3},
Pages = {950-965},
Month = {NOV},
Abstract = {In this paper, a dropout prediction method for e-learning courses, based
   on three popular machine learning techniques and detailed student data,
   is proposed. The machine learning techniques used are feed-forward
   neural networks, support vector machines and probabilistic ensemble
   simplified fuzzy ARTMAP. Since a single technique may fail to accurately
   classify some e-learning students, whereas another may succeed, three
   decision schemes, which combine in different ways the results of the
   three machine learning techniques, were also tested. The method was
   examined in terms of overall accuracy, sensitivity and precision and its
   results were found to be significantly better than those reported in
   relevant literature. (C) 2009 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.compedu.2009.05.010},
ISSN = {0360-1315},
EISSN = {1873-782X},
Unique-ID = {WOS:000269069200037},
}

@article{ WOS:000736977400012,
Author = {Crampon, Kevin and Giorkallos, Alexis and Deldossi, Myrtille and Baud,
   Stephanie and Steffenel, Luiz Angelo},
Title = {Machine-learning methods for ligand-protein molecular docking},
Journal = {DRUG DISCOVERY TODAY},
Year = {2022},
Volume = {27},
Number = {1},
Pages = {151-164},
Month = {JAN},
Abstract = {Artificial intelligence (AI) is often presented as a new Industrial
   Revolution. Many domains use AI, including molecular simulation for drug
   discovery. In this review, we provide an overview of ligand-protein
   molecular docking and how machine learning (ML), especially deep
   learning (DL), a subset of ML, is transforming the field by tackling the
   associated challenges.},
DOI = {10.1016/j.drudis.2021.09.007},
ISSN = {1359-6446},
EISSN = {1878-5832},
ResearcherID-Numbers = {Steffenel, Luiz/AAE-4573-2020
   Baud, Stephanie/AAR-8256-2020},
ORCID-Numbers = {Crampon, Kevin/0000-0001-6124-0719
   Steffenel, Luiz Angelo/0000-0003-3670-4088
   Baud, Stephanie/0000-0002-4436-0652
   },
Unique-ID = {WOS:000736977400012},
}

@article{ WOS:000768730600017,
Author = {Hie, Brian L. and Yang, Kevin K.},
Title = {Adaptive machine learning for protein engineering},
Journal = {CURRENT OPINION IN STRUCTURAL BIOLOGY},
Year = {2022},
Volume = {72},
Pages = {145-152},
Month = {FEB},
Abstract = {Machine-learning models that learn from data to predict how protein
   sequence encodes function are emerging as a useful protein engineering
   tool. However, when using these models to suggest new protein designs,
   one must deal with the vast combinatorial complexity of protein
   sequences. Here, we review how to use a sequence-to-function
   machine-learning surrogate model to select sequences for experimental
   measurement. First, we discuss how to select sequences through a single
   round of machine-learning optimization. Then, we discuss sequential
   optimization, where the goal is to discover optimized sequences and
   improve the model across multiple rounds of training, optimization, and
   experimental measurement.},
DOI = {10.1016/j.sbi.2021.11.002},
ISSN = {0959-440X},
EISSN = {1879-033X},
ORCID-Numbers = {Hie, Brian/0000-0003-3224-8142
   Yang, Kevin/0000-0001-9045-6826},
Unique-ID = {WOS:000768730600017},
}

@article{ WOS:000512216200010,
Author = {Cui, Tong and Wang, Yan and Ji, Shufan and Li, Yan and Hao, Weiting and
   Zou, Haohan and Jhanji, Vishal},
Title = {Applying Machine Learning Techniques in Nomogram Prediction and Analysis
   for SMILE Treatment},
Journal = {AMERICAN JOURNAL OF OPHTHALMOLOGY},
Year = {2020},
Volume = {210},
Pages = {71-77},
Month = {FEB},
Abstract = {PURPOSE: To analyze the outcome of machine learning technique for
   prediction of small incision lenticule extraction (SMILE) nomogram.
   DESIGN: Prospective, comparative clinical study.
   METHODS: A comparative study was conducted on the outcomes of SMILE
   surgery between surgeon group (nomogram set by surgeon) and machine
   learning group (nomogram predicted by machine learning model). The
   machine learning model was trained by 865 ideal cases (spherical
   equivalent {[}SE] within +/- 0.5 diopter {[}D] 3 months postoperatively)
   from an experienced surgeon. The visual outcomes of both groups were
   compared for safety, efficacy, predictability, and SE correction.
   RESULTS: There was no statistically significant difference between the
   baseline data in both groups. The efficacy index in the machine learning
   group (1.48 +/- 1.08) was significantly higher than in the surgeon group
   (1.3 +/- 0.27) (t = -2.17, P < .05). Eighty-three percent of eyes in the
   surgeon group and 93\% of eyes in the machine learning group were within
   +/- 0.50 D, while 98\% of eyes in the surgeon group and 96\% of eyes in
   the machine learning group were within 1.00 D. The error of SE
   correction was -0.09 +/- 0.024 and -0.23 +/- 0.021 for machine learning
   and surgeon groups, respectively.
   CONCLUSIONS: The machine learning technique performed as well as surgeon
   in safety, but significantly better than surgeon in efficacy. As for
   predictability, the machine learning technique was comparable to
   surgeon, although less predictable for high myopia and astigmatism. ((C)
   2019 Published by Elsevier Inc.)},
DOI = {10.1016/j.ajo.2019.10.015},
ISSN = {0002-9394},
EISSN = {1879-1891},
ResearcherID-Numbers = {Jhanji, Vishal/I-5676-2014
   },
ORCID-Numbers = {Wang, Yan/0000-0002-1257-6635},
Unique-ID = {WOS:000512216200010},
}

@article{ WOS:000468859100001,
Author = {Kim, Jonghoon and Hong, Jisu and Park, Hyunjin},
Title = {Prospects of deep learning for medical imaging},
Journal = {PRECISION AND FUTURE MEDICINE},
Year = {2018},
Volume = {2},
Number = {2},
Pages = {37-52},
Month = {JUN},
Abstract = {Machine learning techniques are essential components of medical imaging
   research. Recently, a highly flexible machine learning approach known as
   deep learning has emerged as a disruptive technology to enhance the
   performance of existing machine learning techniques and to solve
   previously intractable problems. Medical imaging has been identified as
   one of the key research fields where deep learning can contribute
   significantly. This review article aims to survey deep learning
   literature in medical imaging and describe its potential for future
   medical imaging research. First, an overview of how traditional machine
   learning evolved to deep learning is provided. Second, a survey of the
   application of deep learning in medical imaging research is given.
   Third, well-known software tools for deep learning are reviewed.
   Finally, conclusions with limitations and future directions of deep
   learning in medical imaging are provided.},
DOI = {10.23838/pfm.2018.00030},
ISSN = {2508-7940},
EISSN = {2508-7959},
ResearcherID-Numbers = {Kim, Junpyo/HGD-2747-2022
   Park, Hyunjin/A-1164-2007},
ORCID-Numbers = {Park, Hyunjin/0000-0001-5681-8918
   },
Unique-ID = {WOS:000468859100001},
}

@article{ WOS:000603902400001,
Author = {Cheng, Lin and Tong, Fei and Li, Yanlong and Yang, Jie and Zheng,
   Dongjian},
Title = {Comparative Study of the Dynamic Back-Analysis Methods of Concrete
   Gravity Dams Based on Multivariate Machine Learning Models},
Journal = {JOURNAL OF EARTHQUAKE ENGINEERING},
Year = {2021},
Volume = {25},
Number = {1},
Pages = {1-22},
Month = {JAN 2},
Abstract = {Two different back-analysis frameworks based on multivariate machine
   learning models used to determine the material dynamic parameters of
   concrete gravity dams are proposed. For the framework I, the
   back-analysis is performed by solving an optimization problem and a
   multivariate machine learning model is trained to replace the FEM
   calculation during the optimization process. While the framework II uses
   a multivariate machine learning model directly and the material dynamic
   parameters are predicted using the machine learning mode. By using a
   numerical example and an experimental investigation, the robustness,
   accuracy, computation efficiency of these proposed back-analysis methods
   is verified.},
DOI = {10.1080/13632469.2018.1452802},
ISSN = {1363-2469},
EISSN = {1559-808X},
ResearcherID-Numbers = {CHENG, LIN/AGR-4452-2022
   Li, Yanlong/HHN-7338-2022},
ORCID-Numbers = {Cheng, Lin/0000-0001-8951-7628
   tong, fei/0000-0002-7526-6722
   },
Unique-ID = {WOS:000603902400001},
}

@article{ WOS:000544969800031,
Author = {Alenezi, Hadeel S. and Faisal, Maha H.},
Title = {Utilizing crowdsourcing and machine learning in education: Literature
   review},
Journal = {EDUCATION AND INFORMATION TECHNOLOGIES},
Year = {2020},
Volume = {25},
Number = {4},
Pages = {2971-2986},
Month = {JUL},
Abstract = {For many years, learning continues to be a vital developing field since
   it is the key measure of the world's civilization and evolution with its
   enormous effect on both individuals and societies. Enhancing existing
   learning activities in general will have a significant impact on
   literacy rates around the world. One of the crucial activities in
   education is the assessment method because it is the primary way used to
   evaluate the student during their studies. The main purpose of this
   review is to examine the existing learning and e-learning approaches
   that use either crowdsourcing, machine learning, or both crowdsourcing
   and machine learning in their proposed solutions. This review will also
   investigate the addressed applications to identify the existing
   researches related to the assessment. Identifying all existing
   applications will assist in finding the unexplored gaps and limitations.
   This study presents a systematic literature review investigating 30
   papers from the following databases: IEEE and ACM Digital Library. After
   performing the analysis, we found that crowdsourcing is utilized in
   47.8\% of the investigated learning activities, while each of the
   machine learning and the hybrid solutions are utilized in 26\% of the
   investigated learning activities. Furthermore, all the existing
   approaches regarding the exam assessment problem that are using machine
   learning or crowdsourcing were identified. Some of the existing
   assessment systems are using the crowdsourcing approach and other
   systems are using the machine learning, however, none of the approaches
   provide a hybrid assessment system that uses both crowdsourcing and
   machine learning. Finally, it is found that using either crowdsourcing
   or machine learning in the online courses will enhance the interactions
   between the students. It is concluded that the current learning
   activities need to be enhanced since it is directly affecting the
   student's performance. Moreover, merging both the machine learning to
   the crowd wisdom will increase the accuracy and the efficiency of
   education.},
DOI = {10.1007/s10639-020-10102-w},
ISSN = {1360-2357},
EISSN = {1573-7608},
ORCID-Numbers = {Alenezi, Hadeel/0000-0002-3774-368X
   Faisal, Maha/0000-0001-8074-5456},
Unique-ID = {WOS:000544969800031},
}

@article{ WOS:001132712600001,
Author = {Heidrich, Louisa and Slany, Emanuel and Scheele, Stephan and Schmid, Ute
   and Cabitza, Federico and Chen, Fang and Zhou, Jianlong and Holzinger,
   Andreas},
Title = {<sc>FairCaipi</sc>: A Combination of Explanatory Interactive and Fair
   Machine Learning for Human and Machine Bias Reduction},
Journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION},
Year = {2023},
Volume = {5},
Number = {4},
Pages = {1519-1538},
Month = {DEC},
Abstract = {The rise of machine-learning applications in domains with critical
   end-user impact has led to a growing concern about the fairness of
   learned models, with the goal of avoiding biases that negatively impact
   specific demographic groups. Most existing bias-mitigation strategies
   adapt the importance of data instances during pre-processing. Since
   fairness is a contextual concept, we advocate for an interactive
   machine-learning approach that enables users to provide iterative
   feedback for model adaptation. Specifically, we propose to adapt the
   explanatory interactive machine-learning approach Caipi for fair machine
   learning. FairCaipi incorporates human feedback in the loop on
   predictions and explanations to improve the fairness of the model.
   Experimental results demonstrate that FairCaipi outperforms a
   state-of-the-art pre-processing bias mitigation strategy in terms of the
   fairness and the predictive performance of the resulting
   machine-learning model. We show that FairCaipi can both uncover and
   reduce bias in machine-learning models and allows us to detect human
   bias.},
DOI = {10.3390/make5040076},
EISSN = {2504-4990},
ResearcherID-Numbers = {Zhou, Jianlong/AGV-6879-2022
   Holzinger, Andreas/E-9530-2010
   Cabitza, Federico/JCO-4001-2023},
ORCID-Numbers = {Scheele, Stephan/0000-0003-0787-3181
   Schmid, Ute/0000-0002-1301-0326
   Slany, Emanuel/0000-0001-5958-8425
   },
Unique-ID = {WOS:001132712600001},
}

@article{ WOS:001225632900001,
Author = {Dewangan, Sheetal Kumar and Jain, Reliance and Bhattacharjee,
   Soumyabrata and Jain, Sandeep and Paswan, Manikant and Samal, Sumanta
   and Ahn, Byungmin},
Title = {Enhancing flow stress predictions in CoCrFeNiV high entropy alloy with
   conventional and machine learning techniques},
Journal = {JOURNAL OF MATERIALS RESEARCH AND TECHNOLOGY-JMR\&T},
Year = {2024},
Volume = {30},
Pages = {2377-2387},
Month = {MAY-JUN},
Abstract = {A machine learning technique leveraging artificial intelligence (AI) has
   emerged as a promising tool for expediting the exploration and design of
   novel high entropy alloys (HEAs) while predicting their mechanical
   properties at both room and elevated temperatures. In this paper, we
   predict the flow stress of hot-compressed CoCrFeNiV HEAs using
   conventional (qualitative and quantitative models) and advanced machine
   learning approaches across various temperature and strain rate
   conditions. Conventional modeling methods, including the modified
   Johnson -Cook (JC), modified Zerilli - Armstrong (ZA), and Arrhenius
   -type constitutive equations, are employed. Simultaneously, machine
   learning models are utilized to forecast flow stress under different hot
   working conditions. The performance of both conventional and machine
   learning models is evaluated using metrics such as coefficient of
   determination (R 2 ), mean abosolute error (MAE), and root mean squared
   error (RMSE). The analysis reveals that the gradient boosting machine
   learning model shows superior prediction accuracy (with value R 2 =
   0.994, MAE = 7.77\%, and RMSE = 9.7\%) compared to conventional models
   and other machine learning approaches.},
DOI = {10.1016/j.jmrt.2024.03.164},
EarlyAccessDate = {APR 2024},
ISSN = {2238-7854},
EISSN = {2214-0697},
ResearcherID-Numbers = {Jain, Sandeep/IYQ-7492-2023
   Ahn, Byungmin/B-3845-2010
   Jain, Reliance/KHZ-8998-2024
   Bhattacharjee, Soumyabrata/A-7988-2017
   Bhattacharjee, Soumyabrata/JWP-6988-2024
   Dewangan, Sheetal/GXH-6841-2022},
ORCID-Numbers = {Ahn, Byungmin/0000-0002-0866-6398
   Jain, Sandeep/0000-0003-4113-4235
   JAIN, RELIANCE/0000-0002-5634-8044
   Bhattacharjee, Soumyabrata/0000-0001-6560-8533
   },
Unique-ID = {WOS:001225632900001},
}

@article{ WOS:001001420400001,
Author = {Qiao, Zenglin and Li, Lynn and Zhao, Xinchao and Liu, Lei and Zhang,
   Qian and Hechmi, Shili and Atri, Mohamed and Li, Xiaohua},
Title = {An enhanced Runge Kutta boosted machine learning framework for medical
   diagnosis},
Journal = {COMPUTERS IN BIOLOGY AND MEDICINE},
Year = {2023},
Volume = {160},
Month = {JUN},
Abstract = {With the development and maturity of machine learning methods, medical
   diagnosis aided with machine learning methods has become a popular
   method to assist doctors in diagnosing and treating patients. However,
   machine learning methods are greatly affected by their hyperparameters,
   for instance, the kernel parameter in kernel extreme learning machine
   (KELM) and the learning rate in residual neural networks (ResNet). If
   the hyperparameters are appropriately set, the performance of the
   classifier can be significantly improved. To boost the performance of
   the machine learning methods, this paper proposes to improve the Runge
   Kutta optimizer (RUN) to adaptively adjust the hyperparameters of the
   machine learning methods for medical diagnosis pur -poses. Although RUN
   has a solid mathematical theoretical foundation, there are still some
   performance defects when dealing with complex optimization problems. To
   remedy these defects, this paper proposes a new enhanced RUN method with
   a grey wolf mechanism and an orthogonal learning mechanism called GORUN.
   The superior performance of the GORUN was validated against other
   well-established optimizers on IEEE CEC 2017 bench-mark functions. Then,
   the proposed GORUN is employed to optimize the machine learning models,
   including the KELM and ResNet, to construct robust models for medical
   diagnosis. The performance of the proposed machine learning framework
   was validated on several medical data sets, and the experimental results
   have demonstrated its superiority.},
DOI = {10.1016/j.compbiomed.2023.106949},
EarlyAccessDate = {MAY 2023},
Article-Number = {106949},
ISSN = {0010-4825},
EISSN = {1879-0534},
ResearcherID-Numbers = {ATRI, Mohamed/C-4069-2014
   Liu, Lei/GSN-5704-2022
   Zhao, Xinchao/LCD-4322-2024
   liu, Lei/GSN-5704-2022
   Atri, Mohamed/C-4069-2014},
ORCID-Numbers = {Qiao, zenglin/0000-0002-3793-7150
   liu, Lei/0000-0002-4698-2965
   Atri, Mohamed/0000-0001-8528-5647},
Unique-ID = {WOS:001001420400001},
}

@article{ WOS:001061249700006,
Author = {Wei, Han and Bao, Hua and Ruan, Xiulin},
Title = {Perspective: Predicting and optimizing thermal transport properties with
   machine learning methods},
Journal = {ENERGY AND AI},
Year = {2022},
Volume = {8},
Month = {MAY},
Abstract = {In recent years, (big) data science has emerged as the ``fourth
   paradigm{''} in physical science research. Data-driven techniques, e.g.
   machine learning, are advantageous in dealing with problems of
   high-dimensional features and complex mappings between quantities, which
   are otherwise of great difficulty or huge cost with other scientific
   paradigms. In the past five years or so, there has been a rapid growth
   of machine learning-assisted research on thermal transport. In this
   perspective, we review the recent progress in the intersection between
   machine learning and thermal transport, where machine learning methods
   generally serve as surrogate models for pre-dicting the thermal
   transport properties, or as tools for designing structures for the
   desired thermal properties and exploring thermal transport mechanisms.
   We provide perspectives about the advantages of machine learning methods
   in comparison to the physics-based methods for studying thermal
   transport properties. We also discuss how to improve the accuracy of
   predictive analytics and efficiency of structural optimization, to
   provide guid-ance for better utilizing machine learning-based methods to
   advance thermal transport research. Finally, we identify several
   outstanding challenges in this active area as well as opportunities for
   future developments, including developing machine learning methods
   suitable for small datasets, discovering effective physics-based
   descriptors, generating dataset from experiments and validating machine
   learning results with experiments, and making breakthroughs via
   discovering new physics.},
DOI = {10.1016/j.egyai.2022.100153},
Article-Number = {100153},
ISSN = {2666-5468},
ResearcherID-Numbers = {Ruan, Xiulin/C-6166-2009
   Bao, Hua/C-8283-2011},
Unique-ID = {WOS:001061249700006},
}

@article{ WOS:000653304100001,
Author = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu,
   Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and
   Rahman, Sk Mostafizur},
Title = {Early Weed Detection Using Image Processing and Machine Learning
   Techniques in an Australian Chilli Farm},
Journal = {AGRICULTURE-BASEL},
Year = {2021},
Volume = {11},
Number = {5},
Month = {MAY},
Abstract = {This paper explores the potential of machine learning algorithms for
   weed and crop classification from UAV images. The identification of
   weeds in crops is a challenging task that has been addressed through
   orthomosaicing of images, feature extraction and labelling of images to
   train machine learning algorithms. In this paper, the performances of
   several machine learning algorithms, random forest (RF), support vector
   machine (SVM) and k-nearest neighbours (KNN), are analysed to detect
   weeds using UAV images collected from a chilli crop field located in
   Australia. The evaluation metrics used in the comparison of performance
   were accuracy, precision, recall, false positive rate and kappa
   coefficient. MATLAB is used for simulating the machine learning
   algorithms; and the achieved weed detection accuracies are 96\% using
   RF, 94\% using SVM and 63\% using KNN. Based on this study, RF and SVM
   algorithms are efficient and practical to use, and can be implemented
   easily for detecting weed from UAV images.},
DOI = {10.3390/agriculture11050387},
Article-Number = {387},
EISSN = {2077-0472},
ResearcherID-Numbers = {Wibowo, Santoso/E-7497-2012
   Wibowo, Santoso/AAA-8870-2022
   Xu, Cheng-Yuan/C-2096-2008
   Moore, Steven/W-2204-2019
   islam, nahina/AAW-4745-2021
   Morshed, Ahsan/AAK-2685-2021
   },
ORCID-Numbers = {Wibowo, Santoso/0000-0002-5318-8428
   Rashid, Md Mamunur/0000-0002-3929-7361
   RAHMAN, Shan/0009-0003-7845-3966
   Islam, Nahina/0000-0002-5469-8104},
Unique-ID = {WOS:000653304100001},
}

@article{ WOS:000489302600009,
Author = {Zhang, Yan and Trubey, Peter},
Title = {Machine Learning and Sampling Scheme: An Empirical Study of Money
   Laundering Detection},
Journal = {COMPUTATIONAL ECONOMICS},
Year = {2019},
Volume = {54},
Number = {3},
Pages = {1043-1063},
Month = {OCT},
Abstract = {This paper studies the interplay of machine learning and sampling scheme
   in an empirical analysis of money laundering detection algorithms. Using
   actual transaction data provided by a U.S. financial institution, we
   study five major machine learning algorithms including Bayes logistic
   regression, decision tree, random forest, support vector machine, and
   artificial neural network. As the incidence of money laundering events
   is rare, we apply and compare two sampling techniques that increase the
   relative presence of the events. Our analysis reveals potential
   advantages of machine learning algorithms in modeling money laundering
   events. This paper provides insights into the use of machine learning
   and sampling schemes in money laundering detection specifically, and
   classification of rare events in general.},
DOI = {10.1007/s10614-018-9864-z},
ISSN = {0927-7099},
EISSN = {1572-9974},
ORCID-Numbers = {Zhang, Yan/0000-0002-9377-187X},
Unique-ID = {WOS:000489302600009},
}

@article{ WOS:001062545900001,
Author = {Zhu, Haibin and Bai, Lu and He, Lidan and Liu, Zhi},
Title = {Forecasting realized volatility with machine learning: Panel data
   perspective},
Journal = {JOURNAL OF EMPIRICAL FINANCE},
Year = {2023},
Volume = {73},
Pages = {251-271},
Month = {SEP},
Abstract = {Machine learning approaches have become very popular in many fields in
   this big data age. This paper considers the problem of forecasting
   realized volatility with machine learning using high-frequency data.
   Instead of treating the realized volatility as a univariate time series
   studied by many existing works in literature, we employ panel data
   analysis to improve forecasting accuracy in the short term. We use six
   effective machine-learning methods for the realized volatility panel
   data. We compare our results with the traditional linear-type models
   under the same panel data framework and with the single time series
   forecasting via the same machine learning methods. The results show that
   the panel-data-based machine learning method (PDML) outperforms the
   other methods.},
DOI = {10.1016/j.jempfin.2023.07.003},
EarlyAccessDate = {AUG 2023},
ISSN = {0927-5398},
EISSN = {1879-1727},
ResearcherID-Numbers = {LIU, Zhi/LQK-8117-2024
   bai, lu/KBD-1444-2024
   },
ORCID-Numbers = {Bai, Lu/0000-0001-9010-381X
   Zhu, Haibin/0000-0001-8169-9703},
Unique-ID = {WOS:001062545900001},
}

@article{ WOS:001035352200001,
Author = {Gong, Youdi and Liu, Guangzhen and Xue, Yunzhi and Li, Rui and Meng,
   Lingzhong},
Title = {A survey on dataset quality in machine learning},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2023},
Volume = {162},
Month = {OCT},
Abstract = {With the rise of big data, the quality of datasets has become a crucial
   factor affecting the performance of machine learning models.
   High-quality datasets are essential for the realization of data value.
   This survey article summarizes the research direction of dataset quality
   in machine learning, including the definition of related concepts,
   analysis of quality issues and risks, and a review of dataset quality
   dimensions and metrics throughout the dataset lifecycle and a review of
   dataset quality metrics analyzed from a dataset lifecycle perspective
   and summarized in literatures. Furthermore, this article introduces a
   comprehensive quality evaluation process, which includes a framework for
   dataset quality evaluation with dimensions and metrics, computation
   methods for quality metrics, and assessment models. These studies
   provide valuable guidance for evaluating dataset quality in the field of
   machine learning, which can help improve the accuracy, efficiency, and
   generalization ability of machine learning models, and promote the
   development and application of artificial intelligence technology.},
DOI = {10.1016/j.infsof.2023.107268},
EarlyAccessDate = {JUN 2023},
Article-Number = {107268},
ISSN = {0950-5849},
EISSN = {1873-6025},
ResearcherID-Numbers = {Liu, Guijian/M-9597-2014},
Unique-ID = {WOS:001035352200001},
}

@article{ WOS:000456150600012,
Author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
Title = {Multimodal Machine Learning: A Survey and Taxonomy},
Journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
Year = {2019},
Volume = {41},
Number = {2},
Pages = {423-443},
Month = {FEB},
Abstract = {Our experience of the world is multimodal - we see objects, hear sounds,
   feel texture, smell odors, and taste flavors. Modality refers to the way
   in which something happens or is experienced and a research problem is
   characterized as multimodal when it includes multiple such modalities.
   In order for Artificial Intelligence to make progress in understanding
   the world around us, it needs to be able to interpret such multimodal
   signals together. Multimodal machine learning aims to build models that
   can process and relate information from multiple modalities. It is a
   vibrant multi-disciplinary field of increasing importance and with
   extraordinary potential. Instead of focusing on specific multimodal
   applications, this paper surveys the recent advances in multimodal
   machine learning itself and presents them in a common taxonomy. We go
   beyond the typical early and late fusion categorization and identify
   broader challenges that are faced by multimodal machine learning,
   namely: representation, translation, alignment, fusion, and co-learning.
   This new taxonomy will enable researchers to better understand the state
   of the field and identify directions for future research.},
DOI = {10.1109/TPAMI.2018.2798607},
ISSN = {0162-8828},
EISSN = {1939-3539},
ResearcherID-Numbers = {Morency, Louis-Philippe/B-2006-2008},
ORCID-Numbers = {Baltrusaitis, Tadas/0000-0001-7923-8780
   },
Unique-ID = {WOS:000456150600012},
}

@article{ WOS:000823410900001,
Author = {Kavitha, S. S. and Kaulgud, Narasimha},
Title = {Quantum machine learning for support vector machine classification},
Journal = {EVOLUTIONARY INTELLIGENCE},
Year = {2024},
Volume = {17},
Number = {2},
Pages = {819-828},
Month = {APR},
Abstract = {Quantum machine learning aims to execute machine learning algorithms in
   quantum computers by utilizing powerful laws like superposition and
   entanglement for solving problems more efficiently. Support vector
   machine (SVM) is proved to be one of the most efficient classification
   machine learning algorithms in today's world. Since in classical
   systems, as datasets become complex or mixed up, the SVM kernel approach
   tends to slow and might fail. Hence our research is focused to examine
   the execution speed and accuracy of quantum support vector machines
   classification compared to classical SVM classification by proper
   quantum feature mapping selection. As the size of the dataset becomes
   complex, a proper feature map has to be selected to outperform or
   equally perform the classification. Hence the paper focuses on the
   selection of the best feature map for some benchmark datasets.
   Additionally experimental results show that the processing time of the
   algorithm is considerably reduced concerning classical machine learning.
   For evaluation of quantum computation over the classical computer,
   Quantum labs from the IBMQ quantum computer cloud have been used.},
DOI = {10.1007/s12065-022-00756-5},
EarlyAccessDate = {JUL 2022},
ISSN = {1864-5909},
EISSN = {1864-5917},
ResearcherID-Numbers = {S, Kavitha/AAU-7945-2021
   },
ORCID-Numbers = {, Kavitha S S/0000-0002-0843-5143},
Unique-ID = {WOS:000823410900001},
}

@article{ WOS:000959408400001,
Author = {Mahmood, Asif and Sandali, Yahya and Wang, Jin-Liang},
Title = {Easy and fast prediction of green solvents for small molecule
   donor-based organic solar cells through machine learning},
Journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS},
Year = {2023},
Volume = {25},
Number = {15},
Pages = {10417-10426},
Month = {APR 12},
Abstract = {Solubility plays a critical role in many aspects of research (drugs to
   materials). Solubility parameters are very useful for selecting
   appropriate solvents/non-solvents for various applications. In the
   present study, Hansen solubility parameters are predicted using machine
   learning. More than 40 machine models are tried in the search for the
   best model. Molecular descriptors and fingerprints are used as inputs to
   get a comparative view. Machine learning models trained using molecular
   descriptors have shown higher prediction ability than the model trained
   using molecular fingerprints. Machine learning models trained using
   molecular descriptors have shown their potential to be easy and fast
   models compared to the density functional theory (DFT)/thermodynamic
   approach. Machine learning creates a ``black box{''} connection to the
   properties. Therefore, minimal computational cost is required. With the
   help of the best-trained machine learning model, green solvents are
   selected for small molecule donors that are used in organic solar cells.
   Our introduced framework can help to select solvents for organic solar
   cells in an easy and fast way.},
DOI = {10.1039/d3cp00177f},
EarlyAccessDate = {MAR 2023},
ISSN = {1463-9076},
EISSN = {1463-9084},
ResearcherID-Numbers = {Mahmood, Asif/S-5579-2019
   wang, jinliang/GRJ-3717-2022
   },
ORCID-Numbers = {Sandali, Yahya/0009-0009-2290-221X
   Mahmood, Asif/0000-0001-9412-1011},
Unique-ID = {WOS:000959408400001},
}

@article{ WOS:000219166600025,
Author = {Wimmer, Hayden and Powell, Loreen},
Title = {A Comparison of the Effects of K-Anonymity on Machine Learning
   Algorithms},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2014},
Volume = {5},
Number = {11},
Pages = {155-160},
Month = {NOV},
Abstract = {While research has been conducted in machine learning algorithms and in
   privacy preserving in data mining (PPDM), a gap in the literature exists
   which combines the aforementioned areas to determine how PPDM affects
   common machine learning algorithms. The aim of this research is to
   narrow this literature gap by investigating how a common PPDM algorithm,
   K-Anonymity, affects common machine learning and data mining algorithms,
   namely neural networks, logistic regression, decision trees, and
   Bayesian classifiers. This applied research reveals practical
   implications for applying PPDM to data mining and machine learning and
   serves as a critical first step learning how to apply PPDM to machine
   learning algorithms and the effects of PPDM on machine learning. Results
   indicate that certain machine learning algorithms are more suited for
   use with PPDM techniques.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Unique-ID = {WOS:000219166600025},
}

@article{ WOS:000657036100005,
Author = {Gu, Qiang and Kumar, Anup and Bray, Simon and Creason, Allison and
   Khanteymoori, Alireza and Jalili, Vahid and Gruening, Bjoern and Goecks,
   Jeremy},
Title = {Galaxy-ML: An accessible, reproducible, and scalable machine learning
   toolkit for biomedicine},
Journal = {PLOS COMPUTATIONAL BIOLOGY},
Year = {2021},
Volume = {17},
Number = {6},
Month = {JUN},
Abstract = {Supervised machine learning is an essential but difficult to use
   approach in biomedical data analysis. The Galaxy-ML toolkit
   (https://galaxyproject.org/community/machine-learning/) makes supervised
   machine learning more accessible to biomedical scientists by enabling
   them to perform end-to-end reproducible machine learning analyses at
   large scale using only a web browser. Galaxy-ML extends Galaxy
   (https://galaxyproject.org), a biomedical computational workbench used
   by tens of thousands of scientists across the world, with a suite of
   tools for all aspects of supervised machine learning.},
DOI = {10.1371/journal.pcbi.1009014},
Article-Number = {e1009014},
ISSN = {1553-734X},
EISSN = {1553-7358},
ResearcherID-Numbers = {KhanTeymoori, Alireza/AAG-4239-2020
   Jalili, Vahid/KFS-7095-2024
   Grüning, Björn/JOZ-0834-2023
   },
ORCID-Numbers = {Khanteymoori, Alireza/0000-0001-6811-9196
   Goecks, Jeremy/0000-0002-4583-5226
   Bray, Simon/0000-0002-0621-6705
   Gruning, Bjorn/0000-0002-3079-6586
   Creason, Allison/0000-0001-5724-1276
   Kumar, Anup/0000-0002-2068-4695},
Unique-ID = {WOS:000657036100005},
}

@article{ WOS:000449215200023,
Author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and
   Schmajuk, Gabriela},
Title = {Potential Biases in Machine Learning Algorithms Using Electronic Health
   Record Data},
Journal = {JAMA INTERNAL MEDICINE},
Year = {2018},
Volume = {178},
Number = {11},
Pages = {1544-1547},
Month = {NOV},
Abstract = {A promise of machine learning in health care is the avoidance of biases
   in diagnosis and treatment; a computer algorithm could objectively
   synthesize and interpret the data in the medical record. Integration of
   machine learning with clinical decision support tools, such as
   computerized alerts or diagnostic support, may offer physicians and
   others who provide health care targeted and timely information that can
   improve clinical decisions. Machine learning algorithms, however, may
   also be subject to biases. The biases include those related to missing
   data and patients not identified by algorithms, sample size and
   underestimation, and misclassification and measurement error. There is
   concern that biases and deficiencies in the data used by machine
   learning algorithms may contribute to socioeconomic disparities in
   health care. This Special Communication outlines the potential biases
   that may be introduced into machine learning-based clinical decision
   support tools that use electronic health record data and proposes
   potential solutions to the problems of overreliance on automation,
   algorithms based on biased data, and algorithms that do not provide
   information that is clinically meaningful. Existing health care
   disparities should not be amplified by thoughtless or excessive reliance
   on machines.},
DOI = {10.1001/jamainternmed.2018.3763},
ISSN = {2168-6106},
EISSN = {2168-6114},
ORCID-Numbers = {Tamang, Suzanne/0000-0003-2077-4620},
Unique-ID = {WOS:000449215200023},
}

@article{ WOS:001129216300001,
Author = {Khan, Md Kamrul Hasan and Guo, Wenjing and Liu, Jie and Dong, Fan and
   Li, Zoe and Patterson, Tucker A. and Hong, Huixiao},
Title = {Machine learning and deep learning for brain tumor MRI image
   segmentation},
Journal = {EXPERIMENTAL BIOLOGY AND MEDICINE},
Year = {2023},
Volume = {248},
Number = {21},
Pages = {1974-1992},
Month = {NOV},
Abstract = {Brain tumors are often fatal. Therefore, accurate brain tumor image
   segmentation is critical for the diagnosis, treatment, and monitoring of
   patients with these tumors. Magnetic resonance imaging (MRI) is a
   commonly used imaging technique for capturing brain images. Both machine
   learning and deep learning techniques are popular in analyzing MRI
   images. This article reviews some commonly used machine learning and
   deep learning techniques for brain tumor MRI image segmentation. The
   limitations and advantages of the reviewed machine learning and deep
   learning methods are discussed. Even though each of these methods has a
   well-established status in their individual domains, the combination of
   two or more techniques is currently an emerging trend.},
DOI = {10.1177/15353702231214259},
EarlyAccessDate = {DEC 2023},
ISSN = {1535-3702},
EISSN = {1535-3699},
ResearcherID-Numbers = {guo, wenjing/AAP-6943-2021},
ORCID-Numbers = {Khan, Md Kamrul Hasan/0009-0004-9835-5594
   },
Unique-ID = {WOS:001129216300001},
}

@article{ WOS:000788104800007,
Author = {Ahmad, Faraz S. and Luo, Yuan and Wehbe, Ramsey M. and Thomas, James D.
   and Shah, Sanjiv J.},
Title = {Advances in Machine Learning Approaches to Heart Failure with Preserved
   Ejection Fraction},
Journal = {HEART FAILURE CLINICS},
Year = {2022},
Volume = {18},
Number = {2, SI},
Pages = {287-300},
Month = {APR},
Abstract = {center dot Machine learning has the potential to guide precision
   medicine approaches for heart failure with preserved ejection fraction,
   such as identification of rare causes, subphenotyping, and increasing
   the efficiency of clinical trial enrollment. center dot Understanding
   the strengths, limitations, and pitfalls of machine learning approaches
   is critical to realizing the potential of machine learning to impact the
   health of the patient with heart failure with preserved ejection
   fraction.},
DOI = {10.1016/j.hfc.2021.12.002},
EarlyAccessDate = {MAR 2022},
ISSN = {1551-7136},
ResearcherID-Numbers = {luo, yuan/JLS-6416-2023
   Shah, Sanjiv/AFQ-9480-2022
   },
ORCID-Numbers = {Wehbe, Ramsey/0000-0003-0599-7957},
Unique-ID = {WOS:000788104800007},
}

@article{ WOS:000306390300001,
Author = {Wang, Shijun and Summers, Ronald M.},
Title = {Machine learning and radiology},
Journal = {MEDICAL IMAGE ANALYSIS},
Year = {2012},
Volume = {16},
Number = {5},
Pages = {933-951},
Month = {JUL},
Abstract = {In this paper, we give a short introduction to machine learning and
   survey its applications in radiology. We focused on six categories of
   applications in radiology: medical image segmentation, registration,
   computer aided detection and diagnosis, brain function or activity
   analysis and neurological disease diagnosis from fMR images,
   content-based image retrieval systems for CT or MRI images, and text
   analysis of radiology reports using natural language processing (NLP)
   and natural language understanding (NLU). This survey shows that machine
   learning plays a key role in many radiology applications. Machine
   learning identifies complex patterns automatically and helps
   radiologists make intelligent decisions on radiology data such as
   conventional radiographs, CT, MRI, and PET images and radiology reports.
   In many applications, the performance of machine learning-based
   automatic detection and diagnosis systems has shown to be comparable to
   that of a well-trained and experienced radiologist. Technology
   development in machine learning and radiology will benefit from each
   other in the long run. Key contributions and common characteristics of
   machine learning techniques in radiology are discussed. We also discuss
   the problem of translating machine learning applications to the
   radiology clinical setting, including advantages and potential barriers.
   (c) 2012 Published by Elsevier B.V.},
DOI = {10.1016/j.media.2012.02.005},
ISSN = {1361-8415},
EISSN = {1361-8423},
ResearcherID-Numbers = {Summers, Ronald/AAX-6290-2021},
Unique-ID = {WOS:000306390300001},
}

@article{ WOS:000559371800001,
Author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Frenay,
   Benoit},
Title = {Legal requirements on explainability in machine learning},
Journal = {ARTIFICIAL INTELLIGENCE AND LAW},
Year = {2021},
Volume = {29},
Number = {2},
Pages = {149-169},
Month = {JUN},
Abstract = {Deep learning and other black-box models are becoming more and more
   popular today. Despite their high performance, they may not be accepted
   ethically or legally because of their lack of explainability. This paper
   presents the increasing number of legal requirements on machine learning
   model interpretability and explainability in the context of private and
   public decision making. It then explains how those legal requirements
   can be implemented into machine-learning models and concludes with a
   call for more inter-disciplinary research on explainability.},
DOI = {10.1007/s10506-020-09270-4},
EarlyAccessDate = {JUL 2020},
ISSN = {0924-8463},
EISSN = {1572-8382},
ORCID-Numbers = {Lognoul, Michael/0009-0005-5137-8278
   Bibal, Adrien/0000-0002-8650-8635
   Frenay, Benoit/0000-0002-7859-2750},
Unique-ID = {WOS:000559371800001},
}

@article{ WOS:000587801200006,
Author = {Ma, Runzhuo and Vanstrum, Erik B. and Lee, Ryan and Chen, Jian and Hung,
   Andrew J.},
Title = {Machine learning in the optimization of robotics in the operative field},
Journal = {CURRENT OPINION IN UROLOGY},
Year = {2020},
Volume = {30},
Number = {6},
Pages = {808-816},
Month = {NOV},
Abstract = {Purpose of review The increasing use of robotics in urologic surgery
   facilitates collection of `big data'. Machine learning enables computers
   to infer patterns from large datasets. This review aims to highlight
   recent findings and applications of machine learning in robotic-assisted
   urologic surgery. Recent findings Machine learning has been used in
   surgical performance assessment and skill training, surgical candidate
   selection, and autonomous surgery. Autonomous segmentation and
   classification of surgical data have been explored, which serves as the
   stepping-stone for providing real-time surgical assessment and
   ultimately, improve surgical safety and quality. Predictive machine
   learning models have been created to guide appropriate surgical
   candidate selection, whereas intraoperative machine learning algorithms
   have been designed to provide 3-D augmented reality and real-time
   surgical margin checks. Reinforcement-learning strategies have been
   utilized in autonomous robotic surgery, and the combination of expert
   demonstrations and trial-and-error learning by the robot itself is a
   promising approach towards autonomy. Robot-assisted urologic surgery
   coupled with machine learning is a burgeoning area of study that
   demonstrates exciting potential. However, further validation and
   clinical trials are required to ensure the safety and efficacy of
   incorporating machine learning into surgical practice.},
DOI = {10.1097/MOU.0000000000000816},
ISSN = {0963-0643},
EISSN = {1473-6586},
ResearcherID-Numbers = {Ma, Runzhuo/GXM-4966-2022
   Lee, Ryan/KYP-9672-2024
   },
ORCID-Numbers = {Ma, Runzhuo/0000-0001-6381-2661},
Unique-ID = {WOS:000587801200006},
}

@article{ WOS:000571725400014,
Author = {Lu, Hanlin and Li, Ming-Ju and He, Ting and Wang, Shiqiang and
   Narayanan, Vijaykrishnan and Chan, Kevin S.},
Title = {Robust Coreset Construction for Distributed Machine Learning},
Journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS},
Year = {2020},
Volume = {38},
Number = {10},
Pages = {2400-2417},
Month = {OCT},
Abstract = {Coreset, which is a summary of the original dataset in the form of a
   small weighted set in the same sample space, provides a promising
   approach to enable machine learning over distributed data. Although
   viewed as a proxy of the original dataset, each coreset is only designed
   to approximate the cost function of a specific machine learning problem,
   and thus different coresets are often required to solve different
   machine learning problems, increasing the communication overhead. We
   resolve this dilemma by developing robust coreset construction
   algorithms that can support a variety of machine learning problems.
   Motivated by empirical evidence that suitably-weighted k-clustering
   centers provide a robust coreset, we harden the observation by
   establishing theoretical conditions under which the coreset provides a
   guaranteed approximation for a broad range of machine learning problems,
   and developing both centralized and distributed algorithms to generate
   coresets satisfying the conditions. The robustness of the proposed
   algorithms is verified through extensive experiments on diverse datasets
   with respect to both supervised and unsupervised learning problems.},
DOI = {10.1109/JSAC.2020.3000373},
ISSN = {0733-8716},
EISSN = {1558-0008},
ResearcherID-Numbers = {Chan, Kevin/JQI-6002-2023
   Wang, Shiqiang/AAR-4091-2020
   Lu, Hanlin/HLH-4504-2023},
Unique-ID = {WOS:000571725400014},
}

@article{ WOS:001319107600001,
Author = {Dehghan, Pegah and Alashwal, Hany and Moustafa, Ahmed A.},
Title = {Applications of machine learning to behavioral sciences: focus on
   categorical data},
Journal = {DISCOVER PSYCHOLOGY},
Year = {2022},
Volume = {2},
Number = {1},
Month = {MAR 15},
Abstract = {In the last two decades, advancements in artificial intelligence and
   data science have attracted researchers' attention to machine learning.
   Growing interests in applying machine learning algorithms can be
   observed in different scientific areas, including behavioral sciences.
   However, most of the research conducted in this area applied machine
   learning algorithms to imagining and physiological data such as EEG and
   fMRI and there are relatively limited non-imaging and non-physiological
   behavioral studies which have used machine learning to analyze their
   data. Therefore, in this perspective article, we aim to (1) provide a
   general understanding of models built for inference, models built for
   prediction (i.e., machine learning), methods used in these models, and
   their strengths and limitations; (2) investigate the applications of
   machine learning to categorical data in behavioral sciences; and (3)
   highlight the usefulness of applying machine learning algorithms to
   non-imaging and non-physiological data (e.g., clinical and categorical)
   data and provide evidence to encourage researchers to conduct further
   machine learning studies in behavioral and clinical sciences.},
DOI = {10.1007/s44202-022-00027-5},
Article-Number = {22},
EISSN = {2731-4537},
ResearcherID-Numbers = {Moustafa, Ahmed/AAM-2836-2021
   },
ORCID-Numbers = {Alashwal, Hany/0000-0002-5721-5104},
Unique-ID = {WOS:001319107600001},
}

@article{ WOS:000505697300001,
Author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet,
   Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie
   and Zdeborova, Lenka},
Title = {Machine learning and the physical sciences},
Journal = {REVIEWS OF MODERN PHYSICS},
Year = {2019},
Volume = {91},
Number = {4},
Month = {DEC 6},
Abstract = {Machine learning (ML) encompasses a broad range of algorithms and
   modeling tools used for a vast array of data processing tasks, which has
   entered most scientific disciplines in recent years. This article
   reviews in a selective way the recent research on the interface between
   machine learning and the physical sciences. This includes conceptual
   developments in ML motivated by physical insights, applications of
   machine learning techniques to several domains in physics, and cross
   fertilization between the two fields. After giving a basic notion of
   machine learning methods and principles, examples are described of how
   statistical physics is used to understand methods in ML. This review
   then describes applications of ML methods in particle physics and
   cosmology, quantum many-body physics, quantum computing, and chemical
   and material physics. Research and development into novel computing
   architectures aimed at accelerating ML are also highlighted. Each of the
   sections describe recent successes as well as domain-specific
   methodology and challenges.},
DOI = {10.1103/RevModPhys.91.045002},
Article-Number = {045002},
ISSN = {0034-6861},
EISSN = {1539-0756},
ResearcherID-Numbers = {Carleo, Giuseppe/M-6198-2013
   Zdeborova, Lenka/B-9999-2014
   Cirac, J. Ignacio/A-9105-2017
   Carleo, Giuseppe/JOZ-3596-2023
   Cirac, J./A-9105-2017},
ORCID-Numbers = {Carleo, Giuseppe/0000-0002-8887-4356
   Cirac, J. Ignacio/0000-0003-3359-1743
   Vogt-Maranto, Leslie/0000-0002-7006-4582
   },
Unique-ID = {WOS:000505697300001},
}

@article{ WOS:000684698800020,
Author = {Mei, Kai and Liu, Jun and Zhang, Xiaochen and Rajatheva, Nandana and
   Wei, Jibo},
Title = {Performance Analysis on Machine Learning-Based Channel Estimation},
Journal = {IEEE TRANSACTIONS ON COMMUNICATIONS},
Year = {2021},
Volume = {69},
Number = {8},
Pages = {5183-5193},
Month = {AUG},
Abstract = {Recently, machine learning-based channel estimation has attracted much
   attention. The performance of machine learning-based estimation has been
   validated by simulation experiments. However, little attention has been
   paid to the theoretical performance analysis. In this paper, we
   investigate the mean square error (MSE) performance of machine
   learning-based estimation. Hypothesis testing is employed to analyze its
   MSE upper bound. Furthermore, we build a statistical model for
   hypothesis testing, which holds when the linear learning module with a
   low input dimension is used in machine learning-based channel
   estimation, and derive a clear analytical relation between the size of
   the training data and performance. Then, we simulate the machine
   learning-based channel estimation in orthogonal frequency division
   multiplexing (OFDM) systems to verify our analysis results. Finally, the
   design considerations for the situation where only limited training data
   is available are discussed. In this situation, our analysis results can
   be applied to assess the performance and support the design of machine
   learning-based channel estimation.},
DOI = {10.1109/TCOMM.2021.3083597},
ISSN = {0090-6778},
EISSN = {1558-0857},
ResearcherID-Numbers = {Liu, Jun/Q-9526-2019
   },
ORCID-Numbers = {Rajatheva, Nandana/0000-0002-7029-5583
   Liu, Jun/0000-0001-8769-7231
   Zhang, Xiaochen/0000-0002-7154-5337
   Mei, Kai/0000-0001-6191-3215},
Unique-ID = {WOS:000684698800020},
}

@article{ WOS:000822914300001,
Author = {Li, Kai-Qi and Liu, Yong and Kang, Qing},
Title = {Estimating the thermal conductivity of soils using six machine learning
   algorithms},
Journal = {INTERNATIONAL COMMUNICATIONS IN HEAT AND MASS TRANSFER},
Year = {2022},
Volume = {136},
Month = {JUL},
Abstract = {Many machine learning algorithms have been applied to determine soil
   properties in recent years. However, the prediction performances of
   thermal conductivity of soils via machine learning algorithms are still
   unclear due to the lack of sufficient databases. In this work, a large
   database containing 2197 data points from various literature was
   compiled. Six machine learning algorithms, namely multivariance linear
   regression (MLR), Gaussian process regression (GPR), support vector
   machine (SVM), decision tree (DT), random forest (RF) and adaptive
   boosting methods (AdaBoost) were implemented to predict the thermal
   conductivity of soils based on the compiled database. In addition,
   Spearman correlation analysis and grey relational analysis were adopted
   to compute the strength of influencing factors in thermal conductivity.
   For obtaining the best prediction model, the performances of six machine
   learning algorithms were assessed via eight evaluation indicators and
   compared with three typical empirical models. Results show that the
   AdaBoost can yield good predicted values of the thermal conductivity of
   soils with minimum errors (i.e., RMSE = 0.099). This study constructs a
   database of thermal conductivity and provides a reference for evaluating
   the thermal conductivity of soils via machine learning algorithms.},
DOI = {10.1016/j.icheatmasstransfer.2022.106139},
EarlyAccessDate = {JUN 2022},
Article-Number = {106139},
ISSN = {0735-1933},
EISSN = {1879-0178},
ResearcherID-Numbers = {Kai-Qi, LI/IRZ-5483-2023
   Liu, Yong/R-4019-2017
   },
ORCID-Numbers = {Li, Kaiqi/0000-0001-5467-763X},
Unique-ID = {WOS:000822914300001},
}

@article{ WOS:001218673600001,
Author = {Sunil, Megha and Pallikkavaliyaveetil, Nazreen and Mithun, N. . and
   Gopinath, Anu and Chidangil, Santhosh and Kumar, Satheesh and Lukose,
   Jijo},
Title = {Machine learning assisted Raman spectroscopy: A viable approach for the
   detection of microplastics},
Journal = {JOURNAL OF WATER PROCESS ENGINEERING},
Year = {2024},
Volume = {60},
Month = {APR},
Abstract = {The accumulation of microplastics (MPs) resulting from disposal of
   plastic waste into water sources, poses a significant threat to aquatic
   organisms. These are readily ingested by organisms, leading to the
   accumulation of harmful substances, disrupting their biological
   processes. Current methods for identifying microplastics have notable
   drawbacks, including low resolution, extended imaging time, and
   restricted particle size analysis. Integrating Raman spectroscopy with
   machine learning (ML) proves to be an effective approach for identifying
   and classifying MPs, especially in scenarios where they are found in
   environmental media or mixed with various types. Machine learning (ML)
   can be vital tool in assisting Raman analysis, owing to its robust
   feature extraction capabilities. This comprehensive review outlined the
   utilization of various machine learning techniques in conjunction with
   Raman spectral features for diverse investigations related to
   microplastics. The methodologies discussed encompass Principal Component
   Analysis, K-Nearest Neighbour, Random Forest, Support Vector Machine,
   and various deep learning algorithms.},
DOI = {10.1016/j.jwpe.2024.105150},
EarlyAccessDate = {MAR 2024},
ISSN = {2214-7144},
ResearcherID-Numbers = {Lukose, Jijo/AAL-1190-2020
   Gopinath, Anu/AAX-8259-2020
   Pallikkavaliyaveetil Mohammedsheriff, Nazreen/ISU-2480-2023},
ORCID-Numbers = {KUMAR, SATHEESH MK/0000-0002-4363-9942
   Gopinath, Anu/0000-0002-9246-6582
   },
Unique-ID = {WOS:001218673600001},
}

@article{ WOS:001153683600001,
Author = {Smith, Colin M. and Weathers, Allison L. and Lewis, Steven L.},
Title = {An overview of clinical machine learning applications in neurology},
Journal = {JOURNAL OF THE NEUROLOGICAL SCIENCES},
Year = {2023},
Volume = {455},
Month = {DEC 15},
Abstract = {Machine learning techniques for clinical applications are evolving, and
   the potential impact this will have on clinical neurology is important
   to recognize. By providing a broad overview on this growing paradigm of
   clinical tools, this article aims to help healthcare professionals in
   neurology prepare to navigate both the opportunities and challenges
   brought on through continued advancements in machine learning. This
   narrative review first elaborates on how machine learning models are
   organized and implemented. Machine learning tools are then classified by
   clinical application, with examples of uses within neurology described
   in more detail. Finally, this article addresses limitations and
   considerations regarding clinical machine learning applications in
   neurology.},
DOI = {10.1016/j.jns.2023.122799},
EarlyAccessDate = {NOV 2023},
Article-Number = {122799},
ISSN = {0022-510X},
EISSN = {1878-5883},
ORCID-Numbers = {Weathers, Allison/0009-0006-9041-5999},
Unique-ID = {WOS:001153683600001},
}

@article{ WOS:000562067400001,
Author = {Fatima, Noreen and Liu, Li and Hong, Sha and Ahmed, Haroon},
Title = {Prediction of Breast Cancer, Comparative Review of Machine Learning
   Techniques, and Their Analysis},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {150360-150376},
Abstract = {Breast cancer is type of tumor that occurs in the tissues of the breast.
   It is most common type of cancer found in women around the world and it
   is among the leading causes of deaths in women. This article presents
   the comparative analysis of machine learning, deep learning and data
   mining techniques being used for the prediction of breast cancer. Many
   researchers have put their efforts on breast cancer diagnoses and
   prognoses, every technique has different accuracy rate and it varies for
   different situations, tools and datasets being used. Our main focus is
   to comparatively analyze different existing Machine Learning and Data
   Mining techniques in order to find out the most appropriate method that
   will support the large dataset with good accuracy of prediction. The
   main purpose of this review is to highlight all the previous studies of
   machine learning algorithms that are being used for breast cancer
   prediction and this article provides the all necessary information to
   the beginners who want to analyze the machine learning algorithms to
   gain the base of deep learning.},
DOI = {10.1109/ACCESS.2020.3016715},
ISSN = {2169-3536},
ResearcherID-Numbers = {Ahmed, Haroon/HSI-2826-2023},
ORCID-Numbers = {Liu, Li/0000-0002-4776-5292
   Fatima, Noreen/0000-0001-9880-9024
   Ahmed, Haroon/0000-0002-7146-7404
   },
Unique-ID = {WOS:000562067400001},
}

@article{ WOS:000802148900003,
Author = {Allogba, Stephanie and Aladin, Sandra and Tremblay, Christine},
Title = {Machine-Learning-Based Lightpath QoT Estimation and Forecasting},
Journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY},
Year = {2022},
Volume = {40},
Number = {10},
Pages = {3115-3127},
Month = {MAY 15},
Abstract = {Machine learning (ML) is more and more used to address the challenges of
   managing the physical layer of increasingly heterogeneous and complex
   optical networks. In this tutorial, we illustrate how simple and more
   sophisticated machine learning methods can be used in lightpath quality
   of transmission (QoT) estimation and forecast tasks. We also discuss
   data processing strategies with the aim to determine relevant features
   to feed the ML classifiers and predictors. We then introduce a
   preliminary study on the application of transfer learning to try to
   overcome the scarcity of field data.},
DOI = {10.1109/JLT.2022.3160379},
ISSN = {0733-8724},
EISSN = {1558-2213},
ORCID-Numbers = {ALADIN, SANDRA/0000-0002-8980-7366
   Allogba, Stephanie/0000-0001-9290-5816},
Unique-ID = {WOS:000802148900003},
}

@article{ WOS:000943845300001,
Author = {Tang, Tao and Zhang, Xiaocai and Liu, Yuansheng and Peng, Hui and Zheng,
   Binshuang and Yin, Yanlin and Zeng, Xiangxiang},
Title = {Machine learning on protein-protein interaction prediction: models,
   challenges and trends},
Journal = {BRIEFINGS IN BIOINFORMATICS},
Year = {2023},
Volume = {24},
Number = {2},
Month = {MAR 19},
Abstract = {Protein-protein interactions (PPIs) carry out the cellular processes of
   all living organisms. Experimental methods for PPI detection suffer from
   high cost and false-positive rate, hence efficient computational methods
   are highly desirable for facilitating PPI detection. In recent years,
   benefiting from the enormous amount of protein data produced by advanced
   high-throughput technologies, machine learning models have been well
   developed in the field of PPI prediction. In this paper, we present a
   comprehensive survey of the recently proposed machine learning-based
   prediction methods. The machine learning models applied in these methods
   and details of protein data representation are also outlined. To
   understand the potential improvements in PPI prediction, we discuss the
   trend in the development of machine learning-based methods. Finally, we
   highlight potential directions in PPI prediction, such as the use of
   computationally predicted protein structures to extend the data source
   for machine learning models. This review is supposed to serve as a
   companion for further improvements in this field.},
DOI = {10.1093/bib/bbad076},
EarlyAccessDate = {MAR 2023},
ISSN = {1467-5463},
EISSN = {1477-4054},
ResearcherID-Numbers = {Peng, Hui/ABB-8324-2021
   zeng, xiangxiang/H-3771-2014
   Zheng, Binshuang/MTD-6197-2025
   yin, yanlin/AFB-7218-2022
   Liu, Yuansheng/GSM-8000-2022},
ORCID-Numbers = {Tang, Tao/0000-0002-1207-4192
   Zhang, Xiaocai/0000-0002-3783-6560
   },
Unique-ID = {WOS:000943845300001},
}

@article{ WOS:001013826800001,
Author = {Taherdoost, Hamed},
Title = {Enhancing Social Media Platforms with Machine Learning Algorithms and
   Neural Networks},
Journal = {ALGORITHMS},
Year = {2023},
Volume = {16},
Number = {6},
Month = {JUN},
Abstract = {Network analysis aids management in reducing overall expenditures and
   maintenance workload. Social media platforms frequently use neural
   networks to suggest material that corresponds with user preferences.
   Machine learning is one of many methods for social network analysis.
   Machine learning algorithms operate on a collection of observable
   features that are taken from user data. Machine learning and neural
   network-based systems represent a topic of study that spans several
   fields. Computers can now recognize the emotions behind particular
   content uploaded by users to social media networks thanks to machine
   learning. This study examines research on machine learning and neural
   networks, with an emphasis on social analysis in the context of the
   current literature.},
DOI = {10.3390/a16060271},
Article-Number = {271},
EISSN = {1999-4893},
ResearcherID-Numbers = {Taherdoost, Hamed/A-1270-2015},
Unique-ID = {WOS:001013826800001},
}

@article{ WOS:000838308500001,
Author = {Ning, Hongwei and Li, Rui and Zhou, Teng},
Title = {Machine learning for microalgae detection and utilization},
Journal = {FRONTIERS IN MARINE SCIENCE},
Year = {2022},
Volume = {9},
Month = {JUL 26},
Abstract = {Microalgae are essential parts of marine ecology, and they play a key
   role in species balance. Microalgae also have significant economic
   value. However, microalgae are too tiny, and there are many different
   kinds of microalgae in a single drop of seawater. It is challenging to
   identify microalgae species and monitor microalgae changes. Machine
   learning techniques have achieved massive success in object recognition
   and classification, and have attracted a wide range of attention. Many
   researchers have introduced machine learning algorithms into microalgae
   applications, and similarly significant effects are gained. The paper
   summarizes recent advances based on various machine learning algorithms
   in microalgae applications, such as microalgae classification, bioenergy
   generation from microalgae, environment purification with microalgae,
   and microalgae growth monitor. Finally, we prospect development of
   machine learning algorithms in microalgae treatment in the future.},
DOI = {10.3389/fmars.2022.947394},
Article-Number = {947394},
EISSN = {2296-7745},
ResearcherID-Numbers = {Zhou, Teng/O-6062-2014
   },
ORCID-Numbers = {Zhou, Teng/0000-0002-8744-9083
   Ning, Hongwei/0000-0003-1752-6079},
Unique-ID = {WOS:000838308500001},
}

@article{ WOS:000326889800022,
Author = {Sun, Shiliang},
Title = {A survey of multi-view machine learning},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2013},
Volume = {23},
Number = {7-8},
Pages = {2031-2038},
Month = {DEC},
Abstract = {Multi-view learning or learning with multiple distinct feature sets is a
   rapidly growing direction in machine learning with well theoretical
   underpinnings and great practical success. This paper reviews theories
   developed to understand the properties and behaviors of multi-view
   learning and gives a taxonomy of approaches according to the machine
   learning mechanisms involved and the fashions in which multiple views
   are exploited. This survey aims to provide an insightful organization of
   current developments in the field of multi-view learning, identify their
   limitations, and give suggestions for further research. One feature of
   this survey is that we attempt to point out specific open problems which
   can hopefully be useful to promote the research of multi-view machine
   learning.},
DOI = {10.1007/s00521-013-1362-6},
ISSN = {0941-0643},
EISSN = {1433-3058},
Unique-ID = {WOS:000326889800022},
}

@article{ WOS:000704376900007,
Author = {Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu},
Title = {Federated learning on non-IID data: A survey},
Journal = {NEUROCOMPUTING},
Year = {2021},
Volume = {465},
Pages = {371-390},
Month = {NOV 20},
Abstract = {Federated learning is an emerging distributed machine learning framework
   for privacy preservation. However, models trained in federated learning
   usually have worse performance than those trained in the standard
   centralized learning mode, especially when the training data are not
   independent and identically distributed (Non-IID) on the local devices.
   In this survey, we provide a detailed analysis of the influence of
   Non-IID data on both parametric and non-parametric machine learning
   models in both horizontal and vertical federated learning. In addition,
   current research work on handling challenges of NonIID data in federated
   learning are reviewed, and both advantages and disadvantages of these
   approaches are discussed. Finally, we suggest several future research
   directions before concluding the paper. (c) 2021 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.neucom.2021.07.098},
EarlyAccessDate = {SEP 2021},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Jin, Yaochu/B-3776-2012
   Jin, Yaochu/GRY-7004-2022},
ORCID-Numbers = {Jin, Yaochu/0000-0003-1100-0631
   Liu, Shiqing/0000-0002-9657-1726
   },
Unique-ID = {WOS:000704376900007},
}

@article{ WOS:000488199200019,
Author = {Shukla, Sanyam and Raghuwanshi, Bhagat Singh},
Title = {Online sequential class-specific extreme learning machine for binary
   imbalanced learning},
Journal = {NEURAL NETWORKS},
Year = {2019},
Volume = {119},
Pages = {235-248},
Month = {NOV},
Abstract = {Many real-world applications suffer from the class imbalance problem, in
   which some classes have significantly fewer examples compared to the
   other classes. In this paper, we focus on online sequential learning
   methods, which are considerably more preferable to tackle the large size
   imbalanced classification problems effectively. For example, weighted
   online sequential extreme learning machine (WOS-ELM), voting based
   weighted online sequential extreme learning machine (VWOS-ELM) and
   weighted online sequential extreme learning machine with kernels
   (WOS-ELMK), etc. handle the imbalanced learning effectively. One of our
   recent works class-specific extreme learning machine (CS-ELM) uses
   class-specific regularization and has been shown to perform better for
   imbalanced learning. This work proposes a novel online sequential
   class-specific extreme learning machine (OSCSELM), which is a variant of
   CS-ELM. OSCSELM supports online learning technique in both chunkby-chunk
   and one-by-one learning mode. It targets to handle the class imbalance
   problem for both small and larger datasets. The proposed work has less
   computational complexity in contrast with WOS-ELM for imbalanced
   learning. The proposed method is assessed by utilizing benchmark
   real-world imbalanced datasets. Experimental results illustrate the
   effectiveness of the proposed approach as it outperforms the other
   methods for imbalanced learning. (C) 2019 Elsevier Ltd. All rights
   reserved.},
DOI = {10.1016/j.neunet.2019.08.018},
ISSN = {0893-6080},
EISSN = {1879-2782},
ResearcherID-Numbers = {Raghuwanshi, Bhagat Singh/Y-2664-2019
   shukla, sanyam/C-3019-2017
   },
ORCID-Numbers = {Raghuwanshi, Bhagat Singh/0000-0002-3027-7831},
Unique-ID = {WOS:000488199200019},
}

@article{ WOS:000899349300014,
Author = {Idowu, Samuel and Struber, Daniel and Berger, Thorsten},
Title = {Asset Management in Machine Learning: State-of-research and
   State-of-practice},
Journal = {ACM COMPUTING SURVEYS},
Year = {2023},
Volume = {55},
Number = {7},
Month = {JUL},
Abstract = {Machine learning components are essential for today's software systems,
   causing a need to adapt traditional software engineering practices when
   developing machine-learning-based systems. This need is pronounced due
   to many development-related challenges of machine learning components
   such as asset, experiment, and dependency management. Recently, many
   asset management tools addressing these challenges have become
   available. It is essential to understand the support such tools offer to
   facilitate research and practice on building new management tools with
   native supports for machine learning and software engineering assets.
   This article positions machine learning asset management as a discipline
   that provides improved methods and tools for performing operations on
   machine learning assets. We present a feature-based survey of 18
   state-of-practice and 12 state-of-research tools supporting
   machine-learning assetmanagement. We overview their features for
   managing the types of assets used in machine learning experiments. Most
   state-of-research tools focus on tracking, exploring, and retrieving
   assets to address development concerns such as reproducibility, while
   the state-of-practice tools also offer collaboration and
   workflow-execution-related operations. In addition, assets are primarily
   tracked intrusively from the source code through APIs and managed via
   web dashboards or command-line interfaces (CLIs). We identify
   asynchronous collaboration and asset reusability as directions for new
   tools and techniques.},
DOI = {10.1145/3543847},
Article-Number = {144},
ISSN = {0360-0300},
EISSN = {1557-7341},
ORCID-Numbers = {Idowu, Samuel/0000-0002-4143-322X},
Unique-ID = {WOS:000899349300014},
}

@article{ WOS:001333821800001,
Author = {Li, Zhixiong and Xiang, Yan and Wen, Yujing and Reker, Daniel},
Title = {Yoked learning in molecular data science},
Journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES},
Year = {2024},
Volume = {5},
Month = {JUN},
Abstract = {Active machine learning is an established and increasingly popular
   experimental design technique where the machine learning model can
   request additional data to improve the model's predictive performance.
   It is generally assumed that this data is optimal for the machine
   learning model since it relies on the model's predictions or model
   architecture and therefore cannot be transferred to other models.
   Inspired by research in pedagogy, we here introduce the concept of yoked
   machine learning where a second machine learning model learns from the
   data selected by another model. We found that in 48\% of the benchmarked
   combinations, yoked learning performed similar or better than active
   learning. We analyze distinct cases in which yoked learning can improve
   active learning performance. In particular, we prototype yoked deep
   learning (YoDeL) where a classic machine learning model provides data to
   a deep neural network, thereby mitigating challenges of active deep
   learning such as slow refitting time per learning iteration and poor
   performance on small datasets. In summary, we expect the new concept of
   yoked (deep) learning to provide a competitive option to boost the
   performance of active learning and benefit from distinct capabilities of
   multiple machine learning models during data acquisition, training, and
   deployment.},
DOI = {10.1016/j.ailsci.2023.100089},
Article-Number = {100089},
EISSN = {2667-3185},
ORCID-Numbers = {Wen, Yujing/0009-0000-4663-2263
   Xiang, Yan/0000-0003-4796-2912
   Reker, Daniel/0000-0003-4789-7380},
Unique-ID = {WOS:001333821800001},
}

@article{ WOS:000688449200012,
Author = {Khan, Latif U. and Saad, Walid and Han, Zhu and Hossain, Ekram and Hong,
   Choong Seon},
Title = {Federated Learning for Internet of Things: Recent Advances, Taxonomy,
   and Open Challenges},
Journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS},
Year = {2021},
Volume = {23},
Number = {3},
Pages = {1759-1799},
Abstract = {The Internet of Things (IoT) will be ripe for the deployment of novel
   machine learning algorithm for both network and application management.
   However, given the presence of massively distributed and private
   datasets, it is challenging to use classical centralized learning
   algorithms in the IoT. To overcome this challenge, federated learning
   can be a promising solution that enables on-device machine learning
   without the need to migrate the private end-user data to a central
   cloud. In federated learning, only learning model updates are
   transferred between end-devices and the aggregation server. Although
   federated learning can offer better privacy preservation than
   centralized machine learning, it has still privacy concerns. In this
   paper, first, we present the recent advances of federated learning
   towards enabling federated learning-powered IoT applications. A set of
   metrics such as sparsification, robustness, quantization, scalability,
   security, and privacy, is delineated in order to rigorously evaluate the
   recent advances. Second, we devise a taxonomy for federated learning
   over IoT networks. Finally, we present several open research challenges
   with their possible solutions.},
DOI = {10.1109/COMST.2021.3090430},
EISSN = {1553-877X},
ResearcherID-Numbers = {Han, Zhu/ABG-1222-2021
   Hong, Choong Seon/ABF-5527-2020
   Hossain, Ekram/ABP-6085-2022
   Saad, Walid/C-7978-2018
   Khan, Latif U./ABH-2245-2021
   },
ORCID-Numbers = {Saad, Walid/0000-0003-2247-2458
   Khan, Latif U./0000-0002-7678-6949
   Hong, Choong Seon/0000-0003-3484-7333},
Unique-ID = {WOS:000688449200012},
}

@article{ WOS:000804965300003,
Author = {Kaddoura, Sanaa and Popescu, Daniela Elena and Hemanth, Jude D.},
Title = {A systematic review on machine learning models for online learning and
   examination systems},
Journal = {PEERJ COMPUTER SCIENCE},
Year = {2022},
Volume = {8},
Month = {MAY 18},
Abstract = {Examinations or assessments play a vital role in every student's life;
   they determine their future and career paths. The COVID pandemic has
   left adverse impacts in all areas, including the academic field. The
   regularized classroom learning and face-to-face real-time examinations
   were not feasible to avoid widespread infection and ensure safety.
   During these desperate times, technological advancements stepped in to
   aid students in continuing their education without any academic breaks.
   Machine learning is a key to this digital transformation of schools or
   colleges from real-time to online mode. Online learning and examination
   during lockdown were made possible by Machine learning methods. In this
   article, a systematic review of the role of Machine learning in Lockdown
   Exam Management Systems was conducted by evaluating 135 studies over the
   last five years. The significance of Machine learning in the entire exam
   cycle from pre-exam preparation, conduction of examination, and
   evaluation were studied and discussed. The unsupervised or supervised
   Machine learning algorithms were identified and categorized in each
   process. The primary aspects of examinations, such as authentication,
   scheduling, proctoring, and cheat or fraud detection, are investigated
   in detail with Machine learning perspectives. The main attributes, such
   as prediction of at-risk students, adaptive learning, and monitoring of
   students, are integrated for more understanding of the role of machine
   learning in exam preparation, followed by its management of the
   post-examination process. Finally, this review concludes with issues and
   challenges that machine learning imposes on the examination system, and
   these issues are discussed with solutions.},
DOI = {10.7717/peerj-cs.986},
Article-Number = {e986},
EISSN = {2376-5992},
ResearcherID-Numbers = {Popescu, Daniela/C-7199-2011
   Kaddoura, Sanaa/AAO-3064-2021
   },
ORCID-Numbers = {Institute of Technology and Sciences, Karunya/0009-0006-0726-2507
   Kaddoura, Sanaa/0000-0002-4384-4364},
Unique-ID = {WOS:000804965300003},
}

@article{ WOS:000460685600001,
Author = {Hatt, Mathieu and Parmar, Chintan and Qi, Jinyi and El Naqa, Issam},
Title = {Machine (Deep) Learning Methods for Image Processing and Radiomics},
Journal = {IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES},
Year = {2019},
Volume = {3},
Number = {2, SI},
Pages = {104-108},
Month = {MAR},
Abstract = {Methods from the field of machine (deep) learning have been successful
   in tackling a number of tasks in medical imaging, from image
   reconstruction or processing to predictive modeling, clinical planning
   and decision-aid systems. The ever growing availability of data and the
   improving ability of algorithms to learn from them has led to the rise
   of methods based on neural networks to address most of these tasks with
   higher efficiency and often superior performance than previous,
   ``shallow{''} machine learning methods. The present editorial aims at
   contextualizing within this framework the recent developments of these
   techniques, including these described in the papers published in the
   present special issue on machine (deep) learning for image processing
   and radiomics in radiation-based medical sciences.},
DOI = {10.1109/TRPMS.2019.2899538},
ISSN = {2469-7311},
EISSN = {2469-7303},
ResearcherID-Numbers = {Hatt, Mathieu/M-8917-2017
   Qi, Jinyi/A-1768-2010
   parmar, chintan/J-2977-2019
   Qi, Jinyi/IXW-8224-2023
   hatt, mathieu/M-8917-2017
   Naqa, Issam/T-3066-2019},
ORCID-Numbers = {Qi, Jinyi/0000-0002-5428-0322
   hatt, mathieu/0000-0002-8938-8667
   },
Unique-ID = {WOS:000460685600001},
}

@article{ WOS:001210794000001,
Author = {Xue, Chaogai and Zhang, Haoxiang and Cao, Haiwang},
Title = {Multi-agent modelling and analysis of the knowledge learning of a
   human-machine hybrid intelligent organization with human-machine trust},
Journal = {SYSTEMS SCIENCE \& CONTROL ENGINEERING},
Year = {2024},
Volume = {12},
Number = {1},
Month = {DEC 31},
Abstract = {Machine learning (ML) technologies have changed the paradigm of
   knowledge discovery in organizations and transformed traditional
   organizational learning to human-machine hybrid intelligent
   organizational learning. However, the general distrust among humans
   towards knowledge derived from machine learning has hindered effective
   knowledge exchange between humans and machines, thereby compromising the
   efficiency of human-machine hybrid intelligent organizational learning.
   To explore this issue, we used multi-agent simulation to construct a
   knowledge learning model of a human-machine hybrid intelligent
   organization with human-machine trust. The simulation showed that
   whether human-machine trust has a positive effect on knowledge level
   depends on the initial input and the magnitude of the effect depends on
   the human learning propensity (exploration and exploitation). When
   humans reconfigure machine learning excessively, whether human-machine
   trust has a positive effect on the knowledge level depends on human
   learning propensity (exploration and exploitation). Maintaining
   appropriate human-machine trust in turbulent environments assists humans
   in integrating diverse knowledge to meet changing knowledge needs. Our
   study extends the human-machine hybrid intelligence organizational
   learning model by modeling human-machine trust. It will assist managers
   in effectively designing the most economical level of human-machine
   trust, thereby enhancing the efficiency of human-machine collaboration
   in human-machine hybrid intelligent organization.},
DOI = {10.1080/21642583.2024.2343301},
Article-Number = {2343301},
EISSN = {2164-2583},
ResearcherID-Numbers = {Chaogai, Xue/MNR-1586-2025
   Xue, Chaogai/MNR-1586-2025},
ORCID-Numbers = {Zhang, Haoxiang/0000-0002-2380-4269
   Xue, Chaogai/0000-0003-4998-4744},
Unique-ID = {WOS:001210794000001},
}

@article{ WOS:000680962300001,
Author = {Luo, Shaobo and Shi, Yuzhi and Chin, Lip Ket and Hutchinson, Paul Edward
   and Zhang, Yi and Chierchia, Giovanni and Talbot, Hugues and Jiang,
   Xudong and Bourouina, Tarik and Liu, Ai-Qun},
Title = {Machine-Learning-Assisted Intelligent Imaging Flow Cytometry: A Review},
Journal = {ADVANCED INTELLIGENT SYSTEMS},
Year = {2021},
Volume = {3},
Number = {11},
Month = {NOV},
Abstract = {Imaging flow cytometry has been widely adopted in numerous applications
   such as optical sensing, environmental monitoring, clinical diagnostics,
   and precision agriculture. The system, with the assistance of machine
   learning, shows unprecedented advantages in automated image analysis,
   thus enabling high-throughput measurement, identification, and sorting
   of biological entities. Recently, with the burgeoning developments of
   machine learning algorithms, deep learning has taken over most of data
   analysis and promised tremendous performance in intelligent imaging flow
   cytometry. Herein, an overview of the basic knowledge of intelligent
   imaging flow cytometry, the evolution of machine learning and the
   typical applications, and how machine learning can be applied to assist
   intelligent imaging flow cytometry is provided. Perspectives of emerging
   machine learning algorithms in implementing future intelligent imaging
   flow cytometry are also discussed.},
DOI = {10.1002/aisy.202100073},
EarlyAccessDate = {JUL 2021},
Article-Number = {2100073},
EISSN = {2640-4567},
ResearcherID-Numbers = {Zhang, Yi/AAG-7916-2021
   BOUROUINA, Tarik/GYE-2827-2022
   Shi, Yuzhi/AGM-3241-2022
   Luo, Shaobo/AAM-9933-2021
   Jiang, Xudong/B-1555-2008
   Chin, Lip Ket/AAT-1535-2020},
ORCID-Numbers = {Talbot, Hugues/0000-0002-2179-3498
   Shi, Yuzhi/0000-0002-9041-0462
   BOUROUINA, Tarik/0000-0003-2342-7149
   },
Unique-ID = {WOS:000680962300001},
}

@article{ WOS:000801156200002,
Author = {Zhang, Xiao-Qing and Hu, Yan and Xiao, Zun-Jie and Fang, Jian-Sheng and
   Higashita, Risa and Liu, Jiang},
Title = {Machine Learning for Cataract Classification/Grading on Ophthalmic
   Imaging Modalities: A Survey},
Journal = {MACHINE INTELLIGENCE RESEARCH},
Year = {2022},
Volume = {19},
Number = {3},
Pages = {184-208},
Month = {JUN},
Abstract = {Cataracts are the leading cause of visual impairment and blindness
   globally. Over the years, researchers have achieved significant progress
   in developing state-of-the-art machine learning techniques for automatic
   cataract classification and grading, aiming to prevent cataracts early
   and improve clinicians' diagnosis efficiency. This survey provides a
   comprehensive survey of recent advances in machine learning techniques
   for cataract classification/grading based on ophthalmic images. We
   summarize existing literature from two research directions: conventional
   machine learning methods and deep learning methods. This survey also
   provides insights into existing works of both merits and limitations. In
   addition, we discuss several challenges of automatic cataract
   classification/grading based on machine learning techniques and present
   possible solutions to these challenges for future research.},
DOI = {10.1007/s11633-022-1329-0},
ISSN = {2731-538X},
EISSN = {2731-5398},
ResearcherID-Numbers = {Hu, Yan/B-4056-2018
   Zhang, Xiaoqing/KFT-2595-2024
   LIU, JIANG/AHB-8921-2022},
ORCID-Numbers = {liu, jiang/0000-0001-6281-6505
   , Xiaoqing/0000-0002-8114-7121
   },
Unique-ID = {WOS:000801156200002},
}

@article{ WOS:000559380900005,
Author = {Cai, Jiazhen and Chu, Xuan and Xu, Kun and Li, Hongbo and Wei, Jing},
Title = {Machine learning-driven new material discovery},
Journal = {NANOSCALE ADVANCES},
Year = {2020},
Volume = {2},
Number = {8},
Pages = {3115-3130},
Month = {AUG 1},
Abstract = {New materials can bring about tremendous progress in technology and
   applications. However, the commonly used trial-and-error method cannot
   meet the current need for new materials. Now, a newly proposed idea of
   using machine learning to explore new materials is becoming popular. In
   this paper, we review this research paradigm of applying machine
   learning in material discovery, including data preprocessing, feature
   engineering, machine learning algorithms and cross-validation
   procedures. Furthermore, we propose to assist traditional DFT
   calculations with machine learning for material discovery. Many
   experiments and literature reports have shown the great effects and
   prospects of this idea. It is currently showing its potential and
   advantages in property prediction, material discovery, inverse design,
   corrosion detection and many other aspects of life.},
DOI = {10.1039/d0na00388c},
ISSN = {2516-0230},
ResearcherID-Numbers = {Li, Hongbo/D-9519-2012
   Wei, Jing/AGO-9658-2022},
ORCID-Numbers = {Wei, Jing/0000-0003-2991-0123
   },
Unique-ID = {WOS:000559380900005},
}

@article{ WOS:000725106400001,
Author = {Liu, Yi and Yang, Menglong and Wang, Yudong and Li, Yongshan and Xiong,
   Tiancheng and Li, Anzhe},
Title = {Applying machine learning algorithms to predict default probability in
   the online credit market: Evidence from China},
Journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS},
Year = {2022},
Volume = {79},
Month = {JAN},
Abstract = {Using data from Renrendai and three machine learning algorithms, namely,
   k-nearest neighbor, support vector machine, and random forest, we
   predicted the default probability of online loan borrowers and compared
   their prediction performance with that of a logistic model. The results
   show that, first, based on the AUC (area under the ROC curve) value,
   accuracy rate and Brier score, the machine learning models can
   accurately predict the default risk of online borrowers. Second, the
   integrated discrimination improvement (IDI) test results show that the
   prediction performance of the machine learning algorithms is
   significantly better than that of the logistic model. Third, after
   constructing the investor profit function with misclassification cost,
   we find that the machine learning algorithms can provide more benefits
   to investors.},
DOI = {10.1016/j.irfa.2021.101971},
EarlyAccessDate = {NOV 2021},
Article-Number = {101971},
ISSN = {1057-5219},
EISSN = {1873-8079},
ResearcherID-Numbers = {wang, yiyi/GWZ-5830-2022},
Unique-ID = {WOS:000725106400001},
}

@article{ WOS:001016803900001,
Author = {Mandalapu, Varun and Elluri, Lavanya and Vyas, Piyush and Roy, Nirmalya},
Title = {Crime Prediction Using Machine Learning and Deep Learning: A Systematic
   Review and Future Directions},
Journal = {IEEE ACCESS},
Year = {2023},
Volume = {11},
Pages = {60153-60170},
Abstract = {Predicting crime using machine learning and deep learning techniques has
   gained considerable attention from researchers in recent years, focusing
   on identifying patterns and trends in crime occurrences. This review
   paper examines over 150 articles to explore the various machine learning
   and deep learning algorithms applied to predict crime. The study
   provides access to the datasets used for crime prediction by researchers
   and analyzes prominent approaches applied in machine learning and deep
   learning algorithms to predict crime, offering insights into different
   trends and factors related to criminal activities. Additionally, the
   paper highlights potential gaps and future directions that can enhance
   the accuracy of crime prediction. Finally, the comprehensive overview of
   research discussed in this paper on crime prediction using machine
   learning and deep learning approaches serves as a valuable reference for
   researchers in this field. By gaining a deeper understanding of crime
   prediction techniques, law enforcement agencies can develop strategies
   to prevent and respond to criminal activities more effectively.},
DOI = {10.1109/ACCESS.2023.3286344},
ISSN = {2169-3536},
ResearcherID-Numbers = {Mandalapu, Varun/AAT-5212-2020
   vyas, piyush/GMW-4110-2022
   Elluri, Lavanya/ACG-4312-2022},
ORCID-Numbers = {Vyas, Piyush/0000-0002-0918-5334
   Elluri, Lavanya/0000-0002-8881-3369},
Unique-ID = {WOS:001016803900001},
}

@article{ WOS:001395340500013,
Author = {Montesuma, Eduardo Fernandes and Mboula, Fred Maurice Ngole and
   Souloumiac, Antoine},
Title = {Recent Advances in Optimal Transport for Machine Learning},
Journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
Year = {2025},
Volume = {47},
Number = {2},
Pages = {1161-1180},
Month = {FEB},
Abstract = {Recently, Optimal Transport has been proposed as a probabilistic
   framework in Machine Learning for comparing and manipulating probability
   distributions. This is rooted in its rich history and theory, and has
   offered new solutions to different problems in machine learning, such as
   generative modeling and transfer learning. In this survey we explore
   contributions of Optimal Transport for Machine Learning over the period
   2012 - 2023, focusing on four sub-fields of Machine Learning:
   supervised, unsupervised, transfer and reinforcement learning. We
   further highlight the recent development in computational Optimal
   Transport and its extensions, such as partial, unbalanced, Gromov and
   Neural Optimal Transport, and its interplay with Machine Learning
   practice.},
DOI = {10.1109/TPAMI.2024.3489030},
ISSN = {0162-8828},
EISSN = {1939-3539},
ResearcherID-Numbers = {Montesuma, Eduardo/AGX-6624-2022
   },
ORCID-Numbers = {Fernandes Montesuma, Eduardo/0000-0003-3850-4602},
Unique-ID = {WOS:001395340500013},
}

@article{ WOS:000431709400001,
Author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey
   Andreyevich and Siskind, Jeffrey Mark},
Title = {Automatic Differentiation in Machine Learning: a Survey},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2018},
Volume = {18},
Abstract = {Derivatives, mostly in the form of gradients and Hessians, are
   ubiquitous in machine learning. Automatic differentiation (AD), also
   called algorithmic differentiation or simply ``autodiff{''}, is a family
   of techniques similar to but more general than backpropagation for
   efficiently and accurately evaluating derivatives of numeric functions
   expressed as computer programs. AD is a small but established field with
   applications in areas including computational fluid dynamics,
   atmospheric sciences, and engineering design optimization. Until very
   recently, the fields of machine learning and AD have largely been
   unaware of each other and, in some cases, have independently discovered
   each other's results. Despite its relevance, general-purpose AD has been
   missing from the machine learning toolbox, a situation slowly changing
   with its ongoing adoption under the names ``dynamic computational
   graphs{''} and ``differentiable programming{''}. We survey the
   intersection of AD and machine learning, cover applications where AD has
   direct relevance, and address the main implementation techniques. By
   precisely defining the main differentiation techniques and their
   interrelationships, we aim to bring clarity to the usage of the terms
   ``autodiff{''}, ``automatic differentiation{''}, and ``symbolic
   differentiation{''} as these are encountered more and more in machine
   learning settings.},
Article-Number = {153},
ISSN = {1532-4435},
ResearcherID-Numbers = {Pearlmutter, Barak/M-8791-2014
   Baydin, Atilim Gunes/M-7029-2014},
Unique-ID = {WOS:000431709400001},
}

@article{ WOS:000297611600005,
Author = {Cao, Jiuwen and Lin, Zhiping and Huang, Guang-Bin and Liu, Nan},
Title = {Voting based extreme learning machine},
Journal = {INFORMATION SCIENCES},
Year = {2012},
Volume = {185},
Number = {1},
Pages = {66-77},
Month = {FEB 15},
Abstract = {This paper proposes an improved learning algorithm for classification
   which is referred to as voting based extreme learning machine. The
   proposed method incorporates the voting method into the popular extreme
   learning machine (ELM) in classification applications. Simulations on
   many real world classification datasets have demonstrated that this
   algorithm generally outperforms the original ELM algorithm as well as
   several recent classification algorithms. (C) 2011 Elsevier Inc. All
   rights reserved.},
DOI = {10.1016/j.ins.2011.09.015},
ISSN = {0020-0255},
ResearcherID-Numbers = {Huang, Guang-Bin/A-5035-2011
   Liu, Nan/HCS-2632-2022
   Lin, Zhiping/AAF-2719-2020
   cao, jiuwen/C-9547-2009
   Huang, Guang-Bin/JZE-2974-2024},
ORCID-Numbers = {Liu, Nan/0000-0003-3610-4883
   Huang, Guang-Bin/0000-0002-2480-4965},
Unique-ID = {WOS:000297611600005},
}

@article{ WOS:000617870300006,
Author = {Chen Hongsong and Zhang Yongpeng and Cao Yongrui and Bhargava, Bharat},
Title = {Security Threats and Defensive Approaches in Machine Learning System
   Under Big Data Environment},
Journal = {WIRELESS PERSONAL COMMUNICATIONS},
Year = {2021},
Volume = {117},
Number = {4, SI},
Pages = {3505-3525},
Month = {APR},
Abstract = {Under big data environment, machine learning has been rapidly developed
   and widely used. It has been successfully applied in computer vision,
   natural language processing, computer security and other application
   fields. However, there are many security problems in machine learning
   under big data environment. For example, attackers can add
   ``poisoned{''} sample to the data source, and big data process system
   will process these ``poisoned{''} sample and use machine learning
   methods to train model, which will directly lead to wrong prediction
   results. In this paper, machine learning system and machine learning
   pipeline are proposed. The security problems that maybe occur in each
   stage of machine learning system under big data processing pipeline are
   analyzed comprehensively. We use four different attack methods to
   compare the attack experimental results.The security problems are
   classified comprehensively, and the defense approaches to each security
   problem are analyzed. Drone-deploy MapEngine is selected as a case
   study, we analyze the security threats and defense approaches in the
   Drone-Cloud machine learning application envirolment. At last,the future
   development drections of security issues and challenages in the machine
   learning system are proposed.},
DOI = {10.1007/s11277-021-08284-8},
EarlyAccessDate = {FEB 2021},
ISSN = {0929-6212},
EISSN = {1572-834X},
ResearcherID-Numbers = {Zhang, Yongpeng/KUC-8347-2024},
Unique-ID = {WOS:000617870300006},
}

@article{ WOS:000838497700001,
Author = {Condran, Sarah and Bewong, Michael and Islam, Md Zahidul and Maphosa,
   Lancelot and Zheng, Lihong},
Title = {Machine Learning in Precision Agriculture: A Survey on Trends,
   Applications and Evaluations Over Two Decades},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {73786-73803},
Abstract = {Precision agriculture represents the new age of conventional
   agriculture. This is made possible by the advancement of various modern
   technologies such as the internet of things. The unparalleled potential
   for data collection and analytics has resulted in an increase in
   multi-disciplinary research within machine learning and agriculture.
   However, the application of machine learning techniques to agriculture
   seems to be out of step with core machine learning research. This gap is
   further exacerbated by the inherent challenges associated with
   agricultural data. In this work, we conduct a systematic review of a
   large body of academic literature published between 2000 and 2022, on
   the application of machine learning techniques to agriculture. We
   identify and discuss some of the key data issues such as class
   imbalance, data sparsity and high dimensionality. Further, we study the
   impact of these data issues on various machine learning approaches
   within the context of agriculture. Finally, we identify some of the
   common pitfalls in the machine learning and agriculture research
   including the misapplication of machine learning evaluation techniques.
   To this end, this survey presents a holistic view on the state of
   affairs in the cross-domain of machine learning and agriculture and
   proposes some suitable mitigation strategies to address these
   challenges.},
DOI = {10.1109/ACCESS.2022.3188649},
ISSN = {2169-3536},
ResearcherID-Numbers = {Bewong, Michael/HDM-1834-2022
   Islam, Zahidul/AAP-1951-2020
   Zheng, Lihong/ACI-9716-2022},
ORCID-Numbers = {Islam, Zahid/0000-0002-4868-4945
   Condran, Sarah/0000-0001-7813-2116
   Bewong, Michael/0000-0002-5848-7451
   Zheng, Lihong/0000-0001-5728-4356},
Unique-ID = {WOS:000838497700001},
}

@article{ WOS:000556625500002,
Author = {Damiati, Safa A.},
Title = {Digital Pharmaceutical Sciences},
Journal = {AAPS PHARMSCITECH},
Year = {2020},
Volume = {21},
Number = {6},
Month = {JUL 26},
Abstract = {Artificial intelligence (AI) and machine learning, in particular, have
   gained significant interest in many fields, including pharmaceutical
   sciences. The enormous growth of data from several sources, the recent
   advances in various analytical tools, and the continuous developments in
   machine learning algorithms have resulted in a rapid increase in new
   machine learning applications in different areas of pharmaceutical
   sciences. This review summarizes the past, present, and potential future
   impacts of machine learning technologies on different areas of
   pharmaceutical sciences, including drug design and discovery,
   preformulation, and formulation. The machine learning methods commonly
   used in pharmaceutical sciences are discussed, with a specific emphasis
   on artificial neural networks due to their capability to model the
   nonlinear relationships that are commonly encountered in pharmaceutical
   research. AI and machine learning technologies in common day-to-day
   pharma needs as well as industrial and regulatory insights are reviewed.
   Beyond traditional potentials of implementing digital technologies using
   machine learning in the development of more efficient, fast, and
   economical solutions in pharmaceutical sciences are also discussed.},
DOI = {10.1208/s12249-020-01747-4},
Article-Number = {206},
ISSN = {1530-9932},
ResearcherID-Numbers = {Damiati, Safa/X-4254-2018},
Unique-ID = {WOS:000556625500002},
}

@article{ WOS:000678520000001,
Author = {Hansen, Kristian Bondo and Borch, Christian},
Title = {The absorption and multiplication of uncertainty in
   machine-learning-driven finance},
Journal = {BRITISH JOURNAL OF SOCIOLOGY},
Year = {2021},
Volume = {72},
Number = {4},
Pages = {1015-1029},
Month = {SEP},
Abstract = {Uncertainty about market developments and their implications
   characterize financial markets. Increasingly, machine learning is
   deployed as a tool to absorb this uncertainty and transform it into
   manageable risk. This article analyses machine-learning-based
   uncertainty absorption in financial markets by drawing on 182 interviews
   in the finance industry, including 45 interviews with informants who
   were actively applying machine-learning techniques to investment
   management, trading, or risk management problems. We argue that while
   machine-learning models are deployed to absorb financial uncertainty,
   they also introduce a new and more profound type of uncertainty, which
   we call critical model uncertainty. Critical model uncertainty refers to
   the inability to explain how and why the machine-learning models
   (particularly neural networks) arrive at their predictions and
   decisions-their uncertainty-absorbing accomplishments. We suggest that
   the dialectical relation between machine-learning models' uncertainty
   absorption and multiplication calls for further research in the field of
   finance and beyond.},
DOI = {10.1111/1468-4446.12880},
EarlyAccessDate = {JUL 2021},
ISSN = {0007-1315},
EISSN = {1468-4446},
ResearcherID-Numbers = {hansen, kristian/JNT-3421-2023
   },
ORCID-Numbers = {Chanelian, Serena/0000-0001-5727-6137
   Hansen, Kristian Bondo/0000-0002-9536-6050
   Borch, Christian/0000-0001-8217-5880},
Unique-ID = {WOS:000678520000001},
}

@article{ WOS:000502568900010,
Author = {Tandon, Neeraj and Tandon, Rajiv},
Title = {Using machine learning to explain the heterogeneity of schizophrenia.
   Realizing the promise and avoiding the hype},
Journal = {SCHIZOPHRENIA RESEARCH},
Year = {2019},
Volume = {214},
Number = {SI},
Pages = {70-75},
Month = {DEC},
Abstract = {Despite extensive research and prodigious advances in neuroscience, our
   comprehension of the nature of schizophrenia remains rudimentary. Our
   failure to make progress is attributed to the extreme heterogeneity of
   this condition, enormous complexity of the human brain, limitations of
   extant research paradigms, and inadequacy of traditional statistical
   methods to integrate or interpret increasingly large amounts of
   multidimensional information relevant to unravelling brain function.
   Fortunately, the rapidly developing science of machine learning appears
   to provide tools capable of addressing each of these impediments.
   Enthusiasm about the potential of machine learning methods to break the
   current impasse is reflected in the steep increase in the number of
   scientific publication about the application of machine learning to the
   study of schizophrenia. Machine learning approaches are, however, poorly
   understood by schizophrenia researchers and clinicians alike. In this
   paper, we provide a simple description of the nature and techniques of
   machine learning and their application to the study of schizophrenia. We
   then summarize its potential and constraints with illustrations from six
   studies of machine learning in schizophrenia and address some common
   misconceptions about machine learning. We suggest some guidelines for
   researchers, readers, science editors and reviewers of the burgeoning
   machine learning literature in schizophrenia. In order to realize its
   enormous promise, we suggest the need for the disciplined application of
   machine learning methods to the study of schizophrenia with a dear
   recognition of its capability and challenges accompanied by a concurrent
   effort to improve machine learning literacy among neuroscientists and
   mental health professionals. (C) 2019 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.schres.2019.08.032},
ISSN = {0920-9964},
EISSN = {1573-2509},
Unique-ID = {WOS:000502568900010},
}

@article{ WOS:000444659200024,
Author = {Biggio, Battista and Roli, Fabio},
Title = {Wild patterns: Ten years after the rise of adversarial machine learning},
Journal = {PATTERN RECOGNITION},
Year = {2018},
Volume = {84},
Pages = {317-331},
Month = {DEC},
Abstract = {Learning-based pattern classifiers, including deep networks, have shown
   impressive performance in several application domains, ranging from
   computer vision to cybersecurity. However, it has also been shown that
   adversarial input perturbations carefully crafted either at training or
   at test time can easily subvert their predictions. The vulnerability of
   machine learning to such wild patterns (also referred to as adversarial
   examples), along with the design of suitable countermeasures, have been
   investigated in the research field of adversarial machine learning. In
   this work, we provide a thorough overview of the evolution of this
   research area over the last ten years and beyond, starting from
   pioneering, earlier work on the security of non-deep learning algorithms
   up to more recent work aimed to understand the security properties of
   deep learning algorithms, in the context of computer vision and
   cybersecurity tasks. We report interesting connections between these
   apparently-different lines of work, highlighting common misconceptions
   related to the security evaluation of machine-learning algorithms. We
   review the main threat models and attacks defined to this end, and
   discuss the main limitations of current work, along with the
   corresponding future challenges towards the design of more secure
   learning algorithms. (C) 2018 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.patcog.2018.07.023},
ISSN = {0031-3203},
EISSN = {1873-5142},
ResearcherID-Numbers = {Biggio, Battista/HCI-4766-2022
   BIGGIO, BATTISTA/M-5931-2016},
ORCID-Numbers = {ROLI, FABIO/0000-0003-4103-9190
   BIGGIO, BATTISTA/0000-0001-7752-509X},
Unique-ID = {WOS:000444659200024},
}

@article{ WOS:000550878000001,
Author = {Hagen, Linda and Uetake, Kosuke and Yang, Nathan and Bollinger, Bryan
   and Chaney, Allison J. B. and Dzyabura, Daria and Etkin, Jordan and
   Goldfarb, Avi and Liu, Liu and Sudhir, K. and Wang, Yanwen and Wright,
   James R. and Zhu, Ying},
Title = {How can machine learning aid behavioral marketing research?},
Journal = {MARKETING LETTERS},
Year = {2020},
Volume = {31},
Number = {4, SI},
Pages = {361-370},
Month = {DEC},
Abstract = {Behavioral science and machine learning have rapidly progressed in
   recent years. As there is growing interest among behavioral scholars to
   leverage machine learning, we present strategies for how these methods
   that can be of value to behavioral scientists using examples centered on
   behavioral research.},
DOI = {10.1007/s11002-020-09535-7},
EarlyAccessDate = {JUL 2020},
ISSN = {0923-0645},
EISSN = {1573-059X},
ResearcherID-Numbers = {Hagen, Linda/IXX-0431-2023
   Bollinger, Bryan/ABH-5961-2020
   Sudhir, K./A-1616-2012
   Goldfarb, Avi/A-1092-2008
   Liu, Liu/HLG-3708-2023},
ORCID-Numbers = {Bollinger, Bryan/0000-0001-8596-6418
   Wright, James/0000-0002-8601-925X
   Hagen, Linda/0000-0001-5596-0648
   },
Unique-ID = {WOS:000550878000001},
}

@article{ WOS:000524750000009,
Author = {Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and
   Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and
   Baker, Thar},
Title = {Analysis of Dimensionality Reduction Techniques on Big Data},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {54776-54788},
Abstract = {Due to digitization, a huge volume of data is being generated across
   several sectors such as healthcare, production, sales, IoT devices, Web,
   organizations. Machine learning algorithms are used to uncover patterns
   among the attributes of this data. Hence, they can be used to make
   predictions that can be used by medical practitioners and people at
   managerial level to make executive decisions. Not all the attributes in
   the datasets generated are important for training the machine learning
   algorithms. Some attributes might be irrelevant and some might not
   affect the outcome of the prediction. Ignoring or removing these
   irrelevant or less important attributes reduces the burden on machine
   learning algorithms. In this work two of the prominent dimensionality
   reduction techniques, Linear Discriminant Analysis (LDA) and Principal
   Component Analysis (PCA) are investigated on four popular Machine
   Learning (ML) algorithms, Decision Tree Induction, Support Vector
   Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using
   publicly available Cardiotocography (CTG) dataset from University of
   California and Irvine Machine Learning Repository. The experimentation
   results prove that PCA outperforms LDA in all the measures. Also, the
   performance of the classifiers, Decision Tree, Random Forest examined is
   not affected much by using PCA and LDA.To further analyze the
   performance of PCA and LDA the eperimentation is carried out on Diabetic
   Retinopathy (DR) and Intrusion Detection System (IDS) datasets.
   Experimentation results prove that ML algorithms with PCA produce better
   results when dimensionality of the datasets is high. When dimensionality
   of datasets is low it is observed that the ML algorithms without
   dimensionality reduction yields better results.},
DOI = {10.1109/ACCESS.2020.2980942},
ISSN = {2169-3536},
ResearcherID-Numbers = {Baker, Thar/H-6073-2019
   Kaluri, Rajesh/AAA-5726-2019
   Srivastava, Gautam/N-5668-2019
   Maddikunta, Praveen Kumar Reddy/HSG-8292-2023
   Kumar Reddy, Dr. Praveen/HSG-8292-2023
   Gadekallu, Thippa Reddy/T-4254-2019
   Lakshmanna, Kuruva/T-4814-2019
   rajput, dharmendra/AAZ-3053-2021
   },
ORCID-Numbers = {KALURI, RAJESH/0000-0003-2073-9833
   Srivastava, Gautam/0000-0001-9851-4103
   Kumar Reddy, Dr. Praveen/0000-0003-4209-2495
   Gadekallu, Thippa Reddy/0000-0003-0097-801X
   Baker, Thar/0000-0002-5166-4873
   Kuruva, Lakshmanna/0000-0003-3480-4851},
Unique-ID = {WOS:000524750000009},
}

@article{ WOS:000703023000002,
Author = {Abdelkader, Wael and Navarro, Tamara and Parrish, Rick and Cotoi, Chris
   and Germini, Federico and Iorio, Alfonso and Haynes, R. Brian and
   Lokker, Cynthia},
Title = {Machine Learning Approaches to Retrieve High-Quality, Clinically
   Relevant Evidence From the Biomedical Literature: Systematic Review},
Journal = {JMIR MEDICAL INFORMATICS},
Year = {2021},
Volume = {9},
Number = {9},
Month = {SEP},
Abstract = {Background: The rapid growth of the biomedical literature makes
   identifying strong evidence a time-consuming task. Applying machine
   learning to the process could be a viable solution that limits effort
   while maintaining accuracy.
   Objective: The goal of the research was to summarize the nature and
   comparative performance of machine learning approaches that have been
   applied to retrieve high-quality evidence for clinical consideration
   from the biomedical literature.
   Methods: We conducted a systematic review of studies that applied
   machine learning techniques to identify high-quality clinical articles
   in the biomedical literature. Multiple databases were searched to July
   2020. Extracted data focused on the applied machine learning model,
   steps in the development of the models, and model performance.
   Results: From 3918 retrieved studies, 10 met our inclusion criteria. All
   followed a supervised machine learning approach and applied, from a
   limited range of options, a high-quality standard for the training of
   their model. The results show that machine learning can achieve a
   sensitivity of 95\% while maintaining a high precision of 86\%.
   Conclusions: Machine learning approaches perform well in retrieving
   high-quality clinical studies. Performance may improve by applying more
   sophisticated approaches such as active learning and unsupervised
   machine learning approaches.},
DOI = {10.2196/30401},
Article-Number = {e30401},
EISSN = {2291-9694},
ResearcherID-Numbers = {Lokker, Cynthia/L-4177-2017
   Germini, Federico/K-6881-2016
   Iorio, Alfonso/B-9478-2013
   },
ORCID-Numbers = {Parrish, Rick/0000-0002-3809-6015
   Abdelkader, Wael/0000-0002-9581-1521
   Germini, Federico/0000-0002-0802-3616
   Iorio, Alfonso/0000-0002-3331-8766
   Lokker, Cynthia/0000-0003-2436-4290
   Haynes, Robert Brian/0000-0002-1453-3196},
Unique-ID = {WOS:000703023000002},
}

@article{ WOS:001091638200001,
Author = {Herhausen, Dennis and Bernritter, Stefan F. and Ngai, Eric W. T. and
   Kumar, Ajay and Delen, Dursun},
Title = {Machine learning in marketing: Recent progress and future research
   directions},
Journal = {JOURNAL OF BUSINESS RESEARCH},
Year = {2024},
Volume = {170},
Month = {JAN},
Abstract = {Decision-making in marketing has changed dramatically in the past
   decade. Companies increasingly use algorithms to generate predictions
   for marketing decisions, such as which consumers to target with which
   offers. Such algorithmic decision-making promises to make marketing more
   intelligent, efficient, consumer-friendly, and, ultimately, more
   effective. Not surprisingly, machine learning is a trending topic for
   marketing researchers and practitioners. However, machine learning also
   introduces important challenges to the marketing landscape. We discuss
   this development by outlining recent progress and future research
   directions of machine learning in marketing. Specifically, we provide an
   overview of typical machine learning applications in marketing and
   present a guiding framework. We position the articles in the Journal of
   Business Research's Special Issue on ``Machine Learning in Marketing{''}
   within this framework and conclude by putting forward a research agenda
   to further guide future research in this area.},
DOI = {10.1016/j.jbusres.2023.114254},
EarlyAccessDate = {OCT 2023},
Article-Number = {114254},
ISSN = {0148-2963},
EISSN = {1873-7978},
ResearcherID-Numbers = {Delen, Dursun/AGA-9892-2022
   KUMAR, AJAY/KIC-8060-2024
   Herhausen, Dennis/AAY-2492-2021
   Bernritter, Stefan/M-4750-2019
   Ngai, Eric/ABC-2167-2020
   },
ORCID-Numbers = {Bernritter, Stefan/0000-0002-4291-7824
   Herhausen, Dennis/0000-0002-4335-1703},
Unique-ID = {WOS:001091638200001},
}

@article{ WOS:001181952800001,
Author = {Ni, Chunchun and Li, Shan Cang},
Title = {Machine learning enabled Industrial IoT Security: Challenges, Trends and
   Solutions},
Journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION},
Year = {2024},
Volume = {38},
Month = {MAR},
Abstract = {Introduction: The increasingly integrated Industrial IoT (IIoT) with
   industrial systems brings benefits such as intelligent analytics,
   predictive maintenance, and remote monitoring. However, it also exposes
   industry systems to malware, cyber attacks, and other security risks.
   Objectives: Machine learning techniques shows promising performance in
   cyber security, including threats detection, vulnerability analysis,
   risk assessment, etc. This work aims to investigate how machine learning
   based techniques can be used in enhancing IIoT security and preventing
   cyber security events against cyber attack, safety threats, and process
   disruption. Methods: A comprehensive survey was conducted to show how
   machine learning techniques learn detection malicious activities
   automatically. Conclusion: This work reviewed current literature related
   to machine learning based methods currently being used in IIoT cyber
   security, and key methods have been presented and compared in terms of
   their capability and performance against cyber attacks.},
DOI = {10.1016/j.jii.2023.100549},
EarlyAccessDate = {FEB 2024},
Article-Number = {100549},
ISSN = {2467-964X},
EISSN = {2452-414X},
ORCID-Numbers = {/0000-0001-5663-7420},
Unique-ID = {WOS:001181952800001},
}

@article{ WOS:000509098300009,
Author = {Zhang, Xinlei and Wu, Jinlong and Coutier-Delgosha, Olivier and Xiao,
   Heng},
Title = {Recent progress in augmenting turbulence models with physics-informed
   machine learning},
Journal = {JOURNAL OF HYDRODYNAMICS},
Year = {2019},
Volume = {31},
Number = {6},
Pages = {1153-1158},
Month = {DEC},
Abstract = {In view of the long stagnation in traditional turbulence modeling,
   researchers have attempted using machine learning to augment turbulence
   models. This paper presents some of the recent progresses in our group
   on augmenting turbulence models with physics-informed machine learning.
   We also discuss our works on ensemble-based field inversion to provide
   training data for constructing machine learning models. Future and
   on-going research efforts are introduced.},
DOI = {10.1007/s42241-019-0089-y},
ISSN = {1001-6058},
EISSN = {1878-0342},
ResearcherID-Numbers = {Xiao, Heng/B-6620-2014
   Wu, Jinlong/X-3225-2018
   Zhang, Xin-Lei/ABC-4192-2021},
ORCID-Numbers = {Wu, Jinlong/0000-0001-7438-4228
   Zhang, Xin-Lei/0000-0003-2281-3363},
Unique-ID = {WOS:000509098300009},
}

@article{ WOS:000582339000004,
Author = {Ma, Liye and Sun, Baohong},
Title = {Machine learning and AI in marketing - Connecting computing power to
   human insights},
Journal = {INTERNATIONAL JOURNAL OF RESEARCH IN MARKETING},
Year = {2020},
Volume = {37},
Number = {3},
Pages = {481-504},
Month = {SEP},
Abstract = {Artificial intelligence (AI) agents driven by machine learning
   algorithms are rapidly transforming the business world, generating
   heightened interest from researchers. In this paper, we review and call
   for marketing research to leverage machine learning methods. We provide
   an overview of common machine learning tasks and methods, and compare
   them with statistical and econometric methods that marketing researchers
   traditionally use. We argue that machine learning methods can process
   large-scale and unstructured data, and have flexible model structures
   that yield strong predictive performance. Meanwhile, such methods may
   lack model transparency and interpretability. We discuss salient
   AI-driven industry trends and practices, and review the still nascent
   academic marketing literature which uses machine learning methods. More
   importantly, we present a unified conceptual framework and a
   multi-faceted research agenda. From five key aspects of empirical
   marketing research: method, data, usage, issue, and theory, we propose a
   number of research priorities, including extending machine learning
   methods and using them as core components in marketing research, using
   the methods to extract insights from large-scale unstructured, tracking,
   and network data, using them in transparent fashions for descriptive,
   causal, and prescriptive analyses, using them to map out customer
   purchase journeys and develop decision-support capabilities, and
   connecting the methods to human insights and marketing theories.
   Opportunities abound for machine learning methods in marketing, and we
   hope our multi-faceted research agenda will inspire more work in this
   exciting area. (c) 2020 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.ijresmar.2020.04.005},
ISSN = {0167-8116},
EISSN = {1873-8001},
Unique-ID = {WOS:000582339000004},
}

@article{ WOS:000764828500048,
Author = {Ren, Zijie and Wan, Jiafu and Deng, Pan},
Title = {Machine-Learning-Driven Digital Twin for Lifecycle Management of Complex
   Equipment},
Journal = {IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING},
Year = {2022},
Volume = {10},
Number = {1},
Pages = {9-22},
Month = {JAN 1},
Abstract = {The full life cycle management of complex equipment is considered
   fundamental to the intelligent transformation and upgrading of the
   modern manufacturing industry. Digital twin technology and machine
   learning have been emerging technologies in recent years. The
   application of these two technologies in the full life cycle management
   of complex equipment can make each stage of the life cycle more
   responsive, predictable, and adaptable. This paper first proposes a
   technical system that embeds machine learning modules into digital
   twins. Next, on this basis, a full life cycle digital twin for complex
   equipment is constructed, and joint application of sub-models and
   machine learning is explored. Then, the application of a combination of
   the digital twin in maintenance with machine learning in predictive
   maintenance of diesel locomotives is presented. The effectiveness of the
   proposed management method is verified by experiments. The abnormal axle
   temperature can be alarmed about one week in advance. Lastly. possible
   application advantages of the combination of digital twin and machine
   learning in addressing future research direction in this field are
   introduced.},
DOI = {10.1109/TETC.2022.3143346},
ISSN = {2168-6750},
ResearcherID-Numbers = {Wan, Jiafu/I-3059-2016},
Unique-ID = {WOS:000764828500048},
}

@article{ WOS:000936903600001,
Author = {Sousa, Diego and Du, Rong and da Silva Jr, Jose Mairton Barros and
   Cavalcante, Charles Casimiro and Fischione, Carlo},
Title = {Leakage detection in water distribution networks using machine-learning
   strategies},
Journal = {WATER SUPPLY},
Year = {2023},
Volume = {23},
Number = {3},
Pages = {1115-1126},
Month = {MAR},
Abstract = {This work proposes a reliable leakage detection methodology for water
   distribution networks (WDNs) using machine-learning strategies. Our
   solution aims at detecting leakage in WDNs using efficient
   machine-learning strategies. We analyze pressure measurements from pumps
   in district metered areas (DMAs) in Stockholm, Sweden, where we consider
   a residential DMA of the water distribution network. Our proposed
   methodology uses learning strategies from unsupervised learning (K-means
   and cluster validation techniques), and supervised learning (learning
   vector quantization algorithms). The learning strategies we propose have
   low complexity, and the numerical experiments show the potential of
   using machine-learning strategies in leakage detection for monitored
   WDNs. Specifically, our experiments show that the proposed learning
   strategies are able to obtain correct classification rates up to
   93.98\%.},
DOI = {10.2166/ws.2023.054},
EarlyAccessDate = {FEB 2023},
ISSN = {1606-9749},
EISSN = {1607-0798},
ResearcherID-Numbers = {Cavalcante, Charles/C-3199-2008
   },
ORCID-Numbers = {Fischione, Carlo/0000-0001-9810-3478
   Cavalcante, Charles/0000-0002-4198-4064
   Perdigao Sousa, Diego/0000-0001-6408-2760
   Du, Rong/0000-0002-1934-9208
   Barros da Silva Junior, Jose Mairton/0000-0002-4503-4242},
Unique-ID = {WOS:000936903600001},
}

@article{ WOS:001087877900001,
Author = {Yao, Jianping and Tran, Son N. and Sawyer, Samantha and Garg, Saurabh},
Title = {Machine learning for leaf disease classification: data, techniques and
   applications},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2023},
Volume = {56},
Number = {SUPPL3, 3},
Pages = {S3571-S3616},
Month = {DEC},
Abstract = {The growing demand for sustainable development brings a series of
   information technologies to help agriculture production. Especially, the
   emergence of machine learning applications, a branch of artificial
   intelligence, has shown multiple breakthroughs which can enhance and
   revolutionize plant pathology approaches. In recent years, machine
   learning has been adopted for leaf disease classification in both
   academic research and industrial applications. Therefore, it is
   enormously beneficial for researchers, engineers, managers, and
   entrepreneurs to have a comprehensive view about the recent development
   of machine learning technologies and applications for leaf disease
   detection. This study will provide a survey in different aspects of the
   topic including data, techniques, and applications. The paper will start
   with publicly available datasets. After that, we summarize common
   machine learning techniques, including traditional (shallow) learning,
   deep learning, and augmented learning. Finally, we discuss related
   applications. This paper would provide useful resources for future study
   and application of machine learning for smart agriculture in general and
   leaf disease classification in particular.},
DOI = {10.1007/s10462-023-10610-4},
EarlyAccessDate = {OCT 2023},
ISSN = {0269-2821},
EISSN = {1573-7462},
ORCID-Numbers = {Yao, Jianping/0000-0003-1066-2377},
Unique-ID = {WOS:001087877900001},
}

@article{ WOS:001306820800003,
Author = {Anand, Pritam and Bharti, Amisha and Rastogi, Reshma},
Title = {Time efficient variants of Twin Extreme Learning Machine},
Journal = {INTELLIGENT SYSTEMS WITH APPLICATIONS},
Year = {2023},
Volume = {17},
Month = {FEB},
Abstract = {Twin Extreme Learning Machine models can obtain better generalization
   ability than the standard Extreme Learning Machine model. But, they
   require to solve a pair of quadratic programming problems for this. It
   makes them more complex and computationally expensive than the standard
   Extreme Learning Machine model. In this paper, we propose two novel
   time-efficient formulations of the Twin Extreme Learning Machine, which
   only require the solution of systems of linear equations for obtaining
   the final classifier. In this sense, they can combine the benefits of
   the Twin Support Vector Machine and standard Extreme Learning Machine in
   the true sense. We term our first formulation as `Least Squared Twin
   Extreme Learning Machine'. It minimizes the L 2-norm of error variables
   in its optimization problem. Our second formulation `Weighted Linear
   loss Twin Extreme Learning Machine' uses the weighted linear loss
   function for calculating the empirical error, which makes it insensitive
   towards outliers. Numerical results obtained with multiple benchmark
   datasets show that proposed formulations are time efficient with better
   generalization ability. Further, we have used the proposed formulations
   in the detection of phishing websites and shown that they are much more
   effective in the detection of phishing websites than other Extreme
   Learning Machine models.},
DOI = {10.1016/j.iswa.2022.200169},
Article-Number = {200169},
EISSN = {2667-3053},
ORCID-Numbers = {Anand, Pritam/0000-0001-9524-745X},
Unique-ID = {WOS:001306820800003},
}

@article{ WOS:001158492200001,
Author = {Liu, Chen-Xu and Yu, Gui-Lan and Liu, Zhanli},
Title = {Machine learning models in phononic metamaterials},
Journal = {CURRENT OPINION IN SOLID STATE \& MATERIALS SCIENCE},
Year = {2024},
Volume = {28},
Month = {FEB},
Abstract = {Machine learning opens up a new avenue for advancing the development of
   phononic crystals and elastic metamaterials. Numerous learning models
   have been employed and developed to address various challenges in the
   field of phononic metamaterials. Here, we provide an overview of
   mainstream machine learning models applied to phononic metamaterials,
   discuss their capabilities as well as limitations, and explore potential
   directions for future research.},
DOI = {10.1016/j.cossms.2023.101133},
EarlyAccessDate = {DEC 2023},
Article-Number = {101133},
ISSN = {1359-0286},
EISSN = {1879-0348},
ORCID-Numbers = {, GUILAN/0000-0002-1123-3518},
Unique-ID = {WOS:001158492200001},
}

@article{ WOS:000339429200011,
Author = {Bharathi, A. and Anandakumar, K.},
Title = {Investigation on Cancer Classification Using Machine Learning Approaches},
Journal = {JOURNAL OF BIOMATERIALS AND TISSUE ENGINEERING},
Year = {2014},
Volume = {4},
Number = {6},
Pages = {492-500},
Month = {JUN},
Abstract = {The objective of this paper is to develop an effective machine learning
   approaches for cancer classification, which could provide reliable
   cancer classification with better accuracy. The work comprises of two
   steps. In the first step, using Analysis of Variance (ANOVA) ranking
   scheme to choose the important genes. The second step involves the
   classification task using an efficient classifier. In this paper we use
   the three efficient machine learning classifiers such as Fast Support
   Vector Machine Learning (FSVML), Fast Extreme Learning Machine Learning
   (FELML) and Relevance Vector Machine Learning (RVMMLML). The
   investigational values are computed using three datasets namely
   Lymphoma, Leukemia and SRBCT. The results are interpreted in terms of
   Testing Accuracy and Training Time. From the investigational results, it
   is observed that the proposed RVMMLML machine learning approach gives
   better testing accuracy results for all the datasets considered.},
DOI = {10.1166/jbt.2014.1186},
ISSN = {2157-9083},
EISSN = {2157-9091},
Unique-ID = {WOS:000339429200011},
}

@article{ WOS:000327552100003,
Author = {Sebag, Michele},
Title = {A tour of machine learning: An AI perspective},
Journal = {AI COMMUNICATIONS},
Year = {2014},
Volume = {27},
Number = {1},
Pages = {11-23},
Abstract = {Machine Learning has been at the core of Artificial Intelligence since
   its inception. Many promises have been held, if one is to consider that
   Google is a living demonstration of AI. This paper presents a historical
   perspective on Machine Learning, describing how the emphasis was
   gradually shifted from logical to statistical induction, from induction
   to optimization, from the search of hypotheses to the search of
   representations. The paper concludes with a discussion about the new
   frontier of Machine Learning.},
DOI = {10.3233/AIC-130580},
ISSN = {0921-7126},
EISSN = {1875-8452},
Unique-ID = {WOS:000327552100003},
}

@article{ WOS:000631777400001,
Author = {Zhang, Lei and Li, Ning},
Title = {Material analysis and big data monitoring of sports training equipment
   based on machine learning algorithm},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2022},
Volume = {34},
Number = {4, SI},
Pages = {2749-2763},
Month = {FEB},
Abstract = {Different machine learning algorithms predict the application effect of
   perovskite materials in sports training equipment. The sensitivity to
   material data is different on different ranges of data sets. Therefore,
   the algorithm needs to be selected according to specific material data
   samples. This study compares the prediction performance of neural
   network prediction algorithm (NN), genetic algorithm, and support vector
   machine-based machine learning algorithm (SVM) and uses statistical
   analysis to perform data analysis and draw corresponding curves.
   Moreover, this study uses a single perovskite material to verify the
   algorithm performance. In addition, based on the real data, the three
   machine learning algorithms of this study are applied to the related
   performance prediction, and the comparative analysis method is used to
   analyze the prediction performance of the machine learning algorithm.
   Through data analysis and chart analysis, we can see that machine
   learning algorithms have a certain effect in the application prediction
   of perovskite materials in sports training equipment. Among the three
   machine learning algorithms selected in this study, the performance of
   the machine learning algorithm based on support vector machine in all
   aspects is more excellent.},
DOI = {10.1007/s00521-021-05852-8},
EarlyAccessDate = {MAR 2021},
ISSN = {0941-0643},
EISSN = {1433-3058},
Unique-ID = {WOS:000631777400001},
}

@article{ WOS:000412100700016,
Author = {Niu, Haiqiang and Reeves, Emma and Gerstoft, Peter},
Title = {Source localization in an ocean waveguide using supervised machine
   learning},
Journal = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
Year = {2017},
Volume = {142},
Number = {3},
Pages = {1176-1188},
Month = {SEP},
Abstract = {Source localization in ocean acoustics is posed as a machine learning
   problem in which data-driven methods learn source ranges directly from
   observed acoustic data. The pressure received by a vertical linear array
   is preprocessed by constructing a normalized sample covariance matrix
   and used as the input for three machine learning methods: feed-forward
   neural networks (FNN), support vector machines (SVM), and random forests
   (RF). The range estimation problem is solved both as a classification
   problem and as a regression problem by these three machine learning
   algorithms. The results of range estimation for the Noise09 experiment
   are compared for FNN, SVM, RF, and conventional matched-field processing
   and demonstrate the potential of machine learning for underwater source
   localization. (C) 2017 Acoustical Society of America.},
DOI = {10.1121/1.5000165},
ISSN = {0001-4966},
EISSN = {1520-8524},
ResearcherID-Numbers = {Gerstoft, Peter/B-2842-2009
   },
ORCID-Numbers = {Niu, Haiqiang/0000-0001-7265-6111
   Gerstoft, Peter/0000-0002-0471-062X},
Unique-ID = {WOS:000412100700016},
}

@article{ WOS:000922362000001,
Author = {Zou, Shirong and Wu, Zhoupeng},
Title = {A narrative review of the application of machine learning in venous
   thromboembolism},
Journal = {VASCULAR},
Year = {2024},
Volume = {32},
Number = {3},
Pages = {698-704},
Month = {JUN},
Abstract = {Objective To summarize the current research progress of machine learning
   and venous thromboembolism. Methods The literature on risk factors,
   diagnosis, prevention and prognosis of machine learning and venous
   thromboembolism in recent years was reviewed. Results Machine learning
   is the future of biomedical research, personalized medicine, and
   computer-aided diagnosis, and will significantly promote the development
   of biomedical research and healthcare. However, many medical
   professionals are not familiar with it. In this review, we will
   introduce several commonly used machine learning algorithms in medicine,
   discuss the application of machine learning in venous thromboembolism,
   and reveal the challenges and opportunities of machine learning in
   medicine. Conclusion The incidence of venous thromboembolism is high,
   the diagnostic measures are diverse, and it is necessary to classify and
   treat machine learning, and machine learning as a research tool, it is
   more necessary to strengthen the special research of venous
   thromboembolism and machine learning.},
DOI = {10.1177/17085381231153216},
EarlyAccessDate = {JAN 2023},
ISSN = {1708-5381},
EISSN = {1708-539X},
ORCID-Numbers = {wu, zhou peng/0000-0002-8704-4908},
Unique-ID = {WOS:000922362000001},
}

@article{ WOS:000922762600001,
Author = {Martin, Tyler B. and Audus, Debra J.},
Title = {Emerging Trends in Machine Learning: A Polymer Perspective},
Journal = {ACS POLYMERS AU},
Year = {2023},
Volume = {3},
Number = {3},
Pages = {239-258},
Month = {JAN 18},
Abstract = {In the last five years, there has been tremendous growth in machine
   learning and artificial intelligence as applied to polymer science.
   Here, we highlight the unique challenges presented by polymers and how
   the field is addressing them. We focus on emerging trends with an
   emphasis on topics that have received less attention in the review
   literature. Finally, we provide an outlook for the field, outline
   important growth areas in machine learning and artificial intelligence
   for polymer science and discuss important advances from the greater
   material science community.},
DOI = {10.1021/acspolymersau.2c00053},
EarlyAccessDate = {JAN 2023},
EISSN = {2694-2453},
ORCID-Numbers = {Martin, Tyler/0000-0001-7253-6507
   Audus, Debra/0000-0002-5937-7721},
Unique-ID = {WOS:000922762600001},
}

@article{ WOS:000447385100002,
Author = {Li, Shuang and Song, Shiji and Wan, Yihe},
Title = {Laplacian twin extreme learning machine for semi-supervised
   classification},
Journal = {NEUROCOMPUTING},
Year = {2018},
Volume = {321},
Pages = {17-27},
Month = {DEC 10},
Abstract = {Twin extreme learning machine (TELM) is an efficient and effective
   method for pattern classification, based on widely known extreme
   learning machine (ELM). However, TELM is mainly used to deal with
   supervised learning problems. In this paper, we extend TELM to handle
   semi-supervised learning problems and propose a novel Laplacian twin
   extreme learning machine (LapTELM), which simultaneously trains two
   related and paired semi-supervised ELMs with two nonparallel separating
   planes for the final classification. The proposed method exploits the
   geometry structure property of the unlabeled samples and incorporates it
   as a manifold regularization term. This allows LapTELM to reap the
   benefits of fully exploring the plentiful unlabeled samples while
   retaining the learning ability and efficiency of TELM. Moreover, the
   paper shows that semi-supervised and supervised TELM can form an unified
   learning framework. Compared with several mainstream semi-supervised
   learning methods, the experimental results on the synthetic and several
   real-world data sets verify the effectiveness and efficiency of LapTELM.
   (c) 2018 Published by Elsevier B.V.},
DOI = {10.1016/j.neucom.2018.08.028},
ISSN = {0925-2312},
EISSN = {1872-8286},
Unique-ID = {WOS:000447385100002},
}

@article{ WOS:001273496200001,
Author = {Adhinata, Faisal Dharma and Wahyono and Sumiharto, Raden},
Title = {A comprehensive survey on weed and crop classi fi cation using machine
   learning and deep learning},
Journal = {ARTIFICIAL INTELLIGENCE IN AGRICULTURE},
Year = {2024},
Volume = {13},
Pages = {45-63},
Month = {SEP},
Abstract = {Machine learning and deep learning are subsets of Artificial
   Intelligence that have revolutionized object detection and
   classification in images or videos. This technology plays a crucial role
   in facilitating the transition from conventional to precision
   agriculture, particularly in the context of weed control. Precision
   agriculture, which previously relied on manual efforts, has now embraced
   the use of smart devices for more efficient weed detection. However,
   several challenges are associated with weed detection, including the
   visual similarity between weed and crop, occlusion and lighting effects,
   as well as the need for early-stage weed control. Therefore, this study
   aimed to provide a comprehensive review of the application of both
   traditional machine learning and deep learning, as well as the
   combination of the two methods, for weed detection across different crop
   fields. The results of this review show the advantages and disadvantages
   of using machine learning and deep learning. Generally, deep learning
   produced superior accuracy compared to machine learning under various
   conditions. Machine learning required the selection of the right
   combination of features to achieve high accuracy in classifying weed and
   crop, particularly under conditions consisting of lighting and early
   growth effects. Moreover, a precise segmentation stage would be required
   in cases of occlusion. Machine learning had the advantage of achieving
   real-time processing by producing smaller models than deep learning,
   thereby eliminating the need for additional GPUs. However, the
   development of GPU technology is currently rapid, so researchers are
   more often using deep learning for more accurate weed identification.
   (c) 2023 The Authors. Publishing services by Elsevier B.V. on behalf of
   KeAi Communications Co., Ltd. This is an open access article under the
   CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
DOI = {10.1016/j.aiia.2024.06.005},
EarlyAccessDate = {JUN 2024},
EISSN = {2589-7217},
ResearcherID-Numbers = {Adhinata, Faisal Dharma/AFB-4217-2022},
Unique-ID = {WOS:001273496200001},
}

@article{ WOS:000708445300016,
Author = {Hosseini, Mohammad-Parsa and Hosseini, Amin and Ahi, Kiarash},
Title = {A Review on Machine Learning for EEG Signal Processing in Bioengineering},
Journal = {IEEE REVIEWS IN BIOMEDICAL ENGINEERING},
Year = {2021},
Volume = {14},
Pages = {204-218},
Abstract = {Electroencephalography (EEG) has been a staple method for identifying
   certain health conditions in patients since its discovery. Due to the
   many different types of classifiers available to use, the analysis
   methods are also equally numerous. In this review, we will be examining
   specifically machine learning methods that have been developed for EEG
   analysis with bioengineering applications. We reviewed literature from
   1988 to 2018 to capture previous and current classification methods for
   EEG in multiple applications. From this information, we are able to
   determine the overall effectiveness of each machine learning method as
   well as the key characteristics. We have found that all the primary
   methods used in machine learning have been applied in some form in EEG
   classification. This ranges from Naive-Bayes to Decision Tree/Random
   Forest, to Support Vector Machine (SVM). Supervised learning methods are
   on average of higher accuracy than their unsupervised counterparts. This
   includes SVM and KNN. While each of the methods individually is limited
   in their accuracy in their respective applications, there is hope that
   the combination of methods when implemented properly has a higher
   overall classification accuracy. This paper provides a comprehensive
   overview of Machine Learning applications used in EEG analysis. It also
   gives an overview of each of the methods and general applications that
   each is best suited to.},
DOI = {10.1109/RBME.2020.2969915},
ISSN = {1937-3333},
EISSN = {1941-1189},
ResearcherID-Numbers = {Seno, Seyed/AAC-9640-2020
   },
ORCID-Numbers = {Hosseini, Mohammad-Parsa/0000-0002-0611-7520},
Unique-ID = {WOS:000708445300016},
}

@article{ WOS:001181945300001,
Author = {Tian, Dingcheng and Chen, Weihao and Xu, Dechao and Xu, Lisheng and Xu,
   Gang and Guo, Yaochen and Yao, Yudong},
Title = {A review of traditional Chinese medicine diagnosis using machine
   learning: Inspection, auscultation-olfaction, inquiry, and palpation},
Journal = {COMPUTERS IN BIOLOGY AND MEDICINE},
Year = {2024},
Volume = {170},
Month = {MAR},
Abstract = {Traditional Chinese medicine (TCM) is an essential part of the Chinese
   medical system and is recognized by the World Health Organization as an
   important alternative medicine. As an important part of TCM, TCM
   diagnosis is a method to understand a patient's illness, analyze its
   state, and identify syndromes. In the longterm clinical diagnosis
   practice of TCM, four fundamental and effective diagnostic methods of
   inspection, auscultation-olfaction, inquiry, and palpation (IAOIP) have
   been formed. However, the diagnostic information in TCM is diverse, and
   the diagnostic process depends on doctors' experience, which is subject
   to a highlevel subjectivity. At present, the research on the automated
   diagnosis of TCM based on machine learning is booming. Machine learning,
   which includes deep learning, is an essential part of artificial
   intelligence (AI), which provides new ideas for the objective and
   AI-related research of TCM. This paper aims to review and summarize the
   current research status of machine learning in TCM diagnosis. First, we
   review some key factors for the application of machine learning in TCM
   diagnosis, including data, data preprocessing, machine learning models,
   and evaluation metrics. Second, we review and summarize the research and
   applications of machine learning methods in TCM IAOIP and the synthesis
   of the four diagnostic methods. Finally, we discuss the challenges and
   research directions of using machine learning methods for TCM diagnosis.},
DOI = {10.1016/j.compbiomed.2024.108074},
EarlyAccessDate = {FEB 2024},
Article-Number = {108074},
ISSN = {0010-4825},
EISSN = {1879-0534},
ResearcherID-Numbers = {XU, Lisheng/C-2974-2008
   Xu, Gang/IVU-7883-2023},
Unique-ID = {WOS:001181945300001},
}

@article{ WOS:001116894500001,
Author = {Pandey, Shyambabu and Basisth, Nihar Jyoti and Sachan, Tushar and
   Kumari, Neha and Pakray, Partha},
Title = {Quantum machine learning for natural language processing application},
Journal = {PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS},
Year = {2023},
Volume = {627},
Month = {OCT 1},
Abstract = {Quantum computing is a speedily emerging area that applies quantum
   mechanics properties to solve complex problems that are difficult for
   classical computing. Machine learning is a sub-field of artificial
   intelligence which makes computers learn patterns from experiences. Due
   to the exponential growth of data, machine learning algorithms may be
   insufficient for big data, whereas on other side quantum computing can
   do fast computing. A combination of quantum computing and machine
   learning gave rise to a new field known as quantum machine learning.
   Quantum machine learning algorithms take advantage of the fast
   processing of quantum computing and show speedup compared to their
   classical counterpart. Natural language processing is another area of
   artificial intelligence that enables the computer to understand human
   languages. Now, researchers are trying to take advantage of quantum
   machine learning speedup in natural language processing applications. In
   this paper, first, we discuss the path from quantum computing to quantum
   machine learning. Then we review the state of the art of quantum machine
   learning for natural language processing applications. We also provide
   classical and quantum-based long short-term memory for parts of speech
   tagging on social media code mixed language. Our experiment shows that
   quantum-based long short-term memory performance is better than
   classical long short-term memory for parts of speech tagging of
   code-mixed datasets.(c) 2023 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.physa.2023.129123},
EarlyAccessDate = {AUG 2023},
Article-Number = {129123},
ISSN = {0378-4371},
EISSN = {1873-2119},
ResearcherID-Numbers = {Kumari, Neha/OHR-5003-2025
   Pakray, Partha/H-7805-2012},
ORCID-Numbers = {Pakray, Partha/0000-0003-3834-5154},
Unique-ID = {WOS:001116894500001},
}

@article{ WOS:000557459300020,
Author = {Sendak, Mark P. and Ratliff, William and Sarro, Dina and Alderton,
   Elizabeth and Futoma, Joseph and Gao, Michael and Nichols, Marshall and
   Revoir, Mike and Yashar, Faraz and Miller, Corinne and Kester, Kelly and
   Sandhu, Sahil and Corey, Kristin and Brajer, Nathan and Tan, Christelle
   and Lin, Anthony and Brown, Tres and Engelbosch, Susan and Anstrom,
   Kevin and Elish, Madeleine Clare and Heller, Katherine and Donohoe,
   Rebecca and Theiling, Jason and Poon, Eric and Balu, Suresh and Bedoya,
   Armando and O'Brien, Cara},
Title = {Real-World Integration of a Sepsis Deep Learning Technology Into Routine
   Clinical Care: Implementation Study},
Journal = {JMIR MEDICAL INFORMATICS},
Year = {2020},
Volume = {8},
Number = {7},
Month = {JUL},
Abstract = {Background: Successful integrations of machine learning into routine
   clinical care are exceedingly rare, and barriers to its adoption are
   poorly characterized in the literature.
   Objective: This study aims to report a quality improvement effort to
   integrate a deep learning sepsis detection and management platform,
   Sepsis Watch, into routine clinical care.
   Methods: In 2016, a multidisciplinary team consisting of statisticians,
   data scientists, data engineers, and clinicians was assembled by the
   leadership of an academic health system to radically improve the
   detection and treatment of sepsis. This report of the quality
   improvement effort follows the learning health system framework to
   describe the problem assessment, design, development, implementation,
   and evaluation plan of Sepsis Watch.
   Results: Sepsis Watch was successfully integrated into routine clinical
   care and reshaped how local machine learning projects are executed.
   Frontline clinical staff were highly engaged in the design and
   development of the workflow, machine learning model, and application.
   Novel machine learning methods were developed to detect sepsis early,
   and implementation of the model required robust infrastructure.
   Significant investment was required to align stakeholders, develop
   trusting relationships, define roles and responsibilities, and to train
   frontline staff, leading to the establishment of 3 partnerships with
   internal and external research groups to evaluate Sepsis Watch.
   Conclusions: Machine learning models are commonly developed to enhance
   clinical decision making, but successful integrations of machine
   learning into routine clinical care are rare. Although there is no
   playbook for integrating deep learning into clinical care, learnings
   from the Sepsis Watch integration can inform efforts to develop machine
   learning technologies at other health care delivery systems.},
DOI = {10.2196/15182},
Article-Number = {e15182},
EISSN = {2291-9694},
ResearcherID-Numbers = {Bedoya, Armando/CAA-2094-2022
   },
ORCID-Numbers = {Ratliff, William/0000-0003-4015-3805
   Sarro, Dina/0000-0001-9749-1317
   Sendak, Mark/0000-0001-5828-4497
   Corey, Kristin/0000-0002-7158-1215
   Gao, Michael/0000-0003-0534-4403
   Miller, Corinne/0000-0001-9020-9825
   Bedoya, Armando/0000-0001-6496-7024
   Sandhu, Sahil/0000-0001-6458-5253
   Poon, Eric/0000-0002-7251-5842
   Revoir, Mike/0000-0003-1834-9078},
Unique-ID = {WOS:000557459300020},
}

@article{ WOS:000656708300001,
Author = {Young, Amber Grace and Majchrzak, Ann and Kane, Gerald C.},
Title = {Organizing workers and machine learning tools for a less oppressive
   workplace},
Journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT},
Year = {2021},
Volume = {59},
Month = {AUG},
Abstract = {Machine learning tools are increasingly infiltrating everyday work life
   with implications for workers. By looking at machine learning tools as
   part of a sociotechnical system, we explore how machine learning tools
   enforce oppression of workers. We theorize, normatively, that with
   reorganizing processes in place, oppressive characteristics could be
   converted to emancipatory characteristics. Drawing on Paulo Freire's
   critical theory of emancipatory pedagogy, we outline similarities
   between the characteristics Freire saw in oppressive societies and the
   characteristics of currently designed partnerships between humans and
   machine learning tools. Freire's theory offers a way forward in
   reorganizing humans and machine learning tools in the workplace. Rather
   than advocating human control or the decoupling of workers and machines,
   we follow Freire's theory in proposing four processes for emancipatory
   organizing of human and machine learning partnership: 1) awakening of a
   critical consciousness, 2) enabling role freedom, 3) instituting
   incentives and sanctions for accountability, and 4) identifying
   alternative emancipatory futures. Theoretical and practical implications
   of this emancipatory organizing theory are drawn.},
DOI = {10.1016/j.ijinfomgt.2021.102353},
EarlyAccessDate = {APR 2021},
Article-Number = {102353},
ISSN = {0268-4012},
EISSN = {1873-4707},
Unique-ID = {WOS:000656708300001},
}

@article{ WOS:001139837400001,
Author = {El Mestari, Soumia Zohra and Lenzini, Gabriele and Demirci, Huseyin},
Title = {Preserving data privacy in machine learning systems},
Journal = {COMPUTERS \& SECURITY},
Year = {2024},
Volume = {137},
Month = {FEB},
Abstract = {The wide adoption of Machine Learning to solve a large set of real-life
   problems came with the need to collect and process large volumes of
   data, some of which are considered personal and sensitive, raising
   serious concerns about data protection. Privacy-enhancing technologies
   (PETs) are often indicated as a solution to protect personal data and to
   achieve a general trustworthiness as required by current EU regulations
   on data protection and AI. However, an off-the-shelf application of PETs
   is insufficient to ensure a high-quality of data protection, which one
   needs to understand. This work systematically discusses the risks
   against data protection in modern Machine Learning systems taking the
   original perspective of the data owners, who are those who hold the
   various data sets, data models, or both, throughout the machine learning
   life cycle and considering the different Machine Learning architectures.
   It argues that the origin of the threats, the risks against the data,
   and the level of protection offered by PETs depend on the data
   processing phase, the role of the parties involved, and the architecture
   where the machine learning systems are deployed. By offering a framework
   in which to discuss privacy and confidentiality risks for data owners
   and by identifying and assessing privacy-preserving countermeasures for
   machine learning, this work could facilitate the discussion about
   compliance with EU regulations and directives.We discuss current
   challenges and research questions that are still unsolved in the field.
   In this respect, this paper provides researchers and developers working
   on machine learning with a comprehensive body of knowledge to let them
   advance in the science of data protection in machine learning field as
   well as in closely related fields such as Artificial Intelligence.},
DOI = {10.1016/j.cose.2023.103605},
EarlyAccessDate = {DEC 2023},
Article-Number = {103605},
ISSN = {0167-4048},
EISSN = {1872-6208},
ORCID-Numbers = {EL MESTARI, Soumia Zohra/0000-0002-1399-605X},
Unique-ID = {WOS:001139837400001},
}

@article{ WOS:001013949600001,
Author = {Pateras, Joseph and Rana, Pratip and Ghosh, Preetam},
Title = {A Taxonomic Survey of Physics-Informed Machine Learning},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {12},
Month = {JUN},
Abstract = {Physics-informed machine learning (PIML) refers to the emerging area of
   extracting physically relevant solutions to complex multiscale modeling
   problems lacking sufficient quantity and veracity of data with learning
   models informed by physically relevant prior information. This work
   discusses the recent critical advancements in the PIML domain. Novel
   methods and applications of domain decomposition in physics-informed
   neural networks (PINNs) in particular are highlighted. Additionally, we
   explore recent works toward utilizing neural operator learning to intuit
   relationships in physics systems traditionally modeled by sets of
   complex governing equations and solved with expensive differentiation
   techniques. Finally, expansive applications of traditional
   physics-informed machine learning and potential limitations are
   discussed. In addition to summarizing recent work, we propose a novel
   taxonomic structure to catalog physics-informed machine learning based
   on how the physics-information is derived and injected into the machine
   learning process. The taxonomy assumes the explicit objectives of
   facilitating interdisciplinary collaboration in methodology, thereby
   promoting a wider characterization of what types of physics problems are
   served by the physics-informed learning machines and assisting in
   identifying suitable targets for future work. To summarize, the major
   twofold goal of this work is to summarize recent advancements and
   introduce a taxonomic catalog for applications of physics-informed
   machine learning.},
DOI = {10.3390/app13126892},
Article-Number = {6892},
EISSN = {2076-3417},
ORCID-Numbers = {Rana, Pratip/0000-0001-9199-2479
   Ghosh, Preetam/0000-0003-3880-5886},
Unique-ID = {WOS:001013949600001},
}

@article{ WOS:001155835100001,
Author = {Bjelic, Milos and Brkovic, Bogdan and Zarkovic, Mileta and Miljkovic,
   Tatjana},
Title = {Machine learning for power transformer SFRA based fault detection},
Journal = {INTERNATIONAL JOURNAL OF ELECTRICAL POWER \& ENERGY SYSTEMS},
Year = {2024},
Volume = {156},
Month = {FEB},
Abstract = {This paper presents machine learning methods for health assessment of
   power transformer based on sweep frequency response analysis. The paper
   presents an overview of monitoring and diagnostics based on statistical
   Sweep Frequency Response Analysis (SFRA) based indicators that are used
   to evaluate the state of the power transformer. Experimental data
   obtained from power transformers with internal short-circuit faults is
   used as a database for applying machine learning. Machine learning is
   implemented to achieve more precise asset management and condition-based
   maintenance. Unsupervised machine learning was applied through the
   k-means cluster method for classifying and dividing the examined power
   transformer state into groups with similar state and probability of
   failure. Artificial neural network (ANN) and Adaptive Neuro Fuzzy
   Inference System (ANFIS) as part of supervised machine learning are
   created in order to detect fault severity in tested power transformers
   of different lifetime. The presented machine learning methods can be
   used to improve health assessment of power transformers.},
DOI = {10.1016/j.ijepes.2023.109779},
EarlyAccessDate = {JAN 2024},
Article-Number = {109779},
ISSN = {0142-0615},
EISSN = {1879-3517},
Unique-ID = {WOS:001155835100001},
}

@article{ WOS:001415042200001,
Author = {Guo, Pengwei and Moghaddas, Seyed A. and Liu, Yiming and Meng, Weina and
   Li, Victor C. and Bao, Yi},
Title = {Applications of machine learning methods for design and characterization
   of high-performance fiber-reinforced cementitious composite (HPFRCC): a
   review},
Journal = {JOURNAL OF SUSTAINABLE CEMENT-BASED MATERIALS},
Year = {2025},
Volume = {14},
Number = {9, SI},
Pages = {1726-1749},
Month = {SEP 2},
Abstract = {High-Performance Fiber-Reinforced Cementitious Composite (HPFRCC)
   represents a family of advanced composite materials with remarkable
   mechanical properties and durability, but their design and
   characterization tasks involve unique challenges. Recently, advancements
   in machine learning techniques have offered new opportunities. This
   paper reviews the application of machine learning techniques in the
   design and characterization of HPFRCC. The application of machine
   learning to the design of HPFRCC is reviewed based on a
   prediction-optimization framework, and the steps for property prediction
   and material design considering fresh properties, cracks, and
   microstructures are elaborated. The latest development of
   knowledge-guided machine learning approach is discussed. The application
   of machine learning to the characterization of HPFRCC is reviewed, and
   the computer vision and deep learning techniques for characterizing
   HPFRCC are elaborated. The challenges and opportunities for the
   applications of machine learning methods are discussed, aiming to
   facilitate applications of machine learning techniques for HPFRCC.},
DOI = {10.1080/21650373.2025.2462183},
EarlyAccessDate = {FEB 2025},
ISSN = {2165-0373},
EISSN = {2165-0381},
ResearcherID-Numbers = {Liu, Yiming/IZE-2916-2023
   guo, pengwei/KCY-8383-2024
   Bao, Yi/D-5334-2019},
ORCID-Numbers = {Guo, Pengwei/0000-0002-6985-7278
   },
Unique-ID = {WOS:001415042200001},
}

@article{ WOS:000697563900010,
Author = {Jiang, Huiling and Li, Qing and Jiang, Yong and Shen, GengBiao and
   Sinnott, Richard and Tian, Chen and Xu, Mingwei},
Title = {When machine learning meets congestion control: A survey and comparison},
Journal = {COMPUTER NETWORKS},
Year = {2021},
Volume = {192},
Month = {JUN 19},
Abstract = {Machine learning has seen a significant surge and uptake across many
   diverse applications. The high flexibility, adaptability, and computing
   capabilities it provides extend traditional approaches used in multiple
   fields including network operation and management. Numerous surveys have
   explored machine learning algorithms in the context of networking, such
   as traffic engineering, performance optimization, and network security.
   Many machine learning approaches focus on clustering, classification,
   regression, and reinforcement learning. The innovation of this research,
   and the contribution of this paper lies in the detailed summary and
   comparison of learning-based congestion control approaches. Compared
   with traditional congestion control algorithms which are typically
   rule-based, capabilities to learn from historical experience are highly
   desirable. From the literature, it is observed that reinforcement
   learning is a crucial trend among learning-based congestion control
   algorithms. In this paper, we explore the performance of reinforcement
   learning-based congestion control algorithms and present current
   problems with reinforcement learning-based congestion control
   algorithms. Moreover, we outline challenges and trends related to
   learning-based congestion control algorithms.},
DOI = {10.1016/j.comnet.2021.108033},
EarlyAccessDate = {APR 2021},
Article-Number = {108033},
ISSN = {1389-1286},
EISSN = {1872-7069},
ResearcherID-Numbers = {Jeong, Yongwook/N-7413-2016
   Jiang, Huiling/ABC-6251-2020
   Li, Qing/U-8995-2018},
ORCID-Numbers = {Li, Qing/0000-0002-6071-473X},
Unique-ID = {WOS:000697563900010},
}

@article{ WOS:000397748100040,
Author = {Thrane, Jakob and Wass, Jesper and Piels, Molly and Diniz, Julio C. M.
   and Jones, Rasmus and Zibar, Darko},
Title = {Machine Learning Techniques for Optical Performance Monitoring From
   Directly Detected PDM-QAM Signals},
Journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY},
Year = {2017},
Volume = {35},
Number = {4},
Pages = {868-875},
Month = {FEB 15},
Note = {Optical Fiber Communications Conference and Exhibition (OFC), Anaheim,
   CA, MAR 20-24, 2016},
Organization = {IEEE Commun Soc; IEEE Photon Soc; Opt Soc},
Abstract = {Linear signal processing algorithms areeffective in dealing with linear
   transmission channel and linear signal detection, whereas the nonlinear
   signal processing algorithms, from the machine learning community, are
   effective in dealing with nonlinear transmission channel and nonlinear
   signal detection. In this paper, a brief overview of the various machine
   learning methods and their application in optical communication is
   presented and discussed. Moreover, supervised machine learning methods,
   such as neural networks and support vector machine, are experimentally
   demonstrated for in-band optical signal to noise ratio estimation and
   modulation format classification, respectively. The proposed methods
   accurately evaluate optical signals employing up to 64 quadrature
   amplitude modulation, at 32 Gbd, using only directly detected data.},
DOI = {10.1109/JLT.2016.2590989},
ISSN = {0733-8724},
EISSN = {1558-2213},
ResearcherID-Numbers = {Medeiros Diniz, Julio Cesar/B-7984-2014
   Zibar, Darko/E-8795-2010
   Diniz, Júlio/B-7984-2014},
ORCID-Numbers = {Medeiros Diniz, Julio Cesar/0000-0002-0999-8327
   Zibar, Darko/0000-0003-4182-7488
   },
Unique-ID = {WOS:000397748100040},
}

@article{ WOS:000595291200006,
Author = {Zhang, Lei and He, Mu and Shao, Shaofeng},
Title = {Machine learning for halide perovskite materials},
Journal = {NANO ENERGY},
Year = {2020},
Volume = {78},
Month = {DEC},
Abstract = {Halide perovskite materials serve as excellent candidates for solar cell
   and optoelectronic devices. Recently, the design of the halide
   perovskite materials is greatly facilitated by machine learning
   techniques, which effectively identify suitable halide perovskite
   candidates and unveil hidden relationships by algorithms that mimic the
   human cognitive functions. In this manuscript, we review recent
   progresses on the machine learning studies of the halide perovskite
   materials, including the prediction and understanding of lead-free and
   stable halide perovskite materials. The structural descriptors to
   describe the property and performance of the halide perovskite materials
   are discussed. In addition, the design strategy of the additive species
   for the halide perovskite materials via the machine learning technique
   is provided. Suggestions to further develop the halide perovskite-based
   systems via the machine learning methods in the future are provided.},
DOI = {10.1016/j.nanoen.2020.105380},
Article-Number = {105380},
ISSN = {2211-2855},
EISSN = {2211-3282},
ResearcherID-Numbers = {Zhang, Lei/A-1194-2014
   Zhang, Lei/U-4622-2019},
ORCID-Numbers = {Zhang, Lei/0000-0001-6873-7314
   },
Unique-ID = {WOS:000595291200006},
}

@article{ WOS:000356700900001,
Author = {Ding, Shifei and Zhang, Nan and Xu, Xinzheng and Guo, Lili and Zhang,
   Jian},
Title = {Deep Extreme Learning Machine and Its Application in EEG Classification},
Journal = {MATHEMATICAL PROBLEMS IN ENGINEERING},
Year = {2015},
Volume = {2015},
Abstract = {Recently, deep learning has aroused wide interest in machine learning
   fields. Deep learning is a multilayer perceptron artificial neural
   network algorithm. Deep learning has the advantage of approximating the
   complicated function and alleviating the optimization difficulty
   associated with deep models. Multilayer extreme learning machine (MLELM)
   is a learning algorithm of an artificial neural network which takes
   advantages of deep learning and extreme learning machine. Not only does
   MLELM approximate the complicated function but it also does not need to
   iterate during the training process. We combining with MLELM and extreme
   learning machine with kernel (KELM) put forward deep extreme learning
   machine (DELM) and apply it to EEG classification in this paper. This
   paper focuses on the application of DELM in the classification of the
   visual feedback experiment, using MATLAB and the second brain-computer
   interface (BCI) competition datasets. By simulating and analyzing the
   results of the experiments, effectiveness of the application of DELM in
   EEG classification is confirmed.},
DOI = {10.1155/2015/129021},
Article-Number = {129021},
ISSN = {1024-123X},
EISSN = {1563-5147},
ResearcherID-Numbers = {Zhang, Keyue/GXF-4304-2022},
ORCID-Numbers = {Zhang, Nan/0000-0001-9620-5665
   },
Unique-ID = {WOS:000356700900001},
}

@article{ WOS:000505074700001,
Author = {Matava, Clyde and Pankiv, Evelina and Ahumada, Luis and Weingarten,
   Benjamin and Simpao, Allan},
Title = {Artificial intelligence, machine learning and the pediatric airway},
Journal = {PEDIATRIC ANESTHESIA},
Year = {2020},
Volume = {30},
Number = {3, SI},
Pages = {264-268},
Month = {MAR},
Abstract = {Artificial intelligence and machine learning are rapidly expanding
   fields with increasing relevance in anesthesia and, in particular,
   airway management. The ability of artificial intelligence and machine
   learning algorithms to recognize patterns from large volumes of complex
   data makes them attractive for use in pediatric anesthesia airway
   management. The purpose of this review is to introduce artificial
   intelligence, machine learning, and deep learning to the pediatric
   anesthesiologist. Current evidence and developments in artificial
   intelligence, machine learning, and deep learning relevant to pediatric
   airway management are presented. We critically assess the current
   evidence on the use of artificial intelligence and machine learning in
   the assessment, diagnosis, monitoring, procedure assistance, and
   predicting outcomes during pediatric airway management. Further, we
   discuss the limitations of these technologies and offer areas for
   focused research that may bring pediatric airway management
   anesthesiology into the era of artificial intelligence and machine
   learning.},
DOI = {10.1111/pan.13792},
EarlyAccessDate = {JAN 2020},
ISSN = {1155-5645},
EISSN = {1460-9592},
ResearcherID-Numbers = {Simpao, Allan/V-6946-2018
   Matava, Clyde/AAK-9531-2020
   Ahumada, Luis/IZE-7655-2023},
ORCID-Numbers = {Simpao, Allan/0000-0001-8871-2579
   },
Unique-ID = {WOS:000505074700001},
}

@article{ WOS:001045762300001,
Author = {Imrie, Fergus and Davis, Robert and van der Schaar, Mihaela},
Title = {Multiple stakeholders drive diverse interpretability requirements for
   machine learning in healthcare},
Journal = {NATURE MACHINE INTELLIGENCE},
Year = {2023},
Volume = {5},
Number = {8},
Pages = {824-829},
Month = {AUG},
Abstract = {Limited interpretability and understanding of machine learning methods
   in healthcare hinder their clinical impact. Imrie et al. discuss five
   types of machine learning interpretability. They examine medical
   stakeholders, highlight how interpretability meets their needs and
   emphasize the role of tailored interpretability in linking machine
   learning advancements to clinical impact.
   Applications of machine learning are becoming increasingly common in
   medicine and healthcare, enabling more accurate predictive models.
   However, this often comes at the cost of interpretability, limiting the
   clinical impact of machine learning methods. To realize the potential of
   machine learning in healthcare, it is critical to understand such models
   from the perspective of multiple stakeholders and various angles,
   necessitating different types of explanation. In this Perspective, we
   explore five fundamentally different types of post-hoc machine learning
   interpretability. We highlight the different types of information that
   they provide, and describe when each can be useful. We examine the
   various stakeholders in healthcare, delving into their specific
   objectives, requirements and goals. We discuss how current notions of
   interpretability can help meet these and what is required for each
   stakeholder to make machine learning models clinically impactful.
   Finally, to facilitate adoption, we release an open-source
   interpretability library containing implementations of the different
   types of interpretability, including tools for visualizing and exploring
   the explanations.},
DOI = {10.1038/s42256-023-00698-2},
EarlyAccessDate = {AUG 2023},
EISSN = {2522-5839},
ResearcherID-Numbers = {Imrie, Fergus/GPX-9474-2022
   },
ORCID-Numbers = {Imrie, Fergus/0000-0002-6241-0123},
Unique-ID = {WOS:001045762300001},
}

@article{ WOS:000678606100001,
Author = {Ren, Yali},
Title = {Optimizing Predictive Maintenance With Machine Learning for Reliability
   Improvement},
Journal = {ASCE-ASME JOURNAL OF RISK AND UNCERTAINTY IN ENGINEERING SYSTEMS PART
   B-MECHANICAL ENGINEERING},
Year = {2021},
Volume = {7},
Number = {3},
Month = {SEP 1},
Abstract = {Predictive maintenance, as a form of pro-active maintenance, has
   increasing usage and shows significant superiority over the corrective
   and preventive maintenance. However, conventional methods of predictive
   maintenance have noteworthy limitations in maintenance optimization and
   reliability improvement. In the last two decades, machine learning has
   flourished and overcome many inherent flaws of conventional maintenance
   prediction methods. Meanwhile, machine learning displays unprecedented
   predictive power in maintenance prediction and optimization. This paper
   compares the features of corrective, preventive, and predictive
   maintenance, examines the conventional approaches to predictive
   maintenance, and analyzes their drawbacks. Subsequently, this paper
   explores the driving forces, and advantages of machine learning over
   conventional solutions in predictive maintenance. Specifically, this
   paper reviews popular supervised learning and reinforcement learning
   algorithms and the associated typical applications in predictive
   maintenance. Furthermore, this paper summarizes the four critical steps
   of machine learning applications in maintenance prediction. Finally, the
   author proposes the future researches concerning how to utilize machine
   learning to optimize maintenance prediction and planning, improve
   equipment reliability, and achieve the best possible benefit.},
DOI = {10.1115/1.4049525},
Article-Number = {030801},
ISSN = {2332-9017},
EISSN = {2332-9025},
Unique-ID = {WOS:000678606100001},
}

@article{ WOS:000691220800003,
Author = {Heil, Benjamin J. and Hoffman, Michael M. and Markowetz, Florian and
   Lee, Su-In and Greene, Casey S. and Hicks, Stephanie C.},
Title = {Reproducibility standards for machine learning in the life sciences},
Journal = {NATURE METHODS},
Year = {2021},
Volume = {18},
Number = {10},
Pages = {1132-1135},
Month = {OCT},
Abstract = {To make machine-learning analyses in the life sciences more
   computationally reproducible, we propose standards based on data, model
   and code publication, programming best practices and workflow
   automation. By meeting these standards, the community of researchers
   applying machine-learning methods in the life sciences can ensure that
   their analyses are worthy of trust.},
DOI = {10.1038/s41592-021-01256-7},
EarlyAccessDate = {AUG 2021},
ISSN = {1548-7091},
EISSN = {1548-7105},
ResearcherID-Numbers = {Greene, Casey/L-2057-2015
   Markowetz, Florian/H-6883-2019
   Hoffman, Michael/I-1924-2012
   },
ORCID-Numbers = {Greene, Casey/0000-0001-8713-9213
   Lee, Su-In/0000-0001-5833-5215
   Markowetz, Florian/0000-0002-2784-5308
   Heil, Benjamin/0000-0002-2811-1031
   Hoffman, Michael/0000-0002-4517-1562
   Hicks, Stephanie/0000-0002-7858-0231},
Unique-ID = {WOS:000691220800003},
}

@article{ WOS:000532109400001,
Author = {Ramos, Gonzalo and Meek, Christopher and Simard, Patrice and Suh, Jina
   and Ghorashi, Soroush},
Title = {Interactive machine teaching: a human-centered approach to building
   machine-learned models},
Journal = {HUMAN-COMPUTER INTERACTION},
Year = {2020},
Volume = {35},
Number = {5-6},
Pages = {413-451},
Month = {NOV 1},
Abstract = {Modern systems can augment people's capabilities by using
   machine-learned models to surface intelligent behaviors. Unfortunately,
   building these models remains challenging and beyond the reach of
   non-machine learning experts. We describe interactive machine teaching
   (IMT) and its potential to simplify the creation of machine-learned
   models. One of the key characteristics of IMT is its iterative process
   in which the human-in-the-loop takes the role of a teacher teaching a
   machine how to perform a task. We explore alternative learning theories
   as potential theoretical foundations for IMT, the intrinsic human
   capabilities related to teaching, and how IMT systems might leverage
   them. We argue that IMT processes that enable people to leverage these
   capabilities have a variety of benefits, including making machine
   learning methods accessible to subject-matter experts and the creation
   of semantic and debuggable machine learning (ML) models. We present an
   integrated teaching environment (ITE) that embodies principles from IMT,
   and use it as a design probe to observe how non-ML experts do IMT and as
   the basis of a system that helps us study how to guide teachers. We
   explore and highlight the benefits and challenges of IMT systems. We
   conclude by outlining six research challenges to advance the field of
   IMT.},
DOI = {10.1080/07370024.2020.1734931},
EarlyAccessDate = {MAY 2020},
ISSN = {0737-0024},
EISSN = {1532-7051},
ResearcherID-Numbers = {Ramos, Gonzalo/AAF-9280-2020},
Unique-ID = {WOS:000532109400001},
}

@article{ WOS:000563833500002,
Author = {Fourcade, Marion and Johns, Fleur},
Title = {Loops, ladders and links: the recursivity of social and machine learning},
Journal = {THEORY AND SOCIETY},
Year = {2020},
Volume = {49},
Number = {5-6, SI},
Pages = {803-832},
Month = {OCT},
Abstract = {Machine learning algorithms reshape how people communicate, exchange,
   and associate; how institutions sort them and slot them into social
   positions; and how they experience life, down to the most ordinary and
   intimate aspects. In this article, we draw on examples from the field of
   social media to review the commonalities, interactions, and
   contradictions between the dispositions of people and those of machines
   as they learn from and make sense of each other.},
DOI = {10.1007/s11186-020-09409-x},
EarlyAccessDate = {AUG 2020},
ISSN = {0304-2421},
EISSN = {1573-7853},
ResearcherID-Numbers = {Fourcade, Marion/F-6003-2012
   Johns, Fleur/O-4537-2019},
Unique-ID = {WOS:000563833500002},
}

@article{ WOS:000663500200003,
Author = {Dong, Shi and Wang, Ping and Abbas, Khushnood},
Title = {A survey on deep learning and its applications},
Journal = {COMPUTER SCIENCE REVIEW},
Year = {2021},
Volume = {40},
Month = {MAY},
Abstract = {Deep learning, a branch of machine learning, is a frontier for
   artificial intelligence, aiming to be closer to its primary
   goal-artificial intelligence. This paper mainly adopts the summary and
   the induction methods of deep learning. Firstly, it introduces the
   global development and the current situation of deep learning. Secondly,
   it describes the structural principle, the characteristics, and some
   kinds of classic models of deep learning, such as stacked auto encoder,
   deep belief network, deep Boltzmann machine, and convolutional neural
   network. Thirdly, it presents the latest developments and applications
   of deep learning in many fields such as speech processing, computer
   vision, natural language processing, and medical applications. Finally,
   it puts forward the problems and the future research directions of deep
   learning. (C) 2021 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.cosrev.2021.100379},
EarlyAccessDate = {MAR 2021},
Article-Number = {100379},
ISSN = {1574-0137},
EISSN = {1876-7745},
ResearcherID-Numbers = {Dong, Shi/ITT-3871-2023
   Abbas, Khushnood/I-6735-2017},
ORCID-Numbers = {Dong, Shi/0000-0003-4616-6519
   Abbas, Khushnood/0000-0002-0096-3179},
Unique-ID = {WOS:000663500200003},
}

@article{ WOS:000413244600013,
Author = {Gillis, Jessie M. and Morsi, Walid G.},
Title = {Non-Intrusive Load Monitoring Using Semi-Supervised Machine Learning and
   Wavelet Design},
Journal = {IEEE TRANSACTIONS ON SMART GRID},
Year = {2017},
Volume = {8},
Number = {6},
Pages = {2648-2655},
Month = {NOV},
Abstract = {This paper presents a new approach based on semi-supervised machine
   learning and wavelet design applied to non-intrusive load monitoring.
   Co-training of two machine learning classifiers is used to automate the
   process of learning the load pattern after designing new wavelets. The
   numerical results demonstrating the effectiveness of the proposed
   approach are discussed and conclusions are drawn.},
DOI = {10.1109/TSG.2016.2532885},
ISSN = {1949-3053},
EISSN = {1949-3061},
ResearcherID-Numbers = {Morsi Ibrahim, Walid/JFL-1090-2023},
Unique-ID = {WOS:000413244600013},
}

@article{ WOS:000837752300005,
Author = {Wang, Zelin and Liu, Xinke and Zhang, Weiye and Zhi, Yingying and Cheng,
   Shi},
Title = {The statistical analysis in the era of big data},
Journal = {INTERNATIONAL JOURNAL OF MODELLING IDENTIFICATION AND CONTROL},
Year = {2022},
Volume = {40},
Number = {2},
Pages = {151-157},
Abstract = {In the big data environment, the traditional machine learning algorithm
   for data processing is somewhat inadequate. Therefore, machine learning
   algorithms adapted to big data environment have become a research
   hotspot. At the time of the marriage of big data and machine learning,
   it is necessary to predict the related challenges and opportunities.
   This paper mainly analyses and summarises the current research status of
   machine learning algorithms for processing big data, and discusses the
   new opportunities and challenges that machine learning paradigm will
   face in the era of big data. It also explores the new technology
   breakthrough that machine learning will produce in the era of big data.},
DOI = {10.1504/IJMIC.2022.124718},
ISSN = {1746-6172},
EISSN = {1746-6180},
ResearcherID-Numbers = {zhang, weiye/NXX-6343-2025
   cheng, shi/HQZ-4620-2023},
ORCID-Numbers = {Cheng, Shi/0000-0002-0597-9823
   },
Unique-ID = {WOS:000837752300005},
}

@article{ WOS:000576604100013,
Author = {Rana, Pratip and Berry, Carter and Ghosh, Preetam and Fong, Stephen S.},
Title = {Recent advances on constraint-based models by integrating machine
   learning},
Journal = {CURRENT OPINION IN BIOTECHNOLOGY},
Year = {2020},
Volume = {64},
Number = {SI},
Pages = {85-91},
Month = {AUG},
Abstract = {Research that meaningfully integrates constraint-based modeling with
   machine learning is at its infancy but holds much promise. Here, we
   consider where machine learning has been implemented within the
   constraint-based modeling reconstruction framework and highlight the
   need to develop approaches that can identify meaningful features from
   large-scale data and connect them to biological mechanisms to establish
   causality to connect genotype to phenotype. We motivate the construction
   of iterative integrative schemes where machine learning can fine-tune
   the input constraints in a constraint-based model or contrarily,
   constraint-based model simulation results are analyzed by machine
   learning and reconciled with experimental data. This can iteratively
   refine a constraint-based model until there is consistency between
   experimental data, machine learning results, and constraint-based model
   simulations.},
DOI = {10.1016/j.copbio.2019.11.007},
ISSN = {0958-1669},
EISSN = {1879-0429},
ResearcherID-Numbers = {Rana, Pratip/AAR-4351-2021
   },
ORCID-Numbers = {Rana, Pratip/0000-0001-9199-2479
   Ghosh, Preetam/0000-0003-3880-5886},
Unique-ID = {WOS:000576604100013},
}

@article{ WOS:000518683500002,
Author = {Gibert, Daniel and Mateu, Carles and Planes, Jordi},
Title = {The rise of machine learning for detection and classification of
   malware: Research developments, trends and challenges},
Journal = {JOURNAL OF NETWORK AND COMPUTER APPLICATIONS},
Year = {2020},
Volume = {153},
Month = {MAR 1},
Abstract = {The struggle between security analysts and malware developers is a
   never-ending battle with the complexity of malware changing as quickly
   as innovation grows. Current state-of-the-art research focus on the
   development and application of machine learning techniques for malware
   detection due to its ability to keep pace with malware evolution. This
   survey aims at providing a systematic and detailed overview of machine
   learning techniques for malware detection and in particular, deep
   learning techniques. The main contributions of the paper are: (1) it
   provides a complete description of the methods and features in a
   traditional machine learning workflow for malware detection and
   classification, (2) it explores the challenges and limitations of
   traditional machine learning and (3) it analyzes recent trends and
   developments in the field with special emphasis on deep learning
   approaches. Furthermore, (4) it presents the research issues and
   unsolved challenges of the state-of-the-art techniques and (5) it
   discusses the new directions of research. The survey helps researchers
   to have an understanding of the malware detection field and of the new
   developments and directions of research explored by the scientific
   community to tackle the problem.},
DOI = {10.1016/j.jnca.2019.102526},
Article-Number = {102526},
ISSN = {1084-8045},
EISSN = {1095-8592},
ResearcherID-Numbers = {Planes, Jordi/B-5416-2013
   Gibert, Daniel/AAE-1366-2020
   Mateu, Carles/H-4638-2012},
ORCID-Numbers = {Planes, Jordi/0000-0003-1861-9736
   Gibert, Daniel/0000-0002-2448-1297
   Mateu, Carles/0000-0002-4864-0328},
Unique-ID = {WOS:000518683500002},
}

@article{ WOS:000656417100004,
Author = {Nasir, Vahid and Sassani, Farrokh},
Title = {A review on deep learning in machining and tool monitoring: methods,
   opportunities, and challenges},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY},
Year = {2021},
Volume = {115},
Number = {9-10},
Pages = {2683-2709},
Month = {AUG},
Abstract = {Data-driven methods provided smart manufacturing with unprecedented
   opportunities to facilitate the transition toward Industry 4.0-based
   production. Machine learning and deep learning play a critical role in
   developing intelligent systems for descriptive, diagnostic, and
   predictive analytics for machine tools and process health monitoring.
   This paper reviews the opportunities and challenges of deep learning
   (DL) for intelligent machining and tool monitoring. The components of an
   intelligent monitoring framework are introduced. The main advantages and
   disadvantages of machine learning (ML) models are presented and compared
   with those of deep models. The main DL models, including autoencoders,
   deep belief networks, convolutional neural networks (CNNs), and
   recurrent neural networks (RNNs), were discussed, and their applications
   in intelligent machining and tool condition monitoring were reviewed.
   The opportunities of data-driven smart manufacturing approach applied to
   intelligent machining were discussed to be (1) automated feature
   engineering, (2) handling big data, (3) handling high-dimensional data,
   (4) avoiding sensor redundancy, (5) optimal sensor fusion, and (6)
   offering hybrid intelligent models. Finally, the data-driven challenges
   in smart manufacturing, including the challenges associated with the
   data size, data nature, model selection, and process uncertainty, were
   discussed, and the research gaps were outlined.},
DOI = {10.1007/s00170-021-07325-7},
EarlyAccessDate = {MAY 2021},
ISSN = {0268-3768},
EISSN = {1433-3015},
ORCID-Numbers = {nasir, vahid/0000-0002-6906-3224},
Unique-ID = {WOS:000656417100004},
}

@article{ WOS:000484532300003,
Author = {Prates, Marcos O.},
Title = {Spatial extreme learning machines: An application on prediction of
   disease counts},
Journal = {STATISTICAL METHODS IN MEDICAL RESEARCH},
Year = {2019},
Volume = {28},
Number = {9},
Pages = {2583-2594},
Month = {SEP},
Note = {10th Bi-Annual International Conference on Spatial Statistics,
   Geographical Epidemiology and Geographical Aspects of Public Health
   (GEOMED), Porto, PORTUGAL, SEP 07-09, 2017},
Abstract = {Extreme learning machines have gained a lot of attention by the machine
   learning community because of its interesting properties and
   computational advantages. With the increase in collection of information
   nowadays, many sources of data have missing information making
   statistical analysis harder or unfeasible. In this paper, we present a
   new model, coined spatial extreme learning machine, that combine spatial
   modeling with extreme learning machines keeping the nice properties of
   both methodologies and making it very flexible and robust. As explained
   throughout the text, the spatial extreme learning machines have many
   advantages in comparison with the traditional extreme learning machines.
   By a simulation study and a real data analysis we present how the
   spatial extreme learning machine can be used to improve imputation of
   missing data and uncertainty prediction estimation.},
DOI = {10.1177/0962280218767985},
ISSN = {0962-2802},
EISSN = {1477-0334},
ResearcherID-Numbers = {Prates, Marcos/D-5657-2018},
Unique-ID = {WOS:000484532300003},
}

@article{ WOS:000598812600004,
Author = {Milojevic-Dupont, Nikola and Creutzig, Felix},
Title = {Machine learning for geographically differentiated climate change
   mitigation in urban areas},
Journal = {SUSTAINABLE CITIES AND SOCIETY},
Year = {2021},
Volume = {64},
Month = {JAN},
Abstract = {Artificial intelligence and machine learning are transforming scientific
   disciplines, but their full potential for climate change mitigation
   remains elusive. Here, we conduct a systematic review of applied machine
   learning studies that are of relevance for climate change mitigation,
   focusing specifically on the fields of remote sensing, urban
   transportation, and buildings. The relevant body of literature spans
   twenty years and is growing exponentially. We show that the emergence of
   big data and machine learning methods enables climate solution research
   to overcome generic recommendations and provide policy solutions at
   urban, street, building and household scale, adapted to specific
   contexts, but scalable to global mitigation potentials. We suggest a
   meta-algorithmic architecture and framework for using machine learning
   to optimize urban planning for accelerating, improving and transforming
   urban infrastructure provision.},
DOI = {10.1016/j.scs.2020.102526},
Article-Number = {102526},
ISSN = {2210-6707},
EISSN = {2210-6715},
ResearcherID-Numbers = {Creutzig, Felix/B-8691-2016},
ORCID-Numbers = {Creutzig, Felix/0000-0002-5710-3348},
Unique-ID = {WOS:000598812600004},
}

@article{ WOS:000466837400054,
Author = {Blomberg, Stig Nikolaj and Folke, Fredrik and Ersboll, Annette Kjaer and
   Christensen, Helle Collatz and Torp-Pedersen, Christian and Sayre,
   Michael R. and Counts, Catherine R. and Lippert, Freddy K.},
Title = {Machine learning as a supportive tool to recognize cardiac arrest in
   emergency calls},
Journal = {RESUSCITATION},
Year = {2019},
Volume = {138},
Pages = {322-329},
Month = {MAY},
Abstract = {Background: Emergency medical dispatchers fail to identify approximately
   25\% of cases of out of hospital cardiac arrest, thus lose the
   opportunity to provide the caller instructions in cardiopulmonary
   resuscitation. We examined whether a machine learning framework could
   recognize out-of-hospital cardiac arrest from audio files of calls to
   the emergency medical dispatch center.
   Methods: For all incidents responded to by Emergency Medical Dispatch
   Center Copenhagen in 2014, the associated call was retrieved. A machine
   learning framework was trained to recognize cardiac arrest from the
   recorded calls. Sensitivity, specificity, and positive predictive value
   for recognizing out-of-hospital cardiac arrest were calculated. The
   performance of the machine learning framework was compared to the actual
   recognition and time-to-recognition of cardiac arrest by medical
   dispatchers.
   Results: We examined 108,607 emergency calls, of which 918 (0.8\%) were
   out-of-hospital cardiac arrest calls eligible for analysis. Compared
   with medical dispatchers, the machine learning framework had a
   significantly higher sensitivity (72.5\% vs. 84.1\%, p < 0.001) with
   lower specificity (98.8\% vs. 97.3\%, p < 0.001). The machine learning
   framework had a lower positive predictive value than dispatchers (20.9\%
   vs. 33.0\%, p < 0.001). Time-to-recognition was significantly shorter
   for the machine learning framework compared to the dispatchers (median
   44 seconds vs. 54 s, p < 0.001).
   Conclusions: A machine learning framework performed better than
   emergency medical dispatchers for identifying out-of-hospital cardiac
   arrest in emergency phone calls. Machine learning may play an important
   role as a decision support tool for emergency medical dispatchers.},
DOI = {10.1016/j.resuscitation.2019.01.015},
ISSN = {0300-9572},
EISSN = {1873-1570},
ResearcherID-Numbers = {Folke, Fredrik/P-3601-2014
   Blomberg, Stig Nikolaj Fasmer/IYS-6536-2023
   Torp-Pedersen, Christian/E-5931-2013
   Christensen, Helle Collatz/ITV-2881-2023
   Sayre, Michael/E-8383-2017},
ORCID-Numbers = {Christensen, Helle/0000-0003-3302-7149
   Blomberg, Stig Nikolaj Fasmer/0000-0002-5073-6820
   Lippert, Freddy/0000-0002-2356-8809
   Counts, Catherine/0000-0002-7152-8887
   Folke, Fredrik/0000-0002-2284-7857
   Sayre, Michael/0000-0003-0322-3181},
Unique-ID = {WOS:000466837400054},
}

@article{ WOS:000729048700005,
Author = {Habehh, Hafsa and Gohel, Suril},
Title = {Machine Learning in Healthcare},
Journal = {CURRENT GENOMICS},
Year = {2021},
Volume = {22},
Number = {4},
Pages = {291-300},
Abstract = {Recent advancements in Artificial Intelligence (AI) and Machine Learning
   (ML) technol-ogy have brought on substantial strides in predicting and
   identifying health emergencies, disease populations, and disease state
   and immune response, amongst a few. Although, skepticism remains
   regarding the practical application and interpretation of results from
   ML-based approaches in healthcare settings, the inclusion of these
   approaches is increasing at a rapid pace. Here we provide a brief
   overview of machine learning-based approaches and learning algorithms
   including super -vised, unsupervised, and reinforcement learning along
   with examples. Second, we discuss the appli-cation of ML in several
   healthcare fields, including radiology, genetics, electronic health
   records, and neuroimaging. We also briefly discuss the risks and
   challenges of ML application to healthcare such as system privacy and
   ethical concerns and provide suggestions for future applications.},
DOI = {10.2174/1389202922666210705124359},
ISSN = {1389-2029},
EISSN = {1875-5488},
ORCID-Numbers = {Gohel, Suril/0000-0003-2387-5021},
Unique-ID = {WOS:000729048700005},
}

@article{ WOS:000540241000001,
Author = {Tuladhar, Anup and Gill, Sascha and Ismail, Zahinoor and Forkert, Nils
   D. and Alzheimers Dis Neuroimaging Initia},
Title = {Building machine learning models without sharing patient data: A
   simulation-based analysis of distributed learning by ensembling},
Journal = {JOURNAL OF BIOMEDICAL INFORMATICS},
Year = {2020},
Volume = {106},
Month = {JUN},
Abstract = {The development of machine learning solutions in medicine is often
   hindered by difficulties associated with sharing patient data.
   Distributed learning aims to train machine learning models locally
   without requiring data sharing. However, the utility of distributed
   learning for rare diseases, with only a few training examples at each
   contributing local center, has not been investigated. The aim of this
   work was to simulate distributed learning models by ensembling with
   artificial neural networks (ANN), support vector machines (SVM), and
   random forests (RF) and evaluate them using four medical datasets.
   Distributed learning by ensembling locally trained agents improved
   performance compared to models trained using the data from a single
   institution, even in cases where only a very few training examples are
   available per local center. Distributed learning improved when more
   locally trained models were added to the ensemble. Local class imbalance
   reduced distributed SVM performance but did not impact distributed RF
   and ANN classification. Our results suggest that distributed learning by
   ensembling can be used to train machine learning models without sharing
   patient data and is suitable to use with small datasets.},
DOI = {10.1016/j.jbi.2020.103424},
Article-Number = {103424},
ISSN = {1532-0464},
EISSN = {1532-0480},
ResearcherID-Numbers = {Forkert, Nils Daniel/K-6273-2012
   Tuladhar, Anup/AAL-7139-2020
   Forkert, Nils/K-6273-2012},
ORCID-Numbers = {Tuladhar, Anup/0000-0002-3942-2732
   Forkert, Nils Daniel/0000-0003-2556-3224
   },
Unique-ID = {WOS:000540241000001},
}

@article{ WOS:000689068400001,
Author = {Mushtaq, Shiza and Islam, M. M. Manjurul and Sohaib, Muhammad},
Title = {Deep Learning Aided Data-Driven Fault Diagnosis of Rotatory Machine: A
   Comprehensive Review},
Journal = {ENERGIES},
Year = {2021},
Volume = {14},
Number = {16},
Month = {AUG},
Abstract = {This paper presents a comprehensive review of the developments made in
   rotating bearing fault diagnosis, a crucial component of a rotatory
   machine, during the past decade. A data-driven fault diagnosis framework
   consists of data acquisition, feature extraction/feature learning, and
   decision making based on shallow/deep learning algorithms. In this
   review paper, various signal processing techniques, classical machine
   learning approaches, and deep learning algorithms used for bearing fault
   diagnosis have been discussed. Moreover, highlights of the available
   public datasets that have been widely used in bearing fault diagnosis
   experiments, such as Case Western Reserve University (CWRU), Paderborn
   University Bearing, PRONOSTIA, and Intelligent Maintenance Systems
   (IMS), are discussed in this paper. A comparison of machine learning
   techniques, such as support vector machines, k-nearest neighbors,
   artificial neural networks, etc., deep learning algorithms such as a
   deep convolutional network (CNN), auto-encoder-based deep neural network
   (AE-DNN), deep belief network (DBN), deep recurrent neural network
   (RNN), and other deep learning methods that have been utilized for the
   diagnosis of rotary machines bearing fault, is presented.},
DOI = {10.3390/en14165150},
Article-Number = {5150},
EISSN = {1996-1073},
ResearcherID-Numbers = {Islam, M M Manjurul/S-9353-2017
   Sohaib, Muhammad/B-7798-2019
   },
ORCID-Numbers = {Sohaib, Muhammad/0000-0003-0218-3595
   Islam, M M Manjurul/0000-0002-1823-1304},
Unique-ID = {WOS:000689068400001},
}

@article{ WOS:000363458900023,
Author = {Huellermeier, Eyke},
Title = {Does machine learning need fuzzy logic?},
Journal = {FUZZY SETS AND SYSTEMS},
Year = {2015},
Volume = {281},
Number = {SI},
Pages = {292-299},
Month = {DEC 15},
Abstract = {This article is a short position paper in which the author outlines his
   (necessarily subjective) perception of current research in fuzzy machine
   learning, that is, the use of formal concepts and mathematical tools
   from fuzzy sets and fuzzy logic in the field of machine learning. The
   paper starts with a critical appraisal of previous contributions to
   fuzzy machine learning and ends with a suggestion of some directions for
   future work. (C) 2015 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.fss.2015.09.001},
ISSN = {0165-0114},
EISSN = {1872-6801},
Unique-ID = {WOS:000363458900023},
}

@article{ WOS:000999504400002,
Author = {Guo, Wenxuan and Zhen, Hui-Ling and Li, Xijun and Luo, Wanqian and Yuan,
   Mingxuan and Jin, Yaohui and Yan, Junchi},
Title = {Machine Learning Methods in Solving the Boolean Satisfiability Problem},
Journal = {MACHINE INTELLIGENCE RESEARCH},
Year = {2023},
Volume = {20},
Number = {5},
Pages = {640-655},
Month = {OCT},
Abstract = {This paper reviews the recent literature on solving the Boolean
   satisfiability problem (SAT), an archetypal NP-complete problem, with
   the aid of machine learning (ML) techniques. Over the last decade, the
   machine learning society advances rapidly and surpasses human
   performance on several tasks. This trend also inspires a number of works
   that apply machine learning methods for SAT solving. In this survey, we
   examine the evolving ML SAT solvers from naive classifiers with
   handcrafted features to emerging end-to-end SAT solvers, as well as
   recent progress on combinations of existing conflict-driven clause
   learning (CDCL) and local search solvers with machine learning methods.
   Overall, solving SAT with machine learning is a promising yet
   challenging research topic. We conclude the limitations of current works
   and suggest possible future directions. The collected paper list is
   available at https://github.com/ThinklabSJTU/awesome-ml4co.},
DOI = {10.1007/s11633-022-1396-2},
EarlyAccessDate = {JUN 2023},
ISSN = {2731-538X},
EISSN = {2731-5398},
ResearcherID-Numbers = {Yan, Junchi/ADK-0658-2022
   },
ORCID-Numbers = {Guo, Wenxuan/0000-0001-6336-3819
   Yan, Junchi/0000-0001-9639-7679
   Jin, Yaohui/0000-0001-6158-6277},
Unique-ID = {WOS:000999504400002},
}

@article{ WOS:000979995400004,
Author = {Janizek, Joseph D. and Dincer, Ayse B. and Celik, Safiye and Chen, Hugh
   and Chen, William and Naxerova, Kamila and Lee, Su-In},
Title = {Uncovering expression signatures of synergistic drug responses via
   ensembles of explainable machine-learning models},
Journal = {NATURE BIOMEDICAL ENGINEERING},
Year = {2023},
Volume = {7},
Number = {6},
Pages = {811+},
Month = {JUN},
Abstract = {Ensembles of explainable machine-learning models increase the quality of
   explanations for the molecular basis of synergetic drug combinations, as
   shown for the treatment of acute myeloid leukaemia.
   Machine learning may aid the choice of optimal combinations of
   anticancer drugs by explaining the molecular basis of their synergy. By
   combining accurate models with interpretable insights, explainable
   machine learning promises to accelerate data-driven cancer pharmacology.
   However, owing to the highly correlated and high-dimensional nature of
   transcriptomic data, naively applying current explainable
   machine-learning strategies to large transcriptomic datasets leads to
   suboptimal outcomes. Here by using feature attribution methods, we show
   that the quality of the explanations can be increased by leveraging
   ensembles of explainable machine-learning models. We applied the
   approach to a dataset of 133 combinations of 46 anticancer drugs tested
   in ex vivo tumour samples from 285 patients with acute myeloid leukaemia
   and uncovered a haematopoietic-differentiation signature underlying drug
   combinations with therapeutic synergy. Ensembles of machine-learning
   models trained to predict drug combination synergies on the basis of
   gene-expression data may improve the feature attribution quality of
   complex machine-learning models.},
DOI = {10.1038/s41551-023-01034-0},
EarlyAccessDate = {MAY 2023},
ISSN = {2157-846X},
ORCID-Numbers = {Naxerova, Kamila/0000-0001-7744-5110
   Lee, Su-In/0000-0001-5833-5215
   Chen, Hugh/0000-0003-0549-4524},
Unique-ID = {WOS:000979995400004},
}

@article{ WOS:000884211300001,
Author = {Geertsema, Paul and Lu, Helen},
Title = {Relative Valuation with Machine Learning},
Journal = {JOURNAL OF ACCOUNTING RESEARCH},
Year = {2023},
Volume = {61},
Number = {1},
Pages = {329-376},
Month = {MAR},
Abstract = {We use machine learning for relative valuation and peer firm selection.
   In out-of-sample tests, our machine learning models substantially
   outperform traditional models in valuation accuracy. This outperformance
   persists over time and holds across different types of firms. The
   valuations produced by machine learning models behave like fundamental
   values. Overvalued stocks decrease in price and undervalued stocks
   increase in price in the following month. Determinants of valuation
   multiples identified by machine learning models are consistent with
   theoretical predictions derived from a discounted cash flow approach.
   Profitability ratios, growth measures, and efficiency ratios are the
   most important value drivers throughout our sample period. We derive a
   novel method to express valuation multiples predicted by our machine
   learning models as weighted averages of peer firm multiples. These
   weights are a measure of peer-firm comparability and can be used for
   selecting peer-groups.},
DOI = {10.1111/1475-679X.12464},
EarlyAccessDate = {NOV 2022},
ISSN = {0021-8456},
EISSN = {1475-679X},
ResearcherID-Numbers = {Geertsema, Paul/G-5257-2019
   Lu, Helen/H-7321-2015},
ORCID-Numbers = {Geertsema, Paul/0000-0002-1461-3807
   Lu, Helen/0000-0001-6525-4608},
Unique-ID = {WOS:000884211300001},
}

@article{ WOS:000569734100001,
Author = {Aiyanyo, Imatitikua D. and Samuel, Hamman and Lim, Heuiseok},
Title = {A Systematic Review of Defensive and Offensive Cybersecurity with
   Machine Learning},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {17},
Month = {SEP},
Abstract = {This is a systematic review of over one hundred research papers about
   machine learning methods applied to defensive and offensive
   cybersecurity. In contrast to previous reviews, which focused on several
   fragments of research topics in this area, this paper systematically and
   comprehensively combines domain knowledge into a single review.
   Ultimately, this paper seeks to provide a base for researchers that wish
   to delve into the field of machine learning for cybersecurity. Our
   findings identify the frequently used machine learning methods within
   supervised, unsupervised, and semi-supervised machine learning, the most
   useful data sets for evaluating intrusion detection methods within
   supervised learning, and methods from machine learning that have shown
   promise in tackling various threats in defensive and offensive
   cybersecurity.},
DOI = {10.3390/app10175811},
Article-Number = {5811},
EISSN = {2076-3417},
ResearcherID-Numbers = {Heuiseok, Lim/IUP-5678-2023
   Samuel, Hamman/AAE-1319-2021
   Aiyanyo, Imatitikua/ADU-2832-2022
   },
ORCID-Numbers = {Aiyanyo, Imatitikua/0000-0002-6191-2922
   Samuel, Hamman/0000-0002-3053-6047},
Unique-ID = {WOS:000569734100001},
}

@article{ WOS:000427574900001,
Author = {Ker, Justin and Wang, Lipo and Rao, Jai and Lim, Tchoyoson},
Title = {Deep Learning Applications in Medical Image Analysis},
Journal = {IEEE ACCESS},
Year = {2018},
Volume = {6},
Pages = {9375-9389},
Abstract = {The tremendous success of machine learning algorithms at image
   recognition tasks in recent years intersects with a time of dramatically
   increased use of electronic medical records and diagnostic imaging. This
   review introduces the machine learning algorithms as applied to medical
   image analysis, focusing on convolutional neural networks, and
   emphasizing clinical aspects of the field. The advantage of machine
   learning in an era of medical big data is that significant hierarchal
   relationships within the data can be discovered algorithmically without
   laborious hand-crafting of features. We cover key research areas and
   applications of medical image classification, localization, detection,
   segmentation, and registration. We conclude by discussing research
   obstacles, emerging trends, and possible future directions.},
DOI = {10.1109/ACCESS.2017.2788044},
ISSN = {2169-3536},
ResearcherID-Numbers = {Wang, Lipo/A-5154-2011
   Lim, Thiam/B-5323-2011},
ORCID-Numbers = {Wang, Lipo/0000-0002-4257-7639
   Lim, CC Tchoyoson/0000-0001-9355-6988
   },
Unique-ID = {WOS:000427574900001},
}

@article{ WOS:000573452600007,
Author = {Weis, V, C. and Jutzeler, C. R. and Borgwardt, K.},
Title = {Machine learning for microbial identification and antimicrobial
   susceptibility testing on MALDI-TOF mass spectra: a systematic review},
Journal = {CLINICAL MICROBIOLOGY AND INFECTION},
Year = {2020},
Volume = {26},
Number = {10},
Pages = {1310-1317},
Month = {OCT},
Abstract = {Background: The matrix assisted laser desorption/ionization and
   time-of-flight mass spectrometry (MALDI-TOF MS) technology has
   revolutionized the field of microbiology by facilitating precise and
   rapid species identification. Recently, machine learning techniques have
   been leveraged to maximally exploit the information contained in
   MALDI-TOF MS, with the ultimate goal to refine species identification
   and streamline antimicrobial resistance determination.
   Objectives: The aim was to systematically review and evaluate studies
   employing machine learning for the analysis of MALDI-TOF mass spectra.
   Data sources: Using PubMed/Medline, Scopus and Web of Science, we
   searched the existing literature for machine learning-supported
   applications of MALDI-TOF mass spectra for microbial species and
   antimicrobial susceptibility identification.
   Study eligibility criteria: Original research studies using machine
   learning to exploit MALDI-TOF mass spectra for microbial specie and
   antimicrobial susceptibility identification were included. Studies
   focusing on single proteins and peptides, case studies and review
   articles were excluded.
   Methods: A systematic review according to the PRISMA guidelines was
   performed and a quality assessment of the machine learning models
   conducted.
   Results: From the 36 studies that met our inclusion criteria, 27
   employed machine learning for species identification and nine for
   antimicrobial susceptibility testing. Support Vector Machines, Genetic
   Algorithms, Artificial Neural Networks and Quick Classifiers were the
   most frequently used machine learning algorithms. The quality of the
   studies ranged between poor and very good. The majority of the studies
   reported how to interpret the predictors (88.89\%) and suggested
   possible clinical applications of the developed algorithm (100\%), but
   only four studies (11.11\%) validated machine learning algorithms on
   external datasets.
   Conclusions: A growing number of studies utilize machine learning to
   optimize the analysis of MALDITOF mass spectra. This review, however,
   demonstrates that there are certain shortcomings of current machine
   learning-supported approaches that have to be addressed to make them
   widely available and incorporated them in the clinical routine. (C) 2020
   The Authors. Published by Elsevier Ltd on behalf of European Society of
   Clinical Microbiology and Infectious Diseases.},
DOI = {10.1016/j.cmi.2020.03.014},
ISSN = {1198-743X},
EISSN = {1469-0691},
ResearcherID-Numbers = {Borgwardt, Karsten/MGA-4594-2025
   },
ORCID-Numbers = {Jutzeler, Catherine/0000-0001-7167-8271
   Borgwardt, Karsten/0000-0001-7221-2393},
Unique-ID = {WOS:000573452600007},
}

@article{ WOS:000530237100003,
Author = {Seshia, Sanjit A. and Jha, Somesh and Dreossi, Tommaso},
Title = {Semantic Adversarial Deep Learning},
Journal = {IEEE DESIGN \& TEST},
Year = {2020},
Volume = {37},
Number = {2},
Pages = {8-18},
Month = {APR},
Abstract = {Adversarial examples have emerged as a key threat for
   machine-learning-based systems, especially the ones that employ deep
   neural networks. Unlike a large body of research in this area, this
   Keynote article accounts for the semantic, context, and specifications
   of the complete system with machine learning components in
   resource-constrained environments. -Muhammad Shafique, Technische
   Universitat Wien},
DOI = {10.1109/MDAT.2020.2968274},
ISSN = {2168-2356},
EISSN = {2168-2364},
Unique-ID = {WOS:000530237100003},
}

@article{ WOS:000988744600001,
Author = {Kamm, Simon and Veekati, Sushma Sri and Mueller, Timo and Jazdi, Nasser
   and Weyrich, Michael},
Title = {A survey on machine learning based analysis of heterogeneous data in
   industrial automation},
Journal = {COMPUTERS IN INDUSTRY},
Year = {2023},
Volume = {149},
Month = {AUG},
Abstract = {In many application domains data from different sources are increasingly
   available to thoroughly monitor and describe a system or device.
   Especially within the industrial automation domain, heterogeneous data
   and its analysis gain a lot of attention from research and industry,
   since it has the potential to improve or enable tasks like diagnostics,
   predictive maintenance, and condition monitoring. For data analysis,
   machine learning based approaches are mostly used in recent literature,
   as these algorithms allow us to learn complex correlations within the
   data. To analyze even heterogeneous data and gain benefits from it in an
   application, data from different sources need to be integrated, stored,
   and managed to apply machine learning algorithms. In a setting with
   heterogeneous data sources, the analysis algorithms should also be able
   to handle data source failures or newly added data sources. In addition,
   existing knowledge should be used to improve the machine learning based
   analysis or its training process. To find existing approaches for the
   machine learning based analysis of heterogeneous data in the industrial
   automation domain, this paper presents the result of a systematic
   literature review. The publications were reviewed, evaluated, and
   discussed concerning five requirements that are derived in this paper.
   We identified promising solutions and approaches and outlined open
   research challenges, which are not yet covered sufficiently in the
   literature.},
DOI = {10.1016/j.compind.2023.103930},
EarlyAccessDate = {APR 2023},
Article-Number = {103930},
ISSN = {0166-3615},
EISSN = {1872-6194},
ResearcherID-Numbers = {Müller, Timo/AAJ-2474-2021},
ORCID-Numbers = {Kamm, Simon/0000-0001-8459-2450
   },
Unique-ID = {WOS:000988744600001},
}

@article{ WOS:000595568500001,
Author = {Basu, Sanjay and Johnson, Karl T. and Berkowitz, Seth A.},
Title = {Use of Machine Learning Approaches in Clinical Epidemiological Research
   of Diabetes},
Journal = {CURRENT DIABETES REPORTS},
Year = {2020},
Volume = {20},
Number = {12},
Month = {DEC},
Abstract = {Purpose of Review Machine learning approaches-which seek to predict
   outcomes or classify patient features by recognizing patterns in large
   datasets-are increasingly applied to clinical epidemiology research on
   diabetes. Given its novelty and emergence in fields outside of
   biomedical research, machine learning terminology, techniques, and
   research findings may be unfamiliar to diabetes researchers. Our aim was
   to present the use of machine learning approaches in an approachable
   way, drawing from clinical epidemiological research in diabetes
   published from 1 Jan 2017 to 1 June 2020. Recent Findings Machine
   learning approaches using tree-based learners-which produce decision
   trees to help guide clinical interventions-frequently have higher
   sensitivity and specificity than traditional regression models for risk
   prediction. Machine learning approaches using neural networking and
   ``deep learning{''} can be applied to medical image data, particularly
   for the identification and staging of diabetic retinopathy and skin
   ulcers. Among the machine learning approaches reviewed, researchers
   identified new strategies to develop standard datasets for rigorous
   comparisons across older and newer approaches, methods to illustrate how
   a machine learner was treating underlying data, and approaches to
   improve the transparency of the machine learning process. Machine
   learning approaches have the potential to improve risk stratification
   and outcome prediction for clinical epidemiology applications. Achieving
   this potential would be facilitated by use of universal open-source
   datasets for fair comparisons. More work remains in the application of
   strategies to communicate how the machine learners are generating their
   predictions.},
DOI = {10.1007/s11892-020-01353-5},
Article-Number = {80},
ISSN = {1534-4827},
EISSN = {1539-0829},
ORCID-Numbers = {Berkowitz, Seth/0000-0003-1030-4297},
Unique-ID = {WOS:000595568500001},
}

@article{ WOS:000879006700002,
Author = {Zhong, Ruizhi and Salehi, Cyrus and Johnson, Ray},
Title = {Machine learning for drilling applications: A review},
Journal = {JOURNAL OF NATURAL GAS SCIENCE AND ENGINEERING},
Year = {2022},
Volume = {108},
Month = {DEC},
Abstract = {In the past several decades, machine learning has gained increasing
   interest in the oil and gas industry. This paper presents a
   comprehensive review of machine learning studies for drilling
   applications in the following categories: (1) drilling fluids; (2)
   drilling hydraulics; (3) drilling dynamics; (4) drilling problems; and
   (5) miscellaneous drilling applications. In each study, the machine
   learning algorithm(s), sample size, inputs and output(s), and
   performance are extracted. In addition, similarities of studies in each
   category are summarized and recommendations are made for future
   development.},
DOI = {10.1016/j.jngse.2022.104807},
EarlyAccessDate = {OCT 2022},
Article-Number = {104807},
ISSN = {1875-5100},
EISSN = {2212-3865},
ResearcherID-Numbers = {Johnson, Raymond/C-9194-2015},
ORCID-Numbers = {Johnson, Raymond/0000-0002-0659-3770},
Unique-ID = {WOS:000879006700002},
}

@article{ WOS:001076506500001,
Author = {Stankeviciute, Kamile and Woillard, Jean-Baptiste and Peck, Richard W.
   and Marquet, Pierre and van der Schaar, Mihaela},
Title = {Bridging the Worlds of Pharmacometrics and Machine Learning},
Journal = {CLINICAL PHARMACOKINETICS},
Year = {2023},
Volume = {62},
Number = {11},
Pages = {1551-1565},
Month = {NOV},
Abstract = {Precision medicine requires individualized modeling of disease and drug
   dynamics, with machine learning-based computational techniques gaining
   increasing popularity. The complexity of either field, however, makes
   current pharmacological problems opaque to machine learning
   practitioners, and state-of-the-art machine learning methods
   inaccessible to pharmacometricians. To help bridge the two worlds, we
   provide an introduction to current problems and techniques in
   pharmacometrics that ranges from pharmacokinetic and pharmacodynamic
   modeling to pharmacometric simulations, model-informed precision dosing,
   and systems pharmacology, and review some of the machine learning
   approaches to address them. We hope this would facilitate collaboration
   between experts, with complementary strengths of principled
   pharmacometric modeling and flexibility of machine learning leading to
   synergistic effects in pharmacological applications.},
DOI = {10.1007/s40262-023-01310-x},
EarlyAccessDate = {OCT 2023},
ISSN = {0312-5963},
EISSN = {1179-1926},
ResearcherID-Numbers = {Woillard, Jean-Baptiste/C-4315-2016
   Woillard, Jean-Baptiste/N-1934-2019
   MARQUET, Pierre/A-1727-2017},
ORCID-Numbers = {Woillard, Jean-Baptiste/0000-0003-1695-0695
   Marquet, Pierre/0000-0001-7698-0760
   Stankeviciute, Kamile/0000-0003-2489-9615
   },
Unique-ID = {WOS:001076506500001},
}

@article{ WOS:000537076300001,
Author = {Hansen, Kristian Bondo},
Title = {The virtue of simplicity: On machine learning models in algorithmic
   trading},
Journal = {BIG DATA \& SOCIETY},
Year = {2020},
Volume = {7},
Number = {1},
Month = {JAN},
Abstract = {Machine learning models are becoming increasingly prevalent in
   algorithmic trading and investment management. The spread of machine
   learning in finance challenges existing practices of modelling and model
   use and creates a demand for practical solutions for how to manage the
   complexity pertaining to these techniques. Drawing on interviews with
   quants applying machine learning techniques to financial problems, the
   article examines how these people manage model complexity in the process
   of devising machine learning-powered trading algorithms. The analysis
   shows that machine learning quants use Ockham's razor - things should
   not be multiplied without necessity - as a heuristic tool to prevent
   excess model complexity and secure a certain level of human control and
   interpretability in the modelling process. I argue that understanding
   the way quants handle the complexity of learning models is a key to
   grasping the transformation of the human's role in contemporary data and
   model-driven finance. The study contributes to social studies of finance
   research on the human-model interplay by exploring it in the context of
   machine learning model use.},
DOI = {10.1177/2053951720926558},
Article-Number = {2053951720926558},
ISSN = {2053-9517},
ResearcherID-Numbers = {hansen, kristian/JNT-3421-2023
   },
ORCID-Numbers = {Hansen, Kristian Bondo/0000-0002-9536-6050},
Unique-ID = {WOS:000537076300001},
}

@article{ WOS:000605079100003,
Author = {Israel, Ronen and Kelly, Bryan and Moskowitz, Tobias},
Title = {CAN MACHINES ``LEARN{''} FINANCE?},
Journal = {JOURNAL OF INVESTMENT MANAGEMENT},
Year = {2020},
Volume = {18},
Number = {2},
Pages = {23-36},
Abstract = {Machine learning for asset management faces a unique set of challenges
   that differ markedly from other domains where machine learning has
   excelled. Understanding these differences is critical for developing
   impactful approaches and realistic expectations for machine learning in
   asset management. We discuss a variety of beneficial use cases and
   potential pitfalls, and emphasize the importance of economic theory and
   human expertise for achieving success through financial machine
   learning.},
ISSN = {1545-9144},
EISSN = {1545-9152},
Unique-ID = {WOS:000605079100003},
}

@article{ WOS:000880955300001,
Author = {Zheng, Ming and Wang, Fei and Hu, Xiaowen and Miao, Yuhao and Cao, Huo
   and Tang, Mingjing},
Title = {A Method for Analyzing the Performance Impact of Imbalanced Binary Data
   on Machine Learning Models},
Journal = {AXIOMS},
Year = {2022},
Volume = {11},
Number = {11},
Month = {NOV},
Abstract = {Machine learning models may not be able to effectively learn and predict
   from imbalanced data in the fields of machine learning and data mining.
   This study proposed a method for analyzing the performance impact of
   imbalanced binary data on machine learning models. It systematically
   analyzes 1. the relationship between varying performance in machine
   learning models and imbalance rate (IR); 2. the performance stability of
   machine learning models on imbalanced binary data. In the proposed
   method, the imbalanced data augmentation algorithms are first designed
   to obtain the imbalanced dataset with gradually varying IR. Then, in
   order to obtain more objective classification results, the evaluation
   metric AFG, arithmetic mean of area under the receiver operating
   characteristic curve (AUC), F-measure and G-mean are used to evaluate
   the classification performance of machine learning models. Finally,
   based on AFG and coefficient of variation (CV), the performance
   stability evaluation method of machine learning models is proposed.
   Experiments of eight widely used machine learning models on 48 different
   imbalanced datasets demonstrate that the classification performance of
   machine learning models decreases with the increase of IR on the same
   imbalanced data. Meanwhile, the classification performances of LR, DT
   and SVC are unstable, while GNB, BNB, KNN, RF and GBDT are relatively
   stable and not susceptible to imbalanced data. In particular, the BNB
   has the most stable classification performance. The Friedman and Nemenyi
   post hoc statistical tests also confirmed this result. The SMOTE method
   is used in oversampling-based imbalanced data augmentation, and
   determining whether other oversampling methods can obtain consistent
   results needs further research. In the future, an imbalanced data
   augmentation algorithm based on undersampling and hybrid sampling should
   be used to analyze the performance impact of imbalanced binary data on
   machine learning models.},
DOI = {10.3390/axioms11110607},
Article-Number = {607},
EISSN = {2075-1680},
ResearcherID-Numbers = {Zheng, Ming/OHR-8163-2025
   },
ORCID-Numbers = {Tang, MingJing/0000-0003-0435-8854},
Unique-ID = {WOS:000880955300001},
}

@article{ WOS:001114383400001,
Author = {Jan, Mahta and Spangaro, Allie and Lenartowicz, Michelle and Usaj, Mojca
   Mattiazzi},
Title = {From pixels to insights: Machine learning and deep learning for bioimage
   analysis},
Journal = {BIOESSAYS},
Year = {2024},
Volume = {46},
Number = {2},
Month = {FEB},
Abstract = {Bioimage analysis plays a critical role in extracting information from
   biological images, enabling deeper insights into cellular structures and
   processes. The integration of machine learning and deep learning
   techniques has revolutionized the field, enabling the automated,
   reproducible, and accurate analysis of biological images. Here, we
   provide an overview of the history and principles of machine learning
   and deep learning in the context of bioimage analysis. We discuss the
   essential steps of the bioimage analysis workflow, emphasizing how
   machine learning and deep learning have improved preprocessing,
   segmentation, feature extraction, object tracking, and classification.
   We provide examples that showcase the application of machine learning
   and deep learning in bioimage analysis. We examine user-friendly
   software and tools that enable biologists to leverage these techniques
   without extensive computational expertise. This review is a resource for
   researchers seeking to incorporate machine learning and deep learning in
   their bioimage analysis workflows and enhance their research in this
   rapidly evolving field.
   Machine learning and deep learning have revolutionized bioimage
   analysis, automating and enhancing tasks like image preprocessing,
   object segmentation and tracking, feature extraction, and
   classification. This review showcases the pivotal role these approaches
   have played in the field, and highlights user-friendly bioimage analysis
   tools for biologists without extensive computational expertise. image},
DOI = {10.1002/bies.202300114},
EarlyAccessDate = {DEC 2023},
ISSN = {0265-9247},
EISSN = {1521-1878},
ResearcherID-Numbers = {Mattiazzi Usaj, Mojca/GLS-1415-2022
   },
ORCID-Numbers = {Spangaro, Allie Lillian Arraial/0009-0000-8356-0668
   Mattiazzi Usaj, Mojca/0000-0002-7389-8478},
Unique-ID = {WOS:001114383400001},
}

@article{ WOS:001279022900001,
Author = {Wang, Qi and Huang, Rui and Xiong, Jianbin and Yang, Jianxiang and Dong,
   Xiangjun and Wu, Yipeng and Wu, Yinbo and Lu, Tiantian},
Title = {A survey on fault diagnosis of rotating machinery based on machine
   learning},
Journal = {MEASUREMENT SCIENCE AND TECHNOLOGY},
Year = {2024},
Volume = {35},
Number = {10},
Month = {OCT 1},
Abstract = {With the booming development of modern industrial technology, rotating
   machinery fault diagnosis is of great significance to improve the
   safety, efficiency and sustainable development of industrial production.
   Machine learning as an effective solution for fault identification, has
   advantages over traditional fault diagnosis solutions in processing
   complex data, achieving automation and intelligence, adapting to
   different fault types, and continuously optimizing. It has high
   application value and broad development prospects in the field of fault
   diagnosis of rotating machinery. Therefore, this article reviews machine
   learning and its applications in intelligent fault diagnosis technology
   and covers advanced topics in emerging deep learning techniques and
   optimization methods. Firstly, this article briefly introduces the
   theories of several main machine learning methods, including Extreme
   Learning Machines (ELM), Support Vector Machines (SVM), Convolutional
   Neural Networks (CNNs), Deep Belief Networks (DBNs) and related emerging
   deep learning technologies such as Transformer, adversarial neural
   network (GAN) and graph neural network (GNN) in recent years. The
   optimization techniques for diagnosing faults in rotating machinery are
   subsequently investigated. Then, a brief introduction is given to the
   papers on the application of these machine learning methods in the field
   of rotating machinery fault diagnosis, and the application
   characteristics of various methods are summarized. Finally, this survey
   discusses the problems to be solved by machine learning in fault
   diagnosis of rotating machinery and proposes an outlook.},
DOI = {10.1088/1361-6501/ad6203},
Article-Number = {102001},
ISSN = {0957-0233},
EISSN = {1361-6501},
ResearcherID-Numbers = {Wu, Yipeng/X-4984-2018
   Dong, Xiangjun/AAJ-3630-2021
   Wang, Qi/LSJ-3765-2024},
Unique-ID = {WOS:001279022900001},
}

@article{ WOS:000976730100001,
Author = {Qi, Yaping and Hu, Dan and Jiang, Yucheng and Wu, Zhenping and Zheng,
   Ming and Chen, Esther Xinyi and Liang, Yong and Sadi, Mohammad A. A. and
   Zhang, Kang and Chen, Yong P. P.},
Title = {Recent Progresses in Machine Learning Assisted Raman Spectroscopy},
Journal = {ADVANCED OPTICAL MATERIALS},
Year = {2023},
Volume = {11},
Number = {14},
Month = {JUL},
Abstract = {With the development of Raman spectroscopy and the expansion of its
   application domains, conventional methods for spectral data analysis
   have manifested many limitations. Exploring new approaches to facilitate
   Raman spectroscopy and analysis has become an area of intensifying focus
   for research. It has been demonstrated that machine learning techniques
   can more efficiently extract valuable information from spectral data,
   creating unprecedented opportunities for analytical science. This paper
   outlines traditional and more recently developed statistical methods
   that are commonly used in machine learning (ML) and ML-algorithms for
   different Raman spectroscopy-based classification and recognition
   applications. The methods include Principal Component Analysis,
   K-Nearest Neighbor, Random Forest, and Support Vector Machine, as well
   as neural network-based deep learning algorithms such as Artificial
   Neural Networks, Convolutional Neural Networks, etc. The bulk of the
   review is dedicated to the research advances in machine learning applied
   to Raman spectroscopy from several fields, including material science,
   biomedical applications, food science, and others, which reached
   impressive levels of analytical accuracy. The combination of Raman
   spectroscopy and machine learning offers unprecedented opportunities to
   achieve high throughput and fast identification in many of these
   application fields. The limitations of current studies are also
   discussed and perspectives on future research are provided.},
DOI = {10.1002/adom.202203104},
EarlyAccessDate = {APR 2023},
ISSN = {2195-1071},
ResearcherID-Numbers = {Qi, Yaping/JNR-1620-2023
   Wu, Zhenping/G-2120-2015
   Qi, Yaping (Joyce)/JNR-1620-2023
   Zhang, Kang/Y-2740-2019
   Jiang, Yucheng/K-5772-2013
   Chen, Yong/K-7017-2012},
ORCID-Numbers = {Wu, Zhenping/0000-0003-2986-8068
   Qi, Yaping (Joyce)/0000-0002-0367-3695
   Chen, Yong/0000-0002-7356-4179},
Unique-ID = {WOS:000976730100001},
}

@article{ WOS:000728330600001,
Author = {Liyew, Chalachew Muluken and Melese, Haileyesus Amsaya},
Title = {Machine learning techniques to predict daily rainfall amount},
Journal = {JOURNAL OF BIG DATA},
Year = {2021},
Volume = {8},
Number = {1},
Month = {DEC 7},
Abstract = {Predicting the amount of daily rainfall improves agricultural
   productivity and secures food and water supply to keep citizens healthy.
   To predict rainfall, several types of research have been conducted using
   data mining and machine learning techniques of different countries'
   environmental datasets. An erratic rainfall distribution in the country
   affects the agriculture on which the economy of the country depends on.
   Wise use of rainfall water should be planned and practiced in the
   country to minimize the problem of the drought and flood occurred in the
   country. The main objective of this study is to identify the relevant
   atmospheric features that cause rainfall and predict the intensity of
   daily rainfall using machine learning techniques. The Pearson
   correlation technique was used to select relevant environmental
   variables which were used as an input for the machine learning model.
   The dataset was collected from the local meteorological office at Bahir
   Dar City, Ethiopia to measure the performance of three machine learning
   techniques (Multivariate Linear Regression, Random Forest, and Extreme
   Gradient Boost). Root mean squared error and Mean absolute Error methods
   were used to measure the performance of the machine learning model. The
   result of the study revealed that the Extreme Gradient Boosting machine
   learning algorithm performed better than others.},
DOI = {10.1186/s40537-021-00545-4},
Article-Number = {153},
EISSN = {2196-1115},
ORCID-Numbers = {Liyew, Chalachew Muluken/0000-0003-4031-8032},
Unique-ID = {WOS:000728330600001},
}

@article{ WOS:000754809300001,
Author = {Bertini, Ayleen and Salas, Rodrigo and Chabert, Steren and Sobrevia,
   Luis and Pardo, Fabian},
Title = {Using Machine Learning to Predict Complications in Pregnancy: A
   Systematic Review},
Journal = {FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY},
Year = {2022},
Volume = {9},
Month = {JAN 19},
Abstract = {Introduction: Artificial intelligence is widely used in medical field,
   and machine learning has been increasingly used in health care,
   prediction, and diagnosis and as a method of determining priority.
   Machine learning methods have been features of several tools in the
   fields of obstetrics and childcare. This present review aims to
   summarize the machine learning techniques to predict perinatal
   complications.Objective: To identify the applicability and performance
   of machine learning methods used to identify pregnancy
   complications.Methods: A total of 98 articles were obtained with the
   keywords ``machine learning,{''} ``deep learning,{''} ``artificial
   intelligence,{''} and accordingly as they related to perinatal
   complications ({''}complications in pregnancy,{''} ``pregnancy
   complications{''}) from three scientific databases: PubMed, Scopus, and
   Web of Science. These were managed on the Mendeley platform and
   classified using the PRISMA method.Results: A total of 31 articles were
   selected after elimination according to inclusion and exclusion
   criteria. The features used to predict perinatal complications were
   primarily electronic medical records (48\%), medical images (29\%), and
   biological markers (19\%), while 4\% were based on other types of
   features, such as sensors and fetal heart rate. The main perinatal
   complications considered in the application of machine learning thus far
   are pre-eclampsia and prematurity. In the 31 studies, a total of sixteen
   complications were predicted. The main precision metric used is the AUC.
   The machine learning methods with the best results were the prediction
   of prematurity from medical images using the support vector machine
   technique, with an accuracy of 95.7\%, and the prediction of neonatal
   mortality with the XGBoost technique, with 99.7\% accuracy.Conclusion:
   It is important to continue promoting this area of research and promote
   solutions with multicenter clinical applicability through machine
   learning to reduce perinatal complications. This systematic review
   contributes significantly to the specialized literature on artificial
   intelligence and women's health.},
DOI = {10.3389/fbioe.2021.780389},
Article-Number = {780389},
ISSN = {2296-4185},
ResearcherID-Numbers = {Sobrevia, Luis/H-9608-2016
   Salas, Rodrigo/O-5204-2018
   Bertini Rojas, Ayleen/LZF-5894-2025},
ORCID-Numbers = {Pardo, Fabian/0000-0002-6994-0418
   Bertini, Ayleen/0000-0003-2917-4437
   },
Unique-ID = {WOS:000754809300001},
}

@article{ WOS:000620348900005,
Author = {Wang, Marcus W. H. and Goodman, Jonathan M. and Allen, Timothy E. H.},
Title = {Machine Learning in Predictive Toxicology: Recent Applications and
   Future Directions for Classification Models},
Journal = {CHEMICAL RESEARCH IN TOXICOLOGY},
Year = {2021},
Volume = {34},
Number = {2},
Pages = {217-239},
Month = {FEB 15},
Abstract = {In recent times, machine learning has become increasingly prominent in
   predictive toxicology as it has shifted from in vivo studies toward in
   silico studies. Currently, in vitro methods together with other
   computational methods such as quantitative structure-activity
   relationship modeling and absorption, distribution, metabolism, and
   excretion calculations are being used. An overview of machine learning
   and its applications in predictive toxicology is presented here,
   including support vector machines (SVMs), random forest (RF) and
   decision trees (DTs), neural networks, regression models, naive Bayes,
   k-nearest neighbors, and ensemble learning. The recent successes of
   these machine learning methods in predictive toxicology are summarized,
   and a comparison of some models used in predictive toxicology is
   presented. In predictive toxicology, SVMs, RF, and DTs are the dominant
   machine learning methods due to the characteristics of the data
   available. Lastly, this review describes the current challenges facing
   the use of machine learning in predictive toxicology and offers insights
   into the possible areas of improvement in the field.},
DOI = {10.1021/acs.chemrestox.0c00316},
ISSN = {0893-228X},
EISSN = {1520-5010},
ResearcherID-Numbers = {Goodman, Jonathan/A-2123-2008
   },
ORCID-Numbers = {Goodman, Jonathan/0000-0002-8693-9136
   Wang, Marcus/0000-0002-8407-9275
   Allen, Timothy/0000-0001-7369-0901},
Unique-ID = {WOS:000620348900005},
}

@article{ WOS:001239040700001,
Author = {Mo, Yu K. and Hahn, Matthew W. and Smith, Megan L.},
Title = {Applications of machine learning in phylogenetics},
Journal = {MOLECULAR PHYLOGENETICS AND EVOLUTION},
Year = {2024},
Volume = {196},
Month = {JUL},
Abstract = {Machine learning has increasingly been applied to a wide range of
   questions in phylogenetic inference. Supervised machine learning
   approaches that rely on simulated training data have been used to infer
   tree topologies and branch lengths, to select substitution models, and
   to perform downstream inferences of introgression and diversification.
   Here, we review how researchers have used several promising machine
   learning approaches to make phylogenetic inferences. Despite the promise
   of these methods, several barriers prevent supervised machine learning
   from reaching its full potential in phylogenetics. We discuss these
   barriers and potential paths forward. In the future, we expect that the
   application of careful network designs and data encodings will allow
   supervised machine learning to accommodate the complex processes that
   continue to confound traditional phylogenetic methods.},
DOI = {10.1016/j.ympev.2024.108066},
EarlyAccessDate = {APR 2024},
Article-Number = {108066},
ISSN = {1055-7903},
EISSN = {1095-9513},
Unique-ID = {WOS:001239040700001},
}

@article{ WOS:000671787900017,
Author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior},
Title = {Adversarial Machine Learning Attacks and Defense Methods in the Cyber
   Security Domain},
Journal = {ACM COMPUTING SURVEYS},
Year = {2021},
Volume = {54},
Number = {5},
Month = {JUN},
Abstract = {In recent years, machine learning algorithms, and more specifically deep
   learning algorithms, have been widely used in many fields, including
   cyber security. However, machine learning systems are vulnerable to
   adversarial attacks, and this limits the application of machine
   learning, especially in non-stationary, adversarial environments, such
   as the cyber security domain, where actual adversaries (e.g., malware
   developers) exist. This article comprehensively summarizes the latest
   research on adversarial attacks against security solutions based on
   machine learning techniques and illuminates the risks they pose. First,
   the adversarial attack methods are characterized based on their stage of
   occurrence, and the attacker' s goals and capabilities. Then, we
   categorize the applications of adversarial attack and defense methods in
   the cyber security domain. Finally, we highlight some characteristics
   identified in recent research and discuss the impact of recent
   advancements in other adversarial learning domains on future research
   directions in the cyber security domain. To the best of our knowledge,
   this work is the first to discuss the unique challenges of implementing
   end-to-end adversarial attacks in the cyber security domain, map them in
   a unified taxonomy, and use the taxonomy to highlight future research
   directions.},
DOI = {10.1145/3453158},
Article-Number = {108},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Rokach, Lior/F-8247-2010},
ORCID-Numbers = {Rosenberg, Ishai/0000-0003-3509-4329
   },
Unique-ID = {WOS:000671787900017},
}

@article{ WOS:000708780800001,
Author = {Kayhan, Behice Meltem and Yildiz, Gokalp},
Title = {Reinforcement learning applications to machine scheduling problems: a
   comprehensive literature review},
Journal = {JOURNAL OF INTELLIGENT MANUFACTURING},
Year = {2023},
Volume = {34},
Number = {3},
Pages = {905-929},
Month = {MAR},
Abstract = {Reinforcement learning (RL) is one of the most remarkable branches of
   machine learning and attracts the attention of researchers from numerous
   fields. Especially in recent years, the RL methods have been applied to
   machine scheduling problems and are among the top five most encouraging
   methods for scheduling literature. Therefore, in this study, a
   comprehensive literature review about RL methods applications to machine
   scheduling problems was conducted. In this regard, Scopus and Web of
   Science databases were searched very inclusively using the proper
   keywords. As a result of the comprehensive research, 80 papers were
   found, published between 1995 and 2020. These papers were analyzed
   considering different aspects of the problem such as applied algorithms,
   machine environments, job and machine characteristics, objectives,
   benchmark methods, and a detailed classification scheme was constructed.
   Job shop scheduling, unrelated parallel machine scheduling, and single
   machine scheduling problems were found as the most studied problem type.
   The main contributions of the study are to examine essential aspects of
   reinforcement learning in machine scheduling problems, identify the most
   frequently investigated problem types, objectives, and constraints, and
   reveal the deficiencies and promising areas in the related literature.
   This study can help researchers who wish to study in this field through
   the comprehensive analysis of the related literature.},
DOI = {10.1007/s10845-021-01847-3},
EarlyAccessDate = {OCT 2021},
ISSN = {0956-5515},
EISSN = {1572-8145},
ResearcherID-Numbers = {yildiz, gokalp/O-7153-2019
   Kayhan, Behice Meltem/IWL-9473-2023
   },
ORCID-Numbers = {Kayhan, Behice Meltem/0000-0001-6881-2580},
Unique-ID = {WOS:000708780800001},
}

@article{ WOS:000880508300001,
Author = {Greis, Noel P. and Nogueira, Monica L. and Bhattacharya, Sambit and
   Spooner, Catherine and Schmitz, Tony},
Title = {Stability modeling for chatter avoidance in self-aware machining: an
   application of physics-guided machine learning},
Journal = {JOURNAL OF INTELLIGENT MANUFACTURING},
Year = {2023},
Volume = {34},
Number = {1},
Pages = {387-413},
Month = {JAN},
Abstract = {Physics-guided machine learning (PGML) offers a new approach to
   stability modeling during machining that leverages experimental data
   generated during the machining process while incorporating decades of
   theoretical process modeling efforts. This approach addresses specific
   limitations of machine learning models and physics-based models
   individually. Data-driven machine learning models are typically black
   box models that do not provide deep insight into the underlying physics
   and do not reflect physical constraints for the modeled system,
   sometimes yielding solutions that violate physical laws or operational
   constraints. In addition, acquiring the large amounts of manufacturing
   data needed for machine learning modeling can be costly. On the other
   hand, many physical processes are not completely understood by domain
   experts and have a high degree of uncertainty. Physics-based models must
   make simplifying assumptions that can compromise prediction accuracy.
   This research explores whether data generated by an uncertain
   physics-based milling stability model that is used to train a
   physics-guided machine learning stability model, and then updated with
   measured data, domain knowledge, and theory-based knowledge provides a
   useful approximation to the unknown true stability model for a specific
   set of factory operating conditions. Four novel strategies for updating
   the machine learning model with experimental data are explored. These
   updating strategies differ in their assumptions about and implementation
   of the type of physics-based knowledge included in the PGML model. Using
   a simulation experiment, these strategies achieve useful approximations
   of the underlying true stability model while reducing the number of
   experimental measurements required for model update.},
DOI = {10.1007/s10845-022-01999-w},
EarlyAccessDate = {NOV 2022},
ISSN = {0956-5515},
EISSN = {1572-8145},
ResearcherID-Numbers = {Spooner, Catherine/F-5761-2013
   },
ORCID-Numbers = {Nogueira, Monica/0000-0003-2270-2129
   Greis, Noel/0000-0002-5557-8155
   Schmitz, Tony L/0000-0001-9955-5775},
Unique-ID = {WOS:000880508300001},
}

@article{ WOS:000787328500007,
Author = {Wang, Junye and Bretz, Michael and Dewan, M. Ali Akber and Delavar,
   Mojtaba Aghajani},
Title = {Machine learning in modelling land-use and land cover-change (LULCC):
   Current status, challenges and prospects},
Journal = {SCIENCE OF THE TOTAL ENVIRONMENT},
Year = {2022},
Volume = {822},
Month = {MAY 20},
Abstract = {Land-use and land-cover change (LULCC) are of importance in natural
   resource management, environmental modelling and assessment, and
   agricultural production management. However, LULCC detection and
   modelling is a complex, data-driven process in the remote sensing field
   due to the processing of massive historical and current data, real-time
   interaction of scenario data, and spatial environmental data. In this
   paper, we review principles and methods of LULCC modelling, using
   machine learning and beyond, such as traditional cellular automata (CA).
   Then, we examine the characteristics, capabilities, limitations, and
   perspectives of machine learning. Machine learning has not yet been
   dramatic in modelling LULCC, such as urbanization prediction and crop
   yield prediction because competition and transition between land cover
   types are dynamic at a local scale under varying natural drivers and
   human activities. Upcoming challenges of machine learning in modelling
   LULCC remain in the detection and prediction of LULC evolutionary
   processes if considering their applicability and feasibility, such as
   the spatio-temporal transition mechanisms to describe occurrence,
   transition, spreading, and spatial patterns of changes, availability of
   training data of all the change drivers, particularly sequence data, and
   identification and inclusion of local ecological, hydrological, and
   social-economic drivers in addressing the spectral feature change. This
   review points out the need for multidisciplinary research beyond image
   processing and pattern recognition of machine learning in accelerating
   and advancing studies of LULCC modelling. Despite this, we believe that
   machine learning has strong potentials to incorporate new exploratory
   variables in modelling LULCC through expanding remote sensing big data
   and advancing transient algorithms.},
DOI = {10.1016/j.scitotenv.2022.153559},
EarlyAccessDate = {FEB 2022},
Article-Number = {153559},
ISSN = {0048-9697},
EISSN = {1879-1026},
ResearcherID-Numbers = {Wang, Junye/AAG-6657-2019},
ORCID-Numbers = {Aghajani Delavar, Mojtaba/0000-0003-0784-9130
   Wang, Junye/0000-0001-5562-1400
   },
Unique-ID = {WOS:000787328500007},
}

@article{ WOS:000472029100018,
Author = {Yang, Kai and Xu, Xinyi and Yang, Benjamin and Cook, Brian and Ramos,
   Herbert and Krishnan, N. M. Anoop and Smedskjaer, Morten M. and Hoover,
   Christian and Bauchy, Mathieu},
Title = {Predicting the Young's Modulus of Silicate Glasses using High-Throughput
   Molecular Dynamics Simulations and Machine Learning},
Journal = {SCIENTIFIC REPORTS},
Year = {2019},
Volume = {9},
Month = {JUN 19},
Abstract = {The application of machine learning to predict materials' properties
   usually requires a large number of consistent data for training.
   However, experimental datasets of high quality are not always available
   or self-consistent. Here, as an alternative route, we combine machine
   learning with high-throughput molecular dynamics simulations to predict
   theYoung's modulus of silicate glasses. We demonstrate that this
   combined approach offers good and reliable predictions over the entire
   compositional domain. By comparing the performances of select machine
   learning algorithms, we discuss the nature of the balance between
   accuracy, simplicity, and interpretability in machine learning.},
DOI = {10.1038/s41598-019-45344-3},
Article-Number = {8739},
ISSN = {2045-2322},
ResearcherID-Numbers = {Krishnan, N. M. Anoop/AAI-6494-2020
   Smedskjaer, Morten/E-9631-2013
   Hoover, Christian/F-7449-2012
   Bauchy, Mathieu/G-3675-2010},
ORCID-Numbers = {Hoover, Christian/0000-0002-0073-2768
   Smedskjaer, Morten/0000-0003-0476-2021
   Bauchy, Mathieu/0000-0003-4600-0631
   Yang, Kai/0000-0002-7188-8049
   Naduvath Mana, Anoop Krishnan/0000-0003-1500-4947
   },
Unique-ID = {WOS:000472029100018},
}

@article{ WOS:000462604300001,
Author = {Ho, Daniel Sik Wai and Schierding, William and Wake, Melissa and
   Saffery, Richard and O'Sullivan, Justin},
Title = {Machine Learning SNP Based Prediction for Precision Medicine},
Journal = {FRONTIERS IN GENETICS},
Year = {2019},
Volume = {10},
Month = {MAR 27},
Abstract = {In the past decade, precision genomics based medicine has emerged to
   provide tailored and effective healthcare for patients depending upon
   their genetic features. Genome Wide Association Studies have also
   identified population based risk genetic variants for common and complex
   diseases. In order to meet the full promise of precision medicine,
   research is attempting to leverage our increasing genomic understanding
   and further develop personalized medical healthcare through ever more
   accurate disease risk prediction models. Polygenic risk scoring and
   machine learning are two primary approaches for disease risk prediction.
   Despite recent improvements, the results of polygenic risk scoring
   remain limited due to the approaches that are currently used. By
   contrast, machine learning algorithms have increased predictive
   abilities for complex disease risk. This increase in predictive
   abilities results from the ability of machine learning algorithms to
   handle multi-dimensional data. Here, we provide an overview of polygenic
   risk scoring and machine learning in complex disease risk prediction. We
   highlight recent machine learning application developments and describe
   how machine learning approaches can lead to improved complex disease
   prediction, which will help to incorporate genetic features into future
   personalized healthcare. Finally, we discuss how the future application
   of machine learning prediction models might help manage complex disease
   by providing tissue-specific targets for customized, preventive
   interventions.},
DOI = {10.3389/fgene.2019.00267},
Article-Number = {267},
EISSN = {1664-8021},
ResearcherID-Numbers = {Saffery, Richard/GLS-1976-2022
   O'Sullivan, Justin/HDO-1639-2022
   Schierding, William/ACD-9101-2022
   Wake, Melissa/J-1396-2012},
ORCID-Numbers = {Saffery, Richard/0000-0002-9510-4181
   Schierding, William/0000-0001-5659-2701
   Ho, Daniel/0000-0002-3699-0419
   O'Sullivan, Justin/0000-0003-2927-450X
   },
Unique-ID = {WOS:000462604300001},
}

@article{ WOS:000438855100013,
Author = {Cui, Laizhong and Yang, Shu and Chen, Fei and Ming, Zhong and Lu, Nan
   and Qin, Jing},
Title = {A survey on application of machine learning for Internet of Things},
Journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS},
Year = {2018},
Volume = {9},
Number = {8},
Pages = {1399-1417},
Month = {AUG},
Abstract = {Internet of Things (IoT) has become an important network paradigm and
   there are lots of smart devices connected by IoT. IoT systems are
   producing massive data and thus more and more IoT applications and
   services are emerging. Machine learning, as an another important area,
   has obtained a great success in several research fields such as computer
   vision, computer graphics, natural language processing, speech
   recognition, decision-making, and intelligent control. It has also been
   introduced in networking research. Many researches study how to utilize
   machine learning to solve networking problems, including routing,
   traffic engineering, resource allocation, and security. Recently, there
   has been a rising trend of employing machine learning to improve IoT
   applications and provide IoT services such as traffic engineering,
   network management, security, Internet traffic classification, and
   quality of service optimization. This survey paper focuses on providing
   an overview of the application of machine learning in the domain of IoT.
   We provide a comprehensive survey highlighting the recent progresses in
   machine learning techniques for IoT and describe various IoT
   applications. The application of machine learning for IoT enables users
   to obtain deep analytics and develop efficient intelligent IoT
   applications. This paper is different from the previously published
   survey papers in terms of focus, scope, and breadth; specifically, we
   have written this paper to emphasize the application of machine learning
   for IoT and the coverage of most recent advances. This paper has made an
   attempt to cover the major applications of machine learning for IoT and
   the relevant techniques, including traffic profiling, IoT device
   identification, security, edge computing infrastructure, network
   management and typical IoT applications. We also make a discussion on
   research challenges and open issues.},
DOI = {10.1007/s13042-018-0834-5},
ISSN = {1868-8071},
EISSN = {1868-808X},
ResearcherID-Numbers = {Chen, Fei/HNQ-3717-2023
   Qin, Jing/J-9807-2016
   Cui, Laizhong/AAX-9571-2020},
ORCID-Numbers = {Qin, Jing/0000-0002-7059-0929
   },
Unique-ID = {WOS:000438855100013},
}

@article{ WOS:000418675900003,
Author = {Huang, Shujun and Cai, Nianguang and Pacheco, Pedro Penzuti and
   Narandes, Shavira and Wang, Yang and Xu, Wayne},
Title = {Applications of Support Vector Machine (SVM) Learning in Cancer Genomics},
Journal = {CANCER GENOMICS \& PROTEOMICS},
Year = {2018},
Volume = {15},
Number = {1},
Pages = {41-51},
Month = {JAN-FEB},
Abstract = {Machine learning with maximization (support) of separating margin
   (vector), called support vector machine (SVM) learning, is a powerful
   classification tool that has been used for cancer genomic classification
   or subtyping. Today, as advancements in high-throughput technologies
   lead to production of large amounts of genomic and epigenomic data, the
   classification feature of SVMs is expanding its use in cancer genomics,
   leading to the discovery of new biomarkers, new drug targets, and a
   better understanding of cancer driver genes. Herein we reviewed the
   recent progress of SVMs in cancer genomic studies. We intend to
   comprehend the strength of the SVM learning and its future perspective
   in cancer genomic applications.},
DOI = {10.21873/cgp.20063},
ISSN = {1109-6535},
EISSN = {1790-6245},
ResearcherID-Numbers = {xu, wayne/ABA-1961-2020
   },
ORCID-Numbers = {xu, wayne/0000-0003-0484-7340},
Unique-ID = {WOS:000418675900003},
}

@article{ WOS:000492778000001,
Author = {Koromina, Maria and Pandi, Maria-Theodora and Patrinos, George P.},
Title = {Rethinking Drug Repositioning and Development with Artificial
   Intelligence, Machine Learning, and Omics},
Journal = {OMICS-A JOURNAL OF INTEGRATIVE BIOLOGY},
Year = {2019},
Volume = {23},
Number = {11},
Pages = {539-548},
Month = {NOV 1},
Abstract = {Pharmaceutical industry and the art and science of drug development are
   sorely in need of novel transformative technologies in the current age
   of digital health and artificial intelligence (AI). Often described as
   game-changing technologies, AI and machine learning algorithms have
   slowly but surely begun to revolutionize pharmaceutical industry and
   drug development over the past 5 years. In this expert review, we
   describe the most frequently used machine learning algorithms in drug
   development pipelines and the -omics databases well poised to support
   machine learning and drug discovery. Subsequently, we analyze the
   emerging new computational approaches to drug discovery and the in
   silico pipelines for drug repositioning and the synergies among -omics
   system sciences, AI and machine learning. As with system sciences, AI
   and machine learning embody a system scale and Big Data driven vision
   for drug discovery and development. We conclude with a future outlook on
   the ways in which machine learning approaches can be implemented to
   buttress and expedite drug discovery and precision medicine. As AI and
   machine learning are rapidly entering pharmaceutical industry and the
   art and science of drug development, we need to critically examine the
   attendant prospects and challenges to benefit patients and public
   health.},
DOI = {10.1089/omi.2019.0151},
EarlyAccessDate = {OCT 2019},
ISSN = {1536-2310},
EISSN = {1557-8100},
Unique-ID = {WOS:000492778000001},
}

@article{ WOS:001085771300006,
Author = {Kaur, Harleen and Kumari, Vinita},
Title = {Predictive modelling and analytics for diabetes using a machine learning
   approach},
Journal = {APPLIED COMPUTING AND INFORMATICS},
Year = {2022},
Volume = {18},
Number = {1/2},
Pages = {90-100},
Month = {MAR 1},
Abstract = {Diabetes is a major metabolic disorder which can affect entire body
   system adversely. Undiagnosed diabetes can increase the risk of cardiac
   stroke, diabetic nephropathy and other disorders. All over the world
   millions of people are affected by this disease. Early detection of
   diabetes is very important to maintain a healthy life. This disease is a
   reason of global concern as the cases of diabetes are rising rapidly.
   Machine learning (ML) is a computational method for automatic learning
   from experience and improves the performance to make more accurate
   predictions. In the current research we have utilized machine learning
   technique in Pima Indian diabetes dataset to develop trends and detect
   patterns with risk factors using R data manipulation tool. To classify
   the patients into diabetic and non-diabetic we have developed and
   analyzed five different predictive models using R data manipulation
   tool. For this purpose we used supervised machine learning algorithms
   namely linear kernel support vector machine (SVM-linear), radial basis
   function (RBF) kernel support vector machine, k-nearest neighbour
   (k-NN), artificial neural network (ANN) and multifactor dimensionality
   reduction (MDR).},
DOI = {10.1016/j.aci.2018.12.004},
ISSN = {2634-1964},
EISSN = {2210-8327},
ORCID-Numbers = {Kaur, Dr. Harleen/0000-0001-9780-2138},
Unique-ID = {WOS:001085771300006},
}

@article{ WOS:000908510300001,
Author = {Ayano, Yehualashet Megersa and Schwenker, Friedhelm and Dufera, Bisrat
   Derebssa and Debelee, Taye Girma},
Title = {Interpretable Machine Learning Techniques in ECG-Based Heart Disease
   Classification: A Systematic Review},
Journal = {DIAGNOSTICS},
Year = {2023},
Volume = {13},
Number = {1},
Month = {JAN},
Abstract = {Heart disease is one of the leading causes of mortality throughout the
   world. Among the different heart diagnosis techniques, an
   electrocardiogram (ECG) is the least expensive non-invasive procedure.
   However, the following are challenges: the scarcity of medical experts,
   the complexity of ECG interpretations, the manifestation similarities of
   heart disease in ECG signals, and heart disease comorbidity. Machine
   learning algorithms are viable alternatives to the traditional diagnoses
   of heart disease from ECG signals. However, the black box nature of
   complex machine learning algorithms and the difficulty in explaining a
   model's outcomes are obstacles for medical practitioners in having
   confidence in machine learning models. This observation paves the way
   for interpretable machine learning (IML) models as diagnostic tools that
   can build a physician's trust and provide evidence-based diagnoses.
   Therefore, in this systematic literature review, we studied and analyzed
   the research landscape in interpretable machine learning techniques by
   focusing on heart disease diagnosis from an ECG signal. In this regard,
   the contribution of our work is manifold; first, we present an elaborate
   discussion on interpretable machine learning techniques. In addition, we
   identify and characterize ECG signal recording datasets that are readily
   available for machine learning-based tasks. Furthermore, we identify the
   progress that has been achieved in ECG signal interpretation using IML
   techniques. Finally, we discuss the limitations and challenges of IML
   techniques in interpreting ECG signals.},
DOI = {10.3390/diagnostics13010111},
Article-Number = {111},
EISSN = {2075-4418},
ResearcherID-Numbers = {Schwenker, Friedhelm/G-6069-2015
   Debelee, Taye Girma/IUP-8447-2023
   Debelee, Taye/IUP-8447-2023
   Ayano, Yehualashet Megersa/AAD-4744-2022
   },
ORCID-Numbers = {Schwenker, Friedhelm/0000-0001-5118-0812
   Debelee, Taye Girma/0000-0002-0876-2021
   Ayano, Yehualashet Megersa/0000-0001-5591-2240
   Dufera, Bisrat/0000-0003-3377-0661},
Unique-ID = {WOS:000908510300001},
}

@article{ WOS:000505643500038,
Author = {Zhu, Shan and Sun, Xinyang and Gao, Xiaoyang and Wang, Jianrong and
   Zhao, Naiqin and Sha, Junwei},
Title = {Equivalent circuit model recognition of electrochemical impedance
   spectroscopy via machine learning},
Journal = {JOURNAL OF ELECTROANALYTICAL CHEMISTRY},
Year = {2019},
Volume = {855},
Month = {DEC 15},
Abstract = {Electrochemical impedance spectroscopy (EIS) is an effective method for
   studying electrochemical systems. The interpretation of EIS is the
   biggest challenge in this technology, which requires reasonable
   modeling. To overcome the subjectivity of human analysis, this work uses
   machine learning to carry out EIS model recognition. Raw EIS data and
   their equivalent circuit models are collected from the literature, and
   the support vector machine (SVM) is used to analyze these data.
   Comparing with other machine learning algorithms, SVM achieves the best
   comprehensive performance in this database. As a result, the optimized
   SVM model can efficiently figure out the most suitable equivalent
   circuit model of the given EIS spectrum. This study demonstrates the
   great potential of machine learning in electrochemical researches.},
DOI = {10.1016/j.jelechem.2019.113627},
Article-Number = {113627},
ISSN = {1572-6657},
EISSN = {1873-2569},
ResearcherID-Numbers = {Zhu, Shan/HZI-6891-2023
   Wang, Jianrong/NXC-2071-2025
   zhao, naiqin/C-6150-2014
   Sha, Junwei/M-5324-2016},
Unique-ID = {WOS:000505643500038},
}

@article{ WOS:000484832800014,
Author = {Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and
   Liu, Vincent X. and Doshi-Velez, Finale and Jung, Kenneth and Heller,
   Katherine and Kale, David and Saeed, Mohammed and Ossorio, Pilar N. and
   Thadaney-Israni, Sonoo and Goldenberg, Anna},
Title = {Do no harm: a roadmap for responsible machine learning for health care},
Journal = {NATURE MEDICINE},
Year = {2019},
Volume = {25},
Number = {9},
Pages = {1337-1340},
Month = {SEP},
Abstract = {Interest in machine-learning applications within medicine has been
   growing, but few studies have progressed to deployment in patient care.
   We present a framework, context and ultimately guidelines for
   accelerating the translation of machine-learning-based interventions in
   health care. To be successful, translation will require a team of
   engaged stakeholders and a systematic process from beginning (problem
   formulation) to end (widespread deployment).},
DOI = {10.1038/s41591-019-0548-6},
ISSN = {1078-8956},
EISSN = {1546-170X},
ResearcherID-Numbers = {liu, V/KIA-9904-2024
   Saria, Suchi/D-1809-2013},
ORCID-Numbers = {Sendak, Mark/0000-0001-5828-4497
   },
Unique-ID = {WOS:000484832800014},
}

@article{ WOS:000457022100009,
Author = {Dai, Bo and Gu, Chongshi and Zhao, Erfeng and Zhu, Kai and Cao, Wenhan
   and Qin, Xiangnan},
Title = {Improved online sequential extreme learning machine for identifying
   crack behavior in concrete dam},
Journal = {ADVANCES IN STRUCTURAL ENGINEERING},
Year = {2019},
Volume = {22},
Number = {2},
Pages = {402-412},
Month = {JAN},
Abstract = {Prediction models are essential in dam crack behavior identification.
   Prototype monitoring data arrive sequentially in dam safety monitoring.
   Given such characteristic, sequential learning algorithms are preferred
   over batch learning algorithms as they do not require retraining
   whenever new data are received. A new methodology using the genetic
   optimized online sequential extreme learning machine and bootstrap
   confidence intervals is proposed as a practical tool for identifying
   concrete dam crack behavior. First, online sequential extreme learning
   machine is adopted to build an online prediction model of crack
   behavior. The characteristic vector of crack behavior, which is taken as
   the online sequential extreme learning machine input, is extracted by
   the statistical model. A genetic algorithm is introduced to optimize the
   input weights and biases of online sequential extreme learning machine.
   Second, the BC(a )method is proposed to produce confidence intervals
   based on the improved online sequential extreme learning machine
   prediction. The improved online sequential extreme learning machine for
   identifying crack behavior is then built. Third, the crack behavior of
   an actual concrete dam is taken as an example. The capability of the
   built model for predicting dam crack opening is evaluated. The
   comparative results demonstrate that the improved online sequential
   extreme learning machine can provide highly accurate forecasts and
   reasonably identify crack behavior.},
DOI = {10.1177/1369433218788635},
ISSN = {1369-4332},
EISSN = {2048-4011},
ResearcherID-Numbers = {Cao, Wenhan/HLH-7629-2023
   Qin, Xiangnan/JED-4857-2023},
Unique-ID = {WOS:000457022100009},
}

@article{ WOS:000818095000001,
Author = {Oviedo, Felipe and Ferres, Juan Lavista and Buonassisi, Tonio and
   Butler, Keith T.},
Title = {Interpretable and Explainable Machine Learning for Materials Science and
   Chemistry},
Journal = {ACCOUNTS OF MATERIALS RESEARCH},
Year = {2022},
Volume = {3},
Number = {6},
Pages = {597-607},
Month = {JUN 24},
Abstract = {CONSPECTUS: Machine learning has become a common and powerful tool in
   materials research. As more data become available, with the use of
   high-performance computing and high-throughput experimentation, machine
   learning has proven potential to accelerate scientific research and
   technology development. Though the uptake of data-driven approaches for
   materials science is at an exciting, early stage, to realize the true
   potential of machine learning models for successful scientific
   discovery, they must have qualities beyond purely predictive power. The
   predictions and inner workings of models should provide a certain degree
   of explainability by human experts, permitting the identification of
   potential model issues or limitations, building trust in model
   predictions, and unveiling unexpected correlations that may lead to
   scientific insights. In this work, we summarize applications of
   interpretability and explainability techniques for materials science and
   chemistry and discuss how these techniques can improve the outcome of
   scientific studies. We start by defining the fundamental concepts of
   interpretability and explainability in machine learning and making them
   less abstract by providing examples in the field. We show how
   interpretability in scientific machine learning has additional
   constraints compared to general applications. Building upon formal
   definitions in machine learning, we formulate the basic trade-offs among
   the explainability, completeness, and scientific validity of model
   explanations in scientific problems. In the context of these trade-offs,
   we discuss how interpretable models can be constructed, what insights
   they provide, and what drawbacks they have. We present numerous examples
   of the application of interpretable machine learning in a variety of
   experimental and simulation studies, encompassing first-principles
   calculations, physicochemical characterization, materials development,
   and integration into complex systems. We discuss the varied impacts and
   uses of interpretabiltiy in these cases according to the nature and
   constraints of the scientific study of interest. We discuss various
   challenges for interpretable machine learning in materials science and,
   more broadly, in scientific settings. In particular, we emphasize the
   risks of inferring causation or reaching generalization by purely
   interpreting machine learning models and the need for uncertainty
   estimates for model explanations. Finally, we showcase a number of
   exciting developments in other fields that could benefit
   interpretability in material science problems. Adding interpretability
   to a machine learning model often requires no more technical know-how
   than building the model itself. By providing concrete examples of
   studies (many with associated open source code and data), we hope that
   this Account will encourage all practitioners of machine learning in
   materials science to look deeper into their models.},
DOI = {10.1021/accountsmr.1c00244},
EISSN = {2643-6728},
ResearcherID-Numbers = {Buonassisi, Tonio/J-2723-2012
   },
ORCID-Numbers = {Butler, Keith/0000-0001-5432-5597},
Unique-ID = {WOS:000818095000001},
}

@article{ WOS:000649679300001,
Author = {Nabi, Wafa and Bansal, Agam and Xu, Bo},
Title = {Applications of artificial intelligence and machine learning approaches
   in echocardiography},
Journal = {ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ULTRASOUND AND ALLIED
   TECHNIQUES},
Year = {2021},
Volume = {38},
Number = {6},
Pages = {982-992},
Month = {JUN},
Abstract = {Artificial intelligence and machine learning approaches have become
   increasingly applied in the field of echocardiography to streamline
   diagnostic and prognostic assessments, and to support treatment
   decisions. Artificial intelligence and machine learning have been
   applied to aid image acquisition and automation. They have also been
   applied to the integration of clinical and imaging data. Applications of
   artificial intelligence and machine learning approaches in
   echocardiography in conjunction with health information databases may be
   promising in improving the classification and treatment of many cardiac
   conditions. This review article provides an overview of the applications
   of artificial intelligence and machine learning approaches in
   echocardiography.},
DOI = {10.1111/echo.15048},
EarlyAccessDate = {MAY 2021},
ISSN = {0742-2822},
EISSN = {1540-8175},
ResearcherID-Numbers = {Xu, Bo/ACS-4641-2022
   },
ORCID-Numbers = {Nabi, Wafa/0000-0003-2818-2320
   Xu, Bo/0000-0002-2985-7468},
Unique-ID = {WOS:000649679300001},
}

@article{ WOS:000471070400002,
Author = {Li, Guangyue and Dong, Yijie and Reetz, Manfred T.},
Title = {Can Machine Learning Revolutionize Directed Evolution of Selective
   Enzymes?},
Journal = {ADVANCED SYNTHESIS \& CATALYSIS},
Year = {2019},
Volume = {361},
Number = {11, SI},
Pages = {2377-2386},
Month = {JUN 6},
Abstract = {Machine learning as a form of artificial intelligence consists of
   algorithms and statistical models for improving computer performance for
   different tasks. Training data are utilized for making decisions and
   predictions. Since directed evolution of enzymes produces huge amounts
   of potential training data, machine learning seems to be ideally suited
   to support this protein engineering technique. Machine learning has been
   used in protein science for a long time with different purposes. This
   mini-review focuses on the utility of machine learning as an aid in the
   directed evolution of selective enzymes. Recent studies have shown that
   the algorithms ASRA and Innov'SAR are well suited as guides when
   performing saturation mutagenesis at sites lining the binding pocket for
   enhancing stereoselectivity and activity.},
DOI = {10.1002/adsc.201900149},
ISSN = {1615-4150},
EISSN = {1615-4169},
Unique-ID = {WOS:000471070400002},
}

@article{ WOS:000855316400001,
Author = {Yu, Chen Xiao and Ying, Song and Min, Zhang Xiao and Feng, Gao},
Title = {Research Progress and Trend of the Machine Learning based on Fusion},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2022},
Volume = {13},
Number = {7},
Pages = {1-7},
Month = {JUL},
Abstract = {Machine learning is widely used in the data processing including data
   classification, data regression, data mining and so on, and based on a
   single type of machine learning technology, it is often difficult to
   meet the requirements of data processing; in recent years, the machine
   learning based on fusion has become an important approach to improve
   data processing effect, and at the same time, corresponding summary
   study is relatively limited. In this study, we summarize and compare
   different types of fusion machine learning such as ensemble learning,
   federated learning and transfer learning from the perspectives of
   classification, principle and characteristics, and try to explore the
   research development trend, in order to provide effective reference for
   subsequent related research and application; furthermore, as an
   application of fusion machine learning,we also conduct a study on the
   modeling optimization for car service complaint text classification.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Unique-ID = {WOS:000855316400001},
}

@article{ WOS:000466934400012,
Author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier},
Title = {Machine Learning for Semi Linear PDEs},
Journal = {JOURNAL OF SCIENTIFIC COMPUTING},
Year = {2019},
Volume = {79},
Number = {3},
Pages = {1667-1712},
Month = {JUN},
Abstract = {Recent machine learning algorithms dedicated to solving semi-linear PDEs
   are improved by using different neural network architectures and
   different parameterizations. These algorithms are compared to a new one
   that solves a fixed point problem by using deep learning techniques.
   This new algorithm appears to be competitive in terms of accuracy with
   the best existing algorithms.},
DOI = {10.1007/s10915-019-00908-3},
ISSN = {0885-7474},
EISSN = {1573-7691},
ORCID-Numbers = {warin, xavier/0000-0002-1784-2875},
Unique-ID = {WOS:000466934400012},
}

@article{ WOS:000882795100003,
Author = {Muhammad and Kennedy, John and Lim, C. W.},
Title = {Machine learning and deep learning in phononic crystals and
   metamaterials-A review},
Journal = {MATERIALS TODAY COMMUNICATIONS},
Year = {2022},
Volume = {33},
Month = {DEC},
Abstract = {Machine learning (ML), as a component of artificial intelligence,
   encourages structural design exploration which leads to new
   technological advancements. By developing and generating data-driven
   methodologies that supplement conventional physics and formula-based
   approaches, deep learning (DL), a subset of machine learning offers an
   efficient way to understand and harness artificial materials and
   structures. Recently, acoustic and mechanics communities have observed a
   surge of research interest in implementing machine learning and deep
   learning methods in the design and optimization of artificial materials.
   In this review we evaluate the recent developments and present a
   state-of-the-art literature survey in machine learning and deep learning
   based phononic crystals and metamaterial designs by giving historical
   context, discussing network architectures and working principles. We
   also explain the application of these network architectures adopted for
   design and optimization of artificial structures. Since this
   multidisciplinary research field is evolving, a summary of the future
   prospects is also covered. This review article serves to update the
   acoustics, mechanics, physics, material science and deep learning
   communities about the recent developments in this newly emerging
   research direction},
DOI = {10.1016/j.mtcomm.2022.104606},
EarlyAccessDate = {OCT 2022},
Article-Number = {104606},
EISSN = {2352-4928},
ResearcherID-Numbers = {Gulzari, Muhammad/AAV-2463-2020
   Kennedy, John/LEN-2948-2024
   },
ORCID-Numbers = {MUHAMMAD, -/0000-0003-3492-0123
   Kennedy, John/0000-0002-8639-9504},
Unique-ID = {WOS:000882795100003},
}

@article{ WOS:001372855700001,
Author = {Karako, Kenji and Tang, Wei},
Title = {Applications of and issues with machine learning in medicine: Bridging
   the gap with explainable AI},
Journal = {BIOSCIENCE TRENDS},
Year = {2024},
Volume = {18},
Number = {6},
Pages = {497-504},
Month = {DEC},
Abstract = {In recent years, machine learning, and particularly deep learning, has
   shown remarkable potential in various fields, including medicine.
   Advanced techniques like convolutional neural networks and transformers
   have enabled high-performance predictions for complex problems, making
   machine learning a valuable tool in medical decision-making. From
   predicting postoperative complications to assessing disease risk,
   machine learning has been actively used to analyze patient data and
   assist healthcare professionals. However, the ``black box{''} problem,
   wherein the internal workings of machine learning models are opaque and
   difficult to interpret, poses a significant challenge in medical
   applications. The lack of transparency may hinder trust and acceptance
   by clinicians and patients, making the development of explainable AI
   (XAI) techniques essential. XAI aims to provide both global and local
   explanations for machine learning models, offering insights into how
   predictions are made and which factors influence these outcomes. In this
   article, we explore various applications of machine learning in
   medicine, describe commonly used algorithms, and discuss explainable AI
   as a promising solution to enhance the interpretability of these models.
   By integrating explainability into machine learning, we aim to ensure
   its ethical and practical application in healthcare, ultimately
   improving patient outcomes and supporting personalized treatment
   strategies.},
DOI = {10.5582/bst.2024.01342},
EarlyAccessDate = {DEC 2024},
ISSN = {1881-7815},
EISSN = {1881-7823},
Unique-ID = {WOS:001372855700001},
}

@article{ WOS:000507900400001,
Author = {Bao, Yang and Ke, Bin and Li, Bin and Yu, Y. Julia and Zhang, Jie},
Title = {Detecting Accounting Fraud in Publicly Traded US Firms Using a Machine
   Learning Approach},
Journal = {JOURNAL OF ACCOUNTING RESEARCH},
Year = {2020},
Volume = {58},
Number = {1},
Pages = {199-235},
Month = {MAR},
Abstract = {We develop a state-of-the-art fraud prediction model using a machine
   learning approach. We demonstrate the value of combining domain
   knowledge and machine learning methods in model building. We select our
   model input based on existing accounting theories, but we differ from
   prior accounting research by using raw accounting numbers rather than
   financial ratios. We employ one of the most powerful machine learning
   methods, ensemble learning, rather than the commonly used method of
   logistic regression. To assess the performance of fraud prediction
   models, we introduce a new performance evaluation metric commonly used
   in ranking problems that is more appropriate for the fraud prediction
   task. Starting with an identical set of theory-motivated raw accounting
   numbers, we show that our new fraud prediction model outperforms two
   benchmark models by a large margin: the Dechow et al. logistic
   regression model based on financial ratios, and the Cecchini et al.
   support-vector-machine model with a financial kernel that maps raw
   accounting numbers into a broader set of ratios.},
DOI = {10.1111/1475-679X.12292},
EarlyAccessDate = {JAN 2020},
ISSN = {0021-8456},
EISSN = {1475-679X},
ResearcherID-Numbers = {Li, Bin/KUD-9533-2024
   Ke, Bin/A-5214-2016
   Bao, Yangming/MZR-3027-2025
   Zhang, Jiehuang/MGV-4525-2025},
ORCID-Numbers = {Yu, Yingri/0000-0003-4260-565X
   },
Unique-ID = {WOS:000507900400001},
}

@article{ WOS:000874966900001,
Author = {Boehnlein, Amber and Diefenthaler, Markus and Sato, Nobuo and Schram,
   Malachi and Ziegler, Veronique and Fanelli, Cristiano and Hjorth-Jensen,
   Morten and Horn, Tanja and Kuchera, Michelle P. and Lee, Dean and
   Nazarewicz, Witold and Ostroumov, Peter and Orginos, Kostas and Poon,
   Alan and Wang, Xin-Nian and Scheinker, Alexander and Smith, Michael S.
   and Pang, Long-Gang},
Title = {Colloquium: Machine learning in nuclear physics},
Journal = {REVIEWS OF MODERN PHYSICS},
Year = {2022},
Volume = {94},
Number = {3},
Month = {SEP 8},
Abstract = {Advances in machine learning methods provide tools that have broad
   applicability in scientific research. These techniques are being applied
   across the diversity of nuclear physics research topics, leading to
   advances that will facilitate scientific discoveries and societal
   applications. This Colloquium provides a snapshot of nuclear physics
   research, which has been transformed by machine learning techniques.},
DOI = {10.1103/RevModPhys.94.031003},
Article-Number = {031003},
ISSN = {0034-6861},
EISSN = {1539-0756},
ResearcherID-Numbers = {Kuchera, Michelle/LIC-8128-2024
   Orginos, Kostas/Z-1237-2019
   Nazarewicz, Witold/C-2056-2012
   Lee, Dean/KFS-1456-2024
   Wang, Xin-Nian/HNR-3357-2023
   Poon, Alan/AAJ-5095-2020
   Nazarewicz, Witold/HSI-2792-2023
   },
ORCID-Numbers = {Poon, Alan/0000-0003-2684-6402
   Horn, Tanja/0000-0003-1925-9541
   Lee, Dean/0000-0002-3630-567X
   Nazarewicz, Witold/0000-0002-8084-7425
   Hjorth-Jensen, Morten/0000-0003-0174-1364
   Schram, Malachi/0000-0002-3475-2871
   Boehnlein, Amber/0000-0002-6987-5994
   Orginos, Kostas/0000-0002-3535-7865
   /0000-0002-1985-1329
   Scheinker, Alexander/0000-0002-3203-0963},
Unique-ID = {WOS:000874966900001},
}

@article{ WOS:000599992800002,
Author = {Mizgajski, Jan and Szymczak, Adrian and Morzy, Mikolaj and Augustyniak,
   Lukasz and Szymanski, Piotr and Zelasko, Piotr},
Title = {Return on Investment in Machine Learning: Crossing the Chasm between
   Academia and Business},
Journal = {FOUNDATIONS OF COMPUTING AND DECISION SCIENCES},
Year = {2020},
Volume = {45},
Number = {4},
Pages = {281-304},
Month = {DEC},
Abstract = {Academia remains the central place of machine learning education. While
   academic culture is the predominant factor influencing the way we teach
   machine learning to students, many practitioners question this culture,
   claiming the lack of alignment between academic and business
   environments. Drawing on professional experiences from both sides of the
   chasm, we describe the main points of contention, in the hope that it
   will help better align academic syllabi with the expectations towards
   future machine learning practitioners. We also provide recommendations
   for teaching of the applied aspects of machine learning.},
DOI = {10.2478/fcds-2020-0015},
ISSN = {0867-6356},
EISSN = {2300-3405},
ResearcherID-Numbers = {Szymanski, Piotr/LWJ-5638-2024
   Żelasko, Piotr/AAA-6412-2019
   Morzy, Mikołaj/I-8489-2016},
ORCID-Numbers = {Mizgajski, Jan/0000-0002-1774-3973
   Szymanski, Piotr/0000-0002-7733-3239
   },
Unique-ID = {WOS:000599992800002},
}

@article{ WOS:000635680800006,
Author = {Nearing, Grey S. and Kratzert, Frederik and Sampson, Alden Keefe and
   Pelissier, Craig S. and Klotz, Daniel and Frame, Jonathan M. and Prieto,
   Cristina and Gupta, Hoshin V.},
Title = {What Role Does Hydrological Science Play in the Age of Machine Learning?},
Journal = {WATER RESOURCES RESEARCH},
Year = {2021},
Volume = {57},
Number = {3},
Month = {MAR},
Abstract = {Y This paper is derived from a keynote talk given at the Google's 2020
   Flood Forecasting Meets Machine Learning Workshop. Recent experiments
   applying deep learning to rainfall-runoff simulation indicate that there
   is significantly more information in large-scale hydrological data sets
   than hydrologists have been able to translate into theory or models.
   While there is a growing interest in machine learning in the
   hydrological sciences community, in many ways, our community still holds
   deeply subjective and nonevidence-based preferences for models based on
   a certain type of ``process understanding{''} that has historically not
   translated into accurate theory, models, or predictions. This commentary
   is a call to action for the hydrology community to focus on developing a
   quantitative understanding of where and when hydrological process
   understanding is valuable in a modeling discipline increasingly
   dominated by machine learning. We offer some potential perspectives and
   preliminary examples about how this might be accomplished.},
DOI = {10.1029/2020WR028091},
Article-Number = {e2020WR028091},
ISSN = {0043-1397},
EISSN = {1944-7973},
ResearcherID-Numbers = {Gupta, Hoshin/D-1642-2010
   Prieto, Cristina/Y-9015-2019
   Kratzert, Frederik/KPB-3566-2024},
ORCID-Numbers = {Frame, Jonathan M./0000-0002-2533-3843
   Kratzert, Frederik/0000-0002-8897-7689
   Sampson, Alden Keefe/0000-0003-1853-3713
   Prieto, Cristina/0000-0002-6693-0396
   Klotz, Daniel/0000-0002-9843-6798
   },
Unique-ID = {WOS:000635680800006},
}

@article{ WOS:000773410500001,
Author = {Janjua, Muhammad Ramzan Saeed Ashraf and Irfan, Ahmad and Hussien,
   Mohamed and Ali, Muhammad and Saqib, Muhammad and Sulaman, Muhammad},
Title = {Machine-Learning Analysis of Small-Molecule Donors for Fullerene Based
   Organic Solar Cells},
Journal = {ENERGY TECHNOLOGY},
Year = {2022},
Volume = {10},
Number = {5},
Month = {MAY},
Abstract = {In recent years, development in organic solar cells speeds up and
   performance continuously increases. From the last few years, machine
   learning gains fame among scientists who are researching on organic
   solar cells. Herein, machine learning is used to screen the
   small-molecule donors for organic solar cells. Molecular descriptors are
   used as input to train machine models. A variety of machine-learning
   models are tested to find the suitable one. Random forest model shows
   best predictive capability (Pearson's coefficient = 0.93). New
   small-molecule donors are also designed from easily synthesizable
   building units. Their power conversion efficiencies (PCEs) are
   predicted. Potential candidates with PCE > 11\% are selected. The
   approach presented herein helps to select the efficient materials in
   short time with ease.},
DOI = {10.1002/ente.202200019},
EarlyAccessDate = {MAR 2022},
Article-Number = {2200019},
ISSN = {2194-4288},
EISSN = {2194-4296},
ResearcherID-Numbers = {Saqib, Muhammad/P-7912-2016
   Irfan, Ahmad/R-7642-2019
   Dessokey, Mohamed/GQH-3025-2022
   Janjua, Prof. Dr. Muhammad Ramzan Saeed Ashraf/ACU-2640-2022
   Sulaman, Muhammad/I-7661-2019
   },
ORCID-Numbers = {, mohamed/0000-0002-3551-9078
   Sulaman, Muhammad/0000-0003-0702-653X
   Saqib, Muhammad/0000-0001-6168-4205},
Unique-ID = {WOS:000773410500001},
}

@article{ WOS:000535945200001,
Author = {Stachl, Clemens and Pargent, Florian and Hilbert, Sven and Harari,
   Gabriella M. and Schoedel, Ramona and Vaid, Sumer and Gosling, Samuel D.
   and Buehner, Markus},
Title = {Personality Research and Assessment in the Era of Machine Learning},
Journal = {EUROPEAN JOURNAL OF PERSONALITY},
Year = {2020},
Volume = {34},
Number = {5, SI},
Pages = {613-631},
Month = {SEP},
Abstract = {The increasing availability of high-dimensional, fine-grained data about
   human behaviour, gathered from mobile sensing studies and in the form of
   digital footprints, is poised to drastically alter the way personality
   psychologists perform research and undertake personality assessment.
   These new kinds and quantities of data raise important questions about
   how to analyse the data and interpret the results appropriately. Machine
   learning models are well suited to these kinds of data, allowing
   researchers to model highly complex relationships and to evaluate the
   generalizability and robustness of their results using resampling
   methods. The correct usage of machine learning models requires
   specialized methodological training that considers issues specific to
   this type of modelling. Here, we first provide a brief overview of past
   studies using machine learning in personality psychology. Second, we
   illustrate the main challenges that researchers face when building,
   interpreting, and validating machine learning models. Third, we discuss
   the evaluation of personality scales, derived using machine learning
   methods. Fourth, we highlight some key issues that arise from the use of
   latent variables in the modelling process. We conclude with an outlook
   on the future role of machine learning models in personality research
   and assessment.},
DOI = {10.1002/per.2257},
EarlyAccessDate = {MAY 2020},
ISSN = {0890-2070},
EISSN = {1099-0984},
ResearcherID-Numbers = {Schoedel, Ramona/ACH-8141-2022
   Gosling, Sam/AAL-5718-2020
   Stachl, Clemens/ABD-9949-2021
   Vaid, Sumer/KVA-7992-2024
   },
ORCID-Numbers = {Gosling, Samuel/0000-0001-8970-591X
   Schoedel, Ramona/0000-0001-7275-0626
   Stachl, Clemens/0000-0002-4498-3067},
Unique-ID = {WOS:000535945200001},
}

@article{ WOS:001091207400001,
Author = {Mohsin, Muhammad and Jamaani, Fouad},
Title = {A novel deep-learning technique for forecasting oil price volatility
   using historical prices of five precious metals in context of green
   financing - A comparison of deep learning, machine learning, and
   statistical models},
Journal = {RESOURCES POLICY},
Year = {2023},
Volume = {86},
Number = {A},
Month = {OCT},
Abstract = {This study proposes a novel deep-learning convolution neural network
   (CNN) to forecast crude oil prices based on historical prices of five
   precious metals (Gold, Silver, Platinum, Palladium, and Rhodium) in
   context of green financing. The proposed deep learning CNN has three
   components: a convolution block called a group block, a novel
   convolutional neural network architecture called GroupNet, and a
   regression layer. The proposed model is tested against seven machine
   learning models and three traditional statistical models for predicting
   oil price volatility using the same independent variables (5 precious
   metals). A comparison of the deep learning model (our proposed model)
   with machine learning/deep learning models and statistical methods
   indicates that the proposed deep learning model has the highest
   prediction accuracy. A feature selection technique is also applied using
   the WEKA ML tool to improve the accuracy of the proposed model and
   existing machine learning and traditional statistical models. The
   findings indicate a non-linear correlation between oil price volatility
   and prices of precious metals. Moreover, statistical analysis indicates
   that deep learning can be used to predict oil price volatility with
   greater accuracy than machine learning and statistical methods while
   using precious metals as predictors. The results also indicate that
   machine learning models (Decision Tables and M5rules) can be used to
   predict oil price volatility with considerable accuracy. Moreover, the
   study proves that traditional statistical models can perform better than
   a few machine learning models (Lazy LWL and GPR).},
DOI = {10.1016/j.resourpol.2023.104216},
EarlyAccessDate = {OCT 2023},
Article-Number = {104216},
ISSN = {0301-4207},
EISSN = {1873-7641},
ResearcherID-Numbers = {Mohsin, Muhammad/JVO-5672-2024
   Jamaani, Fouad/ABI-9083-2022},
Unique-ID = {WOS:001091207400001},
}

@article{ WOS:000614760600005,
Author = {George, Janine and Hautier, Geoffroy},
Title = {Chemist versus Machine: Traditional Knowledge versus Machine Learning
   Techniques},
Journal = {TRENDS IN CHEMISTRY},
Year = {2021},
Volume = {3},
Number = {2},
Pages = {86-95},
Month = {FEB},
Abstract = {Chemical heuristics have been fundamental to the advancement of
   chemistry and materials science. These heuristics are typically
   established by scientists using knowledge and creativity to extract
   patterns from limited datasets. Machine learning offers opportunities to
   perfect this approach using computers and larger datasets. Here, we
   discuss the relationships between traditional heuristics and machine
   learning approaches. We show how traditional rules can be challenged by
   large-scale statistical assessment and how traditional concepts commonly
   used as features are feeding the machine learning techniques. We stress
   the waste involved in relearning chemical rules and the challenges in
   terms of data size requirements for purely data-driven approaches. Our
   view is that heuristic and machine learning approaches are at their best
   when they work together.},
DOI = {10.1016/j.trechm.2020.10.007},
EarlyAccessDate = {JAN 2021},
EISSN = {2589-5974},
ResearcherID-Numbers = {Hautier, Geoffroy/GQP-2814-2022
   George, Janine/B-6224-2019},
ORCID-Numbers = {George, Janine/0000-0001-8907-0336
   },
Unique-ID = {WOS:000614760600005},
}

@article{ WOS:000757584200001,
Author = {Han, Ruocheng and Ketkaew, Rangsiman and Luber, Sandra},
Title = {A Concise Review on Recent Developments of Machine Learning for the
   Prediction of Vibrational Spectra},
Journal = {JOURNAL OF PHYSICAL CHEMISTRY A},
Year = {2022},
Volume = {126},
Number = {6},
Pages = {801-812},
Month = {FEB 17},
Abstract = {Machine learning has become more and more popular in computational
   chemistry, as well as in the important field of spectroscopy. In this
   concise review, we walk the reader through a short summary of machine
   learning algorithms and a comprehensive discussion on the connection
   between machine learning methods and vibrational spectroscopy,
   particularly for the case of infrared and Raman spectroscopy. We also
   briefly discuss state-of-the-art molecular representations which serve
   as meaningful inputs for machine learning to predict vibrational
   spectra. In addition, this review provides an overview of the
   transferability and best practices of machine learning in the prediction
   of vibrational spectra as well as possible future research directions.},
DOI = {10.1021/acs.jpca.1c10417},
ISSN = {1089-5639},
EISSN = {1520-5215},
ResearcherID-Numbers = {Luber, Sandra/M-6475-2016
   Ketkaew, Rangsiman/H-7769-2015},
ORCID-Numbers = {Luber, Sandra/0000-0002-6203-9379
   },
Unique-ID = {WOS:000757584200001},
}

@article{ WOS:000928683100001,
Author = {Sanusi, Ismaila Temitayo and Sunday, Kissinger and Oyelere, Solomon
   Sunday and Suhonen, Jarkko and Vartiainen, Henriikka and Tukiainen,
   Markku},
Title = {Learning machine learning with young children: exploring informal
   settings in an African context},
Journal = {COMPUTER SCIENCE EDUCATION},
Year = {2024},
Volume = {34},
Number = {2},
Pages = {161-192},
Month = {APR 2},
Abstract = {Background and contextResearchers have been investigating ways to
   demystify machine learning for students from kindergarten to twelfth
   grade (K-12) levels. As little evidence can be found in the literature,
   there is a need for additional research to understand and facilitate the
   learning experience of children while also considering the African
   context.ObjectiveThe purpose of this study was to explore how young
   children teach and develop their understanding of machine learning based
   technologies in playful and informal settings.MethodUsing a qualitative
   methodological approach through fine-grained analysis of video
   recordings and interviews, we analysed how 18 children aged 3-13 years
   constructed their interactions with a machine-based technology (Google's
   Teachable Machine).FindingsThis study provides empirical support for the
   claim that Google's Teachable Machine contributes to the development of
   data literacy and conceptual understanding across K-12 irrespective of
   the learners' backgrounds. The results also confirmed children's ability
   to infer the relationship between their own expressions and the output
   of the machine learning-based tool, thus, identifying the input-output
   relationships in machine learning. In addition, this study opens a
   discussion around differentials in emerging technology use across
   different contexts through participatory learning.ImplicationsThe
   results provide a baseline for future research on the topic and
   preliminary evidence to discern how children learn about machine
   learning in the African K-12 context.},
DOI = {10.1080/08993408.2023.2175559},
EarlyAccessDate = {FEB 2023},
ISSN = {0899-3408},
EISSN = {1744-5175},
ResearcherID-Numbers = {Sanusi, Ismaila Temitayo/AAE-6972-2020
   Oyelere, Solomon Sunday/AAE-7541-2020
   Sunday, Kissy/AAW-5876-2021
   },
ORCID-Numbers = {Suhonen, Jarkko/0000-0002-3501-6286
   Oyelere, Solomon Sunday/0000-0001-9895-6796
   Sunday, Kissinger/0000-0001-8435-3433
   Sanusi, Ismaila Temitayo/0000-0002-5705-6684},
Unique-ID = {WOS:000928683100001},
}

@article{ WOS:000590441600001,
Author = {Mashrur, Akib and Luo, Wei and Zaidi, Nayyar A. and Robles-Kelly,
   Antonio},
Title = {Machine Learning for Financial Risk Management: A Survey},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {203203-203223},
Abstract = {Financial risk management avoids losses and maximizes profits, and hence
   is vital to most businesses. As the task relies heavily on
   information-driven decision making, machine learning is a promising
   source for new methods and technologies. In recent years, we have seen
   increasing adoption of machine learning methods for various risk
   management tasks. Machine-learning researchers, however, often struggle
   to navigate the vast and complex domain knowledge and the fast-evolving
   literature. This paper fills this gap, by providing a systematic survey
   of the rapidly growing literature of machine learning research for
   financial risk management. The contributions of the paper are
   four-folds: First, we present a taxonomy of financial-risk-management
   tasks and connect them with relevant machine learning methods. Secondly,
   we highlight significant publications in the past decade. Thirdly, we
   identify major challenges being faced by researchers in this area. And
   finally, we point out emerging trends and promising research directions.},
DOI = {10.1109/ACCESS.2020.3036322},
ISSN = {2169-3536},
ResearcherID-Numbers = {Zaidi, Nayyar/IAN-8832-2023
   Luo, Wei/A-6043-2011},
ORCID-Numbers = {Mashrur, Akib/0000-0002-4404-7471
   Zaidi, Nayyar/0000-0003-4024-2517
   Luo, Wei/0000-0002-4711-7543},
Unique-ID = {WOS:000590441600001},
}

@article{ WOS:000828467800001,
Author = {Frankel, Richard and Jennings, Jared and Lee, Joshua},
Title = {Disclosure Sentiment: Machine Learning vs. Dictionary Methods},
Journal = {MANAGEMENT SCIENCE},
Year = {2022},
Volume = {68},
Number = {7},
Pages = {5514-5532},
Month = {JUL},
Abstract = {We compare the ability of dictionary-based and machine-learning methods
   to capture disclosure sentiment at 10-K filing and conference-call
   dates. Like Loughran and McDonald {[}Loughran T, McDonald B (2011) When
   is a liability not a liability? Textual analysis, dictionaries, and
   10-Ks. J. Finance 66(1):35-65.], we use returns to assess sentiment. We
   find that measures based on machine learning offer a significant
   improvement in explanatory power over dictionary-based measures.
   Specifically, machine-learning measures explain returns at 10-K filing
   dates, whereas measures based on the Loughran and McDonald dictionary
   only explain returns at 10-K filing dates during the time period of
   their study. Moreover, at conference-call dates, machine-learning
   methods offer an improvement over the Loughran and McDonald dictionary
   method of a greater magnitude than the improvement of the Loughran and
   McDonald dictionary over the Harvard Psychosociological Dictionary. We
   further find that the random-forest-regression-tree method better
   captures disclosure sentiment than alternative algorithms, simplifying
   the application of the machine-learning approach. Overall, our results
   suggest that machine-learning methods offer an easily implementable,
   more powerful, and reliable measure of disclosure sentiment than
   dictionary-based methods.},
DOI = {10.1287/mnsc.2021.4156},
EarlyAccessDate = {NOV 2021},
ISSN = {0025-1909},
EISSN = {1526-5501},
ResearcherID-Numbers = {Jennings, Jared/ABF-8477-2020
   FRANKEL, RICHARD/KHD-5128-2024},
ORCID-Numbers = {Lee, Joshua/0009-0001-9737-5997
   Frankel, Richard/0000-0001-6736-0738
   Jennings, Jared/0000-0002-3658-4779
   },
Unique-ID = {WOS:000828467800001},
}

@article{ WOS:000403753100004,
Author = {Mullainathan, Sendhil and Spiess, Jann},
Title = {Machine Learning: An Applied Econometric Approach},
Journal = {JOURNAL OF ECONOMIC PERSPECTIVES},
Year = {2017},
Volume = {31},
Number = {2},
Pages = {87-106},
Month = {SPR},
Abstract = {Machines are increasingly doing ``intelligent{''} things. Face
   recognition algorithms use a large dataset of photos labeled as having a
   face or not to estimate a function that predicts the presence y of a
   face from pixels x. This similarity to econometrics raises questions:
   How do these new empirical tools fit with what we know? As empirical
   economists, how can we use them? We present a way of thinking about
   machine learning that gives it its own place in the econometric toolbox.
   Machine learning not only provides new tools, it solves a different
   problem. Specifically, machine learning revolves around the problem of
   prediction, while many economic applications revolve around parameter
   estimation. So applying machine learning to economics requires finding
   relevant tasks. Machine learning algorithms are now technically easy to
   use: you can download convenient packages in R or Python. This also
   raises the risk that the algorithms are applied naively or their output
   is misinterpreted. We hope to make them conceptually easier to use by
   providing a crisper understanding of how these algorithms work, where
   they excel, and where they can stumble-and thus where they can be most
   usefully applied.},
DOI = {10.1257/jep.31.2.87},
ISSN = {0895-3309},
EISSN = {1944-7965},
ResearcherID-Numbers = {Mullainathan, Sendhil/OHR-8549-2025
   },
ORCID-Numbers = {Mullainathan, Sendhil/0000-0001-8508-4052},
Unique-ID = {WOS:000403753100004},
}

@article{ WOS:000680450500001,
Author = {Mhasawade, Vishwali and Zhao, Yuan and Chunara, Rumi},
Title = {Machine learning and algorithmic fairness in public and population
   health},
Journal = {NATURE MACHINE INTELLIGENCE},
Year = {2021},
Volume = {3},
Number = {8},
Pages = {659-666},
Month = {AUG},
Abstract = {Until now, much of the work on machine learning and health has focused
   on processes inside the hospital or clinic. However, this represents
   only a narrow set of tasks and challenges related to health; there is
   greater potential for impact by leveraging machine learning in health
   tasks more broadly. In this Perspective we aim to highlight potential
   opportunities and challenges for machine learning within a holistic view
   of health and its influences. To do so, we build on research in
   population and public health that focuses on the mechanisms between
   different cultural, social and environmental factors and their effect on
   the health of individuals and communities. We present a brief
   introduction to research in these fields, data sources and types of
   tasks, and use these to identify settings where machine learning is
   relevant and can contribute to new knowledge. Given the key foci of
   health equity and disparities within public and population health, we
   juxtapose these topics with the machine learning subfield of algorithmic
   fairness to highlight specific opportunities where machine learning,
   public and population health may synergize to achieve health equity.
   Algorithmic solutions to improve treatment are starting to transform
   health care. Mhasawade and colleagues discuss in this Perspective how
   machine learning applications in population and public health can extend
   beyond clinical practice. While working with general health data comes
   with its own challenges, most notably ensuring algorithmic fairness in
   the face of existing health disparities, the area provides new kinds of
   data and questions for the machine learning community.},
DOI = {10.1038/s42256-021-00373-4},
EarlyAccessDate = {JUL 2021},
EISSN = {2522-5839},
ORCID-Numbers = {Mhasawade, Vishwali/0000-0003-1269-7071
   Chunara, Rumi/0000-0002-5346-7259},
Unique-ID = {WOS:000680450500001},
}

@article{ WOS:000576604100002,
Author = {Kim, Gi Bae and Kim, Won Jun and Kim, Hyun Uk and Lee, Sang Yup},
Title = {Machine learning applications in systems metabolic engineering},
Journal = {CURRENT OPINION IN BIOTECHNOLOGY},
Year = {2020},
Volume = {64},
Number = {SI},
Pages = {1-9},
Month = {AUG},
Abstract = {Systems metabolic engineering allows efficient development of high
   performing microbial strains for the sustainable production of chemicals
   and materials. In recent years, increasing availability of bio big data,
   for example, omics data, has led to active application of machine
   learning techniques across various stages of systems metabolic
   engineering, including host strain selection, metabolic pathway
   reconstruction, metabolic flux optimization, and fermentation. In this
   paper, recent contributions of machine learning approaches to each major
   step of systems metabolic engineering are discussed. As the use of
   machine learning in systems metabolic engineering will become more
   widespread in accordance with the ever-increasing volume of bio big
   data, future prospects are also provided for the successful applications
   of machine learning.},
DOI = {10.1016/j.copbio.2019.08.010},
ISSN = {0958-1669},
EISSN = {1879-0429},
ResearcherID-Numbers = {Kim, Hyun/F-4509-2018
   Kim, Hyun Uk/F-4509-2018
   Kim, Wonjun/KDO-4306-2024
   Lee, Sang Yup/C-1526-2011},
ORCID-Numbers = {Kim, Hyun Uk/0000-0001-7224-642X
   Kim, Gi Bae/0000-0001-7563-5551
   Lee, Sang Yup/0000-0003-0599-3091},
Unique-ID = {WOS:000576604100002},
}

@article{ WOS:001000655300009,
Author = {Koylu, Troya Cagil and Reinbrecht, Cezar Rodolfo Wedig and Gebregiorgis,
   Anteneh and Hamdioui, Said and Taouil, Mottaqiallah},
Title = {A Survey on Machine Learning in Hardware Security},
Journal = {ACM JOURNAL ON EMERGING TECHNOLOGIES IN COMPUTING SYSTEMS},
Year = {2023},
Volume = {19},
Number = {2},
Month = {APR},
Abstract = {Hardware security is currently a very influential domain, where each
   year countless works are published concerning attacks against hardware
   and countermeasures. A significant number of them use machine learning,
   which is proven to be very effective in other domains. This survey, as
   one of the early attempts, presents the usage of machine learning in
   hardware security in a full and organized manner. Our contributions
   include classification and introduction to the relevant fields of
   machine learning, a comprehensive and critical overview of machine
   learning usage in hardware security, and an investigation of the
   hardware attacks against machine learning (neural network)
   implementations.},
DOI = {10.1145/3589506},
Article-Number = {18},
ISSN = {1550-4832},
EISSN = {1550-4840},
ResearcherID-Numbers = {HAMDIOUI, Said/MXK-0301-2025
   },
ORCID-Numbers = {Taouil, Mottaqiallah/0000-0002-9911-4846
   Wedig Reinbrecht, Cezar Rodolfo/0000-0001-6113-7041
   Gebregiorgis, Anteneh/0000-0001-5909-4927
   Hamdioui, Said/0000-0002-8961-0387},
Unique-ID = {WOS:001000655300009},
}

@article{ WOS:000503335100001,
Author = {Fluke, Christopher J. and Jacobs, Colin},
Title = {Surveying the reach and maturity of machine learning and artificial
   intelligence in astronomy},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY},
Year = {2020},
Volume = {10},
Number = {2},
Month = {MAR},
Abstract = {Machine learning (automated processes that learn by example in order to
   classify, predict, discover, or generate new data) and artificial
   intelligence (methods by which a computer makes decisions or discoveries
   that would usually require human intelligence) are now firmly
   established in astronomy. Every week, new applications of machine
   learning and artificial intelligence are added to a growing corpus of
   work. Random forests, support vector machines, and neural networks are
   now having a genuine impact for applications as diverse as discovering
   extrasolar planets, transient objects, quasars, and gravitationally
   lensed systems, forecasting solar activity, and distinguishing between
   signals and instrumental effects in gravitational wave astronomy. This
   review surveys contemporary, published literature on machine learning
   and artificial intelligence in astronomy and astrophysics. Applications
   span seven main categories of activity: classification, regression,
   clustering, forecasting, generation, discovery, and the development of
   new scientific insights. These categories form the basis of a hierarchy
   of maturity, as the use of machine learning and artificial intelligence
   emerges, progresses, or becomes established. This article is categorized
   under: Application Areas > Science and Technology Fundamental Concepts
   of Data and Knowledge > Motivation and Emergence of Data Mining
   Technologies > Machine Learning},
DOI = {10.1002/widm.1349},
EarlyAccessDate = {DEC 2019},
Article-Number = {e1349},
ISSN = {1942-4787},
EISSN = {1942-4795},
Unique-ID = {WOS:000503335100001},
}

@article{ WOS:000510903200065,
Author = {Zhang, Tao and Zhu, Tianqing and Xiong, Ping and Huo, Huan and Tari,
   Zahir and Zhou, Wanlei},
Title = {Correlated Differential Privacy: Feature Selection in Machine Learning},
Journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS},
Year = {2020},
Volume = {16},
Number = {3},
Pages = {2115-2124},
Month = {MAR},
Abstract = {Privacy preserving in machine learning is a crucial issue in industry
   informatics since data used for training in industries usually contain
   sensitive information. Existing differentially private machine learning
   algorithms have not considered the impact of data correlation, which may
   lead to more privacy leakage than expected in industrial applications.
   For example, data collected for traffic monitoring may contain some
   correlated records due to temporal correlation or user correlation. To
   fill this gap, in this article, we propose a correlation reduction
   scheme with differentially private feature selection considering the
   issue of privacy loss when data have correlation in machine learning
   tasks. The proposed scheme involves five steps with the goal of managing
   the extent of data correlation, preserving the privacy, and supporting
   accuracy in the prediction results. In this way, the impact of data
   correlation is relieved with the proposed feature selection scheme, and
   moreover the privacy issue of data correlation in learning is
   guaranteed. The proposed method can be widely used in machine learning
   algorithms, which provide services in industrial areas. Experiments show
   that the proposed scheme can produce better prediction results with
   machine learning tasks and fewer mean square errors for data queries
   compared to existing schemes.},
DOI = {10.1109/TII.2019.2936825},
ISSN = {1551-3203},
EISSN = {1941-0050},
ResearcherID-Numbers = {zhu, tianqing/ABY-2511-2022
   Zhou, Wanlei/HKV-8022-2023
   Huohuanxin, Huohuanxin/OAL-0066-2025},
ORCID-Numbers = {Zhou, Wanlei/0000-0002-1680-2521
   zhang, tao/0000-0003-4696-641X
   Tari, Zahir/0000-0002-1235-9673
   },
Unique-ID = {WOS:000510903200065},
}

@article{ WOS:000582822600018,
Author = {van der Sommen, Fons and de Groof, Jeroen and Struyvenberg, Maarten and
   van der Putten, Joost and Boers, Tim and Fockens, Kiki and Schoon, Erik
   J. and Curvers, Wouter and de With, Peter and Mori, Yuichi and Byrne,
   Michael and Bergman, Jacques J. G. H. M.},
Title = {Machine learning in GI endoscopy: practical guidance in how to interpret
   a novel field},
Journal = {GUT},
Year = {2020},
Volume = {69},
Number = {11},
Pages = {2035-2045},
Month = {NOV},
Abstract = {There has been a vast increase in GI literature focused on the use of
   machine learning in endoscopy. The relative novelty of this field poses
   a challenge for reviewers and readers of GI journals. To appreciate
   scientific quality and novelty of machine learning studies,
   understanding of the technical basis and commonly used techniques is
   required. Clinicians often lack this technical background, while machine
   learning experts may be unfamiliar with clinical relevance and
   implications for daily practice. Therefore, there is an increasing need
   for a multidisciplinary, international evaluation on how to perform
   high-quality machine learning research in endoscopy. This review aims to
   provide guidance for readers and reviewers of peer-reviewed GI journals
   to allow critical appraisal of the most relevant quality requirements of
   machine learning studies. The paper provides an overview of common
   trends and their potential pitfalls and proposes comprehensive quality
   requirements in six overarching themes: terminology, data, algorithm
   description, experimental setup, interpretation of results and machine
   learning in clinical practice.},
DOI = {10.1136/gutjnl-2019-320466},
ISSN = {0017-5749},
EISSN = {1468-3288},
ResearcherID-Numbers = {Bergman, Jacques/AAS-2500-2021
   van der Sommen, Fons/AEW-0479-2022
   Mori, Yuichi/AAU-5406-2020
   },
ORCID-Numbers = {van der Sommen, Fons/0000-0002-3593-2356
   Bergman, Jacques/0000-0001-7548-6955},
Unique-ID = {WOS:000582822600018},
}

@article{ WOS:000911287700001,
Author = {Carter, A. and Imtiaz, S. and Naterer, G. F.},
Title = {Review of interpretable machine learning for process industries},
Journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION},
Year = {2023},
Volume = {170},
Pages = {647-659},
Month = {FEB},
Abstract = {This review article examines recent advances in the use of machine
   learning for process industries. The article presents common process
   industry tasks that researchers are solving with machine learning
   techniques. It then identifies a lack of consensus among past studies
   when selecting an appropriate model given a prescribed application.
   Furthermore, the article identifies that relatively few past studies
   have considered model inter-pretability - a ``black-box{''} challenge
   holding back machine learning's implementation in more high-risk
   in-dustrial applications. This interdisciplinary field of engineering
   and computer science is still reasonably young. Additional research is
   recommended to standardize methods and establish a strategic framework
   to manage risk during adoption of machine learning models.},
DOI = {10.1016/j.psep.2022.12.018},
EarlyAccessDate = {DEC 2022},
ISSN = {0957-5820},
EISSN = {1744-3598},
ResearcherID-Numbers = {Naterer, Greg/AAF-7156-2019
   Imtiaz, Syed/AAU-5998-2020
   },
ORCID-Numbers = {Imtiaz, Syed/0000-0002-2715-9084},
Unique-ID = {WOS:000911287700001},
}

@article{ WOS:001527924800002,
Author = {Storey, Veda C. and Parsons, Jeffrey and Bueso, Arturo Castellanos and
   Tremblay, Monica Chiarini and Lukyanenko, Roman and Castillo, Alfred and
   Maass, Wolfgang},
Title = {Domain knowledge in artificial intelligence: Using conceptual modeling
   to increase machine learning accuracy and explainability},
Journal = {DATA \& KNOWLEDGE ENGINEERING},
Year = {2025},
Volume = {160},
Month = {NOV},
Abstract = {Machine learning enables the extraction of useful information from
   large, diverse datasets. However, despite many successful applications,
   machine learning continues to suffer from performance and transparency
   issues. These challenges can be partially attributed to the limited use
   of domain knowledge by machine learning models. This research proposes
   using the domain knowledge represented in conceptual models to improve
   the preparation of the data used to train machine learning models. We
   develop and demonstrate a method, called the Conceptual Modeling for
   Machine Learning (CMML), which is comprised of guidelines for data
   preparation in machine learning and based on conceptual modeling
   constructs and principles. To assess the impact of CMML on machine
   learning outcomes, we first applied it to two real-world problems to
   evaluate its impact on model performance. We then solicited an
   assessment by data scientists on the applicability of the method. These
   results demonstrate the value of CMML for improving machine learning
   outcomes.},
DOI = {10.1016/j.datak.2025.102482},
Article-Number = {102482},
ISSN = {0169-023X},
EISSN = {1872-6933},
ResearcherID-Numbers = {Parsons, Jeffrey/AAF-3380-2020
   Tremblay, Monica/H-8650-2012
   Maass, Wolfgang/ACE-7055-2022},
Unique-ID = {WOS:001527924800002},
}

@article{ WOS:000811299400001,
Author = {Jiang, Yiru and Luo, Jing and Huang, Danqing and Liu, Ya and Li, Dan-dan},
Title = {Machine Learning Advances in Microbiology: A Review of Methods and
   Applications},
Journal = {FRONTIERS IN MICROBIOLOGY},
Year = {2022},
Volume = {13},
Month = {MAY 26},
Abstract = {Microorganisms play an important role in natural material and elemental
   cycles. Many common and general biology research techniques rely on
   microorganisms. Machine learning has been gradually integrated with
   multiple fields of study. Machine learning, including deep learning,
   aims to use mathematical insights to optimize variational functions to
   aid microbiology using various types of available data to help humans
   organize and apply collective knowledge of various research objects in a
   systematic and scaled manner. Classification and prediction have become
   the main achievements in the development of microbial community research
   in the direction of computational biology. This review summarizes the
   application and development of machine learning and deep learning in the
   field of microbiology and shows and compares the advantages and
   disadvantages of different algorithm tools in four fields: microbiome
   and taxonomy, microbial ecology, pathogen and epidemiology, and drug
   discovery.},
DOI = {10.3389/fmicb.2022.925454},
Article-Number = {925454},
EISSN = {1664-302X},
ResearcherID-Numbers = {Huang, Danqing/ABC-9114-2021},
Unique-ID = {WOS:000811299400001},
}

@article{ WOS:000939150800001,
Author = {Alotaibi, Afnan and Rassam, Murad A.},
Title = {Adversarial Machine Learning Attacks against Intrusion Detection
   Systems: A Survey on Strategies and Defense},
Journal = {FUTURE INTERNET},
Year = {2023},
Volume = {15},
Number = {2},
Month = {FEB},
Abstract = {Concerns about cybersecurity and attack methods have risen in the
   information age. Many techniques are used to detect or deter attacks,
   such as intrusion detection systems (IDSs), that help achieve security
   goals, such as detecting malicious attacks before they enter the system
   and classifying them as malicious activities. However, the IDS
   approaches have shortcomings in misclassifying novel attacks or adapting
   to emerging environments, affecting their accuracy and increasing false
   alarms. To solve this problem, researchers have recommended using
   machine learning approaches as engines for IDSs to increase their
   efficacy. Machine-learning techniques are supposed to automatically
   detect the main distinctions between normal and malicious data, even
   novel attacks, with high accuracy. However, carefully designed
   adversarial input perturbations during the training or testing phases
   can significantly affect their predictions and classifications.
   Adversarial machine learning (AML) poses many cybersecurity threats in
   numerous sectors that use machine-learning-based classification systems,
   such as deceiving IDS to misclassify network packets. Thus, this paper
   presents a survey of adversarial machine-learning strategies and
   defenses. It starts by highlighting various types of adversarial attacks
   that can affect the IDS and then presents the defense strategies to
   decrease or eliminate the influence of these attacks. Finally, the gaps
   in the existing literature and future research directions are presented.},
DOI = {10.3390/fi15020062},
Article-Number = {62},
ISSN = {1999-5903},
ResearcherID-Numbers = {Rassam, Murad/X-2478-2019
   },
ORCID-Numbers = {Rassam, Murad/0000-0003-3558-6737},
Unique-ID = {WOS:000939150800001},
}

@article{ WOS:000774530000001,
Author = {Wang, Qichen and Ahmad, Waqas and Ahmad, Ayaz and Aslam, Fahid and
   Mohamed, Abdullah and Vatin, Nikolai Ivanovich},
Title = {Application of Soft Computing Techniques to Predict the Strength of
   Geopolymer Composites},
Journal = {POLYMERS},
Year = {2022},
Volume = {14},
Number = {6},
Month = {MAR},
Abstract = {Geopolymers may be the best alternative to ordinary Portland cement
   because they are manufactured using waste materials enriched in
   aluminosilicate. Research on geopolymer composites is accelerating.
   However, considerable work, expense, and time are needed to cast, cure,
   and test specimens. The application of computational methods to the
   stated objective is critical for speedy and cost-effective research. In
   this study, supervised machine learning approaches were employed to
   predict the compressive strength of geopolymer composites. One
   individual machine learning approach, decision tree, and two ensembled
   machine learning approaches, AdaBoost and random forest, were used. The
   coefficient correlation (R-2), statistical tests, and k-fold analysis
   were used to determine the validity and comparison of all models. It was
   discovered that ensembled machine learning techniques outperformed
   individual machine learning techniques in forecasting the compressive
   strength of geopolymer composites. However, the outcomes of the
   individual machine learning model were also within the acceptable limit.
   R-2 values of 0.90, 0.90, and 0.83 were obtained for AdaBoost, random
   forest, and decision models, respectively. The models' decreased error
   values, such as mean absolute error, mean absolute percentage error, and
   root-mean-square errors, further confirmed the ensembled machine
   learning techniques' increased precision. Machine learning approaches
   will aid the building industry by providing quick and cost-effective
   methods for evaluating material properties.},
DOI = {10.3390/polym14061074},
Article-Number = {1074},
EISSN = {2073-4360},
ResearcherID-Numbers = {Gebril, Mohamed/GLN-5529-2022
   Ahmad, Ayaz/ABC-7171-2021
   Ahmad, Waqas/IXX-3152-2023
   Vatin, Nikolai Ivanovich/O-6995-2019
   Aslam, Fahid/AAG-4938-2020
   Wang, Qichen/S-6914-2017
   },
ORCID-Numbers = {Aslam, Fahid/0000-0003-2863-3283
   Ahmad, Ayaz/0000-0002-0312-2965
   Ahmad, waqas/0000-0001-7162-1531
   Vatin, Nikolai Ivanovich/0000-0002-1196-8004
   Ahmad, Waqas/0000-0002-1668-7607},
Unique-ID = {WOS:000774530000001},
}

@article{ WOS:000520045900001,
Author = {Peng, Grace C. Y. and Alber, Mark and Tepole, Adrian Buganza and Cannon,
   William and De, Suvranu and Dura-Bernal, Salvador and Garikipati,
   Krishna and Karniadakis, George and Lytton, William W. and Perdikaris,
   Paris and Petzold, Linda and Kuhl, Ellen},
Title = {Multiscale Modeling Meets Machine Learning: What Can We Learn?},
Journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING},
Year = {2021},
Volume = {28},
Number = {3},
Pages = {1017-1037},
Month = {MAY},
Abstract = {Machine learning is increasingly recognized as a promising technology in
   the biological, biomedical, and behavioral sciences. There can be no
   argument that this technique is incredibly successful in image
   recognition with immediate applications in diagnostics including
   electrophysiology, radiology, or pathology, where we have access to
   massive amounts of annotated data. However, machine learning often
   performs poorly in prognosis, especially when dealing with sparse data.
   This is a field where classical physics-based simulation seems to remain
   irreplaceable. In this review, we identify areas in the biomedical
   sciences where machine learning and multiscale modeling can mutually
   benefit from one another: Machine learning can integrate physics-based
   knowledge in the form of governing equations, boundary conditions, or
   constraints to manage ill-posted problems and robustly handle sparse and
   noisy data; multiscale modeling can integrate machine learning to create
   surrogate models, identify system dynamics and parameters, analyze
   sensitivities, and quantify uncertainty to bridge the scales and
   understand the emergence of function. With a view towards applications
   in the life sciences, we discuss the state of the art of combining
   machine learning and multiscale modeling, identify applications and
   opportunities, raise open questions, and address potential challenges
   and limitations. We anticipate that it will stimulate discussion within
   the community of computational mechanics and reach out to other
   disciplines including mathematics, statistics, computer science,
   artificial intelligence, biomedicine, systems biology, and precision
   medicine to join forces towards creating robust and efficient models for
   biological systems.},
DOI = {10.1007/s11831-020-09405-5},
EarlyAccessDate = {FEB 2020},
ISSN = {1134-3060},
EISSN = {1886-1784},
ResearcherID-Numbers = {Cannon, William/K-8411-2014
   Kuhl, Ellen/G-4444-2011
   Alber, Mark/AAR-1280-2020
   Perdikaris, Paris/LNQ-5994-2024
   },
ORCID-Numbers = {, SUVRANU/0000-0001-8489-0001
   Cannon, William/0000-0003-3789-7889
   Kuhl, Ellen/0000-0002-6283-935X
   Dura-Bernal, Salvador/0000-0002-8561-5324
   Buganza Tepole, Adrian/0000-0001-8531-0603
   Perdikaris, Paris/0000-0002-2816-3229},
Unique-ID = {WOS:000520045900001},
}

@article{ WOS:000940478300001,
Author = {Luo, Xinwei and Chen, Lu and Zhou, Hanlu and Cao, Hongli},
Title = {A Survey of Underwater Acoustic Target Recognition Methods Based on
   Machine Learning},
Journal = {JOURNAL OF MARINE SCIENCE AND ENGINEERING},
Year = {2023},
Volume = {11},
Number = {2},
Month = {FEB},
Abstract = {Underwater acoustic target recognition (UATR) technology has been
   implemented widely in the fields of marine biodiversity detection,
   marine search and rescue, and seabed mapping, providing an essential
   basis for human marine economic and military activities. With the rapid
   development of machine-learning-based technology in the acoustics field,
   these methods receive wide attention and display a potential impact on
   UATR problems. This paper reviews current UATR methods based on machine
   learning. We focus mostly, but not solely, on the recognition of
   target-radiated noise from passive sonar. First, we provide an overview
   of the underwater acoustic acquisition and recognition process and
   briefly introduce the classical acoustic signal feature extraction
   methods. In this paper, recognition methods for UATR are classified
   based on the machine learning algorithms used as UATR technologies using
   statistical learning methods, UATR methods based on deep learning
   models, and transfer learning and data augmentation technologies for
   UATR. Finally, the challenges of UATR based on the machine learning
   method are summarized and directions for UATR development in the future
   are put forward.},
DOI = {10.3390/jmse11020384},
Article-Number = {384},
EISSN = {2077-1312},
ResearcherID-Numbers = {Cao, Hongli/NYT-2804-2025},
ORCID-Numbers = {Chen, Lu/0009-0005-0126-3878
   },
Unique-ID = {WOS:000940478300001},
}

@article{ WOS:001387703100001,
Author = {Sun, Yibo and Ni, Jun},
Title = {Machine Learning Advances in High-Entropy Alloys: A Mini-Review},
Journal = {ENTROPY},
Year = {2024},
Volume = {26},
Number = {12},
Month = {DEC},
Abstract = {The efficacy of machine learning has increased exponentially over the
   past decade. The utilization of machine learning to predict and design
   materials has become a pivotal tool for accelerating materials
   development. High-entropy alloys are particularly intriguing candidates
   for exemplifying the potency of machine learning due to their superior
   mechanical properties, vast compositional space, and intricate chemical
   interactions. This review examines the general process of developing
   machine learning models. The advances and new algorithms of machine
   learning in the field of high-entropy alloys are presented in each part
   of the process. These advances are based on both improvements in
   computer algorithms and physical representations that focus on the
   unique ordering properties of high-entropy alloys. We also show the
   results of generative models, data augmentation, and transfer learning
   in high-entropy alloys and conclude with a summary of the challenges
   still faced in machine learning high-entropy alloys today.},
DOI = {10.3390/e26121119},
Article-Number = {1119},
EISSN = {1099-4300},
ORCID-Numbers = {Sun, Yibo/0009-0007-3610-7423},
Unique-ID = {WOS:001387703100001},
}

@article{ WOS:001102417300001,
Author = {Muntin, A. V. and Zhikharev, P. Yu. and Ziniagin, A. G. and Brayko, D.
   A.},
Title = {Artificial Intelligence and Machine Learning in Metallurgy. Part 1.
   Methods and Algorithms},
Journal = {METALLURGIST},
Year = {2023},
Volume = {67},
Number = {5-6},
Pages = {886-894},
Month = {SEP},
Abstract = {The article contains information about machine learning methods used in
   modern metallurgy. The description of machine learning methods and their
   role in the processing of ``big data{''} formed at metallurgical
   enterprises are given. The topic relevance has to do with the
   effectiveness of solving problems aimed at improving production
   processes using artificial intelligence and machine learning in various
   metallurgical processing stages.},
DOI = {10.1007/s11015-023-01576-3},
EarlyAccessDate = {NOV 2023},
ISSN = {0026-0894},
EISSN = {1573-8892},
ResearcherID-Numbers = {Brayko, Denis/JTT-2201-2023
   Zhikharev, Pavel/HJB-0434-2022},
ORCID-Numbers = {Zinyagin, Alexey/0000-0001-9983-7211
   Brayko, Denis Alexandrovich/0000-0003-3685-5353
   },
Unique-ID = {WOS:001102417300001},
}

@article{ WOS:001137233600005,
Author = {Forero-Corba, Wiston and Bennasar, Francisca Negre},
Title = {Techniques and applications of Machine Learning and Artificial
   Intelligence in education: a systematic review},
Journal = {RIED-REVISTA IBEROAMERICANA DE EDUCACION A DISTANCIA},
Year = {2024},
Volume = {27},
Number = {1},
Month = {JAN},
Abstract = {Machine learning is a field of artificial intelligence that is impacting
   lately in all areas of knowledge. The areas of social sciences,
   especially education, are no stranger to it, so, a systematic review of
   the literature on the techniques and applications of machine learning
   and artificial intelligence in Education is performed. The lack of
   knowledge and skills of educators in machine learning and artificial
   intelligence limits the optimal implementation of these technologies in
   education. The objective of this research is to identify opportunities
   for improving teaching-learning processes and educational management at
   all levels of the educational context through the application of machine
   learning and artificial intelligence. The databases used for the
   bibliographic search were Web of Science and Scopus and the methodology
   applied is based on the PRISMA statement for obtaining and analyzing 55
   articles published in high impact journals between the years 2021-2023.
   The results showed that the studies addressed a total of 33 machine
   learning and artificial intelligence techniques and multiple
   applications that were implemented in educational contexts at primary,
   secondary and higher education levels in 38 countries. The conclusions
   showed the strong impact of the use of machine learning and artificial
   intelligence. This impact is reflected in the use of different
   intelligent techniques in educational contexts and the increase of
   research in secondary schools on artificial intelligence.},
DOI = {10.5944/ried.27.1.37491},
ISSN = {1138-2783},
EISSN = {1390-3306},
ResearcherID-Numbers = {Forero Corba, Wiston/HPG-0999-2023},
ORCID-Numbers = {Forero Corba, Wiston/0000-0002-8567-3954},
Unique-ID = {WOS:001137233600005},
}

@article{ WOS:001126204200001,
Author = {Ma, Yuxin and Minasny, Budiman and Dematte, Jose A. M. and McBratney,
   Alex B.},
Title = {Incorporating soil knowledge into machine-learning prediction of soil
   properties from soil spectra},
Journal = {EUROPEAN JOURNAL OF SOIL SCIENCE},
Year = {2023},
Volume = {74},
Number = {6},
Month = {NOV},
Abstract = {Various machine-learning models have been extensively applied to predict
   soil properties using infrared spectroscopy. Beyond the interpretability
   and transparency of these models, there is an ongoing discussion on the
   reliability of the prediction of soil properties generated from soil
   spectra. In this review, we contribute to this discussion by advocating
   for the integration of soil knowledge into machine-learning models. By
   doing so, researchers can delve deeper into the underlying soil
   constituents, ultimately enhancing prediction accuracy. Our review
   explores the soil information present in spectral data, the fallacy of
   model interpretability, methods to incorporate soil knowledge into
   machine-learning techniques, and the ways in which machine learning and
   soil spectroscopy can assist soil science. The combination of machine
   learning and domain knowledge is recommended to develop more meaningful
   models for predicting soil properties within the field of soil science.},
DOI = {10.1111/ejss.13438},
Article-Number = {e13438},
ISSN = {1351-0754},
EISSN = {1365-2389},
ResearcherID-Numbers = {Minasny, Budiman/B-4744-2011
   Ma, Yuxin/ABE-1261-2020
   Demattê, José/I-5990-2013
   McBratney, Alex/C-2199-2011},
ORCID-Numbers = {Dematte, Jose/0000-0001-5328-0323
   Minasny, Budiman/0000-0002-1182-2371
   },
Unique-ID = {WOS:001126204200001},
}

@article{ WOS:000557470100001,
Author = {Howell, Owen and Cui Wenping and Marsland, III, Robert and Mehta, Pankaj},
Title = {Machine learning as ecology},
Journal = {JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL},
Year = {2020},
Volume = {53},
Number = {33},
Month = {AUG 21},
Abstract = {Machine learning methods have had spectacular success on numerous
   problems. Here we show that a prominent class of learning
   algorithms-including support vector machines (SVMs)-have a natural
   interpretation in terms of ecological dynamics. We use these ideas to
   design new online SVM algorithms that exploit ecological invasions, and
   benchmark performance using the MNIST dataset. Our work provides a new
   ecological lens through which we can view statistical learning and opens
   the possibility of designing ecosystems for machine learning.},
DOI = {10.1088/1751-8121/ab956e},
Article-Number = {334001},
ISSN = {1751-8113},
EISSN = {1751-8121},
ResearcherID-Numbers = {Cui, Wenping/HLW-3177-2023},
ORCID-Numbers = {Cui, Wenping/0000-0002-9900-5092
   },
Unique-ID = {WOS:000557470100001},
}

@article{ WOS:000576782300009,
Author = {Moscatelli, Mirko and Parlapiano, Fabio and Narizzano, Simone and
   Viggiano, Gianluca},
Title = {Corporate default forecasting with machine learning},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2020},
Volume = {161},
Month = {DEC 15},
Abstract = {We analyze the performance of a set of machine learning models in
   predicting default risk, using standard statistical models, such as the
   logistic regression, as a benchmark. When only a limited information set
   is available, for example in the case of an external assessment of
   credit risk, we find that machine learning models provide substantial
   gains in discriminatory power and precision, relative to statistical
   models. This advantage diminishes when confidential information, such as
   credit behavioral indicators, is also available, and it becomes
   negligible when the dataset is small. Moreover, we evaluate the
   consequences of using a credit allocation rule based on machine learning
   ratings on the overall supply of credit and the number of borrowers
   gaining access to credit. Machine learning models concentrate a greater
   extent of credit towards safer and larger borrowers, which would result
   in lower credit losses for their lenders. (c) 2020 Elsevier Ltd. All
   rights reserved.},
DOI = {10.1016/j.eswa.2020.113567},
Article-Number = {113567},
ISSN = {0957-4174},
EISSN = {1873-6793},
Unique-ID = {WOS:000576782300009},
}

@article{ WOS:001100527300001,
Author = {Maltare, Nilesh N. and Vahora, Safvan},
Title = {Air Quality Index prediction using machine learning for Ahmedabad city},
Journal = {DIGITAL CHEMICAL ENGINEERING},
Year = {2023},
Volume = {7},
Month = {JUN},
Abstract = {Prediction of air pollution index may help in traffic routing and
   identifying serious pollutants. Modeling of the complex relationships
   between these variables by sophisticated methods in machine learning is
   a promising field. The objective of this work is to compare the various
   machine learning methods such as SARIMA, SVM and LSTM for the prediction
   of air quality index for Ahmedabad city of Gujarat, India. In this
   research, different preprocessing methods are used to manage the data
   before providing to the machine learning models. This study is carried
   out based on the data provided by the Central Pollution Control Board of
   India and it focuses on the support vector machine algorithm with RBF
   kernel model. So, that the results availed are comparatively better as
   compared to other kernels of the support vector machine models as well
   as SARIMA and LSTM models for Ahmedabad city.},
DOI = {10.1016/j.dche.2023.100093},
EarlyAccessDate = {MAR 2023},
Article-Number = {100093},
ISSN = {2772-5081},
ResearcherID-Numbers = {Maltare, Dr. Nilesh/LMN-9632-2024
   vahora, safvan/JXY-4916-2024},
ORCID-Numbers = {Maltare, Nilesh/0000-0003-4269-2941
   vahora, safvan/0000-0002-8051-8429
   },
Unique-ID = {WOS:001100527300001},
}

@article{ WOS:000603445400001,
Author = {Yang, Jung Ho and Park, Jae Hyeon and Jang, Seong-Ho and Cho, Jaesung},
Title = {Novel Method of Classification in Knee Osteoarthritis: Machine Learning
   Application Versus Logistic Regression Model},
Journal = {ANNALS OF REHABILITATION MEDICINE-ARM},
Year = {2020},
Volume = {44},
Number = {6},
Pages = {415-427},
Month = {DEC},
Abstract = {Objective To present new classification methods of knee osteoarthritis
   (KOA) using machine learning and compare its performance with
   conventional statistical methods as classification techniques using
   machine learning have recently been developed.
   Methods A total of 84 KOA patients and 97 normal participants were
   recruited. KOA patients were clustered into three groups according to
   the Kellgren-Lawrence (K-L) grading system. All subjects completed gait
   trials under the same experimental conditions. Machine learning- based
   classification using the support vector machine (SVM) classifier was
   performed to classify KOA patients and the severity of KOA. Logistic
   regression analysis was also performed to compare the results in
   classifying KOA patients with machine learning method.
   Results In the classification between KOA patients and normal subjects,
   the accuracy of classification was higher in machine learning method
   than in logistic regression analysis. In the classification of KOA
   severity, accuracy was enhanced through the feature selection process in
   the machine learning method. The most significant gait feature for
   classification was flexion and extension of the knee in the swing phase
   in the machine learning method.
   Conclusion The machine learning method is thought to be a new approach
   to complement conventional logistic regression analysis in the
   classification of KOA patients. It can be clinically used for diagnosis
   and gait correction of KOA patients.},
DOI = {10.5535/arm.20071},
ISSN = {2234-0645},
EISSN = {2234-0653},
ResearcherID-Numbers = {Park, Jae Hyeon/O-9384-2018
   },
ORCID-Numbers = {Jang, Seongho/0000-0003-0241-0954
   Park, Jae Hyeon/0000-0001-5619-4818
   Yang, Jung Ho/0000-0002-4142-8949},
Unique-ID = {WOS:000603445400001},
}

@article{ WOS:000865315000004,
Author = {Li, Anhai and Li, Xiaoyuan and Li, Wenwen and Yu, Xiaoqian and Qi,
   Mengmeng and Li, Ding},
Title = {Application of Deep Learning on the Prognosis of Cutaneous Melanoma
   Based on Full Scan Pathology Images},
Journal = {BIOMED RESEARCH INTERNATIONAL},
Year = {2022},
Volume = {2022},
Month = {AUG 28},
Abstract = {Introduction. The purpose of this study is to use deep learning and
   machine learning to learn and classify patients with cutaneous melanoma
   with different prognoses and to explore the application value of deep
   learning in the prognosis of cutaneous melanoma patients. Methods. In
   deep learning, VGG-19 is selected as the network architecture and
   learning model for learning and classification. In machine learning,
   deep features are extracted through the VGG-19 network architecture, and
   the support vector machine (SVM) model is selected for learning and
   classification. Compare and explore the application value of deep
   learning and machine learning in predicting the prognosis of patients
   with cutaneous melanoma. Result. According to receiver operating
   characteristic (ROC) curves and area under the curve (AUC), the average
   accuracy of deep learning is higher than that of machine learning, and
   even the lowest accuracy is better than that of machine learning.
   Conclusion. As the number of learning increases, the accuracy of machine
   learning and deep learning will increase, but in the same number of
   cutaneous melanoma patient pathology maps, the accuracy of deep learning
   will be higher. This study provides new ideas and theories for
   computational pathology in predicting the prognosis of patients with
   cutaneous melanoma.},
DOI = {10.1155/2022/4864485},
Article-Number = {4864485},
ISSN = {2314-6133},
EISSN = {2314-6141},
ResearcherID-Numbers = {Yu, Xiaoqian/HOF-2538-2023
   },
ORCID-Numbers = {Li, Ding/0000-0002-1677-7768
   Li, Xiaoyuan/0000-0003-3305-936X},
Unique-ID = {WOS:000865315000004},
}

@article{ WOS:001222668700001,
Author = {Rathje, Jason and Katila, Riitta and Reineke, Philipp},
Title = {Making the most of AI and machine learning in organizations and strategy
   research: Supervised machine learning, causal inference, and matching
   models},
Journal = {STRATEGIC MANAGEMENT JOURNAL},
Year = {2024},
Volume = {45},
Number = {10},
Pages = {1926-1953},
Month = {OCT},
Abstract = {We spotlight the use of machine learning in two-stage matching models to
   deal with sample selection bias. Recent advances in machine learning
   have unlocked new empirical possibilities for inductive theorizing. In
   contrast, the opportunities to use machine learning in regression
   studies involving large-scale data with many covariates and a causal
   claim are still less well understood. Our core contribution is to guide
   researchers in the use of machine learning approaches to choosing
   matching variables for enhanced causal inference in propensity score
   matching models. We use an analysis of real-world technology invention
   data of public-private relationships to demonstrate the method and find
   that machine learning can provide an alternative approach to ad hoc
   matching. However, as with any method, it is also important to
   understand its limitations. This article explores the use of machine
   learning to enhance decision-making, particularly in addressing sample
   selection bias in large-scale datasets. The rapid development of AI and
   machine learning offers new, powerful tools especially for digital
   ecosystems where complex data and causal relationships are complex to
   analyze. We offer managers and stakeholders insight into the effective
   integration of machine learning for selecting critical variables in
   propensity score matching models. Through a detailed examination of
   real-world data on technology inventions within public-private
   relationships, we demonstrate the effectiveness of machine learning as a
   robust alternative to traditional matching methods.},
DOI = {10.1002/smj.3604},
EarlyAccessDate = {MAY 2024},
ISSN = {0143-2095},
EISSN = {1097-0266},
ORCID-Numbers = {Katila, Riitta/0000-0001-8125-515X
   Reineke, Philipp/0009-0006-6962-020X},
Unique-ID = {WOS:001222668700001},
}

@article{ WOS:000526339000051,
Author = {Dral, Pavlo O.},
Title = {Quantum Chemistry in the Age of Machine Learning},
Journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS},
Year = {2020},
Volume = {11},
Number = {6},
Pages = {2336-2347},
Month = {MAR 19},
Abstract = {As the quantum chemistry (QC) community embraces machine learning (ML),
   the number of new methods and applications based on the combination of
   QC and ML is surging. In this Perspective, a view of the current state
   of affairs in this new and exciting research field is offered,
   challenges of using machine learning in quantum chemistry applications
   are described, and potential future developments are outlined.
   Specifically, examples of how machine learning is used to improve the
   accuracy and accelerate quantum chemical research are shown.
   Generalization and classification of existing techniques are provided to
   ease the navigation in the sea of literature and to guide researchers
   entering the field. The emphasis of this Perspective is on supervised
   machine learning.},
DOI = {10.1021/acs.jpclett.9b03664},
ISSN = {1948-7185},
ResearcherID-Numbers = {Dral, Pavlo/A-6089-2016},
ORCID-Numbers = {Dral, Pavlo/0000-0002-2975-9876},
Unique-ID = {WOS:000526339000051},
}

@article{ WOS:000519447700007,
Author = {Biswas, Rupayan and Rashmi, Richa and Lourderaj, Upakarasamy},
Title = {Machine Learning in Chemical Dynamics},
Journal = {RESONANCE-JOURNAL OF SCIENCE EDUCATION},
Year = {2020},
Volume = {25},
Number = {1},
Pages = {59-75},
Month = {JAN},
Abstract = {Machine learning has been applied to various fields and is envisaged as
   the technology of the future. We discuss here, the applications of
   machine learning methods to represent potential energy surfaces - an
   important aspect of chemical dynamics. We illustrate the process of
   machine learning using simple examples, and demonstrate how it can be
   extended to complicated problems.},
DOI = {10.1007/s12045-019-0922-1},
ISSN = {0971-8044},
EISSN = {0973-712X},
ORCID-Numbers = {Lourderaj, Upakarasamy/0000-0002-5550-9694
   , Richa Rashmi/0009-0009-6565-5511},
Unique-ID = {WOS:000519447700007},
}

@article{ WOS:000468604900004,
Author = {Stephenson, Natalie and Shane, Emily and Chase, Jessica and Rowland,
   Jason and Ries, David and Justice, Nicola and Zhang, Jie and Chan, Leong
   and Cao, Renzhi},
Title = {Survey of Machine Learning Techniques in Drug Discovery},
Journal = {CURRENT DRUG METABOLISM},
Year = {2019},
Volume = {20},
Number = {3},
Pages = {185-193},
Abstract = {Background: Drug discovery, which is the process of discovering new
   candidate medications, is very important for pharmaceutical industries.
   At its current stage, discovering new drugs is still a very expensive
   and time-consuming process, requiring Phases I, II and III for clinical
   trials. Recently, machine learning techniques in Artificial Intelligence
   (AI), especially the deep learning techniques which allow a
   computational model to generate multiple layers, have been widely
   applied and achieved state-of-the-art performance in different fields,
   such as speech recognition, image classification, bioinformatics, etc.
   One very important application of these AI techniques is in the field of
   drug discovery.
   Methods: We did a large-scale literature search on existing scientific
   websites (e.g, ScienceDirect, Arxiv) and start-up companies to
   understand current status of machine learning techniques in drug
   discovery.
   Results: Our experiments demonstrated that there are different patterns
   in machine learning fields and drug discovery fields. For example,
   keywords like prediction, brain, discovery, and treatment are usually in
   drug discovery fields. Also, the total number of papers published in
   drug discovery fields with machine learning techniques is increasing
   every year.
   Conclusion: The main focus of this survey is to understand the current
   status of machine learning techniques in the drug discovery field within
   both academic and industrial settings, and discuss its potential future
   applications. Several interesting patterns for machine learning
   techniques in drug discovery fields are discussed in this survey.},
DOI = {10.2174/1389200219666180820112457},
ISSN = {1389-2002},
EISSN = {1875-5453},
ORCID-Numbers = {Shane, Emily/0000-0003-2928-4679
   Chan, Leong/0000-0001-8647-8416},
Unique-ID = {WOS:000468604900004},
}

@article{ WOS:001198662400026,
Author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe
   and Belkin, Mikhail},
Title = {Mechanism for feature learning in neural networks and
   backpropagation-free machine learning models},
Journal = {SCIENCE},
Year = {2024},
Volume = {383},
Number = {6690},
Pages = {1461-1467},
Month = {MAR 29},
Abstract = {Understanding how neural networks learn features, or relevant patterns
   in data, for prediction is necessary for their reliable use in
   technological and scientific applications. In this work, we presented a
   unifying mathematical mechanism, known as average gradient outer product
   (AGOP), that characterized feature learning in neural networks. We
   provided empirical evidence that AGOP captured features learned by
   various neural network architectures, including transformer-based
   language models, convolutional networks, multilayer perceptrons, and
   recurrent neural networks. Moreover, we demonstrated that AGOP, which is
   backpropagation-free, enabled feature learning in machine learning
   models, such as kernel machines, that a priori could not identify
   task-specific features. Overall, we established a fundamental mechanism
   that captured feature learning in neural networks and enabled feature
   learning in general machine learning models.},
DOI = {10.1126/science.adi5639},
ISSN = {0036-8075},
EISSN = {1095-9203},
ResearcherID-Numbers = {Pandit, Parthe/AAE-8275-2022},
ORCID-Numbers = {Belkin, Mikhail/0009-0005-8633-3926
   Radhakrishnan, Adityanarayanan/0000-0002-0878-815X
   Beaglehole, Daniel/0000-0002-9665-3473
   },
Unique-ID = {WOS:001198662400026},
}

@article{ WOS:000456929100002,
Author = {Zhou YongZhang and Wang Jun and Zuo RenGuang and Xiao Fan and Shen
   WenJie and Wang ShuGong},
Title = {Machine learning, deep learning and Python language in field of geology},
Journal = {ACTA PETROLOGICA SINICA},
Year = {2018},
Volume = {34},
Number = {11},
Pages = {3173-3178},
Abstract = {Geological big data is exponentially expanding. It is the only way to
   catch up with its extraordinary growing to develop intelligent data
   processing. As the core of artificial intelligence, machine learning is
   a fundamental way to endow computer with intelligence. Machine learning
   has been becoming the front hotspot of geological big data mining. It
   will attach wings to geological big data mining, and thereby bring
   revolution to geological research. Machine learning is a data adaptive
   training process and model, resulting in giving a good performance
   decision. As a subclass of machine learning, deep learning develops
   machine learning model with various hidden layers, and makes iterative
   evolution of the model through massive data training, and finally
   extracts essential features to help more exactly classing and
   predicting. The convolution neural network is one of the most frequently
   used deep learning algorithms. It is widely used in image recognition
   and speech analysis. Python language is playing an increasingly
   important role in science research. The Python Scikit-Learn is a machine
   learning-oriented library to provide with data preprocessing,
   classification, regression, clustering, prediction, model analysis and
   other modules. The Keras is a Python deep learning library based on
   Theano and Tensorflow, and can be used to construct concise artificial
   neural network.},
ISSN = {1000-0569},
EISSN = {2095-8927},
ResearcherID-Numbers = {Xiao, Fan/ADT-9995-2022
   Zuo, Renguang/AAW-4922-2021
   Shen, Wenjie/H-5343-2011},
Unique-ID = {WOS:000456929100002},
}

@article{ WOS:000696993100003,
Author = {Ledesma, Dakila and Symes, Steven and Richards, Sean},
Title = {Advancements within Modern Machine Learning Methodology: Impacts and
   Prospects in Biomarker Discovery},
Journal = {CURRENT MEDICINAL CHEMISTRY},
Year = {2021},
Volume = {28},
Number = {32},
Pages = {6512-6531},
Abstract = {Background: The adoption of biomarkers as part of high-throughput,
   complex microarray or sequencing data has necessitated the discovery and
   validation of these data through machine learning. Machine learning has
   remained a fundamental and indispensable tool due to its efficacy and
   efficiency in both feature extraction of relevant biomarkers as well as
   the classification of samples as validation of the discovered
   biomarkers. Objectives: This review aims to present the impact and
   ability of various machine learning methodologies and models to process
   high-throughput, high-dimensionality data found within mass
   spectrometry, microarray, and DNA/RNA-sequence data; data that precluded
   biomarker discovery prior to the use of machine learning. Methods: A
   vast array of literature highlighting machine learning for biomarker
   discovery was reviewed, resulting in the eligibility of 21 machine
   learning algorithms/networks and 3 combinatory architectures, spanning
   17 fields of study. This literature was screened to investigate the
   usage and development of machine learning within the framework of
   biomarker discovery. Results: Out of the 93 papers collected, a total of
   62 biomarker studies were further reviewed across different subfields-49
   of which employed machine learning algorithms, and 13 of which employed
   neural network-based models. Through the application, innovation, and
   creation of tools in biomarker-related machine learning methodologies,
   its use allowed for the discovery, accumulation, validation, and
   interpretation of biomarkers within varied data formats, sources, as
   well as fields of study. Conclusion: The use of machine learning
   methodologies for biomarker discovery is critical to the analysis of
   various types of data used for biomarker discovery, such as mass
   spectrometry, nucleotide and protein sequencing, and image (e.g.
   CT-scan) data. Further studies containing more standardized techniques
   for evaluation, and the use of cutting-edge machine learning
   architectures may lead to more accurate and specific results.},
DOI = {10.2174/0929867328666210208111821},
ISSN = {0929-8673},
EISSN = {1875-533X},
ORCID-Numbers = {Ledesma, Dakila/0000-0002-0614-3155
   Richards, Sean/0000-0003-2251-3184},
Unique-ID = {WOS:000696993100003},
}

@article{ WOS:000993072400001,
Author = {Bashar, Mohammad Z. and Torres-Machi, Cristina},
Title = {Machine learning to enhance the management of highway pavements and
   bridges},
Journal = {INFRASTRUCTURE ASSET MANAGEMENT},
Year = {2023},
Volume = {11},
Number = {3},
Pages = {119-127},
Month = {APR 5},
Abstract = {The adoption of machine learning in transportation asset management is
   hindered by the perception of being a black box, the natural resistance
   to change, and the challenges of integration with existing management
   systems. This paper aims to enhance the understanding of machine
   learning and provide guidance for the development and implementation of
   machine learning to support decision-making in the management of highway
   pavements and bridges. The paper identifies successful research efforts
   using machine learning, identifies opportunities and challenges in
   adopting machine learning, and derives recommendations on when and how
   to apply different machine learning algorithms to support asset
   management decisions. Four main challenges were identified: the
   trade-off between accuracy and interpretability, the shortage of machine
   learning engineers, data quality, and the limitations of machine
   learning algorithms. Although the complexities associated with training
   machine learning algorithms challenge the short-term implementation,
   machine learning offer a wide range of opportunities when compared to
   traditional approaches. The development of hybrid systems combining
   machine learning algorithms with expert opinions and traditional
   approaches seems a reasonable step forward to support agencies asset
   management decisions.},
DOI = {10.1680/jinam.22.00031},
EarlyAccessDate = {APR 2023},
ISSN = {2053-0242},
EISSN = {2053-0250},
ResearcherID-Numbers = {Bashar, MD. Abu/ABC-9917-2020
   Torres-Machi, Cristina/G-3210-2015},
ORCID-Numbers = {Ibne Bashar, Mohammad Zobair/0000-0001-9896-2395
   Torres-Machi, Cristina/0000-0002-4334-4474},
Unique-ID = {WOS:000993072400001},
}

@article{ WOS:001248184000003,
Author = {Ajorloo, Sedighe and Jamarani, Amirhossein and Kashfi, Mehdi and
   Kashani, Mostafa Haghi and Najafizadeh, Abbas},
Title = {A systematic review of machine learning methods in software testing},
Journal = {APPLIED SOFT COMPUTING},
Year = {2024},
Volume = {162},
Month = {SEP},
Abstract = {Background: The quest for higher software quality remains a paramount
   concern in software testing, prompting a shift towards leveraging
   machine learning techniques for enhanced testing efficacy. Objective:
   The objective of this paper is to identify, categorize, and
   systematically compare the present studies on software testing utilizing
   machine learning methods. Method: This study conducts a systematic
   literature review (SLR) of 40 pertinent studies spanning from 2018 to
   March 2024 to comprehensively analyze and classify machine learning
   methods in software testing. The review encompasses supervised learning,
   unsupervised learning, reinforcement learning, and hybrid learning
   approaches. Results: The strengths and weaknesses of each reviewed paper
   are dissected in this study. This paper also provides an in-depth
   analysis of the merits of machine learning methods in the context of
   software testing and addresses current unresolved issues. Potential
   areas for future research have been discussed, and statistics of each
   review paper have been collected. Conclusion: By addressing these
   aspects, this study contributes to advancing the discourse on machine
   learning's role in software testing and paves the way for substantial
   improvements in testing efficacy and software quality.},
DOI = {10.1016/j.asoc.2024.111805},
EarlyAccessDate = {MAY 2024},
Article-Number = {111805},
ISSN = {1568-4946},
EISSN = {1872-9681},
ResearcherID-Numbers = {Haghi Kashani, Mostafa/AAO-4921-2021
   Jamarani, Amirhossein/ITU-9833-2023
   },
ORCID-Numbers = {Haghi Kashani, Mostafa/0000-0002-9812-1331},
Unique-ID = {WOS:001248184000003},
}

@article{ WOS:000532822600001,
Author = {Jacobucci, Ross and Grimm, Kevin J.},
Title = {Machine Learning and Psychological Research: The Unexplored Effect of
   Measurement},
Journal = {PERSPECTIVES ON PSYCHOLOGICAL SCIENCE},
Year = {2020},
Volume = {15},
Number = {3},
Pages = {809-816},
Month = {MAY},
Abstract = {Machine learning (i.e., data mining, artificial intelligence, big data)
   has been increasingly applied in psychological science. Although some
   areas of research have benefited tremendously from a new set of
   statistical tools, most often in the use of biological or genetic
   variables, the hype has not been substantiated in more traditional areas
   of research. We argue that this phenomenon results from measurement
   errors that prevent machine-learning algorithms from accurately modeling
   nonlinear relationships, if indeed they exist. This shortcoming is
   showcased across a set of simulated examples, demonstrating that model
   selection between a machine-learning algorithm and regression depends on
   the measurement quality, regardless of sample size. We conclude with a
   set of recommendations and a discussion of ways to better integrate
   machine learning with statistics as traditionally practiced in
   psychological science.},
DOI = {10.1177/1745691620902467},
EarlyAccessDate = {APR 2020},
Article-Number = {1745691620902467},
ISSN = {1745-6916},
EISSN = {1745-6924},
ORCID-Numbers = {Jacobucci, Ross/0000-0001-7818-7424},
Unique-ID = {WOS:000532822600001},
}

@article{ WOS:000856997600004,
Author = {Zhong, Xiaoting and Gallagher, Brian and Liu, Shusen and Kailkhura,
   Bhavya and Hiszpanski, Anna and Han, T. Yong-Jin},
Title = {Explainable machine learning in materials science},
Journal = {NPJ COMPUTATIONAL MATERIALS},
Year = {2022},
Volume = {8},
Number = {1},
Month = {SEP 22},
Abstract = {Machine learning models are increasingly used in materials studies
   because of their exceptional accuracy. However, the most accurate
   machine learning models are usually difficult to explain. Remedies to
   this problem lie in explainable artificial intelligence (XAI), an
   emerging research field that addresses the explainability of complicated
   machine learning models like deep neural networks (DNNs). This article
   attempts to provide an entry point to XAI for materials scientists.
   Concepts are defined to clarify what explain means in the context of
   materials science. Example works are reviewed to show how XAI helps
   materials science research. Challenges and opportunities are also
   discussed.},
DOI = {10.1038/s41524-022-00884-7},
Article-Number = {204},
EISSN = {2057-3960},
ORCID-Numbers = {Hiszpanski, Anna/0000-0002-2705-3263},
Unique-ID = {WOS:000856997600004},
}

@article{ WOS:000529286600001,
Author = {Sturm, Noe and Mayr, Andreas and Thanh Le Van and Chupakhin, Vladimir
   and Ceulemans, Hugo and Wegner, Joerg and Golib-Dzib, Jose-Felipe and
   Jeliazkova, Nina and Vandriessche, Yves and Bohm, Stanislav and Cima,
   Vojtech and Martinovic, Jan and Greene, Nigel and Vander Aa, Tom and
   Ashby, Thomas J. and Hochreiter, Sepp and Engkvist, Ola and Klambauer,
   Guenter and Chen, Hongming},
Title = {Industry-scale application and evaluation of deep learning for drug
   target prediction},
Journal = {JOURNAL OF CHEMINFORMATICS},
Year = {2020},
Volume = {12},
Number = {1},
Month = {APR 19},
Abstract = {Artificial intelligence (AI) is undergoing a revolution thanks to the
   breakthroughs of machine learning algorithms in computer vision, speech
   recognition, natural language processing and generative modelling.
   Recent works on publicly available pharmaceutical data showed that AI
   methods are highly promising for Drug Target prediction. However, the
   quality of public data might be different than that of industry data due
   to different labs reporting measurements, different measurement
   techniques, fewer samples and less diverse and specialized assays. As
   part of a European funded project (ExCAPE), that brought together
   expertise from pharmaceutical industry, machine learning, and
   high-performance computing, we investigated how well machine learning
   models obtained from public data can be transferred to internal
   pharmaceutical industry data. Our results show that machine learning
   models trained on public data can indeed maintain their predictive power
   to a large degree when applied to industry data. Moreover, we observed
   that deep learning derived machine learning models outperformed
   comparable models, which were trained by other machine learning
   algorithms, when applied to internal pharmaceutical company datasets. To
   our knowledge, this is the first large-scale study evaluating the
   potential of machine learning and especially deep learning directly at
   the level of industry-scale settings and moreover investigating the
   transferability of publicly learned target prediction models towards
   industrial bioactivity prediction pipelines.},
DOI = {10.1186/s13321-020-00428-5},
Article-Number = {26},
ISSN = {1758-2946},
ResearcherID-Numbers = {Hochreiter, Sepp/AAI-5904-2020
   Martinovič, Jan/G-3846-2019
   GOLIB, felipe/L-5422-2013
   Chupakhin, Vladimir/AAI-1859-2021
   Klambauer, Günter/C-5761-2015
   Jeliazkova, Nina/D-2499-2010
   Martinovic, Jan/G-3846-2019
   Wegner, Jörg/C-6598-2009
   Engkvist, Ola/Y-8395-2019
   Chupakhin, Vladimir/G-3050-2010},
ORCID-Numbers = {Jeliazkova, Nina/0000-0002-4322-6179
   Vander Aa, Tom/0000-0002-1504-5266
   Engkvist, Ola/0000-0003-4970-6461
   Greene, Nigel/0000-0003-0433-4596
   Wegner, Jorg Kurt/0000-0002-1852-9434
   Martinovic, Jan/0000-0001-7944-8956
   Chupakhin, Vladimir/0000-0003-1097-8603},
Unique-ID = {WOS:000529286600001},
}

@article{ WOS:000462654200002,
Author = {McIntosh, Andrea and Hassan, Safwat and Hindle, Abram},
Title = {What can Android mobile app developers do about the energy consumption
   of machine learning?},
Journal = {EMPIRICAL SOFTWARE ENGINEERING},
Year = {2019},
Volume = {24},
Number = {2},
Pages = {562-601},
Month = {APR},
Abstract = {Machine learning is a popular method of learning functions from data to
   represent and to classify sensor inputs, multimedia, emails, and
   calendar events. Smartphone applications have been integrating more and
   more intelligence in the form of machine learning. Machine learning
   functionality now appears on most smartphones as voice recognition,
   spell checking, word disambiguation, face recognition, translation,
   spatial reasoning, and even natural language summarization. Excited app
   developers who want to use machine learning on mobile devices face one
   serious constraint that they did not face on desktop computers or cloud
   virtual machines: the end-user's mobile device has limited battery life,
   thus computationally intensive tasks can harm end users' phone
   availability by draining batteries of their stored energy. Currently,
   there are few guidelines for developers who want to employ machine
   learning on mobile devices yet are concerned about software energy
   consumption of their applications. In this paper, we combine empirical
   measurements of different machine learning algorithm implementations
   with complexity theory to provide concrete and theoretically grounded
   recommendations to developers who want to employ machine learning on
   smartphones. We conclude that some implementations of algorithms, such
   as J48, MLP, and SMO, do generally perform better than others in terms
   of energy consumption and accuracy, and that energy consumption is
   well-correlated to algorithmic complexity. However, to achieve optimal
   results a developer must consider their specific application as many
   factors dataset size, number of data attributes, whether the model will
   require updating, etc. affect which machine learning algorithm and
   implementation will provide the best results.},
DOI = {10.1007/s10664-018-9629-2},
ISSN = {1382-3256},
EISSN = {1573-7616},
ResearcherID-Numbers = {Hassan, Safwat/JWP-1498-2024
   },
ORCID-Numbers = {Hindle, Abram/0000-0002-4373-4958},
Unique-ID = {WOS:000462654200002},
}

@article{ WOS:000704764200003,
Author = {Rahimi, M. Hossein and Huynh, Hoai Nam and Altintas, Yusuf},
Title = {On-line chatter detection in milling with hybrid machine learning and
   physics-based model},
Journal = {CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {35},
Pages = {25-40},
Month = {NOV},
Abstract = {Unstable vibrations, chatter, in machining lead to poor surface finish
   and damage to the tool and machine. It is desired to detect and avoid
   chatter on-line without false alarms for improved productivity. This
   paper presents the application of a combined machine learning network
   and physics-based model to detect chatter in milling. The vibration data
   collected during machining is converted into moving short-time frequency
   spectrums, whose features are mapped to five machining states as air
   cut, entry into and exit from the workpiece, stable cut, and chatter
   conditions. The machine learning network was trained and its
   architecture was reduced to a computationally optimal network with 3
   convolution blocks followed by a neural network with one hidden layer. A
   parallel algorithm, which Kalman filters the stable forced vibrations to
   isolate chatter signals in raw data, is used to detect the chatter and
   its frequency. The combination of the machine learning and physics-based
   model led to a 98.90\% success rate in chatter detection while allowing
   to further train the network during production with the help of the
   physics-based, deterministic model. (c) 2021 CIRP.},
DOI = {10.1016/j.cirpj.2021.05.006},
EarlyAccessDate = {JUN 2021},
ISSN = {1755-5817},
EISSN = {1878-0016},
ResearcherID-Numbers = {Huynh, Hoai/AAE-1178-2019},
Unique-ID = {WOS:000704764200003},
}

@article{ WOS:000989319200011,
Author = {Haug, Charlotte J. J. and Drazen, Jeffrey M. M.},
Title = {Artificial Intelligence and Machine Learning in Clinical Medicine, 2023},
Journal = {NEW ENGLAND JOURNAL OF MEDICINE},
Year = {2023},
Volume = {388},
Number = {13},
Pages = {1201-1208},
Month = {MAR 30},
Abstract = {AI and Machine Learning in Clinical Medicine, 2023This first article in
   a series describes the history of artificial intelligence in medicine;
   the use of AI in image analysis, identification of disease outbreaks,
   and diagnosis; and the use of chatbots.},
DOI = {10.1056/NEJMra2302038},
ISSN = {0028-4793},
EISSN = {1533-4406},
ORCID-Numbers = {Haug, Charlotte Johanne/0000-0001-8217-7376},
Unique-ID = {WOS:000989319200011},
}

@article{ WOS:000469544400001,
Author = {Marcelino, Pedro and Antunes, Maria de Lurdes and Fortunato, Eduardo and
   Gomes, Marta Castilho},
Title = {Machine learning approach for pavement performance prediction},
Journal = {INTERNATIONAL JOURNAL OF PAVEMENT ENGINEERING},
Year = {2021},
Volume = {22},
Number = {3},
Pages = {341-354},
Month = {FEB 23},
Abstract = {In recent years, there has been an increasing interest in the
   application of machine learning for the prediction of pavement
   performance. Prediction models are used to predict the future pavement
   condition, helping to optimally allocate maintenance and rehabilitation
   funds. However, few studies have proposed a systematic approach to the
   development of machine learning models for pavement performance
   prediction. Most of the studies focus on artificial neural networks
   models that are trained for high accuracy, disregarding other suitable
   machine learning algorithms and neglecting the importance of models'
   generalisation capability for Pavement Engineering applications. This
   paper proposes a general machine learning approach for the development
   of pavement performance prediction models in pavement management systems
   (PMS). The proposed approach supports different machine learning
   algorithms and emphasizes generalisation performance. A case study for
   prediction of International Roughness Index (IRI) for 5 and 10-years,
   using the Long-Term Pavement Performance, is presented. The proposed
   models were based on a random forest algorithm, using datasets
   comprising previous IRI measurements, structural, climatic, and traffic
   data.},
DOI = {10.1080/10298436.2019.1609673},
EarlyAccessDate = {MAY 2019},
ISSN = {1029-8436},
EISSN = {1477-268X},
ResearcherID-Numbers = {Fortunato, Eduardo/Y-5737-2019
   },
ORCID-Numbers = {Antunes, Maria de Lurdes/0000-0002-1911-517X
   Gomes, Marta Castilho/0000-0002-6662-7397
   Fortunato, Eduardo/0000-0002-9968-0821},
Unique-ID = {WOS:000469544400001},
}

@article{ WOS:000449644300007,
Author = {Takahashi, Keisuke and Miyazato, Itsuki},
Title = {Rapid estimation of activation energy in heterogeneous catalytic
   reactions via machine learning},
Journal = {JOURNAL OF COMPUTATIONAL CHEMISTRY},
Year = {2018},
Volume = {39},
Number = {28},
Pages = {2405-2408},
Month = {OCT 30},
Abstract = {Estimation of activation energies within heterogeneous catalytic
   reactions is performed using machine learning and catalysts dataset. In
   particular, descriptors for determining activation energy are revealed
   within the 788 activation energy dataset. With the implementation of
   machine learning and chosen descriptors, activation energy can be
   instantly predicted with over 90\% accuracy during cross-validation.
   Thus, rapid estimation of activation energies within heterogeneous
   catalytic reactions can be made achievable via machine learning, leading
   toward the acceleration of catalysts design and characterization. (c)
   2018 Wiley Periodicals, Inc.},
DOI = {10.1002/jcc.25567},
ISSN = {0192-8651},
EISSN = {1096-987X},
ResearcherID-Numbers = {Takahashi, Keisuke/Q-3874-2019
   Miyazato, Itsuki/T-9606-2019},
ORCID-Numbers = {Takahashi, Keisuke/0000-0002-9328-1694
   Miyazato, Itsuki/0000-0002-1533-9790},
Unique-ID = {WOS:000449644300007},
}

@article{ WOS:001032105900001,
Author = {Nassis, George P. and Verhagen, Evert and Brito, Joao and Figueiredo,
   Pedro and Krustrup, Peter},
Title = {A review of machine learning applications in soccer with an emphasis on
   injury risk},
Journal = {BIOLOGY OF SPORT},
Year = {2023},
Volume = {40},
Number = {1},
Pages = {233-239},
Abstract = {This narrative review paper aimed to discuss the literature on machine
   learning applications in soccer with an emphasis on injury risk
   assessment. A secondary aim was to provide practical tips for the health
   and performance staff in soccer clubs on how machine learning can
   provide a competitive advantage. Performance analysis is the area with
   the majority of research so far. Other domains of soccer science and
   medicine with machine learning use are injury risk assessment, players'
   workload and wellness monitoring, movement analysis, players' career
   trajectory, club performance, and match attendance. Regarding injuries,
   which is a hot topic, machine learning does not seem to have a high
   predictive ability at the moment (models specificity ranged from
   74.2\%-97.7\%. sensitivity from 15.2\%-55.6\% with area under the curve
   of 0.66-0.83). It seems, though, that machine learning can help to
   identify the early signs of elevated risk for a musculoskeletal injury.
   Future research should account for musculoskeletal injuries' dynamic
   nature for machine learning to provide more meaningful results for
   practitioners in soccer.},
ISSN = {0860-021X},
EISSN = {2083-1862},
ResearcherID-Numbers = {brito, joao/IQU-5611-2023
   Nassis, George/AAC-4938-2020
   Figueiredo, Pedro/J-4178-2013
   Krustrup, Peter/D-2659-2019
   Verhagen, Evert/A-1502-2013},
ORCID-Numbers = {Nassis, George/0000-0003-2953-3911
   Figueiredo, Pedro/0000-0001-5515-3694
   Krustrup, Peter/0000-0002-1461-9838
   },
Unique-ID = {WOS:001032105900001},
}

@article{ WOS:000730393300003,
Author = {Milojevic, Nenad and Redzepagic, Srdjan},
Title = {Prospects of Artificial Intelligence and Machine Learning Application in
   Banking Risk Management},
Journal = {JOURNAL OF CENTRAL BANKING THEORY AND PRACTICE},
Year = {2021},
Volume = {10},
Number = {3},
Pages = {41-57},
Month = {SEP},
Abstract = {Artificial intelligence and machine learning have increasing influence
   on the financial sector, but also on economy as a whole. The impact of
   artificial intelligence and machine learning on banking risk management
   has become particularly interesting after the global financial crisis.
   The research focus is on artificial intelligence and machine learning
   potential for further banking risk management improvement. The paper
   seeks to explore the possibility for successful implementation yet
   taking into account challenges and problems which might occur as well as
   potential solutions. Artificial intelligence and machine learning have
   potential to support the mitigation measures for the contemporary global
   economic and financial challenges, including those caused by the
   COVID-19 crisis. The main focus in this paper is on credit risk
   management, but also on analysing artificial intelligence and machine
   learning application in other risk management areas. It is concluded
   that a measured and well-prepared further application of artificial
   intelligence, machine learning, deep learning and big data analytics can
   have further positive impact, especially on the following risk
   management areas: credit, market, liquidity, operational risk, and other
   related areas.},
DOI = {10.2478/jcbtp-2021-0023},
ISSN = {1800-9581},
EISSN = {2336-9205},
Unique-ID = {WOS:000730393300003},
}

@article{ WOS:001262652200001,
Author = {Lim Meng Kee, Heimrih and Ahmad, Norulhusna and Azri Mohd Izhar, Mohd
   and Anwar, Khoirul and Ng, Soon Xin},
Title = {A Review on Machine Learning for Channel Coding},
Journal = {IEEE ACCESS},
Year = {2024},
Volume = {12},
Pages = {89002-89025},
Abstract = {The usage of artificial intelligence and machine learning in wireless
   communications is the stepping stone towards a technological
   breakthrough in the current limitations of wireless communication
   systems. The trend of future coding schemes towards 6G appears to be
   based on rateless schemes and machine learning. Channel coding is
   important when transmitting data or information reliably as it provides
   error-correcting purposes. However, there is still a demand for more
   research regarding machine learning for channel coding. There is also a
   lack of a specific term or classification for existing machine learning
   applications for channel coding. This paper explores and compiles
   current trending machine learning techniques for channel coding. We are
   also introducing and proposing a new type of machine learning
   classification for channel coding purposes, as well as surveying some of
   the papers that fall under the respective class. This paper also
   discusses current challenges and future machine learning trends for
   channel coding, which are expected to impact future wireless
   communications development, especially in channel coding advancements.},
DOI = {10.1109/ACCESS.2024.3412192},
ISSN = {2169-3536},
ResearcherID-Numbers = {Ng, Soon/AAC-8281-2020
   Anwar, Khoirul/AAB-2200-2019
   Ahmad, Norulhusna/C-9682-2010
   Ahmad, norulhusna/ABB-6075-2020
   Mohd Izhar, Mohd Azri/D-1542-2014},
ORCID-Numbers = {Ahmad, Norulhusna/0000-0001-9991-343X
   Mohd Izhar, Mohd Azri/0000-0003-3761-0630},
Unique-ID = {WOS:001262652200001},
}

@article{ WOS:000435287000005,
Author = {Ramasamy Ramamurthy, Sreenivasan and Roy, Nirmalya},
Title = {Recent trends in machine learning for human activity recognition-A
   survey},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY},
Year = {2018},
Volume = {8},
Number = {4},
Month = {JUL-AUG},
Abstract = {There has been an upsurge recently in investigating machine learning
   techniques for activity recognition (AR) problems as they have been very
   effective in extracting and learning knowledge from the activity
   datasets. The technique ranges from heuristically derived hand-crafted
   feature-based traditional machine learning algorithms to the recently
   developed hierarchically self-evolving feature-based deep learning
   algorithms. AR continues to remain a challenging problem in uncontrolled
   smart environments despite the amount of work contributed by the
   researcher in this field. The complex, volatile, and chaotic nature of
   the activity data presents numerous challenges that influence the
   performance of the AR systems in the wild. In this article, we present a
   comprehensive overview of recent machine learning and data mining
   techniques generally employed for AR and the underpinning problems and
   challenges associated with the existing systems. We also articulate the
   recent advances and state-of-the-art techniques in this domain in an
   attempt to identify the possible directions for future AR research. This
   article is categorized under:
   Application Areas > Science and Technology Algorithmic Development >
   Spatial and Temporal Data Mining Technologies > Machine Learning
   Fundamental Concepts of Data and Knowledge > Motivation and Emergence of
   Data Mining},
DOI = {10.1002/widm.1254},
Article-Number = {e1254},
ISSN = {1942-4787},
EISSN = {1942-4795},
ResearcherID-Numbers = {Ramasamy Ramamurthy, Sreenivasan/AAU-8478-2021
   },
ORCID-Numbers = {Ramasamy Ramamurthy, Sreenivasan/0000-0002-7561-9057},
Unique-ID = {WOS:000435287000005},
}

@article{ WOS:000399838700001,
Author = {Lemaitre, Guillaume and Nogueira, Fernando and Aridas, Christos K.},
Title = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced
   Datasets in Machine Learning},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2017},
Volume = {18},
Abstract = {imbalanced-learn is an open-source python toolbox aiming at providing a
   wide range of methods to cope with the problem of imbalanced dataset
   frequently encountered in machine learning and pattern recognition. The
   implemented state-of-the-art methods can be categorized into 4 groups:
   (i) under-sampling, (ii) over-sampling, (iii) combination of over and
   under-sampling, and (iv) ensemble learning methods. The proposed toolbox
   depends only on numpy, scipy, and scikit-learn and is distributed under
   MIT license. Furthermore, it is fully compatible with scikit-learn and
   is part of the scikit-learn-contrib supported project. Documentation,
   unit tests as well as integration tests are provided to ease usage and
   contribution. Source code, binaries, and documentation can be downloaded
   from https://github.com/scikit-learn-contrib/imbalanced-learn.},
Article-Number = {17},
ISSN = {1532-4435},
ORCID-Numbers = {Lemaitre, Guillaume/0000-0002-0897-6791
   Aridas, Christos/0000-0002-5021-1442},
Unique-ID = {WOS:000399838700001},
}

@article{ WOS:000685204500060,
Author = {Cambronero, Jose P. and Rinard, Martin C.},
Title = {AL: Autogenerating Supervised Learning Programs},
Journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
Year = {2019},
Volume = {3},
Month = {OCT},
Abstract = {We present AL, a novel automated machine learning system that learns to
   generate new supervised learning pipelines from an existing corpus of
   supervised learning programs. In contrast to existing automated machine
   learning tools, which typically implement a search over manually
   selected machine learning functions and classes, AL learns to identify
   the relevant classes in an API by analyzing dynamic program traces that
   use the target machine learning library. AL constructs a conditional
   probability model from these traces to estimate the likelihood of the
   generated supervised learning pipelines and uses this model to guide the
   search to generate pipelines for new datasets. Our evaluation shows that
   AL can produce successful pipelines for datasets that previous systems
   fail to process and produces pipelines with comparable predictive
   performance for datasets that previous systems process successfully.},
DOI = {10.1145/3360601},
Article-Number = {175},
EISSN = {2475-1421},
Unique-ID = {WOS:000685204500060},
}

@article{ WOS:000712048500001,
Author = {Lin, Eugene and Lin, Chieh-Hsin and Lane, Hsien-Yuan},
Title = {Machine Learning and Deep Learning for the Pharmacogenomics of
   Antidepressant Treatments},
Journal = {CLINICAL PSYCHOPHARMACOLOGY AND NEUROSCIENCE},
Year = {2021},
Volume = {19},
Number = {4},
Pages = {577-588},
Month = {NOV},
Abstract = {A growing body of evidence now proposes that machine learning and deep
   learning techniques can serve as a vital foundation for the
   pharmacogenomics of antidepressant treatments in patients with major
   depressive disorder (MDD). In this review, we focus on the latest
   developments for pharmacogenomics research using machine learning and
   deep learning approaches together with neuroimaging and multi-omics
   data. First, we review relevant pharmacogenomics studies that leverage
   numerous machine learning and deep learning techniques to determine
   treatment prediction and potential biomarkers for antidepressant
   treatments in MDD. In addition, we depict some neuroimaging
   pharmacogenomics studies that utilize various machine learning
   approaches to predict antidepressant treatment outcomes in MDD based on
   the integration of research on pharmacogenomics and neuroimaging.
   Moreover, we summarize the limitations in regard to the past
   pharmacogenomics studies of antidepressant treatments in MDD. Finally,
   we outline a discussion of challenges and directions for future
   research. In light of latest advancements in neuroimaging and
   multi-omics, various genomic variants and biomarkers associated with
   antidepressant treatments in MDD are being identified in
   pharmacogenomics research by employing machine learning and deep
   learning algorithms.},
DOI = {10.9758/cpn.2021.19.4.577},
ISSN = {1738-1088},
EISSN = {2093-4327},
ORCID-Numbers = {Lane, Hsien-Yuan/0000-0003-2162-8174},
Unique-ID = {WOS:000712048500001},
}

@article{ WOS:000741981200001,
Author = {Denissen, Stijn and Chen, Oliver Y. and De Mey, Johan and De Vos,
   Maarten and Van Schependom, Jeroen and Sima, Diana Maria and Nagels, Guy},
Title = {Towards Multimodal Machine Learning Prediction of Individual Cognitive
   Evolution in Multiple Sclerosis},
Journal = {JOURNAL OF PERSONALIZED MEDICINE},
Year = {2021},
Volume = {11},
Number = {12},
Month = {DEC},
Abstract = {Multiple sclerosis (MS) manifests heterogeneously among persons
   suffering from it, making its disease course highly challenging to
   predict. At present, prognosis mostly relies on biomarkers that are
   unable to predict disease course on an individual level. Machine
   learning is a promising technique, both in terms of its ability to
   combine multimodal data and through the capability of making
   personalized predictions. However, most investigations on machine
   learning for prognosis in MS were geared towards predicting physical
   deterioration, while cognitive deterioration, although prevalent and
   burdensome, remained largely overlooked. This review aims to boost the
   field of machine learning for cognitive prognosis in MS by means of an
   introduction to machine learning and its pitfalls, an overview of
   important elements for study design, and an overview of the current
   literature on cognitive prognosis in MS using machine learning.
   Furthermore, the review discusses new trends in the field of machine
   learning that might be adopted for future studies in the field.},
DOI = {10.3390/jpm11121349},
Article-Number = {1349},
EISSN = {2075-4426},
ResearcherID-Numbers = {De Vos, Maarten/Q-4650-2018
   CHÉN, Oliver/AAQ-4160-2021
   de Mey, Johan/F-3602-2011
   Van Schependom, J./AAO-9017-2020
   },
ORCID-Numbers = {de Mey, Johan/0000-0002-3601-3212
   De Vos, Maarten/0000-0002-3482-5145
   Denissen, Stijn/0000-0003-2852-5530
   Sima, Diana M/0000-0002-0118-2905
   Van Schependom, Jeroen/0000-0003-1200-5872
   Chen, Oliver Y./0000-0002-5696-3127
   Nagels, Guy/0000-0002-2597-0383},
Unique-ID = {WOS:000741981200001},
}

@article{ WOS:000814423300006,
Author = {Wu, Xing and Chen, Cheng and Li, Pan and Zhong, Mingyu and Wang, Jianjia
   and Qian, Quan and Ding, Peng and Yao, Junfeng and Guo, Yike},
Title = {FTAP: Feature transferring autonomous machine learning pipeline},
Journal = {INFORMATION SCIENCES},
Year = {2022},
Volume = {593},
Pages = {385-397},
Month = {MAY},
Abstract = {An effective method in machine learning often involves considerable
   experience with algorithms and domain expertise. Many existing machine
   learning methods highly rely on feature selection which are always
   domain-specific. However, the intervention by data scientists is
   time-consuming and labor-intensive. To meet this challenge, we propose a
   Feature Transferring Autonomous machine learning Pipeline (FTAP) to
   improve efficiency and performance. The proposed FTAP has been
   extensively evaluated on different modalities of data covering audios,
   images, and texts. Experimental results demonstrate that the proposed
   FTAP not only outperforms state-of-the-art methods on ESC-50 dataset
   with multi-class audio classification but also has good performance in
   distant domain transfer learning. Furthermore, FTAP outperforms TPOT, a
   state-of-the-art autonomous machine learning tool, on learning tasks.
   The quantitative and qualitative analysis proves the feasibility and
   robustness of the proposed FTAP. (C) 2022 Elsevier Inc. All rights
   reserved.},
DOI = {10.1016/j.ins.2022.02.006},
EarlyAccessDate = {FEB 2022},
ISSN = {0020-0255},
EISSN = {1872-6291},
ResearcherID-Numbers = {Pan, Li/GPK-1151-2022
   郭, 伊可/GYD-3212-2022
   Yao, Junfeng/ABE-6440-2020
   Ding, Peng/G-4669-2013
   },
ORCID-Numbers = {Qian, Quan/0000-0002-3020-005X
   Li, Pan/0000-0002-9289-4712
   Ding, Peng/0000-0003-1959-7794
   Guo, Yike/0009-0005-8401-282X},
Unique-ID = {WOS:000814423300006},
}

@article{ WOS:000441903100001,
Author = {Tiffin, Paul A. and Paton, Lewis W.},
Title = {Rise of the machines? Machine learning approaches and mental health:
   opportunities and challenges},
Journal = {BRITISH JOURNAL OF PSYCHIATRY},
Year = {2018},
Volume = {213},
Number = {3},
Pages = {509-510},
Month = {SEP},
Abstract = {Machine learning methods are being increasingly applied to physical
   healthcare. In this article we describe some of the potential benefits,
   challenges and limitations of this approach in a mental health context.
   We provide a number of examples where machine learning could add value
   beyond conventional statistical modelling.Declaration of interestNone.},
DOI = {10.1192/bjp.2018.105},
ISSN = {0007-1250},
EISSN = {1472-1465},
ResearcherID-Numbers = {Tiffin, Paul/AAO-1834-2020
   },
ORCID-Numbers = {Paton, Lewis William/0000-0002-3328-5634
   Tiffin, Paul/0000-0003-1770-5034},
Unique-ID = {WOS:000441903100001},
}

@article{ WOS:000797778000010,
Author = {Filom, Siyavash and Amiri, Amir M. and Razavi, Saiedeh},
Title = {Applications of machine learning methods in port operations-A systematic
   literature review},
Journal = {TRANSPORTATION RESEARCH PART E-LOGISTICS AND TRANSPORTATION REVIEW},
Year = {2022},
Volume = {161},
Month = {MAY},
Abstract = {Ports are pivotal nodes in supply chain and transportation networks, in
   which most of the existing data remain underutilized. Machine learning
   methods are versatile tools to utilize and harness the hidden power of
   the data. Considering ever-growing adoption of machine learning as a
   data driven decision-making tool, the port industry is far behind other
   modes of transportation in this transition. To fill the gap, we aimed to
   provide a comprehensive systematic literature review on this topic to
   analyze the previous research from different perspectives such as area
   of the application, type of application, machine learning method, data,
   and location of the study. Results showed that the number of articles in
   the field has been increasing annually, and the most prevalent use case
   of machine learning methods is to predict different port
   characteristics. However, there are emerging prescriptive and autonomous
   use cases of machine learning methods in the literature. Furthermore,
   research gaps and challenges are identified, and future research
   directions have been discussed from method-centric and
   application-centric points of view.},
DOI = {10.1016/j.tre.2022.102722},
EarlyAccessDate = {APR 2022},
Article-Number = {102722},
ISSN = {1366-5545},
EISSN = {1878-5794},
ORCID-Numbers = {Filom, Siyavash/0000-0002-1420-2338},
Unique-ID = {WOS:000797778000010},
}

@article{ WOS:000569375400002,
Author = {Jinnouchi, Ryosuke and Miwa, Kazutoshi and Karsai, Ferenc and Kresse,
   Georg and Asahi, Ryoji},
Title = {On-the-Fly Active Learning of Interatomic Potentials for Large-Scale
   Atomistic Simulations},
Journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS},
Year = {2020},
Volume = {11},
Number = {17},
Pages = {6946-6955},
Month = {SEP 3},
Abstract = {The on-the-fly generation of machine-learning force fields by
   active-learning schemes attracts a great deal of attention in the
   community of atomistic simulations. The algorithms allow the machine to
   self-learn an interatomic potential and construct machine-learned models
   on the fly during simulations. State-of-the-art query strategies allow
   the machine to judge whether new structures are out of the training data
   set or not. Only when the machine judges the necessity of updating the
   data set with the new structures are first-principles calculations
   carried out. Otherwise, the yet available machine-learned model is used
   to update the atomic positions. In this manner, most of the
   first-principles calculations are bypassed during training, and overall,
   simulations are accelerated by several orders of magnitude while
   retaining almost first-principles accuracy. In this Perspective, after
   describing essential components of the active-learning algorithms, we
   demonstrate the power of the schemes by presenting recent applications.},
DOI = {10.1021/acs.jpclett.0c01061},
ISSN = {1948-7185},
ResearcherID-Numbers = {Jinnouchi, Ryosuke/NNG-1783-2025
   Kresse, Georg/X-2531-2018},
ORCID-Numbers = {Kresse, Georg/0000-0001-9102-4259},
Unique-ID = {WOS:000569375400002},
}

@article{ WOS:000610764000001,
Author = {Blier-Wong, Christopher and Cossette, Helene and Lamontagne, Luc and
   Marceau, Etienne},
Title = {Machine Learning in P\&C Insurance: A Review for Pricing and Reserving},
Journal = {RISKS},
Year = {2021},
Volume = {9},
Number = {1},
Month = {JAN},
Abstract = {In the past 25 years, computer scientists and statisticians developed
   machine learning algorithms capable of modeling highly nonlinear
   transformations and interactions of input features. While actuaries use
   GLMs frequently in practice, only in the past few years have they begun
   studying these newer algorithms to tackle insurance-related tasks. In
   this work, we aim to review the applications of machine learning to the
   actuarial science field and present the current state of the art in
   ratemaking and reserving. We first give an overview of neural networks,
   then briefly outline applications of machine learning algorithms in
   actuarial science tasks. Finally, we summarize the future trends of
   machine learning for the insurance industry.},
DOI = {10.3390/risks9010004},
Article-Number = {4},
EISSN = {2227-9091},
ResearcherID-Numbers = {Marceau, Etienne/HKM-4177-2023
   Blier-Wong, Christopher/ABL-2328-2022},
ORCID-Numbers = {Lamontagne, Luc/0000-0003-0255-5117
   Marceau, Etienne/0000-0001-7962-7487
   },
Unique-ID = {WOS:000610764000001},
}

@article{ WOS:000814485300003,
Author = {Zaadnoordijk, Lorijn and Besold, Tarek R. and Cusack, Rhodri},
Title = {Lessons from infant learning for unsupervised machine learning},
Journal = {NATURE MACHINE INTELLIGENCE},
Year = {2022},
Volume = {4},
Number = {6},
Pages = {510-520},
Month = {JUN},
Abstract = {Unsupervised machine learning algorithms reduce the dependence on
   curated, labeled datasets that are characteristic of supervised machine
   learning. The authors argue that the developmental science of infant
   cognition could inform the design of unsupervised machine learning
   approaches.
   The desire to reduce the dependence on curated, labeled datasets and to
   leverage the vast quantities of unlabeled data has triggered renewed
   interest in unsupervised (or self-supervised) learning algorithms.
   Despite improved performance due to approaches such as the
   identification of disentangled latent representations, contrastive
   learning and clustering optimizations, unsupervised machine learning
   still falls short of its hypothesized potential as a breakthrough
   paradigm enabling generally intelligent systems. Inspiration from
   cognitive (neuro)science has been based mostly on adult learners with
   access to labels and a vast amount of prior knowledge. To push
   unsupervised machine learning forward, we argue that developmental
   science of infant cognition might hold the key to unlocking the next
   generation of unsupervised learning approaches. We identify three
   crucial factors enabling infants' quality and speed of learning: (1)
   babies' information processing is guided and constrained; (2) babies are
   learning from diverse, multimodal inputs; and (3) babies' input is
   shaped by development and active learning. We assess the extent to which
   these insights from infant learning have already been exploited in
   machine learning, examine how closely these implementations resemble the
   core insights, and propose how further adoption of these factors can
   give rise to previously unseen performance levels in unsupervised
   learning.},
DOI = {10.1038/s42256-022-00488-2},
EarlyAccessDate = {JUN 2022},
EISSN = {2522-5839},
ResearcherID-Numbers = {Cusack, Rhodri/A-2231-2010
   },
ORCID-Numbers = {Besold, Tarek Richard/0000-0002-8002-0049
   Cusack, Rhodri/0000-0002-5234-7415
   Zaadnoordijk, Lorijn/0000-0002-7484-6995},
Unique-ID = {WOS:000814485300003},
}

@article{ WOS:000571944500012,
Author = {van Klompenburg, Thomas and Kassahun, Ayalew and Catal, Cagatay},
Title = {Crop yield prediction using machine learning: A systematic literature
   review},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2020},
Volume = {177},
Month = {OCT},
Abstract = {Machine learning is an important decision support tool for crop yield
   prediction, including supporting decisions on what crops to grow and
   what to do during the growing season of the crops. Several machine
   learning algorithms have been applied to support crop yield prediction
   research. In this study, we performed a Systematic Literature Review
   (SLR) to extract and synthesize the algorithms and features that have
   been used in crop yield prediction studies. Based on our search
   criteria, we retrieved 567 relevant studies from six electronic
   databases, of which we have selected 50 studies for further analysis
   using inclusion and exclusion criteria. We investigated these selected
   studies carefully, analyzed the methods and features used, and provided
   suggestions for further research. According to our analysis, the most
   used features are temperature, rainfall, and soil type, and the most
   applied algorithm is Artificial Neural Networks in these models. After
   this observation based on the analysis of machine learning-based 50
   papers, we performed an additional search in electronic databases to
   identify deep learning-based studies, reached 30 deep learning-based
   papers, and extracted the applied deep learning algorithms. According to
   this additional analysis, Convolutional Neural Networks (CNN) is the
   most widely used deep learning algorithm in these studies, and the other
   widely used deep learning algorithms are Long-Short Term Memory (LSTM)
   and Deep Neural Networks (DNN).},
DOI = {10.1016/j.compag.2020.105709},
Article-Number = {105709},
ISSN = {0168-1699},
EISSN = {1872-7107},
ResearcherID-Numbers = {Kassahun, ir./AAA-1724-2021
   Catal, Cagatay/AAF-3929-2019},
ORCID-Numbers = {Kassahun, Ayalew/0000-0003-1066-7127
   Catal, Cagatay/0000-0003-0959-2930
   },
Unique-ID = {WOS:000571944500012},
}

@article{ WOS:000414424200001,
Author = {Macesic, Nenad and Polubriaginof, Fernanda and Tatonetti, Nicholas P.},
Title = {Machine learning: novel bioinformatics approaches for combating
   antimicrobial resistance},
Journal = {CURRENT OPINION IN INFECTIOUS DISEASES},
Year = {2017},
Volume = {30},
Number = {6},
Pages = {511-517},
Month = {DEC},
Abstract = {Purpose of review
   Antimicrobial resistance (AMR) is a threat to global health and new
   approaches to combating AMR are needed. Use of machine learning in
   addressing AMR is in its infancy but has made promising steps. We
   reviewed the current literature on the use of machine learning for
   studying bacterial AMR.
   Recent findings
   The advent of large-scale data sets provided by next-generation
   sequencing and electronic health records make applying machine learning
   to the study and treatment of AMR possible. To date, it has been used
   for antimicrobial susceptibility genotype/phenotype prediction,
   development of AMR clinical decision rules, novel antimicrobial agent
   discovery and antimicrobial therapy optimization.
   Summary
   Application of machine learning to studying AMR is feasible but remains
   limited. Implementation of machine learning in clinical settings faces
   barriers to uptake with concerns regarding model interpretability and
   data quality.
   Future applications of machine learning to AMR are likely to be
   laboratory-based, such as antimicrobial susceptibility phenotype
   prediction.},
DOI = {10.1097/QCO.0000000000000406},
ISSN = {0951-7375},
EISSN = {1473-6527},
Unique-ID = {WOS:000414424200001},
}

@article{ WOS:000778886900003,
Author = {Wang, Zheng},
Title = {Use of supervised machine learning to detect abuse of COVID-19 related
   domain names},
Journal = {COMPUTERS \& ELECTRICAL ENGINEERING},
Year = {2022},
Volume = {100},
Month = {MAY},
Abstract = {A comprehensive evaluation of supervised machine learning models for
   COVID-19 related domain name detection is presented. One representative
   conventional machine learning implementation and nineteen
   state-of-the-art deep learning implementations are evaluated. The deep
   learning implementation architectures evaluated include the recurrent,
   convolutional, and hybrid models. The detection rate metrics and the
   computing time metrics are considered in the evaluation. The result
   reveals that advanced deep learning models outperform conventional
   machine learning models in terms of detection rate. The results also
   show evidence of a tradeoff between detection rate and computing speed
   for the selection of machine learning models/architectures.
   High-frequency lexical analysis is provided for a better understanding
   of the COVID-19 related domain names. The limitations, implications, and
   considerations of the use of supervised machine learning to detect abuse
   of COVID-19 related domain names are discussed.},
DOI = {10.1016/j.compeleceng.2022.107864},
EarlyAccessDate = {MAR 2022},
Article-Number = {107864},
ISSN = {0045-7906},
EISSN = {1879-0755},
ResearcherID-Numbers = {WANG, ZHENG/AAC-8336-2021},
ORCID-Numbers = {Wang, Zheng/0000-0003-2744-9345
   },
Unique-ID = {WOS:000778886900003},
}

@article{ WOS:000534813100012,
Author = {Tuan, Tong Anh and Long, Hoang Viet and Son, Le Hoang and Kumar,
   Raghvendra and Priyadarshini, Ishaani and Son, Nguyen Thi Kim},
Title = {Performance evaluation of Botnet DDoS attack detection using machine
   learning},
Journal = {EVOLUTIONARY INTELLIGENCE},
Year = {2020},
Volume = {13},
Number = {2, SI},
Pages = {283-294},
Month = {JUN},
Abstract = {Botnet is regarded as one of the most sophisticated vulnerability
   threats nowadays. A large portion of network traffic is dominated by
   Botnets. Botnets are conglomeration of trade PCs (Bots) which are
   remotely controlled by their originator (BotMaster) under a Command
   and-Control (C\&C) foundation. They are the keys to several Internet
   assaults like spams, Distributed Denial of Service Attacks (DDoS),
   rebate distortions, malwares and phishing. To over the problem of DDoS
   attack, various machine learning methods typically Support Vector
   Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB),
   Decision Tree (DT), and Unsupervised Learning (USML) (K-means, X-means
   etc.) were proposed. With the increasing popularity of Machine Learning
   in the field of Computer Security, it will be a remarkable
   accomplishment to carry out performance assessment of the machine
   learning methods given a common platform. This could assist developers
   in choosing a suitable method for their case studies and assist them in
   further research. This paper performed an experimental analysis of the
   machine learning methods for Botnet DDoS attack detection. The
   evaluation is done on the UNBS-NB 15 and KDD99 which are well-known
   publicity datasets for Botnet DDoS attack detection. Machine learning
   methods typically Support Vector Machine (SVM), Artificial Neural
   Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised
   Learning (USML) are investigated for Accuracy, False Alarm Rate (FAR),
   Sensitivity, Specificity, False positive rate (FPR), AUC, and Matthews
   correlation coefficient (MCC) of datasets. Performance of KDD99 dataset
   has been experimentally shown to be better as compared to the UNBS-NB 15
   dataset. This validation is significant in computer security and other
   related fields.},
DOI = {10.1007/s12065-019-00310-w},
ISSN = {1864-5909},
EISSN = {1864-5917},
ResearcherID-Numbers = {priyadarshini, ishaani/AAW-3437-2020
   Tuan, Tong/AAT-9592-2021
   Nguyen, Son/CAF-1564-2022
   Long, Hoang/O-7699-2019},
ORCID-Numbers = {Tuan, Tong Anh/0000-0001-6321-6106
   Hoang Son, Le/0000-0001-6356-0046
   },
Unique-ID = {WOS:000534813100012},
}

@article{ WOS:000755564500001,
Author = {Amoore, Louise},
Title = {Machine learning political orders},
Journal = {REVIEW OF INTERNATIONAL STUDIES},
Year = {2023},
Volume = {49},
Number = {1},
Pages = {20-36},
Month = {JAN},
Abstract = {A significant set of epistemic and political transformations are taking
   place as states and societies begin to understand themselves and their
   problems through the paradigm of deep neural network algorithms. A
   machine learning political order does not merely change the political
   technologies of governance, but is itself a reordering of politics, of
   what the political can be. When algorithmic systems reduce the
   pluridimensionality of politics to the output of a model, they
   simultaneously foreclose the potential for other political claims to be
   made and alternative political projects to be built. More than this
   foreclosure, a machine learning political order actively profits and
   learns from the fracturing of communities and the destabilising of
   democratic rights. The transformation from rules-based algorithms to
   deep learning models has paralleled the undoing of rules-based social
   and international orders - from the use of machine learning in the
   campaigns of the UK EU referendum, to the trialling of algorithmic
   immigration and welfare systems, and the use of deep learning in the
   COVID-19 pandemic - with political problems becoming reconfigured as
   machine learning problems. Machine learning political orders decouple
   their attributes, features and clusters from underlying social values,
   no longer tethered to notions of good governance or a good society, but
   searching instead for the optimal function of abstract representations
   of data.},
DOI = {10.1017/S0260210522000031},
EarlyAccessDate = {FEB 2022},
Article-Number = {PII S0260210522000031},
ISSN = {0260-2105},
EISSN = {1469-9044},
ORCID-Numbers = {Amoore, Louise/0000-0001-6728-8553},
Unique-ID = {WOS:000755564500001},
}

@article{ WOS:000692200100001,
Author = {Goto, Takahiro and Tran, Quoc Hoan and Nakajima, Kohei},
Title = {Universal Approximation Property of Quantum Machine Learning Models in
   Quantum-Enhanced Feature Spaces},
Journal = {PHYSICAL REVIEW LETTERS},
Year = {2021},
Volume = {127},
Number = {9},
Month = {AUG 27},
Abstract = {Encoding classical data into quantum states is considered a quantum
   feature map to map classical data into a quantum Hilbert space. This
   feature map provides opportunities to incorporate quantum advantages
   into machine learning algorithms to be performed on near-term
   intermediate-scale quantum computers. The crucial idea is using the
   quantum Hilbert space as a quantum-enhanced feature space in machine
   learning models. Although the quantum feature map has demonstrated its
   capability when combined with linear classification models in some
   specific applications, its expressive power from the theoretical
   perspective remains unknown. We prove that the machine learning models
   induced from the quantum-enhanced feature space are universal
   approximators of continuous functions under typical quantum feature
   maps. We also study the capability of quantum feature maps in the
   classification of disjoint regions. Our work enables an important
   theoretical analysis to ensure that machine learning algorithms based on
   quantum feature maps can handle a broad class of machine learning tasks.
   In light of this, one can design a quantum machine learning model with
   more powerful expressivity.},
DOI = {10.1103/PhysRevLett.127.090506},
Article-Number = {090506},
ISSN = {0031-9007},
EISSN = {1079-7114},
ResearcherID-Numbers = {Nakajima, Kohei/NAZ-8309-2025},
ORCID-Numbers = {Tran, Quoc Hoan/0000-0003-1652-2332
   },
Unique-ID = {WOS:000692200100001},
}

@article{ WOS:001209420900001,
Author = {Xu, Yudian and Cao, Linlin and Chen, Yifan and Zhang, Ziyue and Liu,
   Wanshan and Li, He and Ding, Chenhuan and Pu, Jun and Qian, Kun and Xu,
   Wei},
Title = {Integrating Machine Learning in Metabolomics: A Path to Enhanced
   Diagnostics and Data Interpretation},
Journal = {SMALL METHODS},
Year = {2024},
Volume = {8},
Number = {12},
Month = {DEC},
Abstract = {Metabolomics, leveraging techniques like NMR and MS, is crucial for
   understanding biochemical processes in pathophysiological states. This
   field, however, faces challenges in metabolite sensitivity, data
   complexity, and omics data integration. Recent machine learning
   advancements have enhanced data analysis and disease classification in
   metabolomics. This study explores machine learning integration with
   metabolomics to improve metabolite identification, data efficiency, and
   diagnostic methods. Using deep learning and traditional machine
   learning, it presents advancements in metabolic data analysis, including
   novel algorithms for accurate peak identification, robust disease
   classification from metabolic profiles, and improved metabolite
   annotation. It also highlights multiomics integration, demonstrating
   machine learning's potential in elucidating biological phenomena and
   advancing disease diagnostics. This work contributes significantly to
   metabolomics by merging it with machine learning, offering innovative
   solutions to analytical challenges and setting new standards for omics
   data analysis.},
DOI = {10.1002/smtd.202400305},
EarlyAccessDate = {APR 2024},
Article-Number = {2400305},
ISSN = {2366-9608},
ResearcherID-Numbers = {Zhang, Ziyue/KSL-8395-2024},
Unique-ID = {WOS:001209420900001},
}

@article{ WOS:000760291800021,
Author = {Kirtley, Olivia J. and van Mens, Kasper and Hoogendoorn, Mark and Kapur,
   Navneet and de Beurs, Derek},
Title = {Translating promise into practice: a review of machine learning in
   suicide research and prevention},
Journal = {LANCET PSYCHIATRY},
Year = {2022},
Volume = {9},
Number = {3},
Pages = {243-252},
Month = {MAR},
Abstract = {In ever more pressured health-care systems, technological solutions
   offering scalability of care and better resource targeting are
   appealing. Research on machine learning as a technique for identifying
   individuals at risk of suicidal ideation, suicide attempts, and death
   has grown rapidly. This research often places great emphasis on the
   promise of machine learning for preventing suicide, but overlooks the
   practical, clinical implementation issues that might preclude delivering
   on such a promise. In this Review, we synthesise the broad empirical and
   review literature on electronic health record-based machine learning in
   suicide research, and focus on matters of crucial importance for
   implementation of machine learning in clinical practice. The challenge
   of preventing statistically rare outcomes is well known; progress
   requires tackling data quality, transparency, and ethical issues. In the
   future, machine learning models might be explored as methods to enable
   targeting of interventions to specific individuals depending upon their
   level of need-ie, for precision medicine. Primarily, however, the
   promise of machine learning for suicide prevention is limited by the
   scarcity of high-quality scalable interventions available to individuals
   identified by machine learning as being at risk of suicide.},
DOI = {10.1016/s2215-0366(21)00254-6},
ISSN = {2215-0366},
EISSN = {2215-0374},
ResearcherID-Numbers = {Kirtley, Olivia/AHC-7140-2022
   },
ORCID-Numbers = {de Beurs, Derek/0000-0002-0166-6897
   Kapur, Nav/0000-0002-3100-3234
   Kirtley, Olivia/0000-0001-5879-4120
   Hoogendoorn, Mark/0000-0003-3356-3574},
Unique-ID = {WOS:000760291800021},
}

@article{ WOS:000428582200001,
Author = {Liu, Qiang and Li, Pan and Zhao, Wentao and Cai, Wei and Yu, Shui and
   Leung, Victor C. M.},
Title = {A Survey on Security Threats and Defensive Techniques of Machine
   Learning: A Data Driven View},
Journal = {IEEE ACCESS},
Year = {2018},
Volume = {6},
Pages = {12103-12117},
Abstract = {Machine learning is one of the most prevailing techniques in computer
   science, and it has been widely applied in image processing, natural
   language processing, pattern recognition, cybersecurity, and other
   fields. Regardless of successful applications of machine learning
   algorithms in many scenarios, e.g., facial recognition, malware
   detection, automatic driving, and intrusion detection, these algorithms
   and corresponding training data are vulnerable to a variety of security
   threats, inducing a significant performance decrease. Hence, it is vital
   to call for further attention regarding security threats and
   corresponding defensive techniques of machine learning, which motivates
   a comprehensive survey in this paper. Until now, researchers from
   academia and industry have found out many security threats against a
   variety of learning algorithms, including naive Bayes, logistic
   regression, decision tree, support vector machine (SVM), principle
   component analysis, clustering, and prevailing deep neural networks.
   Thus, we revisit existing security threats and give a systematic survey
   on them from two aspects, the training phase and the testing/inferring
   phase. After that, we categorize current defensive techniques of machine
   learning into four groups: security assessment mechanisms,
   countermeasures in the training phase, those in the testing or inferring
   phase, data security, and privacy. Finally, we provide five notable
   trends in the research on security threats and defensive techniques of
   machine learning, which are worth doing in-depth studies in future.},
DOI = {10.1109/ACCESS.2018.2805680},
ISSN = {2169-3536},
ResearcherID-Numbers = {Leung, Victor/AGU-2462-2022
   Liu, Qiang/AAF-9077-2019
   Zhao, Wentao/JDM-7367-2023
   Yu, Shui/AFL-2699-2022
   Cai, Wei/AAC-4630-2022},
ORCID-Numbers = {Cai, Wei/0000-0002-4658-0034
   Yu, Shui/0000-0003-4485-6743
   },
Unique-ID = {WOS:000428582200001},
}

@article{ WOS:000821577800005,
Author = {Datta, Debaleena and Mallick, Pradeep Kumar and Bhoi, Akash Kumar and
   Ijaz, Muhammad Fazal and Shafi, Jana and Choi, Jaeyoung},
Title = {Hyperspectral Image Classification: Potentials, Challenges, and Future
   Directions},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2022},
Volume = {2022},
Month = {APR 28},
Abstract = {Recent imaging science and technology discoveries have considered
   hyperspectral imagery and remote sensing. The current intelligent
   technologies, such as support vector machines, sparse representations,
   active learning, extreme learning machines, transfer learning, and deep
   learning, are typically based on the learning of the machines. These
   techniques enrich the processing of such three-dimensional, multiple
   bands, and high-resolution images with their precision and fidelity.
   This article presents an extensive survey depicting machine-dependent
   technologies' contributions and deep learning on landcover
   classification based on hyperspectral images. The objective of this
   study is three-fold. First, after reading a large pool of Web of Science
   (WoS), Scopus, SCI, and SCIE-indexed and SCIE-related articles, we
   provide a novel approach for review work that is entirely systematic and
   aids in the inspiration of finding research gaps and developing embedded
   questions. Second, we emphasize contemporary advances in machine
   learning (ML) methods for identifying hyperspectral images, with a
   brief, organized overview and a thorough assessment of the literature
   involved. Finally, we draw the conclusions to assist researchers in
   expanding their understanding of the relationship between machine
   learning and hyperspectral images for future research.},
DOI = {10.1155/2022/3854635},
Article-Number = {3854635},
ISSN = {1687-5265},
EISSN = {1687-5273},
ResearcherID-Numbers = {Shafi, Jana/G-5563-2016
   Shafi, Jana/AAZ-7612-2021
   Ijaz, Muhammad Fazal/Q-3944-2019
   Mallick, Pradeep Kumar/N-9793-2015
   Mallick, Pradeep/AAU-7923-2021
   Bhoi, Dr. Akash Kumar/G-8518-2016
   Bhoi, Dr. Akash/G-8518-2016},
ORCID-Numbers = {Shafi, Jana/0000-0001-6859-670X
   Datta, Debaleena/0000-0003-1096-4375
   Ijaz, Muhammad Fazal/0000-0001-5206-272X
   Mallick, Pradeep Kumar/0000-0002-1207-0757
   Choi, Jaeyoung/0000-0001-9118-8050
   Bhoi, Dr. Akash Kumar/0000-0003-2759-3224
   },
Unique-ID = {WOS:000821577800005},
}

@article{ WOS:000963897500001,
Author = {Rutschi, Corinna and Berente, Nicholas and Nwanganga, Frederick},
Title = {Data Sensitivity and Domain Specificity in Reuse of Machine Learning
   Applications},
Journal = {INFORMATION SYSTEMS FRONTIERS},
Year = {2024},
Volume = {26},
Number = {2, SI},
Pages = {633-640},
Month = {APR},
Abstract = {Data sensitivity and domain specificity challenges arise in reuse of
   machine learning applications. We identify four types of machine
   learning applications based on different reuse strategies: generic,
   distinctive, selective, and exclusive. We conclude with lessons for
   developing and deploying machine learning applications.},
DOI = {10.1007/s10796-023-10388-4},
EarlyAccessDate = {APR 2023},
ISSN = {1387-3326},
EISSN = {1572-9419},
ResearcherID-Numbers = {Berente, Nicholas/AAA-2514-2020},
ORCID-Numbers = {Berente, Nicholas/0000-0002-1403-4696
   },
Unique-ID = {WOS:000963897500001},
}

@article{ WOS:000592355900013,
Author = {Cox, Christopher R. and Moscardini, Emma H. and Cohen, Alex S. and
   Tucker, Raymond P.},
Title = {Machine learning for suicidology: A practical review of exploratory and
   hypothesis-driven approaches},
Journal = {CLINICAL PSYCHOLOGY REVIEW},
Year = {2020},
Volume = {82},
Month = {DEC},
Abstract = {Machine learning is being used to discover models to predict the
   progression from suicidal ideation to action in clinical populations.
   While quantifiable improvements in prediction accuracy have been
   achieved over theory -driven efforts, models discovered through machine
   learning continue to fall short of clinical relevance. Thus, the value
   of machine learning for reaching this objective is hotly contested. We
   agree that machine learning, treated as a ``black box{''} approach
   antithetical to theory-building, will not discover clinically relevant
   models of suicide. However, such models may be developed through
   deliberate synthesis of dataand theory-driven approaches. By providing
   an accessible overview of essential concepts and common methods, we
   highlight how generalizable models and scientific insight may be
   obtained by incorporating prior knowledge and expectations to machine
   learning research, drawing examples from suicidology. We then discuss
   challenges investigators will face when using machine learning to
   discover models of low prevalence outcomes, such as suicide.},
DOI = {10.1016/j.cpr.2020.101940},
Article-Number = {101940},
ISSN = {0272-7358},
EISSN = {1873-7811},
ResearcherID-Numbers = {Moscardini, Emma/MTD-3655-2025
   Tucker, Raymond/AAJ-8458-2021},
Unique-ID = {WOS:000592355900013},
}

@article{ WOS:000684684400001,
Author = {Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen,
   Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold},
Title = {Teaching Machine Learning in K-12 Classroom: Pedagogical and
   Technological Trajectories for Artificial Intelligence Education},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {110558-110572},
Abstract = {Over the past decades, numerous practical applications of machine
   learning techniques have shown the potential of AI-driven and
   data-driven approaches in a large number of computing fields. Machine
   learning is increasingly included in computing curricula in higher
   education, and a quickly growing number of initiatives are expanding it
   in K-12 computing education, too. As machine learning enters K-12
   computing education, understanding how intuition and agency in the
   context of such systems is developed becomes a key research area. But as
   schools and teachers are already struggling with integrating traditional
   computational thinking and traditional artificial intelligence into
   school curricula, understanding the challenges behind teaching machine
   learning in K-12 is an even more daunting challenge for computing
   education research. Despite the central position of machine learning and
   AI in the field of modern computing, the computing education research
   body of literature contains remarkably few studies of how people learn
   to train, test, improve, and deploy machine learning systems. This is
   especially true of the K-12 curriculum space. This article charts the
   emerging trajectories in educational practice, theory, and technology
   related to teaching machine learning in K-12 education. The article
   situates the existing work in the context of computing education in
   general, and describes some differences that K-12 computing educators
   should take into account when facing this challenge. The article focuses
   on key aspects of the paradigm shift that will be required in order to
   successfully integrate machine learning into the broader K-12 computing
   curricula. A crucial step is abandoning the belief that rule-based
   ``traditional{''} programming is a central aspect and building block in
   developing next generation computational thinking.},
DOI = {10.1109/ACCESS.2021.3097962},
ISSN = {2169-3536},
ResearcherID-Numbers = {Kahila, Juho/MHQ-9707-2025
   Pears, Arnold/Q-7734-2019
   Pears, Arnold/C-1431-2009},
ORCID-Numbers = {Jormanainen, Ilkka/0000-0003-3254-2480
   Kahila, Juho/0000-0002-9913-0627
   Pears, Arnold/0000-0002-5184-4743},
Unique-ID = {WOS:000684684400001},
}

@article{ WOS:000403140800087,
Author = {L'Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and
   Capretz, Miriam A. M.},
Title = {Machine Learning With Big Data: Challenges and Approaches},
Journal = {IEEE ACCESS},
Year = {2017},
Volume = {5},
Pages = {7776-7797},
Abstract = {The Big Data revolution promises to transform how we live, work, and
   think by enabling process optimization, empowering insight discovery and
   improving decision making. The realization of this grand potential
   relies on the ability to extract value from such massive data through
   data analytics; machine learning is at its core because of its ability
   to learn from data and provide data driven insights, decisions, and
   predictions. However, traditional machine learning approaches were
   developed in a different era, and thus are based upon multiple
   assumptions, such as the data set fitting entirely into memory, what
   unfortunately no longer holds true in this new context. These broken
   assumptions, together with the Big Data characteristics, are creating
   obstacles for the traditional techniques. Consequently, this paper
   compiles, summarizes, and organizes machine learning challenges with Big
   Data. In contrast to other research that discusses challenges, this work
   highlights the cause effect relationship by organizing challenges
   according to Big Data Vs or dimensions that instigated the issue:
   volume, velocity, variety, or veracity. Moreover, emerging machine
   learning approaches and techniques are discussed in terms of how they
   are capable of handling the various challenges with the ultimate
   objective of helping practitioners select appropriate solutions for
   their use cases. Finally, a matrix relating the challenges and
   approaches is presented. Through this process, this paper provides a
   perspective on the domain, identifies research gaps and opportunities,
   and provides a strong foundation and encouragement for further research
   in the field of machine learning with Big Data.},
DOI = {10.1109/ACCESS.2017.2696365},
ISSN = {2169-3536},
ResearcherID-Numbers = {ElYamany, Hany/AAW-3003-2020
   Grolinger, Katarina/ABB-3261-2021
   Capretz, Miriam/G-2362-2014
   },
ORCID-Numbers = {Grolinger, Katarina/0000-0003-0062-8212},
Unique-ID = {WOS:000403140800087},
}

@article{ WOS:000833393900003,
Author = {Zhou, Siyang and Liu, Shanglin and Kang, Yilan and Cai, Jie and Xie,
   Haimei and Zhang, Qian},
Title = {Physics-based machine learning method and the application to energy
   consumption prediction in tunneling construction},
Journal = {ADVANCED ENGINEERING INFORMATICS},
Year = {2022},
Volume = {53},
Month = {AUG},
Abstract = {Representing causality in machine learning to predict control parameters
   is state-of-the-art research in intelligent control. This study presents
   a physics-based machine learning method providing a prediction model
   that guarantees enhanced interpretability conforming to physical laws.
   The proposed approach encodes physical knowledge as mapping
   relationships between variables in engineering dataset into the learning
   procedure through dimensional analysis. This derives causal
   relationships between the control parameter and its influencing factors.
   The proposed machine learning method's objective function is further
   improved by the penalty term in the regularization strategy.
   Verifications on the energy consumption prediction of tunnel boring
   machine prove that, the established model accords with basic principles
   in this field. Moreover, the proposed approach traces the impact of
   three major factors (structure, operation, and geology) along the
   construction section, offering each component's contribution rates to
   energy consumption. Compared with several commonly used machine learning
   algorithms, the proposed method reduces the need for large amounts of
   training data and demonstrates higher accuracy. The results indicate
   that the revealed causality and enhanced prediction performance of the
   proposed method advance the applicability of machine learning methods to
   intelligent control during construction.},
DOI = {10.1016/j.aei.2022.101642},
EarlyAccessDate = {MAY 2022},
Article-Number = {101642},
ISSN = {1474-0346},
EISSN = {1873-5320},
ResearcherID-Numbers = {谢, 海妹/GVS-1010-2022
   },
ORCID-Numbers = {Zhou, Siyang/0000-0002-0534-2101
   Zhang, Qian/0000-0001-7210-654X
   Xie, Haimei/0000-0003-2593-1306},
Unique-ID = {WOS:000833393900003},
}

@article{ WOS:000834797800001,
Author = {Kubsch, Marcus and Krist, Christina and Rosenberg, Joshua M.},
Title = {Distributing epistemic functions and tasks-A framework for augmenting
   human analytic power with machine learning in science education research},
Journal = {JOURNAL OF RESEARCH IN SCIENCE TEACHING},
Year = {2023},
Volume = {60},
Number = {2},
Pages = {423-447},
Month = {FEB},
Abstract = {Machine learning (ML) has become commonplace in educational research and
   science education research, especially to support assessment efforts.
   Such applications of machine learning have shown their promise in
   replicating and scaling human-driven codes of students' work. Despite
   this promise, we and other scholars argue that machine learning has not
   yet achieved its transformational potential. We argue that this is
   because our field is currently lacking frameworks for supporting
   creative, principled, and critical endeavors to use machine learning in
   science education research. To offer considerations for science
   education researchers' use of ML, we present a framework, Distributing
   Epistemic Functions and Tasks (DEFT), that highlights the functions and
   tasks that pertain to generating knowledge that can be carried out by
   either trained researchers or machine learning algorithms. Such
   considerations are critical decisions that should occur alongside those
   about, for instance, the type of data or algorithm used. We apply this
   framework to two cases, one that exemplifies the cutting-edge use of
   machine learning in science education research and another that offers a
   wholly different means of using machine learning and human-driven
   inquiry together. We conclude with strategies for researchers to adopt
   machine learning and call for the field to rethink how we prepare
   science education researchers in an era of great advances in
   computational power and access to machine learning methods.},
DOI = {10.1002/tea.21803},
EarlyAccessDate = {AUG 2022},
ISSN = {0022-4308},
EISSN = {1098-2736},
ResearcherID-Numbers = {Rosenberg, Joshua/M-6390-2019
   Kubsch, Marcus/JBS-2909-2023
   },
ORCID-Numbers = {Krist, Christina/0000-0002-9738-4308
   Rosenberg, Joshua/0000-0003-2170-0447
   Kubsch, Marcus/0000-0001-5497-8336},
Unique-ID = {WOS:000834797800001},
}

@article{ WOS:000617753000001,
Author = {Ouyang, Yulou and Yu, Cuiqian and Yan, Gang and Chen, Jie},
Title = {Machine learning approach for the prediction and optimization of thermal
   transport properties},
Journal = {FRONTIERS OF PHYSICS},
Year = {2021},
Volume = {16},
Number = {4, SI},
Month = {AUG},
Abstract = {Traditional simulation methods have made prominent progress in aiding
   experiments for understanding thermal transport properties of materials,
   and in predicting thermal conductivity of novel materials. However, huge
   challenges are also encountered when exploring complex material systems,
   such as formidable computational costs. As a rising computational
   method, machine learning has a lot to offer in this regard, not only in
   speeding up the searching and optimization process, but also in
   providing novel perspectives. In this work, we review the
   state-of-the-art studies on material's thermal properties based on
   machine learning technique. First, the basic principles of machine
   learning method are introduced. We then review applications of machine
   learning technique in the prediction and optimization of material's
   thermal properties, including thermal conductivity and interfacial
   thermal resistance. Finally, an outlook is provided for the future
   studies.},
DOI = {10.1007/s11467-020-1041-x},
Article-Number = {43200},
ISSN = {2095-0462},
EISSN = {2095-0470},
ResearcherID-Numbers = {Chen, Jie/G-1716-2012
   Yan, Gang/B-9020-2009
   Ouyang, Yulou/ABH-6107-2020},
ORCID-Numbers = {Chen, Jie/0000-0003-4599-3600
   Yu, Cuiqian/0000-0002-9067-1363
   },
Unique-ID = {WOS:000617753000001},
}

@article{ WOS:000705849400001,
Author = {Saeedi, Seyran and Panahi, Aliakbar and Arodz, Tom},
Title = {Quantum semi-supervised kernel learning},
Journal = {QUANTUM MACHINE INTELLIGENCE},
Year = {2021},
Volume = {3},
Number = {2},
Month = {DEC},
Abstract = {Quantum machine learning methods have the potential to facilitate
   learning using extremely large datasets. While the availability of data
   for training machine learning models is steadily increasing, oftentimes
   it is much easier to collect feature vectors to obtain the corresponding
   labels. One of the approaches for addressing this issue is to use
   semi-supervised learning, which leverages not only the labeled samples,
   but also unlabeled feature vectors. Here, we present a quantum machine
   learning algorithm for training semi-supervised kernel support vector
   machines. The algorithm uses recent advances in quantum sample-based
   Hamiltonian simulation to extend the existing quantum LS-SVM algorithm
   to handle the semi-supervised term in the loss. Through a theoretical
   study of the algorithm's computational complexity, we show that it
   maintains the same speedup as the fully-supervised quantum LS-SVM.},
DOI = {10.1007/s42484-021-00053-x},
Article-Number = {24},
ISSN = {2524-4906},
EISSN = {2524-4914},
ResearcherID-Numbers = {Panahi, Aliakbar/ABD-4778-2021},
Unique-ID = {WOS:000705849400001},
}

@article{ WOS:001138185200001,
Author = {Zhang, Runhao and Yang, Jian},
Title = {State of the art in applications of machine learning in steelmaking
   process modeling},
Journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS},
Year = {2023},
Volume = {30},
Number = {11},
Pages = {2055-2075},
Month = {NOV},
Abstract = {With the development of automation and informatization in the
   steelmaking industry, the human brain gradually fails to cope with an
   increasing amount of data generated during the steelmaking process.
   Machine learning technology provides a new method other than production
   experience and metallurgical principles in dealing with large amounts of
   data. The application of machine learning in the steelmaking process has
   become a research hotspot in recent years. This paper provides an
   overview of the applications of machine learning in the steelmaking
   process modeling involving hot metal pretreatment, primary steelmaking,
   secondary refining, and some other aspects. The three most frequently
   used machine learning algorithms in steelmaking process modeling are the
   artificial neural network, support vector machine, and case-based
   reasoning, demonstrating proportions of 56\%, 14\%, and 10\%,
   respectively. Collected data in the steelmaking plants are frequently
   faulty. Thus, data processing, especially data cleaning, is crucially
   important to the performance of machine learning models. The detection
   of variable importance can be used to optimize the process parameters
   and guide production. Machine learning is used in hot metal pretreatment
   modeling mainly for endpoint S content prediction. The predictions of
   the endpoints of element compositions and the process parameters are
   widely investigated in primary steelmaking. Machine learning is used in
   secondary refining modeling mainly for ladle furnaces,
   Ruhrstahl-Heraeus, vacuum degassing, argon oxygen decarburization, and
   vacuum oxygen decarburization processes. Further development of machine
   learning in the steelmaking process modeling can be realized through
   additional efforts in the construction of the data platform, the
   industrial transformation of the research achievements to the practical
   steelmaking process, and the improvement of the universality of the
   machine learning models.},
DOI = {10.1007/s12613-023-2646-1},
ISSN = {1674-4799},
EISSN = {1869-103X},
ResearcherID-Numbers = {Zhang, Runhao/ABI-3041-2020},
Unique-ID = {WOS:001138185200001},
}

@article{ WOS:000614082300001,
Author = {Eslami, Taban and Almuqhim, Fahad and Raiker, Joseph S. and Saeed, Fahad},
Title = {Machine Learning Methods for Diagnosing Autism Spectrum Disorder and
   Attention- Deficit/Hyperactivity Disorder Using Functional and
   Structural MRI: A Survey},
Journal = {FRONTIERS IN NEUROINFORMATICS},
Year = {2021},
Volume = {14},
Month = {JAN 20},
Abstract = {Here we summarize recent progress in machine learning model for
   diagnosis of Autism Spectrum Disorder (ASD) and
   Attention-deficit/Hyperactivity Disorder (ADHD). We outline and describe
   the machine-learning, especially deep-learning, techniques that are
   suitable for addressing research questions in this domain, pitfalls of
   the available methods, as well as future directions for the field. We
   envision a future where the diagnosis of ASD, ADHD, and other mental
   disorders is accomplished, and quantified using imaging techniques, such
   as MRI, and machine-learning models.},
DOI = {10.3389/fninf.2020.575999},
Article-Number = {575999},
EISSN = {1662-5196},
ResearcherID-Numbers = {Saeed, Fahad/Q-5316-2019
   Raiker, Joseph/AAE-7593-2020},
ORCID-Numbers = {Saeed, Fahad/0000-0002-3410-9552
   },
Unique-ID = {WOS:000614082300001},
}

@article{ WOS:000681124300016,
Author = {Mohr, Felix and Wever, Marcel and Tornede, Alexander and Huellermeier,
   Eyke},
Title = {Predicting Machine Learning Pipeline Runtimes in the Context of
   Automated Machine Learning},
Journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
Year = {2021},
Volume = {43},
Number = {9},
Pages = {3055-3066},
Month = {SEPT 1},
Abstract = {Automated machine learning (AutoML) seeks to automatically find
   so-called machine learning pipelines that maximize the prediction
   performance when being used to train a model on a given dataset. One of
   the main and yet open challenges in AutoMLis an effective use of
   computational resources: An AutoML process involves the evaluation of
   many candidate pipelines, which are costly but often ineffective because
   they are canceled due to a timeout. In this paper, we present an
   approach to predict the runtime of two-step machine learning pipelines
   with up to one pre-processor, which can be used to anticipate whether or
   not a pipeline will time out. Separate runtime models are trained
   offline for each algorithm that may be used in a pipeline, and an
   overall prediction is derived from these models. We empirically show
   that the approach increases successful evaluations made by an AutoML
   tool while preserving or even improving on the previously best
   solutions.},
DOI = {10.1109/TPAMI.2021.3056950},
ISSN = {0162-8828},
EISSN = {1939-3539},
ResearcherID-Numbers = {Wever, Marcel/AAE-3091-2020},
ORCID-Numbers = {Wever, Marcel/0000-0001-9782-6818
   Tornede, Alexander/0000-0002-2415-2186
   },
Unique-ID = {WOS:000681124300016},
}

@article{ WOS:000464121000002,
Author = {Leo, Martin and Sharma, Suneel and Maddulety, K.},
Title = {Machine Learning in Banking Risk Management: A Literature Review},
Journal = {RISKS},
Year = {2019},
Volume = {7},
Number = {1},
Month = {MAR 5},
Abstract = {There is an increasing influence of machine learning in business
   applications, with many solutions already implemented and many more
   being explored. Since the global financial crisis, risk management in
   banks has gained more prominence, and there has been a constant focus
   around how risks are being detected, measured, reported and managed.
   Considerable research in academia and industry has focused on the
   developments in banking and risk management and the current and emerging
   challenges. This paper, through a review of the available literature
   seeks to analyse and evaluate machine-learning techniques that have been
   researched in the context of banking risk management, and to identify
   areas or problems in risk management that have been inadequately
   explored and are potential areas for further research. The review has
   shown that the application of machine learning in the management of
   banking risks such as credit risk, market risk, operational risk and
   liquidity risk has been explored; however, it doesn't appear
   commensurate with the current industry level of focus on both risk
   management and machine learning. A large number of areas remain in bank
   risk management that could significantly benefit from the study of how
   machine learning can be applied to address specific problems.},
DOI = {10.3390/risks7010029},
Article-Number = {29},
ISSN = {2227-9091},
ResearcherID-Numbers = {Leo, Martin/ABD-1440-2021
   Sharma, Suneel/ABH-3083-2021},
ORCID-Numbers = {Leo, Martin/0000-0001-6091-6959
   Sharma, Suneel/0000-0002-4822-5327
   },
Unique-ID = {WOS:000464121000002},
}

@article{ WOS:000253272100001,
Author = {Hsieh, Jer-Guang and Lin, Yih-Lon and Jeng, Jyh-Horng},
Title = {Preliminary studyon Wilcoxon learning machines},
Journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
Year = {2008},
Volume = {19},
Number = {2},
Pages = {201-211},
Month = {FEB},
Abstract = {As is well known in statistics, the resulting linear regressors by using
   the rank-based Wilcoxon approach to linear regression problems are
   usually robust against (or insensitive to) outliers. This motivates us
   to introduce in this paper the Wilcoxon approach to the area, of machine
   learning. Specifically, we investigate four new learning machines,
   namely Wilcoxon neural network (WNN), Wilcoxon generalized radial basis
   function network (WGRBFN), Wilcoxon fuzzy neural network (WFNN), and
   kernel-based Wilcoxon regressor (KWR). These provide alternative
   learning machines when faced with general nonlinear learning problems.
   Simple weights updating rules based on gradient descent will be derived.
   Some numerical examples will be provided to compare the robustness
   against outliers for various learning machines. Simulation results show
   that the Wilcoxon learning machines proposed in this paper have good
   robustness against outliers. We firmly believe that the Wilcoxon
   approach will provide a promising methodology for many machine learning
   problems.},
DOI = {10.1109/TNN.2007.904035},
ISSN = {1045-9227},
EISSN = {1941-0093},
Unique-ID = {WOS:000253272100001},
}

@article{ WOS:000419350700030,
Author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes,
   Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and
   Pande, Vijay},
Title = {MoleculeNet: a benchmark for molecular machine learning},
Journal = {CHEMICAL SCIENCE},
Year = {2018},
Volume = {9},
Number = {2},
Pages = {513-530},
Month = {JAN 14},
Abstract = {Molecular machine learning has been maturing rapidly over the last few
   years. Improved methods and the presence of larger datasets have enabled
   machine learning algorithms to make increasingly accurate predictions
   about molecular properties. However, algorithmic progress has been
   limited due to the lack of a standard benchmark to compare the efficacy
   of proposed methods; most new algorithms are benchmarked on different
   datasets making it challenging to gauge the quality of proposed methods.
   This work introduces MoleculeNet, a large scale benchmark for molecular
   machine learning. MoleculeNet curates multiple public datasets,
   establishes metrics for evaluation, and offers high quality open-source
   implementations of multiple previously proposed molecular featurization
   and learning algorithms (released as part of the DeepChem open source
   library). MoleculeNet benchmarks demonstrate that learnable
   representations are powerful tools for molecular machine learning and
   broadly offer the best performance. However, this result comes with
   caveats. Learnable representations still struggle to deal with complex
   tasks under data scarcity and highly imbalanced classification. For
   quantum mechanical and biophysical datasets, the use of physics-aware
   featurizations can be more important than choice of particular learning
   algorithm.},
DOI = {10.1039/c7sc02664a},
ISSN = {2041-6520},
EISSN = {2041-6539},
ResearcherID-Numbers = {Pande, Vijay/ABE-8145-2020
   Wu, Zhenqin/HPC-5171-2023
   },
ORCID-Numbers = {Geniesse, Caleb/0000-0002-7897-8338
   Leswing, Karl/0000-0003-2361-3267
   Gomes, Joseph/0000-0002-0755-0641
   Wu, Zhenqin/0000-0002-0148-7055},
Unique-ID = {WOS:000419350700030},
}

@article{ WOS:000831186100001,
Author = {Xiao, Sian and Tian, Hao and Tao, Peng},
Title = {PASSer2.0: Accurate Prediction of Protein Allosteric Sites Through
   Automated Machine Learning},
Journal = {FRONTIERS IN MOLECULAR BIOSCIENCES},
Year = {2022},
Volume = {9},
Month = {JUL 11},
Abstract = {Allostery is a fundamental process in regulating protein activities. The
   discovery, design, and development of allosteric drugs demand better
   identification of allosteric sites. Several computational methods have
   been developed previously to predict allosteric sites using static
   pocket features and protein dynamics. Here, we define a baseline model
   for allosteric site prediction and present a computational model using
   automated machine learning. Our model, PASSer2.0, advanced the previous
   results and performed well across multiple indicators with 82.7\% of
   allosteric pockets appearing among the top three positions. The trained
   machine learning model has been integrated with the to facilitate
   allosteric drug discovery.},
DOI = {10.3389/fmolb.2022.879251},
Article-Number = {879251},
EISSN = {2296-889X},
ResearcherID-Numbers = {Xiao, Sian/HGB-9564-2022
   Tian, Hao/AAZ-8334-2020
   Tao, Peng/H-4925-2014
   },
ORCID-Numbers = {Xiao, Sian/0000-0002-3451-5227
   Tian, Hao/0000-0002-0186-9811},
Unique-ID = {WOS:000831186100001},
}

@article{ WOS:000612766700011,
Author = {Quer, Giorgio and Arnaout, Ramy and Henne, Michael and Arnaout, Rima},
Title = {Machine Learning and the Future of Cardiovascular Care JACC
   State-of-the-Art Review},
Journal = {JACC-JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY},
Year = {2021},
Volume = {77},
Number = {3},
Pages = {300-313},
Month = {JAN 26},
Abstract = {The role of physicians has always been to synthesize the data available
   to them to identify diagnostic patterns that guide treatment and follow
   response. Today, increasingly sophisticated machine learning algorithms
   may grow to support clinical experts in some of these tasks. Machine
   learning has the potential to benefit patients and cardiologists, but
   only if clinicians take an active role in bringing these new algorithms
   into practice. The aim of this review is to introduce clinicians who are
   not data science experts to key concepts in machine learning that will
   allow them to better understand the field and evaluate new literature
   and developments. The current published data in machine learning for
   cardiovascular disease is then summarized, using both a bibliometric
   survey, with code publicly available to enable similar analysis for any
   research topic of interest, and select case studies. Finally, several
   ways that clinicians can and must be involved in this emerging field are
   presented. (C) 2021 The Authors. Published by Elsevier on behalf of the
   American College of Cardiology Foundation.},
DOI = {10.1016/j.jacc.2020.11.030},
EarlyAccessDate = {JAN 2021},
ISSN = {0735-1097},
EISSN = {1558-3597},
ResearcherID-Numbers = {Quer, Giorgio/AAE-9910-2020},
ORCID-Numbers = {Quer, Giorgio/0000-0003-2208-7912
   },
Unique-ID = {WOS:000612766700011},
}

@article{ WOS:000808086800005,
Author = {Li, Daofeng and Xu, Yamei and Zhao, Ming and Zhu, Jinkang and Zhang,
   Sihai},
Title = {Knowledge-Driven Machine Learning and Applications in Wireless
   Communications},
Journal = {IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING},
Year = {2022},
Volume = {8},
Number = {2},
Pages = {454-467},
Month = {JUN},
Abstract = {The power of big data and machine learning has been drastically
   demonstrated in many fields during the past twenty years which somehow
   leads to the vague even false understanding that the huge amount of
   precious human knowledge accumulated to date seems to no longer matter.
   In this paper, we are pioneering to propose the knowledge-driven machine
   learning (KDML) model to exhibit that knowledge can play an important
   role in machine learning tasks. Compared with conventional machine
   learning, KDML contains a unique knowledge module based on specific
   domain knowledge, which is able to simplify the machine learning network
   structures, reduce the training overhead and improve interpretability.
   Channel estimation problem of wireless communication is taken as a case
   verification because such machine learning-based solutions face huge
   challenges in terms of accuracy, complexity, and reliability. We
   integrate the classical wireless channel estimation algorithms into
   different machine learning neural networks and propose KDML-based
   channel estimators in Orthogonal Frequency Division Multiplexing (OFDM)
   and Massive Multiple Input Multiple Output (MIMO) system. The
   experimental results in both communication systems validate the
   effectiveness of the proposed KDML-based channel estimators.},
DOI = {10.1109/TCCN.2021.3128597},
ISSN = {2332-7731},
ResearcherID-Numbers = {Zhang, Sihai/JCO-2045-2023
   Xu, Yamei/AFP-5605-2022
   Li, Daofeng/E-5386-2011},
Unique-ID = {WOS:000808086800005},
}

@article{ WOS:001292266400001,
Author = {Xiao, Xiao and Yin, Junyi and Xu, Jing and Tat, Trinny and Chen, Jun},
Title = {Advances in Machine Learning for Wearable Sensors},
Journal = {ACS NANO},
Year = {2024},
Volume = {18},
Number = {34},
Pages = {22734-22751},
Month = {AUG 15},
Abstract = {Recent years have witnessed tremendous advances in machine learning
   techniques for wearable sensors and bioelectronics, which play an
   essential role in real-time sensing data analysis to provide
   clinical-grade information for personalized healthcare. To this end,
   supervised learning and unsupervised learning algorithms have emerged as
   powerful tools, allowing for the detection of complex patterns and
   relationships in large, high-dimensional data sets. In this Review, we
   aim to delineate the latest advancements in machine learning for
   wearable sensors, focusing on key developments in algorithmic
   techniques, applications, and the challenges intrinsic to this evolving
   landscape. Additionally, we highlight the potential of machine-learning
   approaches to enhance the accuracy, reliability, and interpretability of
   wearable sensor data and discuss the opportunities and limitations of
   this emerging field. Ultimately, our work aims to provide a roadmap for
   future research endeavors in this exciting and rapidly evolving area.},
DOI = {10.1021/acsnano.4c05851},
EarlyAccessDate = {AUG 2024},
ISSN = {1936-0851},
EISSN = {1936-086X},
ResearcherID-Numbers = {Yin, Junyi/KAM-0400-2024
   Tat, Trinny/IYL-4399-2023
   Chen, Jun/K-3415-2012
   Xiao, Xiao/AAC-8908-2022},
ORCID-Numbers = {Yin, Junyi/0000-0002-3511-111X
   Xiao, Xiao/0000-0002-7861-5596
   },
Unique-ID = {WOS:001292266400001},
}

@article{ WOS:000431737300083,
Author = {Taniguchi, Hidetaka and Sato, Hiroshi and Shirakawa, Tomohiro},
Title = {A machine learning model with human cognitive biases capable of learning
   from small and biased datasets},
Journal = {SCIENTIFIC REPORTS},
Year = {2018},
Volume = {8},
Month = {MAY 9},
Abstract = {Human learners can generalize a new concept from a small number of
   samples. In contrast, conventional machine learning methods require
   large amounts of data to address the same types of problems. Humans have
   cognitive biases that promote fast learning. Here, we developed a method
   to reduce the gap between human beings and machines in this type of
   inference by utilizing cognitive biases. We implemented a human
   cognitive model into machine learning algorithms and compared their
   performance with the currently most popular methods, naive Bayes,
   support vector machine, neural networks, logistic regression and random
   forests. We focused on the task of spam classification, which has been
   studied for a long time in the field of machine learning and often
   requires a large amount of data to obtain high accuracy. Our models
   achieved superior performance with small and biased samples in
   comparison with other representative machine learning methods.},
DOI = {10.1038/s41598-018-25679-z},
Article-Number = {7397},
ISSN = {2045-2322},
Unique-ID = {WOS:000431737300083},
}

@article{ WOS:000270620400005,
Author = {Lee, Wen-Chiung and Wu, Chin-Chia},
Title = {Some single-machine and m-machine flowshop scheduling problems
   with learning considerations},
Journal = {INFORMATION SCIENCES},
Year = {2009},
Volume = {179},
Number = {22},
Pages = {3885-3892},
Month = {NOV 7},
Abstract = {Scheduling with learning effect has drawn many researchers' attention
   since Biskup {[}D. Biskup, Single-machine scheduling with learning
   considerations, European journal of Opterational Research 115 (1999)
   173-178] introduced the concept of learning into the scheduling field.
   Biskup {[}D. Biskup, A state-of-the-art review on scheduling with
   learning effect, European journal of Opterational Research 188 (2008)
   315-329] classified the learning approaches in the literature into two
   main streams. He claimed that the position-based learning seems to be a
   realistic model for machine learning, while the
   sum-of-processing-time-based learning is a model for human learning. In
   some realistic situations, both the machine and human learning might
   exist simultaneously. For example, robots with neural networks are used
   in computers, motor vehicles, and many assembly lines. The actions of a
   robot are constantly modified through self-learning in processing the
   jobs. On the other hand, the operators in the control center learn how
   to give the commands efficiently through working experience. In this
   paper, we propose a new learning model that unifies the two main
   approaches. We show that some single-machine problems and some specified
   flowshop problems are polynomially solvable. (c) 2009 Elsevier Inc. All
   rights reserved.},
DOI = {10.1016/j.ins.2009.07.011},
ISSN = {0020-0255},
EISSN = {1872-6291},
ResearcherID-Numbers = {Wu, Chin-Chia/P-9171-2018},
ORCID-Numbers = {Wu, Chin-Chia/0000-0002-1598-5127},
Unique-ID = {WOS:000270620400005},
}

@article{ WOS:000412361900042,
Author = {Zhu, Lipeng and Lu, Chao and Dong, Zhao Yang and Hong, Chao},
Title = {Imbalance Learning Machine-Based Power System Short-Term Voltage
   Stability Assessment},
Journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS},
Year = {2017},
Volume = {13},
Number = {5},
Pages = {2533-2543},
Month = {OCT},
Abstract = {In terms of machine learning-based power system dynamic stability
   assessment, it is feasible to collect learning data from massive
   synchrophasor measurements in practice. However, the fact that
   instability events rarely occur would lead to a challenging class
   imbalance problem. Besides, short-term feature extraction from scarce
   instability seems extremely difficult for conventional learning
   machines. Faced with such a dilemma, this paper develops a systematic
   imbalance learning machine for online short-term voltage stability
   assessment. A powerful time series shapelet (discriminative subsequence)
   classification method is embedded into the machine for sequential
   transient feature mining. A forecasting-based nonlinear synthetic
   minority oversampling technique is proposed to mitigate the distortion
   of class distribution. Cost-sensitive learning is employed to intensify
   bias toward those scarce yet valuable unstable cases. Furthermore, an
   incremental learning strategy is put forward for online monitoring,
   contributing to adaptability and reliability enhancement along with
   time. Simulation results on the Nordic test system illustrate the high
   performance of the proposed learning machine and of the assessment
   scheme.},
DOI = {10.1109/TII.2017.2696534},
ISSN = {1551-3203},
EISSN = {1941-0050},
ResearcherID-Numbers = {Lu, Chao/O-7322-2014
   yang, dong zhao/AFP-5094-2022
   },
ORCID-Numbers = {Dong, Zhao Yang/0000-0001-9659-0858
   Zhu, Lipeng/0000-0001-6051-9064},
Unique-ID = {WOS:000412361900042},
}

@article{ WOS:000425074300032,
Author = {Nair, Lekha R. and Shetty, Sujala D. and Shetty, Siddhanth D.},
Title = {Applying spark based machine learning model on streaming big data for
   health status prediction},
Journal = {COMPUTERS \& ELECTRICAL ENGINEERING},
Year = {2018},
Volume = {65},
Pages = {393-399},
Month = {JAN},
Abstract = {Machine learning is one of the driving forces of science and commerce,
   but the proliferation of Big Data demands paradigm shifts from
   traditional methods in the application of machine learning techniques on
   this voluminous data having varying velocity. With the availability of
   large health care datasets and progressions in machine learning
   techniques, computers are now well equipped in diagnosing many health
   issues. This work aims at developing a real time remote health status
   prediction system built around open source Big Data processing engine,
   the Apache Spark, deployed in the cloud which focus on applying machine
   learning model on streaming Big Data. In this scalable system, the user
   tweets his health attributes and the application receives the same in
   real time, extracts the attributes and applies machine learning model to
   predict user's health status which is then directly messaged to him/her
   instantly for taking appropriate action. (C) 2017 Elsevier Ltd. All
   rights reserved.},
DOI = {10.1016/j.compeleceng.2017.03.009},
ISSN = {0045-7906},
EISSN = {1879-0755},
Unique-ID = {WOS:000425074300032},
}

@article{ WOS:000519206300017,
Author = {Nishizaki, Yohei and Horisaki, Ryoichi and Kitaguchi, Katsuhisa and
   Saito, Mamoru and Tanida, Jun},
Title = {Analysis of non-iterative phase retrieval based on machine learning},
Journal = {OPTICAL REVIEW},
Year = {2020},
Volume = {27},
Number = {1},
Pages = {136-141},
Month = {FEB},
Abstract = {In this paper, we analyze a machine-learning-based non-iterative phase
   retrieval method. Phase retrieval and its applications have been
   attractive research topics in optics and photonics, for example, in
   biomedical imaging, astronomical imaging, and so on. Most conventional
   phase retrieval methods have used iterative processes to recover phase
   information; however, the calculation speed and convergence with these
   methods are serious issues in real-time monitoring applications.
   Machine-learning-based methods are promising for addressing these
   issues. Here, we numerically compare conventional methods and a
   machine-learning-based method in which a convolutional neural network is
   employed. Simulations with several conditions show that the
   machine-learning-based method realizes fast and robust phase recovery
   compared with the conventional methods. We also numerically demonstrate
   machine-learning-based phase retrieval from noisy measurements with a
   noisy training data set for improving the noise robustness. The
   machine-learning-based approach used in this study may increase the
   impact of phase retrieval, which is useful in various fields, where
   phase retrieval has been used as a fundamental tool.},
DOI = {10.1007/s10043-019-00574-8},
ISSN = {1340-6000},
EISSN = {1349-9432},
ORCID-Numbers = {Horisaki, Ryoichi/0000-0002-2280-5921},
Unique-ID = {WOS:000519206300017},
}

@article{ WOS:000368151700007,
Author = {Sun, Xudong and Zhou, Mingxing and Sun, Yize},
Title = {Classification of textile fabrics by use of spectroscopy-based pattern
   recognition methods},
Journal = {SPECTROSCOPY LETTERS},
Year = {2016},
Volume = {49},
Number = {2},
Pages = {96-102},
Abstract = {The combination of near-infrared spectroscopy and pattern recognition
   methods, including soft independent modeling of class analogy, least
   squares support machine, and extreme learning machine, was employed for
   textile fabrics classification. The fabrics of cotton, viscose, acrylic,
   polyamide, polyester, and blend fabric of cotton-viscose were divided
   into training and prediction sets (60: 60) for developing models and
   evaluating the classification abilities of the models. The
   classification accuracy and speed of soft independent modeling of class
   analogy, least squares support machine, and extreme learning machine
   were compared. Both least squares support machine and extreme learning
   machine achieved the classification accuracy of 100\% for the prediction
   set. However, extreme learning machine performed much faster than least
   squares support machine, which suggested that extreme learning machine
   may be a promising method for real-time textile fabrics classification
   with a comparable accuracy based on near-infrared spectroscopy.
   Moreover, it might have commercial and regulatory potential to avoid
   time-consuming work, and costly and laborious chemical analysis for
   textile fabrics classification.},
DOI = {10.1080/00387010.2015.1089446},
ISSN = {0038-7010},
EISSN = {1532-2289},
ResearcherID-Numbers = {Zhou, Mingxing/ITV-6653-2023
   Sun, Xudong/AAA-6281-2019},
Unique-ID = {WOS:000368151700007},
}

@article{ WOS:000605202300001,
Author = {Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L.},
Title = {Second opinion needed: communicating uncertainty in medical machine
   learning},
Journal = {NPJ DIGITAL MEDICINE},
Year = {2021},
Volume = {4},
Number = {1},
Month = {JAN 5},
Abstract = {There is great excitement that medical artificial intelligence (AI)
   based on machine learning (ML) can be used to improve decision making at
   the patient level in a variety of healthcare settings. However, the
   quantification and communication of uncertainty for individual
   predictions is often neglected even though uncertainty estimates could
   lead to more principled decision-making and enable machine learning
   models to automatically or semi-automatically abstain on samples for
   which there is high uncertainty. In this article, we provide an overview
   of different approaches to uncertainty quantification and abstention for
   machine learning and highlight how these techniques could improve the
   safety and reliability of current ML systems being used in healthcare
   settings. Effective quantification and communication of uncertainty
   could help to engender trust with healthcare workers, while providing
   safeguards against known failure modes of current machine learning
   approaches. As machine learning becomes further integrated into
   healthcare environments, the ability to say ``I'm not sure{''} or ``I
   don't know{''} when uncertain is a necessary capability to enable safe
   clinical deployment.},
DOI = {10.1038/s41746-020-00367-3},
Article-Number = {4},
ISSN = {2398-6352},
ResearcherID-Numbers = {Beam, Andrew/HKE-7517-2023
   },
ORCID-Numbers = {Beam, Andrew/0000-0002-6657-2787},
Unique-ID = {WOS:000605202300001},
}

@article{ WOS:000969652500001,
Author = {Loeffler, Shane E. and Trayanova, Natalia},
Title = {Primer on Machine Learning in Electrophysiology},
Journal = {ARRHYTHMIA \& ELECTROPHYSIOLOGY REVIEW},
Year = {2023},
Volume = {12},
Month = {JAN},
Abstract = {Artificial intelligence has become ubiquitous. Machine learning, a
   branch of artificial intelligence, leads the current technological
   revolution through its remarkable ability to learn and perform on data
   sets of varying types. Machine learning applications are expected to
   change contemporary medicine as they are brought into mainstream
   clinical practice. In the field of cardiac arrhythmia and
   electrophysiology, machine learning applications have enjoyed rapid
   growth and popularity. To facilitate clinical acceptance of these
   methodologies, it is important to promote general knowledge of machine
   learning in the wider community and continue to highlight the areas of
   successful application. The authors present a primer to provide an
   overview of common supervised (least squares, support vector machine,
   neural networks and random forest) and unsupervised (k-means and
   principal component analysis) machine learning models. The authors also
   provide explanations as to how and why the specific machine learning
   models have been used in arrhythmia and electrophysiology studies.},
DOI = {10.15420/aer.2022.43},
Article-Number = {e06},
ISSN = {2050-3369},
EISSN = {2050-3377},
ORCID-Numbers = {Loeffler, Shane/0000-0002-5080-2330},
Unique-ID = {WOS:000969652500001},
}

@article{ WOS:000644443200006,
Author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun
   and Lyu, Chengfei and Wu, Zhihua},
Title = {From Server-Based to Client-Based Machine Learning: A Comprehensive
   Survey},
Journal = {ACM COMPUTING SURVEYS},
Year = {2021},
Volume = {54},
Number = {1},
Month = {APR},
Abstract = {In recent years, mobile devices have gained increasing development with
   stronger computation capability and larger storage space. Some of the
   computation-intensive machine learning tasks can now be run on mobile
   devices. To exploit the resources available on mobile devices and
   preserve personal privacy, the concept of client-based machine learning
   has been proposed. It leverages the users' local hardware and local data
   to solve machine learning sub-problems on mobile devices and only
   uploads computation results rather than the original data for the
   optimization of the global model. Such an architecture can not only
   relieve computation and storage burdens on servers but also protect the
   users' sensitive information. Another benefit is the bandwidth reduction
   because various kinds of local data can be involved in the training
   process without being uploaded. In this article, we provide a literature
   review on the progressive development of machine learning from server
   based to client based. We revisit a number of widely used server-based
   and client-based machine learning methods and applications. We also
   extensively discuss the challenges and future directions in this area.
   We believe that this survey will give a clear overview of client-based
   machine learning and provide guidelines on applying client-based machine
   learning to practice.},
DOI = {10.1145/3424660},
Article-Number = {6},
ISSN = {0360-0300},
EISSN = {1557-7341},
Unique-ID = {WOS:000644443200006},
}

@article{ WOS:000526850800009,
Author = {Li Qi and Fan Qiu-Ling and Han Qiu-Xia and Geng Wen-Jia and Zhao
   Huan-Huan and Ding Xiao-Nan and Yan Jing-Yao and Zhu Han-Yu},
Title = {Machine learning in nephrology: scratching the surface},
Journal = {CHINESE MEDICAL JOURNAL},
Year = {2020},
Volume = {133},
Number = {6},
Pages = {687-698},
Month = {MAR 20},
Abstract = {Machine learning shows enormous potential in facilitating
   decision-making regarding kidney diseases. With the development of data
   preservation and processing, as well as the advancement of machine
   learning algorithms, machine learning is expected to make remarkable
   breakthroughs in nephrology. Machine learning models have yielded many
   preliminaries to moderate and several excellent achievements in the
   fields, including analysis of renal pathological images, diagnosis and
   prognosis of chronic kidney diseases and acute kidney injury, as well as
   management of dialysis treatments. However, it is just scratching the
   surface of the field; at the same time, machine learning and its
   applications in renal diseases are facing a number of challenges. In
   this review, we discuss the application status, challenges and future
   prospects of machine learning in nephrology to help people further
   understand and improve the capacity for prediction, detection, and care
   quality in kidney diseases.},
DOI = {10.1097/CM9.0000000000000694},
ISSN = {0366-6999},
EISSN = {2542-5641},
ORCID-Numbers = {Ding, Xiaonan/0000-0001-5383-7080
   LI, QI/0000-0003-4229-242X},
Unique-ID = {WOS:000526850800009},
}

@article{ WOS:000608918500001,
Author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma
   and Jurafsky, Dan and Pineau, Joelle},
Title = {Towards the Systematic Reporting of the Energy and Carbon Footprints of
   Machine Learning},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2020},
Volume = {21},
Abstract = {Accurate reporting of energy and carbon usage is essential for
   understanding the potential climate impacts of machine learning
   research. We introduce a framework that makes this easier by providing a
   simple interface for tracking realtime energy consumption and carbon
   emissions, as well as generating standardized online appendices.
   Utilizing this framework, we create a leaderboard for energy efficient
   reinforcement learning algorithms to incentivize responsible research in
   this area as an example for other areas of machine learning. Finally,
   based on case studies using our framework, we propose strategies for
   mitigation of carbon emissions and reduction of energy consumption. By
   making accounting easier, we hope to further the sustainable development
   of machine learning experiments and spur more research into energy
   efficient algorithms.},
Article-Number = {248},
ISSN = {1532-4435},
ORCID-Numbers = {Brunskill, Emma/0000-0002-3971-7127
   Henderson, Peter/0000-0003-3938-0541},
Unique-ID = {WOS:000608918500001},
}

@article{ WOS:000466446500036,
Author = {Wu, Zachary and Kan, S. B. Jennifer and Lewis, Russell D. and Wittmann,
   Bruce J. and Arnold, Frances H.},
Title = {Machine learning-assisted directed protein evolution with combinatorial
   libraries},
Journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA},
Year = {2019},
Volume = {116},
Number = {18},
Pages = {8852-8858},
Month = {APR 30},
Abstract = {To reduce experimental effort associated with directed protein evolution
   and to explore the sequence space encoded by mutating multiple positions
   simultaneously, we incorporate machine learning into the directed
   evolution workflow. Combinatorial sequence space can be quite expensive
   to sample experimentally, but machine-learning models trained on tested
   variants provide a fast method for testing sequence space
   computationally. We validated this approach on a large published
   empirical fitness landscape for human GB1 binding protein, demonstrating
   that machine learning-guided directed evolution finds variants with
   higher fitness than those found by other directed evolution approaches.
   We then provide an example application in evolving an enzyme to produce
   each of the two possible product enantiomers (i.e., stereodivergence) of
   a new-to-nature carbene Si-H insertion reaction. The approach predicted
   libraries enriched in functional enzymes and fixed seven mutations in
   two rounds of evolution to identify variants for selective catalysis
   with 93\% and 79\% ee (enantiomeric excess). By greatly increasing
   throughput with in silico modeling, machine learning enhances the
   quality and diversity of sequence solutions for a protein engineering
   problem.},
DOI = {10.1073/pnas.1901979116},
ISSN = {0027-8424},
EISSN = {1091-6490},
ORCID-Numbers = {Wu, Zachary/0000-0003-2429-9812
   Wittmann, Bruce/0000-0001-8144-9157
   Arnold, Frances H./0000-0002-4027-364X
   Lewis, Russell/0000-0002-5776-7347},
Unique-ID = {WOS:000466446500036},
}

@article{ WOS:000969320000010,
Author = {Ricciardi, Carlo and Cuocolo, Renato and Megna, Rosario and Cesarelli,
   Mario and Petretta, Mario},
Title = {Machine learning analysis: general features, requirements and
   cardiovascular applications},
Journal = {MINERVA CARDIOLOGY AND ANGIOLOGY},
Year = {2022},
Volume = {70},
Number = {1},
Pages = {67-74},
Month = {FEB},
Abstract = {Artificial intelligence represents the science which will probably
   change the future of medicine by solving actually challenging issues. In
   this special article, the general features of machine learning are
   discussed. First, a background explanation regarding the division of
   artificial intelligence, machine learning and deep learning is given and
   a focus on the structure of machine learning subgroups is shown. The
   traditional process of a machine learning analysis is described,
   starting from the collection of data, across features engineering,
   modelling and till the validation and deployment phase. Due to the
   several applications of machine learning performed in literature in the
   last decades and the lack of some guidelines, the need of a
   standardization for reporting machine learning analysis results emerged.
   Some possible standards for reporting machine learning results are
   identified and discussed deeply; these are related to study population
   (number of subjects), repeatability of the analysis, validation,
   results, comparison with current practice. The way to the use of machine
   learning in clinical practice is open and the hope is that, with
   emerging technology and advanced digital and computational tools,
   available from hospitalization and subsequently after discharge, it will
   also be possible, with the help of increasingly powerful hardware, to
   build assistance strategies useful in clinical practice.},
DOI = {10.23736/S2724-5683.21.05637-4},
ISSN = {2724-5683},
EISSN = {2724-5772},
ResearcherID-Numbers = {Petretta, Mario/K-9892-2015
   Cuocolo, Renato/G-3147-2018
   Ricciardi, Carlo/HJI-2511-2023
   megna, rosario/AAW-6333-2020},
ORCID-Numbers = {Cuocolo, Renato/0000-0002-1452-1574
   Petretta, Mario/0000-0002-8001-4298
   Megna, Rosario/0000-0002-5716-2137
   },
Unique-ID = {WOS:000969320000010},
}

@article{ WOS:000936226800001,
Author = {Xie, Jiawang and Zhao, Yuzhi and Zhu, Dezhi and Li, Jiaqun and Qiao,
   Ming and He, Guangzhi and Deng, Shengfa and Yan, Jianfeng},
Title = {A Machine Learning-Combined Flexible Sensor for Tactile Detection and
   Voice Recognition},
Journal = {ACS APPLIED MATERIALS \& INTERFACES},
Year = {2023},
Volume = {15},
Number = {9},
Pages = {12551-12559},
Month = {MAR 8},
Abstract = {Intelligent sensors have attracted substantial attention for various
   applications, including wearable electronics, artificial intelligence,
   healthcare monitoring, and human-machine interactions. However, there
   still remains a critical challenge in developing a multifunctional
   sensing system for complex signal detection and analysis in practical
   applications. Here, we develop a machine learning-combined flexible
   sensor for real-time tactile sensing and voice recognition through
   laser-induced graphitization. The intelligent sensor with a
   triboelectric layer can convert local pressure to an electrical signal
   through a contact electrification effect without external bias, which
   has a characteristic response behavior when exposed to various
   mechanical stimuli. With the special patterning design, a smart
   human-machine interaction controlling system composed of a digital
   arrayed touch panel is constructed to control electronic devices. Based
   on machine learning, the real-time monitoring and recognition of the
   changes of voice are achieved with high accuracy. The machine
   learning-empowered flexible sensor provides a promising platform for the
   development of flexible tactile sensing, real-time health detection,
   human-machine interaction, and intelligent wearable devices.},
DOI = {10.1021/acsami.2c22287},
EarlyAccessDate = {FEB 2023},
ISSN = {1944-8244},
EISSN = {1944-8252},
ResearcherID-Numbers = {Zhao, Yuzhi/HJH-8107-2023
   guangzhi, he/HSG-0824-2023
   Yan, Jianfeng/G-9099-2016
   },
ORCID-Numbers = {He, Guangzhi/0000-0003-3640-7497},
Unique-ID = {WOS:000936226800001},
}

@article{ WOS:001174091000001,
Author = {Suresh, Rahul and Bishnoi, Hardik and Kuklin, Artem V. and Parikh,
   Atharva and Molokeev, Maxim and Harinarayanan, R. and Gharat, Sarvesh
   and Hiba, P.},
Title = {Revolutionizing physics: a comprehensive survey of machine learning
   applications},
Journal = {FRONTIERS IN PHYSICS},
Year = {2024},
Volume = {12},
Month = {FEB 16},
Abstract = {In the context of the 21st century and the fourth industrial revolution,
   the substantial proliferation of data has established it as a valuable
   resource, fostering enhanced computational capabilities across
   scientific disciplines, including physics. The integration of Machine
   Learning stands as a prominent solution to unravel the intricacies
   inherent to scientific data. While diverse machine learning algorithms
   find utility in various branches of physics, there exists a need for a
   systematic framework for the application of Machine Learning to the
   field. This review offers a comprehensive exploration of the fundamental
   principles and algorithms of Machine Learning, with a focus on their
   implementation within distinct domains of physics. The review delves
   into the contemporary trends of Machine Learning application in
   condensed matter physics, biophysics, astrophysics, material science,
   and addresses emerging challenges. The potential for Machine Learning to
   revolutionize the comprehension of intricate physical phenomena is
   underscored. Nevertheless, persisting challenges in the form of more
   efficient and precise algorithm development are acknowledged within this
   review.},
DOI = {10.3389/fphy.2024.1322162},
Article-Number = {1322162},
EISSN = {2296-424X},
ResearcherID-Numbers = {Kuklin, Artem/A-9007-2014
   Molokeev, Maxim/D-1108-2013
   Suresh, Rahul/AAD-4110-2022
   Gharat, Sarvesh/AAV-8223-2020},
ORCID-Numbers = {Suresh, Rahul/0000-0002-6354-0886
   },
Unique-ID = {WOS:001174091000001},
}

@article{ WOS:001040009600001,
Author = {Feng, Jian and Liu, Xin},
Title = {No more free lunch: The increasing popularity of machine learning and
   financial market efficiency},
Journal = {ECONOMIC AND POLITICAL STUDIES-EPS},
Year = {2024},
Volume = {12},
Number = {1},
Pages = {34-57},
Month = {JAN 2},
Abstract = {In this paper, we show that the increasing popularity of machine
   learning improves market efficiency. By analysing the performance of a
   set of popular machine learning-based investment strategies, we find
   that profits from these strategies experience significant declines since
   the wide adoption of machine learning techniques, especially for profits
   based on the more preferred method of neural networks. These declines
   mainly come from long legs. Using the `machine learning' Google search
   index as a proxy for machine learning-based trading intensity, we find
   that returns from the neural networks-based long-short and long-only
   strategies are weaker following high levels of machine learning
   intensity, while no relation is found between machine learning intensity
   and the short-only neural networks-based strategy.},
DOI = {10.1080/20954816.2023.2230622},
EarlyAccessDate = {JUL 2023},
ISSN = {2095-4816},
EISSN = {2470-4024},
Unique-ID = {WOS:001040009600001},
}

@article{ WOS:000523484900001,
Author = {Rauschert, S. and Raubenheimer, K. and Melton, P. E. and Huang, R. C.},
Title = {Machine learning and clinical epigenetics: a review of challenges for
   diagnosis and classification},
Journal = {CLINICAL EPIGENETICS},
Year = {2020},
Volume = {12},
Number = {1},
Month = {APR 3},
Abstract = {Background Machine learning is a sub-field of artificial intelligence,
   which utilises large data sets to make predictions for future events.
   Although most algorithms used in machine learning were developed as far
   back as the 1950s, the advent of big data in combination with
   dramatically increased computing power has spurred renewed interest in
   this technology over the last two decades. Main body Within the medical
   field, machine learning is promising in the development of assistive
   clinical tools for detection of e.g. cancers and prediction of disease.
   Recent advances in deep learning technologies, a sub-discipline of
   machine learning that requires less user input but more data and
   processing power, has provided even greater promise in assisting
   physicians to achieve accurate diagnoses. Within the fields of genetics
   and its sub-field epigenetics, both prime examples of complex data,
   machine learning methods are on the rise, as the field of personalised
   medicine is aiming for treatment of the individual based on their
   genetic and epigenetic profiles. Conclusion We now have an ever-growing
   number of reported epigenetic alterations in disease, and this offers a
   chance to increase sensitivity and specificity of future diagnostics and
   therapies. Currently, there are limited studies using machine learning
   applied to epigenetics. They pertain to a wide variety of disease states
   and have used mostly supervised machine learning methods.},
DOI = {10.1186/s13148-020-00842-4},
Article-Number = {51},
ISSN = {1868-7075},
EISSN = {1868-7083},
ResearcherID-Numbers = {Melton, Phillip/A-5012-2013
   },
ORCID-Numbers = {Melton, Phillip/0000-0003-4026-2964
   Raubenheimer, Kyle/0000-0002-1361-0700},
Unique-ID = {WOS:000523484900001},
}

@article{ WOS:001389793200001,
Author = {Veza, Ibham and Karaoglan, Aslan Deniz and Akpinar, Sener and Spraggon,
   Martin and Idris, Muhammad},
Title = {Machine learning of weighted superposition attraction algorithm for
   optimization diesel engine performance and emission fueled with
   butanol-diesel biofuel},
Journal = {AIN SHAMS ENGINEERING JOURNAL},
Year = {2024},
Volume = {15},
Number = {12},
Month = {DEC},
Abstract = {Machine learning (ML) is a subset of artificial intelligence (AI) and
   computer science that employs data and algorithms and mimics human
   learning to self-enhance its accuracy. In biofuel research, butanol is
   widely recognized as a prospective alternative biofuel. Butanol addition
   in diesel or combustion engine has been more and more studied recently.
   Gaining a comprehensive comprehension of butanol performance and
   emission characteristics using machine learning approach is an essential
   milestone in investigating alcohol-based biofuel addition in diesel
   engines. However, few studies investigated butanol effect on diesel
   engine emissions using machine learning for optimization. A novel
   optimization study is needed. This work aims to investigate the newly
   developed and efficient machine learning, weighted superposition
   attraction (WSA) algorithm, to optimize the emission and performance of
   diesel engines fuelled with butanol-diesel biofuel. Mathematical
   modeling between the factors (butanol (vol.\%) and BMEP (bar)) and the
   responses (BTE (\%), BSFC (g/kWh), Exhaust Temperature Texh (oC), NOx
   (g/kWh), CO (g/kWh), HC (g/kWh), and Smoke Opacity (\%)) are governed
   using regression modeling. The optimized and best factor levels are
   determined employing the machine learning of WSA Algorithm.
   Confirmations are carried out. Optimization results indicate that the
   BTE is maximized, and the remainder of the responses are minimized.},
DOI = {10.1016/j.asej.2024.103126},
EarlyAccessDate = {DEC 2024},
Article-Number = {103126},
ISSN = {2090-4479},
EISSN = {2090-4495},
ResearcherID-Numbers = {Veza, Ibham/AAX-2642-2020
   Idris, Muhammad/GXH-5900-2022
   Akpınar, Şener/O-7141-2019},
ORCID-Numbers = {Veza, Ibham/0000-0002-1674-4798
   },
Unique-ID = {WOS:001389793200001},
}

@article{ WOS:000701254700001,
Author = {Liu, Meijie and Li, Baojuan and Hu, Dewen},
Title = {Autism Spectrum Disorder Studies Using fMRI Data and Machine Learning: A
   Review},
Journal = {FRONTIERS IN NEUROSCIENCE},
Year = {2021},
Volume = {15},
Month = {SEP 15},
Abstract = {Machine learning methods have been frequently applied in the field of
   cognitive neuroscience in the last decade. A great deal of attention has
   been attracted to introduce machine learning methods to study the autism
   spectrum disorder (ASD) in order to find out its neurophysiological
   underpinnings. In this paper, we presented a comprehensive review about
   the previous studies since 2011, which applied machine learning methods
   to analyze the functional magnetic resonance imaging (fMRI) data of
   autistic individuals and the typical controls (TCs). The all-round
   process was covered, including feature construction from raw fMRI data,
   feature selection methods, machine learning methods, factors for high
   classification accuracy, and critical conclusions. Applying different
   machine learning methods and fMRI data acquired from different sites,
   classification accuracies were obtained ranging from 48.3\% up to 97\%,
   and informative brain regions and networks were located. Through
   thorough analysis, high classification accuracies were found to usually
   occur in the studies which involved task-based fMRI data, single dataset
   for some selection principle, effective feature selection methods, or
   advanced machine learning methods. Advanced deep learning together with
   the multi-site Autism Brain Imaging Data Exchange (ABIDE) dataset became
   research trends especially in the recent 4 years. In the future,
   advanced feature selection and machine learning methods combined with
   multi-site dataset or easily operated task-based fMRI data may appear to
   have the potentiality to serve as a promising diagnostic tool for ASD.},
DOI = {10.3389/fnins.2021.697870},
Article-Number = {697870},
EISSN = {1662-453X},
ResearcherID-Numbers = {Hu, Dewen/AAN-8511-2020},
Unique-ID = {WOS:000701254700001},
}

@article{ WOS:001336722300165,
Author = {Choi, Eunsik and Sul, Jinhwan and Kim, Jungin E. and Hong, Sungjin and
   Gonzalez, Beatriz Izquierdo and Cembellin, Pablo and Wang, Yan},
Title = {Quantum Machine Learning for Additive Manufacturing Process Monitoring},
Journal = {MANUFACTURING LETTERS},
Year = {2024},
Volume = {41},
Number = {S},
Pages = {1415-1422},
Month = {OCT},
Note = {52nd SME North American Manufacturing Research Conference (NAMRC), Univ
   Tennessee Knoxville, Knoxville, TN, JUN 17-21, 2024},
Organization = {SME},
Abstract = {Machine learning is useful for analyzing and monitoring complex
   manufacturing processes. However, it has several limitations including
   the curse-of-dimensionality and lack of training data. In this paper, we
   propose a quantum machine learning strategy to tackle these challenges.
   Quantum support vector machine is applied to identify the states of
   machines in fused filament fabrication process based on acoustic
   emission data. Quantum convolutional neural network is used to detect
   spatters in laser powder bed fusion process based on coaxial optical
   images. Our results show that quantum machine learning can achieve the
   similar accuracy levels of predictions by classical machine learning
   counterparts, but with exponentially fewer parameters. (c) 2024 The
   Authors. Published by ELSEVIER Ltd. This is an open access article under
   the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/)},
DOI = {10.1016/j.mfglet.2024.09.168},
EarlyAccessDate = {OCT 2024},
ISSN = {2213-8463},
Unique-ID = {WOS:001336722300165},
}

@article{ WOS:000485090400075,
Author = {Palkovits, Regina and Palkovits, Stefan},
Title = {Using Artificial Intelligence To Forecast Water Oxidation Catalysts},
Journal = {ACS CATALYSIS},
Year = {2019},
Volume = {9},
Number = {9},
Pages = {8383-8387},
Month = {SEP},
Abstract = {Artificial intelligence and various types of machine learning are of
   increasing interest not only in the natural sciences but also in a wide
   range of applied and engineering sciences. In this study, we rethink the
   view on combinatorial heterogeneous catalysis and combine machine
   learning methods with combinatorial approaches in electrocatalysis.
   Several machine learning methods were used to forecast water oxidation
   catalysts on the basis of literature published data sets and data from
   our own work. The machine learning models exhibit a decent prediction
   precision based on the data sets available and confirm that even simple
   models are suitable for good forecasts.},
DOI = {10.1021/acscatal.9b01985},
ISSN = {2155-5435},
ResearcherID-Numbers = {Palkovits, Regina/G-2207-2014
   },
ORCID-Numbers = {Palkovits, Regina/0000-0002-4970-2957
   Palkovits, Stefan/0000-0003-4809-2939},
Unique-ID = {WOS:000485090400075},
}

@article{ WOS:001301816000001,
Author = {Kim, Taehyeon and Kim, Kibum and Hyung, Jinseok and Park, Haekeum and
   Oh, Yoojin and Koo, Jayong},
Title = {An interpretable machine learning-based pitting corrosion depth
   prediction model for steel drinking water pipelines},
Journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION},
Year = {2024},
Volume = {190},
Number = {B},
Pages = {571-585},
Month = {OCT},
Abstract = {Steel pipes are a crucial element of the water supply system and are
   necessary for safely delivering large quantities of water from
   purification plants to consumers. Corrosion is a significant factor that
   deteriorates the interior and exterior of the steel pipes. Although the
   effectiveness of machine learning has been demonstrated in various
   fields, machine learning has rarely been used to identify corrosion
   mechanisms in buried steel pipes. A hybrid machine-learning-based
   corrosion depth prediction model was developed by integrating a
   corrosion depth trend prediction model based only on elapsed years with
   machine-learning algorithms. Shapley additive explanation (SHAP) was
   used to analyze the hybrid machine-learning-based corrosion depth
   prediction models, revealing corrosion mechanisms and explaining the
   interactions among influencing factors through global and local
   interpretations. The SHAP local interpretation showed that the hybrid
   machine-learning-based corrosion depth prediction models can effectively
   capture the interrelationship between soil and water corrosiveness.},
DOI = {10.1016/j.psep.2024.08.038},
EarlyAccessDate = {AUG 2024},
ISSN = {0957-5820},
EISSN = {1744-3598},
ResearcherID-Numbers = {Kim, Taehyeon/LKM-6688-2024},
Unique-ID = {WOS:001301816000001},
}

@article{ WOS:000382418300016,
Author = {Criminisi, A.},
Title = {Machine learning for medical images analysis},
Journal = {MEDICAL IMAGE ANALYSIS},
Year = {2016},
Volume = {33},
Number = {SI},
Pages = {91-93},
Month = {OCT},
Abstract = {This article discusses the application of machine learning for the
   analysis of medical images. Specifically: (i) We show how a special type
   of learning models can be thought of as automatically optimized,
   hierarchically-structured, rule-based algorithms, and (ii) We discuss
   how the issue of collecting large labelled datasets applies to both
   conventional algorithms as well as machine learning techniques. The size
   of the training database is a function of model complexity rather than a
   characteristic of machine learning methods. Crown Copyright (C) 2016
   Published by Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.media.2016.06.002},
ISSN = {1361-8415},
EISSN = {1361-8423},
Unique-ID = {WOS:000382418300016},
}

@article{ WOS:000793810100003,
Author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei},
Title = {Hyperspectral Anomaly Detection Based on Machine Learning: An Overview},
Journal = {IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING},
Year = {2022},
Volume = {15},
Pages = {3351-3364},
Abstract = {Hyperspectral anomaly detection (HAD) is an important hyperspectral
   image application. HAD can find pixels with anomalous spectral
   signatures compared with their neighbor background without any prior
   information. While most of the existed researches are related to
   statistic-based and distance-based techniques, by summarizing the
   background samples with certain models, and then, finding the very few
   outliers by various distance metrics, this review focuses on the HAD
   based on machine learning methods, which have witnessed remarkable
   progress in the recent years. In particular, these studies can generally
   be grouped into the traditional machine learning and deep-learning-based
   methods. Several representative HAD methods, including both traditional
   machine and deep-learning-based methods, are then conducted on four real
   HSIs in the experiments. Finally, conclusions regarding HAD are
   summarized, and prospects and future development direction are
   discussed.},
DOI = {10.1109/JSTARS.2022.3167830},
ISSN = {1939-1404},
EISSN = {2151-1535},
ResearcherID-Numbers = {Zhang, Lefei/HHM-8850-2022
   Du, Bo/AEY-9731-2022
   Xu, Yichu/JPL-4170-2023
   Zhang, Liangpei/ADI-7616-2022},
ORCID-Numbers = {Xu, Yichu/0009-0002-1474-5700
   },
Unique-ID = {WOS:000793810100003},
}

@article{ WOS:000660860700001,
Author = {Lamata, Lucas},
Title = {Quantum machine learning and quantum biomimetics: A perspective},
Journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY},
Year = {2020},
Volume = {1},
Number = {3},
Month = {SEP},
Abstract = {Quantum machine learning has emerged as an exciting and promising
   paradigm inside quantum technologies. It may permit, on the one hand, to
   carry out more efficient machine learning calculations by means of
   quantum devices, while, on the other hand, to employ machine learning
   techniques to better control quantum systems. Inside quantum machine
   learning, quantum reinforcement learning aims at developing
   `intelligent' quantum agents that may interact with the outer world and
   adapt to it, with the strategy of achieving some final goal. Another
   paradigm inside quantum machine learning is that of quantum
   autoencoders, which may allow one for employing fewer resources in a
   quantum device via a training process. Moreover, the field of quantum
   biomimetics aims at establishing analogies between biological and
   quantum systems, to look for previously inadvertent connections that may
   enable useful applications. Two recent examples are the concepts of
   quantum artificial life, as well as of quantum memristors. In this
   Perspective, we give an overview of these topics, describing the related
   research carried out by the scientific community.},
DOI = {10.1088/2632-2153/ab9803},
Article-Number = {033002},
EISSN = {2632-2153},
ResearcherID-Numbers = {Lamata, Lucas/CAG-6488-2022},
ORCID-Numbers = {Lamata, Lucas/0000-0002-9504-8685},
Unique-ID = {WOS:000660860700001},
}

@article{ WOS:000282915500002,
Author = {Barreno, Marco and Nelson, Blaine and Joseph, Anthony D. and Tygar, J.
   D.},
Title = {The security of machine learning},
Journal = {MACHINE LEARNING},
Year = {2010},
Volume = {81},
Number = {2, SI},
Pages = {121-148},
Month = {NOV},
Abstract = {Machine learning's ability to rapidly evolve to changing and complex
   situations has helped it become a fundamental tool for computer
   security. That adaptability is also a vulnerability: attackers can
   exploit machine learning systems. We present a taxonomy identifying and
   analyzing attacks against machine learning systems. We show how these
   classes influence the costs for the attacker and defender, and we give a
   formal structure defining their interaction. We use our framework to
   survey and analyze the literature of attacks against machine learning
   systems. We also illustrate our taxonomy by showing how it can guide
   attacks against SpamBayes, a popular statistical spam filter. Finally,
   we discuss how our taxonomy suggests new lines of defenses.},
DOI = {10.1007/s10994-010-5188-5},
ISSN = {0885-6125},
EISSN = {1573-0565},
ORCID-Numbers = {JOSEPH, Anthony D./0000-0002-6798-9664},
Unique-ID = {WOS:000282915500002},
}

@article{ WOS:000674276800001,
Author = {Qayyum, Adnan and Ijaz, Aneeqa and Usama, Muhammad and Iqbal, Waleed and
   Qadir, Junaid and Elkhatib, Yehia and Al-Fuqaha, Ala},
Title = {Securing Machine Learning in the Cloud: A Systematic Review of Cloud
   Machine Learning Security},
Journal = {FRONTIERS IN BIG DATA},
Year = {2020},
Volume = {3},
Month = {NOV 12},
Abstract = {With the advances in machine learning (ML) and deep learning (DL)
   techniques, and the potency of cloud computing in offering services
   efficiently and cost-effectively, Machine Learning as a Service (MLaaS)
   cloud platforms have become popular. In addition, there is increasing
   adoption of third-party cloud services for outsourcing training of DL
   models, which requires substantial costly computational resources (e.g.,
   high-performance graphics processing units (GPUs)). Such widespread
   usage of cloud-hosted ML/DL services opens a wide range of attack
   surfaces for adversaries to exploit the ML/DL system to achieve
   malicious goals. In this article, we conduct a systematic evaluation of
   literature of cloud-hosted ML/DL models along both the important
   dimensions-attacks and defenses-related to their security. Our
   systematic review identified a total of 31 related articles out of which
   19 focused on attack, six focused on defense, and six focused on both
   attack and defense. Our evaluation reveals that there is an increasing
   interest from the research community on the perspective of attacking and
   defending different attacks on Machine Learning as a Service platforms.
   In addition, we identify the limitations and pitfalls of the analyzed
   articles and highlight open research issues that require further
   investigation.},
DOI = {10.3389/fdata.2020.587139},
Article-Number = {587139},
EISSN = {2624-909X},
ResearcherID-Numbers = {Qadir, Junaid/Q-6329-2019
   Iqbal, Waleed/H-7285-2016
   Qayyum, Adnan/OMN-2548-2025
   Al-Fuqaha, Ala/IQT-0689-2023
   Elkhatib, Yehia/G-9800-2013
   Ijaz, Aneeqa/NUQ-1859-2025
   Usama, Muhammad/JVZ-9420-2024},
ORCID-Numbers = {Usama, Muhammad/0000-0001-5015-8605
   Al-Fuqaha, Ala/0000-0002-0903-1204
   Elkhatib, Yehia/0000-0003-4639-436X
   },
Unique-ID = {WOS:000674276800001},
}

@article{ WOS:000416161200002,
Author = {Mesquita, Diego P. P. and Gomes, Joao P. P. and Souza Junior, Amauri H.},
Title = {Ensemble of Efficient Minimal Learning Machines for Classification and
   Regression},
Journal = {NEURAL PROCESSING LETTERS},
Year = {2017},
Volume = {46},
Number = {3, SI},
Pages = {751-766},
Month = {DEC},
Abstract = {Minimal Learning Machine (MLM) is a recently proposed supervised
   learning algorithm with performance comparable to most state-of-the-art
   machine learning methods. In this work, we propose ensemble methods for
   classification and regression using MLMs. The goal of ensemble
   strategies is to produce more robust and accurate models when compared
   to a single classifier or regression model. Despite its successful
   application, MLM employs a computationally intensive optimization
   problem as part of its test procedure (out-of-sample data estimation).
   This becomes even more noticeable in the context of ensemble learning,
   where multiple models are used. Aiming to provide fast alternatives to
   the standard MLM, we also propose the Nearest Neighbor Minimal Learning
   Machine and the Cubic Equation Minimal Learning Machine to cope with
   classification and single-output regression problems, respectively. The
   experimental assessment conducted on real-world datasets reports that
   ensemble of fast MLMs perform comparably or superiorly to reference
   machine learning algorithms.},
DOI = {10.1007/s11063-017-9587-5},
ISSN = {1370-4621},
EISSN = {1573-773X},
Unique-ID = {WOS:000416161200002},
}

@article{ WOS:000608126900013,
Author = {Kamerzell, Tim J. and Middaugh, C. Russell},
Title = {Prediction Machines: Applied Machine Learning for Therapeutic Protein
   Design and Development},
Journal = {JOURNAL OF PHARMACEUTICAL SCIENCES},
Year = {2021},
Volume = {110},
Number = {2},
Pages = {665-681},
Month = {FEB},
Abstract = {The rapid growth in technological advances and quantity of scientific
   data over the past decade has led to several challenges including data
   storage and analysis. Accurate models of complex datasets were
   previously difficult to develop and interpret. However, improvements in
   machine learning algorithms have since enabled unparalleled
   classification and prediction capabilities. The application of machine
   learning can be seen throughout diverse industries due to their ease of
   use and interpretability. In this review, we describe popular machine
   learning algorithms and highlight their application in pharmaceutical
   protein development. Machine learning models have now been applied to
   better understand the nonlinear concentration dependent viscosity of
   protein solutions, predict protein oxidation and deamidation rates,
   classify sub-visible particles and compare the physical stability of
   proteins. We also applied several machine learning algorithms using
   previously published data and describe models with improved predictions
   and classification. The authors hope that this review can be used as a
   resource to others and encourage continued application of machine
   learning algorithms to problems in pharmaceutical protein development.},
DOI = {10.1016/j.xphs.2020.11.034},
EarlyAccessDate = {JAN 2021},
ISSN = {0022-3549},
EISSN = {1520-6017},
Unique-ID = {WOS:000608126900013},
}

@article{ WOS:000628311200001,
Author = {Chang, Chun-Hung and Lin, Chieh-Hsin and Lane, Hsien-Yuan},
Title = {Machine Learning and Novel Biomarkers for the Diagnosis of Alzheimer's
   Disease},
Journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES},
Year = {2021},
Volume = {22},
Number = {5},
Month = {MAR},
Abstract = {Background: Alzheimer's disease (AD) is a complex and severe
   neurodegenerative disease that still lacks effective methods of
   diagnosis. The current diagnostic methods of AD rely on cognitive tests,
   imaging techniques and cerebrospinal fluid (CSF) levels of amyloid-beta
   1-42 (A beta 42), total tau protein and hyperphosphorylated tau (p-tau).
   However, the available methods are expensive and relatively invasive.
   Artificial intelligence techniques like machine learning tools have
   being increasingly used in precision diagnosis. Methods: We conducted a
   meta-analysis to investigate the machine learning and novel biomarkers
   for the diagnosis of AD. Methods: We searched PubMed, the Cochrane
   Central Register of Controlled Trials, and the Cochrane Database of
   Systematic Reviews for reviews and trials that investigated the machine
   learning and novel biomarkers in diagnosis of AD. Results: In additional
   to A beta and tau-related biomarkers, biomarkers according to other
   mechanisms of AD pathology have been investigated. Neuronal injury
   biomarker includes neurofiliament light (NFL). Biomarkers about synaptic
   dysfunction and/or loss includes neurogranin, BACE1, synaptotagmin,
   SNAP-25, GAP-43, synaptophysin. Biomarkers about neuroinflammation
   includes sTREM2, and YKL-40. Besides, d-glutamate is one of coagonists
   at the NMDARs. Several machine learning algorithms including support
   vector machine, logistic regression, random forest, and naive Bayes) to
   build an optimal predictive model to distinguish patients with AD from
   healthy controls. Conclusions: Our results revealed machine learning
   with novel biomarkers and multiple variables may increase the
   sensitivity and specificity in diagnosis of AD. Rapid and cost-effective
   HPLC for biomarkers and machine learning algorithms may assist
   physicians in diagnosing AD in outpatient clinics.},
DOI = {10.3390/ijms22052761},
Article-Number = {2761},
ISSN = {1661-6596},
EISSN = {1422-0067},
ResearcherID-Numbers = {Chang, Chun-Hung/AAT-1641-2021},
ORCID-Numbers = {Lane, Hsien-Yuan/0000-0003-2162-8174
   Chang, Chun-Hung/0000-0003-2949-5855
   },
Unique-ID = {WOS:000628311200001},
}

@article{ WOS:000485885700001,
Author = {Kou, Gang and Chao, Xiangrui and Peng, Yi and Alsaadi, Fawaz E. and
   Herrera-Viedma, Enrique},
Title = {MACHINE LEARNING METHODS FOR SYSTEMIC RISK ANALYSIS IN FINANCIAL SECTORS},
Journal = {TECHNOLOGICAL AND ECONOMIC DEVELOPMENT OF ECONOMY},
Year = {2019},
Volume = {25},
Number = {5},
Pages = {716-742},
Abstract = {Financial systemic risk is an important issue in economics and financial
   systems. Trying to detect and respond to systemic risk with growing
   amounts of data produced in financial markets and systems, a lot of
   researchers have increasingly employed machine learning methods. Machine
   learning methods study the mechanisms of outbreak and contagion of
   systemic risk in the financial network and improve the current
   regulation of the financial market and industry. In this paper, we
   survey existing researches and methodologies on assessment and
   measurement of financial systemic risk combined with machine learning
   technologies, including big data analysis, network analysis and
   sentiment analysis, etc. In addition, we identify future challenges, and
   suggest further research topics. The main purpose of this paper is to
   introduce current researches on financial systemic risk with machine
   learning methods and to propose directions for future work.},
DOI = {10.3846/tede.2019.8740},
ISSN = {2029-4913},
EISSN = {2029-4921},
ResearcherID-Numbers = {Kou, Gang/G-3869-2010
   HERRERA-VIEDMA, ENRIQUE/C-2704-2008
   Alsaadi, Fawaz/GLT-2606-2022},
Unique-ID = {WOS:000485885700001},
}

@article{ WOS:000705637500013,
Author = {Albuquerque, Pedro C. and Cajueiro, Daniel O. and Rossi, Marina D. C.},
Title = {Machine learning models for forecasting power electricity consumption
   using a high dimensional dataset},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2022},
Volume = {187},
Month = {JAN},
Abstract = {We use regularized machine learning models to forecast Brazilian power
   electricity consumption for short and medium terms. We compare our
   models to benchmark specifications such as Random Walk and
   Autoregressive Integrated Moving Average. Our results show that machine
   learning methods, especially Random Forest and Lasso Lars, give more
   accurate forecasts for all horizons. Random Forest and Lasso Lars
   managed to keep up with the trend and the seasonality for various time
   horizons. The gain in predicting PEC using machine learning models
   relative to the benchmarks is considerably higher for the very
   short-term. Machine learning variable selection further shows that
   lagged consumption values are extremely important for very short-term
   forecasting due to the series high autocorrelation. Other variables such
   as weather and calendar variables are important for longer time
   horizons.},
DOI = {10.1016/j.eswa.2021.115917},
EarlyAccessDate = {SEP 2021},
Article-Number = {115917},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {Cajueiro, Daniel/R-2694-2019},
ORCID-Numbers = {Delmondes de Carvalho Rossi, Marina/0000-0002-0302-5719
   CAJUEIRO, DANIEL/0000-0001-5898-1655
   },
Unique-ID = {WOS:000705637500013},
}

@article{ WOS:000752230700001,
Author = {Makhmet, A. S. and Sharaev, M. G. and Dyusembaev, A. E. and Kustubayeva,
   A. M.},
Title = {Machine learning for brain signal analysis},
Journal = {INTERNATIONAL JOURNAL OF BIOLOGY AND CHEMISTRY},
Year = {2021},
Volume = {14},
Number = {2},
Pages = {4-11},
Abstract = {Machine learning (ML) is an effective tool for analysing signals from
   the human brain. Machine Learning techniques provide new insight into
   the understanding of brain function in healthy subjects and patients
   with neurological and mental disorders. Here we introduce the
   application of machine learning to resonance imaging (fMRI) and
   Electroencephalography (EEG). The article provides a brief overview of
   the theoretical concept of machine learning and its types: supervised,
   unsupervised and reinforcement learning. The potential of machine
   learning applications in pathology is discussed. Differences between EEG
   and fMRI methods regarding machine learning application and an overview
   of the techniques employed in different research studies are reviewed.
   The new machine learning methods invented for analysis of brain signals
   in the resting ate and during the performance of the different cognitive
   tasks would be useful and worth considering in other domains, not
   limited to medicine.},
DOI = {10.26577/ijbch.2021.v14.i2.01},
ISSN = {2218-7979},
EISSN = {2409-370X},
ResearcherID-Numbers = {Sharaev, Maksim/L-9769-2016
   Dyusembaev, A.E./P-8170-2014},
ORCID-Numbers = {Makhmet, Ainur/0000-0002-5423-3550
   },
Unique-ID = {WOS:000752230700001},
}

@article{ WOS:000432883500001,
Author = {James, Scott C. and Zhang, Yushan and O'Donncha, Fearghal},
Title = {A machine learning framework to forecast wave conditions},
Journal = {COASTAL ENGINEERING},
Year = {2018},
Volume = {137},
Pages = {1-10},
Month = {JUL},
Abstract = {A machine learning framework is developed to estimate ocean-wave
   conditions. By supervised training of machine learning models on many
   thousands of iterations of a physics-based wave model, accurate
   representations of significant wave heights and period can be used to
   predict ocean conditions. A model of Monterey Bay was used as the
   example test site; it was forced by measured wave conditions,
   ocean-current nowcasts, and reported winds. These input data along with
   model outputs of spatially variable wave heights and characteristic
   period were aggregated into supervised learning training and test data
   sets, which were supplied to machine learning models. These machine
   learning models replicated wave heights from the physics-based model
   with a root-mean-squared error of 9 cm and correctly identify over 90\%
   of the characteristic periods for the test-data sets. Impressively,
   transforming model inputs to outputs through matrix operations requires
   only a fraction (< 1/1, 000 th) of the computation time compared to
   forecasting with the physics-based model.},
DOI = {10.1016/j.coastaleng.2018.03.004},
ISSN = {0378-3839},
EISSN = {1872-7379},
ResearcherID-Numbers = {James, Scott/E-5469-2018
   O'Donncha, Fearghal/AFP-9619-2022},
ORCID-Numbers = {James, Scott/0000-0001-7955-0491
   O'Donncha, Fearghal/0000-0002-0275-1591
   },
Unique-ID = {WOS:000432883500001},
}

@article{ WOS:000209236900011,
Author = {Natekin, Alexey and Knoll, Alois},
Title = {Gradient boosting machines, a tutorial},
Journal = {FRONTIERS IN NEUROROBOTICS},
Year = {2013},
Volume = {7},
Abstract = {Gradient boosting machines are a family of powerful machine-learning
   techniques that have shown considerable success in a wide range of
   practical applications. They are highly customizable to the particular
   needs of the application, like being learned with respect to different
   loss functions. This article gives a tutorial introduction into the
   methodology of gradient boosting methods with a strong focus on machine
   learning aspects of modeling. A theoretical information is complemented
   with descriptive examples and illustrations which cover all the stages
   of the gradient boosting model design. Considerations on handling the
   model complexity are discussed. Three practical examples of gradient
   boosting applications are presented and comprehensively analyzed.},
DOI = {10.3389/fnbot.2013.00021},
Article-Number = {21},
ISSN = {1662-5218},
ResearcherID-Numbers = {Knoll, Alois/AAN-8417-2021},
ORCID-Numbers = {Knoll, Alois/0000-0003-4840-076X},
Unique-ID = {WOS:000209236900011},
}

@article{ WOS:000829738400001,
Author = {Oliveira Jr, Osvaldo N. and Oliveira, Maria Cristina F.},
Title = {Materials Discovery With Machine Learning and Knowledge Discovery},
Journal = {FRONTIERS IN CHEMISTRY},
Year = {2022},
Volume = {10},
Month = {JUL 7},
Abstract = {Machine learning and other artificial intelligence methods are gaining
   increasing prominence in chemistry and materials sciences, especially
   for materials design and discovery, and in data analysis of results
   generated by sensors and biosensors. In this paper, we present a
   perspective on this current use of machine learning, and discuss the
   prospects of the future impact of extending the use of machine learning
   to encompass knowledge discovery as an essential step towards a new
   paradigm of machine-generated knowledge. The reasons why results so far
   have been limited are given with a discussion of the limitations of
   machine learning in tasks requiring interpretation. Also discussed is
   the need to adapt the training of students and scientists in chemistry
   and materials sciences, to better explore the potential of artificial
   intelligence capabilities.},
DOI = {10.3389/fchem.2022.930369},
Article-Number = {930369},
ISSN = {2296-2646},
ResearcherID-Numbers = {Oliveira, Osvaldo/A-1714-2008
   Oliveira, Beatriz/KGL-8895-2024},
Unique-ID = {WOS:000829738400001},
}

@article{ WOS:000841672300001,
Author = {Katayama, Yotaro and Yokota, Ryo and Akiyama, Taishin and Kobayashi,
   Tetsuya J.},
Title = {Machine Learning Approaches to TCR Repertoire Analysis},
Journal = {FRONTIERS IN IMMUNOLOGY},
Year = {2022},
Volume = {13},
Month = {JUL 15},
Abstract = {Sparked by the development of genome sequencing technology, the quantity
   and quality of data handled in immunological research have been changing
   dramatically. Various data and database platforms are now driving the
   rapid progress of machine learning for immunological data analysis. Of
   various topics in immunology, T cell receptor repertoire analysis is one
   of the most important targets of machine learning for assessing the
   state and abnormalities of immune systems. In this paper, we review
   recent repertoire analysis methods based on machine learning and deep
   learning and discuss their prospects.},
DOI = {10.3389/fimmu.2022.858057},
Article-Number = {858057},
ISSN = {1664-3224},
ORCID-Numbers = {Katayama, Yotaro/0000-0003-2645-5938},
Unique-ID = {WOS:000841672300001},
}

@article{ WOS:000445712400274,
Author = {Liakos, Konstantinos G. and Busato, Patrizia and Moshou, Dimitrios and
   Pearson, Simon and Bochtis, Dionysis},
Title = {Machine Learning in Agriculture: A Review},
Journal = {SENSORS},
Year = {2018},
Volume = {18},
Number = {8},
Month = {AUG},
Abstract = {Machine learning has emerged with big data technologies and
   high-performance computing to create new opportunities for data
   intensive science in the multi-disciplinary agri-technologies domain. In
   this paper, we present a comprehensive review of research dedicated to
   applications of machine learning in agricultural production systems. The
   works analyzed were categorized in (a) crop management, including
   applications on yield prediction, disease detection, weed detection crop
   quality, and species recognition; (b) livestock management, including
   applications on animal welfare and livestock production; (c) water
   management; and (d) soil management. The filtering and classification of
   the presented articles demonstrate how agriculture will benefit from
   machine learning technologies. By applying machine learning to sensor
   data, farm management systems are evolving into real time artificial
   intelligence enabled programs that provide rich recommendations and
   insights for farmer decision support and action.},
DOI = {10.3390/s18082674},
Article-Number = {2674},
EISSN = {1424-8220},
ResearcherID-Numbers = {Bochtis, Dionysis/P-4648-2018
   Dionysis, Bochtis/P-4648-2018
   Moshou, Dimitrios/ABG-4189-2020
   Pearson, Simon/G-4035-2019
   },
ORCID-Numbers = {Bochtis, Dionysis/0000-0002-7058-5986
   Pearson, Simon/0000-0002-4297-4837
   Moshou, Dimitrios/0000-0001-7270-5307},
Unique-ID = {WOS:000445712400274},
}

@article{ WOS:000430994500003,
Author = {Zhao, Yong-Ping and Hu, Qian-Kun and Xu, Jian-Guo and Li, Bing and
   Huang, Gong and Pan, Ying-Ting},
Title = {A robust extreme learning machine for modeling a small-scale turbojet
   engine},
Journal = {APPLIED ENERGY},
Year = {2018},
Volume = {218},
Pages = {22-35},
Month = {MAY 15},
Abstract = {In this paper, a robust extreme learning machine is proposed. In
   comparison with the original extreme learning machine and the
   regularized extreme learning machine, this robust algorithm minimizes
   both the mean and variance of modeling errors in the objective function
   to overcome the bias-variance dilemma. As a result, its generalization
   performance and robustness are enhanced, and these merits are further
   proved theoretically. In addition, this proposed algorithm can keep the
   same computational efficiency as the original extreme learning machine
   and the regularized extreme learning machine. Then, several benchmark
   data sets are used to test the effectiveness and soundness of the
   proposed algorithm. Finally, it is employed to model a real small-scale
   turbojet engine. This engine is fit well. Especially, on the idle phase,
   where the signal-to-noise ratio is low and it is very hard to model, the
   proposed algorithm performs well and its robustness is sufficiently
   showcased. All in all, the proposed algorithm provides a candidate
   technique for modeling real systems.},
DOI = {10.1016/j.apenergy.2018.02.175},
ISSN = {0306-2619},
EISSN = {1872-9118},
ResearcherID-Numbers = {Hu, Qiankun/GZH-0591-2022},
ORCID-Numbers = {Hu, Qiankun/0000-0002-0568-4961},
Unique-ID = {WOS:000430994500003},
}

@article{ WOS:000477891100005,
Author = {Thabtah, Fadi},
Title = {Machine learning in autistic spectrum disorder behavioral research: A
   review and ways forward},
Journal = {INFORMATICS FOR HEALTH \& SOCIAL CARE},
Year = {2019},
Volume = {44},
Number = {3},
Pages = {278-297},
Month = {JUL 3},
Abstract = {Autistic Spectrum Disorder (ASD) is a mental disorder that retards
   acquisition of linguistic, communication, cognitive, and social skills
   and abilities. Despite being diagnosed with ASD, some individuals
   exhibit outstanding scholastic, non-academic, and artistic capabilities,
   in such cases posing a challenging task for scientists to provide
   answers. In the last few years, ASD has been investigated by social and
   computational intelligence scientists utilizing advanced technologies
   such as machine learning to improve diagnostic timing, precision, and
   quality. Machine learning is a multidisciplinary research topic that
   employs intelligent techniques to discover useful concealed patterns,
   which are utilized in prediction to improve decision making. Machine
   learning techniques such as support vector machines, decision trees,
   logistic regressions, and others, have been applied to datasets related
   to autism in order to construct predictive models. These models claim to
   enhance the ability of clinicians to provide robust diagnoses and
   prognoses of ASD. However, studies concerning the use of machine
   learning in ASD diagnosis and treatment suffer from conceptual,
   implementation, and data issues such as the way diagnostic codes are
   used, the type of feature selection employed, the evaluation measures
   chosen, and class imbalances in data among others. A more serious claim
   in recent studies is the development of a new method for ASD diagnoses
   based on machine learning. This article critically analyses these recent
   investigative studies on autism, not only articulating the
   aforementioned issues in these studies but also recommending paths
   forward that enhance machine learning use in ASD with respect to
   conceptualization, implementation, and data. Future studies concerning
   machine learning in autism research are greatly benefitted by such
   proposals.},
DOI = {10.1080/17538157.2017.1399132},
ISSN = {1753-8157},
EISSN = {1753-8165},
ORCID-Numbers = {Fayez, Fadi/0000-0002-2664-4694},
Unique-ID = {WOS:000477891100005},
}

@article{ WOS:000448616200004,
Author = {Wang, Zheng and O'Boyle, Michael},
Title = {Machine Learning in Compiler Optimization},
Journal = {PROCEEDINGS OF THE IEEE},
Year = {2018},
Volume = {106},
Number = {11, SI},
Pages = {1879-1901},
Month = {NOV},
Abstract = {In the last decade, machine-learning-based compilation has moved from an
   obscure research niche to a mainstream activity. In this paper, we
   describe the relationship between machine learning and compiler
   optimization and introduce the main concepts of features, models,
   training, and deployment. We then provide a comprehensive survey and
   provide a road map for the wide variety of different research areas. We
   conclude with a discussion on open issues in the area and potential
   research directions. This paper provides both an accessible introduction
   to the fast moving area of machine-learning-based compilation and a
   detailed bibliography of its main achievements.},
DOI = {10.1109/JPROC.2018.2817118},
ISSN = {0018-9219},
EISSN = {1558-2256},
ResearcherID-Numbers = {Wang, Zheng/AAP-8818-2020},
ORCID-Numbers = {Wang, Zheng/0000-0001-6157-0662
   },
Unique-ID = {WOS:000448616200004},
}

@article{ WOS:000450513100004,
Author = {Handelman, G. S. and Kok, H. K. and Chandra, R. V. and Razavi, A. H. and
   Lee, M. J. and Asadi, H.},
Title = {eDoctor: machine learning and the future of medicine},
Journal = {JOURNAL OF INTERNAL MEDICINE},
Year = {2018},
Volume = {284},
Number = {6},
Pages = {603-619},
Month = {DEC},
Abstract = {Machine learning (ML) is a burgeoning field of medicine with huge
   resources being applied to fuse computer science and statistics to
   medical problems. Proponents of ML extol its ability to deal with large,
   complex and disparate data, often found within medicine and feel that ML
   is the future for biomedical research, personalized medicine,
   computer-aided diagnosis to significantly advance global health care.
   However, the concepts of ML are unfamiliar to many medical professionals
   and there is untapped potential in the use of ML as a research tool. In
   this article, we provide an overview of the theory behind ML, explore
   the common ML algorithms used in medicine including their pitfalls and
   discuss the potential future of ML in medicine.},
DOI = {10.1111/joim.12822},
ISSN = {0954-6820},
EISSN = {1365-2796},
ResearcherID-Numbers = {Chandra, Ronil/GPK-0357-2022},
ORCID-Numbers = {Chandra, Ronil/0000-0001-7555-2297
   },
Unique-ID = {WOS:000450513100004},
}

@article{ WOS:001239078200005,
Author = {Allen, Genevera I. and Gan, Luqin and Zheng, Lili},
Title = {Interpretable Machine Learning for Discovery: Statistical Challenges and
   Opportunities},
Journal = {ANNUAL REVIEW OF STATISTICS AND ITS APPLICATION},
Year = {2024},
Volume = {11},
Pages = {97-121},
Abstract = {New technologies have led to vast troves of large and complex data sets
   across many scientific domains and industries. People routinely use
   machine learning techniques not only to process, visualize, and make
   predictions from these big data, but also to make data-driven
   discoveries. These discoveries are often made using interpretable
   machine learning, or machine learning models and techniques that yield
   human-understandable insights. In this article, we discuss and review
   the field of interpretable machine learning, focusing especially on the
   techniques, as they are often employed to generate new knowledge or make
   discoveries from large data sets.We outline the types of discoveries
   that can be made using interpretable machine learning in both supervised
   and unsupervised settings. Additionally, we focus on the grand challenge
   of how to validate these discoveries in a data-driven manner, which
   promotes trust in machine learning systems and reproducibility in
   science.We discuss validation both from a practical perspective,
   reviewing approaches based on data-splitting and stability, as well as
   from a theoretical perspective, reviewing statistical results on model
   selection consistency and uncertainty quantification via statistical
   inference. Finally, we conclude by highlighting open challenges in using
   interpretable machine learning techniques to make discoveries, including
   gaps between theory and practice for validating data-driven discoveries.},
DOI = {10.1146/annurev-statistics-040120-030919},
ISSN = {2326-8298},
EISSN = {2326-831X},
ResearcherID-Numbers = {Zheng, Lili/B-9847-2013
   GAN, Luqin/IUM-5910-2023},
ORCID-Numbers = {GAN, Luqin/0000-0002-0449-5376
   },
Unique-ID = {WOS:001239078200005},
}

@article{ WOS:001486910100001,
Author = {Zhang, Zhefeng and Wu, Yueqi and Ma, Xiandong},
Title = {Quantum machine learning based wind turbine condition monitoring: State
   of the art and future prospects},
Journal = {ENERGY CONVERSION AND MANAGEMENT},
Year = {2025},
Volume = {332},
Month = {MAY 15},
Abstract = {Wind energy, as a popular renewable resource, has gained extensive
   development and application in recent decades. Effective condition
   monitoring and fault diagnosis are crucial for ensuring the reliable
   operation of wind turbines. While conventional machine learning methods
   have been widely used in wind turbine condition monitoring, these
   approaches often face challenges such as complex feature extraction,
   limited model generalization, and high computational costs when dealing
   with large-scale, high-dimensional, and complex datasets. The emergence
   of quantum computing has opened up a new paradigm of machine learning
   algorithms. Quantum machine learning combines the advantages of quantum
   computing and machine learning, with the potential to surpass classical
   computational capabilities. This paper firstly reviews applications and
   limitations of the state-of-the-art machine learning-based condition
   monitoring techniques for wind turbines. It then reviews the
   fundamentals of quantum computing, quantum machine learning algorithms
   and their applications, covering quantum-based feature extraction,
   classification and regression for fault detection and the use of quantum
   neural networks for predictive maintenance. Through comparison, it is
   observed that quantum machine learning methods, even without extensive
   optimization, can achieve accuracy levels comparable to those of
   optimized conventional machine learning approaches. The challenges of
   applying quantum machine learning are also addressed, along with the
   future research and development prospects. The objective of this review
   is to fill a gap in the published literature by providing a new paradigm
   approach for wind turbine condition monitoring. By promoting quantum
   machine learning in this field, the reliability and efficiency of wind
   power systems are ultimately sought to be enhanced.},
DOI = {10.1016/j.enconman.2025.119694},
EarlyAccessDate = {MAR 2025},
Article-Number = {119694},
ISSN = {0196-8904},
EISSN = {1879-2227},
ResearcherID-Numbers = {Wu, Yueqi/AAF-4543-2020
   },
ORCID-Numbers = {Ma, Xiandong/0000-0001-7363-9727},
Unique-ID = {WOS:001486910100001},
}

@article{ WOS:000664641500020,
Author = {Salam, Mustafa Abdul and Taha, Sanaa and Ramadan, Mohamed},
Title = {COVID-19 detection using federated machine learning},
Journal = {PLOS ONE},
Year = {2021},
Volume = {16},
Number = {6},
Month = {JUN 8},
Abstract = {The current COVID-19 pandemic threatens human life, health, and
   productivity. AI plays an essential role in COVID-19 case classification
   as we can apply machine learning models on COVID-19 case data to predict
   infectious cases and recovery rates using chest x-ray. Accessing
   patient's private data violates patient privacy and traditional machine
   learning model requires accessing or transferring whole data to train
   the model. In recent years, there has been increasing interest in
   federated machine learning, as it provides an effective solution for
   data privacy, centralized computation, and high computation power. In
   this paper, we studied the efficacy of federated learning versus
   traditional learning by developing two machine learning models (a
   federated learning model and a traditional machine learning model)using
   Keras and TensorFlow federated, we used a descriptive dataset and chest
   x-ray (CXR) images from COVID-19 patients. During the model training
   stage, we tried to identify which factors affect model prediction
   accuracy and loss like activation function, model optimizer, learning
   rate, number of rounds, and data Size, we kept recording and plotting
   the model loss and prediction accuracy per each training round, to
   identify which factors affect the model performance, and we found that
   softmax activation function and SGD optimizer give better prediction
   accuracy and loss, changing the number of rounds and learning rate has
   slightly effect on model prediction accuracy and prediction loss but
   increasing the data size did not have any effect on model prediction
   accuracy and prediction loss. finally, we build a comparison between the
   proposed models' loss, accuracy, and performance speed, the results
   demonstrate that the federated machine learning model has a better
   prediction accuracy and loss but higher performance time than the
   traditional machine learning model.},
DOI = {10.1371/journal.pone.0252573},
Article-Number = {e0252573},
ISSN = {1932-6203},
ResearcherID-Numbers = {Ramadan, Mohamed/AAU-5678-2020
   Salam, Mustafa/AAK-1349-2021
   },
ORCID-Numbers = {Abdul Salam, Mustafa/0000-0003-1673-6947},
Unique-ID = {WOS:000664641500020},
}

@article{ WOS:000472796800001,
Author = {Rahangdale, Ashwini and Raut, Shital},
Title = {Machine Learning Methods for Ranking},
Journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING},
Year = {2019},
Volume = {29},
Number = {6},
Pages = {729-761},
Month = {JUN},
Abstract = {Learning-to-rank is one of the learning frameworks in machine learning
   and it aims to organize the objects in a particular order according to
   their preference, relevance or ranking. In this paper, we give a
   comprehensive survey for learning-to-rank. First, we discuss the
   different approaches along with different machine learning methods such
   as regression, SVM, neural network-based, evolutionary, boosting method.
   In order to compare different approaches: we discuss the characteristics
   of each approach. In addition to that, learning-to-rank algorithms
   combine with other machine learning paradigms such as semi-supervised
   learning, active learning, reinforcement learning and deep learning. The
   learning-to-rank models employ with parallel or big data analytics to
   review computational and storage advantage. Many real-time applications
   use learning-to-rank for preference learning. In regard to this, we
   introduce some representative works. Finally, we highlighted future
   directions to investigate learning-to-rank methods.},
DOI = {10.1142/S021819401930001X},
ISSN = {0218-1940},
EISSN = {1793-6403},
ResearcherID-Numbers = {Rahangdale, Ahswini/AAQ-5580-2021
   Raut, Shital/S-1585-2018
   },
ORCID-Numbers = {, shital/0000-0001-6736-7497},
Unique-ID = {WOS:000472796800001},
}

@article{ WOS:001059216400001,
Author = {Zhang, Zhiqiang and Wang, Gongwen and Carranza, Emmanuel John M. and
   Liu, Chong and Li, Junjian and Fu, Chao and Liu, Xinxing and Chen, Chao
   and Fan, Junjie and Dong, Yulong},
Title = {An integrated machine learning framework with uncertainty quantification
   for three-dimensional lithological modeling from multi-source
   geophysical data and drilling data},
Journal = {ENGINEERING GEOLOGY},
Year = {2023},
Volume = {324},
Month = {OCT},
Abstract = {Nowadays, it is commonplace for geological surveys to integrate
   multi-source geophysical data and drilling data in order to construct
   three-dimensional (3D) lithological models. In this context, manual
   translation of complex geophysical data into parameters used for 3D
   lithological modeling is challenging. Machine learning has recently
   shown great potential in 3D lithological modeling. However, the
   performance of machine learning algorithm is influenced by the imbalance
   in number of categories of lithological samples. In addition, the
   uncertainty associated with 3D lithological modeling by machine learning
   has rarely been quantified. This study presents a novel integrated
   machine learning framework to address the imbalance issue and to
   quantify uncertainty in 3D lithological modeling. As its novelty, our
   integrated machine learning framework can subdivide total uncertainty
   into aleatoric and epistemic uncertainties in the 3D lithological
   modeling procedure by stochastic gradient Langevin boosting. Another
   innovation of this study is the use of Bayesian hyperparameter
   optimization for automatic tuning of hyperparameters of the integrated
   machine learning framework. The 3D lithological and uncertainty modeling
   case study in the Jiaojia-Sanshandao gold district of China demonstrated
   the superiority of our proposed integrated machine learning framework.
   The proposed framework has great potential in integrating multi-source
   geophysical and drilling data for 3D lithological and uncertainty
   modeling in engineering geology.},
DOI = {10.1016/j.enggeo.2023.107255},
EarlyAccessDate = {AUG 2023},
Article-Number = {107255},
ISSN = {0013-7952},
EISSN = {1872-6917},
ResearcherID-Numbers = {Zhang, Zhiqiang/KWV-1147-2024
   Carranza, Emmanuel John/D-3837-2009
   Wang, Gongwen/AAC-1682-2022
   li, junjian/HMU-8908-2023},
Unique-ID = {WOS:001059216400001},
}

@article{ WOS:000888574800002,
Author = {Immonen, Riku and Hamalainen, Timo},
Title = {Tiny Machine Learning for Resource-Constrained Microcontrollers},
Journal = {JOURNAL OF SENSORS},
Year = {2022},
Volume = {2022},
Month = {NOV 10},
Abstract = {We use 250 billion microcontrollers daily in electronic devices that are
   capable of running machine learning models inside them. Unfortunately,
   most of these microcontrollers are highly constrained in terms of
   computational resources, such as memory usage or clock speed. These are
   exactly the same resources that play a key role in teaching and running
   a machine learning model with a basic computer. However, in a
   microcontroller environment, constrained resources make a critical
   difference. Therefore, a new paradigm known as tiny machine learning had
   to be created to meet the constrained requirements of the embedded
   devices. In this review, we discuss the resource optimization challenges
   of tiny machine learning and different methods, such as quantization,
   pruning, and clustering, that can be used to overcome these resource
   difficulties. Furthermore, we summarize the present state of tiny
   machine learning frameworks, libraries, development environments, and
   tools. The benchmarking of tiny machine learning devices is another
   thing to be concerned about; these same constraints of the
   microcontrollers and diversity of hardware and software turn to
   benchmark challenges that must be resolved before it is possible to
   measure performance differences reliably between embedded devices. We
   also discuss emerging techniques and approaches to boost and expand the
   tiny machine learning process and improve data privacy and security. In
   the end, we form a conclusion about tiny machine learning and its future
   development.},
DOI = {10.1155/2022/7437023},
Article-Number = {7437023},
ISSN = {1687-725X},
EISSN = {1687-7268},
ORCID-Numbers = {Hamalainen, Timo/0000-0002-4168-9102},
Unique-ID = {WOS:000888574800002},
}

@article{ WOS:000555753600001,
Author = {Ahmad, Amir and Garhwal, Sunita and Ray, Santosh Kumar and Kumar, Gagan
   and Malebary, Sharaf Jameel and Barukab, Omar Mohammed},
Title = {The Number of Confirmed Cases of Covid-19 by using Machine Learning:
   Methods and Challenges},
Journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING},
Year = {2021},
Volume = {28},
Number = {4},
Pages = {2645-2653},
Month = {JUN},
Abstract = {Covid-19 is one of the biggest health challenges that the world has ever
   faced. Public health policy makers need the reliable prediction of the
   confirmed cases in future to plan medical facilities. Machine learning
   methods learn from the historical data and make predictions about the
   events. Machine learning methods have been used to predict the number of
   confirmed cases of Covid-19. In this paper, we present a detailed review
   of these research papers. We present a taxonomy that groups them in four
   categories. We further present the challenges in this field. We provide
   suggestions to the machine learning practitioners to improve the
   performance of machine learning methods for the prediction of confirmed
   cases of Covid-19.},
DOI = {10.1007/s11831-020-09472-8},
EarlyAccessDate = {AUG 2020},
ISSN = {1134-3060},
EISSN = {1886-1784},
ResearcherID-Numbers = {Ray, Santosh Kumar/U-5575-2018
   Ray, Santosh/U-5575-2018
   Malebary, Sharaf/S-5018-2018
   Barukab, Omar/AAS-6659-2021},
ORCID-Numbers = {Ray, Santosh Kumar/0000-0003-0127-7653
   Malebary, Sharaf/0000-0003-4339-3791
   Garhwal, Sunita/0000-0002-8959-3724
   },
Unique-ID = {WOS:000555753600001},
}

@article{ WOS:000982564700004,
Author = {Zhao, Heng and Chen, Yixing and Fu, Xianghua},
Title = {Comparison of Machine Learning Based on Category Theory},
Journal = {JOURNAL OF WEB ENGINEERING},
Year = {2023},
Volume = {22},
Number = {1},
Pages = {41-54},
Abstract = {In recent years, machine learning has been widely used in data analysis
   of network engineering. The increasing types of model and data enhance
   the complexity of machine learning. In this paper, we propose a
   mathematical structure based on category theory as a combination of
   machine learning that combines multiple theories of data mining. We aim
   to study machine learning from the perspective of classification theory.
   Category theory utilizes mathematical language to connect the various
   structures of machine learning. We implement the representation of
   machine learning with category theory. In the experimental section,
   slice categories and functors are introduced in detail to model the data
   preprocessing. We use functors to preprocess the benchmark dataset and
   evaluate the accuracy of nine machine learning models. A key
   contribution is the representation of slice categories. This study
   provides a structural perspective of machine learning and a general
   method for the combination of category theory and machine learning.},
DOI = {10.13052/jwe1540-9589.2213},
ISSN = {1540-9589},
EISSN = {1544-5976},
ResearcherID-Numbers = {Chen, Yixing/GQI-4200-2022},
ORCID-Numbers = {Fu, Xianghua/0009-0008-7222-5264
   },
Unique-ID = {WOS:000982564700004},
}

@article{ WOS:001079720800001,
Author = {Davies, William George and Babamohammadi, Shervan and Yang, Yang and
   Soltani, Salman Masoudi},
Title = {The rise of the machines: A state-of-the-art technical review on
   process modelling and machine learning within hydrogen production with
   carbon capture},
Journal = {GAS SCIENCE AND ENGINEERING},
Year = {2023},
Volume = {118},
Month = {OCT},
Abstract = {This study aims to present a compendious yet technical scrutiny of the
   current trends in process modelling as well as the implementation of
   machine learning within combined hydrogen production and carbon capture
   (i.e. blue hydrogen). The paper is intended to accurately portray the
   role that machine learning is anticipated to play within research and
   development in blue hydrogen production in the forthcoming years. This
   covers the implementation of machine learning at both material and
   process development levels. The paper provides a concise overview of the
   current trends in blue hydrogen production, as well as an intro to
   machine learning and process modelling within the same context. We have
   reinforced our paper by first summarising a brief description of the key
   ``tools{''} used in machine learning and process modelling, before
   painstakingly examining the imple-mentation of these techniques in blue
   hydrogen production and the less-discovered merits and
   de-merits.Ultimately, the paper depicts a clear picture of the
   advancements in machine learning and the major role it is expected to
   play in accelerating research and development in blue hydrogen
   production on both material and process development fronts. The paper
   strives to shed some light on the key advantages that machine learning
   has to offer in blue hydrogen for future research work.},
DOI = {10.1016/j.jgsce.2023.205104},
EarlyAccessDate = {SEP 2023},
Article-Number = {205104},
ISSN = {2949-9097},
EISSN = {2949-9089},
ResearcherID-Numbers = {Yang, Yang/MBG-2753-2025
   Babamohammadi, Shervan/GPF-5142-2022
   Babamohammadi, Shervan/K-8940-2013
   Soltani, Salman/U-5732-2019
   },
ORCID-Numbers = {Yang, Yang/0000-0001-7827-7585
   Babamohammadi, Shervan/0000-0002-9659-4194
   Masoudi Soltani, Salman/0000-0002-5983-0397
   Davies, Billy/0000-0002-5444-7962},
Unique-ID = {WOS:001079720800001},
}

@article{ WOS:000678361100002,
Author = {Maass, Wolfgang and Storey, Veda C.},
Title = {Pairing conceptual modeling with machine learning},
Journal = {DATA \& KNOWLEDGE ENGINEERING},
Year = {2021},
Volume = {134},
Month = {JUL},
Abstract = {Both conceptual modeling and machine learning have long been recognized
   as important areas of research. With the increasing emphasis on
   digitizing and processing large amounts of data for business and other
   applications, it would be helpful to consider how these areas of
   research can complement each other. To understand how they can be
   paired, we provide an overview of machine learning foundations and
   development cycle. We then examine how conceptual modeling can be
   applied to machine learning and propose a framework for incorporating
   conceptual modeling into data science projects. The framework is
   illustrated by applying it to a healthcare application. For the inverse
   pairing, machine learning can impact conceptual modeling through text
   and rule mining, as well as knowledge graphs. The pairing of conceptual
   modeling and machine learning in this way should help lay the
   foundations for future research.},
DOI = {10.1016/j.datak.2021.101909},
EarlyAccessDate = {JUL 2021},
Article-Number = {101909},
ISSN = {0169-023X},
EISSN = {1872-6933},
ResearcherID-Numbers = {Maass, Wolfgang/ACE-7055-2022
   },
ORCID-Numbers = {Storey, Veda/0000-0002-8735-1553
   Maass, Wolfgang/0000-0003-4057-0924},
Unique-ID = {WOS:000678361100002},
}

@article{ WOS:000518864800005,
Author = {Munawar, Usman and Wang, Zhanle},
Title = {A Framework of Using Machine Learning Approaches for Short-Term Solar
   Power Forecasting},
Journal = {JOURNAL OF ELECTRICAL ENGINEERING \& TECHNOLOGY},
Year = {2020},
Volume = {15},
Number = {2},
Pages = {561-569},
Month = {MAR},
Abstract = {Various machine learning approaches are widely applied for short-term
   solar power forecasting, which is highly demanded for renewable energy
   integration and power system planning. However, appropriate selection of
   machine learning models and data features is a significant challenge. In
   this study, a framework is developed to quantitatively evaluate various
   models and feature selection methods, and the best combination for
   short-term solar power forecasting is discovered. More specifically, the
   machine learning methods include the random forest, artificial neural
   network and extreme gradient boosting (XGBoost), and the feature
   selection techniques include the feature importance and principle
   component analysis (PCA). All possible combinations of these machine
   learning and feature selection methods are developed and evaluated for
   solar power forecasting. The best ensemble of machine learning methods
   and feature selection techniques is identified for solar power
   forecasting in Hawaii, US. Simulation results show that the XGBoost
   method with features selected by the PCA method outperforms the other
   approaches. In addition, the random forest and XGBoost models have
   rarely been used for short-term solar forecasting. This framework can be
   used to select appropriate machine learning approaches for short-term
   solar power forecasting and the simulation results can be used as a
   baseline for comparison.},
DOI = {10.1007/s42835-020-00346-4},
ISSN = {1975-0102},
EISSN = {2093-7423},
Unique-ID = {WOS:000518864800005},
}

@article{ WOS:000721705800032,
Author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan,
   Jennifer Wortman and Wallach, Hanna and Daume, III, Hal and Crawford,
   Kate},
Title = {Documentation to facilitate communication between dataset creators and
   consumers},
Journal = {COMMUNICATIONS OF THE ACM},
Year = {2021},
Volume = {64},
Number = {12},
Pages = {86-92},
Month = {DEC},
Abstract = {DATA PLAYS A critical role in machine learning. Every machine learning
   model is trained and evaluated using data, quite often in the form of
   static datasets. The characteristics of these datasets fundamentally
   influence a model's behavior: a model is unlikely to perform well in the
   wild if its deployment context does not match its training or evaluation
   datasets, or if these datasets reflect unwanted societal biases.
   Mismatches like this can have especially severe consequences when
   machine learning models are used in high-stakes domains, such as
   criminal justice,(1,13,24) hiring,(19) critical infrastructure,(11,21)
   and finance.(18) Even in other domains, mismatches may lead to loss of
   revenue or public relations setbacks. Of particular concern are recent
   examples showing that machine learning models can reproduce or amplify
   unwanted societal biases reflected in training datasets.(4,5,12) For
   these and other reasons, the World Economic Forum suggests all entities
   should document the provenance, creation, and use of machine learning
   datasets to avoid discriminatory outcomes.(25) Although data provenance
   has been studied},
DOI = {10.1145/3458723},
ISSN = {0001-0782},
EISSN = {1557-7317},
Unique-ID = {WOS:000721705800032},
}

@article{ WOS:001269821000001,
Author = {Sampaio, Tatiana and Oliveira, Joao P. and Marinho, Daniel A. and Neiva,
   Henrique P. and Morais, Jorge E.},
Title = {Applications of Machine Learning to Optimize Tennis Performance: A
   Systematic Review},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2024},
Volume = {14},
Number = {13},
Month = {JUL},
Abstract = {(1) Background: Tennis has changed toward power-driven gameplay,
   demanding a nuanced understanding of performance factors. This review
   explores the role of machine learning in enhancing tennis performance.
   (2) Methods: A systematic search identified articles utilizing machine
   learning in tennis performance analysis. (3) Results: Machine learning
   applications show promise in psychological state monitoring, talent
   identification, match outcome prediction, spatial and tactical analysis,
   and injury prevention. Coaches can leverage wearable technologies for
   personalized psychological state monitoring, data-driven talent
   identification, and tactical insights for informed decision-making. (4)
   Conclusions: Machine learning offers coaches insights to refine coaching
   methodologies and optimize player performance in tennis. By integrating
   these insights, coaches can adapt to the demands of the sport by
   improving the players' outcomes. As technology progresses, continued
   exploration of machine learning's potential in tennis is warranted for
   further advancements in performance optimization.},
DOI = {10.3390/app14135517},
Article-Number = {5517},
EISSN = {2076-3417},
ResearcherID-Numbers = {Sampaio, Tatiana/HRA-0934-2023
   Oliveira, João/KHV-5584-2024
   Oliveira, Joao/KHV-5584-2024
   Neiva, Henrique/M-5893-2013
   Morais, J./AAX-9541-2020},
ORCID-Numbers = {, Morais, J.E./0000-0002-6885-0648
   Marinho, Daniel/0000-0003-2351-3047
   Sampaio, Tatiana/0000-0001-5694-1457
   Oliveira, Joao/0000-0001-7252-0599
   Neiva, Henrique/0000-0001-9283-312X
   },
Unique-ID = {WOS:001269821000001},
}

@article{ WOS:000685103600001,
Author = {Gibson, Peter B. and Chapman, William E. and Altinok, Alphan and Delle
   Monache, Luca and DeFlorio, Michael J. and Waliser, Duane E.},
Title = {Training machine learning models on climate model output yields skillful
   interpretable seasonal precipitation forecasts},
Journal = {COMMUNICATIONS EARTH \& ENVIRONMENT},
Year = {2021},
Volume = {2},
Number = {1},
Month = {AUG 10},
Abstract = {Seasonal forecasting skill in machine learning methods that are trained
   on large climate model ensembles can compete with, or out-compete,
   existing dynamical models, while retaining physical interpretability.
   A barrier to utilizing machine learning in seasonal forecasting
   applications is the limited sample size of observational data for model
   training. To circumvent this issue, here we explore the feasibility of
   training various machine learning approaches on a large climate model
   ensemble, providing a long training set with physically consistent model
   realizations. After training on thousands of seasons of climate model
   simulations, the machine learning models are tested for producing
   seasonal forecasts across the historical observational period
   (1980-2020). For forecasting large-scale spatial patterns of
   precipitation across the western United States, here we show that these
   machine learning-based models are capable of competing with or
   outperforming existing dynamical models from the North American Multi
   Model Ensemble. We further show that this approach need not be
   considered a `black box' by utilizing machine learning interpretability
   methods to identify the relevant physical processes that lead to
   prediction skill.},
DOI = {10.1038/s43247-021-00225-4},
Article-Number = {159},
EISSN = {2662-4435},
ResearcherID-Numbers = {Chapman, William/JFK-7425-2023},
ORCID-Numbers = {Gibson, Peter/0000-0003-2095-5165
   },
Unique-ID = {WOS:000685103600001},
}

@article{ WOS:000630189900078,
Author = {Mahima, K. T. Y. and Ginige, T. N. D. S. and De Zoysa, Kasun},
Title = {Evaluation of Sentiment Analysis based on AutoML and Traditional
   Approaches},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2021},
Volume = {12},
Number = {2},
Pages = {612-618},
Month = {FEB},
Abstract = {AutoML or Automated Machine Learning is a set of tools to reduce or
   eliminate the necessary skills of a data scientist to build machine
   learning or deep learning models. Those tools are able to automatically
   discover the machine learning models and pipelines for the given dataset
   within very low interaction of the user. This concept was derived
   because developing a machine learning or deep learning model by applying
   the traditional machine learning methods is time-consuming and sometimes
   it is challenging for experts as well. Moreover, present AutoML tools
   are used in most of the areas such as image processing and sentiment
   analysis. In this research, the authors evaluate the implementation of a
   sentiment analysis classification model based on AutoML and Traditional
   approaches. For the evaluation, this research used both deep learning
   and machine learning approaches. To implement the sentiment analysis
   models HyperOpt SkLearn, TPot as AutoML libraries and, as the
   traditional method, Scikit learn libraries were used. Moreover for
   implementing the deep learning models Keras and Auto-Keras libraries
   used. In the implementation process, to build two binary classification
   and two multi-class classification models using the above- mentioned
   libraries. Thereafter evaluate the findings by each AutoML and
   Traditional approach. In this research, the authors were able to
   identify that building a machine learning or a deep learning model
   manually is better than using an AutoML approach.},
ISSN = {2158-107X},
EISSN = {2156-5570},
ResearcherID-Numbers = {Mahima, Yasas/IAO-7186-2023
   },
ORCID-Numbers = {Mahima, K.T.Yasas/0000-0003-4975-9408
   De Zoysa, Kasun/0000-0001-7199-6034
   Ginige, Thepul/0000-0003-1644-5701},
Unique-ID = {WOS:000630189900078},
}

@article{ WOS:000710922600002,
Author = {Breeden, Joseph L.},
Title = {A survey of machine learning in credit risk},
Journal = {JOURNAL OF CREDIT RISK},
Year = {2021},
Volume = {17},
Number = {3},
Pages = {1-62},
Month = {SEP},
Abstract = {Machine learning algorithms have come to dominate several industries.
   After decades of resistance from examiners and auditors, machine
   learning is now moving from the research desk to the application stack
   for credit scoring and a range of other applications in credit risk.
   This migration is not without novel risks and challenges. Much of the
   research is now shifting from how best to make the models to how best to
   use the models in a regulator-compliant business context. This paper
   surveys the impressively broad range of machine learning methods and
   application areas for credit risk. In the process of that survey, we
   create a taxonomy to think about how different machine learning
   components are matched to create specific algorithms. The reasons for
   where machine learning succeeds over simple linear methods are explored
   through a specific lending example. Throughout, we highlight open
   questions, ideas for improvements and a framework for thinking about how
   to choose the best machine learning method for a specific problem.},
DOI = {10.21314/JCR.2021.008},
ISSN = {1744-6619},
EISSN = {1755-9723},
Unique-ID = {WOS:000710922600002},
}

@article{ WOS:000899437900001,
Author = {Wang, Shuyu and Sun, Zhaojia},
Title = {Hydrogel and Machine Learning for Soft Robots' Sensing and Signal
   Processing: A Review},
Journal = {JOURNAL OF BIONIC ENGINEERING},
Year = {2023},
Volume = {20},
Number = {3},
Pages = {845-857},
Month = {MAY},
Abstract = {The soft robotics field is on the rise. The highly adaptive robots
   provide the opportunity to bridge the gap between machines and people.
   However, their elastomeric nature poses significant challenges to the
   perception, control, and signal processing. Hydrogels and machine
   learning provide promising solutions to the problems above. This review
   aims to summarize this recent trend by first assessing the current
   hydrogel-based sensing and actuation methods applied to soft robots. We
   outlined the mechanisms of perception in response to various external
   stimuli. Next, recent achievements of machine learning for soft robots'
   sensing data processing and optimization are evaluated. Here we list the
   strategies for implementing machine learning models from the perspective
   of applications. Last, we discuss the challenges and future
   opportunities in perception data processing and soft robots' high level
   tasks.},
DOI = {10.1007/s42235-022-00320-y},
EarlyAccessDate = {DEC 2022},
ISSN = {1672-6529},
EISSN = {2543-2141},
ResearcherID-Numbers = {Wang, Shuyu/AAJ-7593-2021},
ORCID-Numbers = {Wang, Shuyu/0000-0002-0038-1347
   },
Unique-ID = {WOS:000899437900001},
}

@article{ WOS:000331851700015,
Author = {Gu, Yang and Liu, Junfa and Chen, Yiqiang and Jiang, Xinlong and Yu,
   Hanchao},
Title = {TOSELM: Timeliness Online Sequential Extreme Learning Machine},
Journal = {NEUROCOMPUTING},
Year = {2014},
Volume = {128},
Pages = {119-127},
Month = {MAR 27},
Note = {International Workshop of Extreme Learning Machines (ELM), Singapore,
   SINGAPORE, DEC 11-13, 2012},
Abstract = {For handling data and training model, existing machine learning methods
   do not take timeliness problem into consideration. Timeliness here means
   the data distribution or the data trend changes with time passing by.
   Based on timeliness management scheme, a novel machine learning
   algorithm Timeliness Online Sequential Extreme Learning Machine (TOSELM)
   is proposed, which improves Online Sequential Extreme Learning Machine
   (OSELM) with central tendency and dispersion characteristics of data to
   deal with timeliness problem. The performance of proposed algorithm has
   been validated on several simulated and realistic datasets, and
   experimental results show that TOSELM utilizing adaptive weight scheme
   and iteration scheme can achieve higher learning accuracy, faster
   convergence and better stability than other machine learning methods.
   (C) 2013 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.neucom.2013.02.047},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Liu, Junfa/A-9093-2012
   Yu, Han/R-3297-2017
   gu, yang/NEU-2669-2025},
ORCID-Numbers = {Chen, Yiqiang/0000-0002-8407-0780
   },
Unique-ID = {WOS:000331851700015},
}

@article{ WOS:000352350000001,
Author = {Li, Zhaokai and Liu, Xiaomei and Xu, Nanyang and Du, Jiangfeng},
Title = {Experimental Realization of a Quantum Support Vector Machine},
Journal = {PHYSICAL REVIEW LETTERS},
Year = {2015},
Volume = {114},
Number = {14},
Month = {APR 8},
Abstract = {The fundamental principle of artificial intelligence is the ability of
   machines to learn from previous experience and do future work
   accordingly. In the age of big data, classical learning machines often
   require huge computational resources in many practical cases. Quantum
   machine learning algorithms, on the other hand, could be exponentially
   faster than their classical counterparts by utilizing quantum
   parallelism. Here, we demonstrate a quantum machine learning algorithm
   to implement handwriting recognition on a four-qubit NMR test bench. The
   quantum machine learns standard character fonts and then recognizes
   handwritten characters from a set with two candidates. Because of the
   wide spread importance of artificial intelligence and its tremendous
   consumption of computational resources, quantum speedup would be
   extremely attractive against the challenges of big data.},
DOI = {10.1103/PhysRevLett.114.140504},
Article-Number = {140504},
ISSN = {0031-9007},
EISSN = {1079-7114},
Unique-ID = {WOS:000352350000001},
}

@article{ WOS:000494359400009,
Author = {Fang, He and Wang, Xianbin and Tomasin, Stefano},
Title = {Machine Learning for Intelligent Authentication in 5G and Beyond
   Wireless Networks},
Journal = {IEEE WIRELESS COMMUNICATIONS},
Year = {2019},
Volume = {26},
Number = {5},
Pages = {55-61},
Month = {OCT},
Abstract = {The 5G and beyond wireless networks are critical to support diverse
   vertical applications by connecting heterogeneous devices and machines,
   which directly increase vulnerability for various spoofing attacks.
   Conventional cryptographic and physical layer authentication techniques
   are facing some challenges in complex dynamic wireless environments,
   including significant security overhead, low reliability, as well as
   difficulties in pre-designing a precise authentication model, providing
   continuous protection, and learning time-varying attributes. In this
   article, we envision new authentication approaches based on machine
   learning techniques by opportunistically leveraging physical layer
   attributes, and introduce intelligence to authentication for more
   efficient security provisioning. Machine learning paradigms for
   intelligent authentication design are presented, namely for
   parametric/non-parametric and supervised/ unsupervised/reinforcement
   learning algorithms. In a nutshell, the machine-learning-based
   intelligent authentication approaches utilize specific features in the
   multi-dimensional domain for achieving cost-effective, more reliable,
   model-free, continuous, and situation-aware device validation under
   unknown network conditions and unpredictable dynamics.},
DOI = {10.1109/MWC.001.1900054},
ISSN = {1536-1284},
EISSN = {1558-0687},
ResearcherID-Numbers = {Tomasin, Stefano/AAF-6122-2020
   Wang, Xianbin/AAY-3303-2020
   },
ORCID-Numbers = {Wang, Xianbin/0000-0003-4890-0748
   Tomasin, Stefano/0000-0003-3253-6793},
Unique-ID = {WOS:000494359400009},
}

@article{ WOS:000525375800050,
Author = {Apsemidis, Anastasios and Psarakis, Stelios and Moguerza, Javier M.},
Title = {A review of machine learning kernel methods in statistical process
   monitoring},
Journal = {COMPUTERS \& INDUSTRIAL ENGINEERING},
Year = {2020},
Volume = {142},
Month = {APR},
Abstract = {The complexity of modern problems turns increasingly larger in
   industrial environments, so the classical process monitoring techniques
   have to adapt to deal with those problems. This is one of the reasons
   why new Machine and Statistical Learning methodologies have become very
   popular in the statistical community. Specifically, this article is
   focused on machine learning kernel methods techniques in the process
   monitoring field. After explaining the idea of kernel methods we
   thoroughly examine the process monitoring articles that make use of
   kernel models and the way in which these models are combined with other
   Machine Learning approaches. Finally, we summarize the whole picture of
   the literature and mention some remarkable points.},
DOI = {10.1016/j.cie.2020.106376},
Article-Number = {106376},
ISSN = {0360-8352},
EISSN = {1879-0550},
ResearcherID-Numbers = {Psarakis, Stelios/AAJ-3023-2021
   Moguerza, Javier/AAA-6836-2020},
ORCID-Numbers = {M. Moguerza, Javier/0000-0003-1415-1961
   Apsemidis, Anastasios/0000-0002-5303-3231
   },
Unique-ID = {WOS:000525375800050},
}

@article{ WOS:000732559000001,
Author = {Sharma, Mayuri and Kumar, Chandan Jyoti and Deka, Aniruddha},
Title = {Early diagnosis of rice plant disease using machine learning techniques},
Journal = {ARCHIVES OF PHYTOPATHOLOGY AND PLANT PROTECTION},
Year = {2022},
Volume = {55},
Number = {3},
Pages = {259-283},
Month = {FEB 3},
Abstract = {There is an incredible progress in machine learning applications in the
   field of agricultural research. Detection of various diseases,
   deficiencies, and factors impacting crops' productivity is one of the
   major ongoing research in this field. This paper considers various
   machine learning and deep learning techniques (transfer learning) for
   rice disease detection. In this study three different rice diseases viz.
   bacterial blight, rice blast, and brown spot are considered. A detailed
   comparative analysis of the results indicates the superiority of
   transfer learning techniques over conventional machine learning
   techniques. It is observed that InceptionResNetV2 achieves the best
   result followed by XceptionNet. This work can be incorporated in
   assisting the farmers for early diagnosis of rice disease so that future
   course of action may be taken on time. For future studies, efforts
   should be directed to work with bigger datasets so as to generalize the
   findings of the experiment.},
DOI = {10.1080/03235408.2021.2015866},
EarlyAccessDate = {DEC 2021},
ISSN = {0323-5408},
EISSN = {1477-2906},
ResearcherID-Numbers = {Sharma, Mayuri/IUQ-5658-2023
   Deka, Aniruddha/GJA-2203-2022
   },
ORCID-Numbers = {Deka, Aniruddha/0000-0002-1228-232X
   Sharma, Mayuri/0000-0003-3282-2765},
Unique-ID = {WOS:000732559000001},
}

@article{ WOS:000745560100002,
Author = {Yang, Fan and Zhang, Qi and Ji, Xiaokang and Zhang, Yanchun and Li,
   Wentao and Peng, Shaoliang and Xue, Fuzhong},
Title = {Machine Learning Applications in Drug Repurposing},
Journal = {INTERDISCIPLINARY SCIENCES-COMPUTATIONAL LIFE SCIENCES},
Year = {2022},
Volume = {14},
Number = {1},
Pages = {15-21},
Month = {MAR},
Abstract = {The coronavirus disease (COVID-19) has led to an rush to repurpose
   existing drugs, although the underlying evidence base is of variable
   quality. Drug repurposing is a technique by taking advantage of existing
   known drugs or drug combinations to be explored in an unexpected medical
   scenario. Drug repurposing, hence, plays a vital role in accelerating
   the pre-clinical process of designing novel drugs by saving time and
   cost compared to the traditional de novo drug discovery processes. Since
   drug repurposing depends on massive observed data from existing drugs
   and diseases, the tremendous growth of publicly available large-scale
   machine learning methods supplies the state-of-the-art application of
   data science to signaling disease, medicine, therapeutics, and
   identifying targets with the least error. In this article, we introduce
   guidelines on strategies and options of utilizing machine learning
   approaches for accelerating drug repurposing. We discuss how to employ
   machine learning methods in studying precision medicine, and as an
   instance, how machine learning approaches can accelerate COVID-19 drug
   repurposing by developing Chinese traditional medicine therapy. This
   article provides a strong reasonableness for employing machine learning
   methods for drug repurposing, including during fighting for COVID-19
   pandemic.},
DOI = {10.1007/s12539-021-00487-8},
EarlyAccessDate = {JAN 2022},
ISSN = {1913-2751},
EISSN = {1867-1462},
ResearcherID-Numbers = {Peng, Shaoliang/JFS-1314-2023
   Xue, Fuzhong/AAG-4808-2020
   Xu, Rui/GWM-7132-2022},
ORCID-Numbers = {Peng, Shaoliang/0000-0002-4647-2615
   },
Unique-ID = {WOS:000745560100002},
}

@article{ WOS:001162350100001,
Author = {Zhou, Kai and Wang, Lingxiao and Pang, Long -Gang and Shi, Shuzhe},
Title = {Exploring QCD matter in extreme conditions with Machine Learning},
Journal = {PROGRESS IN PARTICLE AND NUCLEAR PHYSICS},
Year = {2024},
Volume = {135},
Month = {FEB},
Abstract = {In recent years, machine learning has emerged as a powerful
   computational tool and novel problem -solving perspective for physics,
   offering new avenues for studying strongly interacting QCD matter
   properties under extreme conditions. This review article aims to provide
   an overview of the current state of this intersection of fields,
   focusing on the application of machine learning to theoretical studies
   in high energy nuclear physics. It covers diverse aspects, including
   heavy ion collisions, lattice field theory, and neutron stars, and
   discuss how machine learning can be used to explore and facilitate the
   physics goals of understanding QCD matter. The review also provides a
   commonality overview from a methodology perspective, from data -driven
   perspective to physics -driven perspective. We conclude by discussing
   the challenges and future prospects of machine learning applications in
   high energy nuclear physics, also underscoring the importance of
   incorporating physics priors into the purely data -driven learning
   toolbox. This review highlights the critical role of machine learning as
   a valuable computational paradigm for advancing physics exploration in
   high energy nuclear physics.},
DOI = {10.1016/j.ppnp.2023.104084},
EarlyAccessDate = {JAN 2024},
Article-Number = {104084},
ISSN = {0146-6410},
EISSN = {1873-2224},
ResearcherID-Numbers = {Zhou, Kai/AGC-9226-2022
   Wang, Lingxiao/ABA-2907-2021
   },
ORCID-Numbers = {Wang, Lingxiao/0000-0003-3757-3403
   Zhou, Kai/0000-0001-9859-1758},
Unique-ID = {WOS:001162350100001},
}

@article{ WOS:000478732400005,
Author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and Garcia,
   Raul Blazquez and Su, Gang and Lewenstein, Maciej},
Title = {Machine learning by unitary tensor network of hierarchical tree
   structure},
Journal = {NEW JOURNAL OF PHYSICS},
Year = {2019},
Volume = {21},
Month = {JUL 30},
Abstract = {The resemblance between the methods used in quantum-many body physics
   and in machine learning has drawn considerable attention. In particular,
   tensor networks (TNs) and deep learning architectures bear striking
   similarities to the extent that TNs can be used for machine learning.
   Previous results used one-dimensional TNs in image recognition, showing
   limited scalability and flexibilities. In this work, we train
   two-dimensional hierarchical TNs to solve image recognition problems,
   using a training algorithm derived from the multi-scale entanglement
   renormalization ansatz. This approach introduces mathematical
   connections among quantum many-body physics, quantum information theory,
   and machine learning. While keeping the TN unitary in the training
   phase, TN states are defined, which encode classes of images into
   quantum many-body states. We study the quantum features of the TN
   states, including quantum entanglement and fidelity. We find these
   quantities could be properties that characterize the image classes, as
   well as the machine learning tasks.},
DOI = {10.1088/1367-2630/ab31ef},
Article-Number = {073059},
ISSN = {1367-2630},
ResearcherID-Numbers = {Liu, Ding/AAM-5249-2020
   Ran, Shi-Ju/H-9144-2018
   Ran, Shi-Ju/AAB-6643-2019
   Su, Gang/ABI-1851-2020
   Lewenstein, Maciej/I-1337-2014
   Peng, Cheng/R-4428-2018},
ORCID-Numbers = {Ran, Shi-Ju/0000-0003-1844-7268
   Peng, Cheng/0000-0002-9267-1789},
Unique-ID = {WOS:000478732400005},
}

@article{ WOS:000840062800002,
Author = {Villarroya, Sebastian and Baumann, Peter},
Title = {A survey on machine learning in array databases},
Journal = {APPLIED INTELLIGENCE},
Year = {2023},
Volume = {53},
Number = {9},
Pages = {9799-9822},
Month = {MAY},
Abstract = {This paper provides an in-depth survey on the integration of machine
   learning and array databases. First,machine learning support in modern
   database management systems is introduced. From straightforward
   implementations of linear algebra operations in SQL to machine learning
   capabilities of specialized database managers designed to process
   specific types of data, a number of different approaches are overviewed.
   Then, the paper covers the database features already implemented in
   current machine learning systems. Features such as rewriting,
   compression, and caching allow users to implement more efficient machine
   learning applications. The underlying linear algebra computations in
   some of the most used machine learning algorithms are studied in order
   to determine which linear algebra operations should be efficiently
   implemented by array databases. An exhaustive overview of array data and
   relevant array database managers is also provided. Those database
   features that have been proven of special importance for efficient
   execution of machine learning algorithms are analyzed in detail for each
   relevant array database management system. Finally, current state of
   array databases capabilities for machine learning implementation is
   shown through two example implementations in Rasdaman and SciDB.},
DOI = {10.1007/s10489-022-03979-2},
EarlyAccessDate = {AUG 2022},
ISSN = {0924-669X},
EISSN = {1573-7497},
ResearcherID-Numbers = {Villarroya, Sebastian/B-3699-2017
   Baumann, Peter/ABH-1702-2020
   Villarroya, Sebastián/B-3699-2017},
ORCID-Numbers = {Villarroya, Sebastian/0000-0003-0555-8735
   },
Unique-ID = {WOS:000840062800002},
}

@article{ WOS:000444245700003,
Author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi,
   Jaspreet S. and Ronaghe, Pooya},
Title = {REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES},
Journal = {QUANTUM INFORMATION \& COMPUTATION},
Year = {2018},
Volume = {18},
Number = {1-2},
Pages = {51-74},
Month = {FEB},
Abstract = {We investigate whether quantum annealers with select chip layouts can
   outperform classical computers in reinforcement learning tasks. We
   associate a transverse field Ising spin Hamiltonian with a layout of
   qubits similar to that of a deep Boltzmann machine (DBM) and use
   simulated quantum annealing (SQA) to numerically simulate quantum
   sampling from this system. We design a reinforcement learning algorithm
   in which the set of visible nodes representing the states and actions of
   an optimal policy are the first and last layers of the deep network. In
   absence of a transverse field, our simulations show that DBMs are
   trained more effectively than restricted Boltzmann machines (RBM) with
   the same number of nodes. We then develop a framework for training the
   network as a quantum Boltzmann machine (QBM) in the presence of a
   significant transverse field for reinforcement learning. This method
   also outperforms the reinforcement learning method that uses RBMs.},
ISSN = {1533-7146},
Unique-ID = {WOS:000444245700003},
}

@article{ WOS:000562309400001,
Author = {Ma, Yue and Zhao, Kun and Wang, Qi and Tian, Yingjie},
Title = {Incremental Cost-Sensitive Support Vector Machine With
   Linear-Exponential Loss},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {149899-149914},
Abstract = {Incremental learning or online learning as a branch of machine learning
   has attracted more attention recently. For large-scale problems and
   dynamic data problem, incremental learning overwhelms batch learning,
   because of its efficient treatment for new data. However, class
   imbalance problem, which always appears in online classification brings
   a considerable challenge for incremental learning. The serious class
   imbalance problem may directly lead to a useless learning system.
   Cost-sensitive learning is an important learning paradigm for class
   imbalance problems and widely used in many applications. In this
   article, we propose an incremental cost-sensitive learning method to
   tackle the class imbalance problems in the online situation. This
   proposed algorithm is based on a novel cost-sensitive support vector
   machine, which uses the Linear-exponential (LINEX) loss to implement
   high cost for minority class and low cost for majority class. Using the
   half-quadratic optimization, we first put forward the algorithm for the
   cost-sensitive support vector machine, called CSLINEX-SVM{*}. Then we
   propose the incremental cost-sensitive algorithm, ICSL-SVM. The results
   of numeric experiments demonstrate that the proposed incremental
   algorithm outperforms some conventional batch algorithms except the
   proposed CSLINEX-SVM{*}.},
DOI = {10.1109/ACCESS.2020.3015954},
ISSN = {2169-3536},
ResearcherID-Numbers = {Tian, Yingjie/HJO-9048-2023
   Wang, Wei/C-4364-2019},
ORCID-Numbers = {Tian, Yingjie/0000-0002-4675-0398
   },
Unique-ID = {WOS:000562309400001},
}

@article{ WOS:001129260900004,
Author = {Kolachalama, Vijaya B.},
Title = {Machine learning and pre-medical education},
Journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE},
Year = {2022},
Volume = {129},
Month = {JUL},
Abstract = {Machine learning and artificial intelligence (AI)-driven technologies
   are contributing significantly to various facets of medicine and care
   management. It is likely that the next generation of healthcare
   professionals will be confronted with a series of innovations that are
   powered by AI, and they may not have sufficient time during their
   professional tenure to learn about the underlying machine learning
   frameworks that are driving these systems. Educating the aspiring
   clinicians and care providers with the right foundational courses in
   machine learning as part of postsecondary education will likely
   transform them as high-tech physicians and care providers of the future.},
DOI = {10.1016/j.artmed.2022.102313},
Article-Number = {102313},
ISSN = {0933-3657},
EISSN = {1873-2860},
ResearcherID-Numbers = {Kolachalama, Vijaya/AAH-3528-2020},
ORCID-Numbers = {Kolachalama, Vijaya/0000-0002-5312-8644
   },
Unique-ID = {WOS:001129260900004},
}

@article{ WOS:000486611500019,
Author = {Meek, Ryan D. and Lungren, Matthew P. and Gichoya, Judy W.},
Title = {Machine Learning for the Interventional Radiologist},
Journal = {AMERICAN JOURNAL OF ROENTGENOLOGY},
Year = {2019},
Volume = {213},
Number = {4},
Pages = {782-784},
Month = {OCT},
Abstract = {OBJECTIVE. The purpose of this article is to describe key potential
   areas of application of machine learning in interventional radiology.
   CONCLUSION. Machine learning, although in the early stages of
   development within the field of interventional radiology, has great
   potential to influence key areas such as image analysis, clinical
   predictive modeling, and trainee education. A proactive approach from
   current interventional radiologists and trainees is needed to shape
   future directions for machine learning and artificial intelligence.},
DOI = {10.2214/AJR.19.21527},
ISSN = {0361-803X},
EISSN = {1546-3141},
ResearcherID-Numbers = {Gichoya, Judy/E-1657-2011},
Unique-ID = {WOS:000486611500019},
}

@article{ WOS:000464239400001,
Author = {Jennings, Paul C. and Lysgaard, Steen and Hummelshoj, Jens Strabo and
   Vegge, Tejs and Bligaard, Thomas},
Title = {Genetic algorithms for computational materials discovery accelerated by
   machine learning},
Journal = {NPJ COMPUTATIONAL MATERIALS},
Year = {2019},
Volume = {5},
Month = {APR 10},
Abstract = {Materials discovery is increasingly being impelled by machine learning
   methods that rely on pre-existing datasets. Where datasets are lacking,
   unbiased data generation can be achieved with genetic algorithms. Here a
   machine learning model is trained on-the-fly as a computationally
   inexpensive energy predictor before analyzing how to augment convergence
   in genetic algorithm-based approaches by using the model as a surrogate.
   This leads to a machine learning accelerated genetic algorithm combining
   robust qualities of the genetic algorithm with rapid machine learning.
   The approach is used to search for stable, compositionally variant,
   geometrically similar nanoparticle alloys to illustrate its capability
   for accelerated materials discovery, e.g., nanoalloy catalysts. The
   machine learning accelerated approach, in this case, yields a 50-fold
   reduction in the number of required energy calculations compared to a
   traditional ``brute force{''} genetic algorithm. This makes searching
   through the space of all homotops and compositions of a binary alloy
   particle in a given structure feasible, using density functional theory
   calculations.},
DOI = {10.1038/s41524-019-0181-4},
Article-Number = {46},
ISSN = {2057-3960},
ResearcherID-Numbers = {Jennings, Paul/L-6429-2013
   Bligaard, Thomas/A-6161-2011
   Vegge, Tejs/A-9419-2011
   },
ORCID-Numbers = {Bligaard, Thomas/0000-0003-0386-0201
   Lysgaard, Steen/0000-0002-2032-8949},
Unique-ID = {WOS:000464239400001},
}

@article{ WOS:000455128100005,
Author = {Crews, Christian},
Title = {What Machine Learning Can Learn from Foresight: A Human-Centered
   Approach For machine learning-based forecast efforts to succeed, they
   must embrace lessons from corporate foresight to address human and
   organizational challenges.},
Journal = {RESEARCH-TECHNOLOGY MANAGEMENT},
Year = {2019},
Volume = {62},
Number = {1},
Pages = {30-33},
Month = {JAN 2},
Abstract = {Overview: Machine learning applications in business that return
   forecasts or predictions of future market or consumer behavior must pay
   attention to nontechnical aspects of how those forecasts are created and
   used by leaders. Machine learning projects can generate better forecasts
   that have greater effect by embracing key methods developed through
   almost 50 years of corporate foresight practice to improve the adoption
   and use of forecasts in organizations.},
DOI = {10.1080/08956308.2019.1541725},
ISSN = {0895-6308},
EISSN = {1930-0166},
Unique-ID = {WOS:000455128100005},
}

@article{ WOS:000452544100061,
Author = {Dong, Xin Luna and Rekatsinas, Theodoros},
Title = {Data Integration and Machine Learning: A Natural Synergy},
Journal = {PROCEEDINGS OF THE VLDB ENDOWMENT},
Year = {2018},
Volume = {11},
Number = {12},
Pages = {2094-2097},
Month = {AUG},
Abstract = {As data volume and variety have increased, so have the ties between
   machine learning and data integration become stronger. For machine
   learning to be effective, one must utilize data from the greatest
   possible variety of sources; and this is why data integration plays a
   key role. At the same time machine learning is driving automation in
   data integration, resulting in overall reduction of integration costs
   and improved accuracy. This tutorial focuses on three aspects of the
   synergistic relationship between data integration and machine learning:
   (1) we survey how state-of-the-art data integration solutions rely on
   machine learning-based approaches for accurate results and effective
   human-in-the-loop pipelines, (2) we review how end-to-end machine
   learning applications rely on data integration to identify accurate,
   clean, and relevant data for their analytics exercises, and (3) we
   discuss open research challenges and opportunities that span across data
   integration and machine learning.},
DOI = {10.14778/3229863.3229876},
ISSN = {2150-8097},
ORCID-Numbers = {Rekatsinas, Theodoros/0000-0001-6148-1854},
Unique-ID = {WOS:000452544100061},
}

@article{ WOS:001102779100001,
Author = {Zhang, Shen and Wallscheid, Oliver and Porrmann, Mario},
Title = {Machine Learning for the Control and Monitoring of Electric Machine
   Drives: Advances and Trends},
Journal = {IEEE OPEN JOURNAL OF INDUSTRY APPLICATIONS},
Year = {2023},
Volume = {4},
Pages = {188-214},
Abstract = {This review article systematically summarizes the existing literature on
   utilizing machine learning (ML) techniques for the control and
   monitoring of electric machine drives. It is anticipated that with the
   rapid progress in learning algorithms and specialized embedded hardware
   platforms, ML-based data-driven approaches will become standard tools
   for the automated high-performance control and monitoring of electric
   drives. In addition, this article also provides some outlook toward
   promoting its widespread application in the industry with a focus on
   deploying ML algorithms onto embedded system-on-chip field-programmable
   gate array devices.},
DOI = {10.1109/OJIA.2023.3284717},
EISSN = {2644-1241},
ResearcherID-Numbers = {Wallscheid, Oliver/T-5026-2019
   Porrmann, Mario/T-3264-2019
   },
ORCID-Numbers = {Wallscheid, Oliver/0000-0001-9362-8777
   Zhang, Shen/0000-0002-1245-0565
   Porrmann, Mario/0000-0003-1005-5753},
Unique-ID = {WOS:001102779100001},
}

@article{ WOS:000463601900005,
Author = {Kwon, Jae Yung and Karim, Mohammad Ehsanul and Topaz, Maxim and Currie,
   Leanne M.},
Title = {Nurses ``Seeing Forest for the Trees{''} in the Age of Machine Learning
   Using Nursing Knowledge to Improve Relevance and Performance},
Journal = {CIN-COMPUTERS INFORMATICS NURSING},
Year = {2019},
Volume = {37},
Number = {4},
Pages = {203-212},
Month = {APR},
Abstract = {Although machine learning is increasingly being applied to support
   clinical decision making, there is a significant gap in understanding
   what it is and how nurses should adopt it in practice. The purpose of
   this case study is to show how one application of machine learning may
   support nursing work and to discuss how nurses can contribute to
   improving its relevance and performance. Using data from 130 specialized
   hospitals with 101 766 patients with diabetes, we applied various
   advanced statistical methods (known as machine learning algorithms) to
   predict early readmission. The best-performing machine learning
   algorithm showed modest predictive ability with opportunities for
   improvement. Nurses can contribute to machine learning algorithms by (1)
   filling data gaps with nursing-relevant data that provide personalized
   context about the patient, (2) improving data preprocessing techniques,
   and (3) evaluating potential value in practice. These findings suggest
   that nurses need to further process the information provided by machine
   learning and apply ``Wisdom-in-Action{''} to make appropriate clinical
   decisions. Nurses play a pivotal role in ensuring that machine learning
   algorithms are shaped by their unique knowledge of each patient's
   personalized context. By combining machine learning with unique nursing
   knowledge, nurses can provide more visibility to nursing work, advance
   nursing science, and better individualize patient care. Therefore, to
   successfully integrate and maximize the benefits of machine learning,
   nurses must fully participate in its development, implementation, and
   evaluation.},
DOI = {10.1097/CIN.0000000000000508},
ISSN = {1538-2931},
EISSN = {1538-9774},
ResearcherID-Numbers = {Karim, Mohammad Ehsanul/I-1803-2018
   Karim, Ehsan/I-1803-2018
   Topaz, Maxim/AAQ-7121-2021},
ORCID-Numbers = {Karim, Mohammad Ehsanul/0000-0002-0346-2871
   },
Unique-ID = {WOS:000463601900005},
}

@article{ WOS:001232580600001,
Author = {Baako, Tiffany-Marie D. and Kulkarni, Sahil Kaushik and McClendon,
   Jerome L. and Harcum, Sarah W. and Gilmore, Jordon},
Title = {Machine Learning and Deep Learning Strategies for Chinese Hamster Ovary
   Cell Bioprocess Optimization},
Journal = {FERMENTATION-BASEL},
Year = {2024},
Volume = {10},
Number = {5},
Month = {MAY},
Abstract = {The use of machine learning and deep learning has become prominent
   within various fields of bioprocessing for countless modeling and
   prediction tasks. Previous reviews have emphasized machine learning
   applications in various fields of bioprocessing, including
   biomanufacturing. This comprehensive review highlights many of the
   different machine learning and multivariate analysis techniques that
   have been utilized within Chinese hamster ovary cell biomanufacturing,
   specifically due to their rising significance in the industry.
   Applications of machine and deep learning within other bioprocessing
   industries are also briefly discussed.},
DOI = {10.3390/fermentation10050234},
Article-Number = {234},
EISSN = {2311-5637},
ResearcherID-Numbers = {Harcum, Sarah/AAG-5431-2021},
ORCID-Numbers = {Baako, Tiffany-Marie D./0009-0006-8200-5631
   },
Unique-ID = {WOS:001232580600001},
}

@article{ WOS:001380875400005,
Author = {Nguyen, Giang and Sainz-Pardo Diaz, Judith and Calatrava, Amanda and
   Berberi, Lisana and Lytvyn, Oleksandr and Kozlov, Valentin and Tran,
   Viet and Molto, German and Lopez Garcia, Alvaro},
Title = {Landscape of machine learning evolution: privacy-preserving federated
   learning frameworks and tools},
Journal = {ARTIFICIAL INTELLIGENCE REVIEW},
Year = {2024},
Volume = {58},
Number = {2},
Month = {DEC 20},
Abstract = {Machine learning is one of the most widely used technologies in the
   field of Artificial Intelligence. As machine learning applications
   become increasingly ubiquitous, concerns about data privacy and security
   have also grown. The work in this paper presents a broad theoretical
   landscape concerning the evolution of machine learning and deep learning
   from centralized to distributed learning, first in relation to
   privacy-preserving machine learning and secondly in the area of
   privacy-enhancing technologies. It provides a comprehensive landscape of
   the synergy between distributed machine learning and privacy-enhancing
   technologies, with federated learning being one of the most prominent
   architectures. Various distributed learning approaches to privacy-aware
   techniques are structured in a review, followed by an in-depth
   description of relevant frameworks and libraries, more particularly in
   the context of federated learning. The paper also highlights the need
   for data protection and privacy addressed from different approaches, key
   findings in the field concerning AI applications, and advances in the
   development of related tools and techniques.},
DOI = {10.1007/s10462-024-11036-2},
Article-Number = {51},
ISSN = {0269-2821},
EISSN = {1573-7462},
ResearcherID-Numbers = {Nguyen, Giang/S-3291-2016
   Sáinz-Pardo Díaz, Judith/JCE-0368-2023
   Moltó, Germán/C-6994-2008
   García, Álvaro/G-4796-2016
   Calatrava, Amanda/MGV-8288-2025
   Kozlov, Valentin/B-8884-2008},
ORCID-Numbers = {NGUYEN, Giang/0000-0002-6769-0195
   Lytvyn, Oleksandr/0000-0002-1028-2254
   Sainz-Pardo Diaz, Judith/0000-0002-8387-578X
   Kozlov, Valentin/0000-0002-8770-3619},
Unique-ID = {WOS:001380875400005},
}

@article{ WOS:000618359200001,
Author = {Bamisile, Olusola and Oluwasanmi, Ariyo and Ejiyi, Chukwuebuka and
   Yimen, Nasser and Obiora, Sandra and Huang, Qi},
Title = {Comparison of machine learning and deep learning algorithms for hourly
   global/diffuse solar radiation predictions},
Journal = {INTERNATIONAL JOURNAL OF ENERGY RESEARCH},
Year = {2022},
Volume = {46},
Number = {8, SI},
Pages = {10052-10073},
Month = {JUN 25},
Abstract = {Due to the advancement and wide adoption/application of solar-based
   technologies, the prediction of solar irradiance has attracted research
   attention in recent years. In this study, the predictive performance of
   machine learning models is compared with that of deep learning models
   for both global solar radiation (GSR) and diffuse solar radiation (DSR)
   prediction. Different studies have proposed the use of different models
   for solar radiation prediction. While some used machine learning models,
   the use of deep learning algorithms were considered by others. Although
   these algorithms were concluded to be appropriate for solar radiation
   prediction, variation in their performances brings about an intriguing
   quest to compare and determine the most appropriate algorithm. The three
   most common deep learning models in the literature namely; artificial
   neural network, convolutional neural network, and recurrent neural
   network (RNN) are considered within the scope of this study. Also, two
   traditional machine learning models namely polynomial regression and
   support vector regression (SVR) is considered as well as an ensemble
   machine learning model called random forest. These models have been
   applied to four different locations in Nigeria and the typical
   meteorological year data for 12 years in an hourly time step was used to
   train/test the model developed. Results from this study show that deep
   learning models have a better GSR and DSR prediction accuracy in
   comparison to machine learning models. However, the duration for
   training and testing the machine learning models (except SVR) is shorter
   than that of deep learning models making it more desirable for low
   computational applications. The application of RNN for GSR prediction in
   Yobe (with an r value of 0.9546 and root means square error/mean
   absolute error of 82.22 W/m(2)/36.52 W/m(2)) had the overall best model
   performance of all the models developed in this study. This study
   contributes to the existing literature in this field as it highlights
   the disparities between machine learning and deep learning algorithms
   application for solar radiation forecast.},
DOI = {10.1002/er.6529},
EarlyAccessDate = {FEB 2021},
ISSN = {0363-907X},
EISSN = {1099-114X},
ResearcherID-Numbers = {Ejiyi, Chukwuebuka/HNI-0950-2023
   Oluwasanmi, Ariyo/ABD-9254-2021
   Yimen, Nasser/AFL-5968-2022
   Bamisile, Olusola/JBR-9088-2023
   },
ORCID-Numbers = {Ejiyi, Chukwuebuka/0000-0001-9139-7223
   Yimen, Nasser/0000-0002-4630-3424
   Bamisile, Olusola/0000-0002-5154-6404
   Oluwasanmi, Ariyo/0000-0001-9853-9554},
Unique-ID = {WOS:000618359200001},
}

@article{ WOS:000494273000002,
Author = {Fiebrink, Rebecca},
Title = {Machine Learning Education for Artists, Musicians, and Other Creative
   Practitioners},
Journal = {ACM TRANSACTIONS ON COMPUTING EDUCATION},
Year = {2019},
Volume = {19},
Number = {4},
Month = {NOV},
Abstract = {This article aims to lay a foundation for the research and practice of
   machine learning education for creative practitioners. It begins by
   arguing that it is important to teach machine learning to creative
   practitioners and to conduct research about this teaching, drawing on
   related work in creative machine learning, creative computing education,
   and machine learning education. It then draws on research about design
   processes in engineering and creative practice to motivate a set of
   learning objectives for students who wish to design new creative
   artifacts with machine learning. The article then draws on education
   research and knowledge of creative computing practices to propose a set
   of teaching strategies that can be used to support creative computing
   students in achieving these objectives. Explanations of these strategies
   are accompanied by concrete descriptions of how they have been employed
   to develop new lectures and activities, and to design new experiential
   learning and scaffolding technologies, for teaching some of the first
   courses in the world focused on teaching machine learning to creative
   practitioners. The article subsequently draws on data collected from
   these courses-an online course as well as undergraduate and
   masters-level courses taught at a university-to begin to understand how
   this curriculum supported student learning, to understand learners'
   challenges and mistakes, and to inform future teaching and research.},
DOI = {10.1145/3294008},
Article-Number = {31},
ISSN = {1946-6226},
ResearcherID-Numbers = {Fiebrink, Rebecca/A-8317-2017
   Fiebrink, Rebecca/AAU-6856-2021},
ORCID-Numbers = {Fiebrink, Rebecca/0000-0002-7609-2234
   },
Unique-ID = {WOS:000494273000002},
}

@article{ WOS:001028542300001,
Author = {Singh, Santosh Kumar and Tiwari, Arun Kumar and Paliwal, H. K.},
Title = {A state-of-the-art review on the utilization of machine learning in
   nanofluids, solar energy generation, and the prognosis of solar power},
Journal = {ENGINEERING ANALYSIS WITH BOUNDARY ELEMENTS},
Year = {2023},
Volume = {155},
Pages = {62-86},
Month = {OCT},
Abstract = {In the contemporary data-driven era, the fields of machine learning,
   deep learning, big data, statistics, and data science are essential for
   forecasting outcomes and getting insights from data. This paper looks at
   how machine learning approaches can be used to anticipate solar power
   generation, assess heat exchanger heat transfer efficiency, and predict
   the thermo-physical properties of nanofluids. The review specifically
   focuses on the potential use of machine learning in solar thermal
   applications, perovskites, and photovoltaic power forecasting.
   Predictions of nanofluid characteristics and device performance may be
   more accurately made with the development of machine learning
   algorithms. The use of machine learning in the creation of new
   perovskites and the assessment of their effectiveness and stability is
   also included in the review. Additionally, the paper explores
   developments in artificial intelligence, particularly deep learning, in
   this area and offers insights into techniques for forecasting solar
   power, including PV production, cloud motion, and weather
   classification.},
DOI = {10.1016/j.enganabound.2023.06.003},
EarlyAccessDate = {JUN 2023},
ISSN = {0955-7997},
EISSN = {1873-197X},
ResearcherID-Numbers = {Tiwari, Arun/AAV-2954-2020},
Unique-ID = {WOS:001028542300001},
}

@article{ WOS:001130161000001,
Author = {Cavallaro, Claudia and Cutello, Vincenzo and Pavone, Mario and Zito,
   Francesco},
Title = {Machine Learning and Genetic Algorithms: A case study on image
   reconstruction},
Journal = {KNOWLEDGE-BASED SYSTEMS},
Year = {2024},
Volume = {284},
Month = {JAN 25},
Abstract = {In this research, we investigate the application of machine learning
   techniques to optimization problems and propose a novel integration
   between metaheuristics and machine learning for the problem of image
   reconstruction. We propose a modified version of the standard genetic
   algorithm that uses machine learning to quickly drive the search towards
   good solutions by dynamically adjusting its parameters. We conducted
   experiments to compare the performance of our proposed algorithm with
   other metaheuristic algorithms, including Tabu Search, Iterated Local
   Search, and Artificial Immune System. Our results demonstrate the
   effectiveness of our algorithm in finding better solutions and in
   achieving faster convergence times compared to the other algorithms. The
   significant computational time difference between the standard genetic
   algorithm and the genetic algorithm with machine learning highlights the
   innovation of our approach and its potential to improve real-world
   applications.},
DOI = {10.1016/j.knosys.2023.111194},
EarlyAccessDate = {NOV 2023},
Article-Number = {111194},
ISSN = {0950-7051},
EISSN = {1872-7409},
ResearcherID-Numbers = {Cavallaro, Claudia/GRR-9707-2022
   Zito, Francesco/HLX-1288-2023
   Cutello, Vincenzo/J-6497-2014
   Pavone, Mario/AAL-5787-2020},
ORCID-Numbers = {Zito, Francesco/0000-0003-1374-0510
   Cutello, Vincenzo/0000-0002-7521-3516
   Cavallaro, Claudia/0000-0003-3938-0947
   Pavone, Mario F./0000-0003-3421-3293
   },
Unique-ID = {WOS:001130161000001},
}

@article{ WOS:000704195500004,
Author = {Miglani, Arzoo and Kumar, Neeraj},
Title = {Blockchain management and machine learning adaptation for IoT
   environment in 5G and beyond networks: A systematic review},
Journal = {COMPUTER COMMUNICATIONS},
Year = {2021},
Volume = {178},
Pages = {37-63},
Month = {OCT 1},
Abstract = {Keeping in view of the constraints and challenges with respect to big
   data analytics along with security and privacy preservation for 5G and
   B5G applications, the integration of machine learning and blockchain,
   two of the most promising technologies of the modern era is inevitable.
   In comparison to the traditional centralized techniques for security and
   privacy preservation, blockchain uses decentralized consensus algorithms
   for verification and validation of different transactions which are
   supposed to become an integral part of blockchain network. Starting with
   the existing literature survey, we introduce the basic concepts of
   blockchain and machine learning in this article. Then, we presented a
   comprehensive taxonomy for integration of blockchain and machine
   learning in an IoT environment. We also explored federated learning,
   reinforcement learning, deep learning algorithms usage in blockchain
   based applications. Finally, we provide recommendations for future use
   cases of these emerging technologies in 5G and B5G technologies.},
DOI = {10.1016/j.comcom.2021.07.009},
EarlyAccessDate = {JUL 2021},
ISSN = {0140-3664},
EISSN = {1873-703X},
ResearcherID-Numbers = {Kumar, Neeraj/L-3500-2016
   MIGLANI, ARZOO/KHW-6109-2024},
ORCID-Numbers = {Kumar, Neeraj/0000-0002-3020-3947
   },
Unique-ID = {WOS:000704195500004},
}

@article{ WOS:000784962100001,
Author = {Cen, Jian and Yang, Zhuohong and Liu, Xi and Xiong, Jianbin and Chen,
   Honghua},
Title = {A Review of Data-Driven Machinery Fault Diagnosis Using Machine Learning
   Algorithms},
Journal = {JOURNAL OF VIBRATION ENGINEERING \& TECHNOLOGIES},
Year = {2022},
Volume = {10},
Number = {7},
Pages = {2481-2507},
Month = {OCT},
Abstract = {Purpose This article aims to systematically review the recent research
   advances in data-driven machinery fault diagnosis based on machine
   learning algorithms, and provide valuable guidance for future research
   directions in this field. Methods This article reviews the research
   results of data-driven fault diagnosis methods of recent years, and it
   includes the application status and research progress of machinery fault
   diagnosis in three frameworks: shallow machine learning (SML), deep
   learning (DL), and transfer learning (TL). Many publications on this
   topic are classified and summarized. The related theories, application
   research, advantages, and disadvantages of several main algorithms under
   each framework are discussed. Results It has shown that SML-based
   diagnosis models are simple, reliable, and fast to train. For relatively
   uncomplicated systems, SML-based diagnosis models still have important
   applications. For diagnosis tasks with large amounts of training samples
   and the pursuit of higher accuracy, DL-based diagnosis models can
   provide end-to-end diagnostic services for complex systems as well as
   compound faults. TL-based diagnosis models can realize knowledge
   transfer across conditions, machines, and even fields to solve the
   problems of data scarcity and sample imbalance that often occur in fault
   diagnosis. However, in the face of increasingly complex engineering
   systems, the applications of machine learning algorithms in machinery
   fault diagnosis are still challenging. Conclusions In future research,
   the fusion of different machine learning frameworks could solve the
   problems of inadequate feature extraction and slow training of
   diagnostic models. Transformer neural network based on pure attention
   mechanism breaks through the shortcomings of LSTM neural network which
   cannot be computed in parallel, and it is a worthy research direction in
   the field of fault diagnosis. In addition, machinery fault diagnosis
   method based on machine learning algorithms also has great potential for
   improvement in transferability, federated transfer learning, and strong
   noise background. These proposed future research directions can provide
   new ideas for researchers to promote the development of machine learning
   algorithms in machinery fault diagnosis.},
DOI = {10.1007/s42417-022-00498-9},
EarlyAccessDate = {APR 2022},
ISSN = {2523-3920},
EISSN = {2523-3939},
ResearcherID-Numbers = {Chen, Honghua/GVU-0123-2022
   },
ORCID-Numbers = {Yang, Zhuohong/0000-0002-8091-5432},
Unique-ID = {WOS:000784962100001},
}

@article{ WOS:000933899400002,
Author = {Zhou Meng and Luo YiQun and Song HuiChao},
Title = {Applications of machine learning in relativistic heavy ion physics},
Journal = {SCIENTIA SINICA-PHYSICA MECHANICA \& ASTRONOMICA},
Year = {2022},
Volume = {52},
Number = {5},
Abstract = {Recently, with rapid hardware and algorithms development, machine
   learning has been widely used as a significant data analysis method.
   This article reviews the application of di fferent machine learning
   algorithms in heavy ion collisions, including impact parameter
   prediction, nuclear deformation parameter prediction, phase transitions
   classification, fluid evolution simulation, etc. The machine learning
   algorithms comprise classic machine learning algorithms, such as
   ensemble learning and principal component analysis, and deep learning
   algorithms, such as convolutional neural networks and point cloud
   networks. Because of the excellent performance and e fficiency of
   machine learning, these applications will receive much attention in our
   field.},
DOI = {10.1360/SSPMA-2021-0321},
Article-Number = {252002},
ISSN = {1674-7275},
EISSN = {2095-9478},
ResearcherID-Numbers = {Song, Huichao/B-4446-2010
   Luo, Yiqun/HSH-9664-2023},
Unique-ID = {WOS:000933899400002},
}

@article{ WOS:000471643600004,
Author = {Netrapalli, Praneeth},
Title = {Stochastic Gradient Descent and Its Variants in Machine Learning},
Journal = {JOURNAL OF THE INDIAN INSTITUTE OF SCIENCE},
Year = {2019},
Volume = {99},
Number = {2},
Pages = {201-213},
Month = {JUN},
Abstract = {Stochastic gradient descent (SGD) is a fundamental algorithm which has
   had a profound impact on machine learning. This article surveys some
   important results on SGD and its variants that arose in machine
   learning.},
DOI = {10.1007/s41745-019-0098-4},
ISSN = {0970-4140},
EISSN = {0019-4964},
Unique-ID = {WOS:000471643600004},
}

@article{ WOS:001238605200001,
Author = {Nazemi, Abdolreza and Fabozzi, Frank J.},
Title = {Interpretable machine learning for creditor recovery rates},
Journal = {JOURNAL OF BANKING \& FINANCE},
Year = {2024},
Volume = {164},
Month = {JUL},
Abstract = {Machine learning methods have achieved great success in modeling complex
   patterns in finance such as asset pricing and credit risk that enable
   them to outperform statistical models. In addition to the predictive
   accuracy of machine learning methods, the ability to interpret what a
   model has learned is crucial in the finance industry. We address this
   challenge by adapting interpretable machine learning to the context of
   corporate bond recovery rate modeling. In addition to the best
   performance, we show the value of interpretable machine learning by
   finding drivers of recovery rates and their relationship that cannot be
   discovered by the use of traditional machine learning methods. Our
   findings are financially meaningful and consistent with the findings in
   the existing credit risk literature.},
DOI = {10.1016/j.jbankfin.2024.107187},
EarlyAccessDate = {MAY 2024},
Article-Number = {107187},
ISSN = {0378-4266},
EISSN = {1872-6372},
ResearcherID-Numbers = {Nazemi, Abdolreza/AAO-3941-2021},
Unique-ID = {WOS:001238605200001},
}

@article{ WOS:000401022300001,
Author = {Peterson, Andrew A. and Christensen, Rune and Khorshidi, Alireza},
Title = {Addressing uncertainty in atomistic machine learning},
Journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS},
Year = {2017},
Volume = {19},
Number = {18},
Pages = {10978-10985},
Month = {MAY 14},
Abstract = {Machine-learning regression has been demonstrated to precisely emulate
   the potential energy and forces that are output from more expensive
   electronic-structure calculations. However, to predict new regions of
   the potential energy surface, an assessment must be made of the
   credibility of the predictions. In this perspective, we address the
   types of errors that might arise in atomistic machine learning, the
   unique aspects of atomistic simulations that make machine-learning
   challenging, and highlight how uncertainty analysis can be used to
   assess the validity of machine-learning predictions. We suggest this
   will allow researchers to more fully use machine learning for the
   routine acceleration of large, high-accuracy, or extended-time
   simulations. In our demonstrations, we use a bootstrap ensemble of
   neural network-based calculators, and show that the width of the
   ensemble can provide an estimate of the uncertainty when the width is
   comparable to that in the training data. Intriguingly, we also show that
   the uncertainty can be localized to specific atoms in the simulation,
   which may offer hints for the generation of training data to
   strategically improve the machine-learned representation.},
DOI = {10.1039/c7cp00375g},
ISSN = {1463-9076},
EISSN = {1463-9084},
ResearcherID-Numbers = {Christensen, Rune/B-6423-2013},
ORCID-Numbers = {Christensen, Rune/0000-0002-7803-7896
   },
Unique-ID = {WOS:000401022300001},
}

@article{ WOS:000609117400001,
Author = {Singh, Narendra and Singh, Pushpa and Gupta, Mukul},
Title = {An inclusive survey on machine learning for CRM: a paradigm shift},
Journal = {DECISION},
Year = {2020},
Volume = {47},
Number = {4},
Pages = {447-457},
Month = {DEC},
Abstract = {Customer relationship management (CRM) is the tool to enhance customer
   relationship in any business. Due to the exponential growth of data
   volume, in any field, it is significant to develop new techniques to
   discover the customer knowledge, automation of the system and moreover
   customer satisfaction to win customer lifetime value. CRM with machine
   learning could bring a catalytic change in business. Several supervised
   and unsupervised machine learning techniques are utilized to improve the
   customer experience and profitability of business. This paper reviews
   the available literature on the CRM with machine learning techniques for
   customer identification, customer attraction, and customer retention and
   customer development. This study reveals that supervised learning
   techniques are 48.48\% utilized, unsupervised learning techniques are
   utilized 15.15\%, and 9.09\% utilized other techniques in CRM. Paradigm
   is also shifted toward the deep learning from machine learning as
   28.28\% text has been reported to deep learning. Decision tree-based
   algorithm and support vector machine algorithms are most utilized
   algorithm of supervised learning. E-commerce and telecommunication
   sectors are the most important areas identified with the exponential
   growth of the users and hence need a suitable machine learning
   techniques for customer satisfaction and business profitability.},
DOI = {10.1007/s40622-020-00261-7},
EarlyAccessDate = {JAN 2021},
ISSN = {0304-0941},
EISSN = {2197-1722},
ResearcherID-Numbers = {Gupta, Mukul/OPN-1751-2025
   Singh, Narendra/HQZ-1214-2023
   Singh, Pushpa/ABG-3406-2020},
ORCID-Numbers = {Singh, Narendra/0000-0003-0033-5727
   Singh, Narendra/0000-0002-6760-8550
   },
Unique-ID = {WOS:000609117400001},
}

@article{ WOS:001221463000016,
Author = {Packwood, Daniel and Nguyen, Linh Thi Hoai and Cesana, Pierluigi and
   Zhang, Guoxi and Staykov, Aleksandar and Fukumoto, Yasuhide and Nguyen,
   Dinh Hoa},
Title = {Machine Learning in Materials Chemistry: An Invitation},
Journal = {MACHINE LEARNING WITH APPLICATIONS},
Year = {2022},
Volume = {8},
Month = {JUN 15},
Abstract = {Materials chemistry is being profoundly influenced by the uptake of
   machine learning methodologies. Machine learning techniques, in
   combination with established techniques from computational physics,
   promise to accelerate the discovery of new materials by elucidating
   complex structure-property relationships from massive material
   databases. Despite exciting possibilities, further methodological
   developments call for a greater synergism between materials chemists,
   physicists, and engineers on one side, with computer science and math
   majors on the other. In this review, we provide a non -exhaustive
   account of machine learning in materials chemistry for computer
   scientists and applied mathematicians, with an emphasis on molecule
   datasets and materials chemistry problems. The first part of this review
   provides a tutorial on how to prepare such datasets for subsequent model
   building, with an emphasis on the construction of feature vectors. We
   also provide a self-contained introduction to density functional theory,
   a method from computational physics which is widely used to generate
   datasets and compute response variables. The second part reviews two
   machine learning methodologies which represent the status quo in
   materials chemistry at present - kernelized machine learning and
   Bayesian machine learning - and discusses their application to real
   datasets. In the third part of the review, we introduce some emerging
   machine learning techniques which have not been widely adopted by
   materials scientists and therefore present potential avenues for
   computer science and applied math majors. In the final concluding
   section, we discuss some recent machine learning -based approaches to
   real materials discovery problems and speculate on some promising future
   directions.},
DOI = {10.1016/j.mlwa.2022.100265},
Article-Number = {100265},
EISSN = {2666-8270},
ResearcherID-Numbers = {Nguyen, Dinh/V-2176-2019
   Staykov, Aleksandar/A-6214-2009
   Nguyen, Dinh Hoa/V-2176-2019
   },
ORCID-Numbers = {Fukumoto, Yasuhide/0000-0001-7465-3168
   Nguyen, Dinh Hoa/0000-0003-3318-737X
   Zhang, Guoxi/0000-0001-8154-6985},
Unique-ID = {WOS:001221463000016},
}

@article{ WOS:000523935300001,
Author = {Chatzimparmpas, Angelos and Martins, Rafael M. and Jusufi, Ilir and
   Kerren, Andreas},
Title = {A survey of surveys on the use of visualization for interpreting machine
   learning models},
Journal = {INFORMATION VISUALIZATION},
Year = {2020},
Volume = {19},
Number = {3},
Pages = {207-233},
Month = {JUL},
Abstract = {Research in machine learning has become very popular in recent years,
   with many types of models proposed to comprehend and predict patterns
   and trends in data originating from different domains. As these models
   get more and more complex, it also becomes harder for users to assess
   and trust their results, since their internal operations are mostly
   hidden in black boxes. The interpretation of machine learning models is
   currently a hot topic in the information visualization community, with
   results showing that insights from machine learning models can lead to
   better predictions and improve the trustworthiness of the results. Due
   to this, multiple (and extensive) survey articles have been published
   recently trying to summarize the high number of original research papers
   published on the topic. But there is not always a clear definition of
   what these surveys cover, what is the overlap between them, which types
   of machine learning models they deal with, or what exactly is the
   scenario that the readers will find in each of them. In this article, we
   present a meta-analysis (i.e. a ``survey of surveys{''}) of manually
   collected survey papers that refer to the visual interpretation of
   machine learning models, including the papers discussed in the selected
   surveys. The aim of our article is to serve both as a detailed summary
   and as a guide through this survey ecosystem by acquiring, cataloging,
   and presenting fundamental knowledge of the state of the art and
   research opportunities in the area. Our results confirm the increasing
   trend of interpreting machine learning with visualizations in the past
   years, and that visualization can assist in, for example, online
   training processes of deep learning models and enhancing trust into
   machine learning. However, the question of exactly how this assistance
   should take place is still considered as an open challenge of the
   visualization community.},
DOI = {10.1177/1473871620904671},
EarlyAccessDate = {MAR 2020},
Article-Number = {1473871620904671},
ISSN = {1473-8716},
EISSN = {1473-8724},
ResearcherID-Numbers = {Jusufi, Ilir/G-2932-2014
   Kerren, Andreas/AAV-9187-2020
   Martins, Rafael/H-9192-2019
   },
ORCID-Numbers = {Chatzimparmpas, Angelos/0000-0002-9079-2376},
Unique-ID = {WOS:000523935300001},
}

@article{ WOS:000635324300001,
Author = {Zhang, Xiaohang and Wang, Yuan and Li, Zhengren},
Title = {RETRACTED: User acceptance of machine learning models - Integrating
   several important external variables with technology acceptance model
   (Retracted Article)},
Journal = {INTERNATIONAL JOURNAL OF ELECTRICAL ENGINEERING EDUCATION},
Year = {2021},
Month = {2021 MAR 25},
Abstract = {Machine learning models enable data-based decision-making in many areas
   and have attracted extensive attention. By testing the factors that
   influence the adoption of machine learning models, this study expands
   the scope of machine learning models in information technology adoption
   research. Based on the machine learning background and Technology
   Acceptance Model, this study integrates the necessary external
   variables, proposes a research model, and further verifies the validity
   of the model through the survey of 192 users of machine learning models.
   The results showed that organizational factors, trust, perceived
   usefulness, and perceived ease of use are positively correlated with the
   attitude of machine learning models. Moreover, our findings show that
   the interpretability of the model has an important positive effect on
   trust. The factors examined in this study are the basis for the
   development and use of reliable machine learning models. And it has
   important practical significance for promoting user adoption of machine
   learning model. Meanwhile, these theoretical studies also provide a
   strong literature support for the adoption of machine learning models
   and fill the theoretical research gap in this field.},
DOI = {10.1177/00207209211005271},
EarlyAccessDate = {MAR 2021},
Article-Number = {00207209211005271},
ISSN = {0020-7209},
EISSN = {2050-4578},
Unique-ID = {WOS:000635324300001},
}

@article{ WOS:000962500000001,
Author = {Zhou, Cheng-Mao and Wang, Ying and Xue, Qiong and Zhu, Yu},
Title = {Differentiation of Bone Metastasis in Elderly Patients With Lung
   Adenocarcinoma Using Multiple Machine Learning Algorithms},
Journal = {CANCER CONTROL},
Year = {2023},
Volume = {30},
Month = {MAR},
Abstract = {ObjectiveWe tested the performance of general machine learning and joint
   machine learning algorithms in the classification of bone metastasis, in
   patients with lung adenocarcinoma.MethodsWe used R version 3.5.3 for
   statistical analysis of the general information, and Python to construct
   machine learning models.ResultsWe first used the average classifiers of
   the 4 machine learning algorithms to rank the features and the results
   showed that race, sex, whether they had surgery and marriage were the
   first 4 factors affecting bone metastasis. Machine learning results in
   the training group: for area under the curve (AUC), except for RF and
   LR, the AUC values of all machine learning classifiers were greater than
   .8, but the joint algorithm did not improve the AUC for any single
   machine learning algorithm. Among the results related to accuracy and
   precision, the accuracy of other machine learning classifiers except the
   RF algorithm was higher than 70\%, and only the precision of the LGBM
   algorithm was higher than 70\%. Machine learning results in the test
   group: Similarly, for areas under the curve (AUC), except RF and LR, the
   AUC values for all machine learning classifiers were greater than .8,
   but the joint algorithm did not improve the AUC value for any single
   machine learning algorithm. For accuracy, except for the RF algorithm,
   the accuracy of other machine learning classifiers was higher than 70\%.
   The highest precision for the LGBM algorithm was .675.ConclusionThe
   results of this concept verification study show that machine learning
   algorithm classifiers can distinguish the bone metastasis of patients
   with lung cancer. This will provide a new research idea for the future
   use of non-invasive technology to identify bone metastasis in
   lungcancer. However, more prospective multicenter cohort studies are
   needed.},
DOI = {10.1177/10732748231167958},
Article-Number = {10732748231167958},
ISSN = {1073-2748},
EISSN = {1526-2359},
ResearcherID-Numbers = {Zhou, Cheng-Mao/ABF-4435-2020},
ORCID-Numbers = {Zhou, Chengmao/0000-0001-5680-791X
   },
Unique-ID = {WOS:000962500000001},
}

@article{ WOS:000357246100026,
Author = {Li, Ya and Tian, Xinmei and Song, Mingli and Tao, Dacheng},
Title = {Multi-task proximal support vector machine},
Journal = {PATTERN RECOGNITION},
Year = {2015},
Volume = {48},
Number = {10},
Pages = {3249-3257},
Month = {OCT},
Abstract = {With the explosive growth of the use of imagery, visual recognition
   plays an important role in many applications and attracts increasing
   research attention. Given several related tasks, single-task learning
   learns each task separately and ignores the relationships among these
   tasks. Different from single-task learning, multi-task learning can
   explore more information to learn all tasks jointly by using
   relationships among these tasks. In this paper, we propose a novel
   multi-task learning model based on the proximal support vector machine.
   The proximal support vector machine uses the large-margin idea as does
   the standard support vector machines but with looser constraints and
   much lower computational cost. Our multi-task proximal support vector
   machine inherits the merits of the proximal support vector machine and
   achieves better performance compared with other popular multi-task
   learning models. Experiments are conducted on several multi-task
   learning datasets, including two classification datasets and one
   regression dataset. All results demonstrate the effectiveness and
   efficiency of our proposed multi-task proximal support vector machine.
   (C) 2015 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.patcog.2015.01.014},
ISSN = {0031-3203},
EISSN = {1873-5142},
ResearcherID-Numbers = {Li, Shiyan/H-3445-2016
   Tao, Dacheng/A-5449-2012},
Unique-ID = {WOS:000357246100026},
}

@article{ WOS:001326938800003,
Author = {Jiang, Lipeng and Jiang, Xue and Lv, Guocai and Su, Yanjing},
Title = {A mini review of machine learning in inorganic phosphors},
Journal = {JOURNAL OF MATERIALS INFORMATICS},
Year = {2022},
Volume = {2},
Number = {3},
Month = {SEP},
Abstract = {Machine learning has promoted the rapid development of materials
   science. In this review, we provide an overview of recent advances in
   machine learning for inorganic phosphors. We take two aspects of
   material properties prediction and optimization based on iterative
   experiments as entry points to outline the applications of machine
   learning for inorganic phosphors in terms of Debye temperature
   prediction and luminescence intensity and thermal stability
   optimization. By analyzing the machine learning methods and their
   application objectives, current problems are summarized and suggestions
   for subsequent development are proposed.},
DOI = {10.20517/jmi.2022.21},
Article-Number = {14},
EISSN = {2770-372X},
ResearcherID-Numbers = {Lv, Guocai/JQJ-0251-2023
   },
ORCID-Numbers = {Jiang, Lipeng/0000-0002-9876-9517},
Unique-ID = {WOS:001326938800003},
}

@article{ WOS:000360416900003,
Author = {Mohapatra, P. and Chakravarty, S. and Dash, P. K.},
Title = {An improved cuckoo search based extreme learning machine for medical
   data classification},
Journal = {SWARM AND EVOLUTIONARY COMPUTATION},
Year = {2015},
Volume = {24},
Pages = {25-49},
Month = {OCT},
Abstract = {Machine learning techniques are being increasingly used for detection
   and diagnosis of diseases for its accuracy and efficiency in pattern
   classification. In this paper, improved cuckoo search based extreme
   learning machine (ICSELM) is proposed to classify binary medical
   datasets. Extreme learning machine (ELM) is widely used as a learning
   algorithm for training single layer feed forward neural networks (SLFN)
   in the field of classification. However, to make the model more stable,
   an evolutionary algorithm improved cuckoo search (ICS) is used to
   pre-train ELM by selecting the input weights and hidden biases. Like
   ELM, Moore-Penrose (MP) generalized inverse is used in ICSELM to
   analytically determines the output weights. To evaluate the
   effectiveness of the proposed model, four benchmark datasets, i.e.
   Breast Cancer, Diabetes, Bupa and Hepatitis from the UCI Repository of
   Machine Learning are used. A number of useful performance evaluation
   measures including accuracy, sensitivity, specificity, confusion matrix,
   Gmean, F-score and norm of the output weights as well as the area under
   the receiver operating characteristic (ROC) curve are computed. The
   results are analyzed and compared with both ELM based models like ELM,
   on-line sequential extreme learning algorithm (OSELM), CSELM and other
   artificial neural networks i.e. multi-layered perceptron (MLP), MLPCS,
   MLPICS and radial basis function neural network (RBFNN), RBFNNCS,
   RBFNNICS. The experimental results demonstrate that the ICSELM model
   outperforms other models. (C) 2015 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.swevo.2015.05.003},
ISSN = {2210-6502},
EISSN = {2210-6510},
ResearcherID-Numbers = {Chakravarty, Dr. Sujata/JQH-9937-2023},
ORCID-Numbers = {Chakravarty, Dr. Sujata/0000-0002-1293-5378},
Unique-ID = {WOS:000360416900003},
}

@article{ WOS:000546324500005,
Author = {von Lilienfeld, O. Anatole and Mueller, Klaus-Robert and Tkatchenko,
   Alexandre},
Title = {Exploring chemical compound space with quantum-based machine learning},
Journal = {NATURE REVIEWS CHEMISTRY},
Year = {2020},
Volume = {4},
Number = {7},
Pages = {347-358},
Month = {JUL},
Abstract = {Machine-learning techniques have enabled, among many other applications,
   the exploration of molecular properties throughout chemical space. The
   specific development of quantum-based approaches in machine learning can
   now help us unravel new chemical insights.
   Rational design of compounds with specific properties requires
   understanding and fast evaluation of molecular properties throughout
   chemical compound space - the huge set of all potentially stable
   molecules. Recent advances in combining quantum-mechanical calculations
   with machine learning provide powerful tools for exploring wide swathes
   of chemical compound space. We present our perspective on this exciting
   and quickly developing field by discussing key advances in the
   development and applications of quantum-mechanics-based machine-learning
   methods to diverse compounds and properties, and outlining the
   challenges ahead. We argue that significant progress in the exploration
   and understanding of chemical compound space can be made through a
   systematic combination of rigorous physical theories, comprehensive
   synthetic data sets of microscopic and macroscopic properties, and
   modern machine-learning methods that account for physical and chemical
   knowledge.},
DOI = {10.1038/s41570-020-0189-9},
EISSN = {2397-3358},
ResearcherID-Numbers = {von Lilienfeld, O./D-8529-2011
   Tkatchenko, Alexandre/E-7148-2011
   Mueller, Klaus-Robert/C-3196-2013},
ORCID-Numbers = {Mueller, Klaus-Robert/0000-0002-3861-7685
   Tkatchenko, Alexandre/0000-0002-1012-4854
   },
Unique-ID = {WOS:000546324500005},
}

@article{ WOS:000995149000001,
Author = {Palanivinayagam, Ashokkumar and El-Bayeh, Claude Ziad and Damasevicius,
   Robertas},
Title = {Twenty Years of Machine-Learning-Based Text Classification: A Systematic
   Review},
Journal = {ALGORITHMS},
Year = {2023},
Volume = {16},
Number = {5},
Month = {APR 29},
Abstract = {Machine-learning-based text classification is one of the leading
   research areas and has a wide range of applications, which include spam
   detection, hate speech identification, reviews, rating summarization,
   sentiment analysis, and topic modelling. Widely used
   machine-learning-based research differs in terms of the datasets,
   training methods, performance evaluation, and comparison methods used.
   In this paper, we surveyed 224 papers published between 2003 and 2022
   that employed machine learning for text classification. The Preferred
   Reporting Items for Systematic Reviews (PRISMA) statement is used as the
   guidelines for the systematic review process. The comprehensive
   differences in the literature are analyzed in terms of six aspects:
   datasets, machine learning models, best accuracy, performance evaluation
   metrics, training and testing splitting methods, and comparisons among
   machine learning models. Furthermore, we highlight the limitations and
   research gaps in the literature. Although the research works included in
   the survey perform well in terms of text classification, improvement is
   required in many areas. We believe that this survey paper will be useful
   for researchers in the field of text classification.},
DOI = {10.3390/a16050236},
Article-Number = {236},
EISSN = {1999-4893},
ResearcherID-Numbers = {Damaševičius, Robertas/E-1387-2017
   El-Bayeh, Claude/B-9636-2012
   P, Ashokkumar/HGD-4138-2022},
ORCID-Numbers = {El-Bayeh, Claude/0000-0002-8268-8878
   P, Ashokkumar/0000-0003-2531-1326
   },
Unique-ID = {WOS:000995149000001},
}

@article{ WOS:000751673300089,
Author = {Chan, Maria F. and Witztum, Alon and Valdes, Gilmer},
Title = {Integration of AI and Machine Learning in Radiotherapy QA},
Journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE},
Year = {2020},
Volume = {3},
Abstract = {The use of machine learning and other sophisticated models to aid in
   prediction and decision making has become widely popular across a
   breadth of disciplines. Within the greater diagnostic radiology,
   radiation oncology, and medical physics communities promising work is
   being performed in tissue classification and cancer staging, outcome
   prediction, automated segmentation, treatment planning, and quality
   assurance as well as other areas. In this article, machine learning
   approaches are explored, highlighting specific applications in machine
   and patient-specific quality assurance (QA). Machine learning can
   analyze multiple elements of a delivery system on its performance over
   time including the multileaf collimator (MLC), imaging system,
   mechanical and dosimetric parameters. Virtual Intensity-Modulated
   Radiation Therapy (IMRT) QA can predict passing rates using different
   measurement techniques, different treatment planning systems, and
   different treatment delivery machines across multiple institutions.
   Prediction of QA passing rates and other metrics can have profound
   implications on the current IMRT process. Here we cover general concepts
   of machine learning in dosimetry and various methods used in virtual
   IMRT QA, as well as their clinical applications.},
DOI = {10.3389/frai.2020.577620},
Article-Number = {577620},
EISSN = {2624-8212},
ResearcherID-Numbers = {Chan, Maria/Z-2589-2019},
Unique-ID = {WOS:000751673300089},
}

@article{ WOS:000569585500001,
Author = {Pai, Ping-Feng and Wang, Wen-Chang},
Title = {Using Machine Learning Models and Actual Transaction Data for Predicting
   Real Estate Prices},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {17},
Month = {SEP},
Abstract = {Real estate price prediction is crucial for the establishment of real
   estate policies and can help real estate owners and agents make
   informative decisions. The aim of this study is to employ actual
   transaction data and machine learning models to predict prices of real
   estate. The actual transaction data contain attributes and transaction
   prices of real estate that respectively serve as independent variables
   and dependent variables for machine learning models. The study employed
   four machine learning models-namely, least squares support vector
   regression (LSSVR), classification and regression tree (CART), general
   regression neural networks (GRNN), and backpropagation neural networks
   (BPNN), to forecast real estate prices. In addition, genetic algorithms
   were used to select parameters of machine learning models. Numerical
   results indicated that the least squares support vector regression
   outperforms the other three machine learning models in terms of
   forecasting accuracy. Furthermore, forecasting results generated by the
   least squares support vector regression are superior to previous related
   studies of real estate price prediction in terms of the average absolute
   percentage error. Thus, the machine learning-based model is a
   substantial and feasible way to forecast real estate prices, and the
   least squares support vector regression can provide relatively
   competitive and satisfactory results.},
DOI = {10.3390/app10175832},
Article-Number = {5832},
EISSN = {2076-3417},
ORCID-Numbers = {Pai, Ping-Feng/0000-0002-4020-3326},
Unique-ID = {WOS:000569585500001},
}

@article{ WOS:000534638300001,
Author = {Nasir, Tauqir and Asmael, Mohammed and Zeeshan, Qasim and Solyali, Davut},
Title = {Applications of Machine Learning to Friction Stir Welding Process
   Optimization},
Journal = {JURNAL KEJURUTERAAN},
Year = {2020},
Volume = {32},
Number = {2},
Pages = {171-186},
Month = {MAY},
Abstract = {Machine learning (ML) is a branch of artificial intelligent which
   involve the study and development of algorithm for computer to learn
   from data. A computational method used in machine learning to learn or
   get directly information from data without relying on a prearranged
   model equation. The applications of ML applied in the domains of all
   industries. In the field of manufacturing the ability of ML approach is
   utilized to predict the failure before occurrence. FSW and FSSW is an
   advanced form of friction welding and it is a solid state joining
   technique which is mostly used to weld the dissimilar alloys. FSW, FSSW
   has become a dominant joining method in aerospace, railway and ship
   building industries. It observed that the number of applications of
   machine learning increased in FSW, FSSW process which sheared the
   Machine-learning approaches like, artificial Neural Network (ANN),
   Regression model (RSM), Support Vector Machine (SVM) and Adaptive
   Neuro-Fuzzy Inference System (ANFIS). The main purpose of this study is
   to review and summarize the emerging research work of machine learning
   techniques in FSW and FSSW. Previous researchers demonstrate that the
   Machine Learning applications applied to predict the response of FSW and
   FSSW process. The prediction in error percentage in result of ANN and
   RSM model in overall is less than 5\%. In comparison between ANN/RSM the
   obtain result shows that ANN is provide better and accurate than RSM. In
   application of SVM algorithm the prediction accuracy found 100\% for
   training and testing process.},
DOI = {10.17576/jkukm-2020-32(2)-01},
ISSN = {0128-0198},
EISSN = {2289-7526},
ResearcherID-Numbers = {Nasir, Tauqir/AAT-1812-2020
   Asmael, Mohammed/I-5510-2015
   Zeeshan, Qasim/Q-3181-2019
   Nasir, Dr. Tauqir/AAT-1812-2020
   Solyalı, Davut/AAM-7051-2020
   Asmael, MOHAMMED/I-5510-2015},
ORCID-Numbers = {Nasir, Dr. Tauqir/0000-0002-4339-0059
   Asmael, MOHAMMED/0000-0003-2853-0460},
Unique-ID = {WOS:000534638300001},
}

@article{ WOS:000797054700007,
Author = {Zhao, Yiqing and Yu, Yue and Wang, Hanyin and Li, Yikuan and Deng, Yu
   and Jiang, Guoqian and Luo, Yuan},
Title = {Machine Learning in Causal Inference: Application in Pharmacovigilance},
Journal = {DRUG SAFETY},
Year = {2022},
Volume = {45},
Number = {5, SI},
Pages = {459-476},
Month = {MAY},
Abstract = {Monitoring adverse drug events or pharmacovigilance has been promoted by
   the World Health Organization to assure the safety of medicines through
   a timely and reliable information exchange regarding drug safety issues.
   We aim to discuss the application of machine learning methods as well as
   causal inference paradigms in pharmacovigilance. We first reviewed data
   sources for pharmacovigilance. Then, we examined traditional causal
   inference paradigms, their applications in pharmacovigilance, and how
   machine learning methods and causal inference paradigms were integrated
   to enhance the performance of traditional causal inference paradigms.
   Finally, we summarized issues with currently mainstream
   correlation-based machine learning models and how the machine learning
   community has tried to address these issues by incorporating causal
   inference paradigms. Our literature search revealed that most existing
   data sources and tasks for pharmacovigilance were not designed for
   causal inference. Additionally, pharmacovigilance was lagging in
   adopting machine learning-causal inference integrated models. We
   highlight several currently trending directions or gaps to integrate
   causal inference with machine learning in pharmacovigilance research.
   Finally, our literature search revealed that the adoption of causal
   paradigms can mitigate known issues with machine learning models. We
   foresee that the pharmacovigilance domain can benefit from the progress
   in the machine learning field.},
DOI = {10.1007/s40264-022-01155-6},
ISSN = {0114-5916},
EISSN = {1179-1942},
ResearcherID-Numbers = {Zhao, Yiqing/AEL-5533-2022
   luo, yuan/JLS-6416-2023
   Wang, Hanyin/AAB-7835-2022
   Luo, Yuan/K-5563-2016
   deng, yu/HLH-2845-2023},
ORCID-Numbers = {Li, Yikuan/0000-0001-7546-9979
   Luo, Yuan/0000-0003-0195-7456
   },
Unique-ID = {WOS:000797054700007},
}

@article{ WOS:001003539600001,
Author = {Su, Danqi and Zheng, Jiayang and Ma, Junjie and Dong, Zizhe and Chen,
   Zhangjie and Qin, Yanzhou},
Title = {Application of Machine Learning in Fuel Cell Research},
Journal = {ENERGIES},
Year = {2023},
Volume = {16},
Number = {11},
Month = {MAY 29},
Abstract = {A fuel cell is an energy conversion device that utilizes hydrogen energy
   through an electrochemical reaction. Despite their many advantages, such
   as high efficiency, zero emissions, and fast startup, fuel cells have
   not yet been fully commercialized due to deficiencies in service life,
   cost, and performance. Efficient evaluation methods for performance and
   service life are critical for the design and optimization of fuel cells.
   The purpose of this paper was to review the application of common
   machine learning algorithms in fuel cells. The significance and status
   of machine learning applications in fuel cells are briefly described.
   Common machine learning algorithms, such as artificial neural networks,
   support vector machines, and random forests are introduced, and their
   applications in fuel cell performance prediction and optimization are
   comprehensively elaborated. The review revealed that machine learning
   algorithms can be successfully used for performance prediction, service
   life prediction, and fault diagnosis in fuel cells, with good accuracy
   in solving nonlinear problems. Combined with optimization algorithms,
   machine learning models can further carry out the optimization of design
   and operating parameters to achieve multiple optimization goals with
   good accuracy and efficiency. It is expected that this review paper
   could help the reader comprehend the state of the art of machine
   learning applications in fuel fuels and shed light on further
   development directions in fuel cell research.},
DOI = {10.3390/en16114390},
Article-Number = {4390},
EISSN = {1996-1073},
ORCID-Numbers = {Qin, Yanzhou/0000-0003-3348-8957},
Unique-ID = {WOS:001003539600001},
}

@article{ WOS:000695210000001,
Author = {Salman, Omar H. and Taha, Zahraa and Alsabah, Muntadher Q. and Hussein,
   Yaseein S. and Mohammed, Ahmed S. and Aal-Nouman, Mohammed},
Title = {A review on utilizing machine learning technology in the fields of
   electronic emergency triage and patient priority systems in
   telemedicine: Coherent taxonomy, motivations, open research challenges
   and recommendations for intelligent future work},
Journal = {COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE},
Year = {2021},
Volume = {209},
Month = {SEP},
Abstract = {Background: With the remarkable increasing in the numbers of patients,
   the triaging and prioritizing patients into multi-emergency level is
   required to accommodate all the patients, save more lives, and manage
   the medical resources effectively. Triaging and prioritizing patients
   becomes particularly challenging especially for the patients who are far
   from hospital and use telemedicine system. To this end, the researchers
   exploiting the useful tool of machine learning to address this
   challenge. Hence, carrying out an intensive investigation and in-depth
   study in the field of using machine learning in E-triage and patient
   priority are essential and required. Objectives: This research aims to
   (1) provide a literature review and an in-depth study on the roles of
   machine learning in the fields of electronic emergency triage (E-triage)
   and prioritize patients for fast healthcare services in telemedicine
   applications. (2) highlight the effectiveness of machine learning
   methods in terms of algorithms, medical input data, output results, and
   machine learning goals in remote healthcare telemedicine systems. (3)
   present the relationship between machine learning goals and the
   electronic triage processes specifically on the: triage levels, medical
   features for input, outcome results as outputs, and the relevant
   diseases. (4), the outcomes of our analyses are subjected to organize
   and propose a cross-over taxonomy between machine learning algorithms
   and telemedicine structure. (5) present lists of motivations, open
   research challenges and recommendations for future intelligent work for
   both academic and industrial sectors in telemedicine and remote
   healthcare applications. Methods: An intensive research is carried out
   by reviewing all articles related to the field of E-triage and remote
   priority systems that utilise machine learning algorithms and sensors.
   We have searched all related keywords to investigate the databases of
   Science Direct, IEEE Xplore, Web of Science, PubMed, and Medline for the
   articles, which have been published from January 2012 up to date.
   Results: A new crossover matching between machine learning methods and
   telemedicine taxonomy is proposed. The crossover-taxonomy is developed
   in this study to identify the relationship between machine learning
   algorithm and the equivalent telemedicine categories whereas the machine
   learning algorithm has been utilized. The impact of utilizing machine
   learning is composed in proposing the telemedicine architecture based on
   synchronous (real-time/ online) and asynchronous (store-and-forward /
   offline) structure. In addition to that, list of machine learning
   algorithms, list of the performance metrics, list of inputs data and
   outputs results are presented. Moreover, open research challenges, the
   benefits of utilizing machine learning and the recommendations for new
   research opportunities that need to be addressed for the synergistic
   integration of multidisciplinary works are organized and presented
   accordingly. Discussion: The state-of-the-art studies on the E-triage
   and priority systems that utilise machine learning algorithms in
   telemedicine architecture are discussed. This approach allows the
   researchers to understand the modernisation of healthcare systems and
   the efficient use of artificial intelligence and machine learning.
   In particular, the growing worldwide population and various chronic
   diseases such as heart chronic diseases, blood pressure and diabetes,
   require smart health monitoring systems in E-triage and priority
   systems, in which machine learning algorithms could be greatly
   beneficial.
   Conclusions: Although research directions on E-triage and priority
   systems that use machine learning algorithms in telemedicine vary, they
   are equally essential and should be considered. Hence, we provide a
   comprehensive review to emphasise the advantages of the existing
   research in multidisciplinary works of artificial intelligence, machine
   learning and healthcare services. (c) 2021 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.cmpb.2021.106357},
EarlyAccessDate = {AUG 2021},
Article-Number = {106357},
ISSN = {0169-2607},
EISSN = {1872-7565},
ResearcherID-Numbers = {taha, zahraa/X-8888-2019
   Mohammed, Ahmed/AAE-8528-2022
   Salman, Dr. Omar/D-5520-2019
   Alsabah, Muntadher/O-7765-2019
   Hussein, Dr. Yaseein Soubhi/J-1615-2017
   AAL-NOUMAN, MOHAMMED/O-1627-2019
   salman, omar/E-6619-2016
   Hussein, Yaseein/J-1615-2017},
ORCID-Numbers = {Salman, Dr. Omar/0000-0002-4415-1236
   Aal-nouman, Mohammed/0000-0001-5056-8567
   Hussein, Dr. Yaseein Soubhi/0000-0001-5044-5524
   Alsabah, Muntadher/0000-0001-7937-3093
   Mohammed, Ahmed/0000-0003-4306-3274
   },
Unique-ID = {WOS:000695210000001},
}

@article{ WOS:000669541400017,
Author = {Sureshbabu, Shree Hari and Sajjan, Manas and Oh, Sangchul and Kais,
   Sabre},
Title = {Implementation of Quantum Machine Learning for Electronic Structure
   Calculations of Periodic Systems on Quantum Computing Devices},
Journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING},
Year = {2021},
Volume = {61},
Number = {6},
Pages = {2667-2674},
Month = {JUN 28},
Abstract = {Quantum machine learning algorithms, the extensions of machine learning
   to quantum regimes, are believed to be more powerful as they leverage
   the power of quantum properties. Quantum machine learning methods have
   been employed to solve quantum many-body systems and have demonstrated
   accurate electronic structure calculations of lattice models, molecular
   systems, and recently periodic systems. A hybrid approach using
   restricted Boltzmann machines and a quantum algorithm to obtain the
   probability distribution that can be optimized classically is a
   promising method due to its efficiency and ease of implementation. Here,
   we implement the benchmark test of the hybrid quantum machine learning
   on the IBM-Q quantum computer to calculate the electronic structure of
   typical two-dimensional crystal structures: hexagonal-boron nitride and
   graphene. The band structures of these systems calculated using the
   hybrid quantum machine learning approach are in good agreement with
   those obtained by the conventional electronic structure calculations.
   This benchmark result implies that the hybrid quantum machine learning
   method, empowered by quantum computers, could provide a new way of
   calculating the electronic structures of quantum many-body systems.},
DOI = {10.1021/acs.jcim.1c00294},
EarlyAccessDate = {JUN 2021},
ISSN = {1549-9596},
EISSN = {1549-960X},
ResearcherID-Numbers = {Sureshbabu, Shree Hari/JAD-0155-2023
   Oh, Sangchul/C-2374-2012
   },
ORCID-Numbers = {Kais, Sabre/0000-0003-0574-5346
   Sureshbabu, Shree Hari/0000-0002-6265-8268
   Oh, Sangchul/0000-0003-3983-9877},
Unique-ID = {WOS:000669541400017},
}

@article{ WOS:001306200600001,
Author = {Khandelwal, Kapil and Nanda, Sonil and Dalai, Ajay K.},
Title = {Machine learning to predict the production of bio-oil, biogas, and
   biochar by pyrolysis of biomass: a review},
Journal = {ENVIRONMENTAL CHEMISTRY LETTERS},
Year = {2024},
Volume = {22},
Number = {6},
Pages = {2669-2698},
Month = {DEC},
Abstract = {The world energy consumption has increased by + 195\% since 1970 with
   more than 80\% of the energy mix originating from fossil fuels, thus
   leading to pollution and global warming. Alternatively, pyrolysis of
   modern biomass is considered carbon neutral and produces value-added
   biogas, bio-oils, and biochar, yet actual pyrolysis processes are not
   fully optimized. Here, we review the use of machine learning to improve
   the pyrolysis of lignocellulosic biomass, with emphasis on machine
   learning algorithms and prediction of product characteristics.
   Algorithms comprise regression analysis, artificial neural networks,
   decision trees, and the support vector machine. Machine learning allows
   for the prediction of yield, quality, surface area, reaction kinetics,
   techno-economics, and lifecycle assessment of biogas, bio-oil, and
   biochar. The robustness of machine learning techniques and engineering
   applications are discussed.},
DOI = {10.1007/s10311-024-01767-7},
EarlyAccessDate = {SEP 2024},
ISSN = {1610-3653},
EISSN = {1610-3661},
ResearcherID-Numbers = {khandelwal, kapil/E-4037-2012
   Nanda, Sonil/AAD-5541-2020
   },
ORCID-Numbers = {Nanda, Sonil/0000-0001-6047-0846},
Unique-ID = {WOS:001306200600001},
}

@article{ WOS:000670096500001,
Author = {Wang, Shirley B.},
Title = {Machine learning to advance the prediction, prevention and treatment of
   eating disorders},
Journal = {EUROPEAN EATING DISORDERS REVIEW},
Year = {2021},
Volume = {29},
Number = {5},
Pages = {683-691},
Month = {SEP},
Abstract = {Machine learning approaches are just emerging in eating disorders
   research. Promising early results suggest that such approaches may be a
   particularly promising and fruitful future direction. However, there are
   several challenges related to the nature of eating disorders in building
   robust, reliable and clinically meaningful prediction models. This
   article aims to provide a brief introduction to machine learning and to
   discuss several such challenges, including issues of sample size,
   measurement, imbalanced data and bias; I also provide concrete steps and
   recommendations for each of these issues. Finally, I outline key
   outstanding questions and directions for future research in building,
   testing and implementing machine learning models to advance our
   prediction, prevention, and treatment of eating disorders.
   Highlights
   Machine learning holds significant promise to advance eating disorders
   research
   Some key considerations for responsible machine learning application in
   eating disorders research include issues of sample size, measurement,
   imbalanced data and bias
   Future research should prioritize external validation of machine
   learning models},
DOI = {10.1002/erv.2850},
EarlyAccessDate = {JUL 2021},
ISSN = {1072-4133},
EISSN = {1099-0968},
ResearcherID-Numbers = {Wang, Shirley/AAM-4560-2020
   },
ORCID-Numbers = {Wang, Shirley/0000-0002-8583-3014},
Unique-ID = {WOS:000670096500001},
}

@article{ WOS:000658810900038,
Author = {Date, Prasanna and Arthur, Davis and Pusey-Nazzaro, Lauren},
Title = {QUBO formulations for training machine learning models},
Journal = {SCIENTIFIC REPORTS},
Year = {2021},
Volume = {11},
Number = {1},
Month = {MAY 11},
Abstract = {Training machine learning models on classical computers is usually a
   time and compute intensive process. With Moore's law nearing its
   inevitable end and an ever-increasing demand for large-scale data
   analysis using machine learning, we must leverage non-conventional
   computing paradigms like quantum computing to train machine learning
   models efficiently. Adiabatic quantum computers can approximately solve
   NP-hard problems, such as the quadratic unconstrained binary
   optimization (QUBO), faster than classical computers. Since many machine
   learning problems are also NP-hard, we believe adiabatic quantum
   computers might be instrumental in training machine learning models
   efficiently in the post Moore's law era. In order to solve problems on
   adiabatic quantum computers, they must be formulated as QUBO problems,
   which is very challenging. In this paper, we formulate the training
   problems of three machine learning models-linear regression, support
   vector machine (SVM) and balanced k-means clustering-as QUBO problems,
   making them conducive to be trained on adiabatic quantum computers. We
   also analyze the computational complexities of our formulations and
   compare them to corresponding state-of-the-art classical approaches. We
   show that the time and space complexities of our formulations are better
   (in case of SVM and balanced k-means clustering) or equivalent (in case
   of linear regression) to their classical counterparts.},
DOI = {10.1038/s41598-021-89461-4},
Article-Number = {10029},
ISSN = {2045-2322},
ResearcherID-Numbers = {Date, Prasanna/AAH-7294-2019
   Date, Prasanna/I-3475-2013
   },
ORCID-Numbers = {Date, Prasanna/0000-0002-1664-069X
   Arthur, Davis/0000-0001-7757-019X},
Unique-ID = {WOS:000658810900038},
}

@article{ WOS:000571258300007,
Author = {Cichos, Frank and Gustavsson, Kristian and Mehlig, Bernhard and Volpe,
   Giovanni},
Title = {Machine learning for active matter},
Journal = {NATURE MACHINE INTELLIGENCE},
Year = {2020},
Volume = {2},
Number = {2},
Pages = {94-103},
Month = {FEB},
Abstract = {The availability of large datasets has boosted the application of
   machine learning in many fields and is now starting to shape
   active-matter research as well. Machine learning techniques have already
   been successfully applied to active-matter data-for example, deep neural
   networks to analyse images and track objects, and recurrent nets and
   random forests to analyse time series. Yet machine learning can also
   help to disentangle the complexity of biological active matter, helping,
   for example, to establish a relation between genetic code and emergent
   bacterial behaviour, to find navigation strategies in complex
   environments, and to map physical cues to animal behaviours. In this
   Review, we highlight the current state of the art in the application of
   machine learning to active matter and discuss opportunities and
   challenges that are emerging. We also emphasize how active matter and
   machine learning can work together for mutual benefit. This Review
   surveys machine learning techniques that are currently developed for a
   range of research topics in biological and artificial active matter and
   also discusses challenges and exciting opportunities. This research
   direction promises to help disentangle the complexity of active matter
   and gain fundamental insights for instance in collective behaviour of
   systems at many length scales from colonies of bacteria to animal
   flocks.},
DOI = {10.1038/s42256-020-0146-9},
EISSN = {2522-5839},
ResearcherID-Numbers = {Cichos, Frank/AAH-9041-2020
   Gustafsson, Kristian/ISA-7921-2023
   Volpe, Giovanni/B-1862-2008
   },
ORCID-Numbers = {Cichos, Frank/0000-0002-9803-4975
   Volpe, Giovanni/0000-0001-5057-1846
   Gustafsson, Kristian/0000-0002-6613-3821},
Unique-ID = {WOS:000571258300007},
}

@article{ WOS:000636526200001,
Author = {Manikandan, S. and Duraivelu, K.},
Title = {Fault diagnosis of various rotating equipment using machine learning
   approaches - A review},
Journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART E-JOURNAL OF
   PROCESS MECHANICAL ENGINEERING},
Year = {2021},
Volume = {235},
Number = {2},
Pages = {629-642},
Month = {APR},
Abstract = {Fault diagnosis of various rotating equipment plays a significant role
   in industries as it guarantees safety, reliability and prevents
   breakdown and loss of any source of energy. Early identification is a
   fundamental aspect for diagnosing the faults which saves both time and
   costs and in fact it avoids perilous conditions. Investigations are
   being carried out for intelligent fault diagnosis using machine learning
   approaches. This article analyses various machine learning approaches
   used for fault diagnosis of rotating equipment. In addition to this, a
   detailed study of different machine learning strategies which are
   incorporated on various rotating equipment in the context of fault
   diagnosis is also carried out. Mainly, the benefits and advance patterns
   of deep neural network which are applied to multiple components for
   fault diagnosis are inspected in this study. Finally, different
   algorithms are proposed to propagate the quality of fault diagnosis and
   the conceivable research ideas of applying machine learning approaches
   on various rotating equipment are condensed in this article.},
DOI = {10.1177/0954408920971976},
EarlyAccessDate = {NOV 2020},
Article-Number = {0954408920971976},
ISSN = {0954-4089},
EISSN = {2041-3009},
ResearcherID-Numbers = {K, Duraivelu/A-9631-2018
   K, Duraivelu/AHD-8072-2022},
ORCID-Numbers = {K, Duraivelu/0000-0003-1853-6965
   },
Unique-ID = {WOS:000636526200001},
}

@article{ WOS:000588277100001,
Author = {Turgeon, Stephanie and Lanovaz, Marc J.},
Title = {Tutorial: Applying Machine Learning in Behavioral Research},
Journal = {PERSPECTIVES ON BEHAVIOR SCIENCE},
Year = {2020},
Volume = {43},
Number = {4},
Pages = {697-723},
Month = {DEC},
Abstract = {Machine-learning algorithms hold promise for revolutionizing how
   educators and clinicians make decisions. However, researchers in
   behavior analysis have been slow to adopt this methodology to further
   develop their understanding of human behavior and improve the
   application of the science to problems of applied significance. One
   potential explanation for the scarcity of research is that machine
   learning is not typically taught as part of training programs in
   behavior analysis. This tutorial aims to address this barrier by
   promoting increased research using machine learning in behavior
   analysis. We present how to apply the random forest, support vector
   machine, stochastic gradient descent, and k-nearest neighbors algorithms
   on a small dataset to better identify parents of children with autism
   who would benefit from a behavior analytic interactive web training.
   These step-by-step applications should allow researchers to implement
   machine-learning algorithms with novel research questions and datasets.},
DOI = {10.1007/s40614-020-00270-y},
EarlyAccessDate = {NOV 2020},
ISSN = {2520-8969},
EISSN = {2520-8977},
ResearcherID-Numbers = {Lanovaz, Marc/K-9614-2019},
Unique-ID = {WOS:000588277100001},
}

@article{ WOS:000557303500009,
Author = {Nielsen, Ashley N. and Barch, Deanna M. and Petersen, Steven E. and
   Schlaggar, Bradley L. and Greene, Deanna J.},
Title = {Machine Learning With Neuroimaging: Evaluating Its Applications in
   Psychiatry},
Journal = {BIOLOGICAL PSYCHIATRY-COGNITIVE NEUROSCIENCE AND NEUROIMAGING},
Year = {2020},
Volume = {5},
Number = {8},
Pages = {791-798},
Month = {AUG},
Abstract = {Psychiatric disorders are complex, involving heterogeneous
   symptomatology and neurobiology that rarely involves the disruption of
   single, isolated brain structures. In an attempt to better describe and
   understand the complexities of psychiatric disorders, investigators have
   increasingly applied multivariate pattern classification approaches to
   neuroimaging data and in particular supervised machine learning methods.
   However, supervised machine learning approaches also come with unique
   challenges and trade-offs, requiring additional study design and
   interpretation considerations. The goal of this review is to provide a
   set of best practices for evaluating machine learning applications to
   psychiatric disorders. We discuss how to evaluate two common efforts: 1)
   making predictions that have the potential to aid in diagnosis,
   prognosis, and treatment and 2) interrogating the complex
   neurophysiological mechanisms underlying psychopathology. We focus here
   on machine learning as applied to functional connectivity with magnetic
   resonance imaging, as an example to ground discussion. We argue that for
   machine learning classification to have translational utility for
   individual-level predictions, investigators must ensure that the
   classification is clinically informative, independent of confounding
   variables, and appropriately assessed for both performance and
   generalizability. We contend that shedding light on the complex
   mechanisms underlying psychiatric disorders will require consideration
   of the unique utility, interpretability, and reliability of the
   neuroimaging features (e.g., regions, networks, connections) identified
   from machine learning approaches. Finally, we discuss how the rise of
   large, multisite, publicly available datasets may contribute to the
   utility of machine learning approaches in psychiatry.},
DOI = {10.1016/j.bpsc.2019.11.007},
ISSN = {2451-9022},
EISSN = {2451-9030},
ResearcherID-Numbers = {Schlaggar, Bradley/AAZ-4153-2020
   Greene, Deanna/AAB-5956-2019
   Barch, Deanna/G-8638-2013
   Petersen, Steve/ABF-1368-2020},
ORCID-Numbers = {Nielsen, Ashley/0000-0002-7148-8256
   },
Unique-ID = {WOS:000557303500009},
}

@article{ WOS:001283397300001,
Author = {Tian, Wei},
Title = {Towards advanced uncertainty and sensitivity analysis of building energy
   performance using machine learning techniques},
Journal = {JOURNAL OF BUILDING PERFORMANCE SIMULATION},
Year = {2024},
Volume = {17},
Number = {6},
Pages = {655-662},
Month = {NOV 1},
Abstract = {Uncertainty analysis quantifies the inherently uncertain nature of
   building energy performance, whereas sensitivity analysis identifies key
   factors to explain variations in building energy performance. With the
   ability to handle complex relationships, machine learning techniques
   offer an effective approach to more accurate and reliable uncertainty
   and sensitivity analysis. This paper provides valuable insights into the
   current state and future prospects of machine learning-based uncertainty
   and sensitivity analysis for building energy performance. The
   development of machine learning-based uncertainty analysis is discussed
   from three perspectives: observational data-based probabilistic
   prediction, surrogate model-based uncertainty quantification, and
   inverse uncertainty quantification. Variance-based sensitivity analysis
   using surrogate machine learning models decomposes output variance
   associated with each input. In contrast, machine learning-based variable
   importance refers to the change of model predictive performance using
   model-specific or model-agnostic approaches. Finally, future research
   directions on machine learning-based uncertainty and sensitivity
   analysis of building energy performance are presented.},
DOI = {10.1080/19401493.2024.2387071},
EarlyAccessDate = {AUG 2024},
ISSN = {1940-1493},
EISSN = {1940-1507},
ResearcherID-Numbers = {Tian, Wei/A-7888-2018},
ORCID-Numbers = {Tian, Wei/0000-0003-3447-2287},
Unique-ID = {WOS:001283397300001},
}

@article{ WOS:000428618900040,
Author = {Fong, Ruth C. and Scheirer, Walter J. and Cox, David D.},
Title = {Using human brain activity to guide machine learning},
Journal = {SCIENTIFIC REPORTS},
Year = {2018},
Volume = {8},
Month = {MAR 29},
Abstract = {Machine learning is a field of computer science that builds algorithms
   that learn. In many cases, machine learning algorithms are used to
   recreate a human ability like adding a caption to a photo, driving a
   car, or playing a game. While the human brain has long served as a
   source of inspiration for machine learning, little effort has been made
   to directly use data collected from working brains as a guide for
   machine learning algorithms. Here we demonstrate a new paradigm of
   ``neurally-weighted{''} machine learning, which takes fMRI measurements
   of human brain activity from subjects viewing images, and infuses these
   data into the training process of an object recognition learning
   algorithm to make it more consistent with the human brain. After
   training, these neurally-weighted classifiers are able to classify
   images without requiring any additional neural data. We show that our
   neural-weighting approach can lead to large performance gains when used
   with traditional machine vision features, as well as to significant
   improvements with already high-performing convolutional neural network
   features. The effectiveness of this approach points to a path forward
   for a new class of hybrid machine learning algorithms which take both
   inspiration and direct constraints from neuronal data.},
DOI = {10.1038/s41598-018-23618-6},
Article-Number = {5397},
ISSN = {2045-2322},
Unique-ID = {WOS:000428618900040},
}

@article{ WOS:000639523700001,
Author = {Gupta, Rohan and Srivastava, Devesh and Sahu, Mehar and Tiwari, Swati
   and Ambasta, Rashmi K. and Kumar, Pravir},
Title = {Artificial intelligence to deep learning: machine intelligence approach
   for drug discovery},
Journal = {MOLECULAR DIVERSITY},
Year = {2021},
Volume = {25},
Number = {3, SI},
Pages = {1315-1360},
Month = {AUG},
Abstract = {Drug designing and development is an important area of research for
   pharmaceutical companies and chemical scientists. However, low efficacy,
   off-target delivery, time consumption, and high cost impose a hurdle and
   challenges that impact drug design and discovery. Further, complex and
   big data from genomics, proteomics, microarray data, and clinical trials
   also impose an obstacle in the drug discovery pipeline. Artificial
   intelligence and machine learning technology play a crucial role in drug
   discovery and development. In other words, artificial neural networks
   and deep learning algorithms have modernized the area. Machine learning
   and deep learning algorithms have been implemented in several drug
   discovery processes such as peptide synthesis, structure-based virtual
   screening, ligand-based virtual screening, toxicity prediction, drug
   monitoring and release, pharmacophore modeling, quantitative
   structure-activity relationship, drug repositioning, polypharmacology,
   and physiochemical activity. Evidence from the past strengthens the
   implementation of artificial intelligence and deep learning in this
   field. Moreover, novel data mining, curation, and management techniques
   provided critical support to recently developed modeling algorithms. In
   summary, artificial intelligence and deep learning advancements provide
   an excellent opportunity for rational drug design and discovery process,
   which will eventually impact mankind. Graphic abstract The primary
   concern associated with drug design and development is time consumption
   and production cost. Further, inefficiency, inaccurate target delivery,
   and inappropriate dosage are other hurdles that inhibit the process of
   drug delivery and development. With advancements in technology,
   computer-aided drug design integrating artificial intelligence
   algorithms can eliminate the challenges and hurdles of traditional drug
   design and development. Artificial intelligence is referred to as
   superset comprising machine learning, whereas machine learning comprises
   supervised learning, unsupervised learning, and reinforcement learning.
   Further, deep learning, a subset of machine learning, has been
   extensively implemented in drug design and development. The artificial
   neural network, deep neural network, support vector machines,
   classification and regression, generative adversarial networks, symbolic
   learning, and meta-learning are examples of the algorithms applied to
   the drug design and discovery process. Artificial intelligence has been
   applied to different areas of drug design and development process, such
   as from peptide synthesis to molecule design, virtual screening to
   molecular docking, quantitative structure-activity relationship to drug
   repositioning, protein misfolding to protein-protein interactions, and
   molecular pathway identification to polypharmacology. Artificial
   intelligence principles have been applied to the classification of
   active and inactive, monitoring drug release, pre-clinical and clinical
   development, primary and secondary drug screening, biomarker
   development, pharmaceutical manufacturing, bioactivity identification
   and physiochemical properties, prediction of toxicity, and
   identification of mode of action.},
DOI = {10.1007/s11030-021-10217-3},
EarlyAccessDate = {APR 2021},
ISSN = {1381-1991},
EISSN = {1573-501X},
ResearcherID-Numbers = {Gupta, Rohan/JXW-7855-2024
   Ambasta, Rashmi/K-8611-2019
   Kumar, Pravir/AAR-1207-2020},
ORCID-Numbers = {Gupta, PhD, Dr. Rohan/0000-0002-6635-6558
   Kumar, Pravir/0000-0001-7444-2344
   Srivastava, Devesh/0000-0002-6644-3619
   Sahu, Mehar/0000-0003-1561-2440
   },
Unique-ID = {WOS:000639523700001},
}

@article{ WOS:000577262800006,
Author = {von Lilienfeld, O. Anatole and Burke, Kieron},
Title = {Retrospective on a decade of machine learning for chemical discovery},
Journal = {NATURE COMMUNICATIONS},
Year = {2020},
Volume = {11},
Number = {1},
Month = {SEP 29},
Abstract = {Over the last decade, we have witnessed the emergence of ever more
   machine learning applications in all aspects of the chemical sciences.
   Here, we highlight specific achievements of machine learning models in
   the field of computational chemistry by considering selected studies of
   electronic structure, interatomic potentials, and chemical compound
   space in chronological order.},
DOI = {10.1038/s41467-020-18556-9},
Article-Number = {4895},
EISSN = {2041-1723},
ResearcherID-Numbers = {von Lilienfeld, O./D-8529-2011},
Unique-ID = {WOS:000577262800006},
}

@article{ WOS:000928006100002,
Author = {Huang, Guang-Li and Zaslavsky, Arkady and Loke, Seng W. and Abkenar,
   Amin and Medvedev, Alexey and Hassani, Alireza},
Title = {Context-Aware Machine Learning for Intelligent Transportation Systems: A
   Survey},
Journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
Year = {2023},
Volume = {24},
Number = {1},
Pages = {17-36},
Month = {JAN},
Abstract = {Context awareness adds intelligence to and enriches data for
   applications, services and systems while enabling underlying algorithms
   to sense dynamic changes in incoming data streams. Context-aware machine
   learning is often adopted in intelligent services by endowing meaning to
   Internet of Things(IoT)/ubiquitous data. Intelligent transportation
   systems (ITS) are at the forefront of applying context awareness with
   marked success. In contrast to non-context-aware machine learning
   models, context-aware machine learning models often perform better in
   traffic prediction/classification and are capable of supporting complex
   and more intelligent ITS decision-making. This paper presents a
   comprehensive review of recent studies in context-aware machine learning
   for intelligent transportation, especially focusing on road
   transportation systems. State-of-the-art techniques are discussed from
   several perspectives, including contextual data (e.g., location, time,
   weather, road condition and events), applications (i.e., traffic
   prediction and decision making), modes (i.e., specialised and general),
   learning methods (e.g., supervised, unsupervised, semi-supervised and
   transfer learning). Two main frameworks of context-aware machine
   learning models are summarised. In addition, open challenges and future
   research directions of developing context-aware machine learning models
   for ITS are discussed, and a novel context-aware machine learning
   layered engine (CAMILLE) architecture is proposed as a potential
   solution to address identified gaps in the studied body of knowledge.},
DOI = {10.1109/TITS.2022.3216462},
ISSN = {1524-9050},
EISSN = {1558-0016},
ResearcherID-Numbers = {Loke, Seng Wai/AAG-3182-2020
   Loke, Seng/AAG-3182-2020
   Huang, Guang-Li/AED-6392-2022
   },
ORCID-Numbers = {HUANG, Guangli/0000-0001-8698-2946
   Loke, Seng Wai/0000-0002-5339-9305
   Zaslavsky, Arkady/0000-0003-1990-5734},
Unique-ID = {WOS:000928006100002},
}

@article{ WOS:001130213200001,
Author = {Gorment, Nor Zakiah and Selamat, Ali and Cheng, Lim Kok and Krejcar,
   Ondrej},
Title = {Machine Learning Algorithm for Malware Detection: Taxonomy, Current
   Challenges, and Future Directions},
Journal = {IEEE ACCESS},
Year = {2023},
Volume = {11},
Pages = {141045-141089},
Abstract = {Malware has emerged as a cyber security threat that continuously changes
   to target computer systems, smart devices, and extensive networks with
   the development of information technologies. As a result, malware
   detection has always been a major worry and a difficult issue, owing to
   shortcomings in performance accuracy, analysis type, and malware
   detection approaches that fail to identify unexpected malware attacks.
   This paper seeks to conduct a thorough systematic literature review
   (SLR) and offer a taxonomy of machine learning methods for malware
   detection that considers these problems by analyzing 77 chosen research
   works related to malware detection using machine learning algorithm. The
   research investigates malware and machine learning in the context of
   cybersecurity, including malware detection taxonomy and machine learning
   algorithm classification into numerous categories. Furthermore, the
   taxonomy was used to evaluate the most recent machine learning algorithm
   and analysis. The paper also examines the obstacles and associated
   concerns encountered in malware detection and potential remedies.
   Finally, to address the related issues that would motivate researchers
   in their future work, an empirical study was utilized to assess the
   performance of several machine learning algorithms.},
DOI = {10.1109/ACCESS.2023.3256979},
ISSN = {2169-3536},
ResearcherID-Numbers = {Gorment, Nor Zakiah/GLV-2358-2022
   Selamat, Ali/E-9645-2011
   Lim, Kok Cheng/GLV-2295-2022
   Krejcar, Ondrej/A-8639-2008
   GORMENT, NOR ZAKIAH/GLV-2358-2022},
ORCID-Numbers = {Gorment, Nor Zakiah/0000-0001-5596-6599
   },
Unique-ID = {WOS:001130213200001},
}

@article{ WOS:000668074700001,
Author = {Ngoc-Tri Ngo and Anh-Duc Pham and Thi Thu Ha Truong and Ngoc-Son Truong
   and Nhat-To Huynh and Tuan Minh Pham},
Title = {An Ensemble Machine Learning Model for Enhancing the Prediction Accuracy
   of Energy Consumption in Buildings},
Journal = {ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING},
Year = {2022},
Volume = {47},
Number = {4},
Pages = {4105-4117},
Month = {APR},
Abstract = {Predicting building energy use is necessary for energy planning,
   management, and conservation. It is difficult to achieve accurate
   prediction results due to the inherent complexity of building thermal
   characteristics and occupant behavior. Machine learning has been
   recently applied for predicting energy consumption. Improving its
   predictive accuracy and generalization ability is essential. Therefore,
   this study proposed a machine learning model for an ensemble approach to
   forecasting energy consumption in non-residential buildings. Various
   datasets from non-residential buildings were collected to assess the
   predictive performance. Artificial neural networks, support vector
   regression, and M5Rules models were used as baseline models in this
   study. Evaluation results have confirmed the effectiveness of the
   ensemble machine learning model in the next 24-h energy consumption
   prediction in buildings. The mean absolute error (MAE) and mean absolute
   percentage error (MAPE) obtained by the ensemble machine learning model
   were 2.858 kWh and 16.141 kWh, respectively. The ensemble machine
   learning model can improve the MAE by 123.4\% and the MAPE by 209.3\% as
   compared to baseline models. This study contributes to highlighting the
   advantages of machine learning applications for the building sector.
   Ensemble machine learning models can be proposed as an effective method
   for forecasting energy consumption in buildings.},
DOI = {10.1007/s13369-021-05927-7},
EarlyAccessDate = {JUN 2021},
ISSN = {2193-567X},
EISSN = {2191-4281},
ResearcherID-Numbers = {Truong, Thi Thu Ha/GYJ-6126-2022
   Pham, Tuan/AGG-6456-2022
   Pham, Anh/HSH-4888-2023},
ORCID-Numbers = {Truong, Ngoc-Son/0000-0003-4867-8980
   },
Unique-ID = {WOS:000668074700001},
}

@article{ WOS:000929540300001,
Author = {Wang, Hai and Chen, Shengnan},
Title = {Insights into the Application of Machine Learning in Reservoir
   Engineering: Current Developments and Future Trends},
Journal = {ENERGIES},
Year = {2023},
Volume = {16},
Number = {3},
Month = {FEB},
Abstract = {In the past few decades, the machine learning (or data-driven) approach
   has been broadly adopted as an alternative to scientific discovery,
   resulting in many opportunities and challenges. In the oil and gas
   sector, subsurface reservoirs are heterogeneous porous media involving a
   large number of complex phenomena, making their characterization and
   dynamic prediction a real challenge. This study provides a comprehensive
   overview of recent research that has employed machine learning in three
   key areas: reservoir characterization, production forecasting, and well
   test interpretation. The results show that machine learning can automate
   and accelerate many reservoirs engineering tasks with acceptable level
   of accuracy, resulting in more efficient and cost-effective decisions.
   Although machine learning presents promising results at this stage,
   there are still several crucial challenges that need to be addressed,
   such as data quality and data scarcity, the lack of physics nature of
   machine learning algorithms, and joint modelling of multiple data
   sources/formats. The significance of this research is that it
   demonstrates the potential of machine learning to revolutionize the oil
   and gas sector by providing more accurate and efficient solutions for
   challenging problems.},
DOI = {10.3390/en16031392},
Article-Number = {1392},
EISSN = {1996-1073},
ResearcherID-Numbers = {Wang, Hai/IWL-8195-2023},
ORCID-Numbers = {Chen, Shengnan/0000-0002-1704-1007
   Wang, Hai/0000-0002-0592-9834},
Unique-ID = {WOS:000929540300001},
}

@article{ WOS:000727217500001,
Author = {Serey, Joel and Quezada, Luis and Alfaro, Miguel and Fuertes, Guillermo
   and Vargas, Manuel and Ternero, Rodrigo and Sabattin, Jorge and Duran,
   Claudia and Gutierrez, Sebastian},
Title = {Artificial Intelligence Methodologies for Data Management},
Journal = {SYMMETRY-BASEL},
Year = {2021},
Volume = {13},
Number = {11},
Month = {NOV},
Abstract = {This study analyses the main challenges, trends, technological
   approaches, and artificial intelligence methods developed by new
   researchers and professionals in the field of machine learning, with an
   emphasis on the most outstanding and relevant works to date. This
   literature review evaluates the main methodological contributions of
   artificial intelligence through machine learning. The methodology used
   to study the documents was content analysis; the basic terminology of
   the study corresponds to machine learning, artificial intelligence, and
   big data between the years 2017 and 2021. For this study, we selected
   181 references, of which 120 are part of the literature review. The
   conceptual framework includes 12 categories, four groups, and eight
   subgroups. The study of data management using AI methodologies presents
   symmetry in the four machine learning groups: supervised learning,
   unsupervised learning, semi-supervised learning, and reinforced
   learning. Furthermore, the artificial intelligence methods with more
   symmetry in all groups are artificial neural networks, Support Vector
   Machines, K-means, and Bayesian Methods. Finally, five research avenues
   are presented to improve the prediction of machine learning.},
DOI = {10.3390/sym13112040},
Article-Number = {2040},
EISSN = {2073-8994},
ResearcherID-Numbers = {Fuertes, Guillermo/Q-2341-2016
   Sabattin, Jorge/P-2589-2019
   Duran, Claudia/S-1175-2019
   Quezada, Luis/W-8168-2019
   Serey, Joel/Q-7279-2019
   vargas, manuel/AAG-4780-2020},
ORCID-Numbers = {Fuertes, Guillermo/0000-0003-3044-5919
   Gutierrez Lillo, Sebastian/0000-0003-3714-0632
   Vargas Guzman, Manuel Eduardo/0000-0003-4161-6621
   Sabattin, Jorge/0000-0003-0270-6560
   Duran, Claudia/0000-0002-0903-4333
   },
Unique-ID = {WOS:000727217500001},
}

@article{ WOS:000640517400008,
Author = {Kao, Ying-Fang and Venkatachalam, Ragupathy},
Title = {Human and Machine Learning},
Journal = {COMPUTATIONAL ECONOMICS},
Year = {2021},
Volume = {57},
Number = {3, SI},
Pages = {889-909},
Month = {MAR},
Abstract = {In this paper, we consider learning by human beings and machines in the
   light of Herbert Simon's pioneering contributions to the theory of Human
   Problem Solving. Using board games of perfect information as a paradigm,
   we explore differences in human and machine learning in complex
   strategic environments. In doing so, we contrast theories of learning in
   classical game theory with computational game theory proposed by Simon.
   Among theories that invoke computation, we make a further distinction
   between computable and computational or machine learning theories. We
   argue that the modern machine learning algorithms, although impressive
   in terms of their performance, do not necessarily shed enough light on
   human learning. Instead, they seem to take us further away from Simon's
   lifelong quest to understand the mechanics of actual human behaviour.},
DOI = {10.1007/s10614-018-9803-z},
ISSN = {0927-7099},
EISSN = {1572-9974},
ORCID-Numbers = {Venkatachalam, Ragupathy/0000-0002-1190-6321},
Unique-ID = {WOS:000640517400008},
}

@article{ WOS:000760455600001,
Author = {Rozos, Evangelos and Dimitriadis, Panayiotis and Bellos, Vasilis},
Title = {Machine Learning in Assessing the Performance of Hydrological Models},
Journal = {HYDROLOGY},
Year = {2022},
Volume = {9},
Number = {1},
Month = {JAN},
Abstract = {Machine learning has been employed successfully as a tool virtually in
   every scientific and technological field. In hydrology, machine learning
   models first appeared as simple feed-forward networks that were used for
   short-term forecasting, and have evolved into complex models that can
   take into account even the static features of catchments, imitating the
   hydrological experience. Recent studies have found machine learning
   models to be robust and efficient, frequently outperforming the standard
   hydrological models (both conceptual and physically based). However, and
   despite some recent efforts, the results of the machine learning models
   require significant effort to interpret and derive inferences.
   Furthermore, all successful applications of machine learning in
   hydrology are based on networks of fairly complex topology that require
   significant computational power and CPU time to train. For these
   reasons, the value of the standard hydrological models remains
   indisputable. In this study, we suggest employing machine learning
   models not as a substitute for hydrological models, but as an
   independent tool to assess their performance. We argue that this
   approach can help to unveil the anomalies in catchment data that do not
   fit in the employed hydrological model structure or configuration, and
   to deal with them without compromising the understanding of the
   underlying physical processes.},
DOI = {10.3390/hydrology9010005},
Article-Number = {5},
EISSN = {2306-5338},
ResearcherID-Numbers = {Rozos, Evangelos/ABC-7215-2021
   },
ORCID-Numbers = {Bellos, Vasilis/0000-0002-6739-3836
   Rozos, Evangelos/0000-0002-3217-2283
   Dimitriadis, Panayiotis G./0000-0003-4956-8820},
Unique-ID = {WOS:000760455600001},
}

@article{ WOS:000918184200006,
Author = {Iniyan, S. and Varma, V. Akhil and Naidu, Ch Teja},
Title = {Crop yield prediction using machine learning techniques},
Journal = {ADVANCES IN ENGINEERING SOFTWARE},
Year = {2023},
Volume = {175},
Month = {JAN},
Abstract = {Machine Learning is a successful dynamic device for foreseeing crop
   yields, just as for choosing which harvests to plant and what to do
   about them during the developing season. Since it operates with a large
   amount of data produced by several variables, the farming system is
   highly complicated. Methods of machine learning can aid intelligent
   system decision-making. The following paper investigates a variety of
   methods for predicting crop yields using a variety of soil and
   environmental variables. The main purpose of this project is to make a
   machine learning model make predictions. By taking into account several
   variables, machine learning algorithms can help farmers decide which
   crop to grow in addition to increasing yield. Farmers can benefit from
   yield estimation because it allows them to minimize crop loss and obtain
   the best prices for their crops. A machine learning model may be
   descriptive or predictive, depending on the research question and study
   objectives.},
DOI = {10.1016/j.advengsoft.2022.103326},
EarlyAccessDate = {NOV 2022},
Article-Number = {103326},
ISSN = {0965-9978},
EISSN = {1873-5339},
ResearcherID-Numbers = {S, INIYAN/ABE-6984-2021
   },
ORCID-Numbers = {Chintha, Teja Naidu/0009-0001-1469-1477},
Unique-ID = {WOS:000918184200006},
}

@article{ WOS:000401789000024,
Author = {Cihan, Pinar and Gokce, Erhan and Kalipsiz, Oya},
Title = {A Review of Machine Learning Applications in Veterinary Field},
Journal = {KAFKAS UNIVERSITESI VETERINER FAKULTESI DERGISI},
Year = {2017},
Volume = {23},
Number = {4},
Pages = {673-680},
Month = {JUL-AUG},
Abstract = {Machine learning is a sub field of artificial intelligence which allows
   forecasting through learning past behaviors and rules from old data. In
   today's world, machine learning is being used almost in any fields such
   as education, medicine, veterinary, banking, telecommunication,
   security, and bio-medical sciences. In human health, although machine
   learning is generally preferred particularly in predicting diseases and
   identifying respective risk factors, it is obvious that there are a
   limited number of publications where this method was applied on
   veterinary or indicates whether it is correct and applicable. In this
   review, it was observed that the neural network, logistic regression,
   linear regression, multiple regression, principle component analysis and
   k-means methods were frequently used in examined publications and
   machine learning application in veterinary field upward momentum.
   Additionally, it was observed that recent developments in the field of
   machine learning (deep learning, ensemble learning, voice recognition,
   emotion recognition, etc.) is still new in the field of veterinary. In
   this review, publications are examined under clustering, classification,
   regression, multivariate data analysis and image processing topics. This
   review aims at providing basic information on machine learning and to
   increase the number of multidisciplinary publications on computer
   sciences/engineering and veterinary field.},
DOI = {10.9775/kvfd.2016.17281},
ISSN = {1300-6045},
ResearcherID-Numbers = {Cihan, Pınar/ABA-3520-2020},
Unique-ID = {WOS:000401789000024},
}

@article{ WOS:000441141200005,
Author = {An, Yuexuan and Ding, Shifei and Shi, Songhui and Li, Jingcan},
Title = {Discrete space reinforcement learning algorithm based on support vector
   machine classification},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2018},
Volume = {111},
Pages = {30-35},
Month = {AUG 1},
Abstract = {When facing discrete space learning problems, the traditional
   reinforcement learning algorithms often have the problems of slow
   convergence and poor convergence accuracy. Deep reinforcement learning
   needs a large number of learning samples in its learning process, so it
   often faces with the problems that the algorithm is difficult to
   converge and easy to fall into local minimums. In view of the above
   problems, we apply support vector machines classification to
   reinforcement learning, and propose an algorithm named Advantage
   Actor-Critic with Support Vector Machine Classification (SVM-A2C). Our
   algorithm adopts the actor-critic framework and uses the support vector
   machine classification as a result of the actor's action output, while
   Critic uses the advantage function to improve and optimize the
   parameters of support vector machine. In addition, since the environment
   is changing all the time in reinforcement learning, it is difficult to
   find a global optimal solution for the support vector machines, the
   gradient descent method is applied to optimize the parameters of support
   vector machine. So that the agent can quickly learn a more precise
   action selection policy. Finally, the effectiveness of the proposed
   method is proved by the classical experimental environment of
   reinforcement learning. It is proved that the algorithm proposed in this
   paper has shorter episodes to convergence and more accurate results than
   other algorithms. (C) 2018 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.patrec.2018.04.012},
ISSN = {0167-8655},
EISSN = {1872-7344},
ResearcherID-Numbers = {An, Yuexuan/OEO-2667-2025
   },
ORCID-Numbers = {An, Yuexuan/0000-0001-5510-4059
   Shi, Songhui/0000-0001-9723-0918
   Li, Jingcan/0000-0002-7840-8562},
Unique-ID = {WOS:000441141200005},
}

@article{ WOS:000342248100012,
Author = {Wang, Xinying and Han, Min},
Title = {Online sequential extreme learning machine with kernels for
   nonstationary time series prediction},
Journal = {NEUROCOMPUTING},
Year = {2014},
Volume = {145},
Pages = {90-97},
Month = {DEC 5},
Abstract = {In this paper, an online sequential extreme learning machine with
   kernels (OS-ELMK) has been proposed for nonstationary time series
   prediction. An online sequential learning algorithm, which can learn
   samples one-by-one or chunk-by-chunk, is developed for extreme learning
   machine with kernels. A limited memory prediction strategy based on the
   proposed OS-ELMK is designed to model the nonstationary time series.
   Performance comparisons of OS-ELMK with other existing algorithms are
   presented using artificial and real life nonstationary time series data.
   The results show that the proposed OS-ELMK produces similar or better
   accuracies with at least an order-of-magnitude reduction in the learning
   time. (C) 2014 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.neucom.2014.05.068},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Wang, Xing-yuan/I-6353-2015},
Unique-ID = {WOS:000342248100012},
}

@article{ WOS:000506280400001,
Author = {Zhang, Ge and Tang, Liqun and Liu, Zejia and Zhou, Licheng and Liu,
   Yiping and Jiang, Zhenyu},
Title = {Machine-learning-based damage identification methods with features
   derived from moving principal component analysis},
Journal = {MECHANICS OF ADVANCED MATERIALS AND STRUCTURES},
Year = {2020},
Volume = {27},
Number = {21},
Pages = {1789-1802},
Month = {NOV 2},
Abstract = {This paper aims to propose machine-learning-based damage identification
   methods with features derived from moving principal component analysis
   (MPCA) to improve the damage identification performance for engineering
   structures. Previously, machine learning algorithms have usually used
   structural responses as inputs directly. These methods show low damage
   identification capabilities and are susceptible to noise. In this paper,
   the eigenvectors of structural responses derived from MPCA are employed
   as inputs instead. Several traditional machine learning algorithms are
   applied for verification. The results demonstrate that as compared to
   strains and frequencies, their eigenvectors as inputs for machine
   learning algorithms render better performances for damage
   identification.},
DOI = {10.1080/15376494.2019.1710308},
EarlyAccessDate = {JAN 2020},
ISSN = {1537-6494},
EISSN = {1537-6532},
ResearcherID-Numbers = {Jiang, Zhenyu/B-4981-2011},
ORCID-Numbers = {Tang, Liqun/0000-0001-6037-1733
   Jiang, Zhenyu/0000-0001-8497-3405
   },
Unique-ID = {WOS:000506280400001},
}

@article{ WOS:000601113600025,
Author = {Peng, Jia and Muhammad, Ramzan and Wang, Shu-Liang and Zhong, Hai-Zheng},
Title = {How Machine Learning Accelerates the Development of Quantum
   Dots?†},
Journal = {CHINESE JOURNAL OF CHEMISTRY},
Year = {2021},
Volume = {39},
Number = {1},
Pages = {181-188},
Month = {JAN},
Abstract = {With the rapid developments in the field of information technology, the
   material research society is looking for an alternate scientific route
   to the traditional methods of trial and error in material research and
   process development. Machine learning emerges as a new research paradigm
   to accelerate the application-oriented material discovery. Quantum dots
   are expanded as functional nanomaterials to enhance cutting-edge
   photonic technology. However, they suffer from uncertainty in industrial
   fabrication and application. Here, we discuss how machine learning
   accelerates the development of quantum dots. The basic principles and
   operation procedures of machine learning are described with a few
   representative examples of quantum dots. We emphasize how machine
   learning contributes to the optimization of synthesis and the analysis
   of material characterizations. To conclude, we give a short perspective
   discussing the problems of combining machine learning and quantum dots.},
DOI = {10.1002/cjoc.202000393},
ISSN = {1001-604X},
EISSN = {1614-7065},
ResearcherID-Numbers = {WANG, Shuliang/A-2626-2012
   Ramzan, Muhammad/AAH-9959-2020
   shiying, zhang/JJD-0690-2023},
Unique-ID = {WOS:000601113600025},
}

@article{ WOS:000379822600001,
Author = {Wuest, Thorsten and Weimer, Daniel and Irgens, Christopher and Thoben,
   Klaus-Dieter},
Title = {Machine learning in manufacturing: advantages, challenges, and
   applications},
Journal = {PRODUCTION AND MANUFACTURING RESEARCH-AN OPEN ACCESS JOURNAL},
Year = {2016},
Volume = {4},
Number = {1},
Pages = {23-45},
Month = {JUN 24},
Abstract = {The nature of manufacturing systems faces ever more complex, dynamic and
   at times even chaotic behaviors. In order to being able to satisfy the
   demand for high-quality products in an efficient manner, it is essential
   to utilize all means available. One area, which saw fast pace
   developments in terms of not only promising results but also usability,
   is machine learning. Promising an answer to many of the old and new
   challenges of manufacturing, machine learning is widely discussed by
   researchers and practitioners alike. However, the field is very broad
   and even confusing which presents a challenge and a barrier hindering
   wide application. Here, this paper contributes in presenting an overview
   of available machine learning techniques and structuring this rather
   complicated area. A special focus is laid on the potential benefit, and
   examples of successful applications in a manufacturing environment.},
DOI = {10.1080/21693277.2016.1192517},
EISSN = {2169-3277},
ResearcherID-Numbers = {Thoben, Klaus-Dieter/L-1997-2013
   Wuest, Thorsten/A-2245-2014
   Wuest, Thorsten/AAL-4457-2020},
ORCID-Numbers = {Thoben, Klaus-Dieter/0000-0002-5911-805X
   Wuest, Thorsten/0000-0001-7457-7927
   },
Unique-ID = {WOS:000379822600001},
}

@article{ WOS:000500381500027,
Author = {Hegde, Jeevith and Rokseth, Borge},
Title = {Applications of machine learning methods for engineering risk assessment
   - A review},
Journal = {SAFETY SCIENCE},
Year = {2020},
Volume = {122},
Month = {FEB},
Abstract = {The purpose of this article is to present a structured review of
   publications utilizing machine learning methods to aid in engineering
   risk assessment. A keyword search is performed to retrieve relevant
   articles from the databases of Scopus and Engineering Village. The
   search results are filtered according to seven selection criteria. The
   filtering process resulted in the retrieval of one hundred and
   twenty-four relevant research articles. Statistics based on different
   categories from the citation database is presented. By reviewing the
   articles, additional categories, such as the type of machine learning
   algorithm used, the type of input source used, the type of industry
   targeted, the type of implementation, and the intended risk assessment
   phase are also determined. The findings show that the automotive
   industry is leading the adoption of machine learning algorithms for risk
   assessment. Artificial neural networks are the most applied machine
   learning method to aid in engineering risk assessment. Additional
   findings from the review process are also presented in this article.},
DOI = {10.1016/j.ssci.2019.09.015},
Article-Number = {104492},
ISSN = {0925-7535},
EISSN = {1879-1042},
Unique-ID = {WOS:000500381500027},
}

@article{ WOS:000510903200023,
Author = {Li, Xiang and Zhang, Wei and Ding, Qian and Li, Xu},
Title = {Diagnosing Rotating Machines With Weakly Supervised Data Using Deep
   Transfer Learning},
Journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS},
Year = {2020},
Volume = {16},
Number = {3},
Pages = {1688-1697},
Month = {MAR},
Abstract = {Rotating machinery fault diagnosis problems have been well-addressed
   when sufficient supervised data of the tested machine are available
   using the latest data-driven methods. However, it is still challenging
   to develop effective diagnostic method with insufficient training data,
   which is highly demanded in real-industrial scenarios, since
   high-quality data are usually difficult and expensive to collect.
   Considering the underlying similarities of rotating machines, data
   mining on different but related equipments potentially benefit the
   diagnostic performance on the target machine. Therefore, a novel
   transfer learning method for diagnostics based on deep learning is
   proposed in this article, where the diagnostic knowledge learned from
   sufficient supervised data of multiple rotating machines is transferred
   to the target equipment with domain adversarial training. Different from
   the existing studies, a more generalized transfer learning problem with
   different label spaces of domains is investigated, and different fault
   severities are also considered in fault diagnostics. The experimental
   results on four datasets validate the effectiveness of the proposed
   method, and show it is feasible and promising to explore different
   datasets to improve diagnostic performance.},
DOI = {10.1109/TII.2019.2927590},
ISSN = {1551-3203},
EISSN = {1941-0050},
ResearcherID-Numbers = {Zhang, Wei/GLQ-9882-2022
   Li, Xiang/W-6389-2019},
ORCID-Numbers = {Zhang, Wei/0000-0001-6478-3110
   Li, Xiang/0000-0003-0569-2176},
Unique-ID = {WOS:000510903200023},
}

@article{ WOS:001441035000001,
Author = {Krishna, Keshav},
Title = {Advancements in cache management: a review of machine learning
   innovations for enhanced performance and security},
Journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE},
Year = {2025},
Volume = {8},
Month = {FEB 25},
Abstract = {Machine learning techniques have emerged as a promising tool for
   efficient cache management, helping optimize cache performance and
   fortify against security threats. The range of machine learning is vast,
   from reinforcement learning-based cache replacement policies to Long
   Short-Term Memory (LSTM) models predicting content characteristics for
   caching decisions. Diverse techniques such as imitation learning,
   reinforcement learning, and neural networks are extensively useful in
   cache-based attack detection, dynamic cache management, and content
   caching in edge networks. The versatility of machine learning techniques
   enables them to tackle various cache management challenges, from
   adapting to workload characteristics to improving cache hit rates in
   content delivery networks. A comprehensive review of various machine
   learning approaches for cache management is presented, which helps the
   community learn how machine learning is used to solve practical
   challenges in cache management. It includes reinforcement learning, deep
   learning, and imitation learning-driven cache replacement in hardware
   caches. Information on content caching strategies and dynamic cache
   management using various machine learning techniques in cloud and edge
   computing environments is also presented. Machine learning-driven
   methods to mitigate security threats in cache management have also been
   discussed.},
DOI = {10.3389/frai.2025.1441250},
Article-Number = {1441250},
EISSN = {2624-8212},
Unique-ID = {WOS:001441035000001},
}

@article{ WOS:000641162100002,
Author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and
   Zhang, Chengqi and Yu, Philip S.},
Title = {A Comprehensive Survey on Graph Neural Networks},
Journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
Year = {2021},
Volume = {32},
Number = {1},
Pages = {4-24},
Month = {JAN},
Abstract = {Deep learning has revolutionized many machine learning tasks in recent
   years, ranging from image classification and video processing to speech
   recognition and natural language understanding. The data in these tasks
   are typically represented in the Euclidean space. However, there is an
   increasing number of applications, where data are generated from
   non-Euclidean domains and are represented as graphs with complex
   relationships and interdependency between objects. The complexity of
   graph data has imposed significant challenges on the existing machine
   learning algorithms. Recently, many studies on extending deep learning
   approaches for graph data have emerged. In this article, we provide a
   comprehensive overview of graph neural networks (GNNs) in data mining
   and machine learning fields. We propose a new taxonomy to divide the
   state-of-the-art GNNs into four categories, namely, recurrent GNNs,
   convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We
   further discuss the applications of GNNs across various domains and
   summarize the open-source codes, benchmark data sets, and model
   evaluation of GNNs. Finally, we propose potential research directions in
   this rapidly growing field.},
DOI = {10.1109/TNNLS.2020.2978386},
ISSN = {2162-237X},
EISSN = {2162-2388},
ResearcherID-Numbers = {Yu, Philip/A-2815-2012
   wu, zonghan/AAC-6262-2021
   Long, Guodong/T-3441-2019
   Pan, Shirui/K-6763-2018},
ORCID-Numbers = {xiaobing, lv/0009-0002-3773-0595
   Zhang, Chengqi/0000-0001-5715-7154
   LONG, GUODONG/0000-0003-3740-9515
   Wu, Zonghan/0000-0003-4450-7738
   Pan, Shirui/0000-0003-0794-527X},
Unique-ID = {WOS:000641162100002},
}

@article{ WOS:000572537200011,
Author = {Liu, Yingli and Niu, Chen and Wang, Zhuo and Gan, Yong and Zhu, Yan and
   Sun, Shuhong and Shen, Tao},
Title = {Machine learning in materials genome initiative: A review},
Journal = {JOURNAL OF MATERIALS SCIENCE \& TECHNOLOGY},
Year = {2020},
Volume = {57},
Pages = {113-122},
Month = {NOV 15},
Abstract = {Discovering new materials with excellent performance is a hot issue in
   the materials genome initiative. Traditional experiments and
   calculations often waste large amounts of time and money and are also
   limited by various conditions. Therefore, it is imperative to develop a
   new method to accelerate the discovery and design of new materials. In
   recent years, material discovery and design methods using machine
   learning have attracted much attention from material experts and have
   made some progress. This review first outlines available materials
   database and material data analytics tools and then elaborates on the
   machine learning algorithms used in materials science. Next, the field
   of application of machine learning in materials science is summarized,
   focusing on the aspects of structure determination, performance
   prediction, fingerprint prediction, and new material discovery. Finally,
   the review points out the problems of data and machine learning in
   materials science and points to future research. Using machine learning
   algorithms, the authors hope to achieve amazing results in material
   discovery and design. (C) 2020 Published by Elsevier Ltd on behalf of
   The editorial office of Journal of Materials Science \& Technology.},
DOI = {10.1016/j.jmst.2020.01.067},
ISSN = {1005-0302},
EISSN = {1941-1162},
ResearcherID-Numbers = {Gan, Yong/AAD-3732-2020
   Shen, Tao/JQJ-5691-2023},
Unique-ID = {WOS:000572537200011},
}

@article{ WOS:000521984900003,
Author = {Sendak, Mark P. and Gao, Michael and Brajer, Nathan and Balu, Suresh},
Title = {Presenting machine learning model information to clinical end users with
   model facts labels},
Journal = {NPJ DIGITAL MEDICINE},
Year = {2020},
Volume = {3},
Number = {1},
Month = {MAR 23},
Abstract = {There is tremendous enthusiasm surrounding the potential for machine
   learning to improve medical prognosis and diagnosis. However, there are
   risks to translating a machine learning model into clinical care and
   clinical end users are often unaware of the potential harm to patients.
   This perspective presents the ``Model Facts{''} label, a systematic
   effort to ensure that front-line clinicians actually know how, when, how
   not, and when not to incorporate model output into clinical decisions.
   The ``Model Facts{''} label was designed for clinicians who make
   decisions supported by a machine learning model and its purpose is to
   collate relevant, actionable information in 1-page. Practitioners and
   regulators must work together to standardize presentation of machine
   learning model information to clinical end users in order to prevent
   harm to patients. Efforts to integrate a model into clinical practice
   should be accompanied by an effort to clearly communicate information
   about a machine learning model with a ``Model Facts{''} label.},
DOI = {10.1038/s41746-020-0253-3},
Article-Number = {41},
ISSN = {2398-6352},
ORCID-Numbers = {Sendak, Mark/0000-0001-5828-4497},
Unique-ID = {WOS:000521984900003},
}

@article{ WOS:000823964300001,
Author = {Wilman, Wiktoria and Wrobel, Sonia and Bielska, Weronika and Deszynski,
   Piotr and Dudzic, Pawel and Jaszczyszyn, Igor and Kaniewski, Jedrzej and
   Mlokosiewicz, Jakub and Rouyan, Anahita and Satlawa, Tadeusz and Kumar,
   Sandeep and Greiff, Victor and Krawczyk, Konrad},
Title = {Machine-designed biotherapeutics: opportunities, feasibility and
   advantages of deep learning in computational antibody discovery},
Journal = {BRIEFINGS IN BIOINFORMATICS},
Year = {2022},
Volume = {23},
Number = {4},
Month = {JUL 18},
Abstract = {Antibodies are versatile molecular binders with an established and
   growing role as therapeutics. Computational approaches to developing and
   designing these molecules are being increasingly used to complement
   traditional lab-based processes. Nowadays, in silico methods fill
   multiple elements of the discovery stage, such as characterizing
   antibody-antigen interactions and identifying developability
   liabilities. Recently, computational methods tackling such problems have
   begun to follow machine learning paradigms, in many cases deep learning
   specifically. This paradigm shift offers improvements in established
   areas such as structure or binding prediction and opens up new
   possibilities such as language-based modeling of antibody repertoires or
   machine-learning-based generation of novel sequences. In this review, we
   critically examine the recent developments in (deep) machine learning
   approaches to therapeutic antibody design with implications for fully
   computational antibody design.},
DOI = {10.1093/bib/bbac267},
EarlyAccessDate = {JUL 2022},
ISSN = {1467-5463},
EISSN = {1477-4054},
ResearcherID-Numbers = {Greiff, Victor/I-7686-2019
   Kaniewski, Jędrzej/L-3771-2016
   Kumar, Sandeep/IUQ-2320-2023
   },
ORCID-Numbers = {Jaszczyszyn, Igor/0009-0003-3405-0372
   Bielska, Weronika/0009-0003-1311-5704
   Kumar, Sandeep/0000-0003-2840-6398},
Unique-ID = {WOS:000823964300001},
}

@article{ WOS:001026236800015,
Author = {Jerbi, Sofiene and Fiderer, Lukas J. and Poulsen Nautrup, Hendrik and
   Kuebler, Jonas M. and Briegel, Hans J. and Dunjko, Vedran},
Title = {Quantum machine learning beyond kernel methods},
Journal = {NATURE COMMUNICATIONS},
Year = {2023},
Volume = {14},
Number = {1},
Month = {JAN 31},
Abstract = {Machine learning algorithms based on parametrized quantum circuits are
   prime candidates for near-term applications on noisy quantum computers.
   In this direction, various types of quantum machine learning models have
   been introduced and studied extensively. Yet, our understanding of how
   these models compare, both mutually and to classical models, remains
   limited. In this work, we identify a constructive framework that
   captures all standard models based on parametrized quantum circuits:
   that of linear quantum models. In particular, we show using tools from
   quantum information theory how data re-uploading circuits, an apparent
   outlier of this framework, can be efficiently mapped into the simpler
   picture of linear models in quantum Hilbert spaces. Furthermore, we
   analyze the experimentally-relevant resource requirements of these
   models in terms of qubit number and amount of data needed to learn.
   Based on recent results from classical machine learning, we prove that
   linear quantum models must utilize exponentially more qubits than data
   re-uploading models in order to solve certain learning tasks, while
   kernel methods additionally require exponentially more data points. Our
   results provide a more comprehensive view of quantum machine learning
   models as well as insights on the compatibility of different models with
   NISQ constraints. Comparing the capabilities of different quantum
   machine learning protocols is difficult. Here, the authors show that
   different learning models based on parametrized quantum circuits can all
   be seen as quantum linear models, thus driving general conclusions on
   their resource requirements and capabilities.},
DOI = {10.1038/s41467-023-36159-y},
Article-Number = {517},
EISSN = {2041-1723},
ORCID-Numbers = {Briegel, Hans J/0000-0002-9065-1565
   Poulsen Nautrup, Hendrik/0000-0001-7815-7006
   Jerbi, Sofiene/0000-0001-8171-5742
   Fiderer, Lukas/0000-0002-9150-1024},
Unique-ID = {WOS:001026236800015},
}

@article{ WOS:000636768300001,
Author = {Castaldo, Rossana and Cavaliere, Carlo and Soricelli, Andrea and
   Salvatore, Marco and Pecchia, Leandro and Franzese, Monica},
Title = {Radiomic and Genomic Machine Learning Method Performance for Prostate
   Cancer Diagnosis: Systematic Literature Review},
Journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
Year = {2021},
Volume = {23},
Number = {4},
Month = {APR 1},
Abstract = {Background: Machine learning algorithms have been drawing attention at
   the joining of pathology and radiology in prostate cancer research.
   However, due to their algorithmic learning complexity and the
   variability of their architecture, there is an ongoing need to analyze
   their performance.
   Objective: This study assesses the source of heterogeneity and the
   performance of machine learning applied to radiomic, genomic, and
   clinical biomarkers for the diagnosis of prostate cancer. One research
   focus of this study was on clearly identifying problems and issues
   related to the implementation of machine learning in clinical studies.
   Methods: Following the PRISMA (Preferred Reporting Items for Systematic
   Reviews and Meta-Analyses) protocol, 816 titles were identified from the
   PubMed, Scopus, and OvidSP databases. Studies that used machine learning
   to detect prostate cancer and provided performance measures were
   included in our analysis. The quality of the eligible studies was
   assessed using the QUADAS-2 (quality assessment of diagnostic accuracy
   studies-version 2) tool. The hierarchical multivariate model was applied
   to the pooled data in a meta-analysis. To investigate the heterogeneity
   among studies, I-2 statistics were performed along with visual
   evaluation of coupled forest plots. Due to the internal heterogeneity
   among machine learning algorithms, subgroup analysis was carried out to
   investigate the diagnostic capability of machine learning systems in
   clinical practice.
   Results: In the final analysis, 37 studies were included, of which 29
   entered the meta-analysis pooling. The analysis of machine learning
   methods to detect prostate cancer reveals the limited usage of the
   methods and the lack of standards that hinder the implementation of
   machine learning in clinical applications.
   Conclusions: The performance of machine learning for diagnosis of
   prostate cancer was considered satisfactory for several studies
   investigating the multiparametric magnetic resonance imaging and urine
   biomarkers; however, given the limitations indicated in our study,
   further studies are warranted to extend the potential use of machine
   learning to clinical settings. Recommendations on the use of machine
   learning techniques were also provided to help researchers to design
   robust studies to facilitate evidence generation from the use of
   radiomic and genomic biomarkers.},
DOI = {10.2196/22394},
Article-Number = {e22394},
ISSN = {1438-8871},
ResearcherID-Numbers = {Pecchia, Leandro/AAF-7325-2019
   Salvatore, Marco/K-8083-2016
   Cavaliere, Carlo/K-6544-2016
   Franzese, Monica/AAK-4830-2020
   Soricelli, Andrea/C-1133-2009
   soricelli, andrea/C-1133-2009
   Castaldo, Rossana/AAB-7681-2019
   Pecchia, Leandro/P-6344-2015},
ORCID-Numbers = {Salvatore, Marco/0000-0001-9734-7702
   Cavaliere, Carlo/0000-0002-3297-2213
   Soricelli, Andrea/0000-0001-7011-7667
   Franzese, Monica/0000-0002-6490-7694
   Pecchia, Leandro/0000-0002-7900-5415},
Unique-ID = {WOS:000636768300001},
}

@article{ WOS:000498675500022,
Author = {Liu, Yun and Chen, Po-Hsuan Cameron and Krause, Jonathan and Peng, Lily},
Title = {How to Read Articles That Use Machine Learning Users' Guides to the
   Medical Literature},
Journal = {JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION},
Year = {2019},
Volume = {322},
Number = {18},
Pages = {1806-1816},
Month = {NOV 12},
Abstract = {In recent years, many new clinical diagnostic tools have been developed
   using complicated machine learning methods. Irrespective of how a
   diagnostic tool is derived, it must be evaluated using a 3-step process
   of deriving, validating, and establishing the clinical effectiveness of
   the tool. Machine learning-based tools should also be assessed for the
   type of machine learning model used and its appropriateness for the
   input data type and data set size. Machine learning models also
   generally have additional prespecified settings called hyperparameters,
   which must be tuned on a data set independent of the validation set. On
   the validation set, the outcome against which the model is evaluated is
   termed the reference standard. The rigor of the reference standard must
   be assessed, such as against a universally accepted gold standard or
   expert grading.},
DOI = {10.1001/jama.2019.16489},
ISSN = {0098-7484},
EISSN = {1538-3598},
ORCID-Numbers = {Liu, Yun/0000-0003-4079-8275},
Unique-ID = {WOS:000498675500022},
}

@article{ WOS:000530096900011,
Author = {Li, Jing and Kuang, Xiaohui and Lin, Shujie and Ma, Xu and Tang, Yi},
Title = {Privacy preservation for machine learning training and classification
   based on homomorphic encryption schemes},
Journal = {INFORMATION SCIENCES},
Year = {2020},
Volume = {526},
Pages = {166-179},
Month = {JUL},
Abstract = {In recent years, more and more machine learning algorithms depend on the
   cloud computing. When a machine learning system is trained or classified
   in the cloud environment, the cloud server obtains data from the user
   side. Then, the privacy of the data depends on the service provider, it
   is easy to induce the malicious acquisition and utilization of data. On
   the other hand, the attackers can detect the statistical characteristics
   of machine learning data and infer the parameters of machine learning
   model through reverse attacks. Therefore, it is urgent to design an
   effective encryption scheme to protect the data's privacy without
   breaking the performance of machine learning.
   In this paper, we propose a novel homomorphic encryption framework over
   non-abelian rings, and define the homomorphism operations in ciphertexts
   space. The scheme can achieve one-way security based on the Conjugacy
   Search Problem. After that, a homomorphic encryption was proposed over a
   matrix-ring. It supports real numbers encryption based on the
   homomorphism of 2-order displacement matrix coding function and achieves
   fast ciphertexts homomorphic comparison without decrypting any
   ciphetexts operations' intermediate result. Furthermore, we use the
   scheme to realize privacy preservation for machine learning training and
   classification in data ciphertexts environment. The analysis shows that
   our proposed schemes are efficient for encryption/decryption and
   homomorphic operations. (C) 2020 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.ins.2020.03.041},
ISSN = {0020-0255},
EISSN = {1872-6291},
Unique-ID = {WOS:000530096900011},
}

@article{ WOS:001411195100001,
Author = {Wyrembek, Mateusz and Baryannis, George and Brintrup, Alexandra},
Title = {Causal machine learning for supply chain risk prediction and
   intervention planning},
Journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
Year = {2025},
Volume = {63},
Number = {15},
Pages = {5629-5648},
Month = {AUG 3},
Abstract = {The ultimate goal for developing machine learning models in supply chain
   management is to make optimal interventions. However, most machine
   learning models identify correlations in data rather than inferring
   causation, making it difficult to systematically plan for better
   outcomes. In this article, we propose and evaluate the use of causal
   machine learning for developing supply chain risk intervention models,
   and demonstrate its use with a case study in supply chain risk
   management in the maritime engineering sector. Our findings highlight
   that causal machine learning enhances decision-making processes by
   identifying changes that can be achieved under different supply chain
   interventions, allowing `what-if' scenario planning. We therefore
   propose different machine learning developmental pathways for predicting
   risk and planning for interventions to minimise risk and outline key
   steps for supply chain researchers to explore causal machine learning
   and harness its capabilities.},
DOI = {10.1080/00207543.2025.2458121},
EarlyAccessDate = {JAN 2025},
ISSN = {0020-7543},
EISSN = {1366-588X},
ResearcherID-Numbers = {Baryannis, George/P-5884-2014
   Wyrembek, Mateusz/JVZ-2020-2024
   },
ORCID-Numbers = {Baryannis, George/0000-0002-2118-5812
   Wyrembek, Mateusz/0000-0002-7946-948X
   Brintrup, Alexandra/0000-0002-4189-2434},
Unique-ID = {WOS:001411195100001},
}

@article{ WOS:000705187000001,
Author = {Zheng, Jiahao and Cole, Tim and Zhang, Yuxin and Kim, Jeeson and Tang,
   Shi-Yang},
Title = {Exploiting machine learning for bestowing intelligence to microfluidics},
Journal = {BIOSENSORS \& BIOELECTRONICS},
Year = {2021},
Volume = {194},
Month = {DEC 15},
Abstract = {Intelligent microfluidics is an emerging cross-discipline research area
   formed by combining microfluidics with machine learning. It uses the
   advantages of microfluidics, such as high throughput and
   controllability, and the powerful data processing capabilities of
   machine learning, resulting in improved systems in biotechnology and
   chemistry. Compared to traditional microfluidics using manual analysis
   methods, intelligent microfluidics needs less human intervention, and
   results in a more user-friendly experience with faster processing. There
   is a paucity of literature reviewing this burgeoning and highly
   promising cross-discipline. Therefore, we herein comprehensively and
   systematically summarize several aspects of microfluidic applications
   enabled by machine learning. We list the types of microfluidics used in
   intelligent microfluidic applications over the last five years, as well
   as the machine learning algorithms and the hardware used for training.
   We also present the most recent advances in key technologies,
   developments, challenges, and the emerging opportunities created by
   intelligent microfluidics.},
DOI = {10.1016/j.bios.2021.113666},
EarlyAccessDate = {SEP 2021},
Article-Number = {113666},
ISSN = {0956-5663},
EISSN = {1873-4235},
ResearcherID-Numbers = {Zhang, Yuxin/Q-5453-2019
   Tang, Shiyang/I-3580-2018
   ZHENG, JIAHAO/NKQ-1016-2025},
ORCID-Numbers = {Zheng, Jiahao/0000-0001-7313-9941
   Zhang, Yuxin/0000-0001-8867-9350
   Cole, Tim/0000-0002-0221-1560
   Tang, Shiyang/0000-0002-3079-8880
   },
Unique-ID = {WOS:000705187000001},
}

@article{ WOS:000425010500008,
Author = {Varshney, Kush R. and Alemzadeh, Homa},
Title = {On the Safety of Machine Learning: Cyber-Physical Systems, Decision
   Sciences, and Data Products},
Journal = {BIG DATA},
Year = {2017},
Volume = {5},
Number = {3},
Pages = {246-255},
Month = {SEP},
Abstract = {Machine learning algorithms increasingly influence our decisions and
   interact with us in all parts of our daily lives. Therefore, just as we
   consider the safety of power plants, highways, and a variety of other
   engineered socio-technical systems, we must also take into account the
   safety of systems involving machine learning. Heretofore, the definition
   of safety has not been formalized in a machine learning context. In this
   article, we do so by defining machine learning safety in terms of risk,
   epistemic uncertainty, and the harm incurred by unwanted outcomes. We
   then use this definition to examine safety in all sorts of applications
   in cyber-physical systems, decision sciences, and data products. We find
   that the foundational principle of modern statistical machine learning,
   empirical risk minimization, is not always a sufficient objective. We
   discuss how four different categories of strategies for achieving safety
   in engineering, including inherently safe design, safety reserves, safe
   fail, and procedural safeguards can be mapped to a machine learning
   context. We then discuss example techniques that can be adopted in each
   category, such as considering interpretability and causality of
   predictive models, objective functions beyond expected prediction
   accuracy, human involvement for labeling difficult or rare examples, and
   user experience design of software and open data.},
DOI = {10.1089/big.2016.0051},
ISSN = {2167-6461},
EISSN = {2167-647X},
Unique-ID = {WOS:000425010500008},
}

@article{ WOS:000701874800010,
Author = {Alhajjar, Elie and Maxwell, Paul and Bastian, Nathaniel},
Title = {Adversarial machine learning in Network Intrusion Detection Systems},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2021},
Volume = {186},
Month = {DEC 30},
Abstract = {Adversarial examples are inputs to a machine learning system
   intentionally crafted by an attacker to fool the model into producing an
   incorrect output. These examples have achieved a great deal of success
   in several domains such as image recognition, speech recognition and
   spam detection. In this paper, we study the nature of the adversarial
   problem in Network Intrusion Detection Systems (NIDS). We focus on the
   attack perspective, which includes techniques to generate adversarial
   examples capable of evading a variety of machine learning models. More
   specifically, we explore the use of evolutionary computation (particle
   swarm optimization and genetic algorithm) and deep learning (generative
   adversarial networks) as tools for adversarial example generation. To
   assess the performance of these algorithms in evading a NIDS, we apply
   them to two publicly available data sets, namely the NSL-KDD and
   UNSW-NB15, and we contrast them to a baseline perturbation method: Monte
   Carlo simulation. The results show that our adversarial example
   generation techniques cause high misclassification rates in eleven
   different machine learning models, along with a voting classifier. Our
   work highlights the vulnerability of machine learning based NIDS in the
   face of adversarial perturbation.},
DOI = {10.1016/j.eswa.2021.115782},
EarlyAccessDate = {SEP 2021},
Article-Number = {115782},
ISSN = {0957-4174},
EISSN = {1873-6793},
ORCID-Numbers = {Maxwell, Paul/0000-0001-9055-9534
   Alhajjar, Elie/0000-0002-7500-1214
   Bastian, Nathaniel/0000-0001-9957-2778},
Unique-ID = {WOS:000701874800010},
}

@article{ WOS:000457664700017,
Author = {Raghuwanshi, Bhagat Singh and Shukla, Sanyam},
Title = {Generalized class-specific kernelized extreme learning machine for
   multiclass imbalanced learning},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2019},
Volume = {121},
Pages = {244-255},
Month = {MAY 1},
Abstract = {Class imbalanced learning is a well-known issue, which exists in
   real-world applications. Datasets that have skewed class distribution
   raise hindrance to the traditional learning algorithms. Traditional
   classifiers give the same importance to all the samples, which leads to
   the prediction biased towards the majority classes. To solve this
   intrinsic deficiency, numerous strategies have been proposed such as
   weighted extreme learning machine (WELM), weighted support vector
   machine (WSVM), class-specific extreme learning machine (CS-ELM) and
   class-specific kernelized extreme learning machine (CSKELM). This work
   focuses on multiclass imbalance problems, which are more difficult
   compared to the binary class imbalance problems. Kernelized extreme
   learning machine (KELM) yields better results compared to the
   traditional extreme learning machine (ELM), which uses random input
   parameters. This work presents a generalized CSKELM (GCSKELM), the
   extension of our recently proposed CSKELM, which addresses the
   multiclass imbalanced problems more effectively. The proposed GCSKELM
   can be applied directly to solve the multiclass imbalanced problems.
   GCSKELM with Gaussian kernel function avoids the non-optimal hidden node
   problem associated with CS-ELM and other existing variants of ELM. The
   proposed work also has less computational cost in contrast with
   kernelized WELM (KWELM) for multiclass imbalanced learning. This work
   employs class-specific regularization parameters, which are determined
   by employing class proportion. The extensive experimental analysis shows
   that the proposed work obtains promising generalization performance in
   contrast with the other state-of-the-art imbalanced learning methods.
   (C) 2018 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.eswa.2018.12.024},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {Raghuwanshi, Bhagat Singh/Y-2664-2019
   shukla, sanyam/C-3019-2017},
ORCID-Numbers = {Raghuwanshi, Bhagat Singh/0000-0002-3027-7831
   },
Unique-ID = {WOS:000457664700017},
}

@article{ WOS:001169793700001,
Author = {Elkateb, Sherien and Metwalli, Ahmed and Shendy, Abdelrahman and
   Abu-Elanien, Ahmed E. B.},
Title = {Machine learning and IoT - Based predictive maintenance approach for
   industrial applications},
Journal = {ALEXANDRIA ENGINEERING JOURNAL},
Year = {2024},
Volume = {88},
Pages = {298-309},
Month = {FEB},
Abstract = {Unplanned outage in industry due to machine failures can lead to
   significant production losses and increased maintenance costs.
   Predictive maintenance methods use the data collected from IoT-enabled
   devices installed in working machines to detect incipient faults and
   prevent major failures. In this study, a predictive maintenance system
   based on machine learning algorithms, specifically AdaBoost, is
   presented to classify different types of machines stops in real-time
   with application in knitting machines. The data collected from the
   machines include machine speeds and steps, which were pre-processed and
   fed into the machine learning model to classify six types of machines
   stops: gate stop, feeder stop, needle stop, completed roll stop, idle
   stop, and lycra stop. The model is trained and optimized using a
   combination of hyperparameter tuning and cross-validation techniques to
   achieve an accuracy of 92\% on the test set. The results demonstrate the
   potential of the proposed system to accurately classify machine stops
   and enable timely maintenance actions; thereby, improving the overall
   efficiency and productivity of the textile industry.},
DOI = {10.1016/j.aej.2023.12.065},
EarlyAccessDate = {JAN 2024},
ISSN = {1110-0168},
EISSN = {2090-2670},
ResearcherID-Numbers = {Abu Elanien, Ahmed/I-9062-2019
   },
ORCID-Numbers = {Metwalli, Ahmed/0000-0003-3109-3804},
Unique-ID = {WOS:001169793700001},
}

@article{ WOS:000663165500001,
Author = {Montiel, Jacob and Halford, Max and Mastelini, Saulo Martiello and
   Bolmier, Geoffrey and Sourty, Raphael and Vaysse, Robin and Zouitine,
   Adil and Gomes, Heitor Murilo and Read, Jesse and Abdessalem, Talel and
   Bifet, Albert},
Title = {River: machine learning for streaming data in Python},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2021},
Volume = {22},
Abstract = {River is a machine learning library for dynamic data streams and
   continual learning. It provides multiple state-of-the-art learning
   methods, data generators/transformers, performance metrics and
   evaluators for different stream learning problems. It is the result from
   the merger of two popular packages for stream learning in Python: Creme
   and scikit-multiflow. River introduces a revamped architecture based on
   the lessons learnt from the seminal packages. River's ambition is to be
   the go-to library for doing machine learning on streaming data.
   Additionally, this open source package brings under the same umbrella a
   large community of practitioners and researchers. The source code is
   available at https://github.com/online-ml/river.},
Article-Number = {110},
ISSN = {1532-4435},
ResearcherID-Numbers = {Gomes, Heitor/AAC-7287-2019
   Bifet, Albert/E-4984-2017
   Mastelini, Saulo/AAT-5762-2021
   Montiel, Jacob/AAH-8641-2020
   Bifet, Albert/ISA-9610-2023},
ORCID-Numbers = {Bifet, Albert/0000-0002-8339-7773
   Gomes, Heitor Murilo/0000-0002-5276-637X
   },
Unique-ID = {WOS:000663165500001},
}

@article{ WOS:000799624800014,
Author = {Sisejkovic, Dominik and Merchant, Farhad and Reimann, Lennart M. and
   Leupers, Rainer},
Title = {Deceptive Logic Locking for Hardware Integrity Protection Against
   Machine Learning Attacks},
Journal = {IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND
   SYSTEMS},
Year = {2022},
Volume = {41},
Number = {6},
Pages = {1716-1729},
Month = {JUN},
Abstract = {Logic locking has emerged as a prominent key-driven technique to protect
   the integrity of integrated circuits. However, novel
   machine-learning-based attacks have recently been introduced to
   challenge the security foundations of locking schemes. These attacks are
   able to recover a significant percentage of the key without having
   access to an activated circuit. This article address this issue through
   two focal points. First, we present a theoretical model to test locking
   schemes for key-related structural leakage that can be exploited by
   machine learning. Second, based on the theoretical model, we introduce
   D-MUX: a deceptive multiplexer-based logic-locking scheme that is
   resilient against structure-exploiting machine learning attacks. Through
   the design of D-MUX, we uncover a major fallacy in the existing
   multiplexer-based locking schemes in the form of a structural-analysis
   attack. Finally, an extensive cost evaluation of D-MUX is presented. To
   the best of our knowledge, D-MUX is the first machine-learning-resilient
   locking scheme capable of protecting against all known learning-based
   attacks. Hereby, the presented work offers a starting point for the
   design and evaluation of future-generation logic locking in the era of
   machine learning.},
DOI = {10.1109/TCAD.2021.3100275},
ISSN = {0278-0070},
EISSN = {1937-4151},
ResearcherID-Numbers = {Merchant, Farhad/AAC-7701-2019
   Šišejković, Dominik/ABB-6295-2020},
ORCID-Numbers = {Germek, Dominik/0000-0003-3812-727X
   Reimann, Lennart Michael/0009-0003-5825-2665
   },
Unique-ID = {WOS:000799624800014},
}

@article{ WOS:000883846400002,
Author = {Kuehl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and
   Martin, Dominik},
Title = {Human vs. supervised machine learning: Who learns patterns faster?},
Journal = {COGNITIVE SYSTEMS RESEARCH},
Year = {2022},
Volume = {76},
Pages = {78-92},
Month = {JAN},
Abstract = {The capabilities of supervised machine learning (SML), especially
   compared to human abilities, are being discussed in scientific research
   and in the usage of SML. This study provides an answer to how learning
   performance differs between humans and machines when there is limited
   training data. We have designed an experiment in which 44 humans and
   three different machine learning algorithms identify patterns in labeled
   training data and have to label instances according to the patterns they
   find. The results show a high dependency between performance and the
   underlying patterns of the task. Whereas humans perform relatively
   similarly across all patterns, machines show large performance
   differences for the various patterns in our experiment. After seeing 20
   instances in the experiment, human performance does not improve anymore,
   which we relate to theories of cognitive overload. Machines learn slower
   but can reach the same level or may even outperform humans in 2 of the 4
   of used patterns. However, machines need more instances compared to
   humans for the same results. The performance of machines is comparably
   lower for the other 2 patterns due to the difficulty of combining input
   features.},
DOI = {10.1016/j.cogsys.2022.09.002},
ISSN = {2214-4366},
EISSN = {1389-0417},
ORCID-Numbers = {Martin, Dominik/0000-0002-2166-3183
   Kuhl, Niklas/0000-0001-6750-0876},
Unique-ID = {WOS:000883846400002},
}

@article{ WOS:000534712200003,
Author = {Rajadurai, Hariharan and Gandhi, Usha Devi},
Title = {A stacked ensemble learning model for intrusion detection in wireless
   network},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2022},
Volume = {34},
Number = {18, SI},
Pages = {15387-15395},
Month = {SEP},
Abstract = {Intrusion detection pretended to be a major technique for revealing the
   attacks and guarantee the security on the network. As the data increases
   tremendously every year on the Internet, a single algorithm is not
   sufficient for the network security. Because, deploying a single
   learning approach may suffer from statistical, computational and
   representational issues. To eliminate these issues, this paper combines
   multiple machine learning algorithms called stacked ensemble learning,
   to detect the attacks in a better manner than conventional learning,
   where a single algorithm is used to identify the attacks. The stacked
   ensemble system has been taken the benchmark data set, NSL-KDD, to
   compare its performance with other popular machine learning algorithms
   such as ANN, CART, random forest, SVM and other machine learning methods
   proposed by researchers. The experimental results show that stacked
   ensemble learning is a proper technique for classifying attacks than
   other existing methods. And also, the proposed system shows better
   accuracy compare to other intrusion detection models.},
DOI = {10.1007/s00521-020-04986-5},
EarlyAccessDate = {MAY 2020},
ISSN = {0941-0643},
EISSN = {1433-3058},
ResearcherID-Numbers = {GANDHI, USHA DEVI/L-2227-2017
   RAJADURAI, HARIHARAN/ABF-6877-2020
   G, USHA/L-2227-2017},
ORCID-Numbers = {R, HARIHARAN/0000-0001-9970-366X
   GANDHI, USHA DEVI/0000-0002-0164-4372
   },
Unique-ID = {WOS:000534712200003},
}

@article{ WOS:000444816000005,
Author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and
   Dubrawski, Artur},
Title = {Machine Learning for the Developing World},
Journal = {ACM TRANSACTIONS ON MANAGEMENT INFORMATION SYSTEMS},
Year = {2018},
Volume = {9},
Number = {2},
Month = {SEP},
Abstract = {Researchers from across the social and computer sciences are
   increasingly using machine learning to study and address global
   development challenges. This article examines the burgeoning field of
   machine learning for the developing world (ML4D). First, we present a
   review of prominent literature. Next, we suggest best practices drawn
   from the literature for ensuring that ML4D projects are relevant to the
   advancement of development objectives. Finally, we discuss how
   developing world challenges can motivate the design of novel machine
   learning methodologies. This article provides insights into systematic
   differences between ML4D and more traditional machine learning
   applications. It also discusses how technical complications of ML4D can
   be treated as novel research questions, how ML4D can motivate new
   research directions, and where machine learning can be most useful.},
DOI = {10.1145/3210548},
Article-Number = {9},
ISSN = {2158-656X},
EISSN = {2158-6578},
Unique-ID = {WOS:000444816000005},
}

@article{ WOS:001025397400001,
Author = {Peng, Wei and Karimi Sadaghiani, Omid},
Title = {A review on the applications of machine learning and deep learning in
   agriculture section for the production of crop biomass raw materials},
Journal = {ENERGY SOURCES PART A-RECOVERY UTILIZATION AND ENVIRONMENTAL EFFECTS},
Year = {2023},
Volume = {45},
Number = {3},
Pages = {9178-9201},
Month = {AUG 1},
Abstract = {The application of biomass, as an energy resource, depends on four main
   steps of production, pre-treatment, bio-refinery, and upgrading. This
   work reviews Machine Learning applications in the biomass production
   step with focusing on agriculture crops. By investigating numerous
   related works, it is concluded that there is a considerable reviewing
   gap in collecting the applications of Machine Learning in crop biomass
   production. To fill this gap by the current work, the origin of biomass
   raw materials is explained, and the application of Machine Learning in
   this section is scrutinized. Then, the kinds and resources of biomass as
   well as the role of machine learning in these fields are reviewed.
   Meanwhile, the sustainable production of farming-origin biomass and the
   effective factors in this issue are explained, and the application of
   Machine Learning in these areas are surveyed. Summarily, after analysis
   of numerous papers, it is concluded that Machine Learning and Deep
   Learning are widely utilized in crop biomass production areas to enhance
   the crops production quantity, quality, and sustainability, improve the
   predictions, decrease the costs, and diminish the products losses.
   According to the statistical analysis, in 19\% of the studies conducted
   about the application of Machine Learning and Deep Learning in crop
   biomass raw materials, Artificial Neural Network (ANN) algorithm has
   been applied. Afterward, the Random Forest (RF) and Super Vector Machine
   (SVM) are the second and third most-utilized algorithms applied in 17\%
   and 15\% of studies, respectively. Meanwhile, 26\% of studies focused on
   the applications of Machine Learning and Deep Learning in the sugar
   crops. At the second and third places, the starchy crops and algae with
   23\% and 21\% received more attention of researchers in the utilization
   of Machine Learning and Deep Learning techniques.},
DOI = {10.1080/15567036.2023.2232322},
ISSN = {1556-7036},
EISSN = {1556-7230},
ResearcherID-Numbers = {Krimi Sadaghiani, omid/GLR-1274-2022},
Unique-ID = {WOS:001025397400001},
}

@article{ WOS:000621832300030,
Author = {Richardson, Adam and Mulder, Thomas van Florenstein and Vehbi, Tugrul},
Title = {Nowcasting GDP using machine-learning algorithms: A real-time assessment},
Journal = {INTERNATIONAL JOURNAL OF FORECASTING},
Year = {2021},
Volume = {37},
Number = {2},
Pages = {941-948},
Month = {APR-JUN},
Abstract = {Can machine-learning algorithms help central banks understand the
   current state of the economy? Our results say yes! We contribute to the
   emerging literature on forecasting macroeconomic variables using
   machine-learning algorithms by testing the nowcast performance of common
   algorithms in a full `real-time' setting-that is, with real-time
   vintages of New Zealand GDP growth (our target variable) and real-time
   vintages of around 600 predictors. Our results show that
   machine-learning algorithms are able to significantly improve over a
   simple autoregressive benchmark and a dynamic factor model. We also show
   that machine-learning algorithms have the potential to add value to, and
   in one case improve on, the official forecasts of the Reserve Bank of
   New Zealand. (C) 2020 International Institute of Forecasters. Published
   by Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.ijforecast.2020.10.005},
EarlyAccessDate = {FEB 2021},
ISSN = {0169-2070},
EISSN = {1872-8200},
Unique-ID = {WOS:000621832300030},
}

@article{ WOS:000616713000005,
Author = {Bhavsar, Kaustubh Arun and Abugabah, Ahed and Singla, Jimmy and AlZubi,
   Ahmad Ali and Bashir, Ali Kashif and Nikita},
Title = {A Comprehensive Review on Medical Diagnosis Using Machine Learning},
Journal = {CMC-COMPUTERS MATERIALS \& CONTINUA},
Year = {2021},
Volume = {67},
Number = {2},
Pages = {1997-2014},
Abstract = {The unavailability of sufficient information for proper diagnosis,
   incomplete or miscommunication between patient and the clinician, or
   among the healthcare professionals, delay or incorrect diagnosis, the
   fatigue of clinician, or even the high diagnostic complexity in limited
   time can lead to diagnostic errors. Diagnostic errors have adverse
   effects on the treatment of a patient. Unnecessary treatments increase
   the medical bills and deteriorate the health of a patient. Such
   diagnostic errors that harm the patient in various ways could be
   minimized using machine learning. Machine learning algorithms could be
   used to diagnose various diseases with high accuracy. The use of machine
   learning could assist the doctors in making decisions on time, and could
   also be used as a second opinion or supporting tool. This study aims to
   provide a comprehensive review of research articles published from the
   year 2015 to mid of the year 2020 that have used machine learning for
   diagnosis of various diseases. We present the various machine learning
   algorithms used over the years to diagnose various diseases. The results
   of this study show the distribution of machine learning methods by
   medical disciplines. Based on our review, we present future research
   directions that could be used to conduct further research.},
DOI = {10.32604/cmc.2021.014943},
ISSN = {1546-2218},
EISSN = {1546-2226},
ResearcherID-Numbers = {AlZubi, Ahmad/ABB-4190-2020
   Abugabah, Ahed/AAR-2323-2020
   Bashir, Ali Kashif/R-4015-2019},
ORCID-Numbers = {AlZubi, Ahmad/0000-0001-8477-8319
   },
Unique-ID = {WOS:000616713000005},
}

@article{ WOS:000792828100003,
Author = {Sholevar, Nima and Golroo, Amir and Esfahani, Sahand Roghani},
Title = {Machine learning techniques for pavement condition evaluation},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2022},
Volume = {136},
Month = {APR},
Abstract = {Pavement management systems play a significant role in country's economy
   since road authorities are concerned about preserving their priceless
   road assets for a longer time to save maintenance costs. An essential
   part of such systems is how to collect and analyze pavement condition
   data. This paper reviews the state-of-the-art techniques in pavement
   condition data evaluation using machine learning techniques, more
   specifically, the application of machine learning methods: image
   classification, object detection, and segmentation in pavement distress
   assessment is investigated. Furthermore, the pavement automated data
   collection tools and pavement condition indices have been reviewed from
   the lens of machine learning applications. The review concludes that the
   overall trends in pavement condition evaluation is to apply machine
   learning techniques although there are some limitations not only in
   detection of few pavement distresses with complicated patterns but also
   in indication of the severity and density of distresses leading to
   avenues for future research.},
DOI = {10.1016/j.autcon.2022.104190},
EarlyAccessDate = {MAR 2022},
Article-Number = {104190},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Golroo, Amir/M-4651-2018
   },
ORCID-Numbers = {Roghani Esfahani, Sahand/0000-0003-2391-2363},
Unique-ID = {WOS:000792828100003},
}

@article{ WOS:000642962200001,
Author = {Chen, Samuel Yen-Chi and Yoo, Shinjae},
Title = {Federated Quantum Machine Learning},
Journal = {ENTROPY},
Year = {2021},
Volume = {23},
Number = {4},
Month = {APR},
Abstract = {Distributed training across several quantum computers could
   significantly improve the training time and if we could share the
   learned model, not the data, it could potentially improve the data
   privacy as the training would happen where the data is located. One of
   the potential schemes to achieve this property is the federated learning
   (FL), which consists of several clients or local nodes learning on their
   own data and a central node to aggregate the models collected from those
   local nodes. However, to the best of our knowledge, no work has been
   done in quantum machine learning (QML) in federation setting yet. In
   this work, we present the federated training on hybrid quantum-classical
   machine learning models although our framework could be generalized to
   pure quantum machine learning model. Specifically, we consider the
   quantum neural network (QNN) coupled with classical pre-trained
   convolutional model. Our distributed federated learning scheme
   demonstrated almost the same level of trained model accuracies and yet
   significantly faster distributed training. It demonstrates a promising
   future research direction for scaling and privacy aspects.},
DOI = {10.3390/e23040460},
Article-Number = {460},
EISSN = {1099-4300},
ResearcherID-Numbers = {Aboagye, Dr. Samuel/HJP-5530-2023
   },
ORCID-Numbers = {Chen, Samuel Yen-Chi/0000-0003-0114-4826
   Yoo, Shinjae/0000-0003-4378-6448},
Unique-ID = {WOS:000642962200001},
}

@article{ WOS:001199751600001,
Author = {Zhang, Yuefei and Liu, Xuefei and Wang, Wentao},
Title = {Theoretical Calculation Assisted by Machine Learning Accelerate Optimal
   Electrocatalyst Finding for Hydrogen Evolution Reaction},
Journal = {CHEMELECTROCHEM},
Year = {2024},
Volume = {11},
Number = {13},
Month = {JUL 2},
Abstract = {Electrocatalytic hydrogen evolution reaction (HER) is a promising
   strategy to solve and mitigate the coming energy shortage and global
   environmental pollution. Searching for efficient electrocatalysts for
   HER remains challenging through traditional trial-and-error methods from
   numerous potential material candidates. Theoretical high throughput
   calculation assisted by machine learning is a possible method to screen
   excellent HER electrocatalysts effectively. This will pave the way for
   high-efficiency and low-price electrocatalyst findings. In this review,
   we comprehensively introduce the machine learning workflow and standard
   models for hydrogen reduction reactions. This mainly illustrates how
   machine learning is used in catalyst filtration and descriptor
   exploration. Subsequently, several applications, including surface
   electrocatalysts, two-dimensional (2D) electrocatalysts, and single/dual
   atom electrocatalysts using machine learning in electrocatalytic HER,
   are highlighted and introduced. Finally, the corresponding challenge and
   perspective for machine learning in electrocatalytic hydrogen reduction
   reactions are concluded. We hope this critical review can provide a
   comprehensive understanding of machine learning for HER catalyst design
   and guide the future theoretical and experimental investigation of HER
   catalyst findings.
   In this review, we comprehensively introduce the machine learning
   workflow and standard models for hydrogen reduction reactions (HER).
   This mainly illustrates how machine learning is used in catalyst
   filtration and descriptor exploration. Several applications, including
   surface electrocatalysts, two-dimensional electrocatalysts, and
   single/dual atom electrocatalysts using machine learning in
   electrocatalytic HER, are highlighted and introduced. image},
DOI = {10.1002/celc.202400084},
EarlyAccessDate = {APR 2024},
ISSN = {2196-0216},
ORCID-Numbers = {Wentao, Wang/0000-0003-4308-3515},
Unique-ID = {WOS:001199751600001},
}

@article{ WOS:000436866400004,
Author = {McQuillan, Dan},
Title = {People's Councils for Ethical Machine Learning},
Journal = {SOCIAL MEDIA + SOCIETY},
Year = {2018},
Volume = {4},
Number = {2},
Month = {MAY 2},
Abstract = {Machine learning is a form of knowledge production native to the era of
   big data. It is at the core of social media platforms and everyday
   interactions. It is also being rapidly adopted for research and
   discovery across academia, business, and government. This article will
   explores the way the affordances of machine learning itself, and the
   forms of social apparatus that it becomes a part of, will potentially
   erode ethics and draw us in to a drone-like perspective. Unconstrained
   machine learning enables and delimits our knowledge of the world in
   particular ways: the abstractions and operations of machine learning
   produce a ``view from above{''} whose consequences for both ethics and
   legality parallel the dilemmas of drone warfare. The family of machine
   learning methods is not somehow inherently bad or dangerous, nor does
   implementing them signal any intent to cause harm. Nevertheless, the
   machine learning assemblage produces a targeting gaze whose algorithms
   obfuscate the legality of its judgments, and whose iterations threaten
   to create both specific injustices and broader states of exception.
   Given the urgent need to provide some kind of balance before machine
   learning becomes embedded everywhere, this article proposes people's
   councils as a way to contest machinic judgments and reassert openness
   and discourse.},
DOI = {10.1177/2056305118768303},
Article-Number = {2056305118768300},
ISSN = {2056-3051},
ORCID-Numbers = {McQuillan, Dan/0000-0002-1975-1598},
Unique-ID = {WOS:000436866400004},
}

@article{ WOS:000702351700085,
Author = {Gao, Lu and Lu, Pan and Ren, Yihao},
Title = {A deep learning approach for imbalanced crash data in predicting
   highway-rail grade crossings accidents},
Journal = {RELIABILITY ENGINEERING \& SYSTEM SAFETY},
Year = {2021},
Volume = {216},
Month = {DEC},
Abstract = {Accurate accident prediction for highway-rail grade crossings (HRGCs) is
   critically important for assisting at-grade safety improvement decision
   making. Numerous machine-learning methods were developed focusing on
   predicting accidents and identifying contributing physical and
   operational characteristics. A more advanced deep learning-based model
   is explored as a more accurate means of predicting HRGC crashes compared
   to machine learning-based approaches. In particular, the prediction
   performance of the convolution neural network (CNN) model is compared to
   the most commonly used machine learning methods, such as decision tree
   (DT) and random forests (RF). A 19-year HRGCs data in North Dakota (ND)
   is used in this study. Training a machine learning model on an
   imbalanced data (e.g., unequal distribution of labeled data in accident
   and no-accident classes) introduce unique challenges for accurate
   prediction especially for minority class. In this paper, a resampling
   approach was used to address the imbalanced data issue. Various
   performance measurements are used to compare the models' prediction
   performance. The results indicate that resampling the imbalanced dataset
   significantly improves the recall rate. The results also show that the
   proposed deep learning-based approach which deepens the layer levels and
   adapts to the training dataset has better prediction performance
   compared to other machine learning-based methods.},
DOI = {10.1016/j.ress.2021.108019},
EarlyAccessDate = {SEP 2021},
Article-Number = {108019},
ISSN = {0951-8320},
EISSN = {1879-0836},
ResearcherID-Numbers = {Lu, Pan/N-3773-2014
   Gao, Lu/KIA-7316-2024},
ORCID-Numbers = {Ren, Yi Hao/0000-0002-9203-3525
   },
Unique-ID = {WOS:000702351700085},
}

@article{ WOS:001395051600001,
Author = {Zhou, Xiao and Guo, Shuyi and Xin, Kunlun and Tang, Zhenheng and Chu,
   Xiaowen and Fu, Guangtao},
Title = {Network embedding: The bridge between water distribution network
   hydraulics and machine learning},
Journal = {WATER RESEARCH},
Year = {2025},
Volume = {273},
Month = {APR 1},
Abstract = {Machine learning has been increasingly used to solve management problems
   of water distribution networks (WDNs). A critical research gap, however,
   remains in the effective incorporation of WDN hydraulic characteristics
   in machine learning. Here we present a new water distribution network
   embedding (WDNE) method that transforms the hydraulic relationships of
   WDN topology into a vector form to be best suited for machine learning
   algorithms. The nodal relationships are characterized by local
   structure, global structure and attribute information. A conjoint use of
   two deep auto-encoder embedding models ensures that the hydraulic
   relationships and attribute information are simultaneously preserved and
   are effectively utilized by machine learning models. WDNE provides a new
   way to bridge WDN hydraulics with machine learning. It is first applied
   to a pipe burst localization problem. The results show that it can
   increase the performance of machine learning algorithms, and enable a
   lightweight machine learning algorithm to achieve better accuracy with
   less training data compared with a deep learning method reported in the
   literature. Then, applications in node grouping problems show that WDNE
   enables machine learning algorithms to make use of WDN hydraulic
   information, and integrates WDN structural relationships to achieve
   better grouping results. The results highlight the potential of WDNE to
   enhance WDN management by improving the efficiency of machine learning
   models and broadening the range of solvable problems. Codes are
   available at https://github.com/ZhouGroupHFUT/WDNE},
DOI = {10.1016/j.watres.2024.123011},
EarlyAccessDate = {DEC 2024},
Article-Number = {123011},
ISSN = {0043-1354},
EISSN = {1879-2448},
ResearcherID-Numbers = {Fu, Guangtao/ABE-3874-2021
   Chu, Xiaowen/AAF-2857-2020
   },
ORCID-Numbers = {Fu, Guangtao/0000-0003-1045-9125
   XIN, Kunlun/0000-0001-5476-9374
   Guo, Shuyi/0000-0002-8984-3017},
Unique-ID = {WOS:001395051600001},
}

@article{ WOS:000929283300001,
Author = {Sahu, Santosh Kumar and Mokhade, Anil and Bokde, Neeraj Dhanraj},
Title = {An Overview of Machine Learning, Deep Learning, and Reinforcement
   Learning-Based Techniques in Quantitative Finance: Recent Progress and
   Challenges},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {3},
Month = {FEB},
Abstract = {Forecasting the behavior of the stock market is a classic but difficult
   topic, one that has attracted the interest of both economists and
   computer scientists. Over the course of the last couple of decades,
   researchers have investigated linear models as well as models that are
   based on machine learning (ML), deep learning (DL), reinforcement
   learning (RL), and deep reinforcement learning (DRL) in order to create
   an accurate predictive model. Machine learning algorithms can now
   extract high-level financial market data patterns. Investors are using
   deep learning models to anticipate and evaluate stock and foreign
   exchange markets due to the advantage of artificial intelligence. Recent
   years have seen a proliferation of the deep reinforcement learning
   algorithm's application in algorithmic trading. DRL agents, which
   combine price prediction and trading signal production, have been used
   to construct several completely automated trading systems or strategies.
   Our objective is to enable interested researchers to stay current and
   easily imitate earlier findings. In this paper, we have worked to
   explain the utility of Machine Learning, Deep Learning, Reinforcement
   Learning, and Deep Reinforcement Learning in Quantitative Finance (QF)
   and the Stock Market. We also outline potential future study paths in
   this area based on the overview that was presented before.},
DOI = {10.3390/app13031956},
Article-Number = {1956},
EISSN = {2076-3417},
ResearcherID-Numbers = {Mokhade, Anil/JPA-1171-2023
   Bokde, Neeraj/I-2621-2016
   Sahu, SantoshKumar/LKK-1746-2024},
ORCID-Numbers = {Sahu, Santosh Kumar/0000-0003-4054-6078
   Bokde, Neeraj/0000-0002-3493-9302
   },
Unique-ID = {WOS:000929283300001},
}

@article{ WOS:000805800300001,
Author = {Gupta, Brij Bhooshan and Gaurav, Akshat and Marin, Enrique Cano and
   Alhalabi, Wadee},
Title = {Novel Graph-Based Machine Learning Technique to Secure Smart Vehicles in
   Intelligent Transportation Systems},
Journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
Year = {2023},
Volume = {24},
Number = {8},
Pages = {8483-8491},
Month = {AUG},
Abstract = {Intelligent Transport Systems (ITS) is a developing technology that will
   significantly alter the driving experience. In such systems, smart
   vehicles and Road-Side Units (RSUs) communicate through the VANET.
   Safety apps use these data to identify and prevent hazardous situations
   in real-time. Detection of malicious nodes and attack traffic in
   Intelligent Transportation Systems (ITS) is a current research subject.
   Recently, researchers are proposing graph-based machine learning
   techniques to identify malicious users in the ITS environment, through
   which it is easy to analyze the network traffic and detect the malicious
   devices. Therefore, graph-based machine learning techniques could be a
   technique that efficiently detect malicious nodes in the ITS
   environment. In this context, this article aims to provide a technique
   for resolving authentication and security issues in ITS using
   lightweight cryptography and graph-based machine learning. Our solution
   uses the concepts of identity based authentication technique and
   graph-based machine learning in order to provide authentication and
   security to the smart vehicle in ITS. By authenticating smart vehicles
   in ITS and identifying various cyber threats, our proposed method
   substantially contributes to the development of intelligent
   transportation communication environment.},
DOI = {10.1109/TITS.2022.3174333},
EarlyAccessDate = {MAY 2022},
ISSN = {1524-9050},
EISSN = {1558-0016},
ResearcherID-Numbers = {Gupta, Brij/E-9813-2011
   Gupta, Brij/A-1155-2016
   Alhalabi, Wadee/C-2449-2015
   Cano-Marin, Enrique/GZM-2612-2022
   GAURAV, AKSHAT/AAW-4521-2021},
ORCID-Numbers = {Gupta, Brij/0000-0003-4929-4698
   GAURAV, AKSHAT/0000-0002-5796-9424
   Cano Marin, Enrique/0000-0002-7948-1657
   Alhalabi, Wadee/0000-0002-4505-7268
   },
Unique-ID = {WOS:000805800300001},
}

@article{ WOS:000700585500007,
Author = {Tsai, Chun-Wei and Chen, Yi-Ping and Tang, Tzu-Chieh and Luo, Yu-Chen},
Title = {An efficient parallel machine learning-based blockchain framework},
Journal = {ICT EXPRESS},
Year = {2021},
Volume = {7},
Number = {3},
Pages = {300-307},
Month = {SEP},
Abstract = {The unlimited possibilities of machine learning have been shown in
   several successful reports and applications. However, how to make sure
   that the searched results of a machine learning system are not tampered
   by anyone and how to prevent the other users in the same network
   environment from easily getting our private data are two critical
   research issues when we immerse into powerful machine learning-based
   systems or applications. This situation is just like other modern
   information systems that confront security and privacy issues. The
   development of blockchain provides us an alternative way to address
   these two issues. That is why some recent studies have attempted to
   develop machine learning systems with blockchain technologies or to
   apply machine learning methods to blockchain systems. To show what the
   combination of blockchain and machine learning is capable of doing, in
   this paper, we proposed a parallel framework to find out suitable
   hyperparameters of deep learning in a blockchain environment by using a
   metaheuristic algorithm. The proposed framework also takes into account
   the issue of communication cost, by limiting the number of information
   exchanges between miners and blockchain. (C) 2021 The Korean Institute
   of Communications and Information Sciences (KICS). Publishing services
   by Elsevier B.V.},
DOI = {10.1016/j.icte.2021.08.014},
EarlyAccessDate = {SEP 2021},
ISSN = {2405-9595},
ResearcherID-Numbers = {Tang, Tzu-Chieh/HTN-6908-2023
   Tsai, Chun-Wei/R-6389-2019},
Unique-ID = {WOS:000700585500007},
}

@article{ WOS:000866803100001,
Author = {Jiang, Xia and Xu, Chuhan},
Title = {Deep Learning and Machine Learning with Grid Search to Predict Later
   Occurrence of Breast Cancer Metastasis Using Clinical Data},
Journal = {JOURNAL OF CLINICAL MEDICINE},
Year = {2022},
Volume = {11},
Number = {19},
Month = {OCT},
Abstract = {Background: It is important to be able to predict, for each individual
   patient, the likelihood of later metastatic occurrence, because the
   prediction can guide treatment plans tailored to a specific patient to
   prevent metastasis and to help avoid under-treatment or over-treatment.
   Deep neural network (DNN) learning, commonly referred to as deep
   learning, has become popular due to its success in image detection and
   prediction, but questions such as whether deep learning outperforms
   other machine learning methods when using non-image clinical data remain
   unanswered. Grid search has been introduced to deep learning
   hyperparameter tuning for the purpose of improving its prediction
   performance, but the effect of grid search on other machine learning
   methods are under-studied. In this research, we take the empirical
   approach to study the performance of deep learning and other machine
   learning methods when using non-image clinical data to predict the
   occurrence of breast cancer metastasis (BCM) 5, 10, or 15 years after
   the initial treatment. We developed prediction models using the deep
   feedforward neural network (DFNN) methods, as well as models using nine
   other machine learning methods, including naive Bayes (NB), logistic
   regression (LR), support vector machine (SVM), LASSO, decision tree
   (DT), k-nearest neighbor (KNN), random forest (RF), AdaBoost (ADB), and
   XGBoost (XGB). We used grid search to tune hyperparameters for all
   methods. We then compared our feedforward deep learning models to the
   models trained using the nine other machine learning methods. Results:
   Based on the mean test AUC (Area under the ROC Curve) results, DFNN
   ranks 6th, 4th, and 3rd when predicting 5-year, 10-year, and 15-year
   BCM, respectively, out of 10 methods. The top performing methods in
   predicting 5-year BCM are XGB (1st), RF (2nd), and KNN (3rd). For
   predicting 10-year BCM, the top performers are XGB (1st), RF (2nd), and
   NB (3rd). Finally, for 15-year BCM, the top performers are SVM (1st), LR
   and LASSO (tied for 2nd), and DFNN (3rd). The ensemble methods RF and
   XGB outperform other methods when data are less balanced, while SVM, LR,
   LASSO, and DFNN outperform other methods when data are more balanced.
   Our statistical testing results show that at a significance level of
   0.05, DFNN overall performs comparably to other machine learning methods
   when predicting 5-year, 10-year, and 15-year BCM. Conclusions: Our
   results show that deep learning with grid search overall performs at
   least as well as other machine learning methods when using non-image
   clinical data. It is interesting to note that some of the other machine
   learning methods, such as XGB, RF, and SVM, are very strong competitors
   of DFNN when incorporating grid search. It is also worth noting that the
   computation time required to do grid search with DFNN is much more than
   that required to do grid search with the other nine machine learning
   methods.},
DOI = {10.3390/jcm11195772},
Article-Number = {5772},
EISSN = {2077-0383},
ResearcherID-Numbers = {Xu, Chuhan/OHU-7207-2025},
Unique-ID = {WOS:000866803100001},
}

@article{ WOS:000654535500001,
Author = {Bao, Yuequan and Li, Hui},
Title = {Machine learning paradigm for structural health monitoring},
Journal = {STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL},
Year = {2021},
Volume = {20},
Number = {4, SI},
Pages = {1353-1372},
Month = {JUL},
Abstract = {Structural health diagnosis and prognosis is the goal of structural
   health monitoring. Vibration-based structural health monitoring
   methodology has been extensively investigated. However, the conventional
   vibration-based methods find it difficult to detect damages of actual
   structures because of a high incompleteness in the monitoring
   information (the number of sensors is much fewer with respect to the
   number of degrees of freedom of a structure), intense uncertainties in
   the structural conditions and monitoring systems, and coupled effects of
   damage and environmental actions on modal parameters. It is a truth that
   the performance and conditions of a structure must be embedded in the
   monitoring data (vehicles, wind, etc.; acceleration, displacement, cable
   force, strain, images, videos, etc.). Therefore, there is a need to
   develop completely novel structural health diagnosis and prognosis
   methodology based on the various monitoring data. Machine learning
   provides the advanced mathematical frameworks and algorithms that can
   help discover and model the performance and conditions of a structure
   through deep mining of monitoring data. Thus, machine learning takes an
   opportunity to establish novel machine learning paradigm for structural
   health diagnosis and prognosis theory termed the machine learning
   paradigm for structural health monitoring. This article sheds light on
   principles for machine learning paradigm for structural health
   monitoring with some examples and reviews the existing challenges and
   open questions in this field.},
DOI = {10.1177/1475921720972416},
EarlyAccessDate = {NOV 2020},
Article-Number = {1475921720972416},
ISSN = {1475-9217},
EISSN = {1741-3168},
ResearcherID-Numbers = {Bao, Yuequan/H-7395-2012
   },
ORCID-Numbers = {Bao, Yi/0000-0002-0872-6514},
Unique-ID = {WOS:000654535500001},
}

@article{ WOS:000803735500004,
Author = {Wang, Chen and Peng, Guohua and De Baets, Bernard},
Title = {Embedding metric learning into an extreme learning machine for scene
   recognition},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2022},
Volume = {203},
Month = {OCT 1},
Abstract = {Metric learning can be very useful to improve the performance of a
   distance-dependent classifier. However, separating metric learning from
   the classifier learning possibly degenerates the performance, for
   instance in scene recognition, especially for some complicated scene
   images. To address this issue, we propose to embed metric learning into
   an extreme learning machine (EML-ELM) to tackle scene recognition.
   Specifically, metric learning is conducted to fully explore
   discriminative features by taking into account all label information,
   rendering samples of the same class more compact and those of different
   classes more separable. An extreme learning machine is employed as a
   classifier thanks to its effectiveness and fast learning process. By
   explicitly embedding metric learning into an extreme learning machine,
   we can jointly learn the discriminative features and an effective
   classifier in a unified framework, thereby improving the recognition
   performance for complicated scene images. Extensive experiments on four
   benchmark scene datasets demonstrate the competitive performance of
   EML-ELM in comparison with state-of-the-art methods.},
DOI = {10.1016/j.eswa.2022.117505},
EarlyAccessDate = {MAY 2022},
Article-Number = {117505},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {De Baets, Bernard/E-8877-2010},
ORCID-Numbers = {Wang, Chen/0000-0002-0666-6095
   De Baets, Bernard/0000-0002-3876-620X},
Unique-ID = {WOS:000803735500004},
}

@article{ WOS:000470880900001,
Author = {Lian, Wenqian and Wang, Sheng-Tao and Lu, Sirui and Huang, Yuanyuan and
   Wang, Fei and Yuan, Xinxing and Zhang, Wengang and Ouyang, Xiaolong and
   Wang, Xin and Huang, Xianzhi and He, Li and Chang, Xiuying and Deng,
   Dong-Ling and Duan, Luming},
Title = {Machine Learning Topological Phases with a Solid-State Quantum Simulator},
Journal = {PHYSICAL REVIEW LETTERS},
Year = {2019},
Volume = {122},
Number = {21},
Month = {MAY 31},
Abstract = {We report an experimental demonstration of a machine learning approach
   to identify exotic topological phases, with a focus on the
   three-dimensional chiral topological insulators. We show that the
   convolutional neural networks-a class of deep feed-forward artificial
   neural networks with widespread applications in machine learning-can be
   trained to successfully identify different topological phases protected
   by chiral symmetry from experimental raw data generated with a
   solid-state quantum simulator. Our results explicitly showcase the
   exceptional power of machine learning in the experimental detection of
   topological phases, which paves a way to study rich topological
   phenomena with the machine learning toolbox.},
DOI = {10.1103/PhysRevLett.122.210503},
Article-Number = {210503},
ISSN = {0031-9007},
EISSN = {1079-7114},
ResearcherID-Numbers = {Lu, Sirui/AAD-6461-2022
   Deng, Dong-Ling/K-2249-2017
   Yuan, Xinxinf/HHS-4769-2022
   },
ORCID-Numbers = {Huang, Xianzhi/0000-0003-4157-6033
   Wang, Shengtao/0000-0003-1403-5901
   Yuan, Xinxing/0000-0003-4465-6346},
Unique-ID = {WOS:000470880900001},
}

@article{ WOS:000639862400012,
Author = {Kansal, P. and Kumar, A. and Gangadharappa, M.},
Title = {Optimized Extreme Learning Machine for Intelligent Spectrum Sensing in
   5G systems},
Journal = {JOURNAL OF COMMUNICATIONS TECHNOLOGY AND ELECTRONICS},
Year = {2021},
Volume = {66},
Number = {3},
Pages = {322-332},
Month = {MAR},
Abstract = {A two-level learned distributed networking (LDN) structure that uses
   existing machine learning (ML) algorithms and the novel Optimized
   Extreme Learning Machine (OELM) algorithm to perform intelligent
   spectrum sensing for 5G systems has been proposed and implemented. This
   novel technique uses input vectors like received signal strength
   indicator, the distance between Cognitive Radio users and gateways, and
   energy vectors to train the model. Extreme Learning Machine optimized by
   BAT algorithm outperforms the existing Machine Learning techniques in
   terms of detection accuracy, false alarm, detection probability and
   cross validation curves at different SNR scenarios.},
DOI = {10.1134/S1064226921040045},
ISSN = {1064-2269},
EISSN = {1555-6557},
Unique-ID = {WOS:000639862400012},
}

@article{ WOS:000564994700001,
Author = {Proserpio, Davide and Hauser, John R. and Liu, Xiao and Amano, Tomomichi
   and Burnap, Alex and Guo, Tong and Lee, Dokyun (DK) and Lewis, Randall
   and Misra, Kanishka and Schwarz, Eric and Timoshenko, Artem and Xu,
   Lilei and Yoganarasimhan, Hema},
Title = {Soul and machine (learning)},
Journal = {MARKETING LETTERS},
Year = {2020},
Volume = {31},
Number = {4, SI},
Pages = {393-404},
Month = {DEC},
Abstract = {Machine learning is bringing us self-driving cars, medical diagnoses,
   and language translation, but how can machine learning help marketers
   improve marketing decisions? Machine learning models predict extremely
   well, are scalable to ``big data,{''} and are a natural fit to analyze
   rich media content, such as text, images, audio, and video. Examples of
   current marketing applications include identification of customer needs
   from online data, accurate prediction of consumer response to
   advertising, personalized pricing, and product recommendations. But
   without the human input and insight-the soul-the applications of machine
   learning are limited. To create competitive or cooperative strategies,
   to generate creative product designs, to be accurate for ``what-if{''}
   and ``but-for{''} applications, to devise dynamic policies, to advance
   knowledge, to protect consumer privacy, and avoid algorithm bias,
   machine learning needs a soul. The brightest future is based on the
   synergy of what the machine can do well and what humans do well. We
   provide examples and predictions for the future.},
DOI = {10.1007/s11002-020-09538-4},
EarlyAccessDate = {AUG 2020},
ISSN = {0923-0645},
EISSN = {1573-059X},
ResearcherID-Numbers = {Proserpio, Davide/MSW-0672-2025
   Liu, Xiao/ABH-6079-2020
   Guo, Tong/IXE-0074-2023
   Hauser, John/O-3046-2019
   Amano, Tomomichi/IUN-6479-2023
   Timoshenko, Artem/IXN-4676-2023},
ORCID-Numbers = {/0000-0002-5431-2136
   Guo, Tong/0000-0002-3171-2890
   },
Unique-ID = {WOS:000564994700001},
}

@article{ WOS:000651052400024,
Author = {Ding, Jiaqi and Xu, Nan and Manh Tien Nguyen and Qiao, Qi and Shi, Yao
   and He, Yi and Shao, Qing},
Title = {Machine learning for molecular thermodynamics},
Journal = {CHINESE JOURNAL OF CHEMICAL ENGINEERING},
Year = {2021},
Volume = {31},
Number = {SI},
Pages = {227-239},
Month = {MAR},
Abstract = {Thermodynamic properties of complex systems play an essential role in
   developing chemical engineering processes. It remains a challenge to
   predict the thermodynamic properties of complex systems in a wide range
   and describe the behavior of ions and molecules in complex systems.
   Machine learning emerges as a powerful tool to resolve this issue
   because it can describe complex relationships beyond the capacity of
   traditional mathematical functions. This minireview will summarize some
   fundamental concepts of machine learning methods and their applications
   in three aspects of the molecular thermodynamics using several examples.
   The first aspect is to apply machine learning methods to predict the
   thermodynamic properties of a broad spectrum of systems based on known
   data. The second aspect is to integer machine learning and molecular
   simulations to accelerate the discovery of materials. The third aspect
   is to develop machine learning force field that can eliminate the
   barrier between quantum mechanics and all-atom molecular dynamics
   simulations. The applications in these three aspects illustrate the
   potential of machine learning in molecular thermodynamics of chemical
   engineering. We will also discuss the perspective of the broad
   applications of machine learning in chemical engineering. (C) 2021 The
   Chemical Industry and Engineering Society of China, and Chemical
   Industry Press Co., Ltd. All rights reserved.},
DOI = {10.1016/j.cjche.2020.10.044},
ISSN = {1004-9541},
EISSN = {2210-321X},
ResearcherID-Numbers = {he, yi/JDC-6926-2023
   XU, nan/KDP-0628-2024
   Shi, Yujun/H-8790-2018},
ORCID-Numbers = {Ding, Jiaqi/0000-0003-4549-051X
   },
Unique-ID = {WOS:000651052400024},
}

@article{ WOS:001096766000001,
Author = {Wang, Yu and Tian, Hua-Ming},
Title = {Digital geotechnics: from data-driven site characterisation towards
   digital transformation and intelligence in geotechnical engineering},
Journal = {GEORISK-ASSESSMENT AND MANAGEMENT OF RISK FOR ENGINEERED SYSTEMS AND
   GEOHAZARDS},
Year = {2024},
Volume = {18},
Number = {1, SI},
Pages = {8-32},
Month = {JAN 2},
Abstract = {Geotechnical engineering is experiencing a paradigm shift towards
   digital transformation and intelligence, driven by Industry 4.0 and
   emerging digital technologies, such as machine learning. However,
   development and application of machine learning are relatively slow in
   geotechnical practice, because extensive training databases are a key to
   the success of machine learning, but geotechnical data are often small
   and ugly, leading to the difficulty in developing a suitable training
   database required for machine learning. In addition, convincing examples
   from real projects are rare that demonstrate the immediate added value
   of machine learning to geotechnical practices. To facilitate digital
   transformation and machine learning in geotechnical engineering, this
   study proposes to develop a project-specific training database that
   leverages on digital transformation of geotechnical workflow and
   reflects both project-specific data collected from various stages of the
   geotechnical workflow and domain knowledge in geotechnical practices,
   such as soil mechanics, numerical analysis principles, and prior
   engineering experience and judgment. A real ground improvement project
   is presented to illustrate the proposed method and demonstrate the added
   value of digital transformation and machine learning in geotechnical
   practices.},
DOI = {10.1080/17499518.2023.2278136},
EarlyAccessDate = {NOV 2023},
ISSN = {1749-9518},
EISSN = {1749-9526},
ResearcherID-Numbers = {Tian, Hua-Ming/MHQ-2852-2025},
ORCID-Numbers = {Tian, Hua-Ming/0000-0002-8987-928X
   },
Unique-ID = {WOS:001096766000001},
}

@article{ WOS:000794708000001,
Author = {Rahman, Atta-ur and Abbas, Sagheer and Gollapalli, Mohammed and Ahmed,
   Rashad and Aftab, Shabib and Ahmad, Munir and Khan, Muhammad Adnan and
   Mosavi, Amir},
Title = {Rainfall Prediction System Using Machine Learning Fusion for Smart
   Cities},
Journal = {SENSORS},
Year = {2022},
Volume = {22},
Number = {9},
Month = {MAY},
Abstract = {Precipitation in any form-such as rain, snow, and hail-can affect
   day-to-day outdoor activities. Rainfall prediction is one of the
   challenging tasks in weather forecasting process. Accurate rainfall
   prediction is now more difficult than before due to the extreme climate
   variations. Machine learning techniques can predict rainfall by
   extracting hidden patterns from historical weather data. Selection of an
   appropriate classification technique for prediction is a difficult job.
   This research proposes a novel real-time rainfall prediction system for
   smart cities using a machine learning fusion technique. The proposed
   framework uses four widely used supervised machine learning techniques,
   i.e., decision tree, Naive Bayes, K-nearest neighbors, and support
   vector machines. For effective prediction of rainfall, the technique of
   fuzzy logic is incorporated in the framework to integrate the predictive
   accuracies of the machine learning techniques, also known as fusion. For
   prediction, 12 years of historical weather data (2005 to 2017) for the
   city of Lahore is considered. Pre-processing tasks such as cleaning and
   normalization were performed on the dataset before the classification
   process. The results reflect that the proposed machine learning
   fusion-based framework outperforms other models.},
DOI = {10.3390/s22093504},
Article-Number = {3504},
EISSN = {1424-8220},
ResearcherID-Numbers = {Ahmad, Dr. Munir/F-7482-2018
   Aftab, Shabib/JDV-4960-2023
   Rahman, Atta/AAD-6541-2019
   Abbas, Dr.Sagheer/AAB-1365-2020
   Mosavi, Amir/I-7440-2018
   Othman, Rashad/GXZ-8135-2022
   Abbas, Sagheer/AAB-1365-2020
   Khan, Muhammad Adnan/ACJ-2841-2022
   Gollapalli, Mohammed/D-6021-2017
   },
ORCID-Numbers = {Ahmad, Dr. Munir/0000-0002-5240-0984
   Aftab, Shabib/0000-0002-7662-1394
   Rahman, Atta/0000-0001-6696-277X
   Abbas, Sagheer/0000-0001-5289-7831
   Khan, Muhammad Adnan/0000-0003-4854-9935
   Gollapalli, Dr. Mohammed Abdul Salam/0000-0002-7521-5757},
Unique-ID = {WOS:000794708000001},
}

@article{ WOS:001003548300006,
Author = {Radak, Mehran and Lafta, Haider Yabr and Fallahi, Hossein},
Title = {Machine learning and deep learning techniques for breast cancer
   diagnosis and classification: a comprehensive review of medical imaging
   studies},
Journal = {JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY},
Year = {2023},
Volume = {149},
Number = {12},
Pages = {10473-10491},
Month = {SEP},
Abstract = {BackgroundBreast cancer is a major public health concern, and early
   diagnosis and classification are critical for effective treatment.
   Machine learning and deep learning techniques have shown great promise
   in the classification and diagnosis of breast cancer.PurposeIn this
   review, we examine studies that have used these techniques for breast
   cancer classification and diagnosis, focusing on five groups of medical
   images: mammography, ultrasound, MRI, histology, and thermography. We
   discuss the use of five popular machine learning techniques, including
   Nearest Neighbor, SVM, Naive Bayesian Network, DT, and ANN, as well as
   deep learning architectures and convolutional neural
   networks.ConclusionOur review finds that machine learning and deep
   learning techniques have achieved high accuracy rates in breast cancer
   classification and diagnosis across various medical imaging modalities.
   Furthermore, these techniques have the potential to improve clinical
   decision-making and ultimately lead to better patient outcomes.},
DOI = {10.1007/s00432-023-04956-z},
EarlyAccessDate = {JUN 2023},
ISSN = {0171-5216},
EISSN = {1432-1335},
ResearcherID-Numbers = {Fallahi, Hossein/G-5653-2012
   },
ORCID-Numbers = {Fallahi, Hossein/0000-0002-8754-3491},
Unique-ID = {WOS:001003548300006},
}

@article{ WOS:000995071700009,
Author = {Gutierrez, Maria and Moraga, Ma Angeles and Garcia, Felix and Calero,
   Coral},
Title = {Green-IN Machine Learning at a Glance},
Journal = {COMPUTER},
Year = {2023},
Volume = {56},
Number = {6},
Pages = {35-43},
Month = {JUN},
Abstract = {The use of machine learning (ML) algorithms has an environmental impact
   to be fully considered. This article presents a green-in-driven approach
   to the development of ML models. The aim thereof is to meet operational
   requirements while ensuring a suitable tradeoff between
   performance/reliability and energy consumption.},
DOI = {10.1109/MC.2023.3254646},
ISSN = {0018-9162},
EISSN = {1558-0814},
ResearcherID-Numbers = {Calero, Coral/D-4319-2011
   Moraga/G-2615-2015
   García, Félix/D-4299-2011
   Gutierrez, Maria/NHQ-1880-2025
   Garcia, Felix/D-4299-2011
   Moraga, Mª/G-2615-2015},
ORCID-Numbers = {Calero, Coral/0000-0003-0728-4176
   Gonzalez Gutierrez, Maria/0000-0001-9100-2586
   Moraga/0000-0001-9165-7144
   Garcia, Felix/0000-0001-6460-0353
   },
Unique-ID = {WOS:000995071700009},
}

@article{ WOS:000615376800001,
Author = {Diveev, Askhat and Konstantinov, Sergey and Shmalko, Elizaveta and Dong,
   Ge},
Title = {Machine Learning Control Based on Approximation of Optimal Trajectories},
Journal = {MATHEMATICS},
Year = {2021},
Volume = {9},
Number = {3},
Month = {FEB},
Abstract = {The paper is devoted to an emerging trend in control-a machine learning
   control. Despite the popularity of the idea of machine learning, there
   are various interpretations of this concept, and there is an urgent need
   for its strict mathematical formalization. An attempt to formalize the
   concept of machine learning is presented in this paper. The concepts of
   an unknown function, work area, training set are introduced, and a
   mathematical formulation of the machine learning problem is presented.
   Based on the presented formulation, the concept of machine learning
   control is considered. One of the problems of machine learning control
   is the general synthesis of control. It implies finding a control
   function that depends on the state of the object, which ensures the
   achievement of the control goal with the optimal value of the quality
   criterion from any initial state of some admissible region. Supervised
   and unsupervised approaches to solving a problem based on symbolic
   regression methods are considered. As a computational example, a problem
   of general synthesis of optimal control for a spacecraft landing on the
   surface of the Moon is considered as supervised machine learning control
   with a training set.},
DOI = {10.3390/math9030265},
Article-Number = {265},
EISSN = {2227-7390},
ResearcherID-Numbers = {Diveev, Askhat/G-6939-2017
   Konstantinov, Sergey/AAB-6777-2019
   Dong, Ge/N-7866-2017
   Shmalko, Elizaveta/H-8788-2017
   },
ORCID-Numbers = {Dong, Ge/0000-0003-3950-222X
   Shmalko, Elizaveta/0000-0002-0149-9638
   Konstantinov, Sergey/0000-0003-3493-7865},
Unique-ID = {WOS:000615376800001},
}

@article{ WOS:000947665000001,
Author = {Li, Jiuxiang and Wang, Rufeng},
Title = {Machine Learning Adoption in Educational Institutions: Role of Internet
   of Things and Digital Educational Platforms},
Journal = {SUSTAINABILITY},
Year = {2023},
Volume = {15},
Number = {5},
Month = {MAR},
Abstract = {The ever-increasing development of information technologies has led to
   the adoption of advanced learning techniques. In this regard, e-learning
   and machine learning are two of the emerging instructional means for
   educational institutes. The current study investigates the role of the
   Internet of Things (IoT) and digital educational platforms (DEPs) in the
   adoption of machine learning. The present research additionally
   investigated the function of DEPs as mediators between IoT and machine
   learning adoption. The department chairs or heads of 310 departments at
   91 Chinese institutions provided the information. In order to analyze
   the data, we used SPSS 25.0 and SEM (structural equation modeling). The
   results demonstrated how crucial an impact IoT has on DEPs and the
   uptake of machine learning. DEPs directly affect machine learning
   adoption and also act as mediators. The findings also support the
   mediating role of DEPs in the IoT and machine learning adoption link.
   The current study contributes to both theory and practical management by
   examining how IoT is helpful for achieving machine learning adoption.
   Based on the responses of 91 educational departments, this is a unique
   study of the mechanisms to achieve machine learning adoption through IoT
   and DEPs.},
DOI = {10.3390/su15054000},
Article-Number = {4000},
EISSN = {2071-1050},
Unique-ID = {WOS:000947665000001},
}

@article{ WOS:001139775100136,
Author = {Tjaden, Jacob and Tjaden, Brian},
Title = {MLpronto: A tool for democratizing machine learning},
Journal = {PLOS ONE},
Year = {2023},
Volume = {18},
Number = {11},
Month = {NOV 30},
Abstract = {The democratization of machine learning is a popular and growing
   movement. In a world with a wealth of publicly available data, it is
   important that algorithms for analysis of data are accessible and usable
   by everyone. We present MLpronto, a system for machine learning analysis
   that is designed to be easy to use so as to facilitate engagement with
   machine learning algorithms. With its web interface, MLpronto requires
   no computer programming or machine learning background, and it normally
   returns results in a matter of seconds. As input, MLpronto takes a file
   of data to be analyzed. MLpronto then executes some of the more commonly
   used supervised machine learning algorithms on the data and reports the
   results of the analyses. As part of its execution, MLpronto generates
   computer programming code corresponding to its machine learning
   analysis, which it also supplies as output. Thus, MLpronto can be used
   as a no-code solution for citizen data scientists with no machine
   learning or programming background, as an educational tool for those
   learning about machine learning, and as a first step for those who
   prefer to engage with programming code in order to facilitate rapid
   development of machine learning projects. MLpronto is freely available
   for use at https://mlpronto.org/.},
DOI = {10.1371/journal.pone.0294924},
Article-Number = {e0294924},
EISSN = {1932-6203},
Unique-ID = {WOS:001139775100136},
}

@article{ WOS:001230482400001,
Author = {Faulconbridge, James R. and Sarwar, Atif and Spring, Martin},
Title = {Accommodating Machine Learning Algorithms in Professional Service Firms},
Journal = {ORGANIZATION STUDIES},
Year = {2024},
Volume = {45},
Number = {7, SI},
Month = {JUL},
Abstract = {Machine learning algorithms, as one form of artificial intelligence, are
   significant for professional work because they create the possibility
   for some predictions, interpretations and judgements that inform
   decision-making to be made by algorithms. However, little is known about
   whether it is possible to transform professional work to incorporate
   machine learning while also addressing negative responses from
   professionals whose work is changed by inscrutable algorithms. Through
   original empirical analysis of the effects of machine learning
   algorithms on the work of accountants and lawyers, this paper identifies
   the role of accommodating machine learning algorithms in professional
   service firms. Accommodating machine learning algorithms involves
   strategic responses that both justify adoption in the context of the
   possibilities and new contributions of machine learning algorithms and
   respond to the algorithms' limitations and opaque and inscrutable
   nature. The analysis advances understanding of the processes that enable
   or inhibit the cooperative adoption of artificial intelligence in
   professional service firms and develops insights relevant when examining
   the long-term impacts of machine learning algorithms as they become ever
   more sophisticated.},
DOI = {10.1177/01708406241252930},
EarlyAccessDate = {MAY 2024},
ISSN = {0170-8406},
EISSN = {1741-3044},
ResearcherID-Numbers = {Sarwar, Atif/NRB-5422-2025
   Faulconbridge, James/JZE-5154-2024
   },
ORCID-Numbers = {Faulconbridge, James/0000-0003-1809-4271
   Sarwar, Atif/0000-0001-5411-0991
   Spring, Martin/0000-0003-2488-2039},
Unique-ID = {WOS:001230482400001},
}

@article{ WOS:000573712500001,
Author = {Lucas, Tim C. D.},
Title = {A translucent box: interpretable machine learning in ecology},
Journal = {ECOLOGICAL MONOGRAPHS},
Year = {2020},
Volume = {90},
Number = {4},
Month = {NOV},
Abstract = {Machine learning has become popular in ecology but its use has remained
   restricted to predicting, rather than understanding, the natural world.
   Many researchers consider machine learning algorithms to be a black box.
   These models can, however, with careful examination, be used to inform
   our understanding of the world. They are translucent boxes. Furthermore,
   the interpretation of these models can be an important step in building
   confidence in a model or in a specific prediction from a model. Here I
   review a number of techniques for interpreting machine learning models
   at the level of the system, the variable, and the individual prediction
   as well as methods for handling non-independent data. I also discuss the
   limits of interpretability for different methods and demonstrate these
   approaches using a case example of understanding litter sizes in
   mammals.},
DOI = {10.1002/ecm.1422},
EarlyAccessDate = {SEP 2020},
ISSN = {0012-9615},
EISSN = {1557-7015},
ORCID-Numbers = {Lucas, Tim/0000-0003-4694-8107},
Unique-ID = {WOS:000573712500001},
}

@article{ WOS:000756609900013,
Author = {Grundler, Klaus and Krieger, Tommy},
Title = {Using Machine Learning for measuring democracy: A practitioners guide
   and a new updated dataset for 186 countries from 1919 to 2019},
Journal = {EUROPEAN JOURNAL OF POLITICAL ECONOMY},
Year = {2021},
Volume = {70},
Month = {DEC},
Abstract = {We provide a comprehensive overview of the literature on the measurement
   of democracy and present an extensive update of the Machine Learning
   indicator of Grundler and Krieger (2016). Four improvements are
   particularly notable: First, we produce a continuous and a dichotomous
   version of the Machine Learning democracy indicator. Second, we
   calculate intervals that reflect the degree of measurement uncertainty.
   Third, we refine the conceptualization of the Machine Learning Index.
   Finally, we significantly expand the data coverage by providing
   democracy indices for 186 countries in the period from 1919 to 2019.},
DOI = {10.1016/j.ejpoleco.2021.102047},
EarlyAccessDate = {NOV 2021},
Article-Number = {102047},
ISSN = {0176-2680},
EISSN = {1873-5703},
ResearcherID-Numbers = {Krieger, Tommy/V-6276-2019
   },
ORCID-Numbers = {Krieger, Tommy/0000-0003-3110-9714},
Unique-ID = {WOS:000756609900013},
}

@article{ WOS:000877748800001,
Author = {Paturi, Uma Maheshwera Reddy and Palakurthy, Sai Teja and Reddy, N. S.},
Title = {The Role of Machine Learning in Tribology: A Systematic Review},
Journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING},
Year = {2023},
Volume = {30},
Number = {2},
Pages = {1345-1397},
Month = {MAR},
Abstract = {The machine learning (ML) approach, motivated by artificial intelligence
   (AI), is an inspiring mathematical algorithm that accurately simulates
   many engineering processes. Machine learning algorithms solve nonlinear
   and complex relationships through data training; additionally, they can
   infer previously unknown relationships, allowing for a simplified model
   and estimation of hidden data. Unlike other statistical tools, machine
   learning does not impose process parameter restrictions and yields an
   accurate association between input and output parameters. Tribology is a
   branch of surface science concerned with studying and managing friction,
   lubrication, and wear on relatively interacting surfaces. While AI-based
   machine learning approaches have been adopted in tribology applications,
   modern tribo-contact simulation requires a deliberate decomposition of
   complex design challenges into simpler sub-threads, thereby identifying
   the relationships between the numerous interconnected features and
   processes. Numerous studies have established that artificial
   intelligence techniques can accurately model tribological processes and
   their properties based on various process parameters. The primary
   objective of this review is to conduct a thorough examination of the
   role of machine learning in tribological research and pave the way for
   future researchers by providing a specific research direction. In terms
   of future research directions and developments, the expanded application
   of artificial intelligence and various machine learning methods in
   tribology has been emphasized, including the characterization and design
   of complex tribological systems. Additionally, by combining machine
   learning methods with tribological experimental data, interdisciplinary
   research can be conducted to understand efficient resource utilization
   and resource conservation better. At the conclusion of this article, a
   detailed discussion of the limitations and future research opportunities
   associated with implementing various machine learning algorithms in
   tribology and its interdisciplinary fields is presented.},
DOI = {10.1007/s11831-022-09841-5},
EarlyAccessDate = {NOV 2022},
ISSN = {1134-3060},
EISSN = {1886-1784},
ResearcherID-Numbers = {Paturi, UmaMaheshwera Reddy/S-5643-2019
   N. S., Reddy/G-8286-2012
   P, Uma Maheshwera Reddy/S-5643-2019},
ORCID-Numbers = {Paturi, UmaMaheshwera Reddy/0000-0001-5843-466X
   },
Unique-ID = {WOS:000877748800001},
}

@article{ WOS:000955359500001,
Author = {Albahra, Samer and Gorbett, Tom and Robertson, Scott and D'Aleo, Giana
   and Ockunzzi, Samuel and Lallo, Daniel and Hu, Bo and Rashidi, Hooman H.},
Title = {Artificial intelligence and machine learning overview in pathology \&
   laboratory medicine: A general review of data preprocessing and basic
   supervised concepts},
Journal = {SEMINARS IN DIAGNOSTIC PATHOLOGY},
Year = {2023},
Volume = {40},
Number = {2},
Pages = {71-87},
Month = {MAR},
Abstract = {Machine learning (ML) is becoming an integral aspect of several domains
   in medicine. Yet, most pathologists and laboratory professionals remain
   unfamiliar with such tools and are unprepared for their inevitable
   integration. To bridge this knowledge gap, we present an overview of key
   elements within this emerging data science discipline. First, we will
   cover general, well-established concepts within ML, such as data type
   concepts, data preprocessing methods, and ML study design. We will
   describe common supervised and unsupervised learning algorithms and
   their associated common machine learning terms (provided within a
   comprehensive glossary of terms that are discussed within this review).
   Overall, this review will offer a broad overview of the key concepts and
   algorithms in machine learning, with a focus on pathology and laboratory
   medicine. The objective is to provide an updated useful reference for
   those new to this field or those who require a refresher.},
DOI = {10.1053/j.semdp.2023.02.002},
EarlyAccessDate = {MAR 2023},
ISSN = {0740-2570},
EISSN = {1930-1111},
ResearcherID-Numbers = {Hu, Ming/MIK-0824-2025
   },
ORCID-Numbers = {Albahra, Samer/0000-0001-5184-747X},
Unique-ID = {WOS:000955359500001},
}

@article{ WOS:000737752600001,
Author = {Jin, Yabin and He, Liangshu and Wen, Zhihui and Mortazavi, Bohayra and
   Guo, Hongwei and Torrent, Daniel and Djafari-Rouhani, Bahram and
   Rabczuk, Timon and Zhuang, Xiaoying and Li, Yan},
Title = {Intelligent on-demand design of phononic metamaterials},
Journal = {NANOPHOTONICS},
Year = {2022},
Volume = {11},
Number = {3},
Pages = {439-460},
Month = {JAN 25},
Abstract = {With the growing interest in the field of artificial materials, more
   advanced and sophisticated functionalities are required from phononic
   crystals and acoustic metamaterials. This implies a high computational
   effort and cost, and still the efficiency of the designs may be not
   sufficient. With the help of third-wave artificial intelligence
   technologies, the design schemes of these materials are undergoing a new
   revolution. As an important branch of artificial intelligence, machine
   learning paves the way to new technological innovations by stimulating
   the exploration of structural design. Machine learning provides a
   powerful means of achieving an efficient and accurate design process by
   exploring nonlinear physical patterns in high-dimensional space, based
   on data sets of candidate structures. Many advanced machine learning
   algorithms, such as deep neural networks, unsupervised manifold
   clustering, reinforcement learning and so forth, have been widely and
   deeply investigated for structural design. In this review, we summarize
   the recent works on the combination of phononic metamaterials and
   machine learning. We provide an overview of machine learning on
   structural design. Then discuss machine learning driven on-demand design
   of phononic metamaterials for acoustic and elastic waves functions,
   topological phases and atomic-scale phonon properties. Finally, we
   summarize the current state of the art and provide a prospective of the
   future development directions.},
DOI = {10.1515/nanoph-2021-0639},
EarlyAccessDate = {JAN 2022},
ISSN = {2192-8606},
EISSN = {2192-8614},
ResearcherID-Numbers = {Torrent, Daniel/K-1903-2012
   Jin, Yabin/A-9555-2012
   Zhuang, Xiaoying/G-4754-2011
   Guo, Hongwei/LIF-9233-2024
   Mortazavi, Bohayra/AAA-9972-2022
   liang, YU/IYT-4334-2023},
ORCID-Numbers = {Torrent, Daniel/0000-0001-7092-7360
   Mortazavi, Bohayra/0000-0003-3031-5057
   Zhuang, Xiaoying/0000-0001-6562-2618
   He, Liangshu/0000-0001-8282-0886
   Jin, Yabin/0000-0002-6991-8827
   Wen, Zhihui/0000-0002-1252-1327
   liang, YU/0009-0007-3922-3454},
Unique-ID = {WOS:000737752600001},
}

@article{ WOS:001088970300001,
Author = {Latif, Sarmad Dashti and Hazrin, Nur Alyaa Binti and Koo, Chai Hoon and
   Ng, Jing Lin and Chaplot, Barkha and Huang, Yuk Feng and El-Shafie,
   Ahmed and Ahmed, Ali Najah},
Title = {Assessing rainfall prediction models: Exploring the advantages of
   machine learning and remote sensing approaches},
Journal = {ALEXANDRIA ENGINEERING JOURNAL},
Year = {2023},
Volume = {82},
Pages = {16-25},
Month = {NOV 1},
Abstract = {Using a comparison of three different major types, the best predictive
   model was determined. Statistical models and machine learning algorithms
   automatically learn and improve based on data. Deep learning uses neural
   networks to learn complex data patterns and relationships. A combination
   of satellite imagery, radar data, and ground-based observations are used
   and using aircraft or satellites, and remote sensing (RS) collects data
   on distant objects or locations. Satellites and radar are used to gather
   regional precipitation data for hybrid models. An algorithm trained on
   historical rainfall measurements would then process the data. Using
   remote monitoring instrument input features, the machine-learning model
   can predict precipitation. Evaluation of machine learning regression
   methods is based on the degree of agreement between predicted and
   observed values. The RMSE, R2, and MAE statistical measures check on the
   precision of a prediction or forecasting model. Machine learning excels
   at rainfall prediction regardless of climate or timescale. As one of the
   more popular models for predicting rainfall, the LSTM models demonstrate
   their superiority. Remote sensing and hybrid predictive models should be
   investigated further due to their scarcity.},
DOI = {10.1016/j.aej.2023.09.060},
EarlyAccessDate = {SEP 2023},
ISSN = {1110-0168},
EISSN = {2090-2670},
ResearcherID-Numbers = {Huang, Yuk/J-5265-2016
   AHMED, Ali/J-9456-2017
   Latif, Sarmad/ABD-4755-2020
   Ng, Jing Lin/AAE-2518-2021
   Koo, Chai/L-2240-2017
   El-Shafie, Ahmed/J-1799-2014
   Chaplot, Barkha/AAZ-4815-2020
   Huang, Yuk Feng/J-5265-2016},
ORCID-Numbers = {Latif, Sarmad Dashti/0000-0002-0417-3545
   El-Shafie, Ahmed/0000-0001-5018-8505
   Huang, Yuk Feng/0000-0001-6565-947X},
Unique-ID = {WOS:001088970300001},
}

@article{ WOS:000509281100001,
Author = {Orru, Graziella and Monaro, Merylin and Conversano, Ciro and Gemignani,
   Angelo and Sartori, Giuseppe},
Title = {Machine Learning in Psychometrics and Psychological Research},
Journal = {FRONTIERS IN PSYCHOLOGY},
Year = {2020},
Volume = {10},
Month = {JAN 10},
Abstract = {Recent controversies about the level of replicability of behavioral
   research analyzed using statistical inference have cast interest in
   developing more efficient techniques for analyzing the results of
   psychological experiments. Here we claim that complementing the
   analytical workflow of psychological experiments with Machine
   Learning-based analysis will both maximize accuracy and minimize
   replicability issues. As compared to statistical inference, ML analysis
   of experimental data is model agnostic and primarily focused on
   prediction rather than inference. We also highlight some potential
   pitfalls resulting from adoption of Machine Learning based experiment
   analysis. If not properly used it can lead to over-optimistic accuracy
   estimates similarly observed using statistical inference. Remedies to
   such pitfalls are also presented such and building model based on cross
   validation and the use of ensemble models. ML models are typically
   regarded as black boxes and we will discuss strategies aimed at
   rendering more transparent the predictions.},
DOI = {10.3389/fpsyg.2019.02970},
Article-Number = {2970},
ISSN = {1664-1078},
ResearcherID-Numbers = {Monaro, Merylin/AAM-3066-2021
   Sartori, Giuseppe/B-8140-2008
   Conversano, Ciro/ABE-1007-2020
   Gemignani, Angelo/AAC-3308-2022
   Orru, Graziella/AAT-2410-2020},
ORCID-Numbers = {CONVERSANO, CIRO/0000-0002-0726-3634
   Orru, Graziella/0000-0002-8769-7693
   gemignani, angelo/0000-0001-7249-874X
   },
Unique-ID = {WOS:000509281100001},
}

@article{ WOS:000695342100003,
Author = {Hu, Kai and Li, Yaogen and Xia, Min and Wu, Jiasheng and Lu, Meixia and
   Zhang, Shuai and Weng, Liguo},
Title = {Federated Learning: A Distributed Shared Machine Learning Method},
Journal = {COMPLEXITY},
Year = {2021},
Volume = {2021},
Month = {AUG 30},
Abstract = {Federated learning (FL) is a distributed machine learning (ML)
   framework. In FL, multiple clients collaborate to solve traditional
   distributed ML problems under the coordination of the central server
   without sharing their local private data with others. This paper mainly
   sorts out FLs based on machine learning and deep learning. First of all,
   this paper introduces the development process, definition, architecture,
   and classification of FL and explains the concept of FL by comparing it
   with traditional distributed learning. Then, it describes typical
   problems of FL that need to be solved. On the basis of classical FL
   algorithms, several federated machine learning algorithms are briefly
   introduced, with emphasis on deep learning and classification and
   comparisons of those algorithms are carried out. Finally, this paper
   discusses possible future developments of FL based on deep learning.},
DOI = {10.1155/2021/8261663},
Article-Number = {8261663},
ISSN = {1076-2787},
EISSN = {1099-0526},
ResearcherID-Numbers = {Xia, Min/AAC-7472-2019
   Hu, Kai/AAC-1429-2022
   Weng, Liguo/JOK-8060-2023
   },
ORCID-Numbers = {Xia, Min/0000-0003-4681-9129
   Hu, Kai/0000-0001-7181-9935
   Zhang, Shuai/0000-0001-5193-424X
   lu, mei xia/0000-0003-4492-3934
   Yaogen, Li/0000-0001-7745-1713},
Unique-ID = {WOS:000695342100003},
}

@article{ WOS:000423046300023,
Author = {Yan, Deqin and Chu, Yonghe and Zhang, Haiying and Liu, Deshan},
Title = {Information discriminative extreme learning machine},
Journal = {SOFT COMPUTING},
Year = {2018},
Volume = {22},
Number = {2, SI},
Pages = {677-689},
Month = {JAN},
Abstract = {Extreme learning machine (ELM) has become one of the new research
   hotspots in the field of pattern recognition and machine learning.
   However, the existing extreme learning machine algorithms cannot better
   use identification information of data. Aiming at solving this problem,
   we propose a regularized extreme learning machine (algorithm) based on
   discriminative information (called IELM). In order to evaluate and
   verify the effectiveness of the proposed method, experiments use widely
   used image data sets. The comparative experimental results show that the
   proposed algorithm in the paper can significantly improve the
   classification performance and generalization ability of ELM.},
DOI = {10.1007/s00500-016-2372-y},
ISSN = {1432-7643},
EISSN = {1433-7479},
Unique-ID = {WOS:000423046300023},
}

@article{ WOS:000737368700001,
Author = {Li, Zhufeng and Liu, Haixing and Luo, Chunbo and Fu, Guangtao},
Title = {Assessing Surface Water Flood Risks in Urban Areas Using Machine
   Learning},
Journal = {WATER},
Year = {2021},
Volume = {13},
Number = {24},
Month = {DEC},
Abstract = {Urban flooding is a devastating natural hazard for cities around the
   world. Flood risk mapping is a key tool in flood management. However, it
   is computationally expensive to produce flood risk maps using
   hydrodynamic models. To this end, this paper investigates the use of
   machine learning for the assessment of surface water flood risks in
   urban areas. The factors that are considered in machine learning models
   include coordinates, elevation, slope gradient, imperviousness, land
   use, land cover, soil type, substrate, distance to river, distance to
   road, and normalized difference vegetation index. The machine learning
   models are tested using the case study of Exeter, UK. The performance of
   machine learning algorithms, including naive Bayes, perceptron,
   artificial neural networks (ANNs), and convolutional neural networks
   (CNNs), is compared based on a spectrum of indicators, e.g., accuracy,
   F-beta score, and receiver operating characteristic curve. The results
   obtained from the case study show that the flood risk maps can be
   accurately generated by the machine learning models. The performance of
   models on the 30-year flood event is better than 100-year and 1000-year
   flood events. The CNNs and ANNs outperform the other machine learning
   algorithms tested. This study shows that machine learning can help
   provide rapid flood mapping, and contribute to urban flood risk
   assessment and management.},
DOI = {10.3390/w13243520},
Article-Number = {3520},
EISSN = {2073-4441},
ResearcherID-Numbers = {Fu, Guangtao/ABE-3874-2021
   Luo, Chunbo/LDR-6859-2024},
ORCID-Numbers = {Li, Zhufeng/0000-0001-9346-6245
   },
Unique-ID = {WOS:000737368700001},
}

@article{ WOS:001101098500001,
Author = {Rodrigues, Francisco A.},
Title = {Machine learning in physics: A short guide},
Journal = {EPL},
Year = {2023},
Volume = {144},
Number = {2},
Month = {OCT},
Abstract = {Machine learning is a rapidly growing field with the potential to
   revolutionize many areas of science, including physics. This review
   provides a brief overview of machine learning in physics, covering the
   main concepts of supervised, unsupervised, and reinforcement learning,
   as well as more specialized topics such as causal inference, symbolic
   regression, and deep learning. We present some of the principal
   applications of machine learning in physics and discuss the associated
   challenges and perspectives.
   Copyright (c) 2023 EPLA},
DOI = {10.1209/0295-5075/ad0575},
Article-Number = {22001},
ISSN = {0295-5075},
EISSN = {1286-4854},
ResearcherID-Numbers = {Rodrigues, Francisco/E-4418-2011
   },
ORCID-Numbers = {Rodrigues, Francisco/0000-0002-0145-5571},
Unique-ID = {WOS:001101098500001},
}

@article{ WOS:000838722000001,
Author = {Lu, Xinjie and Ma, Feng and Xu, Jin and Zhang, Zehui},
Title = {Oil futures volatility predictability: New evidence based on machine
   learning models},
Journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS},
Year = {2022},
Volume = {83},
Month = {OCT},
Abstract = {This paper comprehensively examines the connection between oil futures
   volatility and the financial market based on a model-rich environment,
   which contains traditional predicting models, machine learning models,
   and combination models. The results highlight the efficiency of machine
   learning models for oil futures volatility forecasting, particularly the
   ensemble models and neural network models. Most interestingly, we
   consider the ``forecast combination puzzle{''} in machine learning
   models, and find that combination models continue to have more
   satisfactory performances in all types of situations. We also discuss
   the model interpretability and each indicator's contribution to the
   prediction. Our paper provides new insights for machine learning
   methods' applications in futures market volatility prediction, which is
   helpful for academics, policy-makers, and investors.},
DOI = {10.1016/j.irfa.2022.102299},
Article-Number = {102299},
ISSN = {1057-5219},
EISSN = {1873-8079},
Unique-ID = {WOS:000838722000001},
}

@article{ WOS:000911819300007,
Author = {Kedar, Sachin and Khazanchi, Deepak},
Title = {Neurology education in the era of artificial intelligence},
Journal = {CURRENT OPINION IN NEUROLOGY},
Year = {2023},
Volume = {36},
Number = {1},
Pages = {51-58},
Month = {FEB},
Abstract = {Purpose of reviewThe practice of neurology is undergoing a paradigm
   shift because of advances in the field of data science, artificial
   intelligence, and machine learning. To ensure a smooth transition,
   physicians must have the knowledge and competence to apply these
   technologies in clinical practice. In this review, we describe physician
   perception and preparedness, as well as current state for clinical
   applications of artificial intelligence and machine learning in
   neurology.Recent findingsDigital health including artificial
   intelligence-based/machine learning-based technology has made
   significant inroads into various aspects of healthcare including
   neurological care. Surveys of physicians and healthcare stakeholders
   suggests an overall positive perception about the benefits of artificial
   intelligence/machine learning in clinical practice. This positive
   perception is tempered by concerns for lack of knowledge and limited
   opportunities to build competence in artificial intelligence/machine
   learning technology. Literature about neurologist's perception and
   preparedness towards artificial intelligence/machine learning-based
   technology is scant. There are very few opportunities for physicians
   particularly neurologists to learn about artificial intelligence/machine
   learning-based technology.Neurologists have not been surveyed about
   their perception and preparedness to adopt artificial
   intelligence/machine learning-based technology in clinical practice. We
   propose development of a practical artificial intelligence/machine
   learning curriculum to enhance neurologists' competence in these newer
   technologies.},
DOI = {10.1097/WCO.0000000000001130},
ISSN = {1350-7540},
EISSN = {1473-6551},
ResearcherID-Numbers = {KEDAR, SACHIN/B-2579-2008},
Unique-ID = {WOS:000911819300007},
}

@article{ WOS:001166813800001,
Author = {Lu, Shasha and Yang, Jianyu and Gu, Yu and He, Dongyuan and Wu, Haocheng
   and Sun, Wei and Xu, Dong and Li, Changming and Guo, Chunxian},
Title = {Advances in Machine Learning Processing of Big Data from Disease
   Diagnosis Sensors},
Journal = {ACS SENSORS},
Year = {2024},
Volume = {9},
Number = {3},
Pages = {1134-1148},
Month = {FEB 16},
Abstract = {Exploring accurate, noninvasive, and inexpensive disease diagnostic
   sensors is a critical task in the fields of chemistry, biology, and
   medicine. The complexity of biological systems and the explosive growth
   of biomarker data have driven machine learning to become a powerful tool
   for mining and processing big data from disease diagnosis sensors. With
   the development of bioinformatics and artificial intelligence (AI),
   machine learning models formed by data mining have been able to guide
   more sensitive and accurate molecular computing. This review presents an
   overview of big data collection approaches and fundamental machine
   learning algorithms and discusses recent advances in machine learning
   and molecular computational disease diagnostic sensors. More
   specifically, we highlight existing modular workflows and key
   opportunities and challenges for machine learning to achieve disease
   diagnosis through big data mining.},
DOI = {10.1021/acssensors.3c02670},
EarlyAccessDate = {FEB 2024},
ISSN = {2379-3694},
ResearcherID-Numbers = {Li, Chang/G-6192-2010
   Guo, Chunxian/B-8259-2019
   sun, wei/JAO-0334-2023},
ORCID-Numbers = {lu, sha sha/0000-0002-5182-9674
   Guo, Chunxian/0000-0002-2603-7181
   Yang, Jianyu/0000-0003-1496-2991
   },
Unique-ID = {WOS:001166813800001},
}

@article{ WOS:000498861800006,
Author = {Ye, X. W. and Ding, Y. and Wan, H. P.},
Title = {Machine learning approaches for wind speed forecasting using long-term
   monitoring data: a comparative study},
Journal = {SMART STRUCTURES AND SYSTEMS},
Year = {2019},
Volume = {24},
Number = {6, SI},
Pages = {733-744},
Month = {DEC},
Abstract = {Wind speed forecasting is critical for a variety of engineering tasks,
   such as wind energy harvesting, scheduling of a wind power system, and
   dynamic control of structures (e.g., wind turbine, bridge, and
   building). Wind speed, which has characteristics of random, nonlinear
   and uncertainty, is difficult to forecast. Nowadays, machine learning
   approaches (generalized regression neural network (GRNN), back
   propagation neural network (BPNN), and extreme learning machine (ELM))
   are widely used for wind speed forecasting. In this study, two schemes
   are proposed to improve the forecasting performance of machine learning
   approaches. One is that optimization algorithms, i.e., cross validation
   (CV), genetic algorithm (GA), and particle swarm optimization (P SO),
   are used to automatically find the optimal model parameters. The other
   is that the combination of different machine learning methods is
   proposed by finite mixture (FM) method. Specifically, CV-GRNN, GA-BPNN,
   PSO-ELM belong to optimization algorithm-assisted machine learning
   approaches, and FM is a hybrid machine learning approach consisting of
   GRNN, BPNN, and ELM. The effectiveness of these machine learning methods
   in wind speed forecasting are fully investigated by one-year field
   monitoring data, and their performance is comprehensively compared.},
DOI = {10.12989/sss.2019.24.6.733},
ISSN = {1738-1584},
EISSN = {1738-1991},
ResearcherID-Numbers = {Ding, Yang/HNS-1656-2023},
ORCID-Numbers = {Ding, Yang/0000-0002-1298-1710
   },
Unique-ID = {WOS:000498861800006},
}

@article{ WOS:000446413400006,
Author = {Russo, Daniel P. and Zorn, Kimberley M. and Clark, Alex M. and Zhu, Hao
   and Ekins, Sean},
Title = {Comparing Multiple Machine Learning Algorithms and Metrics for Estrogen
   Receptor Binding Prediction},
Journal = {MOLECULAR PHARMACEUTICS},
Year = {2018},
Volume = {15},
Number = {10},
Pages = {4361-4370},
Month = {OCT},
Abstract = {Many chemicals that disrupt endocrine function have been linked to a
   variety of adverse biological outcomes. However, screening for endocrine
   disruption using in vitro or in vivo approaches is costly and
   time-consuming. Computational methods, e.g., quantitative structure
   activity relationship models, have become more reliable due to bigger
   training sets, increased computing power, and advanced machine learning
   algorithms, such as multilayered artificial neural networks. Machine
   learning models can be used to predict compounds for endocrine
   disrupting capabilities, such as binding to the estrogen receptor (ER),
   and allow for prioritization and further testing. In this work, an
   exhaustive comparison of multiple machine learning algorithms, chemical
   spaces, and evaluation metrics for ER binding was performed on public
   data sets curated using in-house cheminformatics software (Assay
   Central). Chemical features utilized in modeling consisted of binary
   fingerprints (ECFP6, FCFP6, ToxPrint, or MACCS keys) and continuous
   molecular descriptors from RDKit. Each feature set was subjected to
   classic machine learning algorithms (Bernoulli Naive Bayes, AdaBoost
   Decision Tree, Random Forest, Support Vector Machine) and Deep Neural
   Networks (DNN). Models were evaluated using a variety of metrics:
   recall, precision, F1-score, accuracy, area under the receiver operating
   characteristic curve, Cohen's Kappa, and Matthews correlation
   coefficient. For predicting compounds within the training set, DNN has
   an accuracy higher than that of other methods; however, in 5-fold cross
   validation and external test set predictions, DNN and most classic
   machine learning models perform similarly regardless of the data set or
   molecular descriptors used. We have also used the rank normalized scores
   as a performance-criteria for each machine learning method, and Random
   Forest performed best on the validation set when ranked by metric or by
   data sets. These results suggest classic machine learning algorithms may
   be sufficient to develop high quality predictive models of ER activity.},
DOI = {10.1021/acs.molpharmaceut.8b00546},
ISSN = {1543-8384},
ResearcherID-Numbers = {zh, H/HIK-0903-2022
   },
ORCID-Numbers = {Russo, Daniel/0000-0003-0438-1667
   Zhu, Hao/0000-0002-3559-6129},
Unique-ID = {WOS:000446413400006},
}

@article{ WOS:001136727600004,
Author = {Quinn, Thomas P. and Hess, Jonathan L. and Marshe, Victoria S. and
   Barnett, Michelle M. and Hauschild, Anne-Christin and Maciukiewicz,
   Malgorzata and Elsheikh, Samar S. M. and Men, Xiaoyu and Schwarz,
   Emanuel and Trakadis, Yannis J. and Breen, Michael S. and Barnett, Eric
   J. and Zhang-James, Yanli and Ahsen, Mehmet Eren and Cao, Han and Chen,
   Junfang and Hou, Jiahui and Salekin, Asif and Lin, I, Ping- and
   Nicodemus, Kristin K. and Meyer-Lindenberg, Andreas and Bichindaritz,
   Isabelle and Faraone, Stephen V. and Cairns, Murray J. and Pandey,
   Gaurav and Mueller, Daniel J. and Glatt, Stephen J. and Machine Learning
   Psychiat MLPsych},
Title = {A primer on the use of machine learning to distil knowledge from data in
   biological psychiatry},
Journal = {MOLECULAR PSYCHIATRY},
Year = {2024},
Month = {2024 JAN 4},
Abstract = {Applications of machine learning in the biomedical sciences are growing
   rapidly. This growth has been spurred by diverse cross-institutional and
   interdisciplinary collaborations, public availability of large datasets,
   an increase in the accessibility of analytic routines, and the
   availability of powerful computing resources. With this increased access
   and exposure to machine learning comes a responsibility for education
   and a deeper understanding of its bases and bounds, borne equally by
   data scientists seeking to ply their analytic wares in medical research
   and by biomedical scientists seeking to harness such methods to glean
   knowledge from data. This article provides an accessible and critical
   review of machine learning for a biomedically informed audience, as well
   as its applications in psychiatry. The review covers definitions and
   expositions of commonly used machine learning methods, and historical
   trends of their use in psychiatry. We also provide a set of standards,
   namely Guidelines for REporting Machine Learning Investigations in
   Neuropsychiatry (GREMLIN), for designing and reporting studies that use
   machine learning as a primary data-analysis approach. Lastly, we propose
   the establishment of the Machine Learning in Psychiatry (MLPsych)
   Consortium, enumerate its objectives, and identify areas of opportunity
   for future applications of machine learning in biological psychiatry.
   This review serves as a cautiously optimistic primer on machine learning
   for those on the precipice as they prepare to dive into the field,
   either as methodological practitioners or well-informed consumers.},
DOI = {10.1038/s41380-023-02334-2},
EarlyAccessDate = {JAN 2024},
ISSN = {1359-4184},
EISSN = {1476-5578},
ResearcherID-Numbers = {Lin, Ping-I/AGZ-2326-2022
   Müller, Daniel/L-4159-2016
   Meyer-Lindenberg, Andreas/H-1076-2011
   Elsheikh, Samar Salah Mohamedahmed/HOF-5962-2023
   Zhang-James, Yanli/Q-8309-2018
   Mueller, Daniel/L-4159-2016
   Maciukiewicz, Malgorzata/K-3344-2019
   Glatt, Stephen/A-7212-2009
   Faraone, Stephen/G-5785-2010
   Hauschild, Anne-Christin/KLZ-9144-2024
   Lin, Ping-I (Daniel)/AGZ-2326-2022
   Hou, Jiahui/KJL-4631-2024},
ORCID-Numbers = {Lin, Ping-I/0000-0002-9739-7184
   Barnett, Michelle/0000-0002-2621-8302
   Hauschild, Anne-Christin/0000-0002-7499-4373
   Marshe, Victoria/0000-0002-1762-4819
   Cairns, Murray/0000-0003-2490-2538
   Zhang-James, Yanli/0000-0002-2104-0963
   Mueller, Daniel/0000-0003-4978-4400
   Hess, Jonathan/0000-0001-8406-632X
   },
Unique-ID = {WOS:001136727600004},
}

@article{ WOS:000412062500001,
Author = {Zeng, Xueqiang and Luo, Gang},
Title = {Progressive sampling-based Bayesian optimization for efficient and
   automatic machine learning model selection},
Journal = {HEALTH INFORMATION SCIENCE AND SYSTEMS},
Year = {2017},
Volume = {5},
Month = {SEP 27},
Abstract = {Purpose: Machine learning is broadly used for clinical data analysis.
   Before training a model, a machine learning algorithm must be selected.
   Also, the values of one or more model parameters termed hyper-parameters
   must be set. Selecting algorithms and hyper-parameter values requires
   advanced machine learning knowledge and many labor-intensive manual
   iterations. To lower the bar to machine learning, miscellaneous
   automatic selection methods for algorithms and/or hyper-parameter values
   have been proposed. Existing automatic selection methods are inefficient
   on large data sets. This poses a challenge for using machine learning in
   the clinical big data era.
   Methods: To address the challenge, this paper presents progressive
   sampling-based Bayesian optimization, an efficient and automatic
   selection method for both algorithms and hyper-parameter values.
   Results: We report an implementation of the method. We show that
   compared to a state of the art automatic selection method, our method
   can significantly reduce search time, classification error rate, and
   standard deviation of error rate due to randomization.
   Conclusions: This is major progress towards enabling fast turnaround in
   identifying high-quality solutions required by many machine
   learning-based clinical data analysis tasks.},
DOI = {10.1007/s13755-017-0023-z},
Article-Number = {2},
ISSN = {2047-2501},
ORCID-Numbers = {Zeng, Xueqiang/0000-0002-3256-6207
   Luo, Gang/0000-0001-7217-4008},
Unique-ID = {WOS:000412062500001},
}

@article{ WOS:000483476800034,
Author = {Shafaf, Negin and Malek, Hamed},
Title = {Applications of Machine Learning Approaches in Emergency Medicine; a
   Review Article},
Journal = {ARCHIVES OF ACADEMIC EMERGENCY MEDICINE},
Year = {2019},
Volume = {7},
Number = {1},
Abstract = {Using artificial intelligence and machine learning techniques in
   different medical fields, especially emergency medicine is rapidly
   growing. In this paper, studies conducted in the recent years on using
   artificial intelligence in emergency medicine have been collected and
   assessed. These studies belonged to three categories: prediction and
   detection of disease; prediction of need for admission, discharge and
   also mortality; and machine learning based triage systems. In each of
   these categories, the most important studies have been chosen and
   accuracy and results of the algorithms have been briefly evaluated by
   mentioning machine learning techniques and used datasets.},
Article-Number = {e34},
EISSN = {2645-4904},
ResearcherID-Numbers = {Malek, Hamed/AFV-8091-2022},
ORCID-Numbers = {Cuocolo, Alberto/0000-0003-3431-7658
   Malek, Hamed/0000-0003-4314-6539
   },
Unique-ID = {WOS:000483476800034},
}

@article{ WOS:000663077000011,
Author = {Cui, Jianfei and Zhu, He and Deng, Hao and Chen, Ziwei and Liu, Dianbo},
Title = {FeARH: Federated machine learning with anonymous random hybridization on
   electronic medical records},
Journal = {JOURNAL OF BIOMEDICAL INFORMATICS},
Year = {2021},
Volume = {117},
Month = {MAY},
Abstract = {Electrical medical records are restricted and difficult to centralize
   for machine learning model training due to privacy and regulatory
   issues. One solution is to train models in a distributed manner that
   involves many parties in the process. However, sometimes certain parties
   are not trustable, and in this project, we aim to propose an alternative
   method to traditional federated learning with central analyzer in order
   to conduct training in a situation without a trustable central analyzer.
   The proposed algorithm is called ``federated machine learning with
   anonymous random hybridization (abbreviated as `FeARH'){''}, using
   mainly hybridization algorithm to degenerate the integration of
   connections between medical record data and models' parameters by adding
   randomization into the parameter sets shared to other parties. Based on
   our experiment, our new algorithm has similar AUCROC and AUCPR results
   compared with machine learning in a centralized manner and original
   federated machine learning.},
DOI = {10.1016/j.jbi.2021.103735},
EarlyAccessDate = {APR 2021},
Article-Number = {103735},
ISSN = {1532-0464},
EISSN = {1532-0480},
Unique-ID = {WOS:000663077000011},
}

@article{ WOS:000618935000001,
Author = {Erkinay Ozdemir, Merve and Ali, Zaara and Subeshan, Balakrishnan and
   Asmatulu, Eylem},
Title = {Applying machine learning approach in recycling},
Journal = {JOURNAL OF MATERIAL CYCLES AND WASTE MANAGEMENT},
Year = {2021},
Volume = {23},
Number = {3},
Pages = {855-871},
Month = {MAY},
Abstract = {Waste generation has been increasing drastically based on the world's
   population and economic growth. This has significantly affected human
   health, natural life, and ecology. The utilization of limited natural
   resources, and the harming of the earth in the process of mineral
   extraction, and waste management have far exceeded limits. The recycling
   rate are continuously increasing; however, assessments show that humans
   will be creating more waste than ever before. Some difficulties during
   recycling include the significant expense involved during the separation
   of recyclable waste from non-disposable waste. Machine learning is the
   utilization of artificial intelligence (AI) that provides a framework to
   take as a structural improvement of the fact without being programmed.
   Machine learning concentrates on the advancement of programs that can
   obtain the information and use it to learn to make future decisions. The
   classification and separation of materials in a mixed recycling
   application in machine learning is a division of AI that is playing an
   important role for better separation of complex waste. The primary
   purpose of this study is to analyze AI by focusing on machine learning
   algorithms used in recycling systems. This study is a compilation of the
   most recent developments in machine learning used in recycling
   industries.},
DOI = {10.1007/s10163-021-01182-y},
EarlyAccessDate = {FEB 2021},
ISSN = {1438-4957},
EISSN = {1611-8227},
ResearcherID-Numbers = {Asmatulu, Eylem/OCL-4183-2025
   Erkınay Özdemir, Merve/JWO-7585-2024
   Subeshan, Balakrishnan/AEO-5142-2022},
ORCID-Numbers = {Asmatulu, Eylem/0000-0002-5605-2251
   Erkinay Ozdemir, merve/0000-0001-8864-385X
   },
Unique-ID = {WOS:000618935000001},
}

@article{ WOS:000675886300003,
Author = {Yang, Bin and Xu, Songci and Lei, Yaguo and Lee, Chi-Guhn and Stewart,
   Edward and Roberts, Clive},
Title = {Multi-source transfer learning network to complement knowledge for
   intelligent diagnosis of machines with unseen faults},
Journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING},
Year = {2022},
Volume = {162},
Month = {JAN 1},
Abstract = {Most of the current successes of deep transfer learning-based fault
   diagnosis require two assumptions: 1) the health state set of source
   machines should overlap that of target machines; 2) the number of target
   machine samples is balanced across health states. However, such
   assumptions are unrealistic in engineering scenarios, where target
   machines suffer from fault types that are not seen in source machines
   and the target machines are mostly in a healthy state with only
   occasional faults. As a result, the diagnostic knowledge from source
   machines may not cover all fault types of target machines nor address
   imbalanced target samples. Therefore, we propose a framework, called a
   multi-source transfer learning network (MSTLN), to aggregate and
   transfer diagnostic knowledge from multiple source machines by combining
   multiple partial distribution adaptation sub-networks (PDA-Subnets) and
   a multi-source diagnostic knowledge fusion module. The former weights
   target samples by counter-balancing factors to jointly adapt partial
   distributions of source and target pairs, and the latter releases
   negative effects due to discrepancy among multiple source machines and
   further fuses diagnostic decisions output from multiple PDA-Subnets. Two
   case studies demonstrate that MSTLN can reduce the misdiagnosis rate and
   obtain better transfer performance for imbalanced target samples than
   other conventional methods.},
DOI = {10.1016/j.ymssp.2021.108095},
EarlyAccessDate = {JUN 2021},
Article-Number = {108095},
ISSN = {0888-3270},
EISSN = {1096-1216},
ResearcherID-Numbers = {Lei, Yaguo/N-4891-2014
   Roberts, Clive/W-3204-2019
   Yang, Bin/HIR-6542-2022
   },
ORCID-Numbers = {Stewart, Edd/0000-0002-9582-2861
   Lee, Chi-Guhn/0000-0002-0916-0241},
Unique-ID = {WOS:000675886300003},
}

@article{ WOS:000405536900025,
Author = {Wan, Yihe and Song, Shiji and Huang, Gao and Li, Shuang},
Title = {Twin extreme learning machines for pattern classification},
Journal = {NEUROCOMPUTING},
Year = {2017},
Volume = {260},
Pages = {235-244},
Month = {OCT 18},
Abstract = {Extreme learning machine (ELM) is an efficient and effective learning
   algorithm for pattern classification. For binary classification problem,
   traditional ELM learns only one hyperplane to separate different classes
   in the feature space. In this paper, we propose a novel twin extreme
   learning machine (TELM) to simultaneously train two ELMs with two
   nonparallel classification hyperplanes. Specifically, TELM first
   utilizes the random feature mapping mechanism to construct the feature
   space, and then two nonparallel separating hyperplanes are learned for
   the final classification. For each hyperplane, TELM jointly minimizes
   its distance to one class and requires it to be far away from the other
   class. TELM incorporates the idea of twin support vector machine (TSVM)
   into the basic framework of ELM, thus TELM could have the advantages of
   the both algorithms. Moreover, compared to TSVM, TELM has fewer
   optimization constraint variables but with better classification
   performance. We also introduce a successive over-relaxation technique to
   speed up the training of our algorithm. Comprehensive experimental
   results on a large number of datasets verify the effectiveness and
   efficiency of TELM. (C) 2017 Published by Elsevier B.V.},
DOI = {10.1016/j.neucom.2017.04.036},
ISSN = {0925-2312},
EISSN = {1872-8286},
ResearcherID-Numbers = {Huang, Gao/AAB-1776-2019},
Unique-ID = {WOS:000405536900025},
}

@article{ WOS:000747895100011,
Author = {Deng, Hong-Fei and Sun, Ming-Wei and Wang, Yu and Zeng, Jun and Yuan,
   Ting and Li, Ting and Li, Di-Huan and Chen, Wei and Zhou, Ping and Wang,
   Qi and Jiang, Hua},
Title = {Evaluating machine learning models for sepsis prediction: A systematic
   review of methodologies},
Journal = {ISCIENCE},
Year = {2022},
Volume = {25},
Number = {1},
Month = {JAN 21},
Abstract = {Studies for sepsis prediction using machine learning are developing
   rapidly in medical science recently. In this review, we propose a set of
   new evaluation criteria and reporting standards to assess 21 qualified
   machine learning models for quality analysis based on PRISMA. Our
   assessment shows that (1.) the definition of sepsis is not consistent
   among the studies; (2.) data sources and data preprocessing methods,
   machine learning models, feature engineering, and inclusion types vary
   widely among the studies; (3.) the closer to the onset of sepsis, the
   higher the value of AUROC is; (4.) the improvement in AUROC is primarily
   due to using machine learning as a feature engineering tool; (5.) deep
   neural networks coupled with Sepsis-3 diagnostic criteria tend to yield
   better results on the time series data collected from patients with
   sepsis. The new evaluation criteria and reporting standards will
   facilitate the development of improved machine learning models for
   clinical applications.},
DOI = {10.1016/j.isci.2021.103651},
EarlyAccessDate = {JAN 2022},
EISSN = {2589-0042},
ResearcherID-Numbers = {Qi, Wang/HDM-5801-2022
   Jiang, Hua/ABE-5811-2021
   Li, Tingting/T-6249-2019
   曾, 珺/ISR-8874-2023},
Unique-ID = {WOS:000747895100011},
}

@article{ WOS:000826145900001,
Author = {Shi, Si and Tse, Rita and Luo, Wuman and D'Addona, Stefano and Pau,
   Giovanni},
Title = {Machine learning-driven credit risk: a systemic review},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2022},
Volume = {34},
Number = {17, SI},
Pages = {14327-14339},
Month = {SEP},
Abstract = {Credit risk assessment is at the core of modern economies.
   Traditionally, it is measured by statistical methods and manual
   auditing. Recent advances in financial artificial intelligence stemmed
   from a new wave of machine learning (ML)-driven credit risk models that
   gained tremendous attention from both industry and academia. In this
   paper, we systematically review a series of major research contributions
   (76 papers) over the past eight years using statistical, machine
   learning and deep learning techniques to address the problems of credit
   risk. Specifically, we propose a novel classification methodology for
   ML-driven credit risk algorithms and their performance ranking using
   public datasets. We further discuss the challenges including data
   imbalance, dataset inconsistency, model transparency, and inadequate
   utilization of deep learning models. The results of our review show
   that: 1) most deep learning models outperform classic machine learning
   and statistical algorithms in credit risk estimation, and 2) ensemble
   methods provide higher accuracy compared with single models. Finally, we
   present summary tables in terms of datasets and proposed models.},
DOI = {10.1007/s00521-022-07472-2},
EarlyAccessDate = {JUL 2022},
ISSN = {0941-0643},
EISSN = {1433-3058},
ResearcherID-Numbers = {Shi, Si/JLM-0204-2023
   Pau, Giovanni/ISU-8786-2023
   Luo, Wuman/IUM-9746-2023
   },
ORCID-Numbers = {Shi, Si/0000-0002-6952-9848
   Pau, Giovanni/0000-0003-2216-7170},
Unique-ID = {WOS:000826145900001},
}

@article{ WOS:001255993200001,
Author = {Kang, Mijeong and Kim, Donghyeon and Kim, Jihee and Kim, Nakyung and
   Lee, Seunghun},
Title = {Strategies to Enrich Electrochemical Sensing Data with Analytical
   Relevance for Machine Learning Applications: A Focused Review},
Journal = {SENSORS},
Year = {2024},
Volume = {24},
Number = {12},
Month = {JUN},
Abstract = {In this review, recent advances regarding the integration of machine
   learning into electrochemical analysis are overviewed, focusing on the
   strategies to increase the analytical context of electrochemical data
   for enhanced machine learning applications. While information-rich
   electrochemical data offer great potential for machine learning
   applications, limitations arise when sensors struggle to identify or
   quantitatively detect target substances in a complex matrix of
   non-target substances. Advanced machine learning techniques are crucial,
   but equally important is the development of methods to ensure that
   electrochemical systems can generate data with reasonable variations
   across different targets or the different concentrations of a single
   target. We discuss five strategies developed for building such
   electrochemical systems, employed in the steps of preparing sensing
   electrodes, recording signals, and analyzing data. In addition, we
   explore approaches for acquiring and augmenting the datasets used to
   train and validate machine learning models. Through these insights, we
   aim to inspire researchers to fully leverage the potential of machine
   learning in electroanalytical science.},
DOI = {10.3390/s24123855},
Article-Number = {3855},
EISSN = {1424-8220},
ResearcherID-Numbers = {Kang, Mijeong/IQS-8470-2023
   Lee, Sang/S-5752-2019},
ORCID-Numbers = {Lee, Seunghun/0000-0002-6526-7780
   },
Unique-ID = {WOS:001255993200001},
}

@article{ WOS:000884481200001,
Author = {Sidak, David and Schwarzerova, Jana and Weckwerth, Wolfram and Waldherr,
   Steffen},
Title = {Interpretable machine learning methods for predictions in systems
   biology from omics data},
Journal = {FRONTIERS IN MOLECULAR BIOSCIENCES},
Year = {2022},
Volume = {9},
Month = {OCT 17},
Abstract = {Machine learning has become a powerful tool for systems biologists, from
   diagnosing cancer to optimizing kinetic models and predicting the state,
   growth dynamics, or type of a cell. Potential predictions from complex
   biological data sets obtained by ``omics `` experiments seem endless,
   but are often not the main objective of biological research. Often we
   want to understand the molecular mechanisms of a disease to develop new
   therapies, or we need to justify a crucial decision that is derived from
   a prediction. In order to gain such knowledge from data, machine
   learning models need to be extended. A recent trend to achieve this is
   to design ``interpretable `` models. However, the notions around
   interpretability are sometimes ambiguous, and a universal recipe for
   building well-interpretable models is missing. With this work, we want
   to familiarize systems biologists with the concept of model
   interpretability in machine learning. We consider data sets, data
   preparation, machine learning methods, and software tools relevant to
   omics research in systems biology. Finally, we try to answer the
   question: ``What is interpretability? `` We introduce views from the
   interpretable machine learning community and propose a scheme for
   categorizing studies on omics data. We then apply these tools to review
   and categorize recent studies where predictive machine learning models
   have been constructed from non-sequential omics data.},
DOI = {10.3389/fmolb.2022.926623},
Article-Number = {926623},
EISSN = {2296-889X},
ResearcherID-Numbers = {Waldherr, Steffen/C-7802-2011
   Schwarzerova, Jana/HJA-7282-2022
   Weckwerth, Wolfram/G-5811-2010
   },
ORCID-Numbers = {Waldherr, Steffen/0000-0002-0936-579X
   Schwarzerova, Jana/0000-0003-2918-9313},
Unique-ID = {WOS:000884481200001},
}

@article{ WOS:001068492000001,
Author = {Muduli, Debendra and Kumar, Rakesh Ranjan and Pradhan, Jitesh and Kumar,
   Abhinav},
Title = {An empirical evaluation of extreme learning machine uncertainty
   quantification for automated breast cancer detection},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2023},
Month = {2023 SEP 20},
Abstract = {Early detection and diagnosis are the key factors in decreasing the
   breast cancer mortality rate in medical image analysis. A randomized
   learning technique called extreme learning machine (ELM) plays a vital
   role in learning the single hidden layer feed-forward network with fast
   learning speed and good generalization. The input weight and bias are
   randomly generated and fixed during the ELM training phase, and
   subsequently, the analytical procedure determines the output weight. The
   extreme learning machine's learning ability is based on three
   uncertainty factors: the number of hidden nodes, an input weight
   initialization, and the type of activation function in the hidden layer.
   Various breast classification works have experimented with extreme
   learning machine techniques and did not investigate the following
   factors. This paper evaluates the extreme learning machine model's
   performance with different configurations on the standard ultra-sound
   breast cancer dataset, BUSI. The proposed extreme learning machine
   configuration model experimented on original and filtered ultra-sound
   images. A fivefold stratified cross-validation scheme is applied here to
   enhance the model's generalization performance. The proposed
   computer-aided diagnosis (CAD) model provides 100\% accuracy with the
   best extreme learning machine configurations. Then, we compare the
   classification results of the proposed model with typical variants of
   extreme learning machines like Hybrid ELM (HELM), online-sequential ELM
   (OS-ELM), Weighted ELM, and complex ELM (CELM). The experimental results
   demonstrate that the proposed extreme learning machine model is superior
   to existing models, offering good generalization without any feature
   extraction or reduction method.},
DOI = {10.1007/s00521-023-08992-1},
EarlyAccessDate = {SEP 2023},
ISSN = {0941-0643},
EISSN = {1433-3058},
ResearcherID-Numbers = {Kumar, Rakesh/AAD-2837-2019
   pradhan, jitesh/MGT-2129-2025
   Muduli, Debendra/JCF-0169-2023},
ORCID-Numbers = {Muduli, Debendra/0000-0002-5697-3659
   },
Unique-ID = {WOS:001068492000001},
}

@article{ WOS:000729816900003,
Author = {Meiyazhagan, J. and Sudharsan, S. and Venkatesan, A. and Senthilvelan,
   M.},
Title = {Prediction of occurrence of extreme events using machine learning},
Journal = {EUROPEAN PHYSICAL JOURNAL PLUS},
Year = {2022},
Volume = {137},
Number = {1},
Month = {JAN},
Abstract = {Machine learning models play a vital role in the prediction task in
   several fields of study. In this work, we utilize the ability of machine
   learning algorithms to predict the occurrence of extreme events in a
   nonlinear mechanical system. Extreme events are rare events that occur
   ubiquitously in nature. We consider four machine learning models, namely
   Logistic Regression, Support Vector Machine, Random Forest and
   Multi-Layer Perceptron in our prediction task. We train these four
   machine learning models using training set data and compute the
   performance of each model using the test set data. We show that the
   Multi-Layer Perceptron model performs better among the four models in
   the prediction of extreme events in the considered system. The
   persistent behaviour of the considered machine learning models is
   cross-checked with randomly shuffled training set and test set data.},
DOI = {10.1140/epjp/s13360-021-02249-3},
Article-Number = {16},
ISSN = {2190-5444},
ResearcherID-Numbers = {S, Sudharsan/GMX-3804-2022
   Jaganathan, Meiyazhagan/ABG-5169-2021
   Senthilvelan, M/B-9661-2008
   },
ORCID-Numbers = {Simmahari, Sudharsan/0000-0002-2535-4556
   A, VENKATESAN/0000-0002-6683-7167
   Jaganathan, Meiyazhagan/0000-0001-8971-965X
   Senthilvelan, Murugaian/0000-0002-8695-9604},
Unique-ID = {WOS:000729816900003},
}

@article{ WOS:000445815100007,
Author = {Melo, Francisco S. and Mascarenhas, Samuel and Paiva, Ana},
Title = {A Tutorial on Machine Learning for Interactive Pedagogical Systems},
Journal = {INTERNATIONAL JOURNAL OF SERIOUS GAMES},
Year = {2018},
Volume = {5},
Number = {3},
Pages = {79-112},
Month = {SEP},
Abstract = {This paper provides a short introduction to the field of machine
   learning for interactive pedagogical systems. Departing from different
   examples encountered in interactive pedagogical systems-such as
   intelligent tutoring systems or serious games-we go over several
   representative families of methods in machine learning, introducing key
   concepts in this field. We discuss common challenges in machine learning
   and how current methods address such challenges. Conversely, by
   anchoring our presentation on actual interactive pedagogical systems,
   highlight how machine learning can benefit the development of such
   systems.},
DOI = {10.17083/ijsg.v5i3.256},
ISSN = {2384-8766},
ResearcherID-Numbers = {Paiva, Ana/B-9900-2011
   Melo, Francisco/AAG-8359-2020},
ORCID-Numbers = {Paiva, Ana/0000-0003-3998-5188
   },
Unique-ID = {WOS:000445815100007},
}

@article{ WOS:000542942700029,
Author = {Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
Title = {Blockchained On-Device Federated Learning},
Journal = {IEEE COMMUNICATIONS LETTERS},
Year = {2020},
Volume = {24},
Number = {6},
Pages = {1279-1283},
Month = {JUN},
Abstract = {By leveraging blockchain, this letter proposes a blockchained federated
   learning (BlockFL) architecture where local learning model updates are
   exchanged and verified. This enables on-device machine learning without
   any centralized training data or coordination by utilizing a consensus
   mechanism in blockchain. Moreover, we analyze an end-to-end latency
   model of BlockFL and characterize the optimal block generation rate by
   considering communication, computation, and consensus delays.},
DOI = {10.1109/LCOMM.2019.2921755},
ISSN = {1089-7798},
EISSN = {1558-2558},
ResearcherID-Numbers = {Kim, Young/J-2744-2012
   Park, Jihong/ABC-7334-2020
   Bennis, Mehdi/ABE-5838-2020},
ORCID-Numbers = {Park, Jihong/0000-0001-7623-6552
   Bennis, Mehdi/0000-0003-0261-0171},
Unique-ID = {WOS:000542942700029},
}

@article{ WOS:000453925000014,
Author = {Handelman, Guy S. and Kok, Hong Kuan and Chandra, Ronil V. and Razavi,
   Amir H. and Huang, Shiwei and Brooks, Mark and Lee, Michael J. and
   Asadi, Hamed},
Title = {Peering Into the Black Box of Artificial Intelligence: Evaluation
   Metrics of Machine Learning Methods},
Journal = {AMERICAN JOURNAL OF ROENTGENOLOGY},
Year = {2019},
Volume = {212},
Number = {1},
Pages = {38-43},
Month = {JAN},
Abstract = {OBJECTIVE. Machine learning (ML) and artificial intelligence (AI) are
   rapidly becoming the most talked about and controversial topics in
   radiology and medicine. Over the past few years, the numbers of ML- or
   AI-focused studies in the literature have increased almost
   exponentially, and ML has become a hot topic at academic and industry
   conferences. However, despite the increased awareness of ML as a tool,
   many medical professionals have a poor understanding of how ML works and
   how to critically appraise studies and tools that are presented to us.
   Thus, we present a brief overview of ML, explain the metrics used in ML
   and how to interpret them, and explain some of the technical jargon
   associated with the field so that readers with a medical background and
   basic knowledge of statistics can feel more comfortable when examining
   ML applications.
   CONCLUSION. Attention to sample size, overfitting, underfitting, cross
   validation, as well as a broad knowledge of the metrics of machine
   learning, can help those with little or no technical knowledge begin to
   assess machine learning studies. However, transparency in methods and
   sharing of algorithms is vital to allow clinicians to assess these tools
   themselves.},
DOI = {10.2214/AJR.18.20224},
ISSN = {0361-803X},
EISSN = {1546-3141},
ResearcherID-Numbers = {Huang, Shiwei/JPK-4210-2023
   Chandra, Ronil/GPK-0357-2022
   },
ORCID-Numbers = {Huang, Shiwei/0000-0002-0171-6655
   Chandra, Ronil/0000-0001-7555-2297},
Unique-ID = {WOS:000453925000014},
}

@article{ WOS:000700566400002,
Author = {Shingu, Yuta and Seki, Yuya and Watabe, Shohei and Endo, Suguru and
   Matsuzaki, Yuichiro and Kawabata, Shiro and Nikuni, Tetsuro and
   Hakoshima, Hideaki},
Title = {Boltzmann machine learning with a variational quantum algorithm},
Journal = {PHYSICAL REVIEW A},
Year = {2021},
Volume = {104},
Number = {3},
Month = {SEP 16},
Abstract = {A Boltzmann machine is a powerful tool for modeling probability
   distributions that govern the training data. A thermal equilibrium state
   is typically used for the Boltzmann machine learning to obtain a
   suitable probability distribution. The Boltzmann machine learning
   consists of calculating the gradient of the loss function given in terms
   of the thermal average, which is the most time-consuming procedure.
   Here, we propose a method to implement the Boltzmann machine learning by
   using noisy intermediate-scale quantum devices. We prepare an initial
   pure state that contains all possible computational basis states with
   the same amplitude, and we apply a variational imaginary time
   simulation. Readout of the state after the evolution in the
   computational basis approximates the probability distribution of the
   thermal equilibrium state that is used for the Boltzmann machine
   learning. We perform the numerical simulations of our scheme and confirm
   that the Boltzmann machine learning works well. Our scheme leads to a
   significant step toward an efficient machine learning using quantum
   hardware.},
DOI = {10.1103/PhysRevA.104.032413},
Article-Number = {032413},
ISSN = {2469-9926},
EISSN = {2469-9934},
ResearcherID-Numbers = {Hakoshima, Hideaki/AAR-1877-2020
   Seki, Yuya/AAJ-6863-2020
   Matsuzaki, Yuichiro/M-7347-2017
   Watabe, Shohei/E-8271-2013},
ORCID-Numbers = {Matsuzaki, Yuichiro/0000-0001-6814-3778
   Nikuni, Tetsuro/0000-0003-0839-7965
   Hakoshima, Hideaki/0000-0003-1295-9801
   Seki, Yuya/0000-0001-5393-2280
   Watabe, Shohei/0000-0003-2895-698X},
Unique-ID = {WOS:000700566400002},
}

@article{ WOS:000424191300048,
Author = {Melnikov, Alexey A. and Nautrup, Hendrik Poulsen and Krenn, Mario and
   Dunjko, Vedran and Tiersch, Markus and Zeilinger, Anton and Briegel,
   Hans J.},
Title = {Active learning machine learns to create new quantum experiments},
Journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA},
Year = {2018},
Volume = {115},
Number = {6},
Pages = {1221-1226},
Month = {FEB 6},
Abstract = {How useful can machine learning be in a quantum laboratory? Here we
   raise the question of the potential of intelligent machines in the
   context of scientific research. A major motivation for the present work
   is the unknown reachability of various entanglement classes in quantum
   experiments. We investigate this question by using the projective
   simulation model, a physics-oriented approach to artificial
   intelligence. In our approach, the projective simulation system is
   challenged to design complex photonic quantum experiments that produce
   high-dimensional entangled multiphoton states, which are of high
   interest in modern quantum experiments. The artificial intelligence
   system learns to create a variety of entangled states and improves the
   efficiency of their realization. In the process, the system autonomously
   (re)discovers experimental techniques which are only now becoming
   standard in modern quantum optical experiments-a trait which was not
   explicitly demanded from the system but emerged through the process of
   learning. Such features highlight the possibility that machines could
   have a significantly more creative role in future research.},
DOI = {10.1073/pnas.1714936115},
ISSN = {0027-8424},
ResearcherID-Numbers = {Zeilinger, Anton/A-1170-2011
   Krenn, Mario/A-2799-2013
   Melnikov, Alexey/L-7441-2016},
ORCID-Numbers = {Tiersch, Markus/0000-0002-0562-4379
   Krenn, Mario/0000-0003-1620-9207
   Poulsen Nautrup, Hendrik/0000-0001-7815-7006
   Melnikov, Alexey/0000-0002-5033-4063},
Unique-ID = {WOS:000424191300048},
}

@article{ WOS:000487122600008,
Author = {Dujon, Antoine M. and Schofield, Gail},
Title = {Importance of machine learning for enhancing ecological studies using
   information-rich imagery},
Journal = {ENDANGERED SPECIES RESEARCH},
Year = {2019},
Volume = {39},
Pages = {91-104},
Abstract = {There is increasing demand for efficient ways to process large volumes
   of data from visual-based remote-technology, such as unmanned aerial
   vehicles (UAVs) in ecology and conservation, with machine learning
   methods representing a promising avenue to address varying user demands.
   Here, we evaluated current trends in how machine learning and UAVs are
   used to process imagery data for detecting animals and vegetation across
   habitats, placing emphasis on their utility for endangered species. We
   reviewed 213 publications that used UAVs at 256 study sites, of which
   just 89 (42 \%) used machine learning to assess the visual data. We
   evaluated geographical and temporal trends and identified how each
   technology is used at a global scale. We also identified the most
   commonly encountered machine-learning methods, including potential
   reasons for their limited use in ecology and possible solutions.
   Thirteen out of the 17 habitats defined by the International Union for
   Conservation of Nature (IUCN) habitat classification scheme were
   monitored using UAVs, while 12 habitats were monitored using both UAVs
   and machine learning. Our results show that, while machine learning is
   already being used across many habitat types, it is primarily restricted
   to more uniform habitats at present. Out of 173 plant and animal species
   monitored using UAV surveys, 30 were of conservation concern, with
   machine learning being used to assess UAV imagery data for 9 of these
   species. In conclusion, we anticipate that the joint use of UAVs and
   machine learning for ecological research and conservation will expand as
   machine learning methods become more accessible.},
DOI = {10.3354/esr00958},
ISSN = {1863-5407},
EISSN = {1613-4796},
ResearcherID-Numbers = {Dujon, Antoine/Q-7075-2019
   },
ORCID-Numbers = {Dujon, Antoine/0000-0002-1579-9156},
Unique-ID = {WOS:000487122600008},
}

@article{ WOS:001293612000015,
Author = {Franceschelli, Giorgio and Musolesi, Mirco},
Title = {Creativity and Machine Learning: A Survey},
Journal = {ACM COMPUTING SURVEYS},
Year = {2024},
Volume = {56},
Number = {11},
Month = {NOV},
Abstract = {There is a growing interest in the area of machine learning and
   creativity. This survey presents an overview of the history and the
   state of the art of computational creativity theories, key machine
   learning techniques (including generative deep learning), and
   corresponding automatic evaluation methods. After presenting a critical
   discussion of the key contributions in this area, we outline the current
   research challenges and emerging opportunities in this field.},
DOI = {10.1145/3664595},
Article-Number = {283},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Musolesi, Mirco/C-9329-2014
   Franceschelli, Giorgio/LWJ-9430-2024
   },
ORCID-Numbers = {Musolesi, Mirco/0000-0001-9712-4090
   Franceschelli, Giorgio/0000-0003-3210-3015},
Unique-ID = {WOS:001293612000015},
}

@article{ WOS:000604147200010,
Author = {Lu, Sirui and Duan, Lu-Ming and Deng, Dong-Ling},
Title = {Quantum adversarial machine learning},
Journal = {PHYSICAL REVIEW RESEARCH},
Year = {2020},
Volume = {2},
Number = {3},
Month = {AUG 6},
Abstract = {Adversarial machine learning is an emerging field that focuses on
   studying vulnerabilities of machine learning approaches in adversarial
   settings and developing techniques accordingly to make learning robust
   to adversarial manipulations. It plays a vital role in various machine
   learning applications and recently has attracted tremendous attention
   across different communities. In this paper, we explore different
   adversarial scenarios in the context of quantum machine learning. We
   find that, similar to traditional classifiers based on classical neural
   networks, quantum learning systems are likewise vulnerable to crafted
   adversarial examples, independent of whether the input data is classical
   or quantum. In particular, we find that a quantum classifier that
   achieves nearly the state-of-the-art accuracy can be conclusively
   deceived by adversarial examples obtained via adding imperceptible
   perturbations to the original legitimate samples. This is explicitly
   demonstrated with quantum adversarial learning in different scenarios,
   including classifying real-life images (e.g., handwritten digit images
   in the dataset MNIST), learning phases of matter (such as
   ferromagnetic/paramagnetic orders and symmetry protected topological
   phases), and classifying quantum data. Furthermore, we show that based
   on the information of the adversarial examples at hand, practical
   defense strategies can be designed to fight against a number of
   different attacks. Our results uncover the notable vulnerability of
   quantum machine learning systems to adversarial perturbations, which not
   only reveals another perspective in bridging machine learning and
   quantum physics in theory but also provides valuable guidance for
   practical applications of quantum classifiers based on both near-term
   and future quantum technologies.},
DOI = {10.1103/PhysRevResearch.2.033212},
Article-Number = {033212},
EISSN = {2643-1564},
ResearcherID-Numbers = {Lu, Sirui/AAD-6461-2022
   Deng, Dong-Ling/K-2249-2017
   },
ORCID-Numbers = {Lu, Sirui/0000-0003-2807-6288},
Unique-ID = {WOS:000604147200010},
}

@article{ WOS:001162062000001,
Author = {Freire, Paulina and Freire, Diego and Licon, Carmen C.},
Title = {A comprehensive review of machine learning and its application to dairy
   products},
Journal = {CRITICAL REVIEWS IN FOOD SCIENCE AND NUTRITION},
Year = {2025},
Volume = {65},
Number = {10},
Pages = {1878-1893},
Month = {APR 9},
Abstract = {Machine learning (ML) technology is a powerful tool in food science and
   engineering offering numerous advantages, from recognizing patterns and
   predicting outcomes to customizing and adjusting to individual needs.
   Its further development can enable researchers and industries to
   significantly enhance the efficiency of dairy processing while providing
   valuable insights into the field. This paper presents an overview of the
   role of machine learning in the dairy industry and its potential to
   improve the efficiency of dairy processing. We performed a systematic
   search for articles published between January 2003 and January 2023
   related to machine learning in dairy products and highlighted the
   algorithms used. 48 studies are discussed to assist researchers in
   identifying the best methods that could be applied in their field and
   providing relevant ideas for future research directions. Moreover, a
   step-by-step guide to the machine learning process, including a
   classification of different machine learning algorithms, is provided.
   This review focuses on state-of-the-art machine learning applications in
   milk products and their transformation into other dairy products, but it
   also presents future perspectives and conclusions. The study serves as a
   valuable guide for individuals in the dairy industry interested in
   learning about or getting involved with ML.},
DOI = {10.1080/10408398.2024.2312537},
EarlyAccessDate = {FEB 2024},
ISSN = {1040-8398},
EISSN = {1549-7852},
ResearcherID-Numbers = {Freire Vásconez, Paulina Andrea/HZM-2747-2023
   Licon, Carmen/F-4832-2013},
ORCID-Numbers = {Freire Vasconez, Paulina Andrea/0000-0001-8371-5893
   Licon, Carmen C./0000-0001-9053-9832
   },
Unique-ID = {WOS:001162062000001},
}

@article{ WOS:000874987300005,
Author = {Zhang, Lei and Shao, Shaofeng},
Title = {Image-based machine learning for materials science},
Journal = {JOURNAL OF APPLIED PHYSICS},
Year = {2022},
Volume = {132},
Number = {10},
Month = {SEP 14},
Abstract = {Materials research studies are dealing with a large number of images,
   which can now be facilitated via image-based machine learning
   techniques. In this article, we review recent progress of machine
   learning-driven image recognition and analysis for the materials and
   chemical domains. First, the image-based machine learning that
   facilitates the property prediction of chemicals or materials is
   discussed. Second, the analysis of nanoscale images including those from
   a scanning electron microscope and a transmission electron microscope is
   discussed, which is followed by the discussion about the identification
   of molecular structures via image recognition. Subsequently, the
   image-based machine learning works to identify and classify various
   practical materials such as metal, ceramics, and polymers are provided,
   and the image recognition for a range of real-scenario device
   applications such as solar cells is provided in detail. Finally,
   suggestions and future outlook for image based machine learning for
   classification and prediction tasks in the materials and chemical
   science are presented. This article highlights the importance of the
   integration of the image-based machine learning method into materials
   and chemical science and calls for a large-scale deployment of
   image-based machine learning methods for prediction and classification
   of images in materials and chemical science. Published under an
   exclusive license by AIP Publishing.},
DOI = {10.1063/5.0087381},
Article-Number = {100701},
ISSN = {0021-8979},
EISSN = {1089-7550},
ResearcherID-Numbers = {Zhang, Lei/U-4622-2019
   Zhang, Lei/A-1194-2014},
ORCID-Numbers = {Zhang, Lei/0000-0001-6873-7314},
Unique-ID = {WOS:000874987300005},
}

@article{ WOS:000730729600001,
Author = {Fuster, Andreas and Goldsmith-Pinkham, Paul and Ramadorai, Tarun and
   Walther, Ansgar},
Title = {Predictably Unequal? The Effects of Machine Learning on Credit Markets},
Journal = {JOURNAL OF FINANCE},
Year = {2022},
Volume = {77},
Number = {1},
Pages = {5-47},
Month = {FEB},
Abstract = {Innovations in statistical technology in functions including
   credit-screening have raised concerns about distributional impacts
   across categories such as race. Theoretically, distributional effects of
   better statistical technology can come from greater flexibility to
   uncover structural relationships or from triangulation of otherwise
   excluded characteristics. Using data on U.S. mortgages, we predict
   default using traditional and machine learning models. We find that
   Black and Hispanic borrowers are disproportionately less likely to gain
   from the introduction of machine learning. In a simple equilibrium
   credit market model, machine learning increases disparity in rates
   between and within groups, with these changes attributable primarily to
   greater flexibility.},
DOI = {10.1111/jofi.13090},
EarlyAccessDate = {DEC 2021},
ISSN = {0022-1082},
EISSN = {1540-6261},
ResearcherID-Numbers = {Ramadorai, Tarun/J-7421-2013
   Ramadorai, Tarun/ABF-2035-2020},
ORCID-Numbers = {Ramadorai, Tarun/0000-0002-4145-205X
   },
Unique-ID = {WOS:000730729600001},
}

@article{ WOS:000435400900032,
Author = {Alshammari, Riyad},
Title = {Arabic Text Categorization using Machine Learning Approaches},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2018},
Volume = {9},
Number = {3},
Pages = {226-230},
Month = {MAR},
Abstract = {Arabic Text categorization is considered one of the severe problems in
   classification using machine learning algorithms. Achieving high
   accuracy in Arabic text categorization depends on the preprocessing
   techniques used to prepare the data set. Thus, in this paper, an
   investigation of the impact of the preprocessing methods concerning the
   performance of three machine learning algorithms, namely, Naive
   Bayesian, DMNBtext and C4.5 is conducted. Results show that the DMNBtext
   learning algorithm achieved higher performance compared to other machine
   learning algorithms in categorizing Arabic text.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Unique-ID = {WOS:000435400900032},
}

@article{ WOS:000802887500001,
Author = {Dborin, James and Barratt, Fergus and Wimalaweera, Vinul and Wright,
   Lewis and Green, Andrew G.},
Title = {Matrix product state pre-training for quantum machine learning},
Journal = {QUANTUM SCIENCE AND TECHNOLOGY},
Year = {2022},
Volume = {7},
Number = {3},
Month = {JUL 1},
Abstract = {Hybrid quantum-classical algorithms are a promising candidate for
   developing uses for NISQ devices. In particular, parametrised quantum
   circuits (PQCs) paired with classical optimizers have been used as a
   basis for quantum chemistry and quantum optimization problems. Tensor
   network methods are being increasingly used as a classical machine
   learning tool, as well as a tool for studying quantum systems. We
   introduce a circuit pre-training method based on matrix product state
   machine learning methods, and demonstrate that it accelerates training
   of PQCs for both supervised learning, energy minimization, and
   combinatorial optimization.},
DOI = {10.1088/2058-9565/ac7073},
Article-Number = {035014},
ISSN = {2058-9565},
ResearcherID-Numbers = {Green, Andrew/A-2731-2013
   },
ORCID-Numbers = {Wimalaweera, Vinul/0000-0002-2230-6428
   Green, Andrew/0000-0002-3923-5291},
Unique-ID = {WOS:000802887500001},
}

@article{ WOS:000931530800001,
Author = {Qiao, Qingyao and Yunusa-Kaltungo, Akilu},
Title = {A hybrid agent-based machine learning method for human-centred energy
   consumption prediction},
Journal = {ENERGY AND BUILDINGS},
Year = {2023},
Volume = {283},
Month = {MAR 15},
Abstract = {Occupant behaviour has significant impacts on the performance of machine
   learning algorithms when predicting building energy consumption. Due to
   a variety of reasons (e.g., underperforming building energy management
   systems or restrictions due to privacy policies), the availability of
   occupational data has long been an obstacle that hinders the performance
   of machine learning algorithms in predicting building energy
   consumption. Therefore, this study proposed an agent-based machine
   learning model whereby agent-based modelling was employed to generate
   simulated occupational data as input features for machine learning
   algorithms for building energy consumption prediction. Boruta feature
   selection was also introduced in this study to select all relevant
   features. The results indicated that the perfor-mances of machine
   learning algorithms in predicting building energy consumption were
   significantly improved when using simulated occupational data, with even
   greater improvements after conducting Boruta feature selection.(c) 2023
   The Authors. Published by Elsevier B.V. This is an open access article
   under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
DOI = {10.1016/j.enbuild.2023.112797},
EarlyAccessDate = {JAN 2023},
Article-Number = {112797},
ISSN = {0378-7788},
EISSN = {1872-6178},
ResearcherID-Numbers = {yunusa-kaltungo, akilu/AFM-7988-2022},
ORCID-Numbers = {qiao, qingyao/0000-0002-5541-2681
   yunusa-kaltungo, akilu/0000-0001-5138-3783},
Unique-ID = {WOS:000931530800001},
}

@article{ WOS:000540819200006,
Author = {Lattimer, B. Y. and Hodges, J. L. and Lattimer, A. M.},
Title = {Using machine learning in physics-based simulation of fire},
Journal = {FIRE SAFETY JOURNAL},
Year = {2020},
Volume = {114},
Month = {JUN},
Note = {9th International Seminar on Fire and Explosion Hazards, Saint
   Petersburg, RUSSIA, APR 21-26, 2019},
Abstract = {There is a current need to provide rapid, high fidelity predictions of
   fires to support hazard/risk assessments, use sparse data to understand
   conditions, and develop mitigation strategies. Machine learning is one
   approach that has been used to provide rapid predictions based on large
   amounts of data in business, robotics, and image analysis; however,
   there have been limited applications to support physics-based or science
   applications. This paper provides a general overview of machine learning
   with details on specific techniques being explored for performing
   low-cost, high fidelity fire predictions. Examples of using both
   dimensionality reduction (reduced-order models) and deep learning with
   neural networks are provided. When compared with CFD results, these
   initial studies show that machine learning can provide full-field
   predictions 2-3 orders of magnitude faster than CFD simulations. Further
   work is needed to improve machine learning accuracy and extend these
   models to more general scenarios.},
DOI = {10.1016/j.firesaf.2020.102991},
Article-Number = {102991},
ISSN = {0379-7112},
EISSN = {1873-7226},
ResearcherID-Numbers = {Lattimer, Brian/A-5114-2010
   Hodges, Jonathan/HSE-8894-2023
   },
ORCID-Numbers = {Hodges, Jonathan/0000-0001-6830-0622},
Unique-ID = {WOS:000540819200006},
}

@article{ WOS:000630618200001,
Author = {Cemernek, David and Cemernek, Sandra and Gursch, Heimo and Pandeshwar,
   Ashwini and Leitner, Thomas and Berger, Matthias and Klosch, Gerald and
   Kern, Roman},
Title = {Machine learning in continuous casting of steel: a state-of-the-art
   survey},
Journal = {JOURNAL OF INTELLIGENT MANUFACTURING},
Year = {2022},
Volume = {33},
Number = {6},
Pages = {1561-1579},
Month = {AUG},
Abstract = {Continuous casting is the most important route for the production of
   steel today. Due to the physical, mechanical, and chemical components
   involved in the production, continuous casting is a very complex
   process, pushing conventional methods of monitoring and control to their
   limits. In recent years, this complexity and the increasing global
   competition created a demand for new methods to monitor and control the
   continuous casting process. Due to the success and associated rise of
   machine learning techniques in recent years, machine learning nowadays
   plays an essential role in monitoring and controlling complex processes.
   This publication presents a scientific survey of machine learning
   techniques for the analysis of the continuous casting process. We
   provide an introduction to both the involved fields: an overview of
   machine learning, and an overview of the continuous casting process.
   Therefore, we first analyze the existing work concerning machine
   learning in continuous casting of steel and then synthesize the common
   concepts into categories, supporting the identification of common use
   cases and approaches. This analysis is concluded with the elaboration of
   challenges, potential solutions, and a future outlook of further
   research directions.},
DOI = {10.1007/s10845-021-01754-7},
EarlyAccessDate = {MAR 2021},
ISSN = {0956-5515},
EISSN = {1572-8145},
ResearcherID-Numbers = {Kern, Roman/ABG-3805-2020
   Gursch, Heimo/S-4264-2018},
ORCID-Numbers = {Kern, Roman/0000-0003-0202-6100
   },
Unique-ID = {WOS:000630618200001},
}

@article{ WOS:000890647400007,
Author = {Kanakis, Marios Evangelos and Khalili, Ramin and Wang, Lin},
Title = {Machine Learning for Computer Systems and Networking: A Survey},
Journal = {ACM COMPUTING SURVEYS},
Year = {2023},
Volume = {55},
Number = {4},
Month = {MAY},
Abstract = {Machine learning (ML) has become the de-facto approach for various
   scientific domains such as computer vision and natural language
   processing. Despite recent breakthroughs, machine learning has only made
   its way into the fundamental challenges in computer systems and
   networking recently. This article attempts to shed light on recent
   literature that appeals for machine learning-based solutions to
   traditional problems in computer systems and networking. To this end, we
   first introduce a taxonomy based on a set of major research problem
   domains. Then, we present a comprehensive review per domain, where we
   compare the traditional approaches against the machine learning-based
   ones. Finally, we discuss the general limitations of machine learning
   for computer systems and networking, including lack of training data,
   training overhead, real-time performance, and explainability, and reveal
   future research directions targeting these limitations.},
DOI = {10.1145/3523057},
Article-Number = {71},
ISSN = {0360-0300},
EISSN = {1557-7341},
Unique-ID = {WOS:000890647400007},
}

@article{ WOS:000715130600001,
Author = {Bilgilioglu, Suleyman Sefa and Yilmaz, Haci Murat},
Title = {Comparison of different machine learning models for mass appraisal of
   real estate},
Journal = {SURVEY REVIEW},
Year = {2023},
Volume = {55},
Number = {388},
Pages = {32-43},
Month = {JAN 2},
Abstract = {The present study aimed to compare five machine learning techniques,
   namely, artificial neural network (ANN), support vector machine (SVM),
   chi-square automatic interaction detection (CHAID), classification and
   regression tree (CART), and random forest (RF) for mass appraisal of
   real estate. Firstly, 1982 precedent data was collected throughout the
   entire study area for train and test models. Secondly, a total of 68
   variables were considered for the mass appraisal. Subsequently, the five
   machine learning techniques were applied. Finally, the receiver
   operating characteristic (ROC) and various statistical methods were
   applied to compare five machine learning techniques.},
DOI = {10.1080/00396265.2021.1996799},
EarlyAccessDate = {NOV 2021},
ISSN = {0039-6265},
EISSN = {1752-2706},
ResearcherID-Numbers = {Bilgilioğlu, S. Sefa/ABC-7525-2021
   Yilmaz, H.Murat/J-6711-2016},
ORCID-Numbers = {YILMAZ, H.Murat/0000-0002-9725-5792
   },
Unique-ID = {WOS:000715130600001},
}

@article{ WOS:001109664000001,
Author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Han,
   Song},
Title = {Tiny Machine Learning: Progress and Futures},
Journal = {IEEE CIRCUITS AND SYSTEMS MAGAZINE},
Year = {2023},
Volume = {23},
Number = {3},
Pages = {8-34},
Abstract = {Tiny machine learning (TinyML) is a new frontier of machine learning. By
   squeezing deep learning models into billions of IoT devices and
   microcontrollers (MCUs), we expand the scope of applications and enable
   ubiquitous intelligence. However, TinyML is challenging due to the
   hardware constraints: the tiny memory resource is difficult hold deep
   learning models designed for cloud and mobile platforms. There is also
   limited compiler and inference engine support for bare-metal devices.
   Therefore, we need to co-design the algorithm and system stack to enable
   TinyML. In this review, we will first discuss the definition,
   challenges, and applications of TinyML. We then survey the recent
   progress in TinyML and deep learning on MCUs. Next, we will introduce
   MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT
   devices with system-algorithm co-design. We will further extend the
   solution from inference to training and introduce tiny on-device
   training techniques. Finally, we present future directions in this area.
   Today's ``large{''} model might be tomorrow's ``tiny{''} model. The
   scope of TinyML should evolve and adapt over time.},
DOI = {10.1109/MCAS.2023.3302182},
ISSN = {1531-636X},
EISSN = {1558-0830},
ResearcherID-Numbers = {Han, Song/AAR-9464-2020
   Chen, Wei-Ming/ACA-7706-2022
   Wang, Wei-Chen/ABC-8145-2020},
ORCID-Numbers = {Han, Song/0000-0002-4186-7618
   },
Unique-ID = {WOS:001109664000001},
}

@article{ WOS:000495542300001,
Author = {Kreiner, Aaron and Duca, John V.},
Title = {Can machine learning on economic data better forecast the unemployment
   rate?},
Journal = {APPLIED ECONOMICS LETTERS},
Year = {2020},
Volume = {27},
Number = {17},
Pages = {1434-1437},
Month = {OCT 6},
Abstract = {Using FRED data, a machine-learning model outperforms the Survey of
   Professional Forecasters and other models since 2001 in forecasting the
   unemployment rate.},
DOI = {10.1080/13504851.2019.1688237},
EarlyAccessDate = {NOV 2019},
ISSN = {1350-4851},
EISSN = {1466-4291},
Unique-ID = {WOS:000495542300001},
}

@article{ WOS:000396957800011,
Author = {Zhu, Changming and Wang, Zhe},
Title = {Entropy-based matrix learning machine for imbalanced data sets},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2017},
Volume = {88},
Pages = {72-80},
Month = {MAR 1},
Abstract = {Imbalance problem occurs when negative class contains many more patterns
   than that of positive class. Since conventional Support Vector Machine
   (SVM) and Neural Networks (NN) have been proven not to effectively
   handle imbalanced data, some improved learning machines including Fuzzy
   SVM (FSVM) have been proposed. FSVM applies a fuzzy membership to each
   training pattern such that different patterns can give different
   contributions to the learning machine. However, how to evaluate fuzzy
   membership becomes the key point to FSVM. Moreover, these learning
   machines present disadvantages to process matrix patterns. In order to
   process matrix patterns and to tackle the imbalance problem, this paper
   proposes an entropy-based matrix learning machine for imbalanced data
   sets, adopting the Matrix-pattern oriented Ho-Kashyap learning machine
   with regularization learning (MatMHKS) as the base classifier. The new
   leaning machine is named EMatMHKS and its contributions are: (1)
   proposing a new entropy-based fuzzy membership evaluation approach which
   enhances the importance of patterns, (2) guaranteeing the importance of
   positive patterns and get a more flexible decision surface. Experiments
   on real-world imbalanced data sets validate that EMatMHKS outperforms
   compared learning machines. (C) 2017 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.patrec.2017.01.014},
ISSN = {0167-8655},
EISSN = {1872-7344},
ResearcherID-Numbers = {Zhu, Changming/LEM-1762-2024},
Unique-ID = {WOS:000396957800011},
}

@article{ WOS:000850207400010,
Author = {Jhaveri, Rutvij H. and Revathi, A. and Ramana, Kadiyala and Raut,
   Roshani and Dhanaraj, Rajesh Kumar},
Title = {A Review on Machine Learning Strategies for Real-World Engineering
   Applications},
Journal = {MOBILE INFORMATION SYSTEMS},
Year = {2022},
Volume = {2022},
Month = {AUG 28},
Abstract = {Huge amounts of data are circulating in the digital world in the era of
   the Industry 5.0 revolution. Machine learning is experiencing success in
   several sectors such as intelligent control, decision making, speech
   recognition, natural language processing, computer graphics, and
   computer vision, despite the requirement to analyze and interpret data.
   Due to their amazing performance, Deep Learning and Machine Learning
   Techniques have recently become extensively recognized and implemented
   by a variety of real-time engineering applications. Knowledge of machine
   learning is essential for designing automated and intelligent
   applications that can handle data in fields such as health,
   cyber-security, and intelligent transportation systems. There are a
   range of strategies in the field of machine learning, including
   reinforcement learning, semi-supervised, unsupervised, and supervised
   algorithms. This study provides a complete study of managing real-time
   engineering applications using machine learning, which will improve an
   application's capabilities and intelligence. This work adds to the
   understanding of the applicability of various machine learning
   approaches in real-world applications such as cyber security,
   healthcare, and intelligent transportation systems. This study
   highlights the research objectives and obstacles that Machine Learning
   approaches encounter while managing real-world applications. This study
   will act as a reference point for both industry professionals and
   academics, and from a technical standpoint, it will serve as a benchmark
   for decision-makers on a range of application domains and real-world
   scenarios.},
DOI = {10.1155/2022/1833507},
Article-Number = {1833507},
ISSN = {1574-017X},
EISSN = {1875-905X},
ResearcherID-Numbers = {Dhanaraj, Rajesh Kumar/AAQ-6545-2021
   A, Revathi/ABE-8546-2021
   Jhaveri, Rutvij/A-5354-2018
   Kadiyala, Ramana/AAF-4301-2020
   Raut, Dr Roshani/AAH-1773-2021},
ORCID-Numbers = {Dhanaraj, Rajesh Kumar/0000-0002-2038-7359
   A, Revathi/0000-0003-3980-1473
   Kadiyala, Ramana/0000-0002-4604-846X
   Raut, Dr Roshani/0000-0002-5477-8841
   },
Unique-ID = {WOS:000850207400010},
}

@article{ WOS:000540984000015,
Author = {Sealfon, Rachel S. G. and Mariani, Laura H. and Kretzler, Matthias and
   Troyanskaya, Olga G.},
Title = {Machine learning, the kidney, and genotype-phenotype analysis},
Journal = {KIDNEY INTERNATIONAL},
Year = {2020},
Volume = {97},
Number = {6},
Pages = {1141-1149},
Month = {JUN},
Abstract = {With biomedical research transitioning into data-rich science, machine
   learning provides a powerful toolkit for extracting knowledge from
   large-scale biological data sets. The increasing availability of
   comprehensive kidney omics compendia (transcriptomics, proteomics,
   metabolomics, and genome sequencing), as well as other data modalities
   such as electronic health records, digital nephropathology repositories,
   and radiology renal images, makes machine learning approaches
   increasingly essential for analyzing human kidney data sets. Here, we
   discuss how machine learning approaches can be applied to the study of
   kidney disease, with a particular focus on how they can be used for
   understanding the relationship between genotype and phenotype.},
DOI = {10.1016/j.kint.2020.02.028},
ISSN = {0085-2538},
EISSN = {1523-1755},
ResearcherID-Numbers = {Troyanskaya, Olga/B-1968-2010
   },
ORCID-Numbers = {Troyanskaya, Olga/0000-0002-5676-5737},
Unique-ID = {WOS:000540984000015},
}

@article{ WOS:000454421100001,
Author = {Rao, N. Thirupathi},
Title = {A Review on Industrial Applications of Machine Learning},
Journal = {INTERNATIONAL JOURNAL OF DISASTER RECOVERY AND BUSINESS CONTINUITY},
Year = {2018},
Volume = {9},
Pages = {1-9},
Month = {NOV},
Abstract = {Machine learning is the rapidly growing technology in the field of
   almost all recent technologies in the market. With the successful
   application of machine learning in almost all the recent technologies,
   the growth in all the areas was splendid. The growth in those areas has
   crossed the expectations of the scientists. Recently, the application of
   machine learning in the areas of medicine and pharma is growing in the
   recent times in a rapid fast. In the current paper, the authors
   represent the seven applications or the areas in the field of medicine
   and pharma where the applications of the machine learning were
   implementing and good results are obtaining.},
DOI = {10.14257/ijdrbc.2018.9.01},
ISSN = {2005-4289},
EISSN = {2207-6425},
ResearcherID-Numbers = {N, Thirupathi Rao/F-2672-2014
   },
ORCID-Numbers = {N, Thirupathi Rao/0000-0002-8957-8532},
Unique-ID = {WOS:000454421100001},
}

@article{ WOS:001139560900001,
Author = {Al Miaari, Ahmad and Ali, Hafiz Muhammad},
Title = {Batteries temperature prediction and thermal management using machine
   learning: An overview},
Journal = {ENERGY REPORTS},
Year = {2023},
Volume = {10},
Pages = {2277-2305},
Month = {NOV},
Abstract = {Batteries, particularly lithium-ion batteries, play an important role in
   powering our modern world, from portable devices to electric vehicles
   and renewable energy storage. However, during charging and discharging,
   they generate heat due to chemical reactions within them. This heat can
   lead to reduced performance, shortened lifespan, and even safety risks
   if not properly managed. To address this problem, Machine learning has
   been emerged as a changing tool in battery technology due to its ability
   to analyze large datasets that can be used in predicting battery
   temperatures and enhancing their thermal management. In this work, we
   address machine learning features along with a look at its various
   learning categories, frameworks, and applications. In a comprehensive
   study, various machine learning methods and neural networks used in
   battery temperature prediction and thermal management are analyzed and
   discussed along with its various training algorithms. Moreover, the
   paper reviews and summarizes various research publications examining
   battery temperature prediction and battery thermal management using the
   various machine learning algorithms. As a result, there is no superior
   machine learning algorithm for battery temperature prediction and
   thermal management, as the performance of the model may vary depending
   on the data set, training algorithm, and other parameters. However,
   among these machine learning algorithms researchers are preferring to
   use artificial neural networks due to its accuracy and model complexity.
   In particular, artificial neural network integrated with proper cooling
   technology can reduce the battery temperature by more than 25\%.},
DOI = {10.1016/j.egyr.2023.08.043},
ISSN = {2352-4847},
ResearcherID-Numbers = {Al Miaari, Ahmad/NMJ-7794-2025
   Ali, HM/AAH-3473-2021
   },
ORCID-Numbers = {Al Miaari, Ahmad/0000-0002-0011-9154},
Unique-ID = {WOS:001139560900001},
}

@article{ WOS:000853623600001,
Author = {Dhasaradhan, K. and Jaichandran, R.},
Title = {RETRACTED: Performance analysis of machine learning algorithms in heart
   disease prediction (Retracted Article)},
Journal = {CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS},
Year = {2022},
Volume = {30},
Number = {4},
Pages = {335-343},
Month = {DEC},
Abstract = {This work presents performance analysis of machine learning algorithms
   such as logistic regression, naive bayes, decision tree, k nearest
   neighbour, random forest, support vector machine, and extreme gradient
   boosting in heart disease prediction. Machine learning algorithms are
   implemented in python using Scikit learn library in Jupiter notebook.
   Experiments are conducted by training and testing machine learning
   algorithms using kaggle heart disease dataset under six test cases.
   Performance of machine learning algorithms are evaluated using accuracy,
   precision, recall, F1 score and ROC as metrics. Results show random
   forest reported high accuracy, precision, recall, F1 score and ROC in
   heart disease prediction compared to other machine learning algorithms
   in all six test cases. Results show RF is effective in heart disease
   prediction in Case 3 with 80\% train data and 20\% test data.},
DOI = {10.1177/1063293X221125231},
EarlyAccessDate = {SEP 2022},
ISSN = {1063-293X},
EISSN = {1531-2003},
ResearcherID-Numbers = {K, Dhasaradhan/HWQ-4490-2023
   r, jaichandran/ABF-7443-2020
   },
ORCID-Numbers = {, Dhasaradhan/0000-0001-8665-6905},
Unique-ID = {WOS:000853623600001},
}

@article{ WOS:000567789900003,
Author = {Maas, Martin},
Title = {A Taxonomy of ML for Systems Problems},
Journal = {IEEE MICRO},
Year = {2020},
Volume = {40},
Number = {5},
Pages = {8-16},
Month = {SEP-OCT},
Abstract = {Machine learning has the potential to significantly improve systems, but
   only under certain conditions. We describe a taxonomy to help identify
   whether or not machine learning should be applied to particular systems
   problems, and which approaches are most promising. We believe that this
   taxonomy can help practitioners and researchers decide how to most
   effectively use machine learning in their systems, and provide the
   community with a framework and vocabulary to discuss different
   approaches for applying machine learning in systems.},
DOI = {10.1109/MM.2020.3012883},
ISSN = {0272-1732},
EISSN = {1937-4143},
Unique-ID = {WOS:000567789900003},
}

@article{ WOS:000852913100007,
Author = {Yang, Y. and Zhang, W. and Wang, Zh and Li, Y.},
Title = {Differentiation of Plastics by Combining Raman Spectroscopy and Machine
   Learning},
Journal = {JOURNAL OF APPLIED SPECTROSCOPY},
Year = {2022},
Volume = {89},
Number = {4},
Pages = {790-798},
Month = {SEP},
Abstract = {We combined Raman spectroscopy with machine learning for the
   classification of 11 plastic samples. A confocal Raman system with an
   excitation wavelength of 532 nm was used to collect the Raman spectral
   data of plastic samples and principal component analysis was used for
   feature extraction. The prediction models of plastic classification
   based on three machine learning algorithms are compared. The results
   show that all three machine learning algorithms are able to classify 11
   plastics well. This indicates that the combination of Raman spectroscopy
   and machine learning has great potential in the rapid and nondestructive
   classification of plastics.},
DOI = {10.1007/s10812-022-01426-1},
EarlyAccessDate = {SEP 2022},
ISSN = {0021-9037},
EISSN = {1573-8647},
ResearcherID-Numbers = {li, yuee/AAG-9902-2019
   Wang, Zhong/JNS-5162-2023
   wang, zhong/JNS-5162-2023},
ORCID-Numbers = {wang, zhong/0000-0002-8648-9999},
Unique-ID = {WOS:000852913100007},
}

@article{ WOS:000558696600001,
Author = {Karamitsos, Ioannis and Albarhami, Saeed and Apostolopoulos, Charalampos},
Title = {Applying DevOps Practices of Continuous Automation for Machine Learning},
Journal = {INFORMATION},
Year = {2020},
Volume = {11},
Number = {7},
Month = {JUL},
Abstract = {This paper proposes DevOps practices for machine learning application,
   integrating both the development and operation environment seamlessly.
   The machine learning processes of development and deployment during the
   experimentation phase may seem easy. However, if not carefully designed,
   deploying and using such models may lead to a complex, time-consuming
   approaches which may require significant and costly efforts for
   maintenance, improvement, and monitoring. This paper presents how to
   apply continuous integration (CI) and continuous delivery (CD)
   principles, practices, and tools so as to minimize waste, support rapid
   feedback loops, explore the hidden technical debt, improve value
   delivery and maintenance, and improve operational functions for
   real-world machine learning applications.},
DOI = {10.3390/info11070363},
Article-Number = {363},
EISSN = {2078-2489},
ResearcherID-Numbers = {Thabit, Saeed/ODM-2808-2025
   Karamitsos, Ioannis/AAG-4618-2021
   },
ORCID-Numbers = {Thabit, Saeed/0000-0001-9868-7477
   Karamitsos, Ioannis (Yannis)/0000-0001-6106-6423},
Unique-ID = {WOS:000558696600001},
}
