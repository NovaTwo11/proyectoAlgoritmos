@inproceedings{10.1145/3757110.3757152, title = {Telecom Customer Churn Prediction with Explainable Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {246--250}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757152}, url = {https://doi.org/10.1145/3757110.3757152}, author = {Yuan, Jiaming and Liu, Hao}, keywords = {CatBoost, SHAP, customer churn, machine learning, telecommunication}, abstract = {In the fast-paced era of digital transformation, the telecoms industry is grappling with an increasing issue of customer churn, which endangers stable revenue and operational efficiency. This paper therefore proposes an interpretable machine learning framework that aims to identify customers who are likely to churn more accurately, while also enhancing the interpretability of the model. The performance of the model is evaluated using the publicly available Iranian churn dataset by means of data preprocessing (duplicate removal, outlier detection, feature selection and class imbalance mitigation), training of seven machine learning models tuned with hyper-parameters and selection of five key performance indicators. Experimental results demonstrate that the CatBoost model achieves excellent performance, with an accuracy of 0.9554, precision of 0.9010, recall of 0.8072, F1 score of 0.8503 and AUC-ROC of 0.9823 — surpassing the performance of the other six models. Additionally, this study explored the stacking model, which ultimately proved to be inferior to the CatBoost model. Finally, the optimal model was interpreted using the SHAP technique. The analysis identified three key characteristics affecting customer churn: seconds of use (negative correlation), call failures (positive correlation) and frequency of SMS (negative correlation). This led to the derivation of an operational strategy for customer retention in the telecommunications industry.} }
@inproceedings{10.1145/3769394.3769411, title = {Sustainable Machine Learning: Course 1}, booktitle = {Proceedings of the Conference on 6th ACM Europe Summer School on Data Science}, pages = {17}, year = {2025}, doi = {10.1145/3769394.3769411}, url = {https://doi.org/10.1145/3769394.3769411}, author = {Kemme, Bettina}, abstract = {Machine learning has become increasingly data and processing hungry. A recent report from the International Energy Agency projects that the electricity demand for data centers specialized in AI will more than quadruple by 2030. As such, it has become a pressing need to include energy awareness and environmental sustainability into the Machine Learning life cycle. In fact, a considerable amount of research efforts have been conducted in the last years in this direction.The first part of this tutorial will discuss various mechanisms to assess the environmental impact of machine learning, from power and energy consumption to carbon footprint. This will be put in relation to more traditional performance metrics used in the research literature, from the "goodness" of a ML solution, measured by metrics such as accuracy, to systems performance metrics such as runtime, throughput and scalability. From there, the tutorial will present several concrete research efforts for a quantitative analysis of the environmental footprint of various ML tasks.The second part of the tutorial will outline recent solutions to tackle the huge energy consumption of modern ML. For instance, there have been an increasing number of research efforts to make both the learning and the inference tasks more efficient while providing similar performance in terms of traditional ML performance metrics such as accuracy. A further line of research focuses on adjusting the infrastructure or the execution of ML tasks to be more energy aware, e.g., through scheduling approaches.} }
@inproceedings{10.1145/3744199.3744635, title = {Automated Video Segmentation Machine Learning Pipeline}, booktitle = {Proceedings of the Digital Production Symposium}, year = {2025}, isbn = {9798400720086}, doi = {10.1145/3744199.3744635}, url = {https://doi.org/10.1145/3744199.3744635}, author = {Merz, Johannes and Fostier, Lucien}, keywords = {Image Processing, Video Segmentation, Object Detection, Machine Learning}, abstract = {Visual effects (VFX) production often struggles with slow, resource-intensive mask generation. This paper presents an automated video segmentation pipeline that creates temporally consistent instance masks. It employs machine learning for: (1) flexible object detection via text prompts, (2) refined per-frame image segmentation and (3) robust video tracking to ensure temporal stability. Deployed using containerization and leveraging a structured output format, the pipeline was quickly adopted by our artists. It significantly reduces manual effort, speeds up the creation of preliminary composites, and provides comprehensive segmentation data, thereby enhancing overall VFX production efficiency.} }
@article{10.1145/3704807, title = {Collaborative Distributed Machine Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3704807}, url = {https://doi.org/10.1145/3704807}, author = {Jin, David and Kannengieer, Niclas and Rank, Sascha and Sunyaev, Ali}, keywords = {Collaborative distributed machine learning (CDML), privacy-enhancing technologies (PETs), assisted learning, federated learning (FL), split learning, swarm learning, multi-agent systems (MAS)}, abstract = {Various collaborative distributed machine learning (CDML) systems, including federated learning systems and swarm learning systems, with different key traits were developed to leverage resources for the development and use of machine learning models in a confidentiality-preserving way. To meet use case requirements, suitable CDML systems need to be selected. However, comparison between CDML systems to assess their suitability for use cases is often difficult. To support comparison of CDML systems and introduce scientific and practical audiences to the principal functioning and key traits of CDML systems, this work presents a CDML system conceptualization and CDML archetypes.} }
@inproceedings{10.1145/3769394.3769410, title = {Sustainable Machine Learning: Course 1}, booktitle = {Proceedings of the Conference on 6th ACM Europe Summer School on Data Science}, pages = {16}, year = {2025}, doi = {10.1145/3769394.3769410}, url = {https://doi.org/10.1145/3769394.3769410}, author = {Kemme, Bettina}, abstract = {Machine learning has become increasingly data and processing hungry. A recent report from the International Energy Agency projects that the electricity demand for data centers specialized in AI will more than quadruple by 2030. As such, it has become a pressing need to include energy awareness and environmental sustainability into the Machine Learning life cycle. In fact, a considerable amount of research efforts have been conducted in the last years in this direction.The first part of this tutorial will discuss various mechanisms to assess the environmental impact of machine learning, from power and energy consumption to carbon footprint. This will be put in relation to more traditional performance metrics used in the research literature, from the "goodness" of a ML solution, measured by metrics such as accuracy, to systems performance metrics such as runtime, throughput and scalability. From there, the tutorial will present several concrete research efforts for a quantitative analysis of the environmental footprint of various ML tasks.The second part of the tutorial will outline recent solutions to tackle the huge energy consumption of modern ML. For instance, there have been an increasing number of research efforts to make both the learning and the inference tasks more efficient while providing similar performance in terms of traditional ML performance metrics such as accuracy. A further line of research focuses on adjusting the infrastructure or the execution of ML tasks to be more energy aware, e.g., through scheduling approaches.} }
@article{10.1145/3767157, title = {How do Machine Learning Models Change?}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3767157}, url = {https://doi.org/10.1145/3767157}, author = {Casta\~no, Joel and Caba\~nas, Rafael and Salmer\'on, Antonio and Lo, David and Mart\'nez-Fern\'andez, Silverio}, keywords = {ML Software Evolution, ML Model Changes, ML Software Releases, Commit Type Classification, Bayesian Networks in Software Engineering}, abstract = {The proliferation of Machine Learning (ML) models and their open-source implementations has transformed Artificial Intelligence research and applications. Platforms like Hugging Face (HF) enable this evolving ecosystem, yet a large-scale longitudinal study of how these models change is lacking. This study addresses this gap by analyzing over 680,000 commits from 100,000 models and 2,251 releases from 202 of these models on HF using repository mining and longitudinal methods. We apply an extended ML change taxonomy to classify commits and use Bayesian networks to model temporal patterns in commit and release activities. Our findings show that commit activities align with established data science methodologies, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM), emphasizing iterative refinement. Release patterns tend to consolidate significant updates, particularly in model outputs, sharing, and documentation, distinguishing them from granular commits. Furthermore, projects with higher popularity exhibit distinct evolutionary paths, often starting from a more mature baseline with fewer foundational commits in their public history. In contrast, those with intensive collaboration show unique documentation and technical evolution patterns. These insights enhance the understanding of model changes on community platforms and provide valuable guidance for best practices in model maintenance.} }
@article{10.1145/3761822, title = {Sample Selection Bias in Machine Learning for Healthcare}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761822}, url = {https://doi.org/10.1145/3761822}, author = {Chauhan, Vinod Kumar and Clifton, Lei and Sala\"un, Achille and Lu, Huiqi Yvonne and Branson, Kim and Schwab, Patrick and Nigam, Gaurav and Clifton, David A.}, keywords = {Sample Selection Bias, Target Population, Machine Learning, Healthcare, Risk Prediction}, abstract = {While machine learning algorithms hold promise for personalised medicine, their clinical adoption remains limited, partly due to biases that can compromise the reliability of predictions. In this article, we focus on sample selection bias (SSB), a specific type of bias where the study population is less representative of the target population, leading to biased and potentially harmful decisions. Despite being well-known in the literature, SSB remains scarcely studied in machine learning for healthcare. Moreover, the existing machine learning techniques try to correct the bias mostly by balancing distributions between the study and the target populations, which may result in a loss of predictive performance. To address these problems, our study illustrates the potential risks associated with SSB by examining SSB’s impact on the performance of machine learning algorithms. Most importantly, we propose a new research direction for addressing SSB, based on the target population identification rather than the bias correction. Specifically, we propose two independent networks (T-Net) and a multitasking network (MT-Net) for addressing SSB, where one network/task identifies the target subpopulation which is representative of the study population and the second makes predictions for the identified subpopulation. Our empirical results with synthetic and semi-synthetic datasets highlight that SSB can lead to a large drop in the performance of an algorithm for the target population as compared with the study population, as well as a substantial difference in the performance for the target subpopulations that are representative of the selected and the non-selected patients from the study population. Furthermore, our proposed techniques demonstrate robustness across various settings, including different dataset sizes, event rates and selection rates, outperforming the existing bias correction techniques.} }
@inproceedings{10.1145/3736181.3747139, title = {Introducing Machine Learning to Children in Nigeria}, booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 1}, pages = {204--210}, year = {2025}, isbn = {9798400719295}, doi = {10.1145/3736181.3747139}, url = {https://doi.org/10.1145/3736181.3747139}, author = {John, Avong Emmanuel and Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday}, keywords = {basic education, children, machine learning education, nigeria, location = Gaborone, Botswana}, abstract = {This study explores the potential of machine learning (ML) education for children in Nigeria, addressing the significant dearth of such initiatives in Africa. We propose the design of ML intervention programs aimed at creating engaging learning experiences for children aged 7 to 16 years, who have no prior exposure to ML concepts. Through hands-on activities using online platforms, LearningML, which introduces classification techniques and image/text recognition and DoodleIt, which focuses on teaching convolutional neural networks, we investigate how these tools facilitate learning in a non-formal setting. Employing a mixed-method approach that includes pre- and post-surveys, as well as participant interviews, we analyze the data using descriptive and thematic methods. Our findings reveal that children grasp the importance of data model, training and other fundamental ML concepts, demonstrating that user-friendly ML tools can ignite curiosity and foster a desire to learn. The implications of this study are significant for advancing ML education among young learners in Africa.} }
@inproceedings{10.1145/3721251.3736530, title = {Introduction To Generative Machine Learning}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Labs}, year = {2025}, isbn = {9798400715501}, doi = {10.1145/3721251.3736530}, url = {https://doi.org/10.1145/3721251.3736530}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {This is an intermediate level course for attendees to gain a strong understanding of the basic principles of generative AI. The course will help build intuition around several topics with easy-to-understand explanations and examples from some of the prevalent algorithms and models including Autoencoders, CNN, Diffusion Models, Transformers, and NeRFs.} }
@inproceedings{10.1145/3767052.3767072, title = {Machine Learning Approaches to Creditworthiness Classification}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {127--134}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767072}, url = {https://doi.org/10.1145/3767052.3767072}, author = {Hua, Tianhao}, keywords = {Credit scoring, Random forest, Supervised learning, Support vector machine}, abstract = {Credit risk evaluation is fundamental in financial decision-making, directly influencing lending strategies and default prevention. With the growing availability of structured financial data, machine learning methods have become increasingly prominent in building predictive credit scoring models. This research evaluates Decision Tree, Random Forest, and SVM classifiers for creditworthiness assessment. The Statlog (German Credit Data) dataset from UCI is used with a standardized preprocessing pipeline. Each model was trained and tested on the same dataset split and evaluated using standard classification metrics such as accuracy, precision, recall, and F1 score. Results show that the Random Forest classifier achieved the highest overall performance, particularly in identifying good credit applicants. At the same time, the Decision Tree maintained interpretability, and SVM offered a balanced trade-off. The findings highlight key considerations for model selection in credit scoring applications and suggest ensemble methods as strong candidates for future deployment.} }
@inproceedings{10.1145/3712256.3737464, title = {Rethinking Efficiency in Machine Learning}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {1}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3737464}, url = {https://doi.org/10.1145/3712256.3737464}, author = {Alonso-Betanzos, Amparo}, keywords = {green AI, scalable machine learning, location = NH Malaga Hotel, Malaga, Spain}, abstract = {The success of Artificial Intelligence (AI) has so far relied on developing increasingly precise models. However, this has come at the cost of greater complexity, requiring a higher number of parameters to estimate. As a result, model transparency and explainability have diminished, while the energy demands for training and deployment have skyrocketed. It is estimated that by 2030, AI could account for more than 30\% of the planet's total energy consumption.In this context, green and responsible AI has emerged as a promising alternative, characterized by lower carbon footprints, reduced model sizes, decreased computational complexity, and improved transparency. Various strategies can help achieve these goals, such as improving data quality, developing more energy-efficient execution models, and optimizing energy efficiency in model training and inference. These innovation approaches highlight the potential of green AI to challenge the prevailing paradigm of ever-growing models.} }
@article{10.1145/3705309, title = {Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3705309}, url = {https://doi.org/10.1145/3705309}, author = {Noei, Shayan and Li, Heng and Zou, Ying}, keywords = {Code Refactoring, Refactoring Detection, Python Refactoring, Machine Learning, Code Quality}, abstract = {Refactoring aims to improve the quality of software without altering its functional behaviors. Understanding developers’ refactoring activities is essential to improve software maintainability. The use of machine learning (ML) libraries and frameworks in software systems has significantly increased in recent years, making the maximization of their maintainability crucial. Due to the data-driven nature of ML libraries and frameworks, they often undergo a different development process compared to traditional projects. As a result, they may experience various types of refactoring, such as those related to the data. The state-of-the-art refactoring detection tools have not been tested in the ML technical domain, and they are not specifically designed to detect ML-specific refactoring types (e.g., data manipulation) in ML projects; therefore, they may not adequately find all potential refactoring operations, specifically the ML-specific refactoring operations. Furthermore, a vast number of ML libraries and frameworks are written in Python, which has limited tooling support for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations with relatively high precision. In contrast, for other languages such as Java, state-of-the-art tools are capable of detecting a much more comprehensive list of refactorings. For example, Rminer can detect 99 types of refactoring for Java projects. Inspired by previous work that leverages commit messages to detect refactoring, we introduce MLRefScanner, a prototype tool that applies ML techniques to detect refactoring commits in ML Python projects. MLRefScanner detects commits involving both ML-specific refactoring operations and additional refactoring operations beyond the scope of state-of-the-art refactoring detection tools. To demonstrate the effectiveness of our approach, we evaluate MLRefScanner on 199 ML open source libraries and frameworks and compare MLRefScanner against other refactoring detection tools for Python projects. Our findings show that MLRefScanner outperforms existing tools in detecting refactoring-related commits, achieving an overall precision of 94\% and recall of 82\% for identifying refactoring-related commits. MLRefScanner can identify commits with ML-specific and additional refactoring operations compared to state-of-the-art refactoring detection tools. When combining MLRefScanner with PyRef, we can further increase the precision and recall to 95\% and 99\%, respectively. MLRefScanner provides a valuable contribution to the Python ML community, as it allows ML developers to detect refactoring-related commits more effectively in their ML Python projects. Our study sheds light on the promising direction of leveraging machine learning techniques to detect refactoring activities for other programming languages or technical domains where the commonly used rule-based refactoring detection approaches are not sufficient.} }
@inproceedings{10.1145/3674029.3674033, title = {Diabetes Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {16--20}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674033}, url = {https://doi.org/10.1145/3674029.3674033}, author = {Tian, Stephanie and Hui, Guanghui}, keywords = {Binary classification, Deep Neural Network (DNN), Diabetes prediction, Machine Learning (ML), Modified Sigmoid Function, location = Oslo, Norway}, abstract = {Machine learning (ML) techniques for healthcare informatics provide health professional insight into disease development. Many healthcare topics are suitable for ML research, such as diabetes prediction and classification. Common ML approaches use a classification method to predict the outcome of the disease for given test data, though these solutions tend to have limited accuracy rates. Further tuning with extra manipulation of the dataset helps improve the model's accuracy to a certain level, but this requires certain professional knowledge in the medical domain. In this research, we propose using a DNN (Deep Neural Network) approach to predict the outcome of diabetes from the test data. Based on the dataset statistics, we simply transform 1D diabetes test data arrays to 2D Farrays without complex medical knowledge. We use a 2D convolution function to extract the features for prediction in addition to modifying the final stage activation function, to which the response is similar to a unit step function for binary classification problems. Our DNN model prediction accuracy has improved over the known non-deep learning classification models.} }
@inproceedings{10.1145/3711896.3737860, title = {8th Workshop on Machine Learning in Finance}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6288--6289}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737860}, url = {https://doi.org/10.1145/3711896.3737860}, author = {Nagrecha, Saurabh and Chaturvedi, Isha and Kumar, Senthil and Chawla, Nitesh and Das, Mahashweta and Yadav, Daksha and Rodriguez-Serrano, Jose A. and Kurshan, Eren}, keywords = {ai, finance, genai, machine learning, location = Toronto ON, Canada}, abstract = {The financial industry leverages machine learning in more ways than just finding the right alpha signal. It grapples with supply chains, business processes, marketing, churn, fraud, and money laundering, all while maintaining compliance with the various regulatory frameworks it is beholden to. Due to the sheer volume of wealth being handled by the financial industry and its critical role in everyday life, it has been a lucrative target for a wide spectrum of ever-evolving bad actors. With each successive iteration of this workshop, we have attempted to capture the breadth of these actors - fraudsters, money launderers, market manipulators, and potentially nation-state-level risks. The emerging advances in Generative AI make this a particularly exciting time to host this workshop. GenAI offers groundbreaking approaches to handling the various data types prevalent in the financial sector. From a security point of view, bad actors are actively using Generative AI creatively to thwart conventional defenses (e.g. voice cloning, better synthetic identities), and this workshop's audience would benefit from commonly applicable defenses \&amp; best practices against such threats. Last but not the least, there is now an increasing willingness from the financial industry towards deeper engagement and data sharing with academia.} }
@article{10.1145/3737650, title = {Race Against the Machine Learning Courses}, journal = {ACM Trans. Intell. Syst. Technol.}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3737650}, url = {https://doi.org/10.1145/3737650}, author = {Deshpande, Riddhi and Mlombwa, Donald and Celi, Leo Anthony and Gallifant, Jack and D’couto, Helen}, keywords = {1.32 Tutoring and educational systems, 1 Systems and Applications, 2.6 Highly scalable AI algorithms, 2 AI Technology, 1.20 Medical and health systems, 1 Systems and Applications, 1.14 AI in science, 1 Systems and Applications}, abstract = {Despite the rapid integration of AI in healthcare, a critical gap exists in current machine learning courses: the lack of education on identifying and mitigating bias in datasets. This oversight risks perpetuating existing health disparities through biased AI models. Analyzing 11 prominent online courses, we found only 5 addressed dataset bias, often dedicating minimal time compared to technical aspects. This paper urges course developers to prioritize education on data context, equipping learners with the tools to critically evaluate the origin, collection methods, and potential biases inherent in the data. This approach fosters the creation of fair algorithms and the incorporation of diverse data sources, ultimately mitigating the harmful effects of bias in healthcare AI. While this analysis focused on publicly available courses, it underscores the urgency of addressing bias in all healthcare machine learning education. Early intervention in algorithm development is crucial to prevent the amplification of dataset and model bias, ensuring responsible and equitable AI implementation in healthcare.} }
@inproceedings{10.1145/3623509.3633370, title = {Embodied Machine Learning}, booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}, year = {2024}, isbn = {9798400704024}, doi = {10.1145/3623509.3633370}, url = {https://doi.org/10.1145/3623509.3633370}, author = {Bakogeorge, Alexander and Imtiaz, Syeda Aniqa and Abu Hantash, Nour and Manshaei, Roozbeh and Mazalek, Ali}, keywords = {Embodied interaction, collaboration, machine learning, medical data, tabletop interaction, trust, location = Cork, Ireland}, abstract = {Machine learning becomes more prevalent in specialized domains such as medicine and biology every year, but domain expert trust in machine learning continues to lag behind. Researchers have explored increasing rational trust in AI but little research exists focusing on systems that foster affective and normative trust between domain experts and data scientists who create the models. Tools like Project Jupyter have attempted to bridge this gap between data scientists and domain experts, but failed to see uptake in applied fields or to promote collaboration through co-located synchronous work. To address this we present a proof-of-concept tabletop interactive machine learning system for synchronous, co-located model fine tuning. We tested our system with biology experts and data scientists on a cell biology dataset. Results show that our system promotes interactions between domain experts, data scientists, and the model-in-training and fosters domain expert affective and normative trust in the resulting AI model.} }
@inproceedings{10.1145/3696271.3696274, title = {Customer Clusterization using Machine Learning Approach}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {15--19}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696274}, url = {https://doi.org/10.1145/3696271.3696274}, author = {Purnamasari, Fanindia and Putri Nasution, Umaya Ramadhani and Elveny, Marischa and Hayatunnufus, Hayatunnufus}, keywords = {clustering, customer segmentation, k-means, machine learning, silhouette}, abstract = {Understanding customer is crucial for marketing strategies and increasing customer satisfaction in today's business environment. One method to fulfill of marketing strategies is segment customer based on their purchasing habits and demographic characteristics. This study describes a complete approach to customer segmentation based on K-means clustering, an unsupervised machine learning algorithm. There are three stages namely preprocessing to select feature and variable is used to develop clustering model, clustering model implementation, and validation of model. There are four clusters that compare the relationship of marital status and recency to the grocery purchases (product) made by each customer to find out which ingredients we will use to make better products for customers.} }
@inproceedings{10.1145/3711896.3736560, title = {Data Heterogeneity Modeling for Trustworthy Machine Learning}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6086--6095}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3736560}, url = {https://doi.org/10.1145/3711896.3736560}, author = {Liu, Jiashuo and Cui, Peng}, keywords = {data heterogeneity, out-of-distribution generalization, stability, trustworthy machine learning, location = Toronto ON, Canada}, abstract = {Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline-from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.} }
@inproceedings{10.1145/3641525.3663630, title = {The Idealized Machine Learning Pipeline (IMLP) for Advancing Reproducibility in Machine Learning}, booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability}, pages = {110--120}, year = {2024}, isbn = {9798400705304}, doi = {10.1145/3641525.3663630}, url = {https://doi.org/10.1145/3641525.3663630}, author = {Zheng, Yantong and Stodden, Victoria}, keywords = {Computational Reproducibility, CyberInfrastructure, Human Factors, Machine Learning, Open Code, Open Data, Reproducibility Policy, location = Rennes, France}, abstract = {We investigate the influence of Human Factors on reproducible machine learning, using a novel “Idealized Machine Learning Pipeline” we introduce as a conceptual framework. The study of Human Factors seeks to ensure that systems meet the needs and expectations of people who use and interact with these systems. It also, importantly, seeks to ensure that the capabilities and limitations of those people are accommodated by the system. As increasing the reproducibility of Machine Learning continues to be a community priority, we believe an improved understanding the Human Factors associated with the complex human-machine system of the Machine Learning pipeline could help facilitiate useful steps toward reproducibility. To do this, we first define a practical abstraction of the steps that comprise a typical Machine Learning pipeline for a well-known use case, from raw data through to model estimation used for inference and prediction, that we call the “Idealized Machine Learning Pipeline.” We emphasize that our proposed “Idealized Machine Learning Pipeline” is intended as an abstraction, rather than a directive or description of all Machine Learning workflows, and meant to harmonize and integrate different viewpoints, expertise, and other elements vital to Machine Learning research. Our goal is to enable the research community to coalesce around priorities for theoretical and applied research on reproducible Machine Learning including tools, frameworks, comparisons, and policies, and provide a foundation for communicating and teaching its myriad aspects as an integrated whole. We define the principal steps as follows: 0) Documentation, 1) Problem Definition, 2) Input Data, 3) Data Preparation, 4) Feature Selection, 5) Model Training, 6) Model Evaluation, 7) Preservation \&amp; Publication. To then understand barriers and opportunities attending to the adoption of reproducible Machine Learning pipelines, we leverage the “Idealized Machine Learning Pipeline” for our chosen use case to identify and motivate relevant Human Factors that affect various steps in a reproducible Machine Learning pipeline. We find the identified Human Factors fall into three groups: Incentives; Training; and Error-based Human Factors.} }
@inproceedings{10.1145/3631461.3632516, title = {Distributed Machine Learning}, booktitle = {Proceedings of the 25th International Conference on Distributed Computing and Networking}, pages = {4--7}, year = {2024}, isbn = {9798400716737}, doi = {10.1145/3631461.3632516}, url = {https://doi.org/10.1145/3631461.3632516}, author = {Chatterjee, Bapi}, keywords = {Distributed Machine Learning, Federated Learning, Machine Learning, location = Chennai, India}, abstract = {We explore the landscape of distributed machine learning, focusing on advancements, challenges, and potential future directions in this rapidly evolving field. We delve into the motivation for distributed machine learning, its essential techniques, real-world applications, and open research questions. The theoretical discussion will give an overview of proving the convergence of popular Stochastic Gradient Descent (SGD) Algorithms to train contemporary machine learning models, including the deep learning models with the assumption of non-convexity, in a distributed setting. We will specify the convergence of data parallel SGD for various distributed systems properties, such as asynchronous and compressed communication. We will also discuss distributed machine learning techniques such as model parallelism and tensor parallelism to train large language models (LLMs).} }
@article{10.1145/3729432, title = {Performance Evaluation for Detecting and Alleviating Biases in Predictive Machine Learning Models}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3729432}, url = {https://doi.org/10.1145/3729432}, author = {Khakurel, Utsab and Abdelmoumin, Ghada and Rawat, Danda B.}, keywords = {machine learning, bias, identification, mitigation, predictions, classification, ethical AI}, abstract = {Machine Learning (ML) is widely used in various domains but is susceptible to biases that can lead to unfair decisions. Bias can arise from biased data, algorithms, or data collection processes, making it crucial to develop methods that ensure fairness. This article introduces the Detect and Alleviate Bias (DAB) framework, a novel approach designed to identify and mitigate bias in ML models, focusing on sensitive attributes such as gender and race. The key contributions of DAB include: (1) a holistic pipeline that combines data pre-processing, model enhancement, situation testing, and bias mitigation techniques; (2) the application of Counterfactual Fairness testing to detect individual biases; and (3) the integration of multiple bias mitigation strategies to improve fairness in binary classification tasks. We demonstrate the practical implications of DAB through empirical experiments on two widely used datasets, showing that it reduces bias and improves fairness with a slight compromise in model performance, with a significant potential for making an impact in sensitive domains such as healthcare and criminal justice. The results show that pre-processing and post-processing mitigation techniques achieve the best improvements in fairness for unrepresentative datasets, while in-processing methods are more effective in enhancing fairness for representative datasets; however, an inverse relationship between model performance and fairness is observed.} }
@inproceedings{10.1145/3696271.3696276, title = {Smart Drying with Machine Learning Methods}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {27--33}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696276}, url = {https://doi.org/10.1145/3696271.3696276}, author = {Chandra, Nicholas Li Jian and Chen, Zhiyuan and Law, Chung Lim}, keywords = {Fuzzy Logic, Moisture Content, Random Forest, Smart Drying, Support Vector Machine}, abstract = {This research aims to improve the prediction of process kinetics in food drying by evaluating and comparing various machine learning models and a hybrid model. Traditional methods for modeling the thermal processes in drying agricultural commodities often fall short due to their complexity. This study investigates four commonly used algorithms in this research domain—Artificial Neural Networks (ANN), Fuzzy Logic (FL), Random Forest (RF), Support Vector Machine (SVM)—and one hybrid model, Fuzzy SVM (FSVM). Data preprocessing included handling missing values, scaling numerical features, and one-hot encoding categorical variables. The models were evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²). Results indicated that the RF model with 100 decision trees and the ANN with 1000 epochs provided promising accuracy. However, the FSVM hybrid model did not demonstrate enhanced predictive capabilities beyond those of its base models.} }
@inproceedings{10.1145/3748355.3748363, title = {Empowering machine-learning assisted kernel decisions with eBPFML}, booktitle = {Proceedings of the 3rd Workshop on EBPF and Kernel Extensions}, pages = {28--30}, year = {2025}, isbn = {9798400720840}, doi = {10.1145/3748355.3748363}, url = {https://doi.org/10.1145/3748355.3748363}, author = {Sodhi, Prabhpreet Singh and Liargkovas, Georgios and Kaffes, Kostis}, keywords = {Operating systems, eBPF, hardware acceleration, machine learning, location = Coimbra, Portugal}, abstract = {Machine-learning (ML) techniques can optimize core operating system paths---scheduling, I/O, power, and memory---yet practical deployments remain rare. Existing prototypes either (i) bake simple heuristics directly into the kernel or (ii) off-load inference to user space to exploit discrete accelerators, both of which incur unacceptable engineering or latency cost. We argue that eBPF, the Linux kernel's safe, hot-swappable byte-code runtime, is the missing substrate for moderately complex in-kernel ML. We present eBPFML, a design that (1) extends the eBPF instruction set with matrix-multiply helpers, (2) leverages upcoming CPU matrix engines such as Intel Advanced Matrix Extensions (AMX) through the eBPF JIT, and (3) retains verifier guarantees and CO-RE portability.} }
@inproceedings{10.1145/3711896.3737246, title = {Maturity Framework for Enhancing Machine Learning Quality}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {4296--4307}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737246}, url = {https://doi.org/10.1145/3711896.3737246}, author = {Castelli, Angelantonio and Chouliaras, Georgios Christos and Goldenberg, Dmitri}, keywords = {machine learning maturity framework, machine learning quality, quality framework, reproducibility, location = Toronto ON, Canada}, abstract = {With the rapid integration of Machine Learning (ML) in business applications and processes, it is crucial to ensure the quality, reliability and reproducibility of such systems. We suggest a methodical approach towards ML system quality assessment and introduce a structured Maturity framework for governance of ML. We emphasize the importance of quality in ML and the need for rigorous assessment, driven by issues in ML governance and gaps in existing frameworks. Our primary contribution is a comprehensive open-sourced quality assessment method, validated with empirical evidence, accompanied by a systematic maturity framework tailored to ML systems. Drawing from applied experience at Booking.com, we discuss challenges and lessons learned during large-scale adoption within organizations. The study presents empirical findings, highlighting quality improvement trends and showcasing business outcomes. The maturity framework for ML systems, aims to become a valuable resource to reshape industry standards and enable a structural approach to improve ML maturity in any organization.} }
@article{10.1145/3701031, title = {Overview of Multimodal Machine Learning}, journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.}, volume = {24}, year = {2025}, issn = {2375-4699}, doi = {10.1145/3701031}, url = {https://doi.org/10.1145/3701031}, author = {Al-Zoghby, Aya M. and Al-Awadly, Esraa Mohamed K. and Ebada, Ahmed Ismail and Awad, Wael A.}, keywords = {Multimodal Machine Learning(MML), Natural Language Processing(NLP), Deep Learning(DL), Fusion Techniques, Cross-Modal}, abstract = {Human nature is fundamentally driven by the need for interaction and attention, which are fulfilled through various sensory modalities, including hearing, sight, touch, taste, and smell. These senses enable us to perceive, understand, and engage with the world around us. The quality and depth of our interactions change considerably when we use multiple senses simultaneously, highlighting the importance of multimodal interactions in our daily lives. In the realm of technology, multimodal integration offers immense value, as it aims to create systems that can replicate or complement these natural human abilities for enhanced interaction.This article explores the significance of spatial multimodalities in machine learning, highlighting their role in improving model performance in applications such as autonomous driving, healthcare, and virtual assistants. It addresses challenges like the complexity of fusing diverse sensory data types and proposes solutions such as advanced data fusion techniques, adaptive learning algorithms, and transformer architectures. The goal is to provide an overview of state-of-the-art research and future directions for advancing human–computer interaction.} }
@inproceedings{10.1145/3658644.3690862, title = {Privacy Analyses in Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {5110--5112}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690862}, url = {https://doi.org/10.1145/3658644.3690862}, author = {Ye, Jiayuan}, abstract = {Machine learning models sometimes memorize sensitive training data features, posing privacy risks. To control such privacy risks, Dwork et al. proposed the definition of differential privacy (DP) to measure the privacy risks of an algorithm. However, existing DP models either have significantly lower accuracy than their non-private variants or are computationally expensive to train by requiring to incorporate a large amount of public prior knowledge in terms of data or a large pre-trained model. Thus, the fundamental problem of efficiently training an accurate model while preserving DP is not fully addressed. To tackle this problem, we investigate the potential of improving privacy analysis of machine learning algorithms, thus subsequently allowing improved privacy-utility trade-off. First, we observe that the standard DP bound is not tight for large, overparameterized models. Specifically, the DP bound worsens with the number of iterations, and the privacy-accuracy trade-off worsens with the model dimension. This is despite the algorithm converging during training and the finite dimension of training data space. Such potential untightness is more severe for a realistic adversary that does not observe all model parameters, where prior works suggest empirical privacy amplification, and we investigate theoretically. Finally, we take a close look at the privacy risk of each model prediction about individual training data and analyze how to attribute privacy risk to the properties of the data and the choice of model (e.g., architectures). If successful, our research would enable tighter and more informative privacy bounds for differentially private learning, thus in turn allowing improved privacy-utility trade-offs.} }
@inproceedings{10.1145/3747227.3747245, title = {Research on Intrusion Detection Based on Interpretable Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {109--114}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747245}, url = {https://doi.org/10.1145/3747227.3747245}, author = {Chen, Mao and Ma, Bowen and Jiang, Hao}, keywords = {Deep neural networks, Explainable machine learning, Feature importance analysis, Intrusion detection, Port scanning}, abstract = {Intrusion Detection Systems (IDS) grapple with challenges such as delayed updates and a high false positive rate in traditional rule-based approaches under high-dimensional network traffic data. Although deep learning has significantly improved detection accuracy, the conflict between its "black box" nature and traceability requirements still persists. This paper focuses on port scanning attack detection and provides a “dual-wheel-driven” solution of "architecture optimization and interpretability enhancement" to build a high-precision interpretable learning framework. To tackle the vanishing gradient and overfitting problems of traditional Deep Neural Networks (DNN), an enhanced DNN model architecture is designed: through dynamic residual structure, "expansion-compression" design, and adaptive regularization strategy, the learning ability of high-dimensional network traffic features is improved. Moreover, to address the common "black-box" problem in current models, this paper introduces a feature importance quantification method based on gradient-weighting, which integrates dynamic prediction confidence weights to enhance the interpretability analysis of model decisions. In this study, the CICIDS-2017 dataset is chosen for training and evaluation. The results indicated that the enhanced DNN model had significantly better accuracy on the test set than traditional DNN and traditional machine learning models. More importantly, the enhanced DNN model can also identify key attack features, providing high-precision and transparent methodological support for cybersecurity defense.} }
@article{10.1145/3708479, title = {Bayesian Machine Learning Meets Formal Methods: An Application to Spatio-Temporal Data}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3708479}, url = {https://doi.org/10.1145/3708479}, author = {Vana-G\"ur, Laura and Visconti, Ennio and Nenzi, Laura and Cadonna, Annalisa and Kastner, Gregor}, keywords = {Bayesian predictive inference, spatio-temporal models, formal verification methods, posterior predictive verification, urban mobility}, abstract = {We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in machine learning, with formal methods, rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.} }
@inproceedings{10.1145/3708360.3708380, title = {Research on Multi-Factor Investment Strategies Utilizing Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {123--126}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708380}, url = {https://doi.org/10.1145/3708360.3708380}, author = {Wang, Bojing}, keywords = {machine learning, machine learning, multi-factor, stock selection strategy}, abstract = {In today's complex and changeable financial market, stock selection strategies have always been the focus of investors' attention. The application of machine learning technology in finance is becoming increasingly widespread. As a classic investment approach, the multi-factor stock selection strategy, when combined with machine learning techniques, is expected to enhance the accuracy and effectiveness of stock selection. This paper uses the overall and individual stock performance of the Shanghai 50 index from June 2023 to June 2024 as a case study to analyze a multi-factor stock selection strategy based on machine learning.} }
@article{10.5555/3722577.3722593, title = {Localized debiased machine learning: efficient inference on quantile treatment effects and beyond}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Kallus, Nathan and Mao, Xiaojie and Uehara, Masatoshi}, keywords = {causal inference, Neyman orthogonality, cross-fitting, instrumental variables, conditional value at risk, expectiles}, abstract = {We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisance functions that depend on the target parameter as an input. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Existing approaches based on flexibly estimating the nuisances and plugging in the estimates, such as debiased machine learning (DML), require we learn the nuisance at all possible inputs. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the target parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or exible relationships, as we demonstrate in empirical studies.} }
@inproceedings{10.1145/3715275.3732195, title = {The Data Minimization Principle in Machine Learning}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {3075--3093}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732195}, url = {https://doi.org/10.1145/3715275.3732195}, author = {Ganesh, Prakhar and Tran, Cuong and Shokri, Reza and Fioretto, Ferdinando}, keywords = {Data minimization, Privacy, Data Protection Regulations}, abstract = {The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches. Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations. However, its implementation remains a challenge due to the lack of a rigorous formulation. Our paper addresses this gap and introduces an optimization framework to operationalize the legal definitions of data minimization. It adapts several optimization algorithms to perform data minimization in machine learning and conducts a comprehensive evaluation of their compliance with minimization objectives and their impact on user privacy. Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for data minimization approaches that account for multiple facets of real-world privacy risks.} }
@inbook{10.1145/3696630.3728523, title = {Learning to Edit Interactive Machine Learning Notebooks}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {681--685}, year = {2025}, isbn = {9798400712760}, url = {https://doi.org/10.1145/3696630.3728523}, author = {Jin, Bihui and Wang, Jiayue and Nie, Pengyu}, abstract = {Machine learning (ML) developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Notebooks provide a convenient tool for writing ML pipelines and interactively observing outputs. However, maintaining notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the ML pipeline code. Moreover, there is no existing benchmark related to developer edits on notebooks.In this paper, we present early results of the first study on learning to edit ML pipeline code in notebooks using large language models (LLMs). We collect the first dataset of 48,398 notebook edits derived from 20,095 revisions of 792 ML-related GitHub repositories. Our dataset captures granular details of file-level and cell-level modifications, offering a foundation for understanding real-world maintenance patterns in ML pipelines. We observe that the edits on notebooks are highly localized. Although LLMs have been shown to be effective on general-purpose code generation and editing, our results reveal that the same LLMs, even after finetuning, have low accuracy on notebook editing, demonstrating the complexity of real-world ML pipeline maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing LLMs' capabilities in engineering ML code.} }
@inproceedings{10.1145/3759023.3759127, title = {Using Machine Learning Algorithms to Classify Recyclable Waste}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759127}, url = {https://doi.org/10.1145/3759023.3759127}, author = {Sibanda, Mthokozisi and Bradshaw, Karen}, keywords = {Image classification, Transfer learning, Convolutional neural network}, abstract = {Waste management poses a significant global challenge, with 90\% of South Africa’s waste directed to landfills, leading to environmental and health risks. In many regions in Africa, manual waste sorting exposes workers to hazardous conditions, underscoring the need for safer, more efficient recycling practices. This research explores the possibility of removing human involvement in waste recycling through the application of image processing and machine learning techniques, specifically using convolutional neural networks (CNNs) with transfer learning, to automate the sorting of recyclable materials to identify recyclable waste. By accurately classifying waste into categories such as paper, plastics, glass, metals, cardboard, and other waste, the proposed model aims to reduce landfill waste, promote recycling, and create safer working environments.Several proof-of-concept pretrained models are used for binary and multi-class classification tasks, highlighting the performance of CNNs in distinguishing waste types. While binary classification demonstrates high accuracy for specific waste categories, multi-class classification reflects real-world complexities in sorting mixed waste streams. Though limited in its effectiveness for class imbalance, data augmentation points to potential improvements through oversampling techniques. Ultimately, this research could contribute to the development of an automated waste sorting system, enhancing recycling efficiency and reducing the environmental footprint of waste disposal.} }
@inproceedings{10.1145/3670474.3685973, title = {Machine Learning for High Sigma Analog Designs (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685973}, url = {https://doi.org/10.1145/3670474.3685973}, author = {Jallepalli, Srinivas}, keywords = {Monte Carlo: machine learning, high sigma, importance sampling, parametric yield, scaled sigma sampling, statistical blockade, location = Salt Lake City, UT, USA}, abstract = {Monte Carlo simulations have been the gold standard for assessing parametric yields of analog, mixed signal, and RF circuits as they offer one of the most direct representations of the variation induced by semiconductor manufacturing. However, Monte Carlo analyses are often too expensive for understanding high sigma yields with fewer defects than 1000ppm. To quantify the impact of rare events on circuit yield, we need insights into their probability densities. All rare event sampling techniques that seek to provide this insight employ a machine learning flow of some kind. The various implementations of importance sampling and statistical blockade, for example, try to locate the rare event populations in parametric space through input domain mapping. Despite their popularity, they can sometimes pose a significant challenge, especially when the dimensionality of the input variation space is high, as both feature selection and machine learning can be non-trivial. A good alternative to machine learning in the input parametric domain is the innovative scaled sigma sampling technique that leverages machine learning of the probability density differences produced by scaling input standard deviations. This paper reviews these key approaches for determining the high sigma yields of analog circuits.} }
@article{10.1145/3768158, title = {Physics-informed Machine Learning for Medical Image Analysis}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3768158}, url = {https://doi.org/10.1145/3768158}, author = {Banerjee, Chayan and Nguyen, Kien and Salvado, Olivier and Tran, Truyen and Fookes, Clinton}, keywords = {Physics-informed, physics-guided, physics-constrained, PINNs, neural networks, medical image analysis}, abstract = {The incorporation of physical information in machine learning frameworks is transforming medical image analysis (MIA). Integrating fundamental knowledge and governing physical laws not only improves analysis performance but also enhances the model’s robustness and interpretability. This work presents a systematic review of over 100 articles on the utility of PINNs dedicated to MIA (PIMIA) tasks. We propose a unified taxonomy to investigate what physics knowledge and processes are modeled, how they are represented, and the strategies to incorporate them into MIA models. We delve deep into a wide range of image analysis tasks, from imaging, generation, prediction, inverse imaging (super-resolution and reconstruction), registration, and image analysis (segmentation and classification). For each task, we thoroughly examine and present the central physics-guided operation, the region of interest (with respect to human anatomy), the corresponding imaging modality, the datasets used for model training, the deep network architectures employed, and the primary physical processes, equations, or principles utilized. Additionally, we also introduce a novel metric to compare the performance of PIMIA methods across different tasks and datasets. Based on this review, we summarize and distill our perspectives on the challenges, and highlight open research questions and directions for future research.} }
@inproceedings{10.1145/3718391.3718417, title = {Malware Traffic Analysis using Machine Learning}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {62--67}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3718417}, url = {https://doi.org/10.1145/3718391.3718417}, author = {Ji, Jie and Mogos, Gabriela}, keywords = {Machine learning, malware, security}, abstract = {Malware refers to computer code or software that is installed and operated on a user's computer or other terminal without explicit notification or permission, engaging in activities such as stealing, encrypting, modifying, and deleting data, and monitoring the legitimate rights and interests of users. The types of malwares include viruses, worms, Trojans, ransomware, spyware, and so on. Different types of malwares have different attack methods and can cause different damages, resulting in potential financial losses for users. Five machine learning algorithms were used to conduct comparative analysis and find the best performing model to predict potential malware traffic issues in networks. We used the CIC-IDS-2017 dataset, Pearson correlation coefficient to select features and 5-fold cross validation to evaluate the model's generalization ability.} }
@inproceedings{10.1145/3749096.3750027, title = {Towards Blind Quantum Machine Learning in Entanglement Networks}, booktitle = {Proceedings of the 2nd Workshop on Quantum Networks and Distributed Quantum Computing}, pages = {8--13}, year = {2025}, isbn = {9798400720970}, doi = {10.1145/3749096.3750027}, url = {https://doi.org/10.1145/3749096.3750027}, author = {de Abreu, Diego Medeiros and Abel\'em, Ant\^onio}, keywords = {Blind Quantum Computing, Quantum Networks, location = Coimbra, Portugal}, abstract = {Blind Quantum Computation (BQC) enables clients to delegate quantum computations to a quantum server while maintaining the privacy of their data and algorithms, even when the server is untrusted. In this work, we extend BQC frameworks to Quantum Machine Learning (QML) by implementing a network of entangled clients and a quantum server. Specifically, we explore the integration of Variational Quantum Classifiers (VQC) and Quantum Convolutional Neural Networks (QCNNs) within this paradigm. Our proposed model allows clients to perform classical preprocessing and optimization locally while leveraging the quantum server for computationally expensive quantum tasks. The entanglement-based network is managed by a controller that dynamically allocates resources according to the BQC protocol, allowing secure and efficient execution. We present simulation results indicating the feasibility of this approach, including an analysis of network efficiency and resource consumption, alongside the F1 score on QML benchmark datasets.} }
@inproceedings{10.1145/3637528.3671488, title = {Machine Learning in Finance}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6703}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671488}, url = {https://doi.org/10.1145/3637528.3671488}, author = {Akoglu, Leman and Chawla, Nitesh and Domingo-Ferrer, Josep and Kurshan, Eren and Kumar, Senthil and Naware, Vidyut and Rodriguez-Serrano, Jose A. and Chaturvedi, Isha and Nagrecha, Saurabh and Das, Mahashweta and Faruquie, Tanveer}, keywords = {ai, finance, genai, machine learning, location = Barcelona, Spain}, abstract = {This workshop aims to explore the intersection of Generative AI with the rich tapestry of financial data types, seeking to uncover new methodologies and techniques that can enhance predictive analytics, fraud detection, and customer insights across the sector. By harnessing these advancements in AI, we can pave the way to not only understand customer behavior but also anticipate their needs more effectively, leading to superior customer outcomes and more personalized services. Our objective is to shed light on the challenges and opportunities presented by the diverse data formats in finance. We aim to bridge the gap between the dominance of traditional models for tabular data analysis and the emerging potential of Generative AI to revolutionize the treatment of time series, click streams, and other unstructured data forms.} }
@article{10.5555/3722479.3722535, title = {Finiteness Considerations in Machine Learning}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {250--262}, year = {2024}, issn = {1937-4771}, author = {Jackson, Jeffrey C.}, abstract = {Many machine learning textbooks include at least some coverage of one or both of the No Free Lunch theorems for learning and probably-approximately correct generalization error bounds. However, it is not a simple matter to reconcile the implications of these two topics and provide advice to students (and practitioners) regarding when learning claims such as "this learned hypothesis will be at least 95\% accurate on previously-unseen data" can reasonably be made. This paper shows how finiteness considerations can potentially provide such a reconciliation. It also suggests that finiteness considerations can be used to simplify certain generalization error bounds by eliminating their reliance on the VC-dimension of hypothesis classes, which might be of independent pedagogical interest.} }
@inproceedings{10.1145/3702653.3744293, title = {Supporting Structured Problem-Solving in Machine Learning Education}, booktitle = {Proceedings of the 2025 ACM Conference on International Computing Education Research V.2}, pages = {63--64}, year = {2025}, isbn = {9798400713415}, doi = {10.1145/3702653.3744293}, url = {https://doi.org/10.1145/3702653.3744293}, author = {Witt, Clemens}, keywords = {Machine Learning Education, Problem-Solving Strategies, Stealth Assessment}, abstract = {This PhD project investigates how students develop problem-solving strategies in machine learning (ML) education. Applying a design-based research approach, it integrates stealth assessment techniques and adaptive feedback mechanisms to foster more structured and systematic engagement with complex ML learning tasks. Initial empirical findings inform the iterative development of a transferable framework to support systematic learning processes across diverse ML contexts, thereby advancing understanding of students’ problem-solving in ML education.} }
@inproceedings{10.1145/3728199.3728288, title = {Analysis of Passenger Satisfaction Evaluation Metrics Based on Machine Learning}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {538--543}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728288}, url = {https://doi.org/10.1145/3728199.3728288}, author = {Wang, Xiucui and Dumlao, Menchita F.}, keywords = {HistGBDT model, Passenger satisfaction, machine learning algorithms}, abstract = {The research in this paper is based on a dataset on airline passenger satisfaction downloaded from the Kaggle data platform, which contains 26 characteristics of passengers, totaling nearly 130,000 records. Using this data set, eight machine learning algorithms are used, such as GBDT, Decision tree, AdaBoost, XGBoost, Random forest, Extremerandom tree, CatBoost, HistGBDT. By analyzing the evaluation indexes of these 8 machine learning algorithms, it is concluded that HistGBDT algorithm has higher values in precision and accuracy. This result page verifies that HistGBDT model can improve the predictive ability of passenger satisfaction. Increase the generalization ability of their models. It also shows that the model is superior in predicting airline satisfaction. Through this result analysis, we can help airlines to better provide passengers with better quality services and meet their different needs, so as to provide airlines with more personalized services and decisions, and enhance their competitive advantages.} }
@inproceedings{10.1145/3728199.3728243, title = {Machine Learning Based Structural Design and Performance Prediction of Advanced Materials}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {269--272}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728243}, url = {https://doi.org/10.1145/3728199.3728243}, author = {You, Weichen}, keywords = {Graph neural network, Intelligent optimisation, Machine learning, Material structure design, Performance prediction}, abstract = {This paper explores the core methods of machine learning in material modelling, structure-property correlation analysis and optimal design, including data pre-processing, feature engineering, algorithm selection and model training strategies. Combining deep learning, graph neural network (GNN) and gradient boosting decision tree (GBDT) methods, accurate prediction of material properties can be achieved and material structures can be optimised by Bayesian optimisation, genetic algorithms and other intelligent search strategies. It is shown that the machine learning-based material design method can significantly improve the efficiency of material screening and accelerate the development process of new materials.} }
@proceedings{10.1145/3721146, title = {EuroMLSys '25: Proceedings of the 5th Workshop on Machine Learning and Systems}, year = {2025}, isbn = {9798400715389}, abstract = {EuroMLSys gathers AI researchers and practitioners to share innovative advancements in software infrastructure, tools, design principles, theoretical foundations, algorithms, and applications—all viewed from a systems-oriented perspective and harnessing the power of machine learning.} }
@inproceedings{10.1145/3641555.3705218, title = {Mathematics for Machine Learning: A Bridge Course}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1437--1438}, year = {2025}, isbn = {9798400705328}, doi = {10.1145/3641555.3705218}, url = {https://doi.org/10.1145/3641555.3705218}, author = {Deng, Samuel}, keywords = {bridge course, curriculum, machine learning education, mathematics education, location = Pittsburgh, PA, USA}, abstract = {We present Mathematics for Machine Learning, a one-semester mathematics course designed to strengthen students' foundations before further study and research in machine learning (ML) and data science. Oftentimes, the mathematical prerequisites needed for serious study of ML are taught in a disjointed manner. Our course is designed to bridge this gap and provide emphasis on concepts heavily employed in modern ML, such as spectral analysis in linear algebra or convex optimization in calculus. We structured our course around the three "pillars'' of math that underlie much of modern ML: (i) linear algebra, (ii) calculus and optimization, and (iii) probability and statistics. Weaving each of these together is a central story --- all concepts, ideas, and proofs are introduced relative to two ubiquitous concepts in machine learning: least squares regression and gradient descent, providing a consistent anchoring narrative and constant motivation for mathematical ideas.} }
@inproceedings{10.1145/3745133.3745174, title = {Forecasting ESG Index Based on Machine Learning Methods}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Information Systems}, pages = {241--246}, year = {2025}, isbn = {9798400714375}, doi = {10.1145/3745133.3745174}, url = {https://doi.org/10.1145/3745133.3745174}, author = {Zhao, Juantong}, keywords = {ESG, machine learning, sustainable development, time series prediction}, abstract = {ESG (Environmental, Social, and Governance) is an indicator of corporate sustainability and responsible performance in terms of environmental, social and governance, as well as an important framework for measuring corporate non-financial performance, which is important for balancing economic efficiency with global sustainable development. This study constructs the impact factor system of ESG index in the aspects of environment, society and governance, and then constructs six machine learning models to predict ESG index. The results show that LSTM model performs best, and MSE is 0.14-25.14 times better than other machine learning models BP, CNN, GRU, RNN and MLP. This study provides a scalable, intelligent solution for ESG assessment and reveals the potential value of deep learning in sustainable finance.} }
@inproceedings{10.1145/3746972.3747002, title = {Hybrid Machine Learning for Used Car Price Determinants}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {181--186}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747002}, url = {https://doi.org/10.1145/3746972.3747002}, author = {Feng, Jianli}, keywords = {Gradient Descent, Logistic Regression, Machine Learning, Multiple Linear Regression, Support Vector Machine}, abstract = {This study develops a methodological framework for used car valuation through comparative analysis of regression techniques. Initial factor significance evaluation was conducted using Multiple Linear Regression (MLR) and Logistic Regression (LR), identifying ten key pricing determinants. Twelve regression models were subsequently implemented with these predictors, revealing LR's superior performance over MLR in coefficient stability and interpretability. The predictive phase comparatively evaluates Gradient Descent (GD) and Support Vector Machine (SVM) algorithms, with systematic residual analysis demonstrating SVM's optimal prediction accuracy. The findings provide empirical guidance for machine learning applications in automotive residual value estimation.} }
@article{10.1145/3761823, title = {Integration of fNIRS and Machine Learning for Identifying Parkinson’s Disease}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761823}, url = {https://doi.org/10.1145/3761823}, author = {Sousani, Maryam and Rojas, Raul Fernandez and Preston, Elisabeth and Ghahramani, Maryam}, keywords = {Parkinson’s Disease, fNIRS, Machine Learning, Biomarker}, abstract = {Parkinson’s disease (PD) is a neurodegenerative disorder where early diagnosis is crucial for effective management. However, current diagnostic methods are often invasive or delayed, hindering early intervention. This study evaluates the effectiveness of combining functional near-infrared spectroscopy (fNIRS) with machine learning to distinguish individuals with PD from age-matched controls.Data were collected using fNIRS from 28 people with PD and 32 age-matched controls while performing the Timed Up and Go (TUG) test under three conditions: simple TUG, cognitive dual-task TUG and motor dual-task TUG. Changes in cerebral blood oxygenation in the prefrontal cortex (PFC) were analysed using four machine learning models: Support Vector Machine (SVM), K-Nearest Neighbours (KNN), Random Forest (RF) and Extreme Gradient Boosting (XGB), along with statistical analyses. Two feature selection models identified key features and channels for differentiating PD from controls.The SVM model achieved the highest accuracy (0.85 (pm) 0.35) in distinguishing PD from CG. Feature selection and statistical analysis showed that dual-task activities were more effective than simple tasks for distinguishing PD from CG. Specific PFC subregions exhibited distinct activation patterns, which could serve as potential biomarkers for PD detection. Combining fNIRS with machine learning shows promise for PD diagnosis, with dual-task activities enhancing accuracy. Further investigation into PFC subregion behaviour could reveal stronger biomarkers.} }
@inproceedings{10.1145/3673791.3698439, title = {Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, booktitle = {Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region}, pages = {299--302}, year = {2024}, isbn = {9798400707247}, doi = {10.1145/3673791.3698439}, url = {https://doi.org/10.1145/3673791.3698439}, author = {Diaz, Fernando and Drozdov, Andrew and Kim, To Eun and Salemi, Alireza and Zamani, Hamed}, keywords = {information retrieval, machine learning, location = Tokyo, Japan}, abstract = {Retrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.} }
@inproceedings{10.1145/3724363.3729032, title = {Are Interactive Visualizations in Machine Learning Education Helping Students?}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {2--8}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729032}, url = {https://doi.org/10.1145/3724363.3729032}, author = {Rentea, Ilinca and Migut, Gosia and Krijthe, Jesse}, keywords = {controlled experiment, education, interactive visualizations, knowledge gain, machine learning, motivation, location = Nijmegen, Netherlands}, abstract = {With the fast integration of Machine Learning (ML) across industries, effective pedagogical strategies are essential for teaching this complex and evolving field. Machine Learning is now widely integrated into various university programs and introduced at earlier educational stages, including high school and secondary school. However, ML pedagogy lacks standardized teaching methods compared to other science-related subjects, which have established norms for topic introduction, teaching tools, and assessment methods. Inspired by other fields, this research explores the use of interactive visualizations in teaching ML topics, more specifically in teaching Gradient Descent (GD) and Principal Component Analysis (PCA). The target population consists of Computer Science and Engineering Bachelor students who have not yet followed any Machine Learning courses but have foundational knowledge in calculus, linear algebra, and statistics. The evaluation measures knowledge gained and student motivation, compared to a static version of the materials. Results show a significant positive effect in knowledge related to PCA with interactive visualizations, but no differences in knowledge gain for GD or in learning motivation for either topic. With these results, we contribute to the body of evidence-based teaching methods in Machine Learning and identify further research needed to generalize the effect of interactive visualizations as a teaching method for teaching ML basic concepts.} }
@inproceedings{10.1145/3674029.3674050, title = {Machine Learning Tool for Wildlife Image Classification}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {127--132}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674050}, url = {https://doi.org/10.1145/3674029.3674050}, author = {Seljebotn, Karoline and Lawal, Isah A.}, keywords = {Camera Traps, Deep Learning, Wildlife Classification, location = Oslo, Norway}, abstract = {Wildlife researchers gather a large amount of image data during fieldwork. Reviewing this data is time-consuming and requires specialized expertise. To address this issue, machine learning models can automatically classify animals in these images. This study introduces a new method for classifying animals in both benchmark and camera trap images using a single model. The model achieved a top-1 accuracy of 93\% for benchmark images and 56\% for camera trap images previously unseen. The model was integrated into a web application, making it accessible to wildlife researchers without programming knowledge.} }
@inproceedings{10.1145/3760544.3764125, title = {Machine Learning-Based Distance Estimation for Molecular Communication}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {134--138}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764125}, url = {https://doi.org/10.1145/3760544.3764125}, author = {Cheng, Zhen and Zheng, Jianlong and Xu, Ziyan}, keywords = {molecular communication, distance estimation, machine learning, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication (MC) transmits information through the release, diffusion, and reception of molecules, holding great potential in the field of drug delivery. In an MC system, the prediction of the distance between the transmitter and the receiver is crucial for the receiver's resource consumption. Traditional distance detection strategies mainly focus on known channel state information (CSI). To address this limitation, this paper proposes a method for estimating the distance between the transmitters and the receiver in MC system with unknown CSI using a deep neural network (DNN) model. We employ Monte Carlo simulation to capture the positions of molecules in a three-dimensional environment. The dataset is generated based on the molecular coordinates at each position. Numerical results indicate that the DNN model can accurately estimate the distance between the transmitters and the receiver, demonstrating good detection capabilities and generalization ability. Additionally, the minimum distance between the transmitters and the receiver's boundary also affects the accuracy of the distance estimation.} }
@article{10.1145/3616537, title = {Byzantine Machine Learning: A Primer}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3616537}, url = {https://doi.org/10.1145/3616537}, author = {Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael}, keywords = {Byzantine machine learning, distributed SGD, robust aggregation}, abstract = {The problem of Byzantine resilience in distributed machine learning, a.k.a. Byzantine machine learning, consists of designing distributed algorithms that can train an accurate model despite the presence of Byzantine nodes—that is, nodes with corrupt data or machines that can misbehave arbitrarily. By now, many solutions to this important problem have been proposed, most of which build upon the classical stochastic gradient descent scheme. Yet, the literature lacks a unified structure of this emerging field. Consequently, the general understanding on the principles of Byzantine machine learning remains poor. This article addresses this issue by presenting a primer on Byzantine machine learning. In particular, we introduce three pillars of Byzantine machine learning, namely the concepts of breakdown point, robustness, and gradient complexity, to curate the efficacy of a solution. The introduced systematization enables us to (i) bring forth the merits and limitations of the state-of-the-art solutions, and (ii) pave a clear path for future advancements in this field.} }
@inproceedings{10.1145/3747227.3747246, title = {A Review of Machine Learning Algorithms Applied to Reservoir Exploration and Prediction}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {115--120}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747246}, url = {https://doi.org/10.1145/3747227.3747246}, author = {Lu, Bing and Wang, Hanqing and Zhao, Huilan and Song, Huilan and Li, Yan}, keywords = {Deep learning, Low-permeability reservoirs, Machine learning, Reservoir prediction}, abstract = {Reservoir exploration and prediction are highly complex tasks. They require knowledge from multiple disciplines. Key challenges include interpreting multidimensional geological data and evaluating low-permeability reservoirs. Traditional methods mainly rely on porosity and permeability. However, they often fail to reflect the heterogeneity and true storage capacity of tight reservoirs. In recent years, machine learning has become a promising alternative. Algorithms such as Support Vector Machines (SVM), Random Forests (RF), clustering, and deep learning models like CNNs and RNNs provide new tools for reservoir analysis. These methods are well-suited for handling complex, nonlinear, and high-dimensional data. In this work, we review recent developments in the use of machine learning for reservoir evaluation. It highlights how these techniques improve quantitative assessment and support better decision-making in oil and gas development.} }
@inproceedings{10.1145/3722237.3722398, title = {Machine Learning and Graduate Education in Finance}, booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education}, pages = {937--943}, year = {2025}, isbn = {9798400712692}, doi = {10.1145/3722237.3722398}, url = {https://doi.org/10.1145/3722237.3722398}, author = {Zhang, Kan and Wang, Fengqingyang and Wang, Suze}, keywords = {Causal Inference, Data Analysis, Financial Education, Machine Learning, Prediction}, abstract = {This paper discusses the application of machine learning in graduate education in finance, emphasizing its importance in data analysis, prediction, and causal inference. The paper points out that machine learning can enhance students' professional competence, expand their knowledge areas, and enhance their work abilities. Through theoretical teaching and research applications, machine learning methods can help process large amounts of unstructured data and combine with traditional econometric methods to improve the accuracy and explanatory power of research. The survey results support the effectiveness of machine learning teaching. At the end of the paper, it is proposed to continuously update the teaching content of machine learning technology, while paying attention to its limitations, in order to promote the improvement of students' comprehensive abilities.} }
@article{10.1145/3736751, title = {Maintainability and Scalability in Machine Learning: Challenges and Solutions}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3736751}, url = {https://doi.org/10.1145/3736751}, author = {Shivashankar, Karthik and Al Hajj, Ghadi and Martini, Antonio}, keywords = {Machine learning, deep learning, maintainability, scalability}, abstract = {Rapid advancements in Machine Learning (ML) introduce unique maintainability and scalability challenges. Our research addresses the evolving challenges and identifies ML maintainability and scalability solutions by conducting a thorough literature review of over 17,000 papers, ultimately refining our focus to 124 relevant sources that meet our stringent selection criteria. We present a catalogue of 41 Maintainability and 13 Scalability challenges and solutions across Data, Model Engineering and the overall development of ML applications and systems. This study equips practitioners with insights on building robust ML applications, laying the groundwork for future research on improving ML system robustness at different workflow stages.} }
@article{10.1145/3709705, title = {Modyn: Data-Centric Machine Learning Pipeline Orchestration}, journal = {Proc. ACM Manag. Data}, volume = {3}, year = {2025}, doi = {10.1145/3709705}, url = {https://doi.org/10.1145/3709705}, author = {B\"other, Maximilian and Robroek, Ties and Gsteiger, Viktor and Holzinger, Robin and Ma, Xianzhe and T\"oz\"un, Pnar and Klimovic, Ana}, keywords = {data-centric ai, machine learning pipelines, online learning}, abstract = {In real-world machine learning (ML) pipelines, datasets are continuously growing. Models must incorporate this new training data to improve generalization and adapt to potential distribution shifts. The cost of model retraining is proportional to how frequently the model is retrained and how much data it is trained on, which makes the naive approach of retraining from scratch each time impractical. We present Modyn, a data-centric end-to-end machine learning platform. Modyn's ML pipeline abstraction enables users to declaratively describe policies for continuously training a model on a growing dataset. Modyn pipelines allow users to apply data selection policies (to reduce the number of data points) and triggering policies (to reduce the number of trainings). Modyn executes and orchestrates these continuous ML training pipelines. The system is open-source and comes with an ecosystem of benchmark datasets, models, and tooling. We formally discuss how to measure the performance of ML pipelines by introducing the concept of composite models, enabling fair comparison of pipelines with different data selection and triggering policies. We empirically analyze how various data selection and triggering policies impact model accuracy, and also show that Modyn enables high throughput training with sample-level data selection.} }
@article{10.5555/3648699.3648808, title = {Dimensionless machine learning: imposing exact units equivariance}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Villar, Soledad and Yao, Weichi and Hogg, David W. and Blum-Smith, Ben and Dumitrascu, Bianca}, abstract = {Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods that are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.} }
@article{10.1145/3719663, title = {Machine Learning for Infectious Disease Risk Prediction: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3719663}, url = {https://doi.org/10.1145/3719663}, author = {Liu, Mutong and Liu, Yang and Liu, Jiming}, keywords = {Machine learning, data-driven modeling, epidemiology-inspired learning, infectious disease risk prediction, transmission dynamics characterization}, abstract = {Infectious diseases place a heavy burden on public health worldwide. In this article, we systematically investigate how machine learning (ML) can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation for using ML for infectious disease risk prediction. Next, we describe the development and application of various ML models for infectious disease risk prediction, categorizing them according to the models’ alignment with vital public health concerns specific to two distinct phases of infectious disease propagation: (1) the pandemic and epidemic phases (the P-E phases) and (2) the endemic and elimination phases (the E-E phases), with each presenting its own set of critical questions. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluations. We conclude with a discussion of open questions and future directions.} }
@article{10.1145/3643456, title = {Pitfalls in Machine Learning for Computer Security}, journal = {Commun. ACM}, volume = {67}, pages = {104--112}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3643456}, url = {https://doi.org/10.1145/3643456}, author = {Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad}, abstract = {With the growing processing power of computing systems and the increasing availability of massive datasets, machine learning algorithms have led to major breakthroughs in many different areas. This development has influenced computer security, spawning a series of work on learning-based security systems, such as for malware detection, vulnerability discovery, and binary code analysis. Despite great potential, machine learning in security is prone to subtle pitfalls that undermine its performance and render learning-based systems potentially unsuitable for security tasks and practical deployment.In this paper, we look at this problem with critical eyes. First, we identify common pitfalls in the design, implementation, and evaluation of learning-based security systems. We conduct a study of 30 papers from top-tier security conferences within the past 10 years, confirming that these pitfalls are widespread in the current security literature. In an empirical analysis, we further demonstrate how individual pitfalls can lead to unrealistic performance and interpretations, obstructing the understanding of the security problem at hand. As a remedy, we propose actionable recommendations to support researchers in avoiding or mitigating the pitfalls where possible. Furthermore, we identify open problems when applying machine learning in security and provide directions for further research.} }
@article{10.1145/3725809, title = {Differentiable Economics: Strategic Behavior, Mechanisms, and Machine Learning}, journal = {Commun. ACM}, volume = {68}, pages = {80--88}, year = {2025}, issn = {0001-0782}, doi = {10.1145/3725809}, url = {https://doi.org/10.1145/3725809}, author = {Bichler, Martin and Parkes, David C.}, abstract = {Economics studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these agents. Game theory had a substantial impact on economic modeling because it allows us to model the outcome of such economic interaction while taking the incentives of individual agents into account. Mechanism design does the same when designing the rules of economic institutions. Unfortunately, these economic models have turned out to be computationally hard to solve. For example, finding equilibrium in some incomplete-information models of markets with continuous valuation and action spaces are hard in PP, and designing a revenue-maximizing multi-item auction is #P-hard. This computational complexity poses a fundamental barrier in modeling economic systems but is worst-case and considers non-generic instances. Differentiable economics describes a new approach to solving these central problems in the economic sciences. It uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems. In particular, neural networks and learning algorithms such as Stochastic Gradient Descent have been shown to be very effective. Machine learning has led to breakthroughs in many sciences, and it also holds the potential to fundamentally alter how we analyze and design economic systems.A new approach to solving central problems in the economic sciences uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems.} }
@article{10.1145/3735969, title = {Instrumental Variables in Causal Inference and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3735969}, url = {https://doi.org/10.1145/3735969}, author = {Wu, Anpeng and Kuang, Kun and Xiong, Ruoxuan and Wu, Fei}, keywords = {Causal machine learning, instrumental variable, control function, unmeasured confounders}, abstract = {Causal inference is the process of drawing conclusions about causal relationships between variables using a combination of assumptions, study designs, and estimation strategies. In machine learning, causal inference is crucial for uncovering the mechanisms behind complex systems and making informed decisions. This article provides a comprehensive overview of using Instrumental Variables (IVs) in causal inference and machine learning, with a focus on addressing unobserved confounding that affects both treatment and outcome variables. We review identification conditions under standard assumptions in the IV literature. In this article, we explore three key research areas of IV methods: Two-Stage Least Squares (2SLS) regression, control function (CFN) approaches, and recent advances in IV learning methods. These methods cover both classical causal inference approaches and recent advancements in machine learning research. Additionally, we provide a summary of available datasets and algorithms for implementing these methods. Furthermore, we introduce a variety of applications of IV methods in real-world scenarios. Lastly, we identify open problems and suggest future research directions to further advance the field. A toolkit of reviewed IV methods with machine learning (MLIV) is available at .} }
@inproceedings{10.1145/3749566.3749567, title = {Prediction of employment market trends for college students based on machine learning}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {1--5}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749567}, url = {https://doi.org/10.1145/3749566.3749567}, author = {Shao, Chen and Wang, Xinyan and Qiao, Shengnan and Liu, Shipeng}, keywords = {STEM field, employment market trends, machine learning}, abstract = {This article provides an in-depth prediction and analysis of the employment market trends for college students through machine learning algorithms. By collecting and processing relevant employment data from the past decade, a predictive model including time series analysis and regression analysis was established, aiming to provide valuable information on employment market trends for college students, educational institutions, and governments, and to provide scientific basis for understanding and adapting to the rapidly changing employment environment. The research results indicate that the development trend of the employment market for college students is influenced by various factors, among which economic environment, technological progress, and improvement in education level play a decisive role.} }
@inproceedings{10.1145/3664475.3664574, title = {Machine Learning \&amp; Neural Networks}, booktitle = {ACM SIGGRAPH 2024 Courses}, year = {2024}, isbn = {9798400706837}, doi = {10.1145/3664475.3664574}, url = {https://doi.org/10.1145/3664475.3664574}, author = {Sharma, Rajesh and Tang, Mia} }
@inproceedings{10.1145/3670865.3673573, title = {Machine Learning-Powered Course Allocation}, booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation}, pages = {1099}, year = {2024}, isbn = {9798400707049}, doi = {10.1145/3670865.3673573}, url = {https://doi.org/10.1145/3670865.3673573}, author = {Soumalias, Ermis and Zamanlooy, Behnoosh and Weissteiner, Jakob and Seuken, Sven}, keywords = {course allocation, preference elicitation, combinatorial assignment, location = New Haven, CT, USA}, abstract = {We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7\%--11\% and 17\%--29\%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM while making the upgrade process simple for universities and seamless for their students.} }
@inproceedings{10.1145/3711896.3737858, title = {3rd Workshop on Causal Inference and Machine Learning in Practice}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6290--6291}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737858}, url = {https://doi.org/10.1145/3711896.3737858}, author = {Lee, Jeong-Yoon and Pan, Jing and Wu, Yifeng and Harinen, Totte and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Yin, Sichao and Stevenson, Roland and Wang, Jingshen and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, keywords = {causal inference, machine learning, location = Toronto ON, Canada}, abstract = {The 3rd Workshop on Causal Inference and Machine Learning in Practice at KDD 2025 aims to bring together researchers, industry professionals, and practitioners to explore the application of causal inference within machine learning models. As causal machine learning techniques gain traction across industries, practical challenges related to trustworthiness, robustness, and fairness remain at the forefront. This workshop will provide a forum to discuss methodologies for evaluating causal models in real-world scenarios and explore innovative applications that integrate causal inference with generative AI (GenAI) and large language models (LLMs). Topics of interest include using GenAI and LLMs to facilitate causal inference tasks and leveraging causal inference techniques for evaluating and improving GenAI/LLM models. Building on the success of the previous workshop editions at KDD 2023 and KDD 2024, which attracted over 200 and 250 participants, respectively, this workshop will continue fostering collaboration between academia and industry. Through invited talks, contributed papers, and interactive discussions, we will address key challenges and opportunities at the intersection of causal inference and machine learning. As the field continues to evolve, this workshop serves as a crucial platform for knowledge exchange and innovation, driving forward the application of causal techniques in machine learning and AI.} }
@inproceedings{10.1145/3701716.3715280, title = {Towards Democratized Machine Learning: A Semantic Web Approach}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {697--700}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715280}, url = {https://doi.org/10.1145/3701716.3715280}, author = {Klironomos, Antonis}, keywords = {knowledge graphs, machine learning, semantic web, similarity measures, location = Sydney NSW, Australia}, abstract = {The rapid growth of machine learning (ML) research has produced a vast and expanding collection of algorithms, datasets, and pipelines available on the Web. However, fragmented and dispersed documentation of these resources hampers accessibility, transparency, and effective use, posing challenges for users seeking to understand, adapt, and create ML pipelines. To address these challenges, we leverage Knowledge Graphs (KGs) and ontologies to represent ML pipelines as executable KGs (ExeKGs). This approach fosters an intuitive understanding of pipeline components and their relationships while defined constraints streamline the creation of valid and efficient pipelines. Furthermore, the structure of our KGs enables intelligent exploration and discovery of relevant ML artifacts, including pipelines and datasets. By incorporating KG-based ML techniques, we enhance the discovery and reuse of these artifacts. To consolidate these functionalities and provide users with an intuitive interface, we are developing ExeKGLab, a GUI-based platform for interacting with ExeKGs. This thesis explores the potential of KGs to democratize the ML landscape. We present our ongoing efforts to build a KG for ML, emphasizing its role in simplifying pipeline design, enhancing comprehension, and enabling smart exploration. By creating a structured and interconnected framework, our approach seeks to bridge gaps in accessibility and foster a more collaborative ML ecosystem. We invite discussion and feedback to advance this promising direction for future ML research.} }
@article{10.1145/3733838, title = {Practitioners and Bias in Machine Learning: A Study}, journal = {ACM Trans. Interact. Intell. Syst.}, volume = {15}, year = {2025}, issn = {2160-6455}, doi = {10.1145/3733838}, url = {https://doi.org/10.1145/3733838}, author = {Cinca, Robert and Costanza, Enrico and Musolesi, Mirco}, keywords = {ML Bias, Operationalizing Bias, machine learning, machine learning practitioners, interview study}, abstract = {The increasing adoption of machine learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with 22 participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.} }
@inbook{10.1145/3728725.3728813, title = {Loan Default Prediction Based on Machine Learning Approaches}, booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security}, pages = {557--564}, year = {2025}, isbn = {9798400713453}, url = {https://doi.org/10.1145/3728725.3728813}, author = {Cai, Xinyu and Dai, Wenbo and Lu, Jingyu}, abstract = {To address the credit risk losses incurred by commercial banks due to loan defaults, this study utilizes the loan default prediction dataset from the Alibaba Tianchi platform to develop machine learning models for predicting customer defaults, aiming to mitigate credit risk. Given the characteristics of class imbalance and high dimensionality of loan data, data preprocessing and exploratory data analysis are conducted. Based on a comparative analysis of various models, seven machine learning algorithms that demonstrate superior performance are selected for experimental comparison, including Decision Tree, Random Forest, AdaBoost, Bagging, XGBoost, LightGBM, and CatBoost. The results indicate that ensemble learning algorithms exhibit higher accuracy and predictive performance compared to single algorithms, with the CatBoost model performing best across various indicators, including AUC. The study identifies key features highly correlated with loan defaults, including loan grade, annual income, loan amount, credit history length, and debt-to-income ratio.} }
@inproceedings{10.1145/3627673.3679095, title = {Data Quality-aware Graph Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5534--5537}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679095}, url = {https://doi.org/10.1145/3627673.3679095}, author = {Wang, Yu and Ding, Kaize and Liu, Xiaorui and Kang, Jian and Rossi, Ryan and Derr, Tyler}, keywords = {data-centric artificial intelligence, graph machine learning, location = Boise, ID, USA}, abstract = {Recent years have seen a significant shift in Artificial Intelligence from model-centric to data-centric approaches, highlighted by the success of large foundational models. Following this trend, despite numerous innovations in graph machine learning model design, graph-structured data often suffers from data quality issues, jeopardizing the progress of Data-centric AI in graph-structured applications. Our proposed tutorial addresses this gap by raising awareness about data quality issues within the graph machine-learning community. We provide an overview of existing topology, imbalance, bias, limited data, and abnormality issues in graph data. Additionally, we highlight recent developments in foundational graph models that focus on identifying, investigating, mitigating, and resolving these issues.} }
@article{10.1145/3773898, title = {Eye-Tracking Indicators of Novice Programmers’ Proficiency: A Machine Learning Approach}, journal = {ACM Trans. Comput. Educ.}, year = {2025}, doi = {10.1145/3773898}, url = {https://doi.org/10.1145/3773898}, author = {Ahsan, Zubair and Obaidellah, Unaizah}, keywords = {Machine Learning, Computer Education, Human-Computer Interaction}, abstract = {This study investigates the efficacy of machine learning algorithms in classifying different levels of programming expertise among 60 first-year undergraduate computer science students from Asian demographic backgrounds using eye-tracking data. Existing studies offer limited detail on the construction and selection of feature sets for machine learning modeling. The study identifies Total Fixation Duration (TFD) and Total Visit Duration (TVD) as robust indicators for machine learning models when distinguishing between high and low performers (two levels) achieving accuracy as high as 76\%. However, performance declines notably when classifying expertise into three levels (high, average, and low), with performance dropping below 50\%, indicating that binary labels yield more reliable predictions than finer-grained categorization. Our findings suggest that such fixation-based metrics can provide real-time insights into student engagement and potentially cognitive effort, offering opportunities for adaptive instruction and targeted support. Hence, this model can be utilized for real-time screening of novice students during programming tasks in classroom settings, allowing educators to identify students requiring additional support, thereby enhancing programming education. Future research should address study limitations by increasing sample size, diversifying participant demographics, and cross-validating model performance with students’ grades.} }
@inproceedings{10.1145/3749566.3749594, title = {Predicting New York City Rent through Machine Learning —— Based on Airbnb Data}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {115--122}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749594}, url = {https://doi.org/10.1145/3749566.3749594}, author = {Gong, Youzhe}, keywords = {Airbnb, feature importance analysis, machine learning, rent prediction}, abstract = {This research aims to predict the rent of Airbnb listings in New York City through machine learning models and analyze the key factors may influence the prices. Utilizing Airbnb data from New York City for the year 2024, the study employs four models: Ridge Regression, Decision Tree, Random Forest, and XGBoost. Through feature engineering and parameter tuning, the models’ performances were optimized and compared. The Random Forest model was determined to perform the best, achieving the lowest test set RMSE of 29.6926. Additionally, the study reveals the impacts of key variables such as the number of rooms, location, number of amenities, and minimum stay requirements on rent prediction, based on feature importance ranking and OLS regression results. The significance of this study lies in providing an effective method for housing price prediction and offering a reference for hosts to set reasonable pricing mechanisms.} }
@inproceedings{10.1145/3724363.3729107, title = {Student Perspectives on the Challenges in Machine Learning}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {9--15}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729107}, url = {https://doi.org/10.1145/3724363.3729107}, author = {Sibia, Naaz and Richardson, Amber and Gao, Alice and Petersen, Andrew and Zhang, Lisa}, keywords = {artificial intelligence education, barriers and challenges, computing education, machine learning education, retention, student success, support, theory and practice, location = Nijmegen, Netherlands}, abstract = {Machine learning (ML) has become increasingly important for students, yet university-level ML courses are often perceived as challenging and time-intensive. This study explores the perceived challenges and motivations of students in a university ML course to inform curricular and teaching strategies. Through 5 surveys conducted in two instances of a 12-week introductory ML course, we examined students' engagement with both theoretical and practical aspects of ML. Results indicate that while students initially express strong interest in applying ML concepts, their reported interests can shift toward theoretical foundations. Challenges in both theory and practice are reported, including difficulties in mathematical notation and vectorization of gradient components, as well as model implementation. Students also discuss the time commitment required in a course with both theoretical and practical content. We recommend aligning course content with student motivations, providing targeted support for mathematical notation and vectorization, and balancing theoretical depth with practical application.} }
@article{10.1145/3664595, title = {Creativity and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3664595}, url = {https://doi.org/10.1145/3664595}, author = {Franceschelli, Giorgio and Musolesi, Mirco}, keywords = {Computational creativity, machine learning, generative deep learning, creativity evaluation methods}, abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.} }
@proceedings{10.1145/3733965, title = {WiseML '25: Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning}, year = {2025}, isbn = {9798400715310}, abstract = {We are delighted to welcome you to the ACM Workshop on Wireless Security and Machine Learning (WiseML) 2025. Continuing its tradition as a premier forum, WiseML brings together researchers and practitioners from the machine learning, privacy, security, wireless communications, and networking communities worldwide. The workshop serves as a dynamic platform for presenting cutting-edge research, exchanging innovative ideas, and fostering collaborations that advance these rapidly evolving fields. This year's event will be held in Arlington, VA, USA, and will feature a single-track program to encourage focused and engaging discussions. This year's call for papers attracted submissions from Europe, Asia, and the United States, which were carefully reviewed by 17 technical program committee (TPC) members representing both academia and industrial research labs. We are proud to present an outstanding technical program featuring eight papers that address a broad spectrum of topics in security, privacy, and adversarial machine learning as applied to wireless networks, mobile communications, 5G/IoT systems, cloud and edge computing, vehicular networks, and emerging applications.} }
@inproceedings{10.1145/3747227.3747273, title = {Intelligent Agricultural Greenhouse Control System Based on Internet of Things and Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {292--300}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747273}, url = {https://doi.org/10.1145/3747227.3747273}, author = {Xiao, Yu and Gong, Jiangchuan and Wang, Yanze and Wang, Cangqing}, keywords = {Agricultural greenhouse, Internet of Things (IoT), Machine Learning, RNN model}, abstract = {This study endeavors to conceptualize and execute a sophisticated agricultural greenhouse control system grounded in the amalgamation of the Internet of Things (IoT) and machine learning. Through meticulous monitoring of intrinsic environmental parameters within the greenhouse and the integration of machine learning algorithms, the conditions within the greenhouse are aptly modulated. The envisaged outcome is an enhancement in crop growth efficiency and yield, accompanied by a reduction in resource wastage. In the backdrop of escalating global population figures and the escalating exigencies of climate change, agriculture confronts unprecedented challenges. Conventional agricultural paradigms have proven inadequate in addressing the imperatives of food safety and production efficiency. Against this backdrop, greenhouse agriculture emerges as a viable solution, proffering a controlled milieu for crop cultivation to augment yields, refine quality, and diminish reliance on natural resources. Nevertheless, greenhouse agriculture contends with a gamut of challenges. Traditional greenhouse management strategies, often grounded in experiential knowledge and predefined rules, lack targeted personalized regulation, thereby resulting in resource inefficiencies. The exigencies of real-time monitoring and precise control of the greenhouse's internal environment gain paramount importance with the burgeoning scale of agriculture. To redress this challenge, the study introduces IoT technology and machine learning algorithms into greenhouse agriculture, aspiring to institute an intelligent agricultural greenhouse control system conducive to augmenting the efficiency and sustainability of agricultural production.} }
@inproceedings{10.1145/3718751.3718943, title = {Multivariate machine learning algorithm for stock return prediction modeling Machine learning algorithm for stock return prediction}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {1168--1172}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718943}, url = {https://doi.org/10.1145/3718751.3718943}, author = {Liu, Zile}, keywords = {BP neural network, Double machine learning algorithm, Multiple linear regression, Stock return rate}, abstract = {With the increase of market risk and volatility, it has become an important topic to evaluate stock return rate by using multi-algorithm comprehensive prediction. [Methods]: Multiple linear regression model, BP neural network model and double machine learning (DML) model were used to model stock return. [Data]: Stock weekly regular data of China A-share listed companies from 2010 to 2020 were used. [Results]: The interpretation degree of the model was 46\% by using multiple linear regression model. The F score of BP neural network model is 92\%. The degree of model explanation using DML model is more than 99\%.} }
@inproceedings{10.1145/3680532.3689591, title = {Introduction to Generative Machine Learning}, booktitle = {SIGGRAPH Asia 2024 Courses}, year = {2024}, isbn = {9798400711350}, doi = {10.1145/3680532.3689591}, url = {https://doi.org/10.1145/3680532.3689591}, author = {Sharma, Rajesh and Tang, Mia} }
@inproceedings{10.1145/3736733.3736744, title = {Explanations for Machine Learning Pipelines under Data Drift}, booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics}, year = {2025}, isbn = {9798400719592}, doi = {10.1145/3736733.3736744}, url = {https://doi.org/10.1145/3736733.3736744}, author = {Hasan, Jahid and Pradhan, Romila}, keywords = {pipeline robustness, data preparation, explainable AI, data drift, location = Intercontinental Berlin, Berlin, Germany}, abstract = {Ensuring the robustness of data preprocessing pipelines is essential for maintaining the reliability of machine learning model performance in the face of real-world data shifts. Traditional methods optimize preprocessing sequences for specific datasets but often overlook their vulnerability to future data variations. This research introduces a vulnerability score to quantify the susceptibility of preprocessing components to data shift. We propose a Linear Regression approach to establish a predictive relationship between the vulnerability of the pipeline components and changes in the model's performance. The generated relationships act as explanations for practitioners of the system and help them quantify the robustness of the pipeline to data shift. For a given pipeline, we generate an explanation that highlights a tolerable threshold beyond which a component is considered shift-vulnerable and is likely to contribute to performance degradation. For the shift-vulnerable scenarios, we further suggest a new pipeline for system maintainers that preserves the model performance without retraining. The proposed framework delivers a risk-aware assessment, empowering practitioners to anticipate potential performance changes and adapt their pipeline strategies accordingly. Experimental results on several real-world datasets generate valid explanations for pipeline robustness and demonstrate the opportunities in this field of research.} }
@inproceedings{10.1145/3637528.3671442, title = {Practical Machine Learning for Streaming Data}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6418--6419}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671442}, url = {https://doi.org/10.1145/3637528.3671442}, author = {Gomes, Heitor Murilo and Bifet, Albert}, keywords = {classification, concept drift, data streams, prediction intervals, regression, semi-supervised learning, location = Barcelona, Spain}, abstract = {Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/.} }
@inproceedings{10.1145/3749566.3749612, title = {Predictive Modeling of Airbnb Listing Prices in Boston Using Machine Learning Techniques}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {223--229}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749612}, url = {https://doi.org/10.1145/3749566.3749612}, author = {Tang, Jiaqi}, keywords = {Airbnb prices, Boston, Feature importance, Machine learning, Regression models}, abstract = {This paper aims to build a price prediction model based on the Airbnb official website listing indicators. Airbnb is currently a popular online platform that provides short-term and long-term homestays in multiple countries and regions. It plays the role of an intermediary and charges commission from each booking. This research combines predictive algorithms and explainable machine learning techniques to forecast the prices of Airbnb listings in the Boston, the United States, after the dataset is cleaned and preprocessed. Through multi-model comparison, this research finds the best performing model from a variety of methods. In addition, this study also evaluates the importance of features in the best performing model. Extreme Gradient Boosting (XGBoost) model outperforms other models on both training and test datasets, with a Mean Squared Error (MSE) and Mean Absolute Error (MAE) of 0.1536 and 0.2808 on the test set, respectively. Factors such as “is_entire_home”, “number_of_reviews”, “number of records presented by neighborhoods”, “minimum_nights”, and “pop_density” contribute the most to the prediction of listing prices. Overall, this study provides practical suggestions for Airbnb to optimize its listing strategy and improve its price recommendation system.} }
@inproceedings{10.1145/3670474.3685972, title = {When Device Modeling Meets Machine Learning: Opportunities and Challenges (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685972}, url = {https://doi.org/10.1145/3670474.3685972}, author = {Zhang, Lining and Peng, Baokang and Li, Yu and Liu, Hengyi and Dai, Wu and Wang, Runsheng}, keywords = {artificial neural network, compact model, device modeling, machine learning, location = Salt Lake City, UT, USA}, abstract = {Device modeling is essential for circuit simulations and designs in terms of constructing the circuit matrix equations of KCL and KVL. While there are classical methodologies, machine learning techniques are promising to bring innovations in the landscape of device modeling. This work reviews the device modeling from a top-down perspective, covering two different interpretations of modeling. Then the recent process in the domain-specific machine learning approaches is briefly summarized for logic and memory devices. The challenges ahead, for the machine learning model to support the industry's practical needs, are analyzed. A concept of fusion model, by deeply merging device physics and neural networks, is also explained.} }
@article{10.1145/3729234, title = {Privacy-Preserving Machine Learning Based on Cryptography: A Survey}, journal = {ACM Trans. Knowl. Discov. Data}, volume = {19}, year = {2025}, issn = {1556-4681}, doi = {10.1145/3729234}, url = {https://doi.org/10.1145/3729234}, author = {Chen, Congcong and Wei, Lifei and Xie, Jintao and Shi, Yang}, keywords = {Machine learning, secure multi-party computation, homomorphic encryption, privacy preserving, cryptography}, abstract = {Machine learning has profoundly influenced various aspects of our lives. However, privacy breaches have caused significant unease and concern among the general public. Preserving the privacy of sensitive data during the training and inference phases of machine learning is a key challenge. Cryptography-based privacy-preserving machine learning (crypto-based PPML) offers a viable solution to this challenge. In this article, we studied over 100 publications on crypto-based PPML frameworks published between 2016 and 2024, including 55 client-server architecture frameworks and 64 multi-party architecture frameworks. We provide a comprehensive overview of these frameworks, highlighting their features across various dimensions. Furthermore, we conduct an in-depth analysis, delving into scenarios, privacy goals, threat models, and optimization techniques that underpin these innovative solutions. We also discuss the challenges in the field of crypto-based PPML, including aspects of security and privacy, efficiency, and availability and usability. Finally, we offer an outlook on future research directions, aiming to provide valuable insights for both scholars and practitioners.} }
@inproceedings{10.5555/3712729.3712752, title = {LLM Enhanced Machine Learning Estimators for Classification}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {288--298}, year = {2025}, isbn = {9798331534202}, author = {Wu, Yuhang and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, abstract = {Pre-trained large language models (LLM) have emerged as a powerful tool for simulating various scenarios and generating informative output given specific instructions and multimodal input. In this work, we analyze the specific use of LLM to enhance a classical supervised machine learning method for classification problems. We propose a few approaches to integrate LLM into a classical machine learning estimator to further enhance the prediction performance. We examine the performance of the proposed approaches through both standard supervised learning binary classification tasks, and a transfer learning task where the test data observe distribution changes compared to the training data. Numerical experiments using four publicly available datasets are conducted and suggest that using LLM to enhance classical machine learning estimators can provide significant improvement on prediction performance.} }
@article{10.1145/3744237, title = {Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3744237}, url = {https://doi.org/10.1145/3744237}, author = {Zhong, Zhiqiang and Barkova, Anastasia and Mottin, Davide}, keywords = {Graph machine learning, knowledge-augmented methods, drug discovery, knowledge database, knowledge graph}, abstract = {Artificial Intelligence has become integral to intelligent drug discovery, with Graph Machine Learning (GML) emerging as a powerful structure-based method for modelling graph-structured biomedical data and investigating their properties. However, GML faces challenges such as limited interpretability and heavy dependency on abundant high-quality training data. On the other hand, knowledge-based methods leverage biomedical knowledge databases, e.g., Knowledge Graphs (KGs), to explore unknown knowledge. Nevertheless, KG construction is resource-intensive and often neglects crucial structural information in biomedical data. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with scarce training data. Nevertheless, a systematic definition for this burgeoning research direction is yet to be established. This survey formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery and organises collected KaGML works into four categories following a novel-defined taxonomy. We also present a comprehensive overview of long-standing drug discovery principles and provide the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases. To facilitate research in this promptly emerging field, we share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.} }
@inproceedings{10.1145/3676642.3729206, title = {Has Machine Learning for Systems Reached an Inflection Point?}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages = {3--4}, year = {2025}, isbn = {9798400710803}, doi = {10.1145/3676642.3729206}, url = {https://doi.org/10.1145/3676642.3729206}, author = {Maas, Martin}, keywords = {cluster scheduling, code optimization, data centers, large language models, machine learning for systems, memory allocation, storage systems, location = Rotterdam, Netherlands}, abstract = {A wide range of research areas - from natural language processing to computer vision and software engineering - have been (or are being) revolutionized by machine learning and artificial intelligence. Each of these areas went through an inflection point where they transitioned from ML as one of many approaches to ML becoming a predominant approach of the field. No example symbolizes this better than the AlexNet paper from 2012, which fundamentally transformed the field of computer vision. Computer systems remain a notable exception. In this talk, I will discuss emerging trends in the ML for Systems domain, how systems differ from these other areas, and what an ''AlexNet Moment'' for systems might look like. Along the way, I will describe a framework for categorizing work in the field and discuss emerging research problems and opportunities.} }
@inproceedings{10.1145/3747227.3747228, title = {Comparison and Prediction of Earth Pressure Based on Multiple Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {1--5}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747228}, url = {https://doi.org/10.1145/3747227.3747228}, author = {Li, Daimao and Xiao, Haohan and Guo, Qinghua and Han, Ke and Chen, Siyang}, keywords = {Data Processing, Earth Pressure, Machine Learning Algorithms, TBM}, abstract = {This paper investigates the application of machine learning algorithms in processing data from the first 1500 excavation segments of an Earth Pressure Balanced (EPB) Shield Tunnel Boring Machine (TBM). First, various models developed based on classical algorithms such as Decision Trees, Neural Networks, and Support Vector Machines are introduced, including Random Forest, XGBoost, and BPNN. The paper then elaborates on key data processing steps, noting that all models, except for the three Decision Tree-based models, require data normalization according to specific formulas. The model evaluation criteria are defined, encompassing multiple metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Additionally, the paper discusses how the dataset partitioning method varies based on the characteristics of the models and the configuration of hyperparameters. Finally, a comparative analysis of the predictive performance of each model is presented. The results show that, with the exception of ELM, most models perform similarly and meet engineering requirements. The different models exhibit varying strengths and weaknesses across different excavation segments, providing valuable insights for the application and improvement of machine learning models in related engineering projects.} }
@inproceedings{10.1145/3701047.3701067, title = {Comparative Analysis of Various Machine Learning Techniques for Rockburst Risk Prediction}, booktitle = {Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, pages = {105--109}, year = {2025}, isbn = {9798400711688}, doi = {10.1145/3701047.3701067}, url = {https://doi.org/10.1145/3701047.3701067}, author = {Yu, Hongtao and Li, Mingyao and Xiao, Haohan and Cao, Ruilang}, keywords = {Data processing, Feature clustering, Machine learning, Model evaluation, Rockburst risk prediction}, abstract = {This paper investigates the application of machine learning algorithms in rockburst risk prediction models. It begins with an overview of fundamental machine learning principles, followed by a detailed analysis of common classification algorithms, including Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), Random Forest (RF), and AdaBoost. Subsequently, the paper examines the use of clustering algorithms for feature extraction and reorganization. The technical process for developing accurate rockburst risk prediction models is outlined, encompassing data preprocessing, feature clustering, reorganization, and model evaluation. The findings offer valuable insights and references for the integration of machine learning in rockburst risk prediction, as well as innovative approaches for research and practical applications in related fields.} }
@article{10.1145/3728474, title = {Machine Learning for Blockchain Data Analysis: Progress and Opportunities}, journal = {Distrib. Ledger Technol.}, year = {2025}, doi = {10.1145/3728474}, url = {https://doi.org/10.1145/3728474}, author = {Azad, Poupak and Akcora, Cuneyt and Khan, Arijit}, keywords = {Machine Learning, Blockchain, Cryptocurrency, Graph Neural Networks, Temporal Data, Smart Contracts}, abstract = {Blockchain technology has rapidly emerged to mainstream attention. At the same time, its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data. Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts. Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity, such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies. These unique characteristics present opportunities and challenges for machine learning on blockchain data.On the one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for improving blockchain technology, such as e-crime detection and trends prediction. On the other hand, we shed light on blockchain's pivotal role by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem. This paper is a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field.} }
@inproceedings{10.1145/3724363.3729085, title = {K-12 Students' (Mis-)Conceptions of Machine Learning Paradigms}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {347--353}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729085}, url = {https://doi.org/10.1145/3724363.3729085}, author = {Kr\"uger, Jan Jakob and Gromann, Gabriel and Dengel, Andreas}, keywords = {ai literacy, computer science curriculum, k-12 education, machine learning, student misconceptions}, abstract = {Understanding Artificial Intelligence (AI) is on its way to becoming a key competence in the coming years due to its rapid advancements and growing societal impact. Learning about different paradigms in AI, particularly in machine learning, is crucial to enabling students to critically engage with and shape an AI-driven world. The study investigates how students of three grade levels (5th, 8th, and 11th) understand AI concepts, addressing the challenge of widespread misconceptions and the limited presence of AI in school curricula. A deductive qualitative analysis was used to analyze the students' preconceptions. The results indicate that younger students typically exhibit minimal or anthropomorphic views, reflecting limited exposure to basic AI principles. Older students show somewhat more advanced, yet still partial, understandings, with misconceptions persisting across all groups. The findings underscore the importance of integrating AI literacy into school curricula and aligning instruction with students' developmental stages. For younger learners, hands-on activities can introduce basic AI functionality while dispelling human-like attributions. Older students benefit from exploring ethical, technical, and societal dimensions of AI. This research highlights the need for age-appropriate AI education to foster informed, responsible users and creators of AI systems.} }
@inproceedings{10.1145/3728199.3728203, title = {Data Analysis of College Practice Teaching Based on Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {18--23}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728203}, url = {https://doi.org/10.1145/3728199.3728203}, author = {Wang, Xinwu}, keywords = {association rule analysis, cluster analysis, college practice teaching, data mining, machine learning algorithm}, abstract = {With the rapid development of information technology, especially the wide application of big data and artificial intelligence technology, the field of education is undergoing unprecedented changes. Among them, practical teaching in colleges and universities, as an important part of modern education system, is facing severe challenges. However, the traditional practice teaching mode often has some problems, such as uneven distribution of resources, disjointed teaching content and market demand, and single teaching method, which limit the improvement of practice teaching effect. In this regard, based on the actual needs, this paper will deeply analyze the application feasibility of artificial intelligence and machine learning in the reform of practical teaching in colleges and universities, and put forward a set of data analysis and processing scheme of practical teaching in colleges and universities, aiming at improving the effectiveness and personalized level of practical teaching through the integration, mining and analysis of multi-source data. Practice has proved that machine learning algorithms such as fuzzy C-means (FCM) clustering analysis and frequent pattern growth (FP-growth) correlation analysis are adopted in the scheme, and the data involved in college practice teaching are deeply analyzed, and the feasibility and effectiveness of the model are verified by practical tests, which provides scientific basis and decision support for college practice teaching.} }
@inproceedings{10.1145/3694860.3694868, title = {Dementia Deterioration Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Cloud and Big Data Computing}, pages = {54--59}, year = {2024}, isbn = {9798400717253}, doi = {10.1145/3694860.3694868}, url = {https://doi.org/10.1145/3694860.3694868}, author = {Dawood Almardoud, Layla and Tawfik, Hissam and Majzoub, Sohaib}, keywords = {Dementia prognosis, Interpretable models, Machine Learning, Mild Cognitive Impairment, Permutation Importance}, abstract = {Dementia is a disease that imposes medical, social, and economic challenges on medical professionals, caregivers, and the patients themselves. Dementia monitoring and prognosis are critical factors besides dementia diagnosis. However, recent studies on dementia prognosis involve people with diagnosed dementia and not non-demented with a high risk of being demented, like people with cognitive difficulties. The aim of this paper is to use Machine Learning (ML) algorithms to predict patients with the risk of deterioration from medical histories containing clinical, cognitive, and profile data collected from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The best model was the Random Forest model with a sensitivity of 0.79, accuracy of 0.77, specificity of 0.76, F1-score of 0.78, and an AUROC of 0.83. Moreover, the model was interpreted through permutation importance. Using the permutation importance tool, the study highlighted the strong effect of diagnosis information and specific symptoms like muscle pain for dementia deterioration prediction.} }
@inproceedings{10.1145/3712255.3716543, title = {Machine Learning Assisted Evolutionary Multi-objective Optimization}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1025--1041}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716543}, url = {https://doi.org/10.1145/3712255.3716543}, author = {Deb, Kalyanmoy and Saxena, Dhish Kumar and Mittal, Sukrit} }
@article{10.1145/3723356, title = {PredicTor: A Global, Machine Learning Approach to Tor Path Selection}, journal = {ACM Trans. Priv. Secur.}, volume = {28}, year = {2025}, issn = {2471-2566}, doi = {10.1145/3723356}, url = {https://doi.org/10.1145/3723356}, author = {Barton, Armon and Walsh, Timothy and Imani, Mohsen and Ming, Jiang and Wright, Matthew}, keywords = {Anonymous communications, Tor, machine learning, path selection, network performance}, abstract = {Tor users derive anonymity in part from the size of the Tor user base, but Tor struggles to attract and support more users due to performance limitations. Previous works have proposed modifications to Tor’s path selection algorithm to enhance both performance and security, but many proposals have unintended consequences due to incorporating information related to client location. We instead propose selecting paths using a global view of the network, independent of client location, and we propose doing so with a machine learning classifier to predict the performance of a given path before building a circuit. We show through a variety of simulated and live experimental settings, across different time periods, that this approach can significantly improve performance compared to Tor’s default path selection algorithm and two previously proposed approaches. In addition to evaluating the security of our approach with traditional metrics, we propose a novel anonymity metric that captures information leakage resulting from location-aware path selection, and we show that our path selection approach leaks no more information than the default path selection algorithm.} }
@inproceedings{10.1145/3760544.3764127, title = {Breath Patterns as Signals: A Machine Learning-based Molecular Communication Perspective}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {22--27}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764127}, url = {https://doi.org/10.1145/3760544.3764127}, author = {Bhattacharjee, Sunasheer and Pal, Saswati and Scheepers, P\'eter and Dressler, Falko}, keywords = {biological system, diagnostics, communication system model, machine learning, molecular communication, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication is a core pillar of the Internet of Bio-Nano Things. Exhaled breath, rich in water vapor, offers a viable medium for air-based molecular communication. This paper presents a low-cost, non-invasive approach using a DHT22 sensor to classify breath patterns, namely Eupnea, Bradypnea, and Tachypnea. Humidity and temperature signals from the mouth and nose are processed using machine learning (ML). The model achieves strong classification performance, showing that ML can effectively distinguish breath patterns despite sensor constraints.} }
@article{10.1145/3763132, title = {AccelerQ: Accelerating Quantum Eigensolvers with Machine Learning on Quantum Simulators}, journal = {Proc. ACM Program. Lang.}, volume = {9}, year = {2025}, doi = {10.1145/3763132}, url = {https://doi.org/10.1145/3763132}, author = {Bensoussan, Avner and Chachkarova, Elena and Even-Mendoza, Karine and Fortz, Sophie and Lenihan, Connor}, keywords = {Genetic Algorithms, Machine Learning, Optimisation, Quantum Computing, Quantum Program Analysis, Search-based Software Engineering}, abstract = {We present AccelerQ, a framework for automatically tuning quantum eigensolver (QE) implementations–these are quantum programs implementing a specific QE algorithm–using machine learning and search-based optimisation. Rather than redesigning quantum algorithms or manually tweaking the code of an already existing implementation, AccelerQ treats QE implementations as black-box programs and learns to optimise their hyperparameters to improve accuracy and efficiency by incorporating search-based techniques and genetic algorithms (GA) alongside ML models to efficiently explore the hyperparameter space of QE implementations and avoid local minima. Our approach leverages two ideas: 1) train on data from smaller, classically simulable systems, and 2) use program-specific ML models, exploiting the fact that local physical interactions in molecular systems persist across scales, supporting generalisation to larger systems. We present an empirical evaluation of AccelerQ on two fundamentally different QE implementations: ADAPT-QSCI and QCELS. For each, we trained a QE predictor model, a lightweight XGBoost Python regressor, using data extracted classically from systems of up to 16 qubits. We deployed the model to optimise hyperparameters for executions on larger systems of 20-, 24-, and 28-qubit Hamiltonians, where direct classical simulation becomes impractical. We observed a reduction in error from 5.48\% to 5.3\% with only the ML model and further to 5.05\% with GA for ADAPT-QSCI, and from 7.5\% to 6.5\%, with no additional gain with GA for QCELS. Given inconclusive results for some 20- and 24-qubit systems, we recommend further analysis of training data concerning Hamiltonian characteristics. Nonetheless, our results highlight the potential of ML and optimisation techniques for quantum programs and suggest promising directions for integrating software engineering methods into quantum software stacks.} }
@inbook{10.1145/3745238.3745358, title = {Pricing Strategy Optimization by Machine Learning in E-commerce}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {760--765}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745358}, author = {Liu, Quan and Song, Yunkui}, abstract = {This paper introduces the concept of pricing strategy and its importance in e-commerce, analyzes the limitations of traditional pricing methods. Subsequently, the article elaborates on the application of machine learning in E-commerce pricing strategies. In addition, the article also presents the key technologies for optimizing pricing strategies using machine learning. Finally, it looks forward to the development trend of future pricing systems.} }
@article{10.1145/3711713, title = {Applications of Certainty Scoring for Machine Learning Classification and Out-of-Distribution Detection}, journal = {ACM Trans. Probab. Mach. Learn.}, year = {2025}, doi = {10.1145/3711713}, url = {https://doi.org/10.1145/3711713}, author = {Berenbeim, Alexander M. and Cobb, Adam D. and Roy, Anirban and Jha, Susmit and Bastian, Nathaniel D.}, keywords = {Machine Learning, Uncertainty Quantification, Out-of-Distribution Detection, Computer Vision, Network Security}, abstract = {Quantitative characterizations and estimations of uncertainty are of fundamental importance for machine learning classification, particularly in safety-critical settings where continuous real-time monitoring requires explainable and reliable scoring. Reliance on the maximum a posteriori principle to determine label classification can obscure the certainty of a label assignment. We develop a theoretical framework for quantitative scores of certainty and competence based on predicted probability estimates, formally prove their properties, and empirically confirm the inferential power of these properties across different data modalities, tasks and model architectures. Our theoretical results establish that competent models have distinct distributions of certainty for true and false positives conditioned on inputs similar to training and testing data, and prove that this framework provides a reliable means to infer the quality of model predictions and detect false positives. Our empirical results bear out that there are distinct distributions of certainty scores on training and holdout data, as well as data that is a priori out-of-distribution. For expert models, at least 62.1\% of false positives could be identified when using a cut-off at at the bottom 5\% TP threshold. Further, we found a strong negative correlation between empirical competence and the FPR95TPR rate for EnergyBased out-of-distribution (OOD) detectors. Finally, we developed two forms of an OOD detector that were able to reliably distinguish in-distribution data from OOD data for both frequentist and Bayesian models, performing better on average than previous state-of-the-art EnergyBased OOD detection methods, and improving upon the baseline Monte Carlo Dropout AUPR-OUT performance on average by 14.4\% and 16.5\%, and reducing the FPR95TPR by 54.2\% and 37.6\%.} }
@inproceedings{10.1145/3696271.3696272, title = {Predicting Foreign Exchange EUR/USD Direction Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {1--9}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696272}, url = {https://doi.org/10.1145/3696271.3696272}, author = {Guyard, Kevin Cedric and Deriaz, Michel}, keywords = {Bayesian search, Forex prediction, Machine learning, Meta estimator}, abstract = {The Foreign Exchange market is a significant market for speculators, characterized by substantial transaction volumes and high volatility. Accurately predicting the directional movement of currency pairs is essential for formulating a sound financial investment strategy. This paper conducts a comparative analysis of various machine learning models for predicting the daily directional movement of the EUR/USD currency pair in the Foreign Exchange market. The analysis includes both decorrelated and non-decorrelated feature sets using Principal Component Analysis. Additionally, this study explores meta-estimators, which involve stacking multiple estimators as input for another estimator, aiming to achieve improved predictive performance. Ultimately, our approach yielded a prediction accuracy of 58.52\% for one-day ahead forecasts, coupled with an annual return of 32.48\% for the year 2022.} }
@inproceedings{10.1145/3674029.3674049, title = {Rating Prediction of Football Players using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {121--126}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674049}, url = {https://doi.org/10.1145/3674029.3674049}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco}, abstract = {This research Analysis the prediction of football player ratings through the application of diverse machine learning algorithms. Rating systems for sports teams have gathered considerable attention in academic research. The approach used by the authors of this paper serves as an effort to streamline scouts and performance analytics. Leveraging linear regression, decision tree regressor, random forest regressor, gradient boosting regressor, support vector regressor, voting regressor, ridge regression, lasso regression, k-nearest neighbours’ regression, Huber regression and elastic-net regression. The Analysis explores the efficiency of each algorithm and concludes that Support Vector Regressor algorithm performs the best with 91.84\% accuracy on the testing data followed by the Gradient Boosting Regressor with 90.78\%, Voting Regressor with 91.68\% and Random Forest Regressor with 88.89\%. Apart from them the K-Nearest Neighbours Regression Algorithm highly overfits the model with 100\% accuracy on the training set and 70.71\%. The conclusions drawn underscore the critical importance of judiciously selecting algorithms tailored to the specific characteristics of the dataset for precise and reliable player rating predictions.} }
@inproceedings{10.1145/3723498.3723843, title = {Diverse Level Generation via Machine Learning of Quality Diversity}, booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games}, year = {2025}, isbn = {9798400718564}, doi = {10.1145/3723498.3723843}, url = {https://doi.org/10.1145/3723498.3723843}, author = {Sfikas, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N.}, keywords = {Procedural content generation, machine learning, novelty search, quality diversity, strategy maps}, abstract = {Can we replicate the power of evolutionary algorithms in discovering good and diverse game content via generative machine learning (ML) techniques? This question could subvert current trends in procedural content generation (PCG) and beyond. By learning the behavior of quality-diversity (QD) evolutionary algorithms through ML, we stand to overcome the computational challenges inherent in QD search and ensure that the benefits of QD search are reproduced by efficient generative models. We introduce a novel, end-to-end methodology named Machine Learning of Quality Diversity (MLQD) which is executed in two steps. First, tailored QD evolution creates large and diverse training datasets from the ground up. Second, sophisticated ML architectures such as the Transformer learn the datasets’ underlying distributions, resulting in generative models that can emulate QD search via stochastic inference. We test MLQD on the use-case of generating strategy game map sketches, a task characterized by stringent constraints and a multidimensional feature space. Our findings are promising, demonstrating that the Transformer architecture can capture both the diversity and the quality traits of the training sets, successfully reproducing the behavior of a range of tested QD algorithms. This marks a significant advancement in our quest to automate the creation of high-quality, diverse game content, pushing the boundaries of what is possible in PCG and generative AI at large.} }
@article{10.1145/3757743, title = {Enhancing Blockchain Scalability using Off-Chain and Machine Learning Techniques}, journal = {J. Emerg. Technol. Comput. Syst.}, year = {2025}, issn = {1550-4832}, doi = {10.1145/3757743}, url = {https://doi.org/10.1145/3757743}, author = {Pawar, Manjula and Patil, Prakashgoud and Hiremath, P.S.}, keywords = {Blockchain, Scalability, Machine learning, Off-chain, KNN classification, Artificial Intelligence}, abstract = {Blockchain Technology is a nascent technology that possesses attributes such as immutability, security, transparency, openness, and decentralization. It is widely used in industry and business applications. Though it has the best features, it still suffers from some main characteristics, such as scalability and privacy. Scalability is measured through throughput (transactions per second), space,cost and latency. Bitcoin and Ethereum, which are prominent Blockchain platforms, carry out 7 and 20 transactions per second, respectively. This is much less than popular platforms such as Visa, PayPal, and Amazon, which perform thousands of transactions per second. Therefore, this paper presents comprehensive study of scalability improving techniques for Blockchain and case studies for improving scalability by using some of the techniques. The scalability of Blockchain systems can be enhanced by on-chain, off-chain, and machine learning algorithms.The proposed methodology improves the scalability using off-chain technique for supply chain management and KNN classification for healthcare domain.} }
@inproceedings{10.1145/3745812.3745873, title = {Predictive Analytics for Chemotherapy Effectiveness Using Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745873}, url = {https://doi.org/10.1145/3745812.3745873}, author = {kaur, Lakhwinder and Bamne, Shrikrishna N. and Dehankar, Jiwan and Khetani, Vinit and Pawar, S K and Goyal, Dinesh, -}, keywords = {Cancer Treatment, Chemotherapy, Machine Learning, Personalized Medicine, Predictive Analytics}, abstract = {Abstract: Chemotherapy is still an important part of treating cancer, but how well it works for each patient depends on things like the type of tumor, their genetics, and their general health. Accurately predicting how chemotherapy will work is necessary to make treatment plans work better, lower side effects, and raise patient mortality rates. This study looks into how machine learning (ML) methods can be used in predictive analytics to figure out how well treatment works. Machine learning models like Support Vector Machines (SVM), Random Forests, and Neural Networks are used to figure out which patients are most likely to respond to or not respond to chemotherapy. They do this by looking at information about each patient's medical history, genomic information, treatment plans, and the characteristics of their tumors. The information used to train and test the model comes from clinical studies and patient records. This makes sure that it has a wide range of cancer types and patient traits. To deal with complicated, high-dimensional data and find key drivers of treatment results, both controlled and untrained learning methods are used together. The models are judged by their performance measures, which include their accuracy, precision, recall, and the area under the receiver operating characteristic curve (AUC). The results show that prediction models based on machine learning can accurately predict how well chemotherapy will work, which lets doctors make personalized treatment plans. The results show that using predictive analytics in clinical decision- making could make cancer procedures more efficient and effective, reducing side effects and improving patient outcomes.} }
@inproceedings{10.1145/3732801.3732854, title = {Malicious URL Detection with Explainable Machine Learning Techniques}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {293--299}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732854}, url = {https://doi.org/10.1145/3732801.3732854}, author = {Wang, Bolun}, keywords = {Explainability, Machine Learning, Malicious URL Detection}, abstract = {Malicious Uniform Resource Locator (URL) detection research has been actively worked on and the focus is on achieving higher and more accurate results with machine learning and artificial intelligence approaches. Unfortunately, existing research on this problem has not yet been completed completely, and there remains a research gap in malicious URL detection in developing effective approaches that are capable of dealing with evasive techniques used by the attackers to conceal harmful URLs. This study explores an effective technique of detecting malicious URL detection with machine learnings with explainability. In particular, three advanced ML models are applied on one real parameters URL dataset, Logistic regression (LR), decision trees (DT) and Random Forest (RF) are employed. The results show that RF has the best detection accuracy and the best performance in terms of explain ability.} }
@inproceedings{10.1145/3716368.3735251, title = {Enhancing Modern SAT Solver With Machine Learning Method}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {886--892}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735251}, url = {https://doi.org/10.1145/3716368.3735251}, author = {Chen, Guanting and Wang, Jia}, keywords = {SAT Solver, CDCL, Machine Learning, GNN}, abstract = {Satisfiability (SAT), a well-known NP-complete problem, has been widely studied and drives numerous research fields. State-of-the-art SAT solvers rely on the Conflict-Driven Clause Learning (CDCL) algorithm to solve practical SAT instances with two possible outcomes – a solution for SAT instances or a proof proving no solution exists for UNSAT instances. While many heuristics were manually designed to improve CDCL in the past, recent efforts focus on applying machine and deep learning models, e.g. Graph Neural Networks (GNN), with the hope to make heuristics more effective and adaptive. Nevertheless, the demand for significant GPU resources and the effectiveness in a broader set of SAT and UNSAT instances remain the major challenges. In this paper, we present a GNN-based algorithm that predicts at the same time backbone variables for SAT instances and UNSAT-core variables for UNSAT instances. Leveraging offline model inference, our trained GNN model, and so the whole SAT solver, is able to run entirely on CPU, removing the need of GPU resources. Experimental results confirm that with our algorithm, a modern SAT solver is able to solve up to 5\% and 7\% more instances for different baseline solvers.} }
@inproceedings{10.1145/3746027.3764193, title = {Toward Fast and Exact Machine Learning Platform for Big Data}, booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia}, pages = {14372}, year = {2025}, isbn = {9798400720352}, doi = {10.1145/3746027.3764193}, url = {https://doi.org/10.1145/3746027.3764193}, author = {Fujiwara, Yasuhiro}, keywords = {ai-based data analysis, big data, computational pruning, scalable machine learning, location = Dublin, Ireland}, abstract = {Data has become the foundation of knowledge, and many companies are growing interested in harnessing AI-based data analysis to unlock its value. The volume of digital data is increasing at an unprecedented pace: market research reports estimate that global data volume, approximately 12.5 zettabytes in 2014, will reach around 180 zettabytes by 2025. Extracting patterns and trends from such big data is crucial for enabling data-driven decision-making. However, a key challenge lies in the enormous computational costs required for large-scale analysis, due to the inherent complexities of the task. Approximate methods are often employed to reduce these costs, but they inevitably trade exactness for efficiency. To overcome this limitation, our research aims to develop a machine learning platform that delivers both speed and accuracy. The core of our platform is computational pruning. This talk will introduce three representative pruning strategies. Specifically, it first introduces a pruning method that uses upper and lower bounds to omit computations. This method efficiently identifies unnecessary processes by using upper and lower bounds of scores to skip unnecessary computations. Next, this talk introduces a method to terminate computations that cannot yield solutions. This method maintains patterns that failed during the search process to avoid repeated futile searches. This talk finally introduces a method that prunes computations through optimistic processing. This method temporarily removes a constraint to find a solution quickly and then verifies if the obtained solution meets the constraint. These strategies can open the path to data analysis techniques that are both efficient and exact, ultimately empowering companies to make more reliable and timely decisions in today's increasingly data-driven world.} }
@inproceedings{10.1145/3747227.3747259, title = {Fine-Grained Seismic Damage Assessment of Buildings Based on Machine Learning and Evidential Reasoning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {189--196}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747259}, url = {https://doi.org/10.1145/3747227.3747259}, author = {Zhang, Ying and Guo, Hong-Mei and Yin, Wen-Gang and Zhao, Zhen and He, Zong-Hang}, keywords = {Belief rule base, Damage level, Evidential reasoning, Fine-grained seismic damage assessment, Individual building, Machine learning}, abstract = {Existing seismic damage assessment methods for buildings often suffer from coarse results (based on groups of buildings), complex processes, and poor adaptability. To address these limitations, this paper leverages the advantages of machine learning in complex data processing and algorithmic adaptability, integrates expert knowledge, and combines qualitative and quantitative information to propose a fine-grained seismic damage assessment method for individual buildings based on machine learning and evidential reasoning. Validation using historical seismic damage cases demonstrates that this method enables rapid, accurate, and intelligent assessment of building seismic damage. It outputs all possible damage levels and corresponding probabilities for individual buildings under the combined effects of multiple seismic factors. These quantitative, fine-grained results provide critical support for pre-earthquake risk perception, post-earthquake rapid damage mapping, and precision disaster management.} }
@inproceedings{10.1145/3708778.3708783, title = {Machine Learning-Based Hyper-Heuristics: A Clear Insight}, booktitle = {Proceedings of the 2024 7th International Conference on Computational Intelligence and Intelligent Systems}, pages = {29--37}, year = {2025}, isbn = {9798400717437}, doi = {10.1145/3708778.3708783}, url = {https://doi.org/10.1145/3708778.3708783}, author = {Bouazza, Wassim}, keywords = {Hyper-heuristics, Machine Learning, Optimization, Reinforcement Learning, Meta-Learning, Heuristic Selection, State-of-the-Art Review, Automated Problem Solving}, abstract = {In various scientific and engineering disciplines, decision-making processes frequently rely on optimization strategies to identify the best solution from a multitude of potential options. Traditional optimization methods have proven effective for many challenges; however, they face substantial difficulties when applied to complex, dynamic, and large-scale scenarios. Hyper-heuristics (HHs) have emerged as a promising alternative, offering a flexible and adaptive approach by optimizing the selection or creation of heuristics rather than directly tackling problem solutions. The integration of machine learning (ML) with hyper-heuristics has significantly propelled this field forward, enabling data-driven methods to enhance the selection and creation of heuristics. This article provides a comprehensive overview of the current landscape of machine learning-based hyper-heuristics. It categorizes and analyzes existing approaches, emphasizing the various ML techniques employed and their specific applications. Furthermore, it identifies emerging trends and future research directions aimed at enhancing the efficiency, adaptability, and robustness of HH methodologies through advanced ML techniques. By consolidating these insights, this review aims to equip researchers and practitioners with a deep understanding of the evolving landscape and potential opportunities for leveraging machine learning in the design of optimization heuristics.} }
@inproceedings{10.1145/3706628.3708848, title = {Resource Scheduling for Real-Time Machine Learning}, booktitle = {Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays}, pages = {50}, year = {2025}, isbn = {9798400713965}, doi = {10.1145/3706628.3708848}, url = {https://doi.org/10.1145/3706628.3708848}, author = {Singh, Suyash Vardhan and Ahmad, Iftakhar and Andrews, David and Huang, Miaoqing and Downey, Austin R. J. and Bakos, Jason D.}, keywords = {hardware acceleration, high-level synthesis (hls), real-time control systems, resource scheduling, location = Monterey, CA, USA}, abstract = {Data-driven physics models offer the potential for substantially increasing the sample rate for applications in high-rate cyberphys- ical systems, such as model predictive control, structural health monitoring, and online smart sensing. Making this practical re- quires new model deployment tools that search for networks with maximum accuracy while meeting both real-time performance and resource constraints. Tools that generate customized architectures for machine learning models, such as HLS4ML and FINN, require manual control over latency and cost trade-offs for each layer. This poster describes a proposed end-to-end framework that combines Bayesian optimization for neural architecture search with Integer Linear Optimization of layer cost-latency trade-off using HLS4ML ''reuse factors''. The proposed framework is shown in Fig. 1 and consists of a performance model training phase and two model deployment stages. The performance model training phase generates training data and trains a model to predict the resource cost and latency of an HLS4ML deployment of a given layer and associated reuse factor on a given FPGA. The first model deployment stage takes training, test, and validation data for a physical system-in this case, the Dynamic Reproduction of Projectiles in Ballistic Environments for Advanced Research (DROPBEAR) dataset-and searches the hyper- parameter space for Pareto optimal models with respect to latency and workload, as measured by the number of multiplies required for one forward pass. For each of the models generated, a second stage uses the performance model to optimize the reuse factor of each layer to guarantee that the whole model meets the resource constraint while minimizing end-to-end latency. Table 1 shows the benefit of the reuse factor optimizer that comprises the second stage of the model deployment phase, The results compare the performance of a baseline stochastic search to that of our proposed optimizer for an example model consisting of four convolutional layers, three LSTM layers, and one dense layer. The results show sample stochastic search runs having 1K, 10K, 100K, and 1M trials over a total search space of 209 million reuse factor permutations. The stochastic search reaches a point of diminishing returns with latency 205 𝜂 while the optimizer achieves a latency of 190 𝜂 and requires roughly 1000X less search time.} }
@article{10.1145/3757892.3757908, title = {Re-Evaluating Storage Carbon Emissions In Machine Learning Workloads}, journal = {SIGENERGY Energy Inform. Rev.}, volume = {5}, pages = {111--117}, year = {2025}, doi = {10.1145/3757892.3757908}, url = {https://doi.org/10.1145/3757892.3757908}, author = {Kopczyk, Dorota and Chandra, Abhishek}, keywords = {sustainable computing, ML system, storage carbon emissions, SSD vs HDD, embodied carbon, operational carbon, carbon-aware ML infrastructure}, abstract = {As machine learning (ML) workloads grow in scale, the carbon impact of data storage is underexplored. Despite the dominance of solid-state drives (SSDs) in ML pipelines for their performance benefits, the environmental trade-offs with traditional hard disk drives (HDDs) are not well understood. We compare the performance and total carbon cost of SSDs and HDDs in ML training workloads. To evaluate carbon impact driven by storage, we use the MLPerf Storage benchmark along with carbon emissions data from two energy grids. We find that although SSDs have significantly higher embodied emissions, their lower operational carbon and faster runtimes make them more efficient for I/O-bound ML workloads—especially once data exceeds memory capacity. While SSDs generally amortize their carbon cost over time, often outperforming HDDs in total emissions, there are caveats. When considering regional energy mix, results suggest that carbon-aware ML infrastructure should consider workload size, memory constraints, and grid intensity—not just device specifications.} }
@inproceedings{10.1145/3721146.3721935, title = {Machine Learning-based Deep Packet Inspection at Line Rate for RDMA on FPGAs}, booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems}, pages = {148--155}, year = {2025}, isbn = {9798400715389}, doi = {10.1145/3721146.3721935}, url = {https://doi.org/10.1145/3721146.3721935}, author = {Heer, Maximilian Jakob and Ramhorst, Benjamin and Alonso, Gustavo}, keywords = {FPGA, remote direct memory access (RDMA), deep packet inspection, machine learning, location = World Trade Center, Rotterdam, Netherlands}, abstract = {FPGAs are becoming increasingly important in the cloud and data centers, especially as network-attached accelerators or reconfigurable Network Interface Cards (NICs). In the cloud, Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCEv2) has emerged as the de facto standard protocol for data transport due to its low latency and high throughput. However, RDMA has several access control weaknesses limiting its applicability in the cloud. In this paper, we explore using machine learning-based deep packet inspection (DPI) as an enhancement to an open-source FPGA RDMA stack. The ultra low-latency ML model is integrated on the RDMA datapath and allows for detection of specific content in RDMA payloads (e.g., executables) at a line rate of 100Gbps while using less than 1\% of the available resources. Compared with existing work, our solution operates on the full message payload, at the transport level, and on a complete RDMA stack without sacrificing compatibility with RoCEv2 and its native performance characteristics, proving its potential as an end-to-end solution.} }
@article{10.1145/3749116.3749122, title = {Graph Data Management and Graph Machine Learning: Synergies and Opportunities}, journal = {SIGMOD Rec.}, volume = {54}, pages = {28--42}, year = {2025}, issn = {0163-5808}, doi = {10.1145/3749116.3749122}, url = {https://doi.org/10.1145/3749116.3749122}, author = {Khan, Arijit and Ke, Xiangyu and Wu, Yinghui}, abstract = {The ubiquity of machine learning, particularly deep learning, applied to graphs is evident in applications ranging from cheminformatics (drug discovery) and bioinformatics (protein interaction prediction) to knowledge graph-based query answering, fraud detection, and social network analysis. Concurrently, graph data management deals with the research and development of effective, efficient, scalable, robust, and user-friendly systems and algorithms for storing, processing, and analyzing vast quantities of heterogeneous and complex graph data. Our survey provides a comprehensive overview of the synergies between graph data management and graph machine learning, illustrating how they intertwine and mutually reinforce each other across the entire spectrum of the graph data science and machine learning pipeline. Specifically, the survey highlights two crucial aspects: (1) How graph data management enhances graph machine learning, including contributions such as improved graph neural network performance through graph data cleaning, scalable graph embedding, efficient graph-based vector data management, robust graph neural networks, user-friendly explainability methods; and (2) how graph machine learning, in turn, aids in graph data management, with a focus on applications like query answering over knowledge graphs and various data science tasks. We discuss pertinent open problems and delineate crucial research directions.} }
@inproceedings{10.1145/3680256.3721313, title = {DMML: A Machine-learning Performance Model for Data Migration}, booktitle = {Companion of the 16th ACM/SPEC International Conference on Performance Engineering}, pages = {136--143}, year = {2025}, isbn = {9798400711305}, doi = {10.1145/3680256.3721313}, url = {https://doi.org/10.1145/3680256.3721313}, author = {Ghaneshirazi, Hasti and Hamouda, Fares and Fokaefs, Marios and Haouari, Wejdene and Jania, Dariusz}, keywords = {data migration, data transfer time, decision tree, feature engineering, gradient boosting, hyperparameter tuning, machine learning, model evaluation, predictive modeling, random forest, regression models, xgboost, location = Toronto ON, Canada}, abstract = {Data migration at scale can be a daunting task. It may require significant resources and time, which must be taken from value-adding activities of an enterprise. Besides errors may occur, which can jeopardize the integrity of the data and waste resources. Accurately estimating data migration time and resource performance is critical for optimizing time, cost, and risk in large-scale data transfers. In this paper, we propose the use of machine learning to create performance models for data migration. We utilize DMBench, a benchmarking and load testing tool specifically tailored for data migrations, to generate data, simulating various data migration scenarios with different data sizes, vCPUs, RAM size, and data compression types. We experimented with multiple ML algorithms and showed the effect of hyperparameter tuning in the model's accuracy. Our results show that the XGBoost is the most accurate and consistent across the different scenarios. We demonstrate the model building process and its evaluation on an industrial case study.} }
@inproceedings{10.1145/3689609.3699919, title = {Abstract Domains for Machine Learning Verification (Keynote)}, booktitle = {Proceedings of the 10th ACM SIGPLAN International Workshop on Numerical and Symbolic Abstract Domains}, pages = {1}, year = {2024}, isbn = {9798400712173}, doi = {10.1145/3689609.3699919}, url = {https://doi.org/10.1145/3689609.3699919}, author = {Urban, Caterina}, abstract = {Machine learning (ML) software is increasingly being deployed in high-stakes and sensitive applications, raising important challenges related to safety, privacy, and fairness. In response, ML verification has quickly gained traction within the formal methods community, particularly through techniques like abstract interpretation. However, much of this research has progressed with minimal dialogue and collaboration with the ML community, where it often goes underappreciated. In this talk, we advocate for closing this gap by surveying possible ways to make formal methods more appealing to the ML community. We will survey our recent and ongoing work in the design and development of abstract domains for machine learning verification, and discuss research questions and avenues for future work in this context.} }
@inproceedings{10.1145/3757110.3757193, title = {Attenuation characteristics and machine learning predictions for metamaterial foundation systems}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {501--505}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757193}, url = {https://doi.org/10.1145/3757110.3757193}, author = {Feng, Li and Pi, Saiqi}, keywords = {Bandgap, Local resonance, Metamaterial, Neural network}, abstract = {The ambient vibrations caused by trains have become more serious. It is imperative to isolate those vibration. Thus, this proposes a novel metamaterial foundation based on local resonance theory for reducing vibration induced by trains and establishes a neural network model. The bandgaps of in-plane vibration and out-plane vibration are 30.1∼52.1Hz and 13.8∼25.5Hz. This indicates that the metamaterial foundation can be used to reduce vibrations induced by trains, and the neural network model has good performance, can accurately predict the bandgap based on structural and material parameters.} }
@inproceedings{10.1145/3647750.3647755, title = {Machine Learning-based Models for Predicting Defective Packages}, booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing}, pages = {25--31}, year = {2024}, isbn = {9798400716546}, doi = {10.1145/3647750.3647755}, url = {https://doi.org/10.1145/3647750.3647755}, author = {Wang, Yushuo and Mo, Ran and Zhang, Yao}, keywords = {Code Metrics, Defective Packages Prediction, Machine Learning, location = Singapore, Singapore}, abstract = {Software defects are often expensive to fix, especially when they are identified late in development. Packages encapsulate logical functionality and are often developed by particular teams. Package-level defect prediction provides insights into defective designs or implementations in a system early. However, there is little work studying how to build prediction models at the package level. In this paper, we develop prediction models by using seven machine-learning algorithms and code metrics. After evaluating our approach on 20 open-source projects, we have presented that we can build effective models for predicting defective packages by using an appropriate set of metrics. However, there is no single set of metrics that can be generalized across all projects. Our study demonstrates the potential for machine-learning models to enable effective package-level defect prediction. This can guide testing and quality assurance to efficiently locate and fix defects.} }
@inproceedings{10.1145/3709026.3709071, title = {Regularization of Machine Learning and Linear Algebra}, booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence}, pages = {327--332}, year = {2025}, isbn = {9798400718182}, doi = {10.1145/3709026.3709071}, url = {https://doi.org/10.1145/3709026.3709071}, author = {Liu, Shuang and Kabanikhin, Sergey Igorevich and Strijhak, Sergei Vladimirovich}, keywords = {system of linear algebraic equations, machine learning, linear neural network.}, abstract = {This paper explores the connection between systems of linear algebraic equations (SLAE) and machine learning methods, including regularization techniques, to establish a more novel neural network model based on linear neural networks. The goal is to construct a weight matrix for the neural network, which, by simulating the process of finding pseudo-solutions to SLAE, can generate the optimal answer for any input data. In this new neural network model, linear operations are performed first, followed by nonlinear operations, ultimately yielding an optimized weight matrix that serves as the pseudo-solution to the SLAE. The paper demonstrates how linear neural networks can be simplified to SLAE, how adding nonlinear layers to the linear neural network model can improve accuracy, and how machine learning methods can be used to find pseudo-solutions to SLAE.} }
@inproceedings{10.1145/3674029.3674031, title = {Prediction of Mobile Phone Prices using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {6--10}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674031}, url = {https://doi.org/10.1145/3674029.3674031}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco and Panwar, Disha and Shree, Shadeeksha}, abstract = {This research investigates upon the prediction of mobile phone prices based on various factors through the applications of multiple machine learning algorithms. Leveraging linear regression, decision tree regressor, random forest regressor, gradient boosting regressor, voting\&nbsp;regressor and support vector regressor. This work explores the effectiveness in capturing the intricate relationships between the pricing of mobile phones in the market and the various factors affecting it which may be based on the hardware, software, the brand value, etc. The experimental results reveal distinct strengths and limitations of each algorithm, with the ensemble-based voting regressor demonstrating superior predictive performance with a training accuracy of 93.21\% and testing accuracy of 88.98\%. Gradient boosting regressor overfits the model with a training accuracy of 100\% and testing accuracy of 97.91\% and the linear regression model is observed to be the least accurate with a training and testing accuracy of 7.77\% and 7.12\% respectively. This research lays the groundwork for informed algorithm selection and implementation in the development of advanced mobile price prediction systems.} }
@inproceedings{10.1145/3627673.3679103, title = {Hands-On Introduction to Quantum Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5507--5510}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679103}, url = {https://doi.org/10.1145/3627673.3679103}, author = {Chen, Samuel Yen-Chi and Kim, Joongheon}, keywords = {quantum architecture search, quantum machine learning, quantum neural networks, reinforcement learning, variational quantum circuits, location = Boise, ID, USA}, abstract = {This tutorial offers a hands-on introduction into the captivating field of quantum machine learning (QML). Beginning with the bedrock of quantum information science (QIS)-including essential elements like qubits, single and multiple qubit gates, measurements, and entanglement-the session swiftly progresses to foundational QML concepts. Participants will explore parameterized or variational circuits, data encoding or embedding techniques, and quantum circuit design principles. Delving deeper, attendees will examine various QML models, including the quantum support vector machine (QSVM), quantum feed-forward neural network (QNN), and quantum convolutional neural network (QCNN). Pushing boundaries, the tutorial delves into cutting-edge QML models such as quantum recurrent neural networks (QRNN) and quantum reinforcement learning (QRL), alongside privacy-preserving techniques like quantum federated machine learning, bolstered by concrete programming examples. Throughout the tutorial, all topics and concepts are brought to life through practical demonstrations executed on a quantum computer simulator. Designed with novices in mind, the content caters to those eager to embark on their journey into QML. Attendees will also receive guidance on further reading materials, as well as software packages and frameworks to explore beyond the session.} }
@inproceedings{10.1145/3723178.3723313, title = {Panic Attack Frequency Detection Using Machine Learning Algorithm}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {1014--1025}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723313}, url = {https://doi.org/10.1145/3723178.3723313}, author = {Nasiruddin, Kazi Md and Haque, Md. Jawadul and Hasan, Md. Jahid and Ahmed Shams, Md. Shayed and Shefat, Syed Nafiul}, keywords = {Classification, HoeffdingTree, J48, Machine Learning, Na\"ve Bayes, Panic attack frequency, Prediction, REPTree, Weka}, abstract = {Abstract:An individual's life can be heavily influenced by mental disorders. Machine learning has shown itself to be a possible tool for the prediction and treatment of some diseases. In this study, different machine learning classifiers were used to predict the number of panic episodes using different datasets in various patients. This dataset contained clinical and demographic details for 500 cases. Preparing data, employing different algorithms (e.g., J48, Naive Bayes, HoeffdingTree, REPTree, Ensemble methods), and evaluating model performance indicators are all important steps within this process.The findings showed that machine learning models exhibited accurate predictions of panic attack frequencies; all performance indexes have been led by the J48 decision tree algorithm. At the ‘Often’ frequency level, J48 had 94.8\% accuracy, 0.897 precision, and an F1-score of 0.933, while the recall score was found to be 0.973, i.e., for the same ‘Often’ level. For ‘Often,’ however, Naive Bayes showed the highest precision (0.992) but the lowest overall performance. These results suggest the potential for predicting the frequency of panic attacks through instructional machine learning models, which may assist with patient care and clinical decision making. Nevertheless, more investigation and study are needed to increase precision and generalizability among various demographics.} }
@inproceedings{10.1145/3721146.3721957, title = {Utilizing Large Language Models for Ablation Studies in Machine Learning and Deep Learning}, booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems}, pages = {230--237}, year = {2025}, isbn = {9798400715389}, doi = {10.1145/3721146.3721957}, url = {https://doi.org/10.1145/3721146.3721957}, author = {Sheikholeslami, Sina and Ghasemirahni, Hamid and Payberah, Amir H. and Wang, Tianze and Dowling, Jim and Vlassov, Vladimir}, keywords = {ablation studies, machine learning, deep learning, deep neural networks, feature ablation, model ablation, large language models, location = World Trade Center, Rotterdam, Netherlands}, abstract = {In Machine Learning (ML) and Deep Learning (DL) research, ablation studies are typically performed to provide insights into the individual contribution of different building blocks and components of an ML/DL system (e.g., a deep neural network), as well as to justify that certain additions or modifications to an existing ML/DL system can result in the proposed improved performance. Although dedicated frameworks for performing ablation studies have been introduced in recent years, conducting such experiments is still associated with requiring tedious, redundant work, typically involving maintaining redundant and nearly identical versions of code that correspond to different ablation trials. Inspired by the recent promising performance of Large Language Models (LLMs) in the generation and analysis of ML/DL code, in this paper we discuss the potential of LLMs as facilitators of ablation study experiments for scientific research projects that involve or deal with ML and DL models. We first discuss the different ways in which LLMs can be utilized for ablation studies and then present the prototype of a tool called AblationMage, that leverages LLMs to semi-automate the overall process of conducting ablation study experiments. We showcase the usability of AblationMage as a tool through three experiments, including one in which we reproduce the ablation studies from a recently published applied DL paper.} }
@inproceedings{10.1145/3744367.3744386, title = {The Application and Development of Machine Learning in Depression Diagnosis}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems}, pages = {113--116}, year = {2025}, isbn = {9798400715068}, doi = {10.1145/3744367.3744386}, url = {https://doi.org/10.1145/3744367.3744386}, author = {Wang, Huaixiong and Wang, Jian}, keywords = {Application Research, Depression, Machine Learning}, abstract = {The analysis scrutinizes the integration of machine learning in depression diagnosis, noting its impact on early detection, individualized care, and predictive analytics.The application of machine learning boosts diagnostic accuracy via the examination of clinical, psychological, and neuroimaging data.This study delineates supervised, unsupervised, and reinforcement learning approaches, exploring their deployment in data-driven diagnostics, neuroimaging analytics, and natural language processing (NLP).In addition, the predictive power of machine learning aids in the anticipation of treatment efficacy, facilitating the design of tailored treatment approaches.Despite its bright prospects, unresolved challenges pertain to data integrity, privacy concerns, and model interpretive issues.For wider deployment, further clinical trials and interdepartmental collaboration are imperative.For wider deployment, further clinical trials and interdepartmental collaboration are imperative.} }
@article{10.1145/3723157, title = {Machine Learning for Identifying Risk in Financial Statements: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3723157}, url = {https://doi.org/10.1145/3723157}, author = {Zavitsanos, Elias and Spyropoulou, Eirini and Giannakopoulos, George and Paliouras, Georgios}, keywords = {Risk assessment, misstatement detection, financial distress, bankruptcy prediction, fraud detection, financial reports, financial statements, machine learning, data mining, auditing}, abstract = {The work herein reviews the scientific literature on Machine Learning approaches for financial risk assessment using financial reports. We identify two prominent use cases that constitute fundamental risk factors for a company, namely misstatement detection and financial distress prediction. We further categorize the related work along four dimensions that can help highlight the peculiarities and challenges of the domain. Specifically, we group the related work based on (a) the input features used by each method, (b) the sources providing the labels of the data, (c) the evaluation approaches used to confirm the validity of the methods, and (d) the machine learning methods themselves. This categorization facilitates a technical overview of risk detection methods, revealing common patterns, methodologies, significant challenges, and opportunities for further research in the field.} }
@article{10.1145/3764582, title = {A Survey of Quantum Machine Learning: Foundations, Algorithms, Frameworks, Data and Applications}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3764582}, url = {https://doi.org/10.1145/3764582}, author = {Rodr\'guez-D\'az, Francesc and Guti\'errez-Avil\'es, David and Troncoso, Alicia and Mart\'nez-\'Alvarez, Francisco}, keywords = {Quantum machine learning, quantum foundations, quantum algorithms, quantum computing frameworks, quantum datasets, quantum applications}, abstract = {Quantum machine learning combines quantum computing with machine learning to solve complex computational problems more efficiently than classical approaches. This survey provides an introduction to the foundations, algorithms, frameworks, data and applications of quantum machine learning, serving as a resource for researchers and practitioners. We begin by reviewing existing surveys to identify gaps that this work addresses, followed by a detailed discussion of the foundational principles of quantum mechanics and machine learning essential for quantum machine learning. Key algorithms are examined, highlighting their mechanisms, advantages, and applications across various domains. Current frameworks and platforms for implementing quantum machine learning algorithms are explored, emphasizing their unique features and suitability for different contexts. Existing quantum datasets for practical usage are also reported and commented on. This survey also reviews over 135 articles, categorized into theoretical and practical contributions, to identify key advances, limitations, and application areas within quantum machine learning. Critical challenges such as hardware limitations, error rates, and scalability are analyzed to detect the obstacles that must be addressed for practical deployment. By synthesizing these elements into a structured overview, this survey aims at serving as both an introduction and a guide for advancing research and development in this disruptive field.} }
@inbook{10.1145/3757749.3757773, title = {Research on Concrete Strength Based on Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {149--154}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757773}, author = {Wei, Wanhua}, abstract = {Under the "Dual Carbon" strategic background, there is an increasingly urgent demand for synergistic optimization between low-carbon production and high-performance concrete. To address the limitations of traditional strength prediction methods, including insufficient analysis of multi-component interactions and high trial mix costs, this study proposes the use of Random Forest, Gradient Boosting Decision Tree and Stacking models to investigate the nonlinear effects of various components on concrete compressive strength and establish a concrete strength prediction system. Empirical analysis demonstrates that the Stacking model significantly outperforms other models. Feature importance analysis reveals that Age In Days, Cement Component and Water Component are dominant factors affecting concrete strength, while Fly Ash Component shows relatively lower influence. This model provides a data-driven decision-making tool for industrial solid waste recycling and low-carbon concrete mix design.} }
@article{10.1145/3728368, title = {Fairness Challenges in the Design of Machine Learning Applications for Healthcare}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3728368}, url = {https://doi.org/10.1145/3728368}, author = {Ryan, Seamus and Cai, Wanling and Bowman, Robert and Doherty, Gavin}, keywords = {Fairness, Healthcare, Machine Learning, Interviews}, abstract = {Machine learning-augmented applications have the potential to be powerful tools for decision-making in healthcare. However, healthcare is a complex domain that presents many challenges. These challenges, such as medical errors, clinician–patient relationships and treatment preferences, must be addressed to ensure fairness in ML-augmented healthcare applications. To better understand the influence these challenges have on fairness, 16 experienced engineers and designers with domain knowledge in healthcare technology were interviewed about how they would prioritise fairness in 3 healthcare scenarios (well-being improvement, chronic illness management, acute illness treatment). Using a template analysis, this work identifies the key considerations in the creation of fair ML for healthcare. These considerations clustered into categories related to technology, healthcare context and user perspectives. To explore these categories, we propose the stakeholder fairness conceptual model. This framework aids designers and developers in understanding the complex considerations that stem from the building, management and evaluation of ML-augmented healthcare applications, and how they affect the expectations of fairness. This work then discusses how this model may be applied when the health technology is directly provisioned to users, without a healthcare provider managing its use or adoption. This article contributes to the understanding of fairness requirements in healthcare, including the effect of healthcare errors, clinician-application collaboration and how the evaluation of healthcare technology becomes part of the fairness design process.} }
@article{10.1145/3728369, title = {Prevention of Data Poisonous Threats on Machine Learning Models in e-Health}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3728369}, url = {https://doi.org/10.1145/3728369}, author = {Alruwaili, Etidal and Moulahi, Tarek}, keywords = {Machine Learning, e-health, attack, poisonous threats, prevention}, abstract = {Machine learning is widely used across various fields, including e-health, to enhance efficiency, classify events, and make accurate predictions, such as diagnosing diseases and prescribing medications. However, machine learning models are increasingly vulnerable to data poisoning attacks, which manipulate training data to degrade model accuracy and cause incorrect predictions. This study focuses on detecting data poisoning in e-health applications by simulating label-flipping attacks at different rates (5\%, 25\%, 50\%, 75\%) on breast cancer and diabetes datasets. The performance of machine learning models in disease detection was evaluated before and after poisoning, alongside their ability to detect poisoned data. Results show that models perform significantly better on clean data, with a marked deterioration at higher poisoning rates (50\%–75\%). The Random Forest (RF) and Gradient Boosting (GB) models proved most effective in detecting poisoned data, particularly at higher rates of poisoning. Conversely, the Logistic Regression (LR) and Multi-layer Perceptron (MLP) models tended to overgeneralize, leading to false positives, especially in the breast cancer dataset. This study highlights the importance of safeguarding ML models in e-health from data poisoning threats.} }
@inproceedings{10.1145/3696673.3723076, title = {Obstructive Lung Disease Classification from Electrocardiograms Using Machine Learning}, booktitle = {Proceedings of the 2025 ACM Southeast Conference}, pages = {221--226}, year = {2025}, isbn = {9798400712777}, doi = {10.1145/3696673.3723076}, url = {https://doi.org/10.1145/3696673.3723076}, author = {Abdoulaye Soumana, Aboubacar and Lamichhane, Prajwol and Shabbir, Mehlam and Liu, Xudong and Nasseri, Mona and Helgeson, Scott}, keywords = {obstructive lung disease classification, machine learning, deep learning, electrocardiogram, digital health, health informatics, location = Southeast Missouri State University, Cape Girardeau, MO, USA}, abstract = {Pulmonary diseases, such as chronic obstructive pulmonary disease (COPD) and asthma, are among the leading causes of death in the US. These lung diseases often are diagnosed by pulmonologists using physical exam (e.g., lung auscultation) and objective measurement of lung function with pulmonary function testing (PFT). These extensive tests, however, can be inaccessible to many patients due to limited resources and availability. Nowadays hand-held medical devices (e.g., electrocardiogram (ECG) monitors) are already available to such patients and can yield ECG data that potentially could be used for diagnosis. In this paper, we explore the use of easily accessible ECGs to train machine learning models to classify obstructive lung disease (OLD). Not only do we utilize the time-series raw ECG directly, we also define and employ eleven features derived from the PQRST sequences of the ECGs. To this end, we develop and experiment with two approaches: deep neural network models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)) trained using the ECG signals directly, and non-neural models (e.g., support vector machines (SVMs) and logistic regression) trained using the derived features from ECGs. In the task of classifying whether a patient has OLD or not, our results show that deep neural network models outperformed the non-neural models, though the difference is within 3\% on accuracy and F1-score metrics.} }
@inproceedings{10.1145/3672608.3707891, title = {Data Balancing for Mitigating Sampling Bias in Machine Learning}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {1204--1212}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707891}, url = {https://doi.org/10.1145/3672608.3707891}, author = {Inoc\^encio J\'unior, Ronaldo and Basgalupp, M\'arcio and Ludermir, Teresa and Lorena, Ana Carolina}, keywords = {machine learning, fairness, data balancing, location = Catania International Airport, Catania, Italy}, abstract = {We increasingly integrate technology into our daily activities, and using Machine Learning (ML) algorithms in various domains has become a common practice. However, in crucial sectors where algorithmic decisions significantly impact people's lives, there is a need to scrutinize these decisions more carefully. Using these algorithms in critical areas, such as courtrooms, raises concerns about potential bias and prejudice, directly affecting the justice and partiality of these tools. There is an urge to create algorithms supporting ethical decisions. This paper proposes using data balancing techniques to mitigate the sample bias present in datasets, aiming to make subsequent ML algorithm training more impartial. A version of the ADASYN algorithm is developed, which performs data balancing at both the class level and at the level of protected attributes, enhancing the diversity and representativeness of the protected groups in the datasets. Experimental results show the technique can promote greater fairness in the predictions of different ML models while keeping a good trade-off with overall accuracy.} }
@inproceedings{10.1145/3728199.3728270, title = {Compensation mechanism and machine learning application for cooperative watershed management in pollution control}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {423--430}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728270}, url = {https://doi.org/10.1145/3728199.3728270}, author = {Zhuo, Fenglian}, keywords = {Compensation Mechanism, Decision Tree, LSTM, Machine Learning, RNN, Random Forest}, abstract = {The management of urban and rural watersheds faces challenges due to outdated methods, highlighting the need to strengthen cost-sharing mechanisms and establish an ecological compensation framework. In this study, a comprehensive analysis was conducted using cooperative game theory and machine learning techniques in the Fujian Minjiang River Basin as an example. Based on the average tree solution (A-T solution), a fairer cost-sharing scheme is proposed, and a practical ecological compensation model is constructed by integrating the pollution intensity compensation factor. In addition, a machine learning approach was used to predict wastewater discharge trends and identify factors affecting water quality. The Random Forest model identified key factors including 'water temperature ((^ circ C ))', 'chemical oxygen demand (COD) (mg/L)' and 'dissolved oxygen (mg/L)'. Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) models were used to predict industrial wastewater discharges in Putian City from 1994 to 2022. The results suggest that regional geography has a significant impact on cost sharing and that coalition constraints contribute to a more equitable and balanced distribution of costs. The results of the study highlight the importance of pollution intensity in determining management costs and the need to develop compensation criteria. This take into account regional pollution levels to improve watershed management coordination. The LSTM model showed the highest accuracy with an R2 of 0.09, which exceeded the performance of traditional linear regression. This study provides insights that can inform improved watershed management, equitable cost sharing and ecological compensation, while highlighting the benefits of combining machine learning with traditional methods to improve forecasting and decision making.} }
@inproceedings{10.1145/3670474.3685970, title = {Machine Learning VLSI CAD Experiments Should Consider Atomic Data Groups}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685970}, url = {https://doi.org/10.1145/3670474.3685970}, author = {Gunter, Andrew David and Wilton, Steven}, keywords = {CAD, FPGA, VLSI, information leakage, machine learning, routing, supervised learning, location = Salt Lake City, UT, USA}, abstract = {Machine learning (ML) has proved useful across a wide range of applications in the very-large-scale integration computer-aided design (VLSI CAD) domain. To avoid overestimating ML models' generalization capabilities for real-world deployments, best practices utilize realistic data and avoid test set information leakage during ML model preparation. In this paper we identify a further consideration, atomic data groups, which are sets of very highly correlated data that may also lead to such overestimation if not accounted for in train-test splits during model evaluation. We investigate the potential impact of atomic data groups in experimental design through a case study of field-programmable gate array (FPGA) routing. Our investigations show that model performance in deployment is overestimated by 38\% in this case study when atomic data groups are ignored. We hope that these results motivate other ML CAD practitioners to be critical of their train-test splits and identify when atomic data groups are relevant to their model evaluations.} }
@inproceedings{10.1145/3726302.3731695, title = {The Second Tutorial on Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {4130--4133}, year = {2025}, isbn = {9798400715921}, doi = {10.1145/3726302.3731695}, url = {https://doi.org/10.1145/3726302.3731695}, author = {Diaz, Fernando and Drozdov, Andrew and Kim, To Eun and Salemi, Alireza and Zamani, Hamed}, keywords = {framework, information retrieval, machine learning, location = Padua, Italy}, abstract = {Retrieval-Enhanced Machine Learning (REML) refers to the use of information retrieval (IR) methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability, and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but not limited to, NLP. What is unique to our approach is the use of consistent notations to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at a website. Building on the momentum of our successful workshop at SIGIR 2023 and our tutorial at SIGIR-AP 2024, this year's tutorial features updated content with an emphasis on retrieval technologies used across the broader ML community. We also highlight their role in emerging, future-facing applications such as language agents and evolving scenarios where the extensive body of knowledge from IR can provide critical insights and capabilities.} }
@article{10.5555/3722577.3722945, title = {Localisation of regularised and multiview support vector machine learning}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Gheondea, Aurelian and Tilki, Cankat}, keywords = {operator valued reproducing kernel Hilbert spaces, manifold co-regularised and multiview learning, support vector machine learning, loss functions, representer theorem}, abstract = {We prove some representer theorems for a localised version of a semisupervised, manifold regularised and multiview support vector machine learning problem introduced by H.Q. Minh, L. Bazzani, and V. Murino, Journal of Machine Learning Research, 17(2016) 1-72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional underlying Hilbert spaces are considered. We show that the general framework allows infinite dimensional Hilbert spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that lead to systems of partially nonlinear equations for which some Newton's approximation methods based on the interior point method can be used. Some numerical experiments are performed on a toy model that illustrate the tractability of the methods that we propose.} }
@inproceedings{10.1145/3674029.3674030, title = {Machine Learning Application for Real-Time Simulator}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {1--5}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674030}, url = {https://doi.org/10.1145/3674029.3674030}, author = {Hadadi, Azadeh and Chardonnet, Jean-R\'emy and Guillet, Christophe and Ovtcharova, Jivka}, keywords = {artificial intelligence, auto-adaptive systems, real-time systems, location = Oslo, Norway}, abstract = {This paper presents a groundbreaking research initiative that focuses on the development of an intelligent architecture for Adaptive Virtual Reality Systems (AVRS) in immersive virtual environments. The primary objective of this architecture is to enable real-time artificial intelligence training and adapt the virtual environment based on user states or external parameters. In a case study focused on detecting cybersickness, an undesired side effect in immersive virtual environments, we utilized this architecture to train an artificial intelligence model and personalize it for individual users in a driving simulator application. By leveraging the capabilities of this architecture, we can optimize virtual reality experiences for individual users, leading to increased comfort. We evaluated the system’s performance in terms of memory usage, CPU and GPU usage, temperature monitoring, frame rate, and network performance, and our results demonstrated the efficiency of our proposed architecture.} }
@inproceedings{10.1145/3712255.3716512, title = {Evolutionary Computation meets Machine Learning for Combinatorial Optimisation}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1539--1561}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716512}, url = {https://doi.org/10.1145/3712255.3716512}, author = {Mei, Yi and Raidl, G\"unther} }
@article{10.1145/3652029, title = {Applied Machine Learning for Information Security}, journal = {Digital Threats}, volume = {5}, year = {2024}, doi = {10.1145/3652029}, url = {https://doi.org/10.1145/3652029}, author = {Samtani, Sagar and Raff, Edward and Anderson, Hyrum}, keywords = {Applied machine learning, deep learning, artificial intelligence, information security, cybersecurity}, abstract = {Information security has undoubtedly become a critical aspect of modern cybersecurity practices. Over the past half-decade, numerous academic and industry groups have sought to develop machine learning, deep learning, and other areas of artificial intelligence-enabled analytics into information security practices. The Conference on Applied Machine Learning (CAMLIS) is an emerging venue that seeks to gather researchers and practitioners to discuss applied and fundamental research on machine learning for information security applications. In 2021, CAMLIS partnered with ACM Digital Threats: Research and Practice (DTRAP) to provide opportunities for authors of accepted CAMLIS papers to submit their research for consideration into ACM DTRAP via a Special Issue on Applied Machine Learning for Information Security. This editorial summarizes the results of this Special Issue.} }
@article{10.1145/3586991, title = {Machine Learning Sensors}, journal = {Commun. ACM}, volume = {66}, pages = {25--28}, year = {2023}, issn = {0001-0782}, doi = {10.1145/3586991}, url = {https://doi.org/10.1145/3586991}, author = {Warden, Pete and Stewart, Matthew and Plancher, Brian and Katti, Sachin and Reddi, Vijay Janapa}, abstract = {A design paradigm for the future of intelligent sensors.} }
@inproceedings{10.1145/3712255.3734234, title = {Evidential Fuzzy Rule-Based Machine Learning to Quantify Classification Uncertainty}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {69--70}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734234}, url = {https://doi.org/10.1145/3712255.3734234}, author = {Shiraishi, Hiroki and Ishibuchi, Hisao and Nakata, Masaya}, keywords = {learning fuzzy-classifier systems, evidential reasoning, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Learning Fuzzy-Classifier Systems (LFCSs), also known as evolutionary fuzzy rule-based machine learning, combine evolutionary algorithms with fuzzy rules to create interpretable models. However, traditional inference schemes lack mechanisms to quantify uncertainty in predictions. This paper introduces a novel class inference scheme based on the Dempster-Shafer Theory of Evidence that explicitly models epistemic uncertainty through an "I don't know" state. By calculating belief masses for each class hypothesis and uncertainty, our approach enhances robustness in real-world applications. Experiments on real-world datasets demonstrate statistically significant improvements in classification performance compared to conventional approaches. Our method forms smoother decision boundaries and provides quantitative measures of prediction confidence, addressing a critical gap in reliable decision-making for fuzzy rule-based machine learning.This paper summarizes the ACM TELO article: Hiroki Shiraishi, Hisao Ishibuchi, and Masaya Nakata. 2025. A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems. ACM Transactions on Evolutionary Learning and Optimization. https://doi.org/10.1145/3717613 [7]. Our implementation is available at https://github.com/YNU-NakataLab/jUCS.} }
@inproceedings{10.1145/3731569.3764818, title = {LithOS: An Operating System for Efficient Machine Learning on GPUs}, booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles}, pages = {1--17}, year = {2025}, isbn = {9798400718700}, doi = {10.1145/3731569.3764818}, url = {https://doi.org/10.1145/3731569.3764818}, author = {Coppock, Patrick H. and Zhang, Brian and Solomon, Eliot H. and Kypriotis, Vasilis and Yang, Leon and Sharma, Bikash and Schatzberg, Dan and Mowry, Todd C. and Skarlatos, Dimitrios}, abstract = {The rapid growth of machine learning (ML) has made GPUs indispensable in datacenters and underscores the urgency of improving their efficiency. However, balancing diverse model demands with high utilization remains a fundamental challenge. Transparent, fine-grained GPU resource management that maximizes utilization, energy efficiency, and isolation requires an OS approach. This paper introduces LithOS, a first step towards a GPU OS.LithOS includes the following new abstractions and mechanisms for efficient GPU management: (i) a novel TPC Scheduler that supports spatial scheduling at the granularity of individual TPCs, unlocking efficient TPC stealing between workloads; (ii) a transparent kernel atomizer to reduce head-of-line blocking and allow dynamic resource reallocation mid-execution; (iii) a lightweight hardware right-sizing mechanism that dynamically determines the minimal TPC resources needed per atom; and (iv) a transparent power management mechanism that reduces power consumption based upon in-flight work characteristics.We build LithOS in Rust and evaluate its performance across a broad set of deep learning environments, comparing it to state-of-the-art solutions from NVIDIA and prior research. For inference stacking, LithOS reduces tail latencies by 13 compared to MPS; compared to the best-performing SotA, it reduces tail latencies by 4 while improving aggregate goodput by 1.3. Furthermore, in hybrid inference-training stacking, LithOS reduces tail latencies by 4.7 compared to MPS; compared to the best-performing SotA, it reduces tail latencies by 1.18 while improving aggregate throughput by 1.35. Finally, for a modest performance hit under 4\%, LithOS's hardware right-sizing provides a quarter of GPU capacity savings on average, while for a 7\% hit, LithOS's transparent power management delivers a quarter of GPU total energy savings on average. Overall, LithOS transparently increases GPU efficiency, establishing a foundation for future OS research on GPUs.} }
@article{10.1145/3765735, title = {Applications and Challenges of Fairness APIs in Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3765735}, url = {https://doi.org/10.1145/3765735}, author = {Das, Ajoy and Uddin, Gias and Chowdhury, Shaiful and Akhond, Mostafijur Rahman and Hemmati, Hadi}, keywords = {bias, api, github, fairness}, abstract = {Machine Learning software systems are frequently used in our day-to-day lives. Some of these systems are used in various sensitive environments to make life-changing decisions. Therefore, it is crucial to ensure that these AI/ML systems do not make any discriminatory decisions for any specific groups or populations. In that vein, different bias detection and mitigation open-source software libraries (aka API libraries) are being developed and used. In this paper, we conduct a qualitative study to understand in what scenarios these open-source fairness APIs are used in the wild, how they are used, and what challenges the developers of these APIs face while developing and adopting these libraries. We have analyzed 204 GitHub repositories (from a list of 1885 candidate repositories) which used 13 APIs that are developed to address bias in ML software. We found that these APIs are used for two primary purposes (i.e., learning and solving real-world problems), targeting 17 unique use-cases. Our study suggests that developers are not well-versed in bias detection and mitigation; they face lots of troubleshooting issues, and frequently ask for opinions and resources. Our findings can be instrumental for future bias-related software engineering research, and for guiding educators in developing more state-of-the-art curricula.} }
@inproceedings{10.1145/3643796.3648455, title = {JetTrain: IDE-Native Machine Learning Experiments}, booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments}, pages = {59--61}, year = {2024}, isbn = {9798400705809}, doi = {10.1145/3643796.3648455}, url = {https://doi.org/10.1145/3643796.3648455}, author = {Trofimov, Artem and Kostyukov, Mikhail and Ugdyzhekov, Sergei and Ponomareva, Natalia and Naumov, Igor and Melekhovets, Maksim}, keywords = {integrated development environment, machine learning, MLOps, location = Lisbon, Portugal}, abstract = {Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.} }
@article{10.1145/3688841, title = {An Exploratory Study on Machine Learning Model Management}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3688841}, url = {https://doi.org/10.1145/3688841}, author = {Latendresse, Jasmine and Abedu, Samuel and Abdellatif, Ahmad and Shihab, Emad}, keywords = {Software engineering, machine learning, model management}, abstract = {Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9\% of the identified activities belong to the maintenance category, with activities like refactoring (20.5\%) and documentation (18.3\%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3\%) and bug management (14.9\%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift toward automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models.} }
@inproceedings{10.1145/3696687.3696700, title = {Precise Issuance of Meituan Merchants’ Coupons with Machine Learning}, booktitle = {Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, pages = {71--75}, year = {2024}, isbn = {9798400709876}, doi = {10.1145/3696687.3696700}, url = {https://doi.org/10.1145/3696687.3696700}, author = {Zhang, Xue and Qiu, Jie and Li, Bo}, abstract = {With the popularity of mobile Internet, the “Online-to-Offline” (O2O) business model has become popular. Issuing coupons to attract new customer registrations and keep old customers active is an important marketing tool for O2O companies. But the random distribution of coupons can be annoying to those non-target customers. For merchants, the transition of issuing coupons to merchants will not only increase the promotion cost but also have a negative effect on their brand reputation. The purpose of this study is to analyze transaction data and build a model to predict the redemption of coupons, so as to achieve the precise issue of coupons by merchants. We use machine learning to analyze the consumption data and extract features from five categories: coupons, merchants, consumers, consumers-merchants, and other categories. A total of 44 features are extracted and the XGBoost (eXtreme Gradient Boosting) model is adopted. It has been verified that the prediction results of the application of the XGBoost model can nearly increase 50\% net profits of the merchants.} }
@inproceedings{10.1145/3716368.3735221, title = {A Machine Learning-Assisted Placement Flow with Pin Accessibility Awareness}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {221--226}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735221}, url = {https://doi.org/10.1145/3716368.3735221}, author = {Hsieh, Min-Feng and Wang, Ting-Chi}, keywords = {Placement, Pin Accessibility, Machine Learning}, abstract = {Pin accessibility is a critical concern in placement, as poor accessibility can lead to routing difficulties and design rule violations (DRVs). This paper proposes a pin accessibility-aware placement flow that uses dynamic cell margin generation and nonuniform blockage to improve pin accessibility and routing performance. Additionally, we develop a machine learning-based parameter tuner to optimize the tuning process. Our approach significantly improves post-routing results, enhancing worst negative slack by 72.12\%, total negative slack by 73.06\%, reducing wirelength by 2.48\%, eliminating DRVs by 100\%, and reducing power consumption by 1.31\% and execution time of placement, clock tree synthesis, and routing by 15.97\% compared with a default commercial placement flow.} }
@inproceedings{10.1145/3658644.3690194, title = {Evaluations of Machine Learning Privacy Defenses are Misleading}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {1271--1284}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690194}, url = {https://doi.org/10.1145/3658644.3690194}, author = {Aerni, Michael and Zhang, Jie and Tram\`er, Florian}, keywords = {DP-SGD, audit, machine learning, membership inference, privacy, location = Salt Lake City, UT, USA}, abstract = {Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).} }
@inproceedings{10.1145/3659677.3659701, title = {Crop Recommendation using Machine Learning Algorithms}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659701}, url = {https://doi.org/10.1145/3659677.3659701}, author = {El Barrak, Khaoula and Lakhal, Said and Abdoun, Othman}, keywords = {Agriculture, Crop Recommendation System, Machine Learning, location = Meknes, AA, Morocco}, abstract = {Agricultural productivity is an essential factor in ensuring food security and meeting the growing demands of the world’s population. However, farmers face many challenges such as limited access to information, which can significantly impact their crop yield. A common problem among farmers is not choosing the right crop based on their soil’s needs. This article investigates the efficacy of various machine learning (ML) algorithms in developing a crop recommendation system. The study focuses on algorithms including Decision Trees, Random Forest, Support Vector Machine, K-Nearest Neighbors, Naive Bayes, LightGBM, and Logistic Regression, analyzing their performance metrics such as Accuracy, Precision, Recall, and F1 Score. The research utilizes a dataset encompassing soil and environmental parameters crucial for crop selection. Through rigorous evaluation, Random Forest, Naive Bayes, and LightGBM emerge as the top-performing algorithms, exhibiting high Accuracy and F1 Score for plant recommendation. These findings provide valuable insights for developing robust crop recommendation systems, aiding farmers in optimizing yields and mitigating risks in agricultural practices.} }
@inproceedings{10.1145/3711542.3711573, title = {Constructing Depression Prediction Model using Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {413--418}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711573}, url = {https://doi.org/10.1145/3711542.3711573}, author = {Nuipian, Vatinee and Hanumas, Sorawit and Plangklang, Kannika}, keywords = {Decision Trees, Depression prediction, Machine learning, Natural language, Recommendation system}, abstract = {Depression is a prevalent mental health disorder with significant impacts on individuals and society. Given the increasing presence of depressive expressions on social media, this study constructs a predictive model to classify depression-related content using machine learning algorithms. Six algorithms for comparing a model, such as Decision Trees, Random Forest, K-nearest neighbors, Support Vector Machines (SVM), Logistic Regression, and Convolutional Neural Networks (CNNs), are used to identify the most effective model for accurately predicting depressive posts. The study utilized a dataset from Twitter, balancing depressive and non-depressive texts for unbiased training. Text preprocessing involved tokenization, stop word removal, and feature extraction using TF-IDF. Evaluation metrics such as accuracy, precision, recall, and F1-score were applied to assess performance. The results showed that Decision Trees and Logistic Regression performed best, with accuracy exceeding 99\%. These models were implemented in a prototype for real-time depression risk prediction. Future work aims to expand the model's applicability to Thai language texts, enhancing support for mental health care} }
@inproceedings{10.1145/3745812.3745842, title = {Predictive Analytics in Diabetes Care: A Machine Learning Approach}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745842}, url = {https://doi.org/10.1145/3745812.3745842}, author = {Bisht, Smita and Saxena, Vivek and Goyal, Dinesh and Sharma, Himanshi}, keywords = {Decision Tree, Diabetes Prediction, Healthcare Analytics, Logistic Regression, Machine Learning, Python, Random Forest, Support Vector Machine}, abstract = {Diabetes is not only a hassle; it is one of the fast-growing ailments worldwide. Early detection and treatment are therefore, highly important in order to save lives and avoid complications later on. In the course of this study, we mentioned the potentiality of machine learning (ML) toward the prediction of diabetes. Four different ML techniques, namely Logistic Regression, Decision Trees, Random Forest, and Support Vector Machine (SVM) were analysed while coding in Python. The idea was to find out, with the help of the Pima Indians Diabetes data collection, if diabetes was predictable.Predictive modelling appears to be very powerful in medicine, as this study demonstrates. Future improvement in predicting diabetes may come from reasonably large datasets and perhaps from newer machine learning algorithms. Starting with a cleaning of the data, we made comparisons of all models based on accuracy, precision, recall, and F1-score. Random Forest was the winner! Highest prediction accuracy, but really all models did well, especially this one.} }
@article{10.1613/jair.1.15665, title = {The Human in Interactive Machine Learning: Analysis and Perspectives for Ambient Intelligence}, journal = {J. Artif. Int. Res.}, volume = {81}, year = {2025}, issn = {1076-9757}, doi = {10.1613/jair.1.15665}, url = {https://doi.org/10.1613/jair.1.15665}, author = {Delcourt, Kevin and Trouilhet, Sylvie and Arcangeli, Jean-Paul and Adreit, Francoise}, abstract = {As the vision of Ambient Intelligence (AmI) becomes more feasible, the challenge of designing effective and usable human-machine interaction in this context becomes increasingly important. Interactive Machine Learning (IML) offers a set of techniques and tools to involve end-users in the machine learning process, making it possible to build more trustworthy and adaptable ambient systems. In this paper, our focus is on exploring approaches to effectively integrate and assist human users within ML-based AmI systems. Through a survey of key IML-related contributions, we identify principles for designing effective human-AI interaction in AmI applications. We apply them to the case of Opportunistic Composition, which is an approach to achieve AmI, to enhance collaboration between humans and Artificial Intelligence. Our study highlights the need for user-centered and context-aware design, and provides insights into the challenges and opportunities of integrating IML techniques into AmI systems.} }
@inproceedings{10.1145/3757110.3757159, title = {Construction of Student Safety Management System based on Big Data and Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {283--286}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757159}, url = {https://doi.org/10.1145/3757110.3757159}, author = {Tang, Jiabing}, keywords = {Big Data, Machine Learning, Security Management, System Building}, abstract = {With the development of science and technology, more and more industries begin to involve big data. This paper aims to explore and build a student safety management model based on big data and machine learning. On the basis of summarizing the basic theories, research processes and basic paradigms involved in the previous studies of "data mining and knowledge discovery" and "information extraction and information analysis", a data-driven PMDA-Diki public security intelligence analysis and extraction system model is independently proposed. This model describes the basic evolution path of data processing flow and data existence form in the process of transforming multi-source data into security intelligence, and also defines the operation steps of extracting public security intelligence based on data, namely: By means of "processing → data mining → knowledge discovery → algorithm activation", the data can be gradually extracted into the required public security information according to the order of "data → information → knowledge → intelligence". Big data technology, in short, is the ability to extract valuable information quickly from a wide variety of types of data. Combining with rich contour feature visible light image, using the scale invariant feature transform (SIFT) algorithm for image registration fusion processing, image fusion is both advantages, through the deconvolution of fusion image feature extraction of feature extraction algorithm is exclusive feature mapping matrix, different from traditional random initialization convolution neural network learning method of convolution kernels, An improved neural network identification model, through the characteristics of the migration study will get mapping matrix as the initial convolution convolution neural network model of nuclear matrix, using the improved network model testing of the visible light and infrared images of repeated iterative learning, training a good model can improve the students' safety equipment management system identification accuracy, So as to realize the intelligent identification of equipment. Through the above research, it can realize the intelligent identification and fault warning of safety management equipment, and transform the traditional manual monitoring mode into intelligent monitoring. The rationality and effectiveness of the proposed method are verified by an example analysis of the measured data.} }
@inproceedings{10.1145/3728725.3728770, title = {Optimization of Price Strategy by Machine Learning in E-commerce}, booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security}, pages = {284--288}, year = {2025}, isbn = {9798400713453}, doi = {10.1145/3728725.3728770}, url = {https://doi.org/10.1145/3728725.3728770}, author = {Liu, Quan and Song, Yunkui}, keywords = {E-commerce, Machine learning, Pinduoduo, Price strategy, Taobao}, abstract = {Machine learning algorithms can monitor market dynamics, user behavior and competitor prices in real time, and dynamically adjust commodity prices based on this information to maximize profits. By analyzing historical sales data, market price data, user behavior data, etc., machine learning models can predict the demand and price sensitivity of goods to develop optimal pricing strategies. This study takes Pinduoduo and Taobao as examples to analyze the application of machine learning in E-commerce price strategy optimization, in order to provide useful reference for the E-commerce platforms.} }
@inbook{10.1145/3760023.3760041, title = {Machine Learning-Driven Social Network Analysis of Logistic Standardization Development}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {100--107}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760041}, author = {Wang, Qi and Jin, Zongzhen}, abstract = {Logistics has been developing for nearly a hundred years. Over time, the industry is booming and covering more aspects. In the process of development, many countries also began to summarize the experience of logistics, formulate standards, and restrain and regulate the behavior of enterprises. The formulation of international standards can play a coordinating role, collect information, make logistics develop in a better direction, and play a more important role in the world. This paper applied the social network analysis method and finds that the standards of each country are difficult to integrate as international standards, and the construction of the standard system has been obstructed. This paper will give three possible solutions to overcome those defects. Additionally, this study integrates machine learning algorithms such as Random Forest and Support Vector Machines (SVM) to analyze the correlation between national logistics standards and their adoption in international frameworks. These algorithms enhance the predictive accuracy of standardization barriers and provide data-driven insights for future policy formulation.} }
@inbook{10.1145/3672608.3707742, title = {ASML-REG: Automated Machine Learning for Data Stream Regression}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {440--447}, year = {2025}, isbn = {9798400706295}, url = {https://doi.org/10.1145/3672608.3707742}, author = {Verma, Nilesh and Bifet, Albert and Pfahringer, Bernhard and Bahri, Maroua}, abstract = {Online learning scenarios present a significant challenge for AutoML techniques due to the dynamic nature of data distributions, where the optimal model and configuration may change over time. While most research in machine learning for data streams has primarily focused on classification algorithms, regression methods have received significantly less attention. To address this gap, we propose ASML-REG, an Automated Streaming Machine Learning framework designed specifically for regression tasks on data streams. ASML-REG continuously explores a vast and diverse space of pipeline configurations, adapting to evolving data by focusing on the current best design, performing adaptive random searches in promising areas, and maintaining an ensemble of top-performing pipelines. Our experiments with real and synthetic datasets demonstrate that ASML-REG significantly outperforms current state-of-the-art data stream regression algorithms.} }
@inproceedings{10.1145/3736251.3747313, title = {Building Trust in AI-Powered Assessment Through Explainable Machine Learning Models}, booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 2}, pages = {403--404}, year = {2025}, isbn = {9798400719424}, doi = {10.1145/3736251.3747313}, url = {https://doi.org/10.1145/3736251.3747313}, author = {Ayanwale, Musa Adekunle}, keywords = {ai in higher education, educational assessment, explainable artificial intelligence, learning management system, student engagement, location = Gaborone, Botswana}, abstract = {The increasing integration of artificial intelligence (AI)-based assessment systems in higher education offers efficiency and scalability in grading and feedback but raises concerns about transparency, fairness, and student trust. This study investigates the role of Explainable AI (XAI) in improving trust and engagement within AI-driven assessment environments, focusing on the Thuto Learning Management System (LMS) at the National University of Lesotho (NUL). A cross-sectional survey of 850 undergraduate students, who had engaged with Thuto LMS for at least two semesters, was conducted using a 55-item questionnaire measuring 11 constructs, including digital self-efficacy, AI literacy, ethics awareness, and feedback perceptions. Assessment outcomes were retrieved from LMS records and binarised into pass/fail classifications. Three supervised machine learning models-Logistic Regression, Random Forest, and K-Nearest Neighbours-were developed to predict assessment outcomes, and post hoc interpretability was achieved using SHAP and DALEX frameworks. Logistic Regression demonstrated the highest predictive accuracy (73.7\%), while feature importance analyses revealed that feedback usefulness, discussion engagement, and digital self-efficacy were the strongest predictors of academic success. Findings underscore the potential of XAI for promoting fairness, transparency, and learner trust in AI-powered educational systems, particularly in underexplored African higher education contexts.} }
@inproceedings{10.1145/3674029.3674032, title = {An Analysis of Car Price Prediction using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {11--15}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674032}, url = {https://doi.org/10.1145/3674029.3674032}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco and Gautam, Shivansh}, abstract = {This research paper explores machine learning techniques, such as voting regressors, gradient boosting regressors, random forest regressors, decision tree regressors, and support vector regressors, for car predicting the car price. Each machine learning technique has its own unique advantages and disadvantages, with the voting regressor exhibiting the best results. Methodologically, GridSearchCV is used to tune hyperparameters on a dataset of more than 200 automobiles, each with 26 parameters. The outcomes demonstrate the predictive power of regression and ensemble techniques, providing insightful information to practitioners in the business and academics alike. The training accuracies range from 16.87\% (MAPE) for Linear Regression, 96.78\% for Decision Tree Regressor, 96.49\% for Random Forest Regressor, 97.84\% for Gradient Boosting Regressor,95.8\% for Voting Regressor, 81.89\% for Support Vector Regressor, notably the testing accuracies vary from 19.44\% (MAPE) for Linear Regression, 87.76\% for Decision Tree Regressor, 89.75\% for Random Forest Regressor, 88.67\% for Gradient Boosting Regressor, 88.02\% for Voting Regressor, 79.55\% for Support Vector Regressor.} }
@inproceedings{10.1145/3690771.3690781, title = {Anomaly and Pattern Detection Using Advanced Machine Learning Models}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {20--26}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690781}, url = {https://doi.org/10.1145/3690771.3690781}, author = {Nacchanandana, Piyavachara and Suleiman, Basem and Anaissi, Ali and Picones, Gio}, keywords = {Generative Adversarial Network, Federated Learning, Federated Averaging, FedProx, Federated Personalisation, Datasets, Neutral Networks, Gaze Detection, Text Tagging, Anomaly Detection, Federated Median}, abstract = {This research addresses a unique intersection in the field of machine learning: the convergence of anomaly detection and decentralized learning. Although each domain has been extensively studied in isolation, their combination remains relatively unexplored. This study pioneers in investigating the practicality and efficiency of decentralized anomaly detection. We achieve this by integrating the GANomaly architecture with Federated Learning, a novel approach that allows us to analyze the impact of various factors on system performance. The core of our investigation revolves around four aggregation methods: Federated Averaging, FedProx, Federated Personalization, and the newly introduced Federated Median. We meticulously evaluate the efficacy of these methods within a combined GANomaly and Federated Learning framework, focusing on their performance across different levels of local computation and varying proportions of straggler clients per round. This research not only fills a gap in the existing literature, but also offers new insights into the optimization of decentralized learning systems for effective anomaly detection.} }
@inbook{10.1145/3729706.3729724, title = {Improved Data Flow Matching in SDN Using Machine Learning}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {120--124}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729724}, author = {Wang, Shuo and Liu, Jiaxin and Yang, Man}, abstract = {Data flow subsequence matching is crucial in Software-Defined Networking (SDN) due to the growing demand for efficient data handling. Traditional methods face limitations in accurately and rapidly matching data subsequences, especially under the complexities introduced by non-linear patterns. This paper proposes a machine learning-based algorithm designed for high-accuracy data flow subsequence matching in SDN. The algorithm utilizes feature extraction and transformation for each subsequence and applies similarity measurement techniques to improve matching precision. Furthermore, it enhances a neural network structure by converting a simple neural network into a modular network, improving convergence and performance. Experimental analysis demonstrates that the proposed algorithm achieves shorter feature transformation times, high accuracy in similarity measurement, and improved matching rates across diverse datasets, surpassing traditional methods. This solution provides a reliable approach to optimizing data flow management in SDN.} }
@article{10.1145/3616865, title = {Fairness in Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3616865}, url = {https://doi.org/10.1145/3616865}, author = {Caton, Simon and Haas, Christian}, keywords = {Fairness, accountability, transparency, machine learning}, abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.} }
@article{10.1145/3757484, title = {Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3757484}, url = {https://doi.org/10.1145/3757484}, author = {Ng, Lynnette Hui Xian and Jaidka, Kokil and Tay, Kai Yuan and Chhaya, Niyati}, keywords = {crowdsourcing, diplomacy, discourse, machine learning, natural language processing, negotiation}, abstract = {Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14\% on held-out data and 12\% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction.} }
@inproceedings{10.1145/3746709.3746911, title = {Machine learning-based electronic confrontation system performance evaluation and optimization}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {1184--1188}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746911}, url = {https://doi.org/10.1145/3746709.3746911}, author = {Yuan, Ye and Zhou, Xin and Chen, Qingte}, keywords = {Electronic confrontation, Machine learning, Parameter optimization, Performance evaluation}, abstract = {Electronic confrontation is a key technology in modern warfare, and its complexity and uncertainty brought challenges to system performance evaluation and optimization. This article studies the performance evaluation and optimization method of electronic confrontation system based on machine learning. In terms of performance evaluation, a data-driven assessment framework is proposed. The construction of electronic confrontation efficiency is realized through feature engineering and machine learning model construction. Experimental results show, This method can accurately evaluate the combat effectiveness of electronic confrontation equipment and strategies in complex electromagnetic environments, where the performance of neural network models is the best. In terms of performance optimization, the electronic confrontation system parameter optimization method based on Bayesian optimization and electronic confrontation strategy optimization model based on deep reinforcement learning. The study of this article enriches the theoretical and methods of machine learning in the field of electronic confrontation, and provides new ideas for the intelligent development of the electronic confrontation system.} }
@inproceedings{10.1145/3671127.3699676, title = {A Machine Learning Approach to Benchmark Thermal Comfort}, booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {363--368}, year = {2024}, isbn = {9798400707063}, doi = {10.1145/3671127.3699676}, url = {https://doi.org/10.1145/3671127.3699676}, author = {Jacob, Jeslu Celine and Pandit, Debapratim and Sen, Joy}, keywords = {Machine learning, Personal comfort, Skin temperature, Thermal comfort, location = Hangzhou, China}, abstract = {Thermal comfort models are used as benchmarks in air conditioning design to decide on temperature setpoints. Most existing thermal comfort models are designed for representative occupants considering static environmental conditions. However, inter-personal and intra-personal factors influence thermal preference of occupants. Hence assuming standard thermal comfort conditions lead to uncomfortable thermal environments. With advancement in sensing technology and computation resources there exists an opportunity to model personal thermal preferences of occupants using physiological data in addition to environmental data. This study evaluates the use of machine learning models to predict personal thermal comfort with high accuracy. The results show that classification models such as decision trees, random forest, k-nearest neighbor, neural networks and adaboost can be used to model personal thermal comfort with accuracies higher than 70\%. Random forest models perform the best with accuracies higher than 75\%.} }
@inproceedings{10.1145/3679240.3734628, title = {Machine Learning for Building-Level Heat Risk Mapping}, booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems}, pages = {341--346}, year = {2025}, isbn = {9798400711251}, doi = {10.1145/3679240.3734628}, url = {https://doi.org/10.1145/3679240.3734628}, author = {Domiter, Andrea and Keshav, Srinivasan}, keywords = {Heatwaves, Thermal Vulnerability, Urban Occupants, ML}, abstract = {Climate change is intensifying the frequency and severity of heat waves, increasing risks to public health and energy systems worldwide. Although building simulations can reveal the types of buildings whose occupants are most at risk, they rarely pinpoint the locations of these structures. We present a data-driven workflow that labels real-world buildings from satellite imagery and building characteristics, estimates thermal performance under heatwaves, and visualizes risks on a detailed map. Through a case study of 10 UK cities, we show that our approach can be used to identify areas of concentrated heat risk, providing valuable insights for guiding public health strategies and informing future research into grid reliability and thermal equity.} }
@inbook{10.1145/3658617.3703144, title = {Leveraging Machine Learning Techniques for Traditional EDA Workflow Enhancement}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {676--682}, year = {2025}, isbn = {9798400706356}, url = {https://doi.org/10.1145/3658617.3703144}, author = {Cho, Jinoh and Im, Jaekyung and Lee, Jaeseung and Min, Kyungjun and Park, Seonghyeon and Seo, Jaemin and Yoon, Jongho and Kang, Seokhyeong}, abstract = {As technology nodes advance and feature sizes shrink, the increasing complexity of design rules and routing congestion has resulted in greater design challenges and rising costs. Machine learning (ML) models offer significant potential to enhance design quality by enabling early prediction and optimization during the design flow. However, only a few works have validated the effectiveness of ML model when integrated to the traditional design flow. This paper will cover the effectiveness of ML-enhanced design workflow with some practical applications. Additionally, we will address which problems should be solved to achieve successful ML integration.} }
@article{10.1145/3732786, title = {Graph Machine Learning in the Era of Large Language Models (LLMs)}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {16}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3732786}, url = {https://doi.org/10.1145/3732786}, author = {Wang, Shijie and Huang, Jiani and Chen, Zhikai and Song, Yu and Tang, Wenzhuo and Mao, Haitao and Fan, Wenqi and Liu, Hui and Liu, Xiaorui and Yin, Dawei and Li, Qing}, keywords = {Graph Machine Learning, Large Language Models (LLMs), Pre-training and Fine-tuning, Prompting, and Representation Learning}, abstract = {Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graphs. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications, such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML’s generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations, such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph Heterophily and Out-of-Distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.} }
@inproceedings{10.1145/3736731.3746153, title = {Learning from Irreproducibility: Introducing Data Leakage Case Studies for Machine Learning Education}, booktitle = {Proceedings of the 3rd ACM Conference on Reproducibility and Replicability}, pages = {224--228}, year = {2025}, isbn = {9798400719585}, doi = {10.1145/3736731.3746153}, url = {https://doi.org/10.1145/3736731.3746153}, author = {Fund, Fraida and Saeed, Mohamed and Malik, Shaivi and Ishak, Kyrillos}, keywords = {reproducibility, education, machine learning, data leakage}, abstract = {Data leakage remains a pervasive issue in machine learning (ML), especially when applied to science, leading to overly optimistic performance estimates and irreproducible findings. Despite its prevalence, data leakage receives limited attention in ML education, in part due to the lack of accessible, hands-on teaching resources. To address this gap, we developed interactive learning modules in which students reproduce examples from academic publications that are affected by data leakage, then repeat the evaluation without the data leakage error to see how the finding is affected. These modules were deployed by the authors in two introductory machine learning courses, enabling students to explore common forms of leakage and their impact on model reliability. Following their engagement with these materials, student feedback highlighted increased awareness of subtle pitfalls that can compromise machine learning workflows.} }
@inproceedings{10.1145/3678698.3678702, title = {Financial Big data Visualization: A Machine Learning Perspective}, booktitle = {Proceedings of the 17th International Symposium on Visual Information Communication and Interaction}, year = {2024}, isbn = {9798400709678}, doi = {10.1145/3678698.3678702}, url = {https://doi.org/10.1145/3678698.3678702}, author = {Dong, Xiaodan and Huang, Weidong and Wang, Jitong}, keywords = {Machine Learning, Visualization, Telematics, Big Data}, abstract = {In today’s technology-driven environment, the exponential growth of big data underscores the importance of visualizing and analyzing it to derive actionable insights. This need spans across industrial sectors, with particular importance in the financial industry. While numerous modern models and algorithms have been developed, and utilized in diverse applications, it is crucial to classify these methodologies for users to identify the most suitable ones. In this paper, we embark on a selective review to streamline the classification of financial big data visualization methodologies from a machine learning perspective and explore the latest trends. We categorize techniques based on two key elements: the modeling stage and the nature of big data. The analytical stage divides methods into three phases: pre-model building, during-model building, and post-model building. Additionally, the characteristics of big data play an important role in shaping methodologies. We delve into three primary types of big data—structured, semi-structured, and unstructured and identify the most popular financial data types within each category in current society. We also discuss and highlight some research opportunities that we hope could be useful for visual analytics researchers.} }
@inproceedings{10.1145/3728424.3760766, title = {CAVIR Cognitive Assessment in VR: An Eye-tracking and Machine Learning Approach}, booktitle = {Proceedings of the 2nd International Workshop on Multimedia Computing for Health and Medicine}, pages = {13--19}, year = {2025}, isbn = {9798400718366}, doi = {10.1145/3728424.3760766}, url = {https://doi.org/10.1145/3728424.3760766}, author = {Vulpe-Grigorasi, Adrian and Slijepcevi\'c, Djordje and Sch\"offer, Lucas and Wanner, Sophia and Jespersen, Andreas E. and Miskowiak, Kamilla W. and Leung, Vanessa}, keywords = {eye tracking, machine learning, digital biomarkers, cognitive assessment, virtual reality, cavir, location = Ireland}, abstract = {Assessment of cognitive function in virtual reality (VR) is an emerging area of research that offers immersive and ecologically valid environments to evaluate cognitive skills in more realistic and engaging contexts. In this study, we extend this paradigm by integrating eye-tracking data to analyze gaze patterns during five distinct cognitive tasks within the Cognitive Assessment in Virtual Reality (CAVIR) environment. By extracting and modeling features from raw eye-tracking signals, we demonstrate that these tasks can be effectively distinguished based on gaze behavior. Our machine learning experiments show that a classifier trained to differentiate the five tasks achieved an accuracy of 70\%, while a model trained to classify the broader cognitive categories of verbal memory, processing speed, and attention achieved an accuracy of 89\%. These results indicate that eye tracking provides valuable behavioral digital biomarkers for cognitive function differentiation in VR-based assessments. This approach highlights the potential for developing non-invasive, gaze-driven cognitive profiling tools in virtual environments, paving the way for more personalized and adaptive neurocognitive diagnostics.} }
@article{10.1145/3730576, title = {A Comprehensive Survey on Machine Learning Driven Material Defect Detection}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3730576}, url = {https://doi.org/10.1145/3730576}, author = {Bai, Jun and Wu, Di and Shelley, Tristan and Schubel, Peter and Twine, David and Russell, John and Zeng, Xuesen and Zhang, Ji}, keywords = {Material defect detection, composites manufacturing, machine learning, deep learning, computer vision, machine vision}, abstract = {Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.} }
@inproceedings{10.1145/3732365.3732410, title = {A Survey of Machine Learning Approaches for Malware Detec-tion}, booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering}, pages = {269--273}, year = {2025}, isbn = {9798400713613}, doi = {10.1145/3732365.3732410}, url = {https://doi.org/10.1145/3732365.3732410}, author = {Wu, Yibiao and Zhuang, Honglin and Jia, Yetao and Zhang, Yuting}, keywords = {Deep Learning, Detection technology, LLM, Machine Learning, Malware}, abstract = {At present, due to the rapid evolution of malicious code and the rapid progress of science and technology, the security of information system has been put forward higher requirements. This research systematically reviews the current detection technologies for malicious code, from classical machine learning algorithms to some existing mature algorithms, and introduces machine learning, deep learning, and other technologies on this basis. In addition, malicious code detection technology based on large-scale language modeling (LLMS) is also studied. This study focuses on the advantages, limitations, and problems in practical applications of various detection techniques, and compares traditional methods, machine learning, and deep learning methods, and looks forward to future work.} }
@article{10.1145/3716376, title = {Cost-effective Missing Value Imputation for Data-effective Machine Learning}, journal = {ACM Trans. Database Syst.}, volume = {50}, year = {2025}, issn = {0362-5915}, doi = {10.1145/3716376}, url = {https://doi.org/10.1145/3716376}, author = {Chai, Chengliang and Jin, Kaisen and Tang, Nan and Fan, Ju and Miao, Dongjing and Wang, Jiayi and Luo, Yuyu and Li, Guoliang and Yuan, Ye and Wang, Guoren}, keywords = {Data-centric AI, machine learning, data cleaning, coreset selection}, abstract = {Given a dataset with incomplete data (e.g., missing values), training a machine learning model over the incomplete data requires two steps. First, it requires a data-effective step that cleans the data in order to improve the data quality (and the model quality on the cleaned data). Second, it requires a data-efficient step that selects a core subset of the data (called coreset) such that the trained models on the entire data and the coreset have similar model quality, in order to save the computational cost of training. The first-data-effective-then-data-efficient methods are too costly, because they are expensive to clean the whole data; while the first-data-efficient-then-data-effective methods have low model quality, because they cannot select high-quality coreset for incomplete data.In this article, we investigate the problem of coreset selection over incomplete data for data-effective and data-efficient machine learning. The essential challenge is how to model the incomplete data for selecting high-quality coreset. To this end, we propose the GoodCore framework towards selecting a good coreset over incomplete data with low cost. To model the unknown complete data, we utilize the combinations of possible repairs as possible worlds of the incomplete data. Based on possible worlds, GoodCore selects an expected optimal coreset through gradient approximation without training ML models. We formally define the expected optimal coreset selection problem, prove its NP-hardness, and propose a greedy algorithm with an approximation ratio. To make GoodCore more efficient, we propose optimization methods that incorporate human-in-the-loop imputation or automatic imputation method into our framework. Moreover, a group-based strategy is utilized to further accelerate the coreset selection with incomplete data given large datasets. Experimental results show the effectiveness and efficiency of our framework with low cost.} }
@inbook{10.1145/3715014.3724051, title = {Machine Learning and Big Data on Raspberry Pi: A Performance Evaluation}, booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems}, pages = {648--649}, year = {2025}, isbn = {9798400714795}, url = {https://doi.org/10.1145/3715014.3724051}, author = {Chen, Tan and Liu, Dawei}, abstract = {In this paper we present a performance evaluation of machine learning and big data technologies in edge computing. Existing research focuses on deep learning methods on high-end edge computing devices. There have been few reports on classical machine learning methods on common edge computing devices. To close this gap, we evaluate support vector machine (SVM) and random forest (RF) on a Raspberry Pi platform. Our evaluation includes method execution time and data storage time. The latter is considered an important component in machine learning execution lifecycle and has not been jointly evaluated in existing research.Some interesting observations include the significant impact of the data storage engine on efficiency, and how RF can extend the performance bottleneck of edge devices. These findings show that, in today's common edge computing devices, reasonable algorithm selection and data optimization strategies are essential to fully exploit the potential of the device.} }
@article{10.1145/3742471, title = {Myoelectric Prosthetic Hands: A Review of Muscle Synergy, Machine Learning and Edge Computing}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3742471}, url = {https://doi.org/10.1145/3742471}, author = {Farag, Hamdy O. and Gaber, Mohamed Medhat and Awad, Mohammed Ibrahim and Elhady, Nancy E.}, keywords = {Myoelectric control, upper limb prostheses, prosthetic assessment, muscle synergies, edge deployment, machine learning, sensor fusion, neural network optimization}, abstract = {Over the past decade, the integration of electromyography (EMG) techniques with machine learning has significantly advanced prosthetic device control. Researchers have developed sophisticated deep learning classifiers for gesture recognition and created EMG controllers capable of simultaneous proportional control across multiple degrees of freedom. However, the increasing complexity of these machine learning models demands greater computational power, creating challenges for real-time deployment on embedded prosthetic controllers. Various optimization techniques - including hyperdimensional computing, pruning, and quantization - have demonstrated effectiveness in reducing computational requirements while preserving system performance. Concurrently, biomedical research has explored muscle and task synergies as methods to simplify inputs for machine learning models. This review examines synergy extraction in upper limb prosthetics research and identifies the need for standardized hardware specifications to facilitate proper validation and comparison of research outcomes. Furthermore, it explores how optimization techniques from Internet of Things (IoT) applications could enhance EMG controllers in biomedical settings. The analysis identifies sensor fusion and high-density EMG as particularly promising approaches for achieving robust, generalized control of upper limb prosthetics.} }
@inproceedings{10.1145/3708657.3708769, title = {Machine Learning vs. Transformer: Indonesia News Classification}, booktitle = {Proceedings of the 2024 10th International Conference on Communication and Information Processing}, pages = {699--704}, year = {2025}, isbn = {9798400717444}, doi = {10.1145/3708657.3708769}, url = {https://doi.org/10.1145/3708657.3708769}, author = {Harditya, Michael and Purnamasari, Prima Dewi}, keywords = {text classification, evaluation, machine learning, BART, BERT, na\"ve bayes classification}, abstract = {The shift from traditional newspapers to digital media in Indonesia has led to challenges in news publication, including the spread of less credible or fake news. This paper addresses the problem by evaluating text classification methods to improve news tagging and eventually improve the publishing process. Specifically, it compares the performance of Na\"ve Bayes and Transformer models (BERT and BART) for multiclass classification of Indonesian news headlines. Using a dataset of 150,466 news articles, the study preprocesses the data, tokenizes it, and trains the classifiers. Results show that Na\"ve Bayes with unigram preprocessing achieves comparable accuracy to BERT and BART, highlighting the potential of simpler models in certain contexts. The findings suggest that both traditional and advanced models can be effective, depending on the dataset and preprocessing methods used.} }
@inproceedings{10.1145/3711896.3737870, title = {Machine Learning on Graphs in the Era of Generative Artificial Intelligence}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6296--6297}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737870}, url = {https://doi.org/10.1145/3711896.3737870}, author = {Wang, Yu and Zhang, Yu and Guo, Zhichun and Shomer, Harry and Han, Haoyu and Derr, Tyler and Ahmed, Nesreen K. and Halappanavar, Mahantesh and Tang, Jiliang}, keywords = {generative artificial intelligence, graph machine learning, location = Toronto ON, Canada}, abstract = {Graphs, which encode pairwise relations between entities, serve as a fundamental data structure across real-world domains. Many critical applications can be formulated as graph-based tasks, and graph machine learning (GML), from the shallow embedding models to graph neural networks and further advanced to the most powerful graph transformers, has been well-established to automate knowledge discovery and decision-making on graphs. In parallel, the recent emergence of large foundational models has driven machine learning into a new era of Generative Artificial Intelligence (Gen-AI), and this revolution presents both unprecedented opportunities and profound challenges for the well-established GML paradigms. However, few investigations have analyzed and envisioned how GML should evolve to harness these opportunities, address these challenges, and embrace this new Gen-AI era. To fill in this gap, we organize the first international Workshop on Machine Learning on Graphs in the Era of Generative Artificial Intelligence (MLoG-GenAI), held in connection with the 31st ACM Conference on Knowledge Discovery and Data Mining, which provides a venue to gather academic researchers and industry practitioners to discuss and picture the development of GML in the new Gen-AI era.} }
@inproceedings{10.1145/3725843.3756092, title = {SkipReduce: (Interconnection) Network Sparsity to Accelerate Distributed Machine Learning}, booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture}, pages = {643--658}, year = {2025}, isbn = {9798400715730}, doi = {10.1145/3725843.3756092}, url = {https://doi.org/10.1145/3725843.3756092}, author = {Kasan, Hans and Abts, Dennis and Choi, Jungwook and Kim, John}, abstract = {The interconnection network is a critical component for building scalable systems, as its communication bandwidth directly impacts the collective communication performance of distributed training. In this work, we exploit interconnection network sparsity (or communication sparsity) to address challenges of communication performance and scalability. In particular, we identify how gradients (or packets) during communication can be randomly skipped with minimal impact on accuracy. However, skipping gradients in fine granularity (or individually) results in a loss of gradient information without improving communication performance, due to the synchronous nature of collective communication. Thus, we propose coarse-grained skipping where gradient slices are skipped, which enables skipping of some AllReduce steps to accelerate communication. In particular, we propose SkipReduce collective communication that intentionally skips random gradients during AllReduce. However, a naive implementation of SkipReduce can degrade accuracy by repeatedly skipping gradients from the same node, which introduces bias. To mitigate this accuracy loss, we show how randomizing the skipped gradient slices improves training accuracy with negligible additional runtime. We also observe that not all layers have similar communication sparsity and propose applying SkipReduce selectively where only the sparse layers (or gradients) are skipped to minimize the accuracy impact of SkipReduce. Compared to prior work on communication acceleration, SkipReduce can be seamlessly integrated into existing collective communication libraries with minimal overhead. We implement SkipReduce on top of NCCL’s ring-based AllReduce algorithm. Our results show that this method accelerates collective communication while preserving final training accuracy. Compared to baseline AllReduce, SkipReduce provides up to a 1.58 speedup in time-to-accuracy. Beyond this performance gain in data parallelism, this work also discusses the broader implications of SkipReduce, including its application to other parallelism strategies and logical topologies, as well as its benefits as a model regularizer.} }
@article{10.1145/3769292, title = {Machine Learning Systems: A Survey from a Data-Oriented Perspective}, journal = {ACM Comput. Surv.}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3769292}, url = {https://doi.org/10.1145/3769292}, author = {Cabrera, Christian and Paleyes, Andrei and Thodoroff, Pierre and Lawrence, Neil}, keywords = {Artificial Intelligence, Machine Learning, Real-World Deployment, Systems Architecture, Data-Oriented Architecture.}, abstract = {Engineers are deploying ML models as parts of real-world systems with the upsurge of AI technologies. Real-world environments challenge the deployment of such systems because these environments produce large amounts of heterogeneous data, and users require increasingly efficient responses. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-Oriented Architecture (DOA) is an emerging style that better equips systems to integrate ML models. Even though papers on deployed ML-based systems do not mention DOA, their authors make design decisions that implicitly follow DOA. Implicit decisions create a knowledge gap, limiting practitioners’ ability to implement ML-based systems. This paper surveys why, how, and to what extent practitioners have adopted DOA to implement ML-based systems. We overcome the knowledge gap by answering these questions and explicitly showing the design decisions and practices behind these systems. The survey follows a well-known systematic and semi-automated methodology for reviewing papers in software engineering. The majority of reviewed works partially adopt DOA. Such an adoption enables systems to address big data management, low-latency processing, resource management, security, and privacy requirements. Based on these findings, we formulate practical advice to facilitate the deployment of ML-based systems.} }
@inproceedings{10.1145/3587135.3592769, title = {Machine Learning Application Benchmark}, booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers}, pages = {229--235}, year = {2023}, isbn = {9798400701405}, doi = {10.1145/3587135.3592769}, url = {https://doi.org/10.1145/3587135.3592769}, author = {Koch, Andreas and Petry, Michael and Ghiglione, Max and Raoofy, Amir and Dax, Gabriel and Furano, Gianluca and Werner, Martin and Trinitis, Carsten and Langer, Martin}, keywords = {FPGA, Machine learning, benchmark, datasets, neural networks, power consumption, location = Bologna, Italy}, abstract = {This paper presents the MLAB project, a research and development activity funded by ESA General Support Technology Programme under the lead of Airbus Defence and Space GmbH, with the goal of developing a machine learning application benchmark for space applications. First, the need for a benchmark dedicated to machine learning applications in spacecraft is explained, and examples of applications are described including their design challenges. Then the benchmark design is presented, including the rules of the metrics, guidelines and scenarios for references. These scenarios include a description of the reference workloads that have been selected during the activity as representative for spacecraft applications. Lastly, the submission concept is introduced.} }
@inproceedings{10.1145/3665939.3665960, title = {Transparent Data Preprocessing for Machine Learning}, booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics}, pages = {1--6}, year = {2024}, isbn = {9798400706936}, doi = {10.1145/3665939.3665960}, url = {https://doi.org/10.1145/3665939.3665960}, author = {Strasser, Sebastian and Klettke, Meike}, keywords = {data preprocessing, data profiles, change profiles, transparency, location = Santiago, AA, Chile}, abstract = {Data preprocessing is an important task in machine learning which can significantly improve model outcomes. However, evaluating the impact of data preprocessing is often difficult. There is a need for tools which make it transparent to the user on how certain transformations conducted in preprocessing affect the data. Thus, we propose a vision of a transparency system for data preprocessing that provides insights into data preparation pipelines. Our envisioned system consists of a Python library which enables users to log transformations and processed data. Subsequently, the system generates summaries of the data which was processed in the pipeline and so-called change profiles which capture the changes conducted in each processing step. These abstractions offer insight into the transformations and their effects on data. Additionally, the system includes an user interface where users can interactively discover the implemented pipeline and the changes made during preprocessing. This paper presents an initial concept of such a system. It also examines further challenges related to making preprocessing transparent and discusses potential solutions to address these challenges.} }
@inproceedings{10.1145/3630106.3658914, title = {Insights From Insurance for Fair Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {407--421}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658914}, url = {https://doi.org/10.1145/3630106.3658914}, author = {Fr\"ohlich, Christian and Williamson, Robert C.}, keywords = {aggregates, fair machine learning, insurance, responsibility, statistics, location = Rio de Janeiro, Brazil}, abstract = {We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.} }
@inproceedings{10.1145/3706594.3726983, title = {Stop Wasting your Cache! Bringing Machine Learning into Cache Computing}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {86--89}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3726983}, url = {https://doi.org/10.1145/3706594.3726983}, author = {Petrolo, Vincenzo and Guella, Flavia and Caon, Michele and Masera, Guido and Martina, Maurizio}, keywords = {In-Cache Computing, Near-memory Computing, Deep Neural Networks}, abstract = {The rapid evolution of Machine Learning (ML) workloads, particularly Deep Neural Networks (DNNs) and Transformer-based models, has intensified demands on computing architectures, highlighting the limitations of traditional von Neumann systems due to the memory bottleneck. To address these challenges, this paper investigates the mapping of fundamental Machine Learning (ML) operations onto ARCANE, a Near-Memory Computing (NMC)-based architecture that integrates Vector Processing Units (VPUs) directly within the data cache. ARCANE offers a flexible ISA-extension (xmnmc) abstracting memory management, effectively reducing data movement and enhancing performance. We specifically explore the acceleration capabilities of ARCANE when executing fundamental Deep Neural Network (DNN) and Transformer-based operations. Experimental results show that, with a contained area overhead, ARCANE achieves consistent speedups, delivering up to 150 improvement in 2D convolution, 305 in Linear layer, and over 32 in Fused-Weight Self-Attention (FWSA), compared to conventional CPU approaches. These findings underline ARCANE’s significant benefits in supporting efficient deployment of edge-oriented Machine Learning (ML) workloads.} }
@article{10.1145/3729376, title = {Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729376}, url = {https://doi.org/10.1145/3729376}, author = {Nguyen Tung, Lam and Cho, Steven and Du, Xiaoning and Neelofar, Neelofar and Terragni, Valerio and Ruberto, Stefano and Aleti, Aldeida}, keywords = {Explainability, Oracle Problem, SE4AI, Trustworthy Text Classifier}, abstract = {Machine learning (ML) for text classification has been widely used in various domains, such as toxicity detection, chatbot consulting, and review analysis. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Several studies indicate that traditional uncertainty metrics, such as model confidence, and performance metrics, like accuracy, are insufficient to build human trust in ML models. These models often learn spurious correlations during training and predict based on them during inference. When deployed in the real world, where such correlations are absent, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are made reasonably based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. So far, due to the lack of automated trustworthiness oracles, the assessment requires manual validation, based on the decision process disclosed by explanation methods. However, this approach is time-consuming, error-prone, and not scalable. To address this problem, we propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanation methods to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. As a demonstration of its practical usefulness, we also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. We compare TOKI with a naive baseline based solely on model confidence. To evaluate their alignment with human judgement, experiments are conducted on human-created ground truths of approximately 8,000 predictions. Additionally, we compare the effectiveness of TOKI-guided adversarial attack method with A2T, a state-of-the-art adversarial attack method for text classification. Results show that (1) relying on prediction uncertainty metrics, such as model confidence, cannot effectively distinguish between trustworthy and untrustworthy predictions, (2) TOKI achieves 142\% higher accuracy than the naive baseline, and (3) TOKI-guided adversarial attack method is more effective with fewer perturbations than A2T.} }
@inproceedings{10.1145/3725843.3756126, title = {Misam: Machine Learning Assisted Dataflow Selection in Accelerators for Sparse Matrix Multiplication}, booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture}, pages = {824--838}, year = {2025}, isbn = {9798400715730}, doi = {10.1145/3725843.3756126}, url = {https://doi.org/10.1145/3725843.3756126}, author = {Yadav, Sanjali and Namjoo, Amirmahdi and Asgari, Bahar}, keywords = {Flexible SpGEMM accelerators, FPGA Reconfiguration, Decision Tree.}, abstract = {The performance of Sparse Matrix-Matrix Multiplication (SpGEMM), a foundational operation in scientific computing and machine learning, is highly sensitive to the diverse and dynamic sparsity patterns of its input matrices. While specialized hardware accelerators improve efficiency, their reliance on fixed dataflows, each optimized for a narrow sparsity regime, results in suboptimal performance on real-world workloads. Even recent flexible accelerators that support multiple dataflows face two critical limitations: (1) the lack of a fast and principled mechanism for runtime dataflow selection, and (2) the area overhead and hardware underutilization incurred to provide that flexibility. We present Misam, a machine learning framework that addresses these challenges to enable adaptive and hardware-efficient SpGEMM acceleration. Misam employs a lightweight decision tree to dynamically predict the optimal hardware configuration from matrix features. To overcome hardware underutilization, Misam leverages FPGA reconfigurability to deploy specialized, resource-efficient bitstreams on demand. This process is governed by an intelligent reconfiguration engine that evaluates whether the anticipated performance gain justifies the overhead of switching hardware configurations. Misam’s dynamic approach yields up to a 10.76 speedup by judiciously reconfiguring. Misam demonstrates that a synergistic combination of machine learning-based prediction and judicious hardware reconfiguration can achieve high performance across a wide spectrum of sparsity patterns, bridging the gap between specialized efficiency and general-purpose adaptability.} }
@inproceedings{10.1145/3728199.3728237, title = {Machine Learning-Based Multi-User Semi-Blind Signal Detection in Non-Orthogonal Multiple Access}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {234--239}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728237}, url = {https://doi.org/10.1145/3728199.3728237}, author = {Li, Shuangyuan and Liu, Chang and Si, Pengbo and Li, Meng}, keywords = {Bit error rate, Channel estimation, Machine learning, Multi-user signal detection, Non-orthogonal multiple access}, abstract = {With the rapid development of fifth generation (5G) and sixth generation (6G) wireless communication technologies, non-orthogonal multiple access (NOMA) systems have become crucial to improve spectral efficiency and support more user connections. However, multi-user signal detection faces challenges due to complex channel environments and signal interference, and efficient and accurate detection methods need to be developed. In this paper, we propose a novel machine learning (ML) algorithm for semi-blind signal detection in NOMA uplink systems. Uniquely, the algorithm does not require separate channel estimation and can directly and accurately recover user signals received at the base station. Specifically, a convolutional neural network long term memory (CNN-LSTM) framework is used to analyse the synthetic signals received at the base station and estimate the channel state information. The estimation results are then fed into a deep neural network (DNN) based on joint maximum likelihood (JML) detection. By integrating with the synthetic signals, the transmitted signals of each user are detected separately. We evaluate the detection performance of both users under different modulation schemes. Simulation results show that using the proposed machine learning algorithm, our approach achieves significantly lower BER than traditional serial interference cancellation methods and the best JML detection performance.} }
@inproceedings{10.1145/3697090.3699865, title = {Impact of Data Anonymization in Machine Learning Models}, booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing}, pages = {188--191}, year = {2024}, isbn = {9798400717406}, doi = {10.1145/3697090.3699865}, url = {https://doi.org/10.1145/3697090.3699865}, author = {Pimenta, Ivo and Silva, Douglas and Moura, Evellin and Silveira, Matheus and Gomes, Rafael Lopes}, keywords = {Artificial Intelligence, Privacy, Anonymization}, abstract = {Data leakage compromises companies’ confidentiality and directly impacts existing privacy laws, where it is necessary to ensure integration with legacy systems to avoid harming the performance of their services while efficiently using the data for training machine learning (ML) models. Within this context, this work applies a technique to anonymize the data and train ML models in the context of DDoS attacks, aiming to evaluate the performance of this anonymization technique.} }
@inproceedings{10.1145/3695053.3731053, title = {LightML: A Photonic Accelerator for Efficient General Purpose Machine Learning}, booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture}, pages = {18--33}, year = {2025}, isbn = {9798400712616}, doi = {10.1145/3695053.3731053}, url = {https://doi.org/10.1145/3695053.3731053}, author = {Liu, Liang and Kari, Sadra Rahimi and Xin, Xin and Youngblood, Nathan and Zhang, Youtao and Yang, Jun}, keywords = {Optical Computing Crossbar, Coherent Photonic Multiplication, Ultralow-Energy Machine Learning}, abstract = {The rapid integration of AI technologies into everyday life across sectors such as healthcare, autonomous driving, and smart home applications requires extensive computational resources, placing strain on server infrastructure and incurring significant costs.We present LightML, the first system-level photonic crossbar design, optimized for high-performance machine learning applications. This work provides the first complete memory and buffer architecture carefully designed to support the high-speed photonic crossbar, achieving over 80\% utilization. LightML\&nbsp;also introduces solutions for key ML functions, including large-scale matrix multiplication (MMM), element-wise operations, non-linear functions, and convolutional layers. Delivering 325 TOP/s at only 3 watts, LightML offers significant improvements in speed and power efficiency, making it ideal for both edge devices and dense data center workloads.} }
@article{10.1145/3656021.3656029, title = {Fair Machine Learning Post Affirmative Action}, journal = {SIGCAS Comput. Soc.}, volume = {52}, pages = {22}, year = {2024}, issn = {0095-2737}, doi = {10.1145/3656021.3656029}, url = {https://doi.org/10.1145/3656021.3656029}, author = {Perello, Nick and Grabowicz, Przemyslaw}, abstract = {The U.S. Supreme Court, in a 6-3 decision on June 29, effectively ended the use of race in college admissions [1]. Indeed, national polls found that a plurality of Americans - 42\%, according to a poll conducted by the University of Massachusetts [2] - agree that the policy should be discontinued, while 33\% support its continued use in admissions decisions. As scholars of fair machine learning, we ponder how the Supreme Court decision shifts points of focus in the field. The most popular fair machine learning methods aim to achieve some form of "impact parity" by diminishing or removing the correlation between decisions and protected attributes, such as race or gender, similarly to the 80\% rule of thumb of the Equal Employment Opportunity Commision. Impact parity can be achieved by reversing historical discrimination, which corresponds to affirmative actions, or by diminishing or removing the influence of the attributes correlated with the protected attributes, which is impractical as it severely undermines model accuracy. Besides, impact disparity is not necessarily a bad thing, e.g., African-American patients suffer from a higher rate of chronic illnesses than White patients and, hence, it may be justified to admit them to care programs at a proportionally higher rate [3]. The U.S. burden-shifting framework under Title VII offers solutions alternative to impact parity. To determine employment discrimination, U.S. courts rely on the McDonnell-Douglas burden-shifting framework where the explanations, justifications, and comparisons of employment practices play a central role. Can similar methods be applied in machine learning?} }
@inproceedings{10.1145/3737821.3749562, title = {SweePix! A Machine Learning Driven Gallery Cleaning App Adapted to User Behavior}, booktitle = {Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction}, year = {2025}, isbn = {9798400719707}, doi = {10.1145/3737821.3749562}, url = {https://doi.org/10.1145/3737821.3749562}, author = {Sahin undefinedppoliti, Hatice and Bender-Saebelkampf, Christian and Abdenebaoui, Larbi}, keywords = {machine learning, image processing, gallery cleaning, distributed learning, similarity, aesthetics, personalization}, abstract = {As individuals accumulate large numbers of images, many of which are redundant or low-quality, identifying and deleting unnecessary photos becomes a time-consuming task. This study explores a gallery cleaning app that aims to use machine learning to help users manage their digital photo collections more efficiently. The app adapts to individual preferences by learning from user behavior, offering personalized suggestions for deletions. We evaluate the effectiveness of the app in cleaning participants’ galleries and examine whether its personalization features impact the overall cleaning process. While personalization did not significantly improve the results in the current study, the findings contribute to the growing field of Human-Centered AI, demonstrating how adaptive systems may improve user experiences by aligning with individual needs in AI-driven applications.} }
@inproceedings{10.1145/3696630.3731614, title = {Causal Models in Requirement Specifications for Machine Learning: A vision}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {1402--1405}, year = {2025}, isbn = {9798400712760}, doi = {10.1145/3696630.3731614}, url = {https://doi.org/10.1145/3696630.3731614}, author = {Heyn, Hans-Martin and Mao, Yufei and Wei, Roland and Knauss, Eric}, keywords = {AI engineering, causal modelling, data requirements, requirements engineering, location = Clarion Hotel Trondheim, Trondheim, Norway}, abstract = {Specifying data requirements for machine learning (ML) software systems remains a challenge in requirements engineering (RE). This vision paper explores causal modelling as an RE activity that allows the systematic integration of prior domain knowledge into the design of ML software systems. We propose a workflow to elicit low-level model and data requirements from high-level prior knowledge using causal models. The approach is demonstrated on an industrial fault detection system. This paper outlines future research needed to establish causal modelling as an RE practice.} }
@article{10.1145/3747347, title = {Understanding Open Source Contributor Profiles in Popular Machine Learning Libraries}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3747347}, url = {https://doi.org/10.1145/3747347}, author = {Liu, Jiawen and Zhang, Haoxiang and Zou, Ying}, keywords = {Open Source Software, Developer Profiles, Collaborative Software Development, Machine Learning Libraries}, abstract = {With the increasing popularity of machine learning (ML), many open source software (OSS) contributors are attracted to developing and adopting ML approaches. Comprehensive understanding of ML contributors is crucial for successful ML OSS development and maintenance. Without such knowledge, there is a risk of inefficient resource allocation and hindered collaboration in ML OSS projects. Existing research focuses on understanding the difficulties and challenges perceived by ML contributors through user surveys. There is a lack of understanding of ML contributors based on their activities recorded in the software repositories. In this paper, we aim to understand ML contributors by identifying contributor profiles in ML libraries. We further study contributors’ OSS engagement from four aspects: workload composition, work preferences, technical importance, and ML-specific vs SE contributions. By investigating 11,949 contributors from 8 popular ML libraries (i.e., TensorFlow, PyTorch, scikit-learn, Keras, MXNet, Theano/Aesara, ONNX, and deeplearning4j), we categorize them into four contributor profiles: Core-Nighttime, Core-Daytime, Peripheral-Nighttime, and Peripheral-Daytime. We find that: 1) project experience, authored files, collaborations, pull requests comments received and approval ratio, and geographical location are significant features of all profiles; 2) contributors in Core profiles exhibit significantly different OSS engagement compared to Peripheral profiles; 3) contributors’ work preferences and workload compositions are significantly correlated with project popularity; and 4) long-term contributors evolve towards making fewer, constant, balanced and less technical contributions.} }
@inproceedings{10.1145/3675888.3676049, title = {Android Malware Detection System using Machine Learning}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {186--191}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676049}, url = {https://doi.org/10.1145/3675888.3676049}, author = {Kaur, Amanpreet and Lal, Sangeeta and Goel, Shruti and Pandey, Mrinal and Agarwal, Astha}, abstract = {Detecting Android malware is imperative for safeguarding user privacy, securing data, and preserving device performance. Consequently, numerous studies have underscored the complexities associated with Android malware detection, prompting a multidimensional approach to tackle these challenges effectively. This research leverages machine learning techniques, emphasizing feature extraction, classification algorithms, and both supervised and unsupervised learning methodologies. The exploration begins with in-depth Exploratory Data Analysis (EDA) to gain insights into the dataset, paving the way for informed decision-making. Principal Component Analysis (PCA) is employed for dimensionality reduction, a pivotal step in handling the multivariate nature of the data. The integration of API calls, clustering, and anomaly detection further enriches the model's capability to discern between benign and malicious applications. Crucially, the study delves into the intricacies of sampling, evaluation, and the Confusion Matrix to quantify the model's performance accurately. The utilization of diverse classification algorithms, including Support Vector Machines (SVM), Multi-Layer Perceptrons (MLP), Random Forest, GaussianNB, Decision Tree, and Logistic Regression, underscores the comprehensive nature of the approach. These algorithms collectively contribute to a robust and versatile Android malware detection model capable of adapting to varying threat scenarios. The dataset employed for training and evaluation is sourced from Kaggle, encompassing 29,999 Android applications categorized as benign or malicious based on permissions sought. Current detection methods, deemed resource-intensive and exhaustive, face the challenge of keeping pace with the relentless evolution of new malware strains. This research seeks to address this gap by proposing a sophisticated, machine learning-driven model that not only enhances accuracy but also demonstrates efficiency and adaptability in the face of a dynamic threat landscape.} }
@inproceedings{10.1145/3652963.3655064, title = {Machine Learning Systems are Bloated and Vulnerable}, booktitle = {Abstracts of the 2024 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems}, pages = {37--38}, year = {2024}, isbn = {9798400706240}, doi = {10.1145/3652963.3655064}, url = {https://doi.org/10.1145/3652963.3655064}, author = {Zhang, Huaifeng and Alhanahnah, Mohannad and Ahmed, Fahmi Abdulqadir and Fatih, Dyako and Leitner, Philipp and Ali-Eldin, Ahmed}, keywords = {machine learning systems, software debloating, location = Venice, Italy}, abstract = {Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from operating systems and applications to containers. Containers are lightweight virtualization technologies used to package code and dependencies, providing portable, reproducible and isolated environments. For their ease of use, data scientists often utilize machine learning containers to simplify their workflow. However, this convenience comes at a cost: containers are often bloated with unnecessary code and dependencies, resulting in very large sizes. In this paper, we analyze and quantify bloat in machine learning containers. We develop MMLB, a framework for analyzing bloat in software systems, focusing on machine learning containers. MMLB measures the amount of bloat at both the container and package levels, quantifying the sources of bloat. In addition, MMLB integrates with vulnerability analysis tools and performs package dependency analysis to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from TensorFlow, PyTorch, and Nvidia, we show that bloat accounts for up to 80\% of machine learning container sizes, increasing container provisioning times by up to 370\% and exacerbating vulnerabilities by up to 99\%. For more detail, see the full paper, ~citezhang2024machine.} }
@inproceedings{10.1145/3706890.3706940, title = {Machine Learning-Based Survival Prediction Model for Melanoma}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {289--295}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3706940}, url = {https://doi.org/10.1145/3706890.3706940}, author = {Jing, Zijie and Wang, Jianghong}, keywords = {Machine learning, Melanoma, Survival}, abstract = {The high growth rate of melanoma poses a huge challenge to healthcare delivery worldwide. At this stage, rapid and accurate diagnosis and timely treatment of melanoma is crucial. In this study, we utilized multiple metrics to predict survival in melanoma patients. We built multiple models of different types of machine learning, deep learning, and integrated learning, of which the best results were Logistic regression, Long short-term memory, and Categorical boosting, with AUCs of 0.9267, 0.9365, and 0.9417, respectively. We performed interpretability analyses of these models to improve the models' applicability in the clinic. The conclusions obtained from the analysis have important implications for the treatment of melanoma. This study represents a significant advancement in the use of machine learning methods to predict the survival status of individuals with melanoma, providing a new way of thinking about melanoma treatment and prognosis.} }
@inproceedings{10.1145/3670474.3685969, title = {Enabling Risk Management of Machine Learning Predictions for FPGA Routability}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685969}, url = {https://doi.org/10.1145/3670474.3685969}, author = {Gunter, Andrew David and Thomas, Maya and Ghanathe, Nikhil Pratap and Wilton, Steven}, keywords = {CAD, FPGA, ML, confidence, early exit, risk, routing, uncertainty, location = Salt Lake City, UT, USA}, abstract = {Machine Learning (ML) models sometimes make inaccurate predictions for the routability of field-programmable gate array (FPGA) circuit designs. This risks time wasted attempting to route an unroutable design or the premature termination of a routable design's compilation. While improving model accuracy is beneficial, we explore a complementary approach to mitigate the risk of inaccurate predictions by assessing the confidence of ML models. This approach could allow individuals to customize their own trade-off for the competing risks of wasted time and premature compilation termination. In this paper, we introduce a novel mixture of experts ML system for FPGA routability prediction and further quantify the confidence calibration of this system to determine its suitability as a risk management tool. We evaluate our prediction system for the purpose of enabling user risk management in FPGA routability prediction, comparing against a baseline inspired by prior work. Our evaluation finds our approach to achieve almost 2 the precision in risk trade-off between time wasted on unroutable designs and premature termination of routable designs.} }
@article{10.1145/3639368, title = {Fairness-Driven Private Collaborative Machine Learning}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {15}, year = {2024}, issn = {2157-6904}, doi = {10.1145/3639368}, url = {https://doi.org/10.1145/3639368}, author = {Pessach, Dana and Tassa, Tamir and Shmueli, Erez}, keywords = {Privacy, algorithmic fairness, collaborative machine learning, federated learning, secure multi-party computation}, abstract = {The performance of machine learning algorithms can be considerably improved when trained over larger datasets. In many domains, such as medicine and finance, larger datasets can be obtained if several parties, each having access to limited amounts of data, collaborate and share their data. However, such data sharing introduces significant privacy challenges. While multiple recent studies have investigated methods for private collaborative machine learning, the fairness of such collaborative algorithms has been overlooked. In this work, we suggest a feasible privacy-preserving pre-process mechanism for enhancing fairness of collaborative machine learning algorithms. An extensive evaluation of the proposed method shows that it is able to enhance fairness considerably with only a minor compromise in accuracy.} }
@inproceedings{10.1145/3658644.3690373, title = {TabularMark: Watermarking Tabular Datasets for Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {3570--3584}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690373}, url = {https://doi.org/10.1145/3658644.3690373}, author = {Zheng, Yihao and Xia, Haocheng and Pang, Junyuan and Liu, Jinfei and Ren, Kui and Chu, Lingyang and Cao, Yang and Xiong, Li}, keywords = {data ownership, machine learning, tabular dataset, watermark, location = Salt Lake City, UT, USA}, abstract = {Watermarking is broadly utilized to protect ownership of shared data while preserving data utility. However, existing watermarking methods for tabular datasets fall short on the desired properties (detectability, non-intrusiveness, and robustness) and only preserve data utility from the perspective of data statistics, ignoring the performance of downstream ML models trained on the datasets. Can we watermark tabular datasets without significantly compromising their utility for training ML models while preventing attackers from training usable ML models on attacked datasets?In this paper, we propose a hypothesis testing-based watermarking scheme, TabularMark. Data noise partitioning is utilized for data perturbation during embedding, which is adaptable for numerical and categorical attributes while preserving the data utility. For detection, a custom-threshold one proportion z-test is employed, which can reliably determine the presence of the watermark. Experiments on real-world and synthetic datasets demonstrate the superiority of TabularMark in detectability, non-intrusiveness, and robustness.} }
@article{10.1145/3762199, title = {What is new, and what is old, in fairness and machine learning}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3762199}, url = {https://doi.org/10.1145/3762199}, author = {Hu, Lily}, keywords = {Algorithmic fairness, Algorithmic decision-making, Political Philosophy}, abstract = {This article explores the issue of normative distinctiveness in machine learning decision systems alongside Solon Barocas, Moritz Hardt, and Arvind Narayanan's landmark book Fairness and Machine Learning. What, if anything, is different this time, with the rise of machine learning-based aids to bureaucratic decision-making? I show how a focus on normative distinctiveness can obscure from view a much more significant upshot of machine learning: that the mere existence of feasible alternatives presses new justificatory demands not just on the design of new technical systems but on the prevailing human-centered decision regime. I argue that depoliticizing conventional bureaucratic structures of decision-making leads to a missed opportunity for a broader normative reevaluation of what we owe to each other in a world of expanded practical possibility.} }
@inproceedings{10.1145/3578356.3592581, title = {Actionable Data Insights for Machine Learning}, booktitle = {Proceedings of the 3rd Workshop on Machine Learning and Systems}, pages = {1--7}, year = {2023}, isbn = {9798400700842}, doi = {10.1145/3578356.3592581}, url = {https://doi.org/10.1145/3578356.3592581}, author = {Wu, Ming-Chuan and B\"ahr, Manuel and Braun, Nils and Honauer, Katrin}, keywords = {data insights, interactive diagnostics, data-centric machine learning, location = Rome, Italy}, abstract = {Artificial Intelligence (AI) and Machine Learning (ML) have made tremendous progress in the recent decade and have become ubiquitous in almost all application domains. Many recent advancements in the ease-of-use of ML frameworks and the low-code model training automations have further reduced the threshold for ML model building. As ML algorithms and pre-trained models become commodities, curating the appropriate training datasets and model evaluations remain critical challenges. However, these tasks are labor-intensive and require ML practitioners to have bespoke data skills. Based on the feedback from different ML projects, we built ADIML (Actionable Data Insights for ML) - a holistic data toolset. The goal is to democratize data-centric ML approaches by removing big data and distributed system barriers for engineers. We show in several case studies how the application of ADIML has helped solve specific data challenges and shorten the time to obtain actionable insights.} }
@inbook{10.1145/3756423.3756471, title = {Research on Optimization of Food Process Parameters Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing}, pages = {297--301}, year = {2025}, isbn = {9798400714351}, url = {https://doi.org/10.1145/3756423.3756471}, author = {Chen, Xiangjun and Shi, Qinhong}, abstract = {The article is limited to machine learning and discusses parameter optimisation in the bread production process. This paper introduces the basic technical parameters used in bread manufacturing: the moisture ratio, yeast fermentation time, kneading rate and baking temperature, as well as baking time, and how these parameters affect bread quality. The research collected plenty of process parameters and quality index through experiments, and established a prediction model of regression analysis, support vector machine and random forest based on machine learning theory. Meanwhile, the parameters were modified with genetic optimization and particle swarm optimization. It is possible to accurately analyze the relationship between the different process parameters and the quality of bread, optimize the production process, and upgrade the product quality, which has applicational value to the food production.} }
@inproceedings{10.1145/3708360.3708388, title = {Construction of Hotel Energy Consumption Prediction and Management Scheme Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {174--178}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708388}, url = {https://doi.org/10.1145/3708360.3708388}, author = {Xue, Wenting}, keywords = {Energy consumption prediction, Hotel energy management, Machine learning, Multi model integration}, abstract = {As demand for energy management in the hotel increases, energy efficiency and energy consumption costs are important research directions. This paper introduces the energy consumption prediction and management plan of the hotel based on machine learning, and adopts the hierarchical structure design. The power consumption prediction using the depth learning model such as LSTM, Gru, TCN, and so on is optimized by using the dynamic weight distribution strategy. The system integrates an adaptive alarm mechanism, provides real-time energy monitoring, and provides optimization suggestions. Experimental results show that this method has high predictive accuracy and stability under different loading conditions, effectively reducing the energy consumption of the hotel and improving the energy management efficiency. As a result, the energy management system based on machine learning has a high degree of adaptability under complicated circumstances and provides a wide range of applications.} }
@inproceedings{10.1145/3715335.3735471, title = {Bridging the Gap: Climate Scientists' Concerns and Expectations for Machine Learning}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {284--296}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735471}, url = {https://doi.org/10.1145/3715335.3735471}, author = {Wu, Siyi and Easterbrook, Steve M.}, keywords = {Machine Learning, Climate Science, Socio-Technical Systems}, abstract = {As a field with huge volumes of spatio-temporal data and challenging computational bottlenecks, climate science stands to benefit from recent advances in Machine Learning. But ML represents both an opportunity and a threat. While ML has the potential to overcome technical barriers in studying and predicting the climate system, it also up-ends long-standing scientific norms for how climate knowledge is produced, validated, and communicated. Acceptance of these new methods in the field therefore depends on how experts from different fields collaborate and how they address concerns over applicability, interpretability, and scientific rigor. To explore the socio-technical setting in which ML is being adopted by climate scientists, we conducted an interview study to identify how climate scientists are using (or resisting the use of) ML in their work, what concerns and challenges they face, and how what expectations they have for the potential of AI in climate science. The results show that while some climate scientists embrace ML for its computational advantages and potential to enhance climate modeling, others remain skeptical due to concerns about interpretability, validation, and alignment with established scientific practices. Participants highlighted the need for transparent, domain-informed AI methodologies and emphasized barriers to interdisciplinary collaboration between climate scientists and AI researchers. These findings underscore that AI’s role in climate science is not predetermined but actively shaped by domain experts through their expectations, concerns, and engagement with these technologies.} }
@inproceedings{10.1145/3767052.3767068, title = {Machine Learning Models for Predicting Second-hand House Prices: A Comparative Study}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {99--106}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767068}, url = {https://doi.org/10.1145/3767052.3767068}, author = {Jiang, Haoran}, keywords = {LightGBM, Linear Regression, Machine learning, Random Forest, Ridge Regression, Second-hand house price prediction, XGBoost}, abstract = {This paper presents a comparative analysis of the application of machine learning models in predicting the prices of second-hand houses. It underscores the importance of the real estate market within the economy and points out the advantages of precise predictions for various stakeholders, including homebuyers, real estate agents, and financial institutions. Traditional methods frequently fail to grasp the intricate interactions that impact house prices, whereas machine learning techniques have garnered attention for their capability to handle such complexities. The study made use of the “Shanghai Second-hand Housing Prices from 2024 Anjuke Website” dataset from Kaggle, which comprises 175,135 records, each having nearly 30 features. After undergoing preprocessing procedures like duplicate removal, feature transformation, outlier removal, missing value imputation, feature encoding, feature engineering, and data standardization, the data was divided into training and testing sets. Four machine learning models were compared: Linear Regression, Ridge Regression, Random Forest, and XGBoost. The findings revealed that Random Forest and XGBoost surpassed the linear models, with Random Forest attaining the lowest Mean Absolute Error (MAE) and the highest R² Score. The study concluded that Random Forest was the most effective model for predicting the prices of second-hand houses in Shanghai. Even though the results were promising, the study acknowledged certain limitations such as the temporal and geographical constraints of the dataset and the reliance on historical data. Future research ought to concentrate on real-time data integration, model updating, incorporating additional data sources, and exploring advanced techniques to improve the performance and interpretability of the model.} }
@inproceedings{10.1145/3714334.3714341, title = {Machine Learning-Based Income Inequality Prediction: A Case Study}, booktitle = {Proceedings of the 2024 2nd International Conference on Artificial Intelligence, Systems and Network Security}, pages = {34--39}, year = {2025}, isbn = {9798400711237}, doi = {10.1145/3714334.3714341}, url = {https://doi.org/10.1145/3714334.3714341}, author = {Ji, Jiexin}, keywords = {Income Inequality, Income Prediction, Machine Learning, Socioeconomic Factors, XGBoost}, abstract = {Income inequality is an important socioeconomic issue, especially in countries such as the United States, where income distribution gaps continue to widen. Predicting income levels and understanding the factors that contribute to these gaps can provide valuable insights for policymakers and researchers. This paper explores the application of XGBoost (Extreme Gradient Boosting) modeling to income level prediction. The task consists of predicting whether an individual's annual income exceeds $50,000 based on their socioeconomic attributes. After preprocessing the dataset (including dealing with missing values and encoding categorical features), the XGBoost model was trained and optimized using hyper-parameter tuning and cross-validation. The ROC AUC score was 0.93, highlighting the ability of the trained machine learning model to effectively discriminate between income categories.} }
@inproceedings{10.1145/3641554.3701803, title = {Larger than Life In-Class Demonstrations for Introductory Machine Learning}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {220--226}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701803}, url = {https://doi.org/10.1145/3641554.3701803}, author = {Chai, Henry and Gormley, Matthew R.}, keywords = {data science, education, machine learning, unplugged demos, location = Pittsburgh, PA, USA}, abstract = {This paper presents a collection of in-class demonstrations for an introductory machine learning (ML) class. Each demonstration engages students actively in visualizing the behavior of a machine learning algorithm in order to build an intuitive understanding. These demonstrations are in direct contrast to purely slide- or whiteboard- based presentations of the same concepts by being student-paced and highly interactive, leveraging the physical space of the classroom. We developed demonstrations for six common ML methods: decision trees, k-nearest neighbors, the Perceptron, stochastic gradient descent (SGD), neural networks, and multi-armed bandits. Survey data from two semesters show that our demonstrations enhance student retention of and engagement with the material, relative to lectures without similar in-class demonstrations. Our demonstrations use readily available materials and student volunteers, making them easily reproducible for any educator seeking to complement their existing ML course.} }
@inproceedings{10.1145/3725899.3725948, title = {Motivation Factor Analytic for Training Business Using Machine Learning}, booktitle = {Proceedings of the 2025 8th International Conference on Software Engineering and Information Management}, pages = {184--189}, year = {2025}, isbn = {9798400710438}, doi = {10.1145/3725899.3725948}, url = {https://doi.org/10.1145/3725899.3725948}, author = {Kulkarineetham, Suwanee and Amsupan, Satanphop and Maliyaem, Maleerat}, keywords = {K-nearest neighbors, Machine learning, Motivation factor analytic, Principal component analysis, Random forest}, abstract = {In today's competitive and technology-driven business environment, ongoing employee training is essential for professional growth and organizational success. This research aims to analyze and identify the key motivational factors that drive both self-sponsored and company-sponsored participants to enroll in marketing training programs used Principal Component Analysis (PCA) and using machine learning models K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF) to predict the participants returned to enroll training. Dataset was collected through surveys, processed using PCA to reduce dimensionality, and analyzed to enhance the interpretability of features such as career development motivations and company-driven policies. The predictive models purposed high accuracy, with KNN model based on PCA achieving 97.09\% for the company-sponsored participants and RF model based on PCA achieving 96.00\% for the self-sponsored participants. This research aims to identify key motivational factors for training program enrollment among self-sponsored and company-sponsored participants and predict the likelihood of participants returning for further training based on motivational factors. The insights offer valuable guidance for training providers to tailor their programs to participant motivations, effectively supporting talent development in the digital economy.} }
@inproceedings{10.1145/3593013.3594000, title = {‘Affordances’ for Machine Learning}, booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency}, pages = {324--332}, year = {2023}, isbn = {9798400701924}, doi = {10.1145/3593013.3594000}, url = {https://doi.org/10.1145/3593013.3594000}, author = {Davis, Jenny L}, keywords = {AI Alignment, Affordances, Design Studies, Machine Learning, Mechanisms and Conditions Framework, Principles-to-Practice, location = Chicago, IL, USA}, abstract = {The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.} }
@inbook{10.1145/3730436.3730535, title = {Empirical Study on Machine Learning Testing Based on Mutation Testing}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {608--613}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730535}, author = {Che, Zijie and Zhao, Ruilian and Wang, Weiwei}, abstract = {In the context of machine learning projects, the inherent randomness of machine learning algorithms and the intuitive nature of developer test assertions frequently result in unstable and ineffective test cases. Mutation testing is a proven method to enhance the efficacy of test cases. However, the implementation of mutation testing for machine learning is confronted with challenges such as the generation of a substantial number of mutants and suboptimal execution efficiency. This paper undertakes a comprehensive analysis of six prominent open-source machine learning projects, optimises their test cases through the utilisation of mutation testing, and conducts a detailed investigation into the performance of various mutants. The objective is to provide recommendations and assistance in the selection of effective mutants in machine learning projects. The primary conclusions of this study are as follows: 1) The predominant cause of the failure of machine learning mutants is the substantial semantic discrepancies between the mutant program statements and the original program. 2) In existing machine learning projects, the proportion of mutants eliminated by test assertions is minimal, with an average of merely 4.39\% of mutants being eliminated. 3) Mutation operators involving numeric and operator types are highly effective in adjusting the boundaries of test assertions.} }
@inproceedings{10.1145/3555041.3589337, title = {Mixed Methods Machine Learning}, booktitle = {Companion of the 2023 International Conference on Management of Data}, pages = {3--4}, year = {2023}, isbn = {9781450395076}, doi = {10.1145/3555041.3589337}, url = {https://doi.org/10.1145/3555041.3589337}, author = {Murdock, Vanessa}, keywords = {UX research, e-commerce, mixed-methods research, shopping, location = Seattle, WA, USA}, abstract = {Machine learning is ubiquitous: many of our everyday interactions, both online and offline, are backed by machine learning. Typically, machine learned systems start as an idea from the business or engineering team for a service or an app that helps the customer achieve a goal. The app is built iteratively, starting with the minimum lovable version, and undergoes several rounds of improvements to become more sophisticated. Success is measured with an online A/B test on live traffic, on the assumption that if customers engage with the app, it is serving their needs.We propose a different approach to developing such systems, that employs mixed-methods research to understand what to build, and how to make it satisfying and helpful for the customer. The Mixed Methods Machine Learning (MXML) paradigm, starts with a user study, to understand how people behave in an everyday setting (such as shopping for groceries in a grocery store), and to identify points of friction that can be automated, or experiences that can be made more enjoyable. The study observations are mapped to interactions recorded in the system's behavioral log data, which is the basis for the machine learned system. Mapping the study observations to the log data is a key step in directing the machine learning to solve a customer problem. The MXML system is evaluated with a follow-on user study, in addition to the traditional online A/B test, to assess whether the system is satisfying, helpful and delightful. In this talk we present the MXML paradigm, with real-world examples.} }
@inproceedings{10.1145/3669940.3707284, title = {PartIR: Composing SPMD Partitioning Strategies for Machine Learning}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, pages = {794--810}, year = {2025}, isbn = {9798400706981}, doi = {10.1145/3669940.3707284}, url = {https://doi.org/10.1145/3669940.3707284}, author = {Alabed, Sami and Belov, Daniel and Chrzaszcz, Bart and Franco, Juliana and Grewe, Dominik and Maclaurin, Dougal and Molloy, James and Natan, Tom and Norman, Tamara and Pan, Xiaoyue and Paszke, Adam and Rink, Norman A. and Schaarschmidt, Michael and Sitdikov, Timur and Swietlik, Agnieszka and Vytiniotis, Dimitrios and Wee, Joel}, keywords = {distributed systems, language/compiler support for machine learning, machine learning model partitioning, parallelism, scalability, spmd, systems for machine learning, systems for tensor computing, location = Rotterdam, Netherlands}, abstract = {Training modern large neural networks (NNs) requires a combination of parallelization strategies, including data, model, or optimizer sharding. To address the growing complexity of these strategies, we introduce PartIR, a hardware-and-runtime agnostic NN partitioning system. PartIR is: 1) Expressive: It allows for the composition of multiple sharding strategies, whether user-defined or automatically derived; 2) Decoupled: the strategies are separate from the ML implementation; and 3) Predictable: It follows a set of well-defined general rules to partition the NN. PartIR utilizes a schedule-like API that incrementally rewrites the ML program intermediate representation (IR) after each strategy, allowing simulators and users to verify the strategy's performance. PartIR has been successfully used both for training large models and across diverse model architectures, demonstrating its predictability, expressiveness, and performance.} }
@inproceedings{10.1145/3508352.3561121, title = {Machine Learning for Testing Machine-Learning Hardware: A Virtuous Cycle}, booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design}, year = {2022}, isbn = {9781450392174}, doi = {10.1145/3508352.3561121}, url = {https://doi.org/10.1145/3508352.3561121}, author = {Chaudhuri, Arjun and Talukdar, Jonti and Chakrabarty, Krishnendu}, abstract = {The ubiquitous application of deep neural networks (DNN) has led to a rise in demand for AI accelerators. DNN-specific functional criticality analysis identifies faults that cause measurable and significant deviations from acceptable requirements such as the inferencing accuracy. This paper examines the problem of classifying structural faults in the processing elements (PEs) of systolic-array accelerators. We first present a two-tier machine-learning (ML) based method to assess the functional criticality of faults. While supervised learning techniques can be used to accurately estimate fault criticality, it requires a considerable amount of ground truth for model training. We therefore describe a neural-twin framework for analyzing fault criticality with a negligible amount of ground-truth data. We further describe a topological and probabilistic framework to estimate the expected number of PE's primary outputs (POs) flipping in the presence of defects and use the PO-flip count as a surrogate for determining fault criticality. We demonstrate that the combination of PO-flip count and neural twin-enabled sensitivity analysis of internal nets can be used as additional features in existing ML-based criticality classifiers.} }
@inproceedings{10.1145/3610538.3614646, title = {Machine Learning \&amp; Neural Networks}, booktitle = {SIGGRAPH Asia 2023 Courses}, year = {2023}, isbn = {9798400703096}, doi = {10.1145/3610538.3614646}, url = {https://doi.org/10.1145/3610538.3614646}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {Use and development of computer systems that are able to learn and adapt without following explicit instructions by using algorithms and statistical models to analyze and draw inferences from patterns in data.} }
@article{10.1145/3708495, title = {Distributed Machine Learning in Edge Computing: Challenges, Solutions and Future Directions}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708495}, url = {https://doi.org/10.1145/3708495}, author = {Tu, Jingke and Yang, Lei and Cao, Jiannong}, keywords = {Edge computing, distributed machine learning, model optimization, data heterogeneity, communication constraints}, abstract = {Distributed machine learning on edges is widely used in intelligent transportation, smart home, industrial manufacturing, and underground pipe network monitoring to achieve low latency and real time data processing and prediction. However, the presence of a large number of sensing and edge devices with limited computing, storage, and communication capabilities prevents the deployment of huge machine learning models and hinders its application. At the same time, although distributed machine learning on edges forms an emerging and rapidly growing research area, there has not been a systematic survey on this topic. The article begins by detailing the challenges of distributed machine learning in edge environments, such as limited node resources, data heterogeneity, privacy, security issues, and summarizes common metrics for model optimization. We then present a detailed analysis of parallelism patterns, distributed architectures, and model communication and aggregation schemes in edge computing. we subsequently present a comprehensive classification and intensive description of node resource-constrained processing, heterogeneous data processing, attacks and protection of privacy. The article ends by summarizing the applications of distributed machine learning in edge computing and presenting problems and challenges for further research.} }
@inproceedings{10.1145/3691573.3691592, title = {Machine Learning Applied to Locomotion in Virtual Reality}, booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality}, pages = {134--139}, year = {2024}, isbn = {9798400709791}, doi = {10.1145/3691573.3691592}, url = {https://doi.org/10.1145/3691573.3691592}, author = {Sakabe, Fernando Kenji and Ayres, Fabio Jos\'e and Soares, Luciano Pereira}, keywords = {Feet Tracking, Machine Learning, User Navigation, Virtual Locomotion, Walking Pattern Recognition, Walking in Virtual Reality, location = Manaus, Brazil}, abstract = {The objective of this research project is to recognize a user’s walking pattern on a treadmill-like platform for navigation in Virtual Reality (VR) environments. To achieve this, a walking recognition software solution was developed using Convolutional Neural Networks (CNNs), a type of Machine Learning (ML) architecture. The ML model was trained on a dataset collected from users’ movements in a virtual simulation, tracking the positions and rotations of their feet as they walked in various directions and orientations. Tracking was accomplished using 6 degrees of freedom (6DoF) trackers placed on the users’ feet. The neural network achieved a 94\% accuracy rate during testing. Due to the network’s lightweight configuration, the response time is, on average, 0.1 seconds, demonstrating its potential as an efficient natural controller for real-time user navigation in multiple directions.} }
@inproceedings{10.1145/3650215.3650355, title = {IO Behaviour Analysis Based on Machine Learning}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {805--808}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650355}, url = {https://doi.org/10.1145/3650215.3650355}, author = {Pan, Weibo and Li, Fei and Li, Sheng and Zhao, Yongcai and Zhang, Jun and Yang, Di and Wan, Xiang}, abstract = {This article uses machine learning algorithms to predict the latency of an IO request without additional internal information on the device, treating it as a black box to model equipment performance prediction. Different from algorithms such as CART in machines used in amounts of IO performance prediction, this paper mainly analyzes the impact of attributes on prediction performance and implements and compares the performance of multiple algorithms on different workloads. Based on the GBDT algorithm and the attribute selection algorithm, it analyzes different metrics (attribute sets). It proposes that using a “lastdelay” attribute can improve the prediction effect and takes removing the distance attribute from SSDs based on traditional metrics into consideration. According to unstable effects on SSDs and unstable performance of the distance attribute, this paper points out the necessity of using different metrics for different loads and different devices.} }
@inproceedings{10.1145/3766918.3766947, title = {ExHybridNet: An Explainable Hybrid Machine Learning Model for Corporate Financial Risk Prediction}, booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business}, pages = {172--178}, year = {2025}, isbn = {9798400716027}, doi = {10.1145/3766918.3766947}, url = {https://doi.org/10.1145/3766918.3766947}, author = {Luo, Yuexin}, keywords = {Explainability, Financial Risk Prediction, Hybrid Machine learning, Neural Network}, abstract = {In a complex and volatile economic environment, accurately predicting the financial risks of enterprises is of vital importance to investors, creditors and regulatory authorities. This study proposes an interpretable hybrid machine learning model, ExHybridNet, for enterprise financial risk prediction. This model effectively captures dynamic nonlinear relationships in financial data through a three-path fusion architecture of convolutional neural networks, attention mechanism and random forest. ExHybridNet also provides transparent feature importance explanations through SHAP analysis. Based on the financial and audit data of listed companies on the Shenzhen and Beijing stock Exchanges in the CSMAR database from 2005 to 2024, ExHybridNet significantly outperforms traditional machine learning models (logistic regression, random forest, support vector machine), deep learning models (CNN, MLP), and hybrid machine learning (RF-CNN, CNN-Attention) in terms of accuracy, precision, and F1 score. The research results of SHAP analysis have revealed the roles of risk drivers such as high leverage ratio (D/A) and short-term borrowing dependence (STBD), as well as risk mitigation factors such as profit stability (EV) and tangible asset ratio (TAR), providing actionable insights for enterprise financial management, risk prevention and risk control.} }
@inproceedings{10.1145/3630106.3658983, title = {Achieving Reproducibility in EEG-Based Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {1464--1474}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658983}, url = {https://doi.org/10.1145/3630106.3658983}, author = {Kinahan, Sean and Saidi, Pouria and Daliri, Ayoub and Liss, Julie and Berisha, Visar}, keywords = {EEG, Machine Learning, Reproducibility, location = Rio de Janeiro, Brazil}, abstract = {Despite the inherent complexity of electroencephalogram (EEG) data characterized by its high dimensionality, artifactual noise, and biological variability, many machine learning (ML) studies claim impressive performance in decoding or classifying EEG signals. Recently, several studies have highlighted that flawed data analysis is a prevalent issue in the literature, leading to irreproducible results and exaggerated claims. To address this issue, we propose a framework that addresses three primary obstacles in EEG ML research: data leakage, data scarcity, and flawed model selection. We introduce the EEG ML Model Card, a standardized and transparent EEG ML model documentation tool that aims to directly address these pitfalls and enhance reproducibility and trustworthiness in EEG ML research.} }
@inproceedings{10.1145/3646547.3689668, title = {Poster: Predicting Internet Shutdowns - A Machine Learning Approach}, booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference}, pages = {763--764}, year = {2024}, isbn = {9798400705922}, doi = {10.1145/3646547.3689668}, url = {https://doi.org/10.1145/3646547.3689668}, author = {Zirikana, Jules and Phokeer, Amreesh}, keywords = {internet shutdowns, random forest, shutdown risk prediction, location = Madrid, Spain}, abstract = {Internet shutdowns, often enforced by governments to control communication and access to information, have significant socio-political and economic implications. This study presents a machine learning approach to predict the likelihood of internet shutdowns, developing an Internet Shutdown Risk Score using public datasets from 125 countries. A Random Forest classifier, achieving an AUC of 0.97, was used to calculate risk scores. Key features were identified using the Shapley algorithm, highlighting factors like political unrest, economic conditions, and digital infrastructure. Case studies in Pakistan, India, and Sudan demonstrate rising shutdown risks due to protests from 2019 to 2022. Globally, the Internet Shutdown Risk Index has been consistently high since 2019, indicating increased threats of internet shutdowns in politically unstable regions.} }
@article{10.5555/3722577.3722856, title = {On doubly robust inference for double machine learning in semiparametric regression}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Dukes, Oliver and Vansteelandt, Stijn and Whitney, David}, keywords = {doubly robust estimation, semiparametric inference, causal inference, conditional independence testing}, abstract = {Due to concerns about parametric model misspecification, there is interest in using machine learning to adjust for confounding when evaluating the causal effect of an exposure on an outcome. Unfortunately, exposure effect estimators that rely on machine learning predictions are generally subject to so-called plug-in bias, which can render naive p-values and confidence intervals invalid. Progress has been made via proposals like targeted minimum loss estimation and more recently double machine learning, which rely on learning the conditional mean of both the outcome and exposure. Valid inference can then be obtained so long as both predictions converge (sufficiently fast) to the truth. Focusing on partially linear regression models, we show that a specific implementation of the machine learning techniques can yield exposure effect estimators that have small bias even when one of the first-stage predictions does not converge to the truth. The resulting tests and confidence intervals are doubly robust. We also show that the proposed estimators may fail to be regular when only one nuisance parameter is consistently estimated; nevertheless, we observe in simulation studies that our proposal can lead to reduced bias and improved confidence interval coverage in moderate-to-large samples.} }
@inproceedings{10.1109/ICSE-Companion66252.2025.00076, title = {The Balancing Act of Policies in Developing Machine Learning Explanations}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings}, pages = {237--238}, year = {2025}, isbn = {9798331536831}, doi = {10.1109/ICSE-Companion66252.2025.00076}, url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00076}, author = {Tjaden, Jacob}, abstract = {Due to the nature of opaque machine learning (ML) models, software engineers and data scientists struggle to understand how ML models make decisions [1]. Explainability research aims to provide transparency for these models [2] through two types of explanations. Global explanations describe how a model works generally and provide insight into its accuracy, biases, and fairness. Local explanations describe individual predictions made by the model in specific use cases.} }
@inbook{10.1145/3676641.3716249, title = {Relax: Composable Abstractions for End-to-End Dynamic Machine Learning}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2}, pages = {998--1013}, year = {2025}, isbn = {9798400710797}, url = {https://doi.org/10.1145/3676641.3716249}, author = {Lai, Ruihang and Shao, Junru and Feng, Siyuan and Lyubomirsky, Steven and Hou, Bohan and Lin, Wuwei and Ye, Zihao and Jin, Hongyi and Jin, Yuchen and Liu, Jiawei and Jin, Lesheng and Cai, Yaxing and Jiang, Ziheng and Wu, Yong and Park, Sunghyun and Srivastava, Prakalp and Roesch, Jared and Mowry, Todd C. and Chen, Tianqi}, abstract = {Dynamic shape computations have become critical in modern machine learning workloads, especially in emerging large language models. The success of these models has driven the demand for their universal deployment across a diverse set of backend environments. In this paper, we present Relax, a compiler abstraction for optimizing end-to-end dynamic machine learning workloads. Relax introduces a cross-level abstraction that encapsulates computational graphs, loop-level tensor programs, and external library calls in a single representation. Relax also introduces first-class symbolic shape annotations to track dynamic shape computations globally across the program, enabling dynamic shape-aware cross-level optimizations. We build an end-to-end compilation framework using the proposed approach to optimize dynamic shape models. Experimental results on LLMs show that Relax delivers performance competitive with state-of-the-art systems across various GPUs and enables deployment of emerging models to a broader set of emerging environments, including mobile phones, embedded devices, and web browsers.} }
@inproceedings{10.1145/3712255.3716519, title = {Evolutionary Art and Design in the Machine Learning Era}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1207--1247}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716519}, url = {https://doi.org/10.1145/3712255.3716519}, author = {Machado, Penousal and Correia, Jo\~ao} }
@article{10.14778/3750601.3750695, title = {Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {5460--5465}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3750601.3750695}, url = {https://doi.org/10.14778/3750601.3750695}, author = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Adhikari, Saugat}, abstract = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand.In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems. Slides are available at https://github.com/akhlaqueak/VLDB-2025-Tutorial.} }
@inproceedings{10.1145/3679240.3734623, title = {Probabilistic and Explainable Machine Learning for Tabular Power Grid Data}, booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems}, pages = {213--231}, year = {2025}, isbn = {9798400711251}, doi = {10.1145/3679240.3734623}, url = {https://doi.org/10.1145/3679240.3734623}, author = {Nikoltchovska, Alexandra and P\"utz, Sebastian and Li, Xiao and Hagenmeyer, Veit and Sch\"afer, Benjamin}, keywords = {power grid frequency stability, probabilistic machine learning, explainable artificial intelligence, tabular data, deep learning, TabNetProba}, abstract = {Modeling power grid frequency stability is becoming increasingly challenging due to the integration of renewable energy sources. Machine learning approaches, such as gradient-boosted trees, have shown promise in analyzing the complex characteristics of power systems. However, these models are inherently deterministic, providing only point estimates. Meanwhile, the task of capturing the underlying uncertainty, particularly through (deep) probabilistic models, is still underexplored, despite its potential to better account for the stochastic nature of power grid dynamics. In this paper, we first compare the performance of TabNet, a deep learning architecture designed for tabular data, to XGBoost for modeling power grid frequency stability. We then present TabNetProba: a probabilistic extension of TabNet, that enables uncertainty-aware estimates comparable to NGBoost. Using these (trained) models, we leverage explainable artificial intelligence (XAI) to analyze the drivers influencing grid stability and identify sources of uncertainty in two major European synchronous areas: Continental Europe and the Nordic region. Our results demonstrate that TabNetProba achieves competitive performance with state-of-the-art methods while providing reliable uncertainty estimates. We find that load and conventional generation ramps, as well as forecast errors, are the key quantities for modeling and explaining mean stability indicators in both synchronous areas. In Continental Europe, renewable generation emerges as a key factor in explaining model uncertainty, while in the Nordic region, load and generation features dominate uncertainty estimation, allowing for more reliable and interpretable stability estimates for modern power systems.} }
@article{10.1145/3687230.3687232, title = {Planter: Rapid Prototyping of In-Network Machine Learning Inference}, journal = {SIGCOMM Comput. Commun. Rev.}, volume = {54}, pages = {2--21}, year = {2024}, issn = {0146-4833}, doi = {10.1145/3687230.3687232}, url = {https://doi.org/10.1145/3687230.3687232}, author = {Zheng, Changgang and Zang, Mingyuan and Hong, Xinpeng and Perreault, Liam and Bensoussane, Riyad and Vargaftik, Shay and Ben-Itzhak, Yaniv and Zilberman, Noa}, keywords = {P4, dimension reduction, in-network computing, machine learning, machine learning compilers, modular framework, programmable switches}, abstract = {In-network machine learning inference provides high throughput and low latency. It is ideally located within the network, power efficient, and improves applications' performance. Despite its advantages, the bar to in-network machine learning research is high, requiring significant expertise in programmable data planes, in addition to knowledge of machine learning and the application area. Existing solutions are mostly one-time efforts, hard to reproduce, change, or port across platforms. In this paper, we present Planter: a modular and efficient open-source framework for rapid prototyping of in-network machine learning models across a range of platforms and pipeline architectures. By identifying general mapping methodologies for machine learning algorithms, Planter introduces new machine learning mappings and improves existing ones. It provides users with several example use cases and supports different datasets, and was already extended by users to new fields and applications. Our evaluation shows that Planter improves machine learning performance compared with previous model-tailored works, while significantly reducing resource consumption and co-existing with network functionality. Planter-supported algorithms run at line rate on unmodified commodity hardware, providing billions of inference decisions per second.} }
@article{10.1145/3736579, title = {Machine Learning-Assisted VCD Processing for Accelerated Dynamic Voltage Drop Analysis}, journal = {ACM Trans. Des. Autom. Electron. Syst.}, year = {2025}, issn = {1084-4309}, doi = {10.1145/3736579}, url = {https://doi.org/10.1145/3736579}, author = {Hu, Jingchao and Chen, Yufei and Sun, Songyu and Song, Jianfei and Zhang, Li and Yin, Xunzhao and Jin, Zhou and Zhuo, Cheng}, keywords = {VCD, Machine learning, Power supply, Noise, Profiling}, abstract = {With escalating power integrity challenges in advanced technologies, acquiring accurate dynamic power supply noise through Dynamic Voltage Drop (DVD) analysis becomes increasingly demanding. As noise margins shrink, the use of Value Change Dump (VCD) files for precise DVD analysis is indispensable but computationally expensive. Furthermore, the substantial storage requirements of VCD files, which record digital waveforms from logical simulations, pose significant challenges. In this paper, we propose a machine learning (ML)-assisted VCD processing framework to accelerate DVD analysis and improve data efficiency. Transitions recorded in VCD files are mapped to a Physical Design-Aware Circuit Hierarchy Tree (CHT) for efficient feature extraction. These features are leveraged by an XGBoost-based predictor to identify critical vector time windows within the VCD, significantly reducing simulation complexity. Additionally, Huffman encoding is applied to compress signal names, further optimizing storage utilization. Experimental results show that DVD analysis using our profiled VCD files achieves a speedup of approximately 3.53 with an error margin of only 3.89\%.} }
@inproceedings{10.1145/3708036.3708177, title = {Machine Learning Based Supply Chain Risk Prediction and Management}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {842--847}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708177}, url = {https://doi.org/10.1145/3708036.3708177}, author = {Li, Changxia and Chen, Haolin}, keywords = {machine learning, predictive parsing, security, supply chain risk management}, abstract = {Traditional supply chain risk management methods show limitations when facing unexpected events and complex supply chain networks, however, machine learning techniques that can handle unstructured and multi-dimensional data and extract valuable risk information from complex data sets through efficient algorithms have provided new ideas for supply chain management in recent years. This paper proposes a continuous multi-perspective conceptual framework that aims to systematically explore how machine learning can be effectively applied in supply chain risk prediction and management. Meanwhile, this paper will also investigate the potential contribution of utilizing innovative technologies such as blockchain in enhancing supply chain security and competitiveness, with a view to providing more comprehensive and profound insights into modern supply chain management.} }
@inproceedings{10.1145/3637528.3671478, title = {The Third Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6714--6715}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671478}, url = {https://doi.org/10.1145/3637528.3671478}, author = {Goldenberg, Dmitri and Meir Lador, Shir and Sokolova, Elena and Cheong, Lin Lee and Sukhwani, Mohak and Potdar, Saloni}, keywords = {data science management, genai and compliance, machine learning management, ml product development, location = Barcelona, Spain}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions,including the emerging field of General AI (GenAI), has created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Third KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@inproceedings{10.1145/3711896.3737201, title = {Chasing the Timber Trail: Machine Learning to Reveal Harvest Location Misrepresentation}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {4796--4805}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737201}, url = {https://doi.org/10.1145/3711896.3737201}, author = {Sarkar, Shailik and Yousuf, Raquib Bin and Wang, Linhan and Mayer, Brian and Mortier, Thomas and Deklerck, Victor and Truszkowski, Jakub and Simeone, John C. and Norman, Marigold and Saunders, Jade and Lu, Chang-Tien and Ramakrishnan, Naren}, keywords = {Gaussian processes, ML applications, multitask learning, stable isotope ratio analysis (SIRA), uncertainty estimation, location = Toronto ON, Canada}, abstract = {Illegal logging poses a significant threat to global biodiversity, climate stability, and depresses international prices for legal wood harvesting and responsible forest products trade, affecting livelihoods and communities across the globe. Stable isotope ratio analysis (SIRA) is rapidly becoming an important tool for determining the harvest location of traded, organic, products. The spatial pattern in stable isotope ratio values depends on factors such as atmospheric and environmental conditions and can thus be used for geographic origin identification. We present here the results of a deployed machine learning pipeline where we leverage both isotope values and atmospheric variables to determine timber harvest location. Additionally, the pipeline incorporates uncertainty estimation to facilitate the interpretation of harvest location determination for analysts. We present our experiments on a collection of oak (Quercus spp.) tree samples from its global range. Our pipeline outperforms comparable state-of-the-art models determining geographic harvest origin of commercially traded wood products, and has been used by European enforcement agencies to identify harvest location misrepresentation. We also identify opportunities for further advancement of our framework and how it can be generalized to help identify the origin of falsely labeled organic products throughout the supply chain.} }
@inproceedings{10.1145/3722212.3724483, title = {Ninth Workshop on Data Management for End-to-End Machine Learning (DEEM)}, booktitle = {Companion of the 2025 International Conference on Management of Data}, pages = {878--879}, year = {2025}, isbn = {9798400715648}, doi = {10.1145/3722212.3724483}, url = {https://doi.org/10.1145/3722212.3724483}, author = {Grafberger, Stefan and Hulsebos, Madelon and Interlandi, Matteo and Shankar, Shreya}, keywords = {data management, machine learning, systems, location = Berlin, Germany}, abstract = {The DEEM'25 workshop (Data Management for End-to-End Machine Learning) is held on Friday, June 27th, in conjunction with SIGMOD/PODS 2025. DEEM brings together researchers and practitioners at the intersection of applied machine learning, data management, and systems research, with the goal of discussing the arising data management issues in ML application scenarios. The workshop solicits regular research papers (8 pages) describing preliminary and ongoing research results, including industrial experience reports of end-to-end ML deployments, related to DEEM topics. In addition, DEEM 2025 has a category for short papers (4 pages) as a forum for sharing interesting use cases, problems, datasets, benchmarks, visionary ideas, system designs, preliminary results, and descriptions of system components and tools related to end-to-end ML pipelines. This year, the workshop received 18 high-quality submissions on diverse topics relevant to DEEM.} }
@article{10.5555/3648699.3649032, title = {Weisfeiler and Leman go machine learning: the story so far}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten}, keywords = {machine learning for graphs, graph neural networks, Weisfeiler-Leman algorithm, expressivity, equivariance}, abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.} }
@inproceedings{10.1145/3757110.3757209, title = {Research on Machine Learning-Based Prediction Models for Liver Cirrhosis Complicated by Hepatocellular Carcinoma}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {591--597}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757209}, url = {https://doi.org/10.1145/3757110.3757209}, author = {Wang, Weice and Zhang, Xingchen and Chen, Hongyang and You, Linqiang and Shen, Yufeng and Zhang, Han and Lin, Yuxuan and Huang, Yuting and Yang, Changping and Huang, Zhihui}, keywords = {Hepatocellular carcinoma, Machine learning-based prediction models, Regression modeling, SVM}, abstract = {This study aims to develop machine learning-based prediction models for hepatocellular carcinoma (HCC) development in liver cirrhosis patients, analyze potential influencing factors, and evaluate the impact of body weight through regression modeling, thereby providing evidence for clinical prevention and individualized treatment strategies. Utilizing clinical data from 419 cirrhosis patients at a specialized hospital in Fuzhou, we systematically compared the performance of Logistic Regression, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Genetic Algorithm in model construction to optimize prediction accuracy and generalization capability. The results demonstrate that the SVM model achieved superior performance in HCC prediction, with an overall accuracy of 93.6\%, precision of 90.1\%, recall of 98.0\%, and F1-score of 93.9\%. The ANN model also exhibited strong predictive capability, showing training and test set AUC values of 0.86857 and 0.79826 respectively. In contrast, Logistic Regression underperformed in all evaluation metrics compared to SVM and ANN. Notably, the SVM model significantly enhanced screening sensitivity for high-risk HCC cases while demonstrating robustness against sample imbalance. This study successfully established and validated HCC prediction models based on SVM, Logistic Regression, and ANN, with particular emphasis on the superior predictive performance of the SVM framework. These comparative findings provide clinicians with enhanced tools for precise HCC risk assessment, facilitating early identification of high-risk patients and advancing the development of personalized therapeutic strategies.} }
@inproceedings{10.1145/3670474.3685943, title = {Enhancing the Capabilities of Quantum Transport Simulations Utilizing Machine Learning Strategies}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685943}, url = {https://doi.org/10.1145/3670474.3685943}, author = {Naseer, Ateeb and Zarkob, Yawar Hayat and Rafiq, Musaib and Nazir, Mohammad Sajid and Ahmad, Owais and Agarwal, Amit and Bhowmick, Somnath and Chauhan, Yogesh Singh}, keywords = {Density functional theory (DFT), NanoTCAD, machine learning (ML), maximally localized Wannier functions (MLWFs), nonequilibrium Green's function (NEGF), novel 2-D materials, quantum transport simulations, location = Salt Lake City, UT, USA}, abstract = {The ongoing pursuit of exploring novel materials for potential future device applications continues to strengthen the vital role of Technology Computer-Aided Design (TCAD) simulations within the device community. However, its computationally intensive and time-consuming nature necessitates novel methodologies to overcome the limitations. Machine learning (ML) is a potential remedy in our contemporary data-driven society. In this work, we present a unique approach based on Neural Networks (NNs) to generate an efficient potential profile guess. The predicted ML potential enhances the convergence of the coupled Schrodinger and Poisson equation and speeds up the simulation process, ~ 2.5x, by reducing the number of self-consistent iterations at each bias point. Moreover, our method demonstrates versatility by providing reasonable prediction capacity and accuracy for grid reduction, doping, and channel material variations.} }
@inproceedings{10.1145/3735014.3735894, title = {Graduate Enrollment Information Management System Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {173--177}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735894}, url = {https://doi.org/10.1145/3735014.3735894}, author = {Li, Kainan and Jiang, Hailin}, keywords = {Enrollment information, Graduate student, Machine learning, Management system}, abstract = {Graduate enrollment and admission work are crucial to the higher education system and essential for maintaining academic heritage and innovation. With the rapid advancement of information technology (IT), especially as China's education sector enters a new stage of comprehensive digital transformation, universities face new development opportunities and severe challenges. The traditional enrollment information management system has become outdated regarding processing speed, information security, and intelligent decision-making assistance. It cannot meet the current situation of expanding graduate enrollment scale, complex processes, and diversified needs of candidates. Therefore, this article proposes an innovative design scheme for a graduate enrollment information management system based on machine learning (ML) technology. This solution integrates advanced data mining (DM), natural language processing (NLP), and predictive analysis algorithms to achieve automated collection, intelligent screening, precise matching, and efficient operation of enrollment information. Experimental results have shown that the system effectively improves the efficiency and quality of graduate enrollment work.} }
@inproceedings{10.1145/3626203.3670522, title = {Adversarial Robustness and Explainability of Machine Learning Models}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670522}, url = {https://doi.org/10.1145/3626203.3670522}, author = {Gafur, Jamil and Goddard, Steve and Lai, William}, keywords = {Framework, Machine Learning, eXplainable AI, packaging, location = Providence, RI, USA}, abstract = {The rapid advancement of machine learning has brought forth sophisticated neural network models harnessing computational prowess and vast datasets for diverse applications. Nonetheless, with the proliferation of these complex models, apprehensions have surfaced regarding their resilience, interpretability, and biases. To mitigate these concerns, we propose the “Adversarial Observation” framework, amalgamating explainable and adversarial methodologies for comprehensive neural network scrutiny. By integrating explainable techniques, users gain profound insights into the model’s internal mechanisms, fostering transparency and facilitating bias identification. This framework aims to enhance the trustworthiness and accountability of neural network systems amidst their expanding utility.} }
@inproceedings{10.1145/3681778.3698784, title = {Rail transit delay forecasting with Causal Machine Learning}, booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Spatiotemporal Causal Analysis}, pages = {1--10}, year = {2024}, isbn = {9798400711541}, doi = {10.1145/3681778.3698784}, url = {https://doi.org/10.1145/3681778.3698784}, author = {Srivastava, Nishtha and Gohil, Bhavesh N. and Ray, Suprio}, keywords = {Arrival time prediction, Average Treatment Effect, Individual Treatment Effect, Public transport, Rail transit, causal ML, location = Atlanta, GA, USA}, abstract = {The rapid evolution of public transport and advances in analytics have significantly transformed the way we enhance transit services. Rail transit systems, celebrated for their comfort, speed, and minimal environmental impact, face ongoing challenges due to persistent delays. We introduce a novel approach that integrates causal inference with machine learning techniques to predict rail transit delays and uncover key causal factors. Utilizing the New Jersey Transit dataset, we apply uplift modeling and causal inference methods to enhance delay predictions. The study employs Individual Treatment Effect (ITE) and Average Treatment Effect (ATE) metrics to interpret and validate the predictions. Our research offers a comprehensive understanding of rail transit delays and provides actionable insights for policymakers, urban planners, and public health officials. By advancing causal analytical techniques, this work aims to improve transit reliability and efficiency on a global scale.} }
@inproceedings{10.1145/3701047.3701072, title = {Research on Key Technologies and Applications of Recommendation System Based on Machine Learning}, booktitle = {Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, pages = {136--141}, year = {2025}, isbn = {9798400711688}, doi = {10.1145/3701047.3701072}, url = {https://doi.org/10.1145/3701047.3701072}, author = {Liu, Dan and Sun, Yuan and Hao, Jianmin and Fu, Lin}, keywords = {hybrid recommendation algorithm model, key technologies, machine learning, recommendation system}, abstract = {In the current highly information-based and digital social environment, numerous technical applications are constantly enriching the sources and communication channels of network information data, so that users are facing the problems of information explosion and information overload while enjoying the convenience of information acquisition. Therefore, the emergence and development of recommendation system has become the only way to solve this problem. However, the conventional recommendation system still has obvious lag when dealing with data sparsity and cold start, and it is difficult to achieve accurate recommendation. Based on the actual application requirements, this paper will comprehensively explore the feasibility of machine learning algorithm in the field of recommendation system, and propose a set of recommendation algorithm construction scheme based on machine learning to make up for the shortcomings of the existing recommendation system. Practice has proved that the hybrid recommendation algorithm model based on decision tree (DT), random forest (RF) and logistic regression (LR) can automatically combine and screen features, mine valuable feature information from sparse data, and use these feature information to make efficient classification prediction, and then generate more comprehensive and accurate recommendation results, so as to improve user satisfaction and the performance of recommendation system.} }
@inproceedings{10.1145/3716554.3716616, title = {Machine Learning-Based Geometric Interpolation for Fluid-Flow Modelling}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {407--412}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716616}, url = {https://doi.org/10.1145/3716554.3716616}, author = {Aravanis, Theofanis and Chrimatopoulos, Grigorios and Xenos, Michalis and Tzirtzilakis, Efstratios}, keywords = {Computational Fluid Dynamics, Two-Dimensional Fluid Flow, Artificial Neural Networks, Machine Learning}, abstract = {Modelling fluid flows accurately is essential in engineering and science, but it often depends on solving complex Partial Differential Equations (PDEs), which can be computationally demanding. In this study, we leverage Artificial Neural Networks (ANNs) as efficient surrogates for traditional numerical methods, predicting fluid-flow characteristics with significantly reduced computational cost. Building on our recent previous work, we extend our analysis to explore geometric interpolation within a two-dimensional (2D) channel. While we investigate interpolation with respect to the geometry of the fluid-flow problem, we explore extrapolation in terms of Reynolds numbers, thereby validating our Machine-Learning model on fluid flows with Reynolds numbers beyond the training range. The obtained results demonstrate that while training on finer grids yields high accuracy, training on coarser grids still achieves comparable performance with minimal computational effort; this outcome highlights in turn the promise of ANNs for efficient modelling of fluid dynamics. Such capabilities are particularly valuable in contemporary applications such as digital twins, enabling real-time system monitoring and optimization.} }
@inproceedings{10.1145/3640824.3640839, title = {Artificial Aesthetics: Bridging Neuroaesthetics and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Control Engineering and Artificial Intelligence}, pages = {98--101}, year = {2024}, isbn = {9798400707971}, doi = {10.1145/3640824.3640839}, url = {https://doi.org/10.1145/3640824.3640839}, author = {Liang, Ting-Wen and Lau, Bee Theng and White, David and Barron, Deirdre}, keywords = {Aesthetic preference, Deep Learning, Design Object, Machine Learning, Neural Network algorithm, location = Shanghai, China}, abstract = {This paper presents an innovative exploration of neuroscience, aesthetics, and artificial intelligence. This paper discusses the potential of machine learning in enhancing our understanding of the neural underpinnings of aesthetic experiences and artistic creation. Neuroaesthetics seeks to unravel the cerebral processes involved in art perception and emotional engagement. Integrating these insights with the capabilities of advanced ML models, particularly those inspired by human brain architecture, opens new avenues for analyzing and generating art. This interdisciplinary approach leverages neural network algorithms to mimic and extrapolate human aesthetic preferences and interpretations. Our research employs deep machine learning techniques to analyze electroencephalogram data, categorized based on aesthetic preferences. The datasets from prior research studies provide data for examining the neural correlates of aesthetic judgment and experience. By leveraging machine learning algorithms, we uncover intricate patterns within the EEG readings that correlate with participants' aesthetic preferences, thereby deepening our understanding of the neural mechanisms underlying aesthetic appreciation for design objects.} }
@inproceedings{10.1145/3711542.3711555, title = {Sarcasm Detection of Facebook's Posts Using Machine Learning Models}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {349--354}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711555}, url = {https://doi.org/10.1145/3711542.3711555}, author = {Chiu, Shu-i and Jhou, Ting-Wei}, keywords = {Sarcasm, deep learning, machine learning, social media}, abstract = {The coronavirus disease 2019 (COVID-19) has brought massive challenges to the world, altering people's lives. We conducted a study on people's mental states during Taiwan's National Epidemic Level 3 Alert in 2021. On May 22, 2021, during a regular press conference held by the Taiwan Centers for Disease Control (CDC), the Minister of Health and Welfare introduced the term 'Retrospective Adjustment', which left the entire population in shock. Our approach consists of a two-stage task. Constructing the model is the first stage and detecting sarcasm is the second. First, we integrated TCNN\&nbsp;and BiLSTM models. Second, we focused on misclassified by the model and performed feature engineering based on these misclassified data. After constructing features, we performed well using machine learning models. Finally, the experimental results show that our approach performs well in detecting sarcasm using linguistic and lexical-based features. In the second stage, the LSTM model detects sarcasm, achieving a performance of 0.73. We integrate the results of two stages to adjust accuracy. By improving the accuracy of the misclassified data to 0.6, the overall accuracy for negative posts has increased to 0.76.} }
@inbook{10.1145/3729706.3729742, title = {Using Explainable Machine Learning to Predict Loan Risk in Consumer Finance}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {235--238}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729742}, author = {Zhang, Lixin}, abstract = {The high default rate of consumer finance loans poses a significant challenge in predicting credit risk. This study presents a framework for predicting loan risk by leveraging the strengths of machine learning models. Compared to logistic regression models, LightGBM and CatBoost models demonstrate superior predictive performance. Furthermore, model fusion techniques contribute to improved performance over individual approaches. To improve the interpretability of the predictions, we use Shapley Additive Explanations (SHAP), which clarify the impact of each feature on the model's outputs. Key risk factors identified include gender, days past due, income type, late payment percentage, contract start date, total loan payments, and outstanding amount. By combining the strong predictive capabilities of LightGBM and CatBoost with the transparency provided by SHAP, this study aims to build trust in machine learning applications within the consumer finance sector. Our findings offer valuable recommendations for improving credit risk management practices, ultimately contributing to more reliable and transparent decision-making processes in consumer finance loan assessments.} }
@inproceedings{10.1145/3655693.3661296, title = {Towards Access Control for Machine Learning Embeddings}, booktitle = {Proceedings of the 2024 European Interdisciplinary Cybersecurity Conference}, pages = {219--220}, year = {2024}, isbn = {9798400716515}, doi = {10.1145/3655693.3661296}, url = {https://doi.org/10.1145/3655693.3661296}, author = {Matzutt, Roman}, keywords = {Attribute-based encryption, access control, embeddings, location = Xanthi, Greece}, abstract = {In this work, we explore the potential to make embeddings, which are becoming an integral part of machine-learning pipelines, shareable with the general public while providing self-contained access control. To this end, we apply attribute-based encryption and discuss a potential application for supply chain management.} }
@inproceedings{10.1145/3759179.3760446, title = {Explainable Machine Learning for Detecting Malicious Student Behavior in Campus Networks}, booktitle = {Proceedings of the 10th International Conference on Cyber Security and Information Engineering}, pages = {299--305}, year = {2025}, isbn = {9798400718632}, doi = {10.1145/3759179.3760446}, url = {https://doi.org/10.1145/3759179.3760446}, author = {Yan, Qi}, keywords = {Anomaly Detection, Campus Network Security, Explainable Machine Learning, LIME, LSTM, Malicious Behavior Detection, SHAP, XGBoost}, abstract = {As digital infrastructure in higher education expands, campus networks face increasing threats from malicious student behaviors such as unauthorized resource access and exam-related cheating. While machine learning models have shown promise in anomaly detection, their lack of interpretability undermines trust and limits deployment in sensitive academic environments. This study proposes a hybrid explainable artificial intelligence (XAI) framework that integrates Extreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) for behavior classification, enhanced by SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) for global and local interpretability. Tested on over 2.3 million real-world campus network sessions, the system achieves an F1-score of 88.7\% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 93.1\%, while increasing administrator trust scores by 38\%. A live deployment during exam periods further demonstrates its practical value, reducing false positives to 10.1\%, cutting average investigation time by 50\%, and supporting proportional policy enforcement. The results highlight the operational, ethical, and governance benefits of embedding explainability into campus cybersecurity systems.} }
@article{10.1145/3773084, title = {Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3773084}, url = {https://doi.org/10.1145/3773084}, author = {Gu, Yang and You, Hengyu and Cao, Jian and Yu, Muran and Fan, Haoran and Qian, Shiyou}, keywords = {Machine Learning Workflows, Large Language Models, Software Engineering, AutoML, Survey}, abstract = {Machine Learning (ML) workflows—spanning data preprocessing and feature engineering, model selection and hyperparameter optimization, and workflow evaluation—are increasingly embedded in complex software systems. Building these workflows manually demands substantial ML expertise, domain knowledge, and engineering effort. Automated ML (AutoML) frameworks address parts of this challenge but often suffer from constrained search spaces, limited adaptability, and low interpretability. Recent advances in Large Language Models (LLMs) have opened new opportunities to automate and enhance ML workflows by leveraging their capabilities in language understanding, reasoning, interaction, and code generation, posing new practical and theoretical challenges for software engineering (SE). This survey provides the first SE-oriented, stage-wise review of LLM-based ML workflow automation. We introduce a taxonomy covering all three workflow stages, systematically compare and analyze state-of-the-art methods, and synthesize both stage-specific and cross-stage trends. Our analysis yields SE-oriented implications, including the need for robust verification, quality management, context-aware deployment, and risk mitigation, alongside ensuring key quality attributes such as usability, modularity, traceability, and performance. The findings also call for adapting development models, rethinking lifecycle boundaries, and formalizing uncertainty handling to address the probabilistic and collaborative nature of LLM-assisted workflow generation. We further identify major open challenges and outline future research directions to guide the reliable and effective adoption of LLMs in ML workflow development. Our artifacts are publicly available at .} }
@inproceedings{10.1145/3637528.3672068, title = {EcoVal: An Efficient Data Valuation Framework for Machine Learning}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {2866--2875}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3672068}, url = {https://doi.org/10.1145/3637528.3672068}, author = {Tarun, Ayush and Chundawat, Vikram and Mandal, Murari and Tan, Hong Ming and Chen, Bowei and Kankanhalli, Mohan}, keywords = {data valuation, machine learning, shapley value, location = Barcelona, Spain}, abstract = {Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall value of the data can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as aproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. The code is available at https://github.com/respai-lab/ecoval.} }
@proceedings{10.1145/3747227, title = {MLNN '25: Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, year = {2025}, isbn = {9798400714382} }
@inproceedings{10.1145/3698205.3733941, title = {Digital Cognitive Apprenticeship: Scaling Rigorous Machine Learning Education in High Schools}, booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale}, pages = {286--290}, year = {2025}, isbn = {9798400712913}, doi = {10.1145/3698205.3733941}, url = {https://doi.org/10.1145/3698205.3733941}, author = {Perach, Shai and Alexandron, Giora}, keywords = {blended learning, cdlr-centered framework, cognitive apprenticeship, digital learning resources, educational scaling, high school stem, machine learning education, professional content integration, teacher capacity, location = Palermo, Italy}, abstract = {Current initiatives to introduce machine learning (ML) in high schools typically rely on simplified content tthat avoids mathematical foundations, creating a significant gap between secondary and advanced ML education. We present a fundamentally different approach to scaling rigorous ML education: a blended learning framework enabling advanced STEM track students to learn directly from professional ML resources while addressing the critical challenge of teaching capacity limitations. Our framework strategically positions prominent Contemporary Digital Learning Resources (CDLRs) --- high-quality resources created by professionals for adult learners --- at the center of instruction, redistributing educational responsibilities among teachers, students, and digital resources to transform classroom dynamics. Our framework addresses a critical scaling challenge in ML education: the shortage of teachers with ML expertise. By drawing on Cognitive Apprenticeship, Community of Inquiry, and Self-Efficacy theories, we create structured learning environments that potentially extend apprenticeship models beyond direct expert-novice relationships. Initial implementation across six high schools shows promising results, with external evaluation revealing substantial student achievement and high completion rates. Student data indicates a consistent preference for professional resources over simplified alternatives, suggesting the value of authentic disciplinary engagement for such science-track students when adequately supported. Our ongoing work focuses on developing empirical measures to evaluate how effectively digital resources implement cognitive apprenticeship elements and identifying critical factors affecting successful implementation. Our work contributes to theoretical understanding of cognitive apprenticeship in digital environments and provides empirical evidence for redistributing educational roles around professional learning resources, with implications that extend beyond ML education to other rapidly evolving technical fields where traditional teacher preparation may prove insufficient.} }
@inproceedings{10.1145/3600211.3604689, title = {Machine Learning practices and infrastructures}, booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {466--481}, year = {2023}, isbn = {9798400702310}, doi = {10.1145/3600211.3604689}, url = {https://doi.org/10.1145/3600211.3604689}, author = {Berman, Glen}, keywords = {infrastructure studies, machine learning, social practice, location = Montr\'eal, QC, Canada}, abstract = {Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers’ have demonstrated to be particularly salient for the societal impact of deployed ML systems.} }
@inproceedings{10.1145/3626246.3653389, title = {The Hopsworks Feature Store for Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {135--147}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3653389}, url = {https://doi.org/10.1145/3626246.3653389}, author = {de la R\'ua Mart\'nez, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr\"om, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim}, keywords = {arrow flight, duckdb, feature store, mlops, rondb, location = Santiago AA, Chile}, abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference. In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.} }
@inproceedings{10.1145/3716554.3716563, title = {An Intelligent Chatbot in Greek Using Machine Learning Technology}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {57--62}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716563}, url = {https://doi.org/10.1145/3716554.3716563}, author = {Nikologiannis, Orestis and Tsampos, Ioannis and Marakakis, Emmanouil}, keywords = {CCS CONCEPTS • Artificial Intelligence, Machine Learning, Natural Language Processing}, abstract = {In this paper, we present the development of a sophisticated conversational chatbot for querying and answering in Greek using machine learning technology. We have developed an interactive dialogue-based assistant which provides answers in Greek by processing questions in Greek related to an academic domain. The aim of this research work is the development of a conversational framework which will be domain and language independent and its users would be able to use their natural language to communicate and retrieve information from a problem domain. We have used the state-of-the-art Rasa framework which supports conversational Artificial Intelligence. Rasa is a suitable tool for handling natural conversations between a user and a computer, not only because of its ability to detect the intent of its user, but also because it can trigger actions based on the specific user's intent. The key tools for the development of our system are Rasa for natural language understanding, Python for scripting custom actions and Neo4j for efficient data storage and retrieval. In this research work, we have used a university application domain moreover our approach can be used in other problem domains as well. So, an advantage of our approach is its independence from the application domain. Our approach can be used in other languages apart the Greek one which has been used as test language. So, another advantage is the language independence of our approach.} }
@inproceedings{10.1145/3724154.3724247, title = {Bankruptcy Prediction of Listed Companies Based on Machine Learning Model}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {563--567}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724247}, url = {https://doi.org/10.1145/3724154.3724247}, author = {Zhang, Rongqiang and Cao, Qilong and Wei, Haohao and Sun, Xin}, keywords = {Corporate bankruptcy risk assessment, Machine learning, XGBoost}, abstract = {Machine learning has found extensive applications in the field of financial research, particularly in corporate finance. This study applies machine learning algorithms to predict corporate bankruptcy risk, employing the XGBoost algorithm to develop a predictive model for listed companies. The model's predictive outcomes are compared with those of logistic regression and random forest algorithms. Results demonstrate that XGBoost outperforms the alternatives in predicting bankruptcy risk, exhibiting significantly superior evaluation metrics. Further analysis identifies net cash flow from financing activities, shareholders' equity turnover, tangible net worth debt ratio, operating profit growth rate, and operating profit ratio as critical factors influencing bankruptcy risk.} }
@inproceedings{10.1145/3689236.3696268, title = {The Application of Machine Learning in Spam Filtering}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {258--262}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3696268}, url = {https://doi.org/10.1145/3689236.3696268}, author = {Liu, Xiangwei and Tang, Liang}, keywords = {feature extraction, machine learning method, natural language processing, python, spam filtering}, abstract = {With the wide application of the Internet, Spam is increasingly rampant, which seriously affects the normal communication and network environment of users. Aiming at this problem, this paper deeply studies the related technology of spam filter. Through the analysis of a large number of spam samples, a variety of effective features are extracted, such as keywords, word frequency, sentence structure and so on. Machine learning algorithms, such as logistic and naive Bayes, are used to construct efficient classification models. At the same time, the model is optimized and improved to improve the understanding and recognition accuracy of semantics. The experimental results show that the proposed filter has significant improvement in accuracy, recall rate and F1 value, which can effectively filter spam and provide users with a cleaner email environment, and has important practical application value.} }
@inproceedings{10.1145/3600100.3626271, title = {Thermal Preference Prediction with Machine Learning}, booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {303--304}, year = {2023}, isbn = {9798400702303}, doi = {10.1145/3600100.3626271}, url = {https://doi.org/10.1145/3600100.3626271}, author = {Odeyemi, Julianah and Streblow, Rita}, keywords = {Longitudinal dataset., Machine Learning, Personal Comfort Model, Thermal Preference, location = Istanbul, Turkey}, abstract = {This study aimed to develop a personal comfort model (PCM) using machine learning techniques on an open-access longitudinal dataset. Most prior studies on PCMs were based on controlled climate chamber data, which limits their generalisability to real-world settings. The study examined individual and ensemble classifiers for predicting thermal preference. The Support Vector Classifier (SVC) displayed strong predictive power with an accuracy of 0.843, as well as macro precision (0.724), recall (0.847), and F1 score (0.763). Similarly, the Extra Trees (ET) classifier achieved the highest accuracy of 0.924, with macro precision (0.865), recall (0.915), and F1 score (0.887). Overall, ensemble methods such as Extra Trees, Extreme Gradient Boost, and SVC as an individual model proved effective in predicting thermal preference.} }
@inproceedings{10.1145/3658617.3703638, title = {Invited Paper: Boosting Standard Cell Library Characterization with Machine Learning}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {385--391}, year = {2025}, isbn = {9798400706356}, doi = {10.1145/3658617.3703638}, url = {https://doi.org/10.1145/3658617.3703638}, author = {Chen, Zhengrui and Guo, Chengjun and Song, Zixuan and Feng, Guozhu and Wang, Shizhang and Zhang, Li and Yin, Xunzhao and Wu, Zhenhua and Yan, Zheyu and Zhuo, Cheng}, keywords = {standard cell, library characterization, machine learning, location = Tokyo, Japan}, abstract = {As VLSI designs grow more complex and transition to smaller process nodes, accurate and efficient library characterization has become increasingly crucial within DTCO and STCO flows. Current open-source tools, however, are constrained to basic library characterization functions and fail to adequately meet modern design demands. In this paper, we review the existing open-source standard cell characterization tools, summarize their limitations, and introduce ZlibBoost---a new open-source framework designed to offer both flexibility and efficiency. We leverage ZlibBoost for LUT index optimization, dynamic power supply noise modeling, and machine learning-based prediction to enhance efficiency and accuracy in library characterization. Experimental results show that such a tool is helpful for both academia and industry to effectively navigate DTCO and STCO challenges.} }
@inproceedings{10.1145/3648115.3648123, title = {Accelerating Machine Learning Inference on GPUs with SYCL}, booktitle = {Proceedings of the 12th International Workshop on OpenCL and SYCL}, year = {2024}, isbn = {9798400717901}, doi = {10.1145/3648115.3648123}, url = {https://doi.org/10.1145/3648115.3648123}, author = {Panagou, Ioanna-Maria and Bellas, Nikolaos and Moneta, Lorenzo and Sengupta, Sanjiban}, keywords = {Deep Learning, GPU, Machine Learning, Parallel Computing, SYCL, location = Chicago, IL, USA}, abstract = {Recently, machine learning has established itself as a valuable tool for researchers to analyze their data and draw conclusions in various scientific fields, such as High Energy Physics (HEP). Commonly used machine learning libraries, such as Keras and PyTorch, might provide functionality for inference, but they only support their own models and are constrained by heavy dependencies, which render their deployment on embedded or bare-metal environments infeasible. SOFIE\&nbsp;[3], which stands for System for Optimized Fast Inference code Emit, a part of the ROOT project developed at CERN, creates standalone C++ inference code from an input model in one of the popular machine learning formats. This code is directly invokable from other C++ projects and has minimal dependencies. In this work, we extend the functionality of SOFIE to generate SYCL code for machine learning model inference that can run on various GPU platforms and is only dependent on Intel MKL BLAS and portBLAS libraries.} }
@inproceedings{10.1145/3732801.3732807, title = {Machine Learning-Based Academic Performance Prediction: A Comparative Empirical Analysis}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {27--32}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732807}, url = {https://doi.org/10.1145/3732801.3732807}, author = {Wu, Meng and Wei, Yi and Zhang, Yirong and Li, Cailing and Mei, Yelin and Subramaniam, Geetha}, keywords = {Academic Performance, AdaBoost Algorithm, Experimental Design, Machine Learning, Prediction Algorithm}, abstract = {In this study, we explore the role of various machine learning algorithms in predicting student academic performance from online behavior data while overcoming some limitations posed by traditional subjective assessments. The Decision Tree, Plain Bayes and AdaBoost algorithms were run against the dataset developed from hundreds of observations collected from software engineering students. From the evaluation, it is concluded that prediction accuracy using AdaBoost was 73\% which is improved significantly compared to other models. Results thus created give educators some data-driven tools for the early identification of academic risk and optimization of teaching strategies, which extend into a scalable framework for other disciplines in support of improved educational decision-making for successful student outcomes.} }
@article{10.1145/3716818, title = {Facial Expression Analysis in Parkinsons’s Disease Using Machine Learning: A Review}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3716818}, url = {https://doi.org/10.1145/3716818}, author = {Oliveira, Guilherme and Ngo, Quoc and Passos, Leandro and Jodas, Danilo and Papa, Joao and Kumar, Dinesh}, keywords = {Neurological disorders, hypomimia, Parkinson’s disease, facial expression, artificial intelligence, machine learning, deep learning}, abstract = {Computerised facial expression analysis is performed for a range of social and commercial applications and more recently its potential in medicine such as to detect Parkinson’s Disease (PD) is emerging. This has possibilities for use in telehealth and population screening. The advancement of facial expression analysis using machine learning is relatively recent, with a majority of the published work being post-2019. We have performed a systematic review of the English-based publication on the topic from 2019 to 2024 to capture the trends and identify research opportunities that will facilitate the translation of this technology for recognising Parkinson’s disease. The review shows significant advancements in the field, with facial expressions emerging as a potential biomarker for PD. Different machine learning models, from shallow to deep learning, could detect PD faces. However, the main limitation is the reliance on limited datasets. Furthermore, while significant progress has been made, model generalization must be tested before clinical applications.} }
@inproceedings{10.1145/3706598.3713524, title = {Perceptions of the Fairness Impacts of Multiplicity in Machine Learning}, booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, year = {2025}, isbn = {9798400713941}, doi = {10.1145/3706598.3713524}, url = {https://doi.org/10.1145/3706598.3713524}, author = {Meyer, Anna P. and Kim, Yea-Seul and D'Antoni, Loris and Albarghouthi, Aws}, keywords = {Fairness, Fairness Perceptions, Fairness in Machine Learning, Multiplicity, Stakeholder Survey}, abstract = {Machine learning (ML) is increasingly used in high-stakes settings, yet multiplicity – the existence of multiple good models – means that some predictions are essentially arbitrary. ML researchers and philosophers posit that multiplicity poses a fairness risk, but no studies have investigated whether stakeholders agree. In this work, we conduct a survey to see how multiplicity impacts lay stakeholders’ – i.e., decision subjects’ – perceptions of ML fairness, and which approaches to address multiplicity they prefer. We investigate how these perceptions are modulated by task characteristics (e.g., stakes and uncertainty). Survey respondents think that multiplicity threatens the fairness of model outcomes, but not the appropriateness of using the model, even though existing work suggests the opposite. Participants are strongly against resolving multiplicity by using a single model (effectively ignoring multiplicity) or by randomizing the outcomes. Our results indicate that model developers should be intentional about dealing with multiplicity in order to maintain fairness.} }
@inproceedings{10.1145/3711129.3711341, title = {Integrated Machine Learning for Enhanced Supply Chain Risk Prediction}, booktitle = {Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering}, pages = {1254--1259}, year = {2025}, isbn = {9798400710094}, doi = {10.1145/3711129.3711341}, url = {https://doi.org/10.1145/3711129.3711341}, author = {Jin, Tian}, keywords = {Supply chain risk, machine learning, integrated model, data preprocessing, prediction accuracy}, abstract = {Supply chain risk prediction has become increasingly critical as organizations navigate complex and volatile environments characterized by rapid market changes, geopolitical uncertainties, and supply disruptions. In this context, effective risk management is essential for maintaining operational efficiency and competitiveness. This study proposes an innovative integrated model that combines Random Forest, Gradient Boosting Machine (GBM), and Neural Networks to enhance prediction accuracy and reliability in supply chain risk assessment. By employing comprehensive data preprocessing techniques—such as missing value imputation, normalization, and anomaly detection—alongside advanced algorithmic strategies, the model effectively addresses the limitations of traditional approaches. The integration of these diverse machine learning techniques not only leverages their individual strengths but also enhances the model’s adaptability and robustness in varying scenarios.} }
@inproceedings{10.1145/3638530.3654405, title = {Benchmark Problems for Machine Learning in Control Synthesis}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {2123--2126}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3654405}, url = {https://doi.org/10.1145/3638530.3654405}, author = {Shmalko, Elizaveta and Diveev, Askhat}, keywords = {evolutionary machine learning, symbolic regression, control, unsupervised learning, location = Melbourne, VIC, Australia}, abstract = {Symbolic regression methods are becoming increasingly popular and they are a worthy competitor to neural networks in many tasks. However, symbolic regression methods have the greatest promise in those problems where there is no training sample and it is necessary to search for a solution to a machine learning problem only on the basis of some given general quality criterion. A wide class of such problems includes control synthesis problems. Machine learning control tasks consist in searching for a vector- function of control according to some criterion. As a rule, there is no training sample for such tasks, except to approximate the operator's actions. However, this approach is unlikely to satisfy the given criterion. Some symbolic regression methods have already shown good results in control problems. But the complexity of the tasks reveals the need to develop new approaches and modifications of existing methods. In this regard, a benchmark of machine learning control problems is proposed to test symbolic regression methods. Problem formulations and reference solutions are presented.} }
@inproceedings{10.1145/3750069.3757879, title = {Resilient AI Framework for Secure and Ethical Machine Learning Systems (RAISE) - 1st edition}, booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter}, year = {2025}, isbn = {9798400721021}, doi = {10.1145/3750069.3757879}, url = {https://doi.org/10.1145/3750069.3757879}, author = {Breve, Bernardo and Caruccio, Loredana and Polese, Giuseppe}, keywords = {Resilient AI, Human-Centered AI, Fairness and Accountability, Data Quality and Profiling, Verification and Validation}, abstract = {This document provides a summary of the First Workshop on ‘Resilient AI Framework for Secure and Ethical Machine Learning Systems’ (RAISE 2025), which has been accepted at the 6th Biannual Conference of the Italian SIGCHI Chapter (CHItaly 2025).} }
@inproceedings{10.1145/3734436.3734453, title = {Machine Learning in Access Control: A Taxonomy [Systematization of Knowledge Paper]}, booktitle = {Proceedings of the 30th ACM Symposium on Access Control Models and Technologies}, pages = {145--156}, year = {2025}, isbn = {9798400715037}, doi = {10.1145/3734436.3734453}, url = {https://doi.org/10.1145/3734436.3734453}, author = {Nobi, Mohammad Nur and Gupta, Maanak and Krishnan, Ram and Rana, Md Shohel and Praharaj, Lopamudra and Abdelsalam, Mahmoud}, keywords = {access control, mlbac, machine learning based access control, location = USA}, abstract = {Developing and managing access control systems is challenging due to the dynamic nature of users, resources, and environments. Recent advancements in machine learning (ML) offer promising solutions for automating the extraction of access control attributes, policy mining, verification, and decision-making. Despite these advancements, the application of ML in access control remains fragmented, resulting in an incomplete understanding of best practices. This work aims to systematize the use of ML in access control by identifying key components where ML can address various access control challenges. We propose a novel taxonomy of ML applications within this domain, highlighting current limitations such as the scarcity of public real-world datasets, the complexities of administering ML-based systems, and the opacity of ML model decisions. Additionally, we outline potential future research directions to guide both new and experienced researchers in effectively integrating ML into access control practices.} }
@inproceedings{10.1145/3757110.3757150, title = {Urban Tourism Hotel Recommendation Model Based on Geographic Spatial Machine Learning Algorithm and Spatial Route Planning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {236--241}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757150}, url = {https://doi.org/10.1145/3757110.3757150}, author = {Wang, Jingyi and Zhou, Xiao and Xian, Mengling and Pan, Juan}, keywords = {geographic spatial data, machine learning, spatial route planning, urban hotel recommendation}, abstract = {Based on the analysis of the cost of urban traveling and the constrained tourism demands of tourists, we construct an urban tourism hotel recommendation model based on geographic spatial machine learning algorithm and spatial route planning. Firstly, based on the constraints of urban geographic space, an improved DIANA clustering algorithm is constructed to spatially cluster the scenic spots in the city and obtain a local distribution model of the scenic spots. Secondly, taking the scenic spot cluster as the research scope, a spatial route planning algorithm is constructed to output tour routes starting and ending at a certain hotel, and the optimal complete binary tree algorithm is used to output the cost priority of each route, thereby obtaining the hotel recommendation priority. The experiment proves that the proposed algorithm can effectively output the clusters of scenic spots under geographic spatial constraints, and recommend hotel accommodation with lowest traveling cost for tourists within the clustering range, providing the decision support for tourists’ tourism planning.} }
@article{10.1145/3511299, title = {Interpretable Machine Learning: Moving from mythos to diagnostics}, journal = {Queue}, volume = {19}, pages = {28--56}, year = {2022}, issn = {1542-7730}, doi = {10.1145/3511299}, url = {https://doi.org/10.1145/3511299}, author = {Chen, Valerie and Li, Jeffrey and Kim, Joon Sik and Plumb, Gregory and Talwalkar, Ameet}, abstract = {The emergence of machine learning as a society-changing technology in the past decade has triggered concerns about people's inability to understand the reasoning of increasingly complex models. The field of IML (interpretable machine learning) grew out of these concerns, with the goal of empowering various stakeholders to tackle use cases, such as building trust in models, performing model debugging, and generally informing real human decision-making.} }
@inproceedings{10.1145/3747227.3747258, title = {Research on the Application of Machine Learning in the Cost Control of Enterprise R\&amp;D Projects}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {183--188}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747258}, url = {https://doi.org/10.1145/3747227.3747258}, author = {Liu, Xiu}, keywords = {BP neural network, Machine learning, predictive analysis model, project cost control}, abstract = {Under the background of global economic integration and increasingly fierce market competition, enterprises are facing enormous cost pressure. Among them, R\&amp;D project is the core driving force of enterprise innovation and development, and its cost control effect is directly related to the competitiveness and profitability of enterprises. However, in the face of complex and changeable market environment and technical conditions, the traditional cost control methods of R\&amp;D projects show insurmountable limitations and can no longer meet the practical application needs of enterprises. In this regard, based on the current problems, this paper deeply analyzes the application feasibility of artificial intelligence and machine learning technology in this field, and puts forward a brand-new cost control framework for enterprise R\&amp;D projects to improve the accuracy and efficiency of cost control. Practice has proved that this framework is based on BP neural network, and through learning and analyzing historical project cost data, it can obtain hidden rules and patterns in the data, provide more accurate prediction and decision support for enterprise's subsequent R\&amp;D project cost control, and enhance the flexibility and response speed of cost control.} }
@inproceedings{10.1145/3675888.3676055, title = {Interpretable Machine Learning Techniques for Students Grade Prediction}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {213--225}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676055}, url = {https://doi.org/10.1145/3675888.3676055}, author = {Josephine, Namakula and Cedric, Ahumuza and McDaniel, Ssemwanga Trevor and Kanagwa, Benjamin and Joseph, Tibakanya and Marvin, Ggaliwango}, abstract = {Predicting student grades within an academic cycle is a remarkable opportunity to improve academic results. Robust prediction methods such as use of machine learning allow educational leaders to allocate enough resources and personalized instructions where necessary, fostering improved outcomes in Academia. We have presented various machine-learning models that have helped predict students' grades as an early intervention to determine how well the models achieve the required objectives using a combination of input features and past data or grades. We obtained a student dataset with various demographics, economic, educational factors, and grades from different subjects with their corresponding averages where we came up with 15 models including decision trees, random forest, linear regression, k-nearest neighbor, AdaBoost, Gradient Boosting, Support Vector Machine XGBoost, Lasso, Ridge, Elasticnet and among Deep Neural Networks. Our findings show that Linear Regression based model is the best with a MAPE of 8.14 and R-squared of 0.2536, and the Graph neural networks performed worst with a MAPE of 47.2 and R-squared of -83.7. We achieve computational outcomes indicative of the model's predictive performance. Our results show promising accuracy and generalization capabilities and that these models can predict very well, meaning we can rely on them to figure out how students might perform in academia.} }
@inproceedings{10.1145/3642970.3655828, title = {The Environmental Cost of Engineering Machine Learning-Enabled Systems: A Mapping Study}, booktitle = {Proceedings of the 4th Workshop on Machine Learning and Systems}, pages = {200--207}, year = {2024}, isbn = {9798400705410}, doi = {10.1145/3642970.3655828}, url = {https://doi.org/10.1145/3642970.3655828}, author = {Chadli, Kouider and Botterweck, Goetz and Saber, Takfarinas}, keywords = {DevOps, Environmental Cost, MLOps, Machine Learning-Enabled Systems, Sustainability, location = Athens, Greece}, abstract = {The integration of Machine Learning (ML) across public and industrial sectors has become widespread, posing unique challenges in comparison to conventional software development methods throughout the lifecycle of ML-Enabled Systems. Particularly, with the rising importance of ML platforms in software operations and the computational power associated with their frequent training, testing, and retraining, there is a growing concern about the sustainability of DevOps practices in the context of Al-enabled software. Despite the increasing interest in this domain, a comprehensive overview that offers a holistic perspective on research related to sustainable AI is currently lacking. This paper addresses this gap by presenting a Systematic Mapping Study that thoroughly examines techniques, tools, and lessons learned to assess and promote environmental sustainability in MLOps practices for ML-Enabled Systems.} }
@inproceedings{10.1145/3716368.3735169, title = {Optimal Device Sequencing and Kernel Assignment for Multiple Heterogeneous Machine Learning Accelerators}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {746--751}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735169}, url = {https://doi.org/10.1145/3716368.3735169}, author = {Bachhav, Tejas and Kerkar, Amol and Rana, Rahul and Madden, Patrick}, keywords = {Machine Learning Accelerators, Kernel Mapping, Heterogeneous Systems, Device Sequencing}, abstract = {Applications utilizing machine learning and artificial intelligence have exploded in popularity in recent years. This has driven massive changes in system designs, with software systems being tightly coupled to the underlying architecture; co-optimization of both pays huge dividends. The new applications and hardware have also driven massive capital infrastructure investments; the compute and power demands for machine learning are skyrocketing.In this paper, we present an optimal algorithmic approach to map machine learning kernel graphs to multiple machine learning accelerators – and in particular, we focus on heterogeneous sets of accelerators. By integrating devices from different technology generations, the useful life span of each device is increased, lowing the overall cost of achieving high performance. We use benchmarks from a recent ISPD contest, and adapt them to consider multiple accelerators with varying performance characteristics.} }
@inproceedings{10.1145/3735014.3735871, title = {Machine Learning Application in Stock Portfolio Optimization and Price Prediction}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {40--47}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735871}, url = {https://doi.org/10.1145/3735014.3735871}, author = {Zhou, Yi and Chen, Xiaoxin and Kan, Pu and Wu, Meng and Zhao, Zhenru}, keywords = {LSTM Model, Machine Learning, Portfolio, Price Prediction, SVM Model, XGBoost Model}, abstract = {This article selects the top ten stocks with the highest returns in the CSI A50 index, and first uses the wavelet denoising method to smooth the original stock price data. Compared with VaR, this article uses CVaR to reduce the potential impact of extreme events, which can better reflect tail risk. Based on this, a high return, low-risk investment portfolio was constructed, and three machine learning models LSTM, SVM, and XGBoost were used to predict future stock prices. After comparative analysis, the XGBoost model with the highest prediction accuracy was selected as the final model. The research findings can provide investors with a new investment strategy by combining wavelet analysis, risk management, and machine learning prediction to optimize portfolio construction and predict future returns.} }
@inproceedings{10.1145/3603165.3607365, title = {Causal Inspired Trustworthy Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023}, pages = {3--4}, year = {2023}, isbn = {9798400702334}, doi = {10.1145/3603165.3607365}, url = {https://doi.org/10.1145/3603165.3607365}, author = {Kuang, Kun}, abstract = {In causality-based trustworthy machine learning, finding mechanisms from data-driven correlation analysis to causal inference and constructing a machine learning framework from correlation-driven to causality-driven are two significant challenges. To address these challenges, we propose a series of innovations, including data-driven causal inference mechanisms, causality-inspired interpretable and stable learning frameworks, causality-based generalizable graph neural network learning frameworks, and other fundamental theories and key technologies. To further support the development of the field, we make the corresponding codes and resources public in the open-source community, including the big data causal inference framework based on instrumental variables (https://github.com/causal-machine-learning-lab/mliv) and the large-scale graph neural network computing and edge-cloud collaborative learning platform (https://github.com/luoxi-model/luoxi_models).} }
@inproceedings{10.1145/3715275.3732033, title = {Identities are not Interchangeable: The Problem of Overgeneralization in Fair Machine Learning}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {485--497}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732033}, url = {https://doi.org/10.1145/3715275.3732033}, author = {Wang, Angelina}, keywords = {machine learning fairness, discrimination, context specificity, social identities}, abstract = {A key value proposition of machine learning is generalizability: the same methods and model architecture should be able to work across different domains and different contexts. While powerful, this generalization can sometimes go too far, and miss the importance of the specifics. In this work, we look at how fair machine learning has often treated as interchangeable the identity axis along which discrimination occurs. In other words, racism is measured and mitigated the same way as sexism, as ableism, as ageism. Disciplines outside of computer science have pointed out both the similarities and differences between these different forms of oppression, and in this work we draw out the implications for fair machine learning. While certainly not all aspects of fair machine learning need to be tailored to the specific form of oppression, there is a pressing need for greater attention to such specificity than is currently evident. Ultimately, context specificity can deepen our understanding of how to build more fair systems, widen our scope to include currently overlooked harms, and, almost paradoxically, also help to narrow our scope and counter the fear of an infinite number of group-specific methods of analysis.} }
@article{10.1145/3757699, title = {Venire: A Machine Learning-Guided Panel Review System for Community Content Moderation}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3757699}, url = {https://doi.org/10.1145/3757699}, author = {Koshy, Vinay and Choi, Frederick and Chiang, Yi-Shyuan and Sundaram, Hari and Chandrasekharan, Eshwar and Karahalios, Karrie}, keywords = {content moderation, decision-making, human-AI interaction, online communities}, abstract = {Research into community content moderation often assumes that moderation teams govern with a single, unified voice. However, recent work has found that moderators disagree with one another at modest, but concerning rates. The problem is not the root disagreements themselves. Subjectivity in moderation is unavoidable, and there are clear benefits to including diverse perspectives within a moderation team. Instead, the crux of the issue is that, due to resource constraints, moderation decisions end up being made by individual decision-makers. The result is decision-making that is inconsistent, which is frustrating for community members. To address this, we develop Venire, an ML-backed system for panel review on Reddit. Venire uses a machine learning model trained on log data to identify the cases where moderators are most likely to disagree. Venire fast-tracks these cases for multi-person review. Ideally, Venire allows moderators to surface and resolve disagreements that would have otherwise gone unnoticed. We conduct three studies through which we design and evaluate Venire: a set of formative interviews with moderators, technical evaluations on two datasets, and a think-aloud study in which moderators used Venire to make decisions on real moderation cases. Quantitatively, we demonstrate that Venire is able to improve decision consistency and surface latent disagreements. Qualitatively, we find that Venire helps moderators resolve difficult moderation cases more confidently. Venire represents a novel paradigm for human-AI content moderation, and shifts the conversation from replacing human decision-making to supporting it.} }
@inproceedings{10.1145/3747227.3747275, title = {Development of a Predictive Model for Hydropower Capacity in Jiangsu Province Utilizing Machine Learning Techniques}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {307--310}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747275}, url = {https://doi.org/10.1145/3747227.3747275}, author = {Zhao, Shuya}, keywords = {Hydropower Generation, Jiangsu Province, XGBoost Model}, abstract = {The strategic importance and contradictions of hydropower development have become evident. As an economic powerhouse, Jiangsu Province's energy demand continues to grow, and hydropower, as a clean energy source, holds a significant position under the "dual carbon" goals. In recent years, machine learning has gradually matured in water resource management, but its potential in predicting hydropower capacity has yet to be fully realized. The variables affecting hydropower generation are investigated, and the seasonal fluctuations and long-term trends of hydropower capacity (such as the difference between wet and dry seasons) are analyzed based on the time series data from 2005 to 2024. This provides a basis for dynamically adjusting power generation strategies and fully exploring the potential of XGBoost in hydropower capacity prediction. A systematic approach using the XGBoost model is adopted to reduce the dimension of data and identify the crucial factors that impact water resources. XGBoost model is evaluated using a dataset on water resource management from Jiangsu Province. Accuracy metrics are used to gauge the performance of the XGBoost model. Prediction accuracy achieved by the XGBoost model's training is 83.3\%, with further potential to optimize extended works. However, 100\% accuracy was obtained model's testing. This approach facilitates a balanced water ecology sheet by integrating environmental and economic considerations. XGBoost model identifies total water resources. The systematic machine learning approach helps policymakers in deciding the appropriate way of managing water resources, and infrastructures. Sustainable resource management techniques support decision-making through data-driven insights.} }
@article{10.1145/3640313, title = {Machine Learning for Refining Knowledge Graphs: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3640313}, url = {https://doi.org/10.1145/3640313}, author = {Subagdja, Budhitama and Shanthoshigaa, D. and Wang, Zhaoxia and Tan, Ah-Hwee}, keywords = {Knowledge graphs, knowledge graph refinement}, abstract = {Knowledge graph (KG) refinement refers to the process of filling in missing information, removing redundancies, and resolving inconsistencies in KGs. With the growing popularity of KG in various domains, many techniques involving machine learning have been applied, but there is no survey dedicated to machine learning-based KG refinement yet. Based on a novel framework following the KG refinement process, this article presents a survey of machine learning approaches to KG refinement according to the kind of operations in KG refinement, the training datasets, mode of learning, and process multiplicity. Furthermore, the survey aims to provide broad practical insights into the development of fully automated KG refinement.} }
@inproceedings{10.1145/3719159.3721223, title = {Towards A Modular End-To-End Machine Learning Benchmarking Framework}, booktitle = {Proceedings of the 3rd International Workshop on Testing Distributed Internet of Things Systems}, pages = {23--26}, year = {2025}, isbn = {9798400715266}, doi = {10.1145/3719159.3721223}, url = {https://doi.org/10.1145/3719159.3721223}, author = {Bayer, Robert and Robroek, Ties and T\"oz\"un, Pinar}, keywords = {Benchmarking, Deep Learning, Edge Computing, location = Rotterdam, Netherlands}, abstract = {Machine learning (ML) benchmarks are crucial for evaluating the performance, efficiency, and scalability of ML systems, especially as the adoption of complex ML pipelines, such as retrieval-augmented generation (RAG), continues to grow. These pipelines introduce intricate execution graphs that require more advanced benchmarking approaches. Additionally, collocating workloads can improve resource efficiency but may introduce contention challenges that must be carefully managed. Detailed insights into resource utilization are necessary for effective collocation and optimized edge deployments. However, existing benchmarking frameworks often fail to capture these critical aspects.We introduce a modular end-to-end ML benchmarking framework designed to address these gaps. Our framework emphasizes modularity and reusability by enabling reusable pipeline stages, facilitating flexible benchmarking across diverse ML workflows. It supports complex workloads and measures their end-to-end performance. The workloads can be collocated, with the framework providing insights into resource utilization and contention between the concurrent workloads.} }
@inproceedings{10.1145/3643651.3659891, title = {Machine Learning Training on Encrypted Data with TFHE}, booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics}, pages = {71--76}, year = {2024}, isbn = {9798400705564}, doi = {10.1145/3643651.3659891}, url = {https://doi.org/10.1145/3643651.3659891}, author = {Montero, Luis and Frery, Jordan and Kherfallah, Celia and Bredehoft, Roman and Stoian, Andrei}, keywords = {homomorphic encryption, machine learning, quantization, location = Porto, Portugal}, abstract = {We present an approach for outsourcing the training of machine learning (ML) models while preserving data confidentiality from malicious parties. We use fully homomorphic encryption (FHE) to build a unified training framework that works on encrypted data and learns quantized ML models. Our approach finds future applications in collaborative settings involving multiple parties working on confidential data, which can be horizontally or vertically split between data owners. We train logistic regression and multi-layer perceptrons on several datasets and show results that are comparable to the state-of-the-art.} }
@inproceedings{10.1145/3757110.3757170, title = {Research on Income Assessment of Poor Families in Latin America Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {351--356}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757170}, url = {https://doi.org/10.1145/3757110.3757170}, author = {Hu, Zhongyuan}, keywords = {Bagging model, Income assessment, feature engineering}, abstract = {This study tackles the critical challenge of accurately evaluating incomes in economically vulnerable households by constructing a machine learning framework utilizing Costa Rican household data. The proposed methodology emphasizes systematic feature engineering to identify socioeconomic attributes with strong statistical correlations to income levels, including housing conditions, educational attainment, and employment patterns. By prioritizing these discriminative features through rigorous correlation analysis and dimensionality reduction techniques, the framework substantially optimizes input data quality for predictive modeling. Comparative experiments demonstrate that this feature selection strategy consistently enhances the performance of diverse baseline algorithms, including decision trees, random forests, and gradient boosting machines, achieving notable improvements in prediction accuracy and generalization capability. The findings highlight the importance of domain-informed feature optimization in developing robust socioeconomic assessment tools, offering valuable insights for policymakers designing targeted poverty alleviation programs. This data-driven approach provides a scalable solution for income estimation challenges in developing regions, particularly where traditional survey methods face implementation barriers.} }
@inproceedings{10.1145/3748825.3748867, title = {Application Research on Employee Turnover Prediction Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {256--262}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748867}, url = {https://doi.org/10.1145/3748825.3748867}, author = {Liang, Zhiying and Jin, Aitong and Cai, Yongheng and Gan, Haohong and Liu, Zicong and Wang, Shu}, keywords = {Employee Turnover Prediction, Machine Learning Algorithms}, abstract = {This study examines how employee turnover hinders business operations, strategy execution, and cost-efficiency, while management talent loss impacts long-term growth. In the big data era, talent retention poses a challenge. Using machine learning algorithms, we aim to predict turnover more accurately, aid career planning, and strengthen corporate culture. Through "IBM Human Resources dataset"t, we build single (e.g., Decision Tree) and ensemble (Random Forest, AdaBoost, XGBoost) models to analyze turnover factors and propose solutions for talent retention, resource optimization, and improved incentives.} }
@inproceedings{10.1145/3662739.3662743, title = {Machine learning-based seismic prediction of building structures}, booktitle = {Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications}, pages = {256--261}, year = {2024}, isbn = {9798400718144}, doi = {10.1145/3662739.3662743}, url = {https://doi.org/10.1145/3662739.3662743}, author = {Liu, Shuai and Peng, Hailiang and Deng, Xiaolu}, keywords = {building structure, earthquake, machine learning, neural networks, seismicity prediction, location = Ningbo, China}, abstract = {The impact of earthquakes on building structures is the most direct and significant aspect of earthquake disasters, and the seismic resistance of building structures is a key factor in ensuring human safety and reducing economic losses. Ground shaking caused by earthquakes can create strong impact and vibration, which may lead to building collapse, damage, or deformation. In strong earthquakes, many buildings can suffer severe damage, such as complete collapse of the structure, cracking of walls, bending of columns, sagging of beams, and pose great danger to people. With the rapid development of modern computer technology in terms of hardware and software, the earthquake damage prediction methods and earthquake damage management of buildings have entered a brand new period. In this paper, classical machine learning methods such as decision tree, bagging tree, boosting tree, BP neural network and generalized regression network are used to construct prediction models and evaluate their prediction effects respectively. Combining the principles of different machine learning methods, the shortcomings of different models in small sample data training applications are analyzed, and the generalized regression network is further proposed. The prediction results show that the maximum values of seismic coefficients for buildings with rectangular cross-sections of different aspect ratios occur around an aspect ratio of 2, which is slightly larger than the normative value suggested by 1.4. The value stabilizes around 1.35 when the aspect ratio is greater than 4. The results can be used as a reference for similar structures The results can be used as a reference for the seismic design of similar structures.} }
@inproceedings{10.1145/3655755.3655781, title = {Machine Learning Algorithms for Diabetes Diagnosis Prediction}, booktitle = {Proceedings of the 2024 6th International Conference on Image, Video and Signal Processing}, pages = {192--199}, year = {2024}, isbn = {9798400716829}, doi = {10.1145/3655755.3655781}, url = {https://doi.org/10.1145/3655755.3655781}, author = {Zambrana, Amanda and Fanek, Loreen and Carrera, Pablo Salar and Ali, Md Liakat and Narasareddygari, Mourya Reddy}, keywords = {Accuracy, Diabetes Diagnosis, Machine Learning, Precision, Prediction, Recall, location = Ikuta, Japan}, abstract = {Diabetes is one of the most common health conditions in the United States, affecting more than 37 million U.S. adults. Despite how many people are affected by this disease, a permanent cure does not currently exist. Therefore, it is very important for patients to get the proper diagnosis and treatment. If diabetes is not diagnosed and properly treated to control blood sugar levels, serious health problems can occur. Though undiagnosed cases are very dangerous, still 1 in 5 U.S. adults with diabetes do not know that they have it. Therefore, a method for early detection or prediction of the disease is imperative for healthcare workers to provide timely distribution of treatment. Presently, hospitals collect large volumes of patient data with various tests and provide appropriate treatment based on diagnoses. With big data analytics, hidden patterns can be found in the data that help to predict diagnosis outcomes. Such predictions can help lower the rate of undiagnosed patients. In our paper, we propose a prediction model for classification of diabetes based on various features of patient data. We developed and applied six machine learning algorithms on two diabetes data sets to predict diagnoses. We found that the best algorithms for prediction with our two data sets were Ridge Classifier with an accuracy of 83.12\%, along with Random Forest and Decision Tree classifiers, both achieving accuracies of 95\%.} }
@inproceedings{10.1145/3643834.3660688, title = {A Steampunk Critique of Machine Learning Acceleration}, booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference}, pages = {246--257}, year = {2024}, isbn = {9798400705830}, doi = {10.1145/3643834.3660688}, url = {https://doi.org/10.1145/3643834.3660688}, author = {Cremaschi, Michele and Dorfmann, Max and De Angeli, Antonella}, keywords = {Acceleration, Interactive Art, Making, Repurposing of Outdated Technology, Semiotic, Slow Technology, location = Copenhagen, Denmark}, abstract = {The application of Machine Learning is driven by the techno–capitalist struggle for productivity across various domains, including the creative industry. Sociological research has demonstrated how technology–induced temporality introduces challenges at the individual and societal levels. Art creativity conflicts with speed and mass production. This paper describes Isotta, a critical artefact combining a Mignon typewriter and a Language Model to spark discussion about ML–induced acceleration. Fourteen artists evaluated Isotta in an interview study, and semiotics was used as the analytical lens. Results exposed ideological assumptions around the consequences of technology in the writing realm. We discuss these insights in the context of interactive design in times of techno–capitalistic acceleration. Our findings highlight the significance of temporal factors in designing generative writing interactions and underscore how complex societal challenges can be approached in design through the contrast–eliciting property that outdated technologies offer when juxtaposed with contemporary technologies.} }
@inproceedings{10.1145/3635638.3635646, title = {Predictive Analysis of NBA Game Outcomes through Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence}, pages = {46--55}, year = {2024}, isbn = {9798400709456}, doi = {10.1145/3635638.3635646}, url = {https://doi.org/10.1145/3635638.3635646}, author = {Wang, Junwen}, keywords = {Machine Learning, NBA Game Outcomes, Predictive Analysis, Sports Analytics, location = Chongqing, China}, abstract = {This study delved into the realm of sports analytics, employing machine learning techniques to predict the outcomes of NBA games based on player performance and team statistics. Through meticulous data collection, filtering, and model comparison, we gained insights into the factors that significantly impact game results. Logistic Regression, Support Vector Machines, Deep Neural Networks (DNN) and Random Forest models were rigorously evaluated, showcasing the power of advanced algorithms in uncovering intricate patterns within the data. The structured methodology of this study provides a versatile framework applicable to various sports analytics scenarios. The study shows the better performance of DNN and Random Forest in predicting the game results and the importance of field goal percentage in the predicting. Limitations still exist in this work, including data quality and model constraints; however, the findings have immediate implications for sports professionals seeking actionable insights. As we look ahead, this research underscores the potential for future advancements, encouraging exploration into more sophisticated algorithms, deeper feature analysis, and the integration of temporal patterns for comprehensive predictive accuracy.} }
@inproceedings{10.1145/3704304.3704306, title = {Machine Learning-Based Crop Recommendation for IoT-Enabled Smart Agriculture}, booktitle = {Proceedings of the 2024 9th International Conference on Cloud Computing and Internet of Things}, pages = {11--15}, year = {2025}, isbn = {9798400717161}, doi = {10.1145/3704304.3704306}, url = {https://doi.org/10.1145/3704304.3704306}, author = {Sonata, Ilvico}, keywords = {Internet of Things, Machine Learning, Random Forest, Sensor, Smart Agriculture}, abstract = {Human food needs are increasing all the time. This condition is in line with the increasing world population and exacerbated by climate change. Agricultural products are one of the sources of food to meet food needs. The use of technology is one of the keys to success in increasing agricultural production. One of the technologies used in agricultural technology is the Internet of Things (IoT). IoT in agricultural technology can be used to monitor soil and weather conditions to ensure that these conditions are in accordance with the needs of agricultural crops. The use of machine learning technology can be combined with IoT to help predict the types of crops that are suitable for planting based on soil and weather conditions. The results of the experiment show that the machine learning model can be used to predict the right types of plants to plant based on soil and weather conditions obtained through IoT. This method can be used to increase agricultural yields.} }
@inproceedings{10.1145/3746709.3746883, title = {Research on Tomato Leaf Disease Classification Based on Machine Learning}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {1026--1030}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746883}, url = {https://doi.org/10.1145/3746709.3746883}, author = {Yuting, Xia}, keywords = {Disease classification, YOLOv8, deep learning, optimization algorithm}, abstract = {Timely and accurate detection of plant diseases is an ongoing challenge in agriculture. The occurrence of tomato leaf diseases can affect the normal growth of tomatoes, leading to a reduction in quality and yield, resulting in significant economic losses. Traditional manual identification methods are time consuming and labour intensive, while traditional computer vision identification methods lack portability. Conducting research on tomato leaf disease classification based on machine learning can effectively alleviate the shortcomings of manual classification and traditional image classification methods in practical applications. It can improve the classification efficiency and accuracy, reduce the labour cost, realize the timely detection, identification and solution of diseases, reduce the misuse of pesticides, improve the tomato yield, and meet the national demand for green agriculture. Therefore, it is of great significance. Using deep learning technology to classify and identify tomato leaf diseases, we realised the classification of 10 tomato leaf diseases, while improving the classification efficiency and accuracy.} }
@proceedings{10.1145/3735654, title = {DEEM '25: Proceedings of the Workshop on Data Management for End-to-End Machine Learning}, year = {2025}, isbn = {9798400719240} }
@inproceedings{10.1145/3647444.3652435, title = {Machine Learning Algorithms for Advanced Rainfall Prediction}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652435}, url = {https://doi.org/10.1145/3647444.3652435}, author = {Saxena, Vivek and Singh, Uday Pratap and Kumari, Bersha and Khandelwal, Ansh}, keywords = {Keywords— Rainfall forecast, accuracy, machine learning, prediction, location = Jaipur, India}, abstract = {Rainfall forecast is essential in water resource management, agricultural planning, and disaster preparedness. Traditional rainfall forecasting systems have accuracy and lead time constraints. The rise of machine learning (ML) techniques provides a viable route for addressing these issues. This review paper examines current advancements in the application of machine learning for forecasting rainfall. It investigates the difficulties connected with rainfall forecasting, such as the complex and nonlinear nature of precipitation systems, data scarcity, and the requirement for real-time forecasting. The research investigates several machine learning models, data sources, and pre-processing approaches used to improve prediction accuracy. It also covers the incorporation of satellite data, weather radar data, and climate models into ML-based rainfall forecast systems. A comprehensive review of model performance in various geographic locations and climatic situations is offered, along with an assessment of limits and future development potential. This paper intends to assist academics, meteorologists, and policymakers in utilizing ML to provide more accurate and dependable rainfall forecasts, hence improving water resource management and disaster mitigation.} }
@article{10.1145/3761827, title = {Detecting Smart Ponzi Schemes on Blockchain using Machine Learning: A Comprehensive Survey}, journal = {Distrib. Ledger Technol.}, year = {2025}, doi = {10.1145/3761827}, url = {https://doi.org/10.1145/3761827}, author = {Kumar, Dheeraj and Palaniswami, Marimuthu and Muthukkumarasamy, Vallipuram}, keywords = {Smart Ponzi schemes, Cryptocurrencies, Bitcoin, Ponzi smart contract, Ethereum}, abstract = {Ponzi schemes, a more than a century-old fraud, have recently infiltrated blockchain-based cryptocurrency domain led by an explosion of such schemes in two most popular cryptocurrencies: Bitcoin and Ethereum. On these two platforms alone, the perpetrators of these frauds have fleeced gullible investors of billions of dollars annually. Smart Ponzi schemes are a hazard to these cryptocurrency ecosystems, diminishing investor confidence in these cutting-edge technologies, threatening their integrity, and hindering their growth and broader adaptation. These smart Ponzi schemes have also created a nightmare for law enforcement as tracking and taking countermeasures against fraudsters and recovering the victims’ investment is challenging. Over the years, researchers have utilized significant advances in machine learning and artificial intelligence to detect and promptly caution users against investing in Ponzi schemes on Bitcoin and Ethereum. However, this research still exists in silos, and there is a lack of a detailed survey paper critically analyzing various aspects of the approaches focusing on the menace of smart Ponzi schemes. This paper surveys the state-of-the-art techniques proposed in the literature to detect smart Ponzi schemes on two popular blockchain platforms: Bitcoin and Ethereum. We list, categorize, and discuss papers that contributed benchmark datasets, developed novel features concerning various aspects of smart Ponzi schemes, and proposed novel machine-learning approaches to detect them.} }
@inproceedings{10.1145/3746237.3746301, title = {Comparative Analysis of Downsampling Techniques for Machine Learning on Cultural Heritage Objects}, booktitle = {Proceedings of the 30th International Conference on 3D Web Technology}, pages = {1--10}, year = {2025}, isbn = {9798400720383}, doi = {10.1145/3746237.3746301}, url = {https://doi.org/10.1145/3746237.3746301}, author = {Tzermia, Chrysoula and Malamos, Athanasios G.}, keywords = {Classification, Downsampling, Point Clouds}, abstract = {Presenting point clouds in web environments is a very demanding process. It requires downsampling of point clouds in specific file formats, such us X3D, that uses web architecture to work across diverse devices and lightweight 3D graphics to ensure web page functionality. This paper evaluates the performance of several point cloud downsampling methods with the objective of identifying those that best balance data reduction for efficient web deployment with the preservation of essential features required for AI and machine learning applications. We present a comparative analysis of Voxel Grid Downsampling (VGD), Uniform Grid Subsampling (UGS), Curvature Preserving Sampling (CPS), Random Sampling (RS) and Farthest Point Sampling (FPS). To assess the impact of each approach on the distribution and structure of sampled point clouds, we conduct a number of experiments by combining SHREC 2021 Cultural Heritage dataset with PointNet.} }
@article{10.1145/3639032, title = {Machine Learning Systems are Bloated and Vulnerable}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, volume = {8}, year = {2024}, doi = {10.1145/3639032}, url = {https://doi.org/10.1145/3639032}, author = {Zhang, Huaifeng and Alhanahnah, Mohannad and Ahmed, Fahmi Abdulqadir and Fatih, Dyako and Leitner, Philipp and Ali-Eldin, Ahmed}, keywords = {machine learning systems, software debloating}, abstract = {Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from operating systems and applications to containers. Containers are lightweight virtualization technologies used to package code and dependencies, providing portable, reproducible and isolated environments. For their ease of use, data scientists often utilize machine learning containers to simplify their workflow. However, this convenience comes at a cost: containers are often bloated with unnecessary code and dependencies, resulting in very large sizes. In this paper, we analyze and quantify bloat in machine learning containers. We develop MMLB, a framework for analyzing bloat in software systems, focusing on machine learning containers. MMLB measures the amount of bloat at both the container and package levels, quantifying the sources of bloat. In addition, MMLB integrates with vulnerability analysis tools and performs package dependency analysis to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from TensorFlow, PyTorch, and Nvidia, we show that bloat accounts for up to 80\% of machine learning container sizes, increasing container provisioning times by up to 370\% and exacerbating vulnerabilities by up to 99\%.} }
@article{10.1145/3737284, title = {Keeper: Automated Testing and Fixing of Machine Learning Software - RCR Report}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3737284}, url = {https://doi.org/10.1145/3737284}, author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Maire, Michael and Hoffmann, Henry and Lu, Shan}, abstract = {This artifact aims to provide source code, benchmark suite, results, and materials used in our study “Keeper: Automated Testing and Fixing of Machine Learning Software” [3]. We developed an automated testing and fixing tool Keeper and its IDE plugin for ML software. It automatically detects software defects and attempts to change how ML APIs are used to alleviate software misbehavior. This artifact provides guidelines to set up and execute Keeper, and also guidelines to interpret our evaluation results. We hope this artifact can motivate and help future research to further tackle ML API misuses. All related data are available online.} }
@inproceedings{10.1145/3745812.3745822, title = {Anomaly Detection in Transactions using Machine Learning: A Comparative Study}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745822}, url = {https://doi.org/10.1145/3745812.3745822}, author = {Tikoo, Divya and Ramesh, Nandhini and Vaidya, Atharva and Vora, Rutva Ashwin and Mali, Swati}, keywords = {Decision Tree, KNN, Metaverse, Random Forest, SMOTE, SVC}, abstract = {Detecting anomalous transactions in the metaverse is a major prob- lem that has serious security implications. The review of available studies on machine learning models used to detect these anomalies is also scanty. To achieve this, four machine learning approaches (i.e., Decision Trees, Random Forest, Support Vector Classification (SVC), and K-Nearest Neighbors (KNN)) will be evaluated and com- pared in terms of their ability to determine the most effective model for anomaly detection. The dataset contains 78,600 transaction records with fields such as timestamps, addresses, amounts, and risk classifications. This comparative analysis, which considers ac- curacy, precision, F1 score, and recall as evaluation metrics, reveals that Random Forest outperforms the other models. The findings of this study are expected to provide insights for building safer and more secure metaverse environments.} }
@inproceedings{10.1145/3650203.3663329, title = {AIDB: a Sparsely Materialized Database for Queries using Machine Learning}, booktitle = {Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning}, pages = {23--28}, year = {2024}, isbn = {9798400706110}, doi = {10.1145/3650203.3663329}, url = {https://doi.org/10.1145/3650203.3663329}, author = {Jin, Tengjun and Mittal, Akash and Mo, Chenghao and Fang, Jiahao and Zhang, Chengsong and Dai, Timothy and Kang, Daniel}, abstract = {Analysts and scientists are interested in automatically analyzing the semantic contents of unstructured, non-tabular data (videos, images, text, and audio). These analysts have turned to unstructured data systems leveraging machine learning (ML). The most common method of using ML in analytics systems is to call them as user-defined functions (UDFs). Unfortunately, UDFs can be difficult for query optimizers to reason over. Furthermore, they can be difficult to implement and unintuitive to application users.Instead of specifying ML models via UDFs, we propose specifying mappings between virtual columns in a structured table, where virtual rows are sparsely materialized via ML models. Querying sparsely materialized tables has unique challenges: even the cardinality of tables is unknown ahead of time, rendering a wide range of standard optimization techniques unusable. We propose novel optimizations for accelerating approximate and exact queries over sparsely materialized tables to address these challenges, providing up to 350x cheaper queries. We implement our techniques in AIDB and deploy them in four real-world datasets. Several of these datasets were constructed with collaborators including law professors studying court cases, showing AIDB's wide applicability.} }
@inproceedings{10.1145/3688268.3688271, title = {Prediction of Undergraduate Success Through Machine Learning Models}, booktitle = {Proceedings of the 2024 12th International Conference on Computer and Communications Management}, pages = {12--18}, year = {2024}, isbn = {9798400718038}, doi = {10.1145/3688268.3688271}, url = {https://doi.org/10.1145/3688268.3688271}, author = {Chaiya, Supap and Songpan, Wararat}, keywords = {logistic regression model, predictive model, strategic planning, success factors}, abstract = {The aim of this study is to investigate the factors that affect the graduation rates of undergraduates at Khon Kaen University. The focus is on students who were admitted through the direct admission test in 2018 and graduated in 2022. The university has recently implemented a new direct admission testing system, which has led to the need for a passing level score for university entrance. To address this issue, the study uses a management-by-fact approach, utilizing data from admissions and registration databases that are analyzed using machine learning models. The research categorizes the influencing factors into three main groups: 1) personal factors, including gender, family income, and parents' occupations; 2) educational factors, such as total admission scores, first-year grade point averages (GPA) for both semesters, and English admission scores; and 3) university service-related factors, including teacher and overall class evaluation scores by students. The study found that the GPAs of the first and second semesters in the first year (GPA1 and GPA2, respectively) had a significant impact on the graduation rates of students. The comparison of ten models showed that logistic regression was the most effective, achieving 88.05\% accuracy in predicting outcomes. This finding highlights the potential of machine learning in educational settings, offering valuable insights for strategic planning and interventions aimed at improving graduation rates.} }
@inproceedings{10.1145/3660853.3660935, title = {Enhanced Network Traffic Classification with Machine Learning Algorithms}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {322--327}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660935}, url = {https://doi.org/10.1145/3660853.3660935}, author = {Najm, Ihab Ahmed and Saeed, Ahmed Hikmat and Ahmad, B.A. and Ahmed, Saadaldeen Rashid and Sekhar, Ravi and Shah, Pritesh and Veena, B.S.}, keywords = {Enhanced real-world network, Network traffic, classification, machine learning algorithms, location = undefinedstanbul, Turkiye}, abstract = {Network traffic classification plays a critical role in maintaining the security and efficiency of modern computer networks. Existing techniques often have problems effectively identifying and establishing network traffic patterns. This research seeks to fill this gap by offering an updated approach to network traffic classification using machine learning algorithms. Our research extends upon earlier studies by focusing on robust feature engineering techniques and applying a broad assortment of machine learning algorithms, including decision trees, random forests, support vector machines, and recurrent neural networks. We employ a comprehensive dataset containing diverse network traffic situations to train. We demonstrate the efficacy of our approach by achieving promising accuracy rates: 93\% for decision trees, 97.89\% for random forests, 91\% for support vector machines, and 89.49\% for recurrent neural networks. Our findings emphasize the promise of machine learning for addressing real-world network traffic classification challenges.} }
@inproceedings{10.1145/3722212.3725636, title = {Navigating Data Errors in Machine Learning Pipelines: Identify, Debug, and Learn}, booktitle = {Companion of the 2025 International Conference on Management of Data}, pages = {813--820}, year = {2025}, isbn = {9798400715648}, doi = {10.1145/3722212.3725636}, url = {https://doi.org/10.1145/3722212.3725636}, author = {Karlas, Bojan and Salimi, Babak and Schelter, Sebastian}, keywords = {data debugging, data errors, data importance, missing data, location = Berlin, Germany}, abstract = {Addressing data errors-such as missing, incorrect, noisy, biased, or out-of-distribution values-is essential to building reliable machine learning (ML) systems. Traditional methods often focus on refining the training process to minimize error symptoms or repairing data errors indiscriminately, without addressing their root causes. These isolated approaches ignore how errors originate and propagate through the interconnected stages of ML pipelines-data preprocessing, model training, and prediction-resulting in superficial fixes and suboptimal solutions. Consequently, they miss the opportunity to understand how data errors impact downstream tasks and to implement targeted, effective interventions. In recent years, the research community has made significant progress in developing holistic approaches to identify the most harmful data errors, prioritize impactful repairs, and reason about their effects when errors cannot be fully resolved. This tutorial surveys prominent work in this area and introduces practical tools designed to address data quality issues across the ML lifecycle. By combining theoretical insights with hands-on demonstrations, attendees will gain actionable strategies to diagnose, repair, and manage data errors, enhancing the reliability, fairness, and transparency of ML systems in real-world applications.} }
@inproceedings{10.1145/3716368.3735230, title = {An Efficient Distributed Machine Learning Inference Framework with Byzantine Fault Detection}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {56--63}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735230}, url = {https://doi.org/10.1145/3716368.3735230}, author = {Zhou, Xuan and Mohan, Utkarsh and Liu, Yao and Beerel, Peter}, keywords = {distributed inference, security, machine learning inference security, Byzantine fault, fault detection}, abstract = {The gap between the complexity of the most advanced machine learning (ML) models, e.g., the large language model (LLMs), PaLM, with 540 billion parameters, and what hardware resources at the edge can support is growing. Two approaches to mitigate this gap are leveraging cloud-based ML servers, which introduce widely studied security, privacy, and reliability risks, and distributed inference, in which several local edge-based devices share the computational burden. Motivated by the fact that the security of the distributed inference approach has received far less attention, this paper proposes a low-cost and versatile scheme to add redundancy to distributed inference to mitigate compromised devices that can exhibit faulty or malicious behavior modeled as Byzantine faults. We mathematically derive the number of inferences required to detect the attack as a function of computation overhead and also develop a simulator. The simulation results on an LLM align closely with the theoretical values. Specifically, with a redundancy overhead of 10\% and 4 out of 8 devices in a single layer compromised, the average number of inferences required to detect all malicious devices is only 39.42.} }
@inproceedings{10.1145/3677779.3677809, title = {Traveling Classification Profiler towards Passport Service Optimization using Machine Learning}, booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {182--187}, year = {2024}, isbn = {9798400709760}, doi = {10.1145/3677779.3677809}, url = {https://doi.org/10.1145/3677779.3677809}, author = {Manurung, Raphael Fransiskus and Sembiring, Jaka and Bandung, Yoanes}, abstract = {The issue of passport misuse poses significant threats to both national and international security, as well as the integrity of a country's immigration system. Enhancing the effectiveness of the immigration system to detect and prevent passport misuse is crucial in anticipating as well as mitigating of the potential threats. This study investigates into the implementation of traveler movement extraction to support passport control mechanisms and mitigate the associated risks. The primary objective is to enhance the quality of information to address vulnerabilities in the passport management system and streamline passport verification processes. This approach focuses on the benefits of leveraging the integration of data from diverse microservices such as passport validation, authentication, and real-time border monitoring through web service and utilizing machine learning to extract the valuable data then transforms into insights. By using real-world data sources ensures the accuracy and fidelity of representations. Notably, most informative variables such as duration, frequency, age, gender, country, and category are extracted to inform the analysis. Results indicate that the random forest classifier algorithm outperforms support vector machine and decision tree algorithms in terms of accuracy, achieving rates of 96\% and above across varying dataset thresholds.} }
@inproceedings{10.1145/3695220.3695229, title = {Tourist Destination Recommendation System based on Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Big Data and Computing}, pages = {58--67}, year = {2024}, isbn = {9798400718205}, doi = {10.1145/3695220.3695229}, url = {https://doi.org/10.1145/3695220.3695229}, author = {Kongpeng, Sumitra and Hanskunatai, Anantaporn}, keywords = {Data mining, Less visited area, Machine learning, Tourist recommendation system, location = Bangkok, Thailand}, abstract = {Thailand has a wide variety of tourist attractions, making it difficult for tourist to choose where to go on vacation. The tourist destination recommendation system is a challenge for creating a system to help recommend tourist destinations that are appropriate for personal. Therefore, the principal aims of this research encompass two distinct objectives: firstly, to create a recommendation system for tourist destinations in Thailand by applying machine learning algorithms; and secondly, to analyze factors influencing tourists' choices of destinations. The dataset was gathered from an online survey conducted via Google Forms, comprising responses from 429 tourists in Thailand. In the experiments, three different types of feature selection methods were applied in a data preprocessing step. In the modeling process, four machine learning algorithms, namely Decision Tree, Random Forest, k-Nearest Neighbors (k-NN), and Multi-Layer Perceptron (MLP), were used to construct the model and compare the predictive performance of the recommendation system based on hit rate and NDCG. The experimental results showed that suggesting tourist destinations in the Central region was the most effective, with the highest hit rate and NDCG compared to other regions. The average hit rate and NDCG for the five regions were 0.8 and 0.59, respectively. In addition, there has been an analysis of key factors influencing destination selection, such as activity, travel month, travel budget, and the age of tourists, to understand their impact on travel choices in each region of Thailand.} }
@inproceedings{10.1145/3736539.3754446, title = {Earth Embeddings: Harnessing the Information in Earth Observation Data with Machine Learning}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Frontiers}, year = {2025}, isbn = {9798400719462}, doi = {10.1145/3736539.3754446}, url = {https://doi.org/10.1145/3736539.3754446}, author = {Rolf, Esther and Klemmer, Konstantin and Ruwurm, Marc}, abstract = {Machine learning (ML) for Earth Observation (EO) data is revolutionizing the speed and scope at which science and policy can operate — filling critical data gaps across fields such as ecology and development economics. Helping fuel this progress is a class of ML for EO models that distill global satellite data into compact, multi-purpose representations of the Earth. These “Earth embedding” models include image embeddings designed to capture the unique characteristics of satellite imagery and an emerging class of location encoders that serve as implicit neural representations of Earth’s data. These models are already unlocking new use cases and capabilities, with much research yet to be explored.} }
@inproceedings{10.1145/3639479.3639512, title = {Experimental design of emotion recognition based on machine learning}, booktitle = {Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing}, pages = {155--160}, year = {2024}, isbn = {9798400709241}, doi = {10.1145/3639479.3639512}, url = {https://doi.org/10.1145/3639479.3639512}, author = {Yu, Jintao and Jiang, Mingze and Liang, Tingwei}, keywords = {Emotion recognition, Experimental design, Machine learning, Physiological signal, location = Sanya, China}, abstract = {In this paper, a wrist bracelet was used to measure the subjects' heart rate signals and skin electrical signals, and three types of emotions (positive, neutral and negative) were induced by designing an experimental environment. The heart rate and skin electrical signals corresponding to emotions were collected through the bracelet, and the recognition accuracy of the three types of emotions was finally obtained through data preprocessing and the support vector machine model in machine learning. At present, how to build an experimental environment that can fully induce subjects' emotions is a major difficulty in emotion recognition, so this paper provides a specific emotion recognition environment design and detailed steps. Meanwhile, through a large number of experiments, the kernel function in the support vector machine model, namely Gaussian kernel function, is determined, and grid search is introduced to search for hyperparameter C. To get the optimal parameters and results. A total of 20 people were measured in this experiment. The experimental results obtained by SVM model were positive: the recognition accuracy of skin electrical signal was 0.8422, the recognition accuracy of heart rate signal was 0.8345, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion was 0.8832. Negative: the recognition accuracy of skin electrical signal is 0.9812, the recognition accuracy of heart rate signal is 0.9385, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion is 0.9902. Neutral: the recognition accuracy of skin electrical signal was 0.6403, the recognition accuracy of heart rate signal was 0.5308, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion was 0.5438} }
@article{10.14778/3746405.3746437, title = {ArrayMorph: Optimizing Hyperslab Queries on the Cloud for Machine Learning Pipelines}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {3189--3202}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3746405.3746437}, url = {https://doi.org/10.14778/3746405.3746437}, author = {Jiang, Ruochen and Blanas, Spyros}, abstract = {Cloud storage services such as Amazon S3, Azure Blob Storage, and Google Cloud Storage are widely used to store raw data for machine learning applications. When the data is later processed, the analysis predominantly focuses on regions of interest (such as a small bounding box in a larger image) and discards uninteresting regions. Machine learning applications can significantly accelerate their I/O if they push this data filtering step to the cloud. Prior work has proposed different methods to partially read array (tensor) objects, such as chunking, reading a contiguous byte range, and evaluating a lambda function. No method is optimal; estimating the total time and cost of a data retrieval requires an understanding of the data serialization order, the chunk size and platform-specific properties. This paper introduces ArrayMorph, a cloud-based array data storage system that automatically determines which is the best method to use to retrieve regions of interest from data on the cloud. ArrayMorph formulates data accesses as hyperslab queries, and optimizes them using a multi-phase cost-based approach. ArrayMorph seamlessly integrates with Python/PyTorch-based ML applications, and is experimentally shown to transfer up to 9.8X less data than existing systems. This makes ML applications run up to 1.7X faster and 9X cheaper than prior solutions.} }
@inproceedings{10.1145/3766918.3766922, title = {Machine Learning-Driven Multi-Factor Quantitative Model: A Study on the Ethereum Market}, booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business}, pages = {19--28}, year = {2025}, isbn = {9798400716027}, doi = {10.1145/3766918.3766922}, url = {https://doi.org/10.1145/3766918.3766922}, author = {Yu, Zhijie}, keywords = {Ethereum, machine learning, multi-factor model, on-chain factors, quantitative trading}, abstract = {This study constructs a machine learning-driven multi-factor model for Ethereum quantitative trading, combining traditional technical indicators (RSI, MACD), on-chain metrics (gas usage, active addresses), and X platform social sentiment to predict short-term returns. Backtesting from Q4 2021 to Q3 2024, using online learning and genetic algorithms for dynamic factor updates, yields a 97\% annualized return, a Sharpe ratio of 2.5, and an information ratio of 1.2, outperforming Ethereum's raw returns. Simulated trading in Q4 2024 (bull market) achieves a 33\% quarterly return with an 18\% maximum drawdown, while Q1 2025 (bear market) records a -10\% quarterly return with a 12\% drawdown, confirming robustness. Technical and sentiment factors drive performance, though a 22\% maximum drawdown in backtesting highlights volatility risks. An optimal Z-score threshold (±1.0) and 4-hour trading frequency balance profitability and costs. Future enhancements include high-frequency mainnet data integration and advanced risk management to strengthen model resilience in Ethereum's volatile market.} }
@inbook{10.1145/3760023.3760045, title = {The Machine Learning-Enhanced DCF Model and Probabilistic Cash Flow Forecasting}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {126--133}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760045}, author = {Lu, Tianyi and Zhao, Yihao}, abstract = {Traditional DCF models rely on a high level of subjective analysts' judgment in cash flow forecasting and traditionally assume perpetual constant growth rates and WACC, which are ill-suited to capture market volatility and tail risks. To bypass these pitfalls, this research suggests a hybrid modeling approach with LSTM networks to distill temporal trends from historical financial data and forecast future cash flows as a first input. These forecasts are then run through a Monte Carlo simulation in order to generate probabilistic distributions, with a jump-diffusion process also included to cover Black Swan events [13]. An empirical investigation with data of the top 500 publicly traded US companies demonstrates that: (1) the machine learning can enhance the valuation range of DCF models; (2) compared to the traditional DCF model, the new approach provides a more complete measure of tail risks [10], (3) statistical tests confirm that valuation results of the two models are different significantly. This study introduces a more integrated risk-return investment decision-making framework, ideally suited to high-volatility industries and periods of market uncertainty [9,14].} }
@inproceedings{10.1145/3712255.3734339, title = {Interpreting Machine Learning Pipelines Produced by Evolutionary AutoML for Biochemical Property Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1944--1952}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734339}, url = {https://doi.org/10.1145/3712255.3734339}, author = {de S\'a, Alex G. C. and Pappa, Gisele L. and Freitas, Alex A. and Ascher, David B.}, keywords = {automated machine learning (AutoML), cheminformatics, drug discovery, AutoML-generated pipeline interpretability, Bayesian networks, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Machine learning (ML) has been playing a crucial role in drug discovery, mainly through quantitative structure-activity relationship models that relate molecular structures to properties, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties. However, traditional ML approaches often lack customisation to a particular biochemical task and fail to generalise to new biochemical spaces, resulting in reduced predictive performance. Automated machine learning (AutoML) has emerged to address these limitations by automatically selecting the suitable ML pipelines for a given input dataset. Despite its potential, AutoML is underutilised in cheminformatics, and its decisions often lack interpretability, reducing user trust - especially among non-experts. Accordingly, this paper proposes an evolutionary AutoML method for biochemical property prediction that outputs an interpretable model for understanding the evolved ML pipelines. It combines grammar-based genetic programming with Bayesian networks to guide search and enhance the searched pipelines' interpretability. The evaluation on 12 benchmark ADMET datasets showed that the proposed AutoML method obtained similar or better results than three existing methods. Additionally, the interpretable Bayesian network identified, among the ML pipelines' components generated by the AutoML method (i.e. components like biochemical feature extraction methods, preprocessing techniques and ML algorithms), which components affect the ML pipelines' predictive performance.} }
@inproceedings{10.1145/3669940.3707266, title = {Design and Operation of Shared Machine Learning Clusters on Campus}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, pages = {295--310}, year = {2025}, isbn = {9798400706981}, doi = {10.1145/3669940.3707266}, url = {https://doi.org/10.1145/3669940.3707266}, author = {Xu, Kaiqiang and Sun, Decang and Wang, Hao and Ren, Zhenghang and Wan, Xinchen and Liao, Xudong and Wang, Zilong and Zhang, Junxue and Chen, Kai}, keywords = {multi-tenant cluster operations, resource management, shared gpu cluster, location = Rotterdam, Netherlands}, abstract = {The rapid advancement of large machine learning (ML) models has driven universities worldwide to invest heavily in GPU clusters. Effectively sharing these resources among multiple users is essential for maximizing both utilization and accessibility. However, managing shared GPU clusters presents significant challenges, ranging from system configuration to fair resource allocation among users.This paper introduces SING, a full-stack solution tailored to simplify shared GPU cluster management. Aimed at addressing the pressing need for efficient resource sharing with limited staffing, SING enhances operational efficiency by reducing maintenance costs and optimizing resource utilization. We provide a comprehensive overview of its four extensible architectural layers, explore the features of each layer, and share insights from real-world deployment, including usage patterns and incident management strategies.As part of our commitment to advancing shared ML cluster management, we open-source SING's resources to support the development and operation of similar systems.} }
@inproceedings{10.1145/3707127.3707129, title = {Multi-modal Machine Learning in Gastrointestinal Endoscopy: A Review}, booktitle = {Proceedings of the 2024 11th International Conference on Biomedical and Bioinformatics Engineering}, pages = {10--17}, year = {2025}, isbn = {9798400718274}, doi = {10.1145/3707127.3707129}, url = {https://doi.org/10.1145/3707127.3707129}, author = {Chan, In Neng and Wong, Pak Kin and Yan, Tao and Hu, Yanyan and Chan, Chon In}, keywords = {deep learning, multi-modal learning, upper endoscopy}, abstract = {The gastrointestinal (GI) tract plays a crucial role in the human body but is frequently affected by diseases that pose a significant global health issue. GI endoscopy is the primary diagnostic tool for early disease detection. However, its efficacy is limited by human factors such as fatigue and diagnostic variability. The integration of artificial intelligence (AI) in GI endoscopy, particularly through machine learning (ML) and deep learning (DL), offers a promising solution to enhance diagnostic accuracy and efficiency. Many studies have primarily focused on the sole use of endoscopic images, which limits the potential benefits of more comprehensive analysis with unimodal data. With advancements in DL algorithms that can handle complex data modalities, multi-modal machine learning (MMML) has emerged as an innovative approach that leverages multiple data modalities to enhance model performance by combining diverse information sources. Despite its potential, there is no specific review on the application of MMML in GI endoscopy. This paper reviews the applications of MMML in GI endoscopy and highlights its benefits in addressing the complexities of GI endoscopy tasks. Besides, it also explores the potential clinical applications and analyzes the challenges and provides recommendations for further advancements in this emerging field.} }
@inproceedings{10.1145/3747912.3747950, title = {Spatio-temporal prediction model of urban waterlogging based on machine learning}, booktitle = {Proceedings of the 2025 International Conference on Software Engineering and Computer Applications}, pages = {243--247}, year = {2025}, isbn = {9798400715136}, doi = {10.1145/3747912.3747950}, url = {https://doi.org/10.1145/3747912.3747950}, author = {Zhang, Hui and Xu, Junsong and Zhang, Lan and Wanyan, Jianfei and Duan, Shikun}, keywords = {Keywords, Urban waterlogging, Machine learning, Hydrodynamic control equation, Improved long Short-Term memory network, Spatio-temporal prediction}, abstract = {This study proposes a spatio-temporal prediction model for urban waterlogging based on machine learning, which predicts the accumulation of urban waterlogging from both temporal and spatial dimensions. The spatio-temporal data set of urban waterlogging was constructed by using the waterlogging inundation results of Infoworks ICM under different recurrence periods of rainstorm and pipeline siltation scenarios. The random forest algorithm and the improved long short-term memory neural network algorithm were adopted to predict the waterlogging accumulation in the study area in terms of time and space. The results show that the spatio-temporal prediction accuracy of the urban waterlogging prediction model in space is 81.7\%, and the Nash efficiency coefficient of time series prediction exceeds 0.8. The waterlogging prediction method proposed in this study can effectively improve the accuracy of waterlogging early warning and response speed.} }
@inproceedings{10.1145/3688671.3688762, title = {CLBO: Conditional Local Bayesian Optimization for Automated Machine Learning}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688762}, url = {https://doi.org/10.1145/3688671.3688762}, author = {Paterakis, George and Borboudakis, Giorgos and Paraschakis, Konstantinos and Charonyktakis, Pavlos and Tsamardinos, Ioannis}, keywords = {machine learning, Bayesian optimization, AutoML, optimization}, abstract = {In this paper, we present a novel Bayesian optimization method named Conditional Local Bayesian Optimization (CLBO) designed specifically to address challenges in optimizing Automated Machine Learning (AutoML) tasks. Inspired by a controller-responder architecture, our method leverages a controller that selects promising pipelines based on an acquisition function. Local responders employ Bayesian optimization to refine the search within sub-spaces. CLBO introduces a progressive budget system, which dynamically allocates the optimization budget. Our method utilizes a group of surrogate models to initially optimize a simpler objective function. Notably, CLBO outperforms current state-of-the-art algorithms on 35 classification datasets from the OpenML repository. Finally, CLBO reports unbiased performance estimates through the use of a performance correction method.} }
@inproceedings{10.1145/3759023.3759101, title = {Enhancing DDoS Detection in Software-Defined Networking: A Machine Learning and Deep Learning Approach}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759101}, url = {https://doi.org/10.1145/3759023.3759101}, author = {Hill, Winston and Mason, Janelle and Aldrich, Bernard and Acquaah, Yaa Takyiwaa and Roy, Kaushik}, keywords = {Software-Defined Networking (SDN), DDoS (Distributed Denial of Service), Machine Learning, Deep Learning, Multiclass Classification}, abstract = {Software-Defined Networking (SDN) enables dynamic policy updates and lowers hardware costs but introduces new vulnerabilities, especially to DDoS attacks. This study analyzes two public DDoS datasets using multiclass classification techniques. A range of machine learning models—SVM, Logistic Regression, Naive Bayes, Decision Trees, KNN, Random Forest, XGBoost, and AdaBoost—are evaluated alongside deep learning models, including LSTM and attention-based LSTM, to enhance detection performance across multiple attack types.} }
@inproceedings{10.1145/3703187.3703200, title = {Improving English Speech Recognition System Accuracy Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial Intelligence}, pages = {73--78}, year = {2024}, isbn = {9798400707254}, doi = {10.1145/3703187.3703200}, url = {https://doi.org/10.1145/3703187.3703200}, author = {Xu, Hui}, keywords = {Accuracy Improvement, Deep Learning, Machine Learning, Reinforcement Learning, Speech Recognition, Transfer Learning}, abstract = {This research significantly advanced English speech recognition technology by leveraging advanced machine learning techniques. The study developed a novel hybrid model combining deep learning, transfer learning, and reinforcement learning, which markedly improved recognition accuracy in complex environments. A diverse 1000-hour dataset was used for training, encompassing various accents and environmental conditions. The model's architecture integrated LSTM, CNN, and attention mechanisms, coupled with innovative feature extraction methods. Results showed substantial improvements across multiple metrics. The overall word error rate was reduced to 11.25\%, outperforming existing benchmark models. Notably, the system achieved 86\% accuracy in noisy environments and 92\% in accent adaptation. For rapid speech, error rates decreased from 20\% to 12\%. These advancements expand the technology's applicability in fields such as healthcare, education, and customer service, paving the way for more robust and versatile speech recognition systems in real-world applications.} }
@inproceedings{10.1145/3638209.3638229, title = {Image Scenario classification using Machine learning}, booktitle = {Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems}, pages = {130--138}, year = {2024}, isbn = {9798400709067}, doi = {10.1145/3638209.3638229}, url = {https://doi.org/10.1145/3638209.3638229}, author = {Almazrouei, khawla and Bou Nassif, Ali J.}, keywords = {Inception V3, Multilayer Perceptron, Support Vector Machine, and CNN layers, location = Tokyo, Japan}, abstract = {Image Scenario classification is widespread for many IoT applications. Classifying scenario helps in making proper decisions. The study aims at classifying six different scenarios using a deep neural network algorithm. The proposed InceptionV3 classification algorithm could predict the scenarios and achieve 92.00\% accuracy. A quick comparison is shown with the traditional machine learning algorithms, SVM and MLP. The study shows the power of the deep neural algorithm and classifies the scene image dataset with higher precision.} }
@inproceedings{10.1109/SCW63240.2024.00008, title = {Machine Learning Aboard the ADAPT Gamma-Ray Telescope}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {4--10}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00008}, url = {https://doi.org/10.1109/SCW63240.2024.00008}, author = {Htet, Ye and Sudvarg, Marion and Butzel, Andrew and Buhler, Jeremy D. and Chamberlain, Roger D. and Buckley, James H.}, keywords = {machine learning, multi-messenger astrophysics, neural networks, location = Atlanta, GA, USA}, abstract = {The Advanced Particle-astrophysics Telescope (APT) is an orbital mission concept designed to contribute to multi-messenger observations of transient phenomena in deep space. APT will be uniquely able to detect and accurately localize short-duration gamma-ray bursts (GRBs) in the sky in real time. Current detection and analysis systems require resource-intensive ground-based computations; in contrast, APT will perform on-board analysis of GRBs, demanding analytical tools that deliver accurate results under severe size, weight, and power constraints.In this work, we describe a neural network approach in our computation pipeline for GRB localization, demonstrating the capabilities of two neural networks: one to discard signals from background radiation, and one to estimate the uncertainty of GRB source direction constraints associated with individual gamma-ray photons. We validate the accuracy and computational efficiency of our networks using a physical simulation of GRB detection in the Antarctic Demonstrator for APT (ADAPT), a high-altitude balloon-borne prototype for APT.} }
@inproceedings{10.1145/3631700.3664913, title = {Towards Exploring Personalized Hyperlink Recommendations Through Machine Learning}, booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization}, pages = {528--533}, year = {2024}, isbn = {9798400704666}, doi = {10.1145/3631700.3664913}, url = {https://doi.org/10.1145/3631700.3664913}, author = {Bompotas, Agorakis and Triantafyllopoulos, Panagiotis and Raptis, George E. and Katsini, Christina and Makris, Christos}, keywords = {Content Overload, Hyperlink Analysis, Machine Learning, Natural Language Processing, Personalization, Recommendation Systems, User Experience, Web Navigation, Web Usability, location = Cagliari, Italy}, abstract = {The Internet offers a wealth of content, making it increasingly difficult for users to navigate website information. The volume of hyperlinks on a website often leaves users struggling with content overload, hindering their ability to find relevant information of high interest. This problem highlights the critical need for tools to improve the user experience by providing personalized hyperlink recommendations on a specific website. This paper introduces HypeRec, a browser extension that attempts to address this problem by leveraging and comparing different machine learning and recommendation algorithms to guide users to content consistent with their interests and preferences. Our approach involves extracting hyperlinks from a webpage and subjecting the corresponding textual content to natural language processing techniques. In this way, it simplifies the users’ navigation within a website and promotes a more intuitive and satisfying web browsing experience.} }
@inproceedings{10.1145/3718391.3727887, title = {Study on the models of machine learning for better classification}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {319--324}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3727887}, url = {https://doi.org/10.1145/3718391.3727887}, author = {Cui, Yangfan and Yang, Guangsong and Liu, Yuanfa and Lei, Guowei and Ni, Wenqing}, keywords = {Gaussian mixture models, Random forests, hidden Markov models, noise immunity, over-fitting}, abstract = {Up to now, with the development of artificial intelligence, artificial energy technology has been widely used in medicine, text recognition, intelligent speech and other fields. As an important branch of artificial intelligence, machine learning plays a pivotal role in accumulating knowledge for artificial intelligence. In order to study better machine learning methods, this paper compares three machine learning models, Gaussian mixture model (GMM), random forest (RF) and hidden Markov model (HMM), analyzes and compares their advantages and disadvantages and better application fields in terms of accuracy and complexity, and combines simple algorithms to explore a model with higher accuracy. Experimental results have validated the model} }
@inproceedings{10.1145/3649329.3663510, title = {Invited: Leveraging Machine Learning for Quantum Compilation Optimization}, booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference}, year = {2024}, isbn = {9798400706011}, doi = {10.1145/3649329.3663510}, url = {https://doi.org/10.1145/3649329.3663510}, author = {Ren, Xiangyu and Zhang, Tianyu and Xu, Xiong and Zheng, Yi-Cong and Zhang, Shengyu}, keywords = {quantum computation, compiler optimization, machine learning, qubit routing, location = San Francisco, CA, USA}, abstract = {The design of quantum algorithms typically assumes the availability of an ideal quantum computer, characterized by full connectivity, noiseless operation, and unlimited coherence time. However, Noisy Intermediate-Scale Quantum (NISQ) devices present a stark contrast, with a limited number of qubits, non-negligible quantum operation errors, and stringent constraints on the connectivity of physical qubits within a Quantum Processing Unit (QPU). This necessitates the dynamic remapping of logical qubits to physical qubits within the compiler to facilitate the execution of two-qubit gates in the algorithm. However, this introduces additional operations, consequently reducing the fidelity of the algorithm. Therefore, minimizing the number of added gates becomes crucial. Finding such an optimal routing problem is NP-hard, and the task is conventionally addressed using human-crafted heuristics to search for SWAP sequences, but these lack performance guarantees. In this study, we employ a Seq2Seq machine learning model for the qubit routing task, incorporating a Transformer neural network to learn the routing information in the gate and SWAP sequence. Compared to heuristic search-based algorithms, our approach significantly reduces the overhead of quantum computing resources required to adapt logical circuits to physical circuits executable on specific quantum backend hardware.} }
@inproceedings{10.1145/3660317.3660324, title = {Benchmarking Machine Learning Applications on Heterogeneous Architecture using Reframe}, booktitle = {Proceedings of the 4th Workshop on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy}, pages = {16--22}, year = {2024}, isbn = {9798400706455}, doi = {10.1145/3660317.3660324}, url = {https://doi.org/10.1145/3660317.3660324}, author = {Rae, Christopher and Lee, Joseph K. L. and Richings, James and Weiland, Mich\`ele}, keywords = {HPC, reframe, kubernetes, benchmarking, data science, machine learning, MLPerf, GPU, graphcore, cerebras, location = Pisa, Italy}, abstract = {With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues. Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm. We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures.} }
@inproceedings{10.1145/3701716.3715863, title = {Graph Machine Learning under Distribution Shifts: Adaptation, Generalization and Extension to LLM}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {53--56}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715863}, url = {https://doi.org/10.1145/3701716.3715863}, author = {Wang, Xin and Li, Haoyang and Zhang, Zeyang and Zhu, Wenwu}, keywords = {adaptation, distribution shift, generalization, graph machine learning, large language model, location = Sydney NSW, Australia}, abstract = {Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution (I.D.) hypothesis, i.e., testing and training graph data are sampled from the identical distribution. However, this I.D. hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, several advanced graph machine learning techniques which go beyond the I.D. hypothesis, have made great progress and attracted ever-increasing attention from the research community. This tutorial is to disseminate and promote the recent research achievement on graph out-of-distribution adaptation, graph out-of-distribution generalization, and large language models for tackling distribution shifts, which are exciting and fast-growing research directions in the general field of machine learning and data mining. We will advocate novel, high-quality research findings, as well as innovative solutions to the challenging problems in graph machine learning under distribution shifts and the applications on graphs. This topic is at the core of the scope of The Web Conference, and is attractive to machine learning as well as data mining audience from both academia and industry.} }
@inproceedings{10.1145/3726854.3727285, title = {Exploring Function Granularity for Serverless Machine Learning Application with GPU Sharing}, booktitle = {Abstracts of the 2025 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems}, pages = {76--78}, year = {2025}, isbn = {9798400715938}, doi = {10.1145/3726854.3727285}, url = {https://doi.org/10.1145/3726854.3727285}, author = {Hui, Xinning and Xu, Yuanchao and Shen, Xipeng}, keywords = {cloud computing, function granularity, gpu sharing, machine learning for systems, serverless computing, location = Stony Brook, NY, USA}, abstract = {Recent years have witnessed increasing interest in machine learning (ML) inferences on serverless computing due to its auto-scaling and cost-effective properties. However, one critical aspect, function granularity, has been largely overlooked, limiting the potential of serverless ML. This paper explores the impact of function granularity on serverless ML, revealing its important effects on the SLO hit rates and resource costs of serverless applications. It further proposes adaptive granularity as an approach to addressing the phenomenon that no single granularity fits all applications and situations. It explores three predictive models and presents programming tools and runtime extensions to facilitate the integration of adaptive granularity into existing serverless platforms. Experiments show adaptive granularity produces up to a 29.2\% improvement in SLO hit rates and up to a 24.6\% reduction in resource costs over the state-of-the-art serverless ML which uses fixed granularity.} }
@inproceedings{10.1145/3656650.3656716, title = {User Insights shaping Machine Learning applied to Archives}, booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces}, year = {2024}, isbn = {9798400717642}, doi = {10.1145/3656650.3656716}, url = {https://doi.org/10.1145/3656650.3656716}, author = {Kasturi, Surya and Shenfield, Alex and Roast, Christopher}, keywords = {Archives, Machine Learning, UI, UX, user-centered designs, location = Arenzano, Genoa, Italy}, abstract = {Archives hold vast amounts of historical and cultural information, but navigating and extracting knowledge can be a daunting task. Machine learning (ML) offers immense potential to unlock these archives, yet its effectiveness hinges on understanding user needs. This paper explores how user insights can shape the development and application of ML in archives. Here “user” refers to editors and publishers who are crucial part of archival sorting and publication in the company. This paper emphasizes the importance of an iterative user centred design process to guide development and ensure user acceptance and empowerment. This approach reveals the distance between user expectations and functional integrity.} }
@inproceedings{10.1145/3672608.3707845, title = {SeismicSense: Phase Picking of Seismic Events with Embedded Machine Learning}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {551--559}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707845}, url = {https://doi.org/10.1145/3672608.3707845}, author = {Zainab, Tayyaba and Harms, Laura and Karstens, Jens and Landsiedel, Olaf}, keywords = {seismological data analysis, earthquake detection, TinyML, deep neural networks, low-power, internet of things, edge AI, location = Catania International Airport, Catania, Italy}, abstract = {Analyzing seismic data is essential for understanding natural geological processes and anthropogenic activities, particularly in localizing seismic events. While recent advances in seismic analysis rely heavily on resource-intensive machine learning approaches, these methods are impractical in resource-constrained environments such as underwater, underground, or rural areas. To address this, we introduce SeismicSense, a lightweight neural network (NN)-based solution for sensor-level seismic data analysis. SeismicSense detects seismic events and localizes them by identifying seismic event phases through a cascading architecture. Initially, SeismicSense uses an NN to filter out non-earthquake events, minimizing false positives. Upon identifying an earthquake, it detects the P- and S- phases, which are crucial for determining the origin and magnitude of seismic activity. SeismicSense significantly reduces data transmission by communicating only the arrival times of these phases to the cloud, enabling efficient and selective communication during seismic events. Despite being 20 times smaller than state-of-the-art models and requiring just 186 KB of RAM, SeismicSense achieves exceptional performance, with F1-scores of 99.4\% for earthquake detection, 98\% for P-wave detection, and 96\% for S-wave detection. Additionally, leveraging integer acceleration on modern MCUs enhances efficiency, reducing inference time on Cortex-M MCUs by 18-fold compared to non-accelerated methods, enabling real-time execution.} }
@inproceedings{10.1145/3733006.3733009, title = {Diabetes Detection: Predicting Type II Diabetes with Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 International Conference on Health Big Data}, pages = {16--22}, year = {2025}, isbn = {9798400713446}, doi = {10.1145/3733006.3733009}, url = {https://doi.org/10.1145/3733006.3733009}, author = {Zhang, Wenshan}, keywords = {Diabetes Management, Imputation Techniques, Logistic Regression, Neural Network, Random Forest, Ridge Regularization, Support Vector Machine}, abstract = {This project aims to predict type II diabetes using machine learning models and identify the one with the best performance, followed by recommendations for prevention strategies. To enhance data quality and address abnormal values, three separate imputation methods were evaluated, as well as the impact of feature standardization on various models. Logistic regression with Ridge regularization, support vector machine, Random Forest and a simple neural network with one hidden layer were implemented with four models for comparison. Some important aspects of working with the dataset included feature engineering, and hyperparameter-tuning with cross-validation to get the best model fit. Feature importance analysis also provided valuable insights into the relationships between health metrics and diabetes risk.} }
@inproceedings{10.1145/3569009.3572739, title = {Mix \&amp; Match Machine Learning: An Ideation Toolkit to Design Machine Learning-Enabled Solutions}, booktitle = {Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}, year = {2023}, isbn = {9781450399777}, doi = {10.1145/3569009.3572739}, url = {https://doi.org/10.1145/3569009.3572739}, author = {Jansen, Anniek and Colombo, Sara}, keywords = {ML capabilities, data types, design education, design ideation toolkit, machine learning, tangible user interface, location = Warsaw, Poland}, abstract = {Machine learning (ML) provides designers with a wide range of opportunities to innovate products and services. However, the design discipline struggles to integrate ML knowledge in education and prepare designers to ideate with ML. We propose the Mix \&amp; Match Machine Learning toolkit, which provides relevant ML knowledge in the form of tangible tokens and a web interface to support designers’ ideation processes. The tokens represent data types and ML capabilities. By using the toolkit, designers can explore, understand, combine, and operationalize the capabilities of ML and understand its limitations, without depending on programming or computer science knowledge. We evaluated the toolkit in two workshops with design students, and we found that it supports both learning and ideation goals. We discuss the design implications and potential impact of a hybrid toolkit for ML on design education and practice.} }
@inproceedings{10.1145/3595360.3595859, title = {MLflow2PROV: Extracting Provenance from Machine Learning Experiments}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595859}, url = {https://doi.org/10.1145/3595360.3595859}, author = {Schlegel, Marius and Sattler, Kai-Uwe}, keywords = {W3C PROV, provenance, MLflow, machine learning experiments, location = Seattle, WA, USA}, abstract = {Supporting iterative and explorative workflows for developing machine learning (ML) models, ML experiment management systems (ML EMSs), such as MLflow, are increasingly used to simplify the structured collection and management of ML artifacts, such as ML models, metadata, and code. However, EMSs typically suffer from limited provenance capabilities. As a consequence, it is hard to analyze provenance information and gain knowledge that can be used to improve both ML models and their development workflows. We propose a W3C-PROV-compliant provenance model capturing ML experiment activities that originate from Git and MLflow usage. Moreover, we present the tool MLflow2PROV that extracts provenance graphs according to our model, enabling querying, analyzing, and further processing of collected provenance information.} }
@inproceedings{10.1145/3747227.3747274, title = {The Challenges and Adaptations of the ‘New Elderly’ in China: A Study Based on SVM Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {301--306}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747274}, url = {https://doi.org/10.1145/3747227.3747274}, author = {Shao, Baowen and Rokeman, Muhammad Ihsan and Wang, Xueqin and Yang, Dongmei and Zhang, Shaohan}, keywords = {Learning adaptation, Machine learning, Mobile learning, New elderly, Support Vector Machine (SVM)}, abstract = {The rapid development of information technology, accompanied with the enormous demand of the “new elderly” for home-acceptance and the emerging number of “new elderly” in China, the connection of mobile learning with the life of new elderly has gained rapid popularity. However, due to different from young learners with lower educational backgrounds or rather closed learning attitudes, there are rare studies applied the approach and computational tool to analyze the adaptation process and main influencing factors between new elderly learning and mobile learning. In this research, we aim to analyze challenges and learning adaptation strategy of the new elderly in China learning by mobile learning, and the influential factors in this field using the computational tool of machine learning. We carried out the qualitative data method for analyzing process, that is semi-structured interview for elderly participants, who show active interest and attitudes of mobile learning, extract the main topics with thematic analysis method and utilize Support Vector Machine (SVM) algorithm for identifying the importance and prediction of influential factors between adaptation level. The interview results shows: (1) New elderly suffered from big challenges of anxiety from new technology, uncertainties in the result of learning and emotion. (2) But simultaneously they demonstrated good adaptation ability and motivation for learning continue. The SVM-based machine learning identified the high or low influence variables contributing adaptation level successfully. The two findings provide implications for “new elderly” education development. Not only the high-level data model but also a new path to show what are the main influencing variables contribute to adaptive learning in the machine learning model.} }
@inproceedings{10.1145/3635638.3635643, title = {Research on Diabetes Prediction Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence}, pages = {29--33}, year = {2024}, isbn = {9798400709456}, doi = {10.1145/3635638.3635643}, url = {https://doi.org/10.1145/3635638.3635643}, author = {Zhao, Kaina and Wang, Zhiping}, keywords = {Diabetes Prediction, Extreme Gradient Boosting Tree, Machine Learning, Random Forest, Support Vector Machine, location = Chongqing, China}, abstract = {Diabetes is an irreversible, chronic metabolic disease, and is now the third most important non-communicable disease threatening human health. Therefore, early diagnosis of diabetes is essential. In this paper, after preprocessing the Pima Indian diabetes dataset from the UCI database, support vector machine (SVM), random forest (RF), extreme gradient boosting tree (XG-BOOST) models and the stacking meta model which is created by combining these three models were used to diagnose the disease. The accuracy of the models SVM, RF, XG-BOOST and stacking were found to be 75.22\%, 72.99\%, 77.88\% and 98.67\%, respectively. F1_score, AUC value and recall were utilized for a detailed analysis of the models' classification results. The obtained results revealed that the stacking model has higher accuracy than single model.} }
@inbook{10.1145/3672608.3707756, title = {A Machine Learning-Based Approach For Detecting Malicious PyPI Packages}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {1617--1626}, year = {2025}, isbn = {9798400706295}, url = {https://doi.org/10.1145/3672608.3707756}, author = {Samaana, Haya and Costa, Diego Elias and Shihab, Emad and Abdellatif, Ahmad}, abstract = {Background. In modern software development, the use of external libraries and packages is increasingly prevalent, streamlining the software development process and enabling developers to deploy feature-rich systems with little coding. While this reliance on reusing code offers substantial benefits, it also introduces serious risks for deployed software in the form of malicious packages -harmful and vulnerable code disguised as useful libraries. Aims. Popular ecosystems, such PyPI, receive thousands of new package contributions every week, and distinguishing safe contributions from harmful ones presents a significant challenge. There is a dire need for reliable methods to detect and address the presence of malicious packages in these environments. Method. To address these challenges, we propose a data-driven approach that uses machine learning and static analysis to examine the package's metadata, code, files, and textual characteristics to identify malicious packages. Results. In evaluations conducted within the PyPI ecosystem, we achieved an F1-measure of 0.94 for identifying malicious packages using a stacking ensemble classifier. Conclusions. This tool can be seamlessly integrated into package vetting pipelines and has the capability to flag entire packages, not just malicious function calls. This enhancement strengthens security measures and reduces the manual workload for developers and registry maintainers, thereby contributing to the overall integrity of the ecosystem.} }
@inproceedings{10.1145/3747357.3747391, title = {Extraction of biomarkers from lung adenocarcinoma based on machine learning}, booktitle = {Proceedings of the 2025 International Symposium on Bioinformatics and Computational Biology}, pages = {220--227}, year = {2025}, isbn = {9798400714368}, doi = {10.1145/3747357.3747391}, url = {https://doi.org/10.1145/3747357.3747391}, author = {Zhang, Yiyi and Yang, Yuxia and Chaoluomeng and Li, Wentong and Li, Chuanhong}, keywords = {Biomarkers, Feature extraction, Lung adenocarcinoma, Machine learning, Optimization algorithm}, abstract = {Lung adenocarcinoma is one of the most common types of non-small cell lung cancer, which seriously affects the quality of life and survival rate of patients. In response to this problem, this study aims to optimize the feature extraction algorithm through in-depth analysis of lung adenocarcinoma related biomarkers, so as to improve the ability of early diagnosis and precise treatment. We used Lasso regression, non-negative matrix decomposition (NMF), point double correlation and principal component analysis (PCA) for feature extraction. The reliability and effectiveness of each method were evaluated by cross-validation, ROC curve and PR curve, and their performance in high dimensional data was compared. The optimized LASSO feature extraction method achieves up to 0.15 improvement in PR curve and up to 0.13 improvement in ROC curve. The method significantly improved the ability to identify key genes associated with tumor progression. The study demonstrated the effectiveness of the optimized feature extraction algorithm in the single cell sequencing data of lung adenocarcinoma, and found a series of biomarkers, such as sgrn, itk, ptprc, cst7, hcst,\&nbsp;cnot6l and rgs1\&nbsp;7 key genes, providing a new perspective for understanding the pathogenesis of lung adenocarcinoma. Although the sample size of this study was small and the clinical validation analysis was lacking, our findings provide new directions for future research, especially in the formulation of individualized treatment protocols. Future studies will aim to expand the sample size and further validate the clinical applicability of these biomarkers in combination with clinical data, thereby driving the development of early diagnosis and precision treatment of lung adenocarcinoma.} }
@inproceedings{10.1145/3653724.3653764, title = {Prediction of Water's Safety for Consumption by Machine Learning}, booktitle = {Proceedings of the International Conference on Mathematics and Machine Learning}, pages = {234--239}, year = {2024}, isbn = {9798400716973}, doi = {10.1145/3653724.3653764}, url = {https://doi.org/10.1145/3653724.3653764}, author = {Li, Jingyi}, abstract = {Water quality is the most important topic in the world. Most researchers study water using machine learning techniques to determine its potability. This paper applies similar approaches to predict the water potability and discusses why the accuracy differs. Water quality is closely related to people's lives. Cultivated land and industrial production both rely on water. The emission of sewage resulting from industrial production also endanger human health. According to the Logistic Regression, Decision Tree, Random Forest, and Extreme Gradient Boosting models, an analysis was performed to determine the best model for predicting water potability. The Random Forest model is the best model for predicting whether water is potable or not. However, the accuracy for this model is lower. The reasons may be featuring choice, parameter modification, and deficient evaluation standards. For the results, it is recommended to use all the features to train the model in similar research in the future. And set multiple parameters before building the models and compare them.} }
@inproceedings{10.1145/3759023.3759106, title = {Integrating Machine Learning, Internet of Things, and Unmanned Aerial Vehicles for Crop Farming}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759106}, url = {https://doi.org/10.1145/3759023.3759106}, author = {Petja, Moshito Mareme and Langa, Makhulu Relebogile and Moeti, Michael Nthabiseng}, keywords = {Agriculture, Crop Farming, IoT-ML-UAV, Soil Nutrition, Soil Testing.}, abstract = {Agriculture is essential for sustaining the increasing world population. However, enhancing crop yield and resource management continues to be a considerable challenge for farmers. This research study presents a novel machine learning-enabled Internet of Things (IoT) device and Unmanned Aerial Vehicles (UAV) for monitoring soil nutrients and delivering precise crop suggestions. Other studies show the use of UAVs, IoT, and/or machine learning (ML) in the domain of agriculture and yet these technologies were utilized separately, leaving integration an area to explore. This study integrates these technologies, in addition with a mobile application that the farmers can use to acquire tailored recommendations. The prototype employs Analogue Soil Moisture, Nitrogen, Phosphorus, and Potassium (NPK), and pH sensors that are attached to a UAV to monitor soil nutrients. These make it suitable for the UAV to fly to any area of the farm to collect soil samples. The gathered data is conveyed to a local server via the Arduino UNO 3’s gateway protocol. ML algorithms are utilized to analyze the gathered data and produce tailored recommendations, including a potential list of high-yield crops, specific fertilizers, and their quantities based on crop needs and soil nutrients. The mobile application enables the assessment of user acceptability at the farmer’s level. The system’s efficacy is assessed by field studies, contrasting its performance with conventional approaches. The findings illustrate the prototype’s capacity to improve crop yield and maximize resource efficiency, fostering sustainable agricultural methods and food security. This research advances IoT-enabled agriculture by illustrating the efficacy of machine learning approaches in enhancing soil nutrient management, enabling informed decisions regarding crop fertilizers, and evaluating the quality of crops.} }
@inproceedings{10.1145/3696271.3696275, title = {Developing A System for Automatic Prediction of Polycystic Ovary Syndrome Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {20--26}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696275}, url = {https://doi.org/10.1145/3696271.3696275}, author = {Akanbi, Kemi and Adepoju, Odunayo Gabriel and Nti, Kofi Isaac}, keywords = {Diagnosis, Infertility, Machine Learning, Polycystic Ovary Syndrome}, abstract = {Polycystic Ovary Syndrome (PCOS) is a condition that leads to lifelong health problems outside of infertility. The lack of a single, known cause and universal symptoms makes diagnosis challenging. The early and accurate prediction will prevent many subsequent serious and morbid illnesses that can arise from PCOS. Therefore, this study proposes a predictive Machine Learning (ML) model to identify patients at risk of PCOS and alert healthcare professionals, allowing for early intervention. The predictive performance of the Random Forest Classifier, Logistic Regression, Gradient Boost, Adaptive Boost, and XGBoost machine learning algorithms was compared based on accuracy, precision, recall, and F1-score with a publicly available dataset from Kaggle. The experiment was performed in Google Colab. Our experimental results showed a good predictive performance of 90\% across all evaluation metrics. However, Random Forest outperformed all other models achieving 96\% accuracy, precision, recall, and F1-score, respectively. The high accuracy (96\%) obtained by this study suggests that the proposed model could effectively identify patients at risk of PCOS, potentially aiding early diagnosis and intervention.\&nbsp;} }
@inproceedings{10.1145/3731545.3736817, title = {Performance Evaluation of Machine Learning Applications Using WebAssembly Across Different Programming Languages}, booktitle = {Proceedings of the 34th International Symposium on High-Performance Parallel and Distributed Computing}, year = {2025}, isbn = {9798400718694}, doi = {10.1145/3731545.3736817}, url = {https://doi.org/10.1145/3731545.3736817}, author = {Khan, Sallar and Malik, Tania and Hasanov, Khalid}, keywords = {webassembly, wasm, WASI, machine learning applications, Python, C++, rust, location = University of Notre Dame Conference Facilities, Notre Dame, IN, USA}, abstract = {WebAssembly (WASM) has emerged as a promising compilation target for languages traditionally not executed on the web and enable the cross-platform deployment of high-performance applications. While its general use cases have been well studied, the performance implications of executing Machine Learning (ML) workloads via WASM across different programming languages and runtime environments remain relatively unexplored. This paper presents a systematic evaluation of two representative ML models, K-Means and Logistic Regression, implemented in Python, Rust, and C++ and compiled to WASM. These models are executed in two distinct environments: a web browser and the WebAssembly System Interface (WASI), and their execution time and accuracy is compared against each programming language across both environments. This study aims to provide insight into the trade-offs and practical considerations involved in deploying ML workloads using WebAssembly across different language ecosystems and runtime configurations.} }
@article{10.1145/3729394, title = {Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729394}, url = {https://doi.org/10.1145/3729394}, author = {Sharmin, Shaila and Zahid, Anwar Hossain and Bhattacharjee, Subhankar and Igwilo, Chiamaka and Kim, Miryung and Le, Wei}, keywords = {Fuzzing, Machine learning code, Numerical instability, Soft assertions}, abstract = {Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted ”tumor”, but instead, it incorrectly predicted ”no tumor” due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32.} }
@inproceedings{10.1145/3728199.3728220, title = {An Improved and Combined Model for Data Pricing Based on Automated Machine Learning and Knowledge Graph}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {137--141}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728220}, url = {https://doi.org/10.1145/3728199.3728220}, author = {Su, Xinyi and Wang, Xiaodong and Li, Biao and Huang, Xiaoping and Zhong, Yuyanzhen and Shuai, Yong}, keywords = {Automated machine learning models, Data pricing, Feature recommendation model, Knowledge graph}, abstract = {Aiming at the problems of the current data pricing model, such as the difficulty of evaluation, the lack of scientific and unified rules for data transactions, the lack of privacy protection of data, and the poor accuracy of pricing, this dissertation proposes an improved data pricing combination model, which firstly establishes a feature recommendation model based on the knowledge graph, selects the optimal feature combinations, and improves the automated machine learning algorithms to obtain the optimal machine learning regression model for data pricing. Finally, the case study demonstrates that the method proposed in this paper has good applicability and accuracy} }
@proceedings{10.1145/3759928, title = {IPMLP '25: Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, year = {2025}, isbn = {9798400715884} }
@inproceedings{10.1145/3637528.3671471, title = {Graph Machine Learning Meets Multi-Table Relational Data}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6502--6512}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671471}, url = {https://doi.org/10.1145/3637528.3671471}, author = {Gan, Quan and Wang, Minjie and Wipf, David and Faloutsos, Christos}, keywords = {data augmentation, graph machine learning, graph neural networks, relational databases, tabular modeling, location = Barcelona, Spain}, abstract = {While graph machine learning, and notably graph neural networks (GNNs), have gained immense traction in recent years, application is predicated on access to a known input graph upon which predictive models can be trained. And indeed, within the most widely-studied public evaluation benchmarks such graphs are provided, with performance comparisons conditioned on curated data explicitly adhering to this graph. However, in real-world industrial applications, the situation is often quite different. Instead of a known graph, data are originally collected and stored across multiple tables in a repository, at times with ambiguous or incomplete relational structure. As such, to leverage the latest GNN architectures it is then up to a skilled data scientist to first manually construct a graph using intuition and domain knowledge, a laborious process that may discourage adoption in the first place. To narrow this gap and broaden the applicability of graph ML, we survey existing tools and strategies that can be combined to address the more fundamental problem of predictive tabular modeling over data native to multiple tables, with no explicit relational structure assumed a priori. This involves tracing a comprehensive path through related table join discovery and fuzzy table joining, column alignment, automated relational database (RDB) construction, extracting graphs from RDBs, graph sampling, and finally, graph-centric trainable predictive architectures. Although efforts to build deployable systems that integrate all of these components while minimizing manual effort remain in their infancy, this survey will nonetheless reduce barriers to entry and help steer the graph ML community towards promising research directions and wider real-world impact.} }
@inproceedings{10.1145/3637528.3671483, title = {2nd Workshop on Causal Inference and Machine Learning in Practice}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6726}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671483}, url = {https://doi.org/10.1145/3637528.3671483}, author = {Lee, Jeong-Yoon and Wu, Yifeng and Harinen, Totte and Pan, Jing and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Zheng, Zeyu and Vanchinathan, Hasta and Wang, Yingfei and Stevenson, Roland}, keywords = {causal inference, machine learning, location = Barcelona, Spain}, abstract = {The workshop's rationale stems from the escalating interest in causal inference and machine learning methodologies within various industrial contexts. This surge in demand underscores the importance for both scholars and practitioners to exchange knowledge and best practices regarding the application of these techniques to tackle real-world challenges. Yet, applying causal machine learning techniques in real-world scenarios presents a range of challenges not addressed in the academic literature. This workshop aims to address the challenges for practical causal machine learning and explore new industry use cases. The workshop will provide a forum for practitioners and researchers to exchange ideas and explore new collaborations. Moreover, this workshop aims to capitalize on the success and achievements of the KDD 2023 Workshop titled "Causal Inference and Machine Learning in Practice".} }
@inproceedings{10.1145/3641554.3701783, title = {Approachable Machine Learning Education: A Spiral Pedagogy Approach with Experiential Learning}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {924--930}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701783}, url = {https://doi.org/10.1145/3641554.3701783}, author = {Qin, Meiying}, keywords = {computer science education, experiential learning, machine learning, spiral approach, location = Pittsburgh, PA, USA}, abstract = {Machine learning (ML) is an important subject for computer science students to learn due to its broad applications. Introductory courses often present techniques in a linear sequence, resulting in a steep learning curve that can overwhelm students and limit the time for experiential learning through course projects. To address this, I restructured the course using a spiral approach, presenting concepts in three iterations. Each iteration delves deeper into the material and introduces complex computational topics progressively. This method includes a built-in repetition mechanism that reinforces learning and enhances understanding. Moreover, this approach allows time for hands-on projects that apply theory to real-world scenarios, helping students better understand the course materials. The spiral approach was implemented in an ML course at a local university, resulting in positive student feedback and improved course retention rates.} }
@inproceedings{10.1145/3715669.3723129, title = {Eye Movements as Indicators of Deception: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3723129}, url = {https://doi.org/10.1145/3715669.3723129}, author = {Foucher, Valentin and de Leon-Martinez, Santiago and Moro, Robert}, keywords = {Eye Movements, Gaze, Pupil, Deception Detection, Concealed Information Test, Machine Learning, Feature Importance}, abstract = {Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74\% in a binary classification task (Revealing vs. Concealing) and 49\% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.} }
@inproceedings{10.1145/3617574.3617859, title = {MLGuard: Defend Your Machine Learning Model!}, booktitle = {Proceedings of the 1st International Workshop on Dependability and Trustworthiness of Safety-Critical Systems with Machine Learned Components}, pages = {10--13}, year = {2023}, isbn = {9798400703799}, doi = {10.1145/3617574.3617859}, url = {https://doi.org/10.1145/3617574.3617859}, author = {Wong, Sheng and Barnett, Scott and Rivera-Villicana, Jessica and Simmons, Anj and Abdelkader, Hala and Schneider, Jean-Guy and Vasa, Rajesh}, keywords = {system validation, error handling, design by contract, ML validation, location = San Francisco, CA, USA}, abstract = {Machine Learning (ML) is used in critical highly regulated and high-stakes fields such as finance, medicine, and transportation. The correctness of these ML applications is important for human safety and economic benefit. Progress has been made on improving ML testing and monitoring of ML. However, these approaches do not provide i) pre/post conditions to handle uncertainty, ii) defining corrective actions based on probabilistic outcomes, or iii) continual verification during system operation. In this paper, we propose MLGuard, a new approach to specify contracts for ML applications. Our approach consists of a) an ML contract specification defining pre/post conditions, invariants, and altering behaviours, b) generated validation models to determine the probability of contract violation, and c) an ML wrapper generator to enforce the contract and respond to violations. Our work is intended to provide the overarching framework required for building ML applications and monitoring their safety.} }
@article{10.1145/3764578, title = {Emerging Trends in Early Dementia Diagnosis: An Analysis on Advanced Machine Learning Approaches}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3764578}, url = {https://doi.org/10.1145/3764578}, author = {Gami, Badal and Agrawal, Manav and Katarya, Rahul}, keywords = {Alzheimer’s, Artificial Intelligence (AI), dementia, Machine Learning(ML), Natural Language Processing (NLP), neuroimaging}, abstract = {Dementia is the waning of cognitive abilities, which is typically seen with the natural aging process and includes issues with memory, language, and problem-solving abilities. Artificial Intelligence (AI) techniques are one viable method for the diagnosis of dementia. Despite recent advances in dementia informatics research and AI, accurate early diagnoses are still far from ideal. This study focuses on showcasing a comprehensive analysis of emerging AI approaches applied to early dementia diagnosis, highlighting trends across neuroimaging, speech, EEG, and clinical data. The proposed work’s main contributions include a summary of the potential challenges and vulnerabilities with dementia informatics research, a wide range of diagnostic issues in dementia care, a descriptive comparison of the elementary manuscripts judged on evaluation parameters such as precision, responsiveness, and definiteness and an offering of a descriptive set of data for developing Machine Learning (ML) and Deep Learning (DL) models. The manuscript also provides a valuable overview of new avenues for informatics research on dementia and advanced ML. The main objective is to fill a gap in the literature by offering an in-depth analysis and overview of the application of AI in dementia research, providing a foundational roadmap for accelerating impactful, data-driven dementia care solutions.} }
@inbook{10.1145/3730436.3730437, title = {Ballistic wind speed identification method based on machine learning}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {1--5}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730437}, author = {Zhou, Hanbing and Qu, Wenbo and Zhang, Zechen and Fu, Jian}, abstract = {This study aims to improve the accuracy and reliability of trajectory prediction by using an ensemble learning regression model to accurately identify ballistic wind speed. We train and optimize the ensemble of multiple regression models, and combine the Bayesian optimizer to adjust the hyperparameters to improve the model performance. In this study, two ensemble learning regression models are used to train the horizontal wind and vertical wind respectively, and the five-fold cross-validation method is used to compare the identification effect of the proposed model and the Kalman filter method. The experimental results show that our proposed method has achieved significant improvement in ballistic wind speed identification task, and has higher accuracy and stability than traditional methods. This study provides a new idea and method for improving the accuracy of trajectory prediction, which has important scientific significance and practical application prospects.} }
@inproceedings{10.1145/3688671.3688739, title = {Molecular Simulation of Coarse-grained Systems using Machine Learning}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688739}, url = {https://doi.org/10.1145/3688671.3688739}, author = {Gerakinis, Dimitrios-Paraskevas and Ricci, Eleonora and Giannakopoulos, George and Karkaletsis, Vangelis and Theodorou, Doros N. and Vergadou, Niki}, keywords = {Coarse Graining, Convolutional Graph Neural Networks, Machine Learning, Molecular Simulations, Multiscale Modelling}, abstract = {Hierarchical multiscale methods are essential for molecular simulations of complex chemical systems, such as organic fluids and soft matter systems in order to reach longer length and time scales. Coarse-Graining (CG) is often the basis of multiscale schemes. Machine Learning (ML) techniques have been recently explored for the development of atomistic force fields based on quantum mechanical calculations. However, integrating ML methods into CG force fields for bulk molecular systems is still rare. In this work, Graph Convolutional Neural Network (GCNN) architectures were adopted to develop CG Machine Learned potentials for bulk amorphous systems, implementing a strategy that includes a force-matching scheme using benzene liquid as a test system. The ML-based CG force fields developed were evaluated by conducting molecular dynamics simulations at the CG level, and the extracted properties were compared with the atomistic simulations to assess the effectiveness of the ML CG interaction potentials. The impact of hyperparameters, loss function construction, and GCNN architecture size have been examined, providing valuable insights for ML-based CG approaches in bulk soft matter systems.} }
@inproceedings{10.1145/3587423.3595530, title = {Machine Learning \&amp; Neural Networks}, booktitle = {ACM SIGGRAPH 2023 Courses}, year = {2023}, isbn = {9798400701450}, doi = {10.1145/3587423.3595530}, url = {https://doi.org/10.1145/3587423.3595530}, author = {Sharma, Rajesh and Tang, Mia} }
@inproceedings{10.1145/3737895.3768305, title = {UAV-aided Fast Data Collection via Machine Learning Using AERPAW's Digital Twin}, booktitle = {Proceedings of the ACM Workshop on Wireless Network Testbeds, Experimental Evaluation \&amp; Characterization}, pages = {89--96}, year = {2025}, isbn = {9798400719721}, doi = {10.1145/3737895.3768305}, url = {https://doi.org/10.1145/3737895.3768305}, author = {Sadique, Joarder Jafor and Khan, Mahfizur Rahman and Ibrahim, Ahmed S.}, keywords = {digital twin, data collection, k-means clustering, trajectory optimization, and UAV, location = Kerry Hotel, Hong Kong, Hong Kong, China}, abstract = {Unmanned aerial vehicles (UAVs) have emerged as essential components for 5G and beyond networks. Particularly, UAVs serve as aerial data mules, capable of efficiently collecting and transferring data from geographically dispersed ground-based nodes, such as ground base stations. However, practical limitations like battery life constraints, stringent mission timeliness, and varying wireless conditions present significant challenges for efficient data collection. To reduce the mission completion time, optimizing UAV's trajectory is often considered a primary challenge. In addition, data collection time under hovering mode also greatly affect the mission time. In this work, we propose machine learning (ML)-based UAV-aided data collection model that leverages K-means clustering to identify optimal waypoints near each base station within Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) geofence area, located in Raleigh, North Carolina, USA. By adopting the clustering strategy, our primary goal is to reduce flight time without compromising the high signal-to-noise ratio (SNR) across trajectories. Our proposed model also tackles the challenge of dynamic hovering time adjustment at each waypoint by incorporating an adaptive approach based on instantaneous link quality and data volume. To validate the proposed data collection model, we utilize a digital twin simulation environment from AERPAW. The results emulated over the AERPAW's digital twin demonstrate the effectiveness of the proposed algorithm, ensuring enhanced efficiency in the UAV-aided fast data collection strategy.} }
@inproceedings{10.1145/3715071.3750419, title = {Impact of Time Discrepancies on Machine Learning Performance for Multi-Wearable Human Activity Recognition}, booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers}, pages = {98--105}, year = {2025}, isbn = {9798400714818}, doi = {10.1145/3715071.3750419}, url = {https://doi.org/10.1145/3715071.3750419}, author = {Wolling, Florian and Kostolani, David and Trollmann, Patrick and Michahelles, Florian}, keywords = {action segmentation, cnn, human activity recognition, machine learning, ms-tcn, offset, skew, synchronization, time discrepancy, location = Espoo, Finland}, abstract = {Wearable-based human activity recognition (HAR) has become a relevant tool for identifying everyday activities in various domains like healthcare, sports, and human-computer interaction (HCI). The classification performance can be improved by using multiple complementary sensors, which require accurately matched time bases. Although previous studies on synchronization in HAR suggested that sub-second accuracy is advisable while sub-100 ms accuracy is unnecessary, the specific effect of time discrepancies on machine learning models remained unexplored. We address this with an empirical evaluation of the impact of time discrepancies in multi-wearable HAR. We apply a systematic approach using the example of multi-stage temporal convolutional networks (MS-TCN) for action segmentation, simulating the time discrepancies of time offset and clock skew via rational resampling. Our evaluation spanned 30,025 training and validation runs across different model configurations, totaling over one million core-hours of computation. Our results reveal that time offsets larger than 150 ms should be avoided in training datasets, and offsets beyond 300 ms can already significantly degrade the HAR performance for typical activities of daily living (ADLs). The findings highlight the need for adequate synchronization of training datasets. Our findings have implications for the design and deployment of multi-wearable HAR systems and may extend to other multi-sensor contexts.} }
@inproceedings{10.1145/3677779.3677842, title = {Research on the Load_breast_cancer data set under Multiple Machine Learning Algorithms}, booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {386--392}, year = {2024}, isbn = {9798400709760}, doi = {10.1145/3677779.3677842}, url = {https://doi.org/10.1145/3677779.3677842}, author = {Song, Meirui and Wu, Shixiao and Huang, Rujuan}, abstract = {breast cancer ranks first in female malignant tumors. Early detection and diagnosis is the key to treatment. This paper uses the open-source load_break_cancer breast cancer data set, mainly uses random forest, support vector machine, logical regression, Gauss naive Bayesian algorithm, BP neural network algorithm, k-neighborhood algorithm and XGBoost algorithm to classify and predict the breast cancer data set, conducts a lot of training and testing on the data set under a variety of machine learning algorithms, analyzes the learning curve in the training process, analyzes the training and testing results, and analyzes the performance of the algorithm processing data, which is of great significance for breast cancer diagnosis and treatment.} }
@inproceedings{10.1145/3731806.3731840, title = {Enhancing Panic Attack Predictions: Addressing Data Imbalance in Machine Learning Models}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {1--5}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731840}, url = {https://doi.org/10.1145/3731806.3731840}, author = {Alliandre, Fazrul Ridha and Hikmawati, Erna}, keywords = {Data Balancing Strategy, Decision Tree, Machine Learning, Panic Attack Prediction, Random Forest}, abstract = {Panic disorder (PD) is a prevalent mental health condition characterized by recurring panic attacks, affecting millions globally. Despite advancements in machine learning (ML) for mental health predictions, data imbalance remains a significant challenge, often impairing model performance. This study aims to enhance the accuracy and reliability of panic attack prediction models by implementing a novel data balancing strategy. Using the College Student Mental Health Dataset, ML models—Decision Tree and Random Forest—were evaluated before and after applying data balancing techniques. Initial findings revealed suboptimal model performance due to imbalanced data, with AUC values of 0.434 (Decision Tree) and 0.527 (Random Forest). Post-balancing, significant improvements were observed, with AUC increasing to 0.768 and 0.612, respectively. This study highlights the critical role of data balancing in optimizing ML models, even with limited datasets, and provides a foundation for future research on scalable predictive technologies for mental health disorders. The findings underscore the potential of simple yet effective strategies in advancing early detection systems for mental health conditions.} }
@article{10.1145/3624010, title = {A Survey of Privacy Attacks in Machine Learning}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3624010}, url = {https://doi.org/10.1145/3624010}, author = {Rigaki, Maria and Garcia, Sebastian}, keywords = {model inversion, reconstruction, model extraction, property inference, membership inference, machine learning, Privacy}, abstract = {As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 45 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.} }
@inproceedings{10.1145/3715335.3735488, title = {Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {530--546}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735488}, url = {https://doi.org/10.1145/3715335.3735488}, author = {Bhaskar, Mihir and Luo, Jun Tao and Geng, Zihan and Hajra, Asmita and Howell, Junia and Gormley, Matthew R.}, keywords = {historical document understanding, housing data, computer vision, OCR, machine learning, regression}, abstract = {Despite well-documented consequences of the U.S. government’s 1930s housing policies on racial wealth disparities, scholars have struggled to quantify its precise financial effects due to the inaccessibility of historical property appraisal records. Many counties still store these records in physical formats, making large-scale quantitative analysis difficult. We present an approach scholars can use to digitize historical housing assessment data, applying it to build and release a dataset for one county. Starting from publicly available scanned documents, we manually annotated property cards for over 12,000 properties to train and validate our methods. We use OCR to label data for an additional 50,000 properties, based on our two-stage approach combining classical computer vision techniques with deep learning-based OCR. For cases where OCR cannot be applied, such as when scanned documents are not available, we show how a regression model based on building feature data can estimate the historical values, and test the generalizability of this model to other counties. With these cost-effective tools, scholars, community activists, and policy makers can better analyze and understand the historical impacts of redlining.} }
@inproceedings{10.1145/3712623.3712646, title = {zkTaylor: Zero Knowledge Proofs for Machine Learning via Taylor Series Transformation}, booktitle = {Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications}, pages = {37--41}, year = {2025}, isbn = {9798400712883}, doi = {10.1145/3712623.3712646}, url = {https://doi.org/10.1145/3712623.3712646}, author = {Pan, Dong and Liu, Kezhen and Li, Bingtao and Zheng, Yongsheng and Ma, Jiren}, keywords = {Exponential function, Machine learning, Taylor expansion, Zero-knowledge Proof}, abstract = {In order to enable more types of machine learning models to use zero-knowledge proofs to enhance their computational verifiability, this study proposes a zero-knowledge machine learning conversion method based on the Taylor series. Firstly, a polynomial expansion of structures with transcendental functions in ordinary machine learning models is performed using Taylor's formula. The corresponding arithmetic circuit descriptions are written in ZKP based on the converted model structures. Finally, the proof body is generated, which allows the verifier to verify the correctness of the results quickly. The basic experimental idea is also given, and the scheme's feasibility is verified, which can be done to provide a verification path for the model without seriously affecting its accuracy.} }
@inproceedings{10.1145/3678299.3678313, title = {A machine learning approach to gesture detection in violin performance}, booktitle = {Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures}, pages = {144--151}, year = {2024}, isbn = {9798400709685}, doi = {10.1145/3678299.3678313}, url = {https://doi.org/10.1145/3678299.3678313}, author = {Lucena, Raquel and Ramirez, Rafael}, keywords = {audio features, gesture prediction, machine learning, pose estimation, violin, location = Milan, Italy}, abstract = {Playing a musical instrument is a highly complex activity. It requires mental and sensorimotor skills, which are learned during a long trajectory. Learning to play an instrument typically involves long periods of practice without teacher supervision. Systems providing feedback on the student’s performance during these self-study periods hold significant potential for improving the learning process. We present a machine learning approach to assess the correctness of body gestures in violin performances. We collect images and audio of several violinists performing correct and incorrect postures, extract image and audio descriptors, and apply machine learning algorithms to classify violin gestures. Finally, a real-time feedback system designed for pedagogical use is implemented, in which users receive visual feedback about whether the given gesture is performed properly or not. The system not only has the potential to facilitate the development of sensorimotor skills essential for playing violin, but also can enhance the learning experience for musicians, potentially providing benefits in musical education, performance, and health.} }
@proceedings{10.1145/3708360, title = {ICMML '24: Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, year = {2024}, isbn = {9798400711657} }
@article{10.1145/3766551, title = {TCAD-Machine Learning Enabled TID Compact Model Development for Commercial SiC MOSFET}, journal = {ACM Trans. Des. Autom. Electron. Syst.}, year = {2025}, issn = {1084-4309}, doi = {10.1145/3766551}, url = {https://doi.org/10.1145/3766551}, author = {Gao, Xujiao and Ray, Jaideep and Rummel, Brian and Glaser, Caleb and Rhoades, Elaine and Young, Joshua and Musson, Larry and Buchheit, Thomas}, keywords = {COTS, SiC MOSFET, TID, TCAD, Charon, Dakota, threshold voltage shift, Bayesian inference, Bayesian calibration, surrogate models, random forests}, abstract = {We propose a TCAD-machine learning coupled approach that combines a TCAD tool (Charon), optimization/uncertainty quantification tool (Dakota), surrogate models, and Bayesian learning capabilities. The coupling approach is used for accurate modeling and calibration of total ionizing dose (TID) induced threshold voltage (Vth) shifts in Commercial-Off-The-Shelf (COTS) semiconductor devices and to develop physics-informed TID compact models. This versatile approach is applied to model the TID effect in an exemplar COTS 3.3 kV SiC power MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor). With the Charon-Dakota coupling, we can determine key device geometry and doping values based on device physics, which are difficult to obtain or not available for COTS devices but important for TCAD simulation; additionally, we can efficiently generate thousands of simulation results in a large parameter space, which makes it possible to develop data-driven surrogate models and perform Bayesian calibration. Utilizing the full tool-coupling approach, we achieve calibrated TCAD simulation models that accurately capture the average TID-induced Vth shifts behavior with total doses and Vth shifts saturation at high doses as observed in experimental data. More importantly, the calibrated TCAD simulations are obtained with determined TID model parameters (e.g., hole trap density and capture cross section) values that contain well quantified uncertainties. Furthermore, we can isolate and quantify the noises that are not captured by the TCAD models but exist in the measured data due to measurements and devices variabilities. Lastly, the calibrated surrogate models are used to develop physics-informed TID compact models. The method is generalizable to other devices and/or radiation conditions with little modifications and can provide well-determined uncertainties.} }
@inproceedings{10.1145/3627703.3629563, title = {Accelerating Privacy-Preserving Machine Learning With GeniBatch}, booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems}, pages = {489--504}, year = {2024}, isbn = {9798400704376}, doi = {10.1145/3627703.3629563}, url = {https://doi.org/10.1145/3627703.3629563}, author = {Huang, Xinyang and Zhang, Junxue and Cheng, Xiaodian and Zhang, Hong and Jin, Yilun and Hu, Shuihai and Tian, Han and Chen, Kai}, keywords = {batch compiler, homomorphic encryption, privacy-preserving machine learning, location = Athens, Greece}, abstract = {Cross-silo privacy-preserving machine learning (PPML) adopt; Partial Homomorphic Encryption (PHE) for secure data combination and high-quality model training across multiple organizations (e.g., medical and financial). However, PHE introduces significant computation and communication overheads due to data inflation. Batch optimization is an encouraging direction to mitigate the problem by compressing multiple data into a single ciphertext. While promising, it is impractical for a large number of cross-silo PPML applications due to the limited vector operations support and severe data corruption.In this paper, we present GeniBatch, a batch compiler that translates a PPML program with PHE into an efficient program with batch optimization. GeniBatch adopts a set of conversion rules to allow PHE programs involving all vector operations required in cross-silo PPML and ensures end-to-end result consistency before/after compiling. By proposing bit-reserving algorithms, GeniBatch avoids bit-overflow for the correctness of compiled programs and maximizes the compression ratio. We have integrated GeniBatch into FATE, a representative cross-silo PPML framework, and provided SIMD APIs to harness hardware acceleration. Experiments across six popular applications show that GeniBatch achieves up to 22.6 speedup and reduces network traffic by 5.4-23.8 for generic cross-silo PPML applications.} }
@inproceedings{10.1145/3762329.3762353, title = {Workshop temperature and humidity prediction based on machine learning algorithms}, booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence of Things and Computing}, pages = {136--145}, year = {2025}, isbn = {9798400718625}, doi = {10.1145/3762329.3762353}, url = {https://doi.org/10.1145/3762329.3762353}, author = {Yang, Hualun and Ouyang, Shaojun and Sun, Fuyan and Zeng, Xiang and Zeng, Yi and Zhang, Ruiping and Liu, Hui}, keywords = {Feature Engineering, Model Prediction, Random Forest, Temperature and Humidity Prediction, Tobacco Processing Workshop}, abstract = {The tobacco leaf processing workshop lacks constant temperature control, causing its production environment to be susceptible to external climatic influences, leading to fluctuations in product quality. To improve product quality stability, this paper uses historical indoor and outdoor temperature and humidity data from February 2023 to August 2024. Feature engineering was employed to enhance data representation, and a Random Forest (RF) model was used for temperature and humidity prediction. The results indicate that, under optimal parameters, the root mean square error (RMSE) for temperature prediction is 0.01, and for humidity prediction is 0.26, demonstrating high predictive accuracy. By accurately forecasting future workshop temperature and humidity, the system provides data support to operators, effectively improving the accuracy of the moisture setting in the tobacco processing process and ensuring product quality.} }
@inproceedings{10.1145/3718751.3718889, title = {Trading Stocks on RSI and KDJ: A Machine Learning Model}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {846--853}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718889}, url = {https://doi.org/10.1145/3718751.3718889}, author = {Chen, Jiawei}, keywords = {Machine learning, Regression model, Technical financial index}, abstract = {This study aims to forecast future stock prices by establishing statistical models, thereby aiding investors in making more informed investment decisions. Initially, this paper discusses in detail how to select the most suitable technical financial indicators for use as model inputs. Subsequently, statistical models targeted for stock price prediction are constructed using machine learning techniques. These models utilize meticulously filtered historical data to ensure the highest efficacy. Moreover, the study involves verification of the model's application across a variety of stocks, ensuring the universal applicability of the proposed prediction model. The empirical analysis results of this paper indicate that the constructed statistical models are capable of effectively predicting future stock price trends, providing a powerful decision-support tool for different types of investors, and thereby potentially enhancing investors' profitability in the complex and ever-changing financial markets.} }
@article{10.1145/3670007, title = {Machine Learning with Confidential Computing: A Systematization of Knowledge}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3670007}, url = {https://doi.org/10.1145/3670007}, author = {Mo, Fan and Tarkhani, Zahra and Haddadi, Hamed}, keywords = {Privacy-preserving machine learning, confidential computing, trusted execution environment}, abstract = {Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML’s pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this article, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide (i)\&nbsp;confidentiality guarantees and (ii)\&nbsp;integrity assurances and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, TEE-aware ML, and ML full pipeline guarantees. By providing these potential solutions in our systematization of knowledge, we aim to build the bridge to help achieve a much stronger TEE-enabled ML for privacy guarantees without introducing computation and system costs.} }
@inproceedings{10.1109/SCW63240.2024.00127, title = {An Efficient Checkpointing System for Large Machine Learning Model Training}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {896--900}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00127}, url = {https://doi.org/10.1109/SCW63240.2024.00127}, author = {Xu, Wubiao and Huang, Xin and Meng, Shiman and Zhang, Weiping and Guo, Luanzheng and Sato, Kento}, abstract = {Checkpointing is one of the fundamental techniques to resume training while system fails. It has been generally used in various domains, such as high-performance computing (HPC) and machine learning (ML). However, as machine learning models increase in size and complexity rapidly, the cost of checkpointing in ML training became a bottleneck in storage and performance (time). For example, the latest GPT-4 model has massive parameters at the scale of 1.76 trillion. It is highly time and storage consuming to frequently writes the model to checkpoints with more than 1 trillion floating point values to storage. This work aims to understand and attempt to mitigate this problem. First, we characterize the checkpointing interface in a collection of representative large machine learning/language models with respect to storage consumption and performance overhead. Second, we propose the two optimizations: i) A periodic cleaning strategy that periodically cleans up outdated checkpoints to reduce the storage burden; ii) A data staging optimization that coordinates checkpoints between local and shared file systems for performance improvement. The experimental results with GPT-2 variants show that, overall the proposed optimizations significantly reduce the storage consumption to a constant while improves performance by average 2.1 for checkpointing in GPT-2 training.} }
@inproceedings{10.1145/3600046.3600048, title = {Image Watermarking for Machine Learning Datasets}, booktitle = {Proceedings of the Second ACM Data Economy Workshop}, pages = {7--13}, year = {2023}, isbn = {9798400708466}, doi = {10.1145/3600046.3600048}, url = {https://doi.org/10.1145/3600046.3600048}, author = {Maesen, Palle and undefinedsler, Devris and Laoutaris, Nikolaos and Erkin, Zekeriya}, keywords = {machine learning, data ownership, data economy, Watermarking, location = Seattle, WA, USA}, abstract = {Machine learning has received increasing attention for the last decade due to its significant success in classification problems in almost every application domain. For its success, the amount of available data for training plays a crucial role in the creation of a machine-learning model. However, the data-gathering process for machine learning algorithms is a tedious and time-consuming task. In many cases, the developers rely on publicly available datasets, which are not always of high quality. Recently, we are witnessing a data market paradigm where valuable datasets are sold. Thus, once the dataset is created or bought, protecting the dataset against illegal use or (re)sale and establishing intellectual property rights is necessary. In this paper, we investigate the question of deploying well-studied image watermarking techniques to be applied to classification algorithm datasets, without degrading the quality of the dataset. We investigate whether Singular Value Decomposition (SVD)-based techniques from image watermarking could be deployed on machine learning datasets or not. To this end, we chose the watermarking technique described in [8] and applied it to a machine-learning dataset. We provide experimental results on the robustness of the scheme. Our results show that the watermark embedding scheme provides decent imperceptibility and robustness against update, zero-out, and insertion attacks but, it is not successful against deletion attacks. We believe our work can inspire researchers who might want to consider applying well-studied image watermarking techniques to machine learning datasets.} }
@inproceedings{10.1145/3630106.3658923, title = {Diversified Ensembling: An Experiment in Crowdsourced Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {529--545}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658923}, url = {https://doi.org/10.1145/3630106.3658923}, author = {Globus-Harris, Ira and Harrison, Declan and Kearns, Michael and Perona, Pietro and Roth, Aaron}, keywords = {Crowdsourcing, Ensembling Methods, Fairness, location = Rio de Janeiro, Brazil}, abstract = {Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In [12], the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants’ efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams’ approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.} }
@inproceedings{10.1145/3718751.3718800, title = {Machine learning-based impact analysis of social factors in psychiatric patients}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {308--313}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718800}, url = {https://doi.org/10.1145/3718751.3718800}, author = {Ma, Huanying and Liang, Liang}, keywords = {influencing factors, machine learning, psychiatric patients, social functioning}, abstract = {Machine learning is an interdisciplinary subject involving multiple fields such as probability theory, statistics, approximation theory, and convex analysis, etc. Machine learning can effectively analyze the main factors affecting the recovery and enhancement of social functioning of psychiatric patients, and to analyze the main social factors affecting the level of social functioning of psychiatric patients by using machine learning theory.} }
@inproceedings{10.1145/3729706.3729821, title = {Machine Learning for Predicting Tensile Properties of Fiber-Reinforced Metal Matrix Composites}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {729--733}, year = {2025}, isbn = {9798400712715}, doi = {10.1145/3729706.3729821}, url = {https://doi.org/10.1145/3729706.3729821}, author = {Gan, Guorong and Liu, Qiqing and Leng, Guoyang and Wang, Zhengcui and Lai, Daohui}, keywords = {Fiber-Metal Matrix Composites, Machine Learning, Process Optimization, Tensile Properties}, abstract = {Fiber-reinforced metal matrix composites (FMMCs) hold broad application prospects in numerous fields due to their excellent comprehensive properties. The tensile properties of these materials, as a key performance indicator, play a decisive role in their engineering applications. Therefore, leveraging the data processing advantages and the ability to identify complex relationships of machine learning (ML) technology can significantly enhance the study of the tensile properties of fiber-metal matrix composites. This paper provides a comprehensive overview of machine learning algorithms and models, as well as the current status of ML in predicting the performance of fiber-reinforced composites. It discusses the application of ML technology in predicting the tensile properties of fiber-metal matrix composites, optimizing manufacturing processes, and elucidating the relationship between microstructure and performance. Additionally, it explores the challenges faced by ML in predicting material properties and offers insights into future development directions, aiming to provide a reference for advancing research and applications in this field.} }
@inproceedings{10.1145/3617184.3618056, title = {Constructing Dynamic Honeypot Using Machine Learning}, booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering}, pages = {116--120}, year = {2023}, isbn = {9798400708800}, doi = {10.1145/3617184.3618056}, url = {https://doi.org/10.1145/3617184.3618056}, author = {Zhang, Yingying and Shi, Yue}, keywords = {Active Defense, Dynamic Honeypot, Machine learning, Network security, location = Putrajaya, Malaysia}, abstract = {Honeypot is an active security defense technology that uses false information to lure attackers into attacking and record their behavior. Traditional honeypots are usually static, and inherent features and services can accelerate attackers' recognition of honeypots, causing them to lose value. We designs a dynamic honeypot based on machine learning, which can adapt to dynamic and constantly changing network environments while improving the authenticity of the honeypot. It automatically generates configuration files, simulates the characteristics and behavior of devices in the network. The method proposed is to achieve active monitoring and defense of network attacks by actively scanning Nmap and obtaining network device information through P0f, and combining feature clustering methods to classify devices and generate honeypot configuration files, active monitoring and defense of network attacks can be achieved. The results shows that this methods can effectively enhance the attack capture ability and camouflage ability of honeypots.} }
@inproceedings{10.1145/3748825.3748940, title = {Machine Learning-Assisted Public Opinion Segmentation and Diversified Policy Response Mechanism}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {742--747}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748940}, url = {https://doi.org/10.1145/3748825.3748940}, author = {Chen, Sinian}, keywords = {Graph Neural Network, Policy Response, Public Opinion Segmentation, Variational Autoencoder}, abstract = {In the age of information explosion, governments and related organizations are confronted with two issues: how to accurately grasp the scattered opinions in public and how to make an open and comprehensive policy response on time? In our work, we present an advanced machine learning framework, HT-GAPM (Hierarchical Topic-aware Variational Autoencoders and Graph-structured Adaptive Policy Matcher), for fine-grained opinion segmentation and diverse policy responses. Methodologically, HT-GAPM draws on the framework of state-of-the-art Transformer topic modeling and dynamic graph neural network, and can learn latent representations from multi-source public sentiment texts, which can capture semantic consistency and temporal evolution features, and the Graph-structured Adaptive Policy Matcher is designed to build a heterogeneous policy-opinion graph and accurately match segmented opinion groups with the most relevant policy templates using attention-weighted node representations. This model is the first to propose a feature enhanced dual optimization objective function that jointly improves the accuracy of opinion segmentation and the flexibility of policy response, while bias-sening and policy consistent are controlled via domain-prior regularization terms. Experiments show that the proposed model can get higher quality of opinion segmentation granularity and policy response diversity than state-of-the-art methods.} }
@inproceedings{10.1145/3543873.3589751, title = {Machine Learning for Streaming Media}, booktitle = {Companion Proceedings of the ACM Web Conference 2023}, pages = {759}, year = {2023}, isbn = {9781450394192}, doi = {10.1145/3543873.3589751}, url = {https://doi.org/10.1145/3543873.3589751}, author = {Lamkhede, Sudarshan and Chandar, Praveen and Radosavljevic, Vladan and Goyal, Amit and Luo, Lan}, abstract = {Streaming media has become a popular medium for consumers of all ages, with people spending several hours a day streaming videos, games, music, or podcasts across devices. Most global streaming services have introduced Machine Learning (ML) into their operations to personalize consumer experience, improve content, and further enhance the value proposition of streaming services. Despite the rapid growth, there is a need to bridge the gap between academic research and industry requirements and build connections between researchers and practitioners in the field. This workshop aims to provide a unique forum for practitioners and researchers interested in Machine Learning to get together, exchange ideas and get a pulse for the state of the art in research and burning issues in the industry.} }
@article{10.5555/3715602.3715621, title = {A Machine Learning Based Sentiment Analysis for Twitter Data}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {145--157}, year = {2024}, issn = {1937-4771}, author = {Arafat, Kazi Abdullah Al and Creer, Kode and Roni, Mahmudur Rahman and Parvez, Imtiaz}, abstract = {Sentiment analysis, also known as opinion mining, is a computational study of people's opinions, sentiments, evaluations, attitudes, and emotions expressed in textual data. This study explores the application of machine learning algorithms for sentiment analysis on a preprocessed dataset. The study employs Support Vector Machines (SVM), Maximum Entropy (Max Ent), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) for sentiment classification. The process includes feature extraction, model training, and evaluation using standard classification metrics. Out of the four algorithms, SVM, CNN, and RNN scored 97\% accuracy, while Max Ent achieved a slightly lower 95\% accuracy. More specifically, in sentiment analysis, SVM demonstrated overall better performances.} }
@article{10.1145/3710966, title = {Equality Engine: Fostering Critical Machine Learning Bias Literacy Through a Transformational Game}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3710966}, url = {https://doi.org/10.1145/3710966}, author = {Showkat, Dilruba and Wang, Lingqing and Chan, Laveda and To, Alexandra}, keywords = {AI/ML literacy, ML biases, ML ethics, game design, machine learning, transformational games}, abstract = {Machine Learning (ML) is now integrated from everyday technologies to sophisticated infrastructures, providing fast, efficient, and scalable decision-making services, with increasing evidence of ML perpetuating invisible harms and biases. The hidden and socio-technical nature of ML biases can make them difficult to detect and prevent without proper literacy. To investigate this, we developed a novel online multiplayer board game Equality Engine, where players learn about various ML biases and debiasing techniques. We conducted a mixed-method formative playtest case study to solicit feedback from post-secondary students (N = 12) with a range of ML experience. We found that students' self-reported ML bias-debias knowledge improved significantly after playing the game. The game was perceived as easy to use because of the social interaction and immersion the game enabled. Students would also use the game in the future because of the self-reported knowledge gained from the game. Although these positive results may be influenced by measurement bias, our study contributes to the design of an approachable game, which not only facilitates exposure, collaboration, and opportunities for critical reflection on ML biases but also provides recommendations for future game designs that can facilitate ML ethics discourse and literacy among a broader audience.} }
@inproceedings{10.1145/3721239.3734127, title = {Machine Learning Meets Lighting: Using Depth Estimation To Build The Light Rigs}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Talks}, year = {2025}, isbn = {9798400715419}, doi = {10.1145/3721239.3734127}, url = {https://doi.org/10.1145/3721239.3734127}, author = {Shlyaev, Sergey}, keywords = {Image based lighting, pipeline, rendering}, abstract = {This paper describes the techniques we use to build the complex light rigs in the Sony Pictures Imageworks lighting pipeline. Typically we receive the panoramic HDRI from the set and need to make a light rig from it. Building a light rig has several steps: extracting area lights from HDRI, placing them in a 3D scene, aligning lights with Lidar from set. We describe how this process can be sped up. For example, the positioning of extracted lights is automated using PatchFusion [Li et\&nbsp;al. 2023], an off-the-shelf machine learning model for high resolution monocular depth estimation. PatchFusion computes accurate metric depth directly from the HDRI. The depth map is used as a distance from light to camera to place the area lights in 3D scene. This removes the need for manual distance measurements or guesswork. Our approach significantly reduces manual labor. The time required to build the light rig goes from hours to about a minute.} }
@article{10.1145/3617897, title = {Machine Learning (In) Security: A Stream of Problems}, journal = {Digital Threats}, volume = {5}, year = {2024}, doi = {10.1145/3617897}, url = {https://doi.org/10.1145/3617897}, author = {Ceschin, Fabr\'cio and Botacin, Marcus and Bifet, Albert and Pfahringer, Bernhard and Oliveira, Luiz S. and Gomes, Heitor Murilo and Gr\'egio, Andr\'e}, keywords = {Machine learning, cybersecurity, data streams}, abstract = {Machine Learning (ML) has been widely applied to cybersecurity and is considered state-of-the-art for solving many of the open issues in that field. However, it is very difficult to evaluate how good the produced solutions are, since the challenges faced in security may not appear in other areas. One of these challenges is the concept drift, which increases the existing arms race between attackers and defenders: malicious actors can always create novel threats to overcome the defense solutions, which may not consider them in some approaches. Due to this, it is essential to know how to properly build and evaluate an ML-based security solution. In this article, we identify, detail, and discuss the main challenges in the correct application of ML techniques to cybersecurity data. We evaluate how concept drift, evolution, delayed labels, and adversarial ML impact the existing solutions. Moreover, we address how issues related to data collection affect the quality of the results presented in the security literature, showing that new strategies are needed to improve current solutions. Finally, we present how existing solutions may fail under certain circumstances and propose mitigations to them, presenting a novel checklist to help the development of future ML solutions for cybersecurity.} }
@inproceedings{10.1145/3681769.3698586, title = {Machine Learning Model Specification for Cataloging Spatio-Temporal Models (Demo Paper)}, booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data}, pages = {36--39}, year = {2024}, isbn = {9798400711480}, doi = {10.1145/3681769.3698586}, url = {https://doi.org/10.1145/3681769.3698586}, author = {Charette-Migneault, Francis and Avery, Ryan and Pondi, Brian and Omojola, Joses and Vaccari, Simone and Membari, Parham and Peressutti, Devis and Yu, Jia and Sundwall, Jed}, keywords = {Catalog, Machine Learning, STAC, Search, Spatio-Temporal Models, location = Atlanta, GA, USA}, abstract = {The Machine Learning Model (MLM) extension is a specification that extends the SpatioTemporal Asset Catalogs (STAC) framework to catalog machine learning models. This demo paper introduces the goals of the MLM, highlighting its role in improving searchability and reproducibility of geospatial models. The MLM is contextualized within the STAC ecosystem, demonstrating its compatibility and the advantages it brings to discovering relevant geospatial models and describing their inference requirements.A detailed overview of the MLM's structure and fields describes the tasks, hardware requirements, frameworks, and inputs/outputs associated with machine learning models. Three use cases are presented, showcasing the application of the MLM in describing models for land cover classification and image segmentation. These examples illustrate how the MLM facilitates easier search and better understanding of how to deploy models in inference pipelines.The discussion addresses future challenges in extending the MLM to account for the diversity in machine learning models, including foundational and fine-tuned models, multi-modal models, and the importance of describing the data pipeline and infrastructure models depend on. Finally, the paper demonstrates the potential of the MLM to be a unifying standard to enable benchmarking and comparing geospatial machine learning models.} }
@inproceedings{10.1145/3724154.3724167, title = {Construction of Machine Learning-based Grid Marketing Service Analysis Model}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {73--78}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724167}, url = {https://doi.org/10.1145/3724154.3724167}, author = {Li, Jiajin and Hao, Chengcheng and Ma, Yuankui and Wu, Xiaozhi and Lu, Wei and Zhong, Haiting and Xuan, Baolong and Wang, Qiyuan}, keywords = {analysis model, data middle platform, machine learning, marketing services}, abstract = {State Grid Energy Internet Marketing service System (Marketing 2.0) is a new generation of power marketing service system independently developed by State Grid Corporation. The system takes the customer as the center, takes the market as the guidance, takes the digitalization, the network, the intelligence as the guidance, builds the platform system of omni-channel service and the whole business application. It serves the marketing personnel of power supply companies in provinces, cities and counties (districts) across the country, effectively promoting the digital transformation and high-quality development of the marketing business of the State Grid Corporation. Nanjing Nari Digital service Co.,Ltd. completed the design and development of 15 analysis models in accordance with the overall planning requirements of marketing 2.0, and realized the deep integration of digital construction and business needs. This paper introduces the model of customer payment prediction analysis, charge strategy adjustment analysis and charge recovery risk analysis based on machine learning algorithm. Through the integration of artificial intelligence technology, the system has achieved technological innovation, improved model accuracy and business accuracy, and promoted the construction, application and iterative upgrading of the platform.} }
@inproceedings{10.1145/3644815.3644943, title = {Engineering Carbon Emission-aware Machine Learning Pipelines}, booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI}, pages = {118--128}, year = {2024}, isbn = {9798400705915}, doi = {10.1145/3644815.3644943}, url = {https://doi.org/10.1145/3644815.3644943}, author = {Husom, Erik Johannes and Sen, Sagar and Goknil, Arda}, abstract = {The proliferation of machine learning (ML) has brought unprecedented advancements in technology, but it has also raised concerns about its environmental impact, particularly concerning carbon emissions. To address the imperative of environmentally responsible ML, we present in this paper a novel ML pipeline, named CEMAI, designed to monitor and analyze carbon emissions across the entire lifecycle of ML model development, from data preparation to training and deployment. Our endeavor involves an exhaustive evaluation process underpinned by three industrial case studies. These case studies are structured around the application of ML models to predict tool wear, estimate remaining useful lifetimes, and detect anomalies in the Industrial Internet of Things (IIoT). Leveraging sensor data originating from CNC machining and broaching operations, our research shows empirically the efficacy of carbon emissions as a dependable metric guiding the configuration of an ML development process. The essence of our approach lies in striking a balance between superior performance and minimal carbon emissions. Our findings reveal the potential to optimize pipeline configurations for ML models in a manner that not only enhances performance but also drastically reduces carbon emissions, thereby underlining the significance of adopting ecologically responsible engineering practices.} }
@article{10.1145/3672451, title = {Keeper: Automated Testing and Fixing of Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {33}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3672451}, url = {https://doi.org/10.1145/3672451}, author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Hoffmann, Henry and Maire, Michael and Lu, Shan}, keywords = {Software testing, machine learning, machine learning API}, abstract = {The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior.Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78\% of end-users and 95\% of developers agree with Keeper’s detection and fixing results.} }
@inproceedings{10.1145/3716554.3716834, title = {From Volunteers to Voters: Machine Learning Insights into Citizen Engagement}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {358--363}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716834}, url = {https://doi.org/10.1145/3716554.3716834}, author = {Tsoni, Rozita and Tolika, Maria and Karapiperis, Dimitris and Verykios, Vassilios}, abstract = {This study examines the link between volunteering and voting behavior. The main goal is to identify the key factors responsible for the impact on volunteers’ decisions to vote and build a predictive model to assess the likelihood of individual volunteers participating in elections. Data collected from 650 volunteers at the Paris 2024 Olympic Games were analyzed using machine learning methods, with Gradient Boosted Trees achieving the highest accuracy. These findings provide insights for enhancing civic engagement and voter turnout. Their responses were used to train a model aimed at predicting whether the participants had voted in the most recent elections. Various Machine Learning (ML) algorithms based on Decision Trees (DT) and kNN were employed for classification and prediction tasks. A key feature of the proposed approach is the use of an ML pipeline developed with an open-source visual programming tool, streamlining the entire process. The methodology encompasses a pre-processing stage and a modeling stage involving five classifiers. Among the algorithms, the Gradient Boosted Trees method demonstrated the highest performance with an accuracy of 0.92.} }
@inproceedings{10.1145/3605098.3635919, title = {Detection of Slowloris Attacks using Machine Learning Algorithms}, booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing}, pages = {1321--1330}, year = {2024}, isbn = {9798400702433}, doi = {10.1145/3605098.3635919}, url = {https://doi.org/10.1145/3605098.3635919}, author = {Rios, Vinicius and Inacio, Pedro and Magoni, Damien and Freire, Mario}, keywords = {denial of service (DoS) attack, fuzzy logic, low-rate DoS attack, machine learning, Slowloris, location = Avila, Spain}, abstract = {The Slowloris attack, a variant of the slow Denial-of-Service (DoS) attack, is a stealthy threat that aims to take down web services provided by companies and institutions. It is able to pass through the traditional defense systems, due to the low amount and high latency of its attack traffic, often mimicking legitimate user traffic. Therefore, it is necessary to investigate techniques that can detect and mitigate this type of attack and simultaneously prevent legitimate user traffic from being blocked. In this work, we investigate nine machine learning algorithms for detecting Slowloris attacks, as well as a new combination based on Fuzzy Logic (FL), Random Forest (RF), and Euclidean Distance (ED) that we call FRE. We first generate Slowloris attack traffic traces in various environments. We then assess these algorithms under two scenarios: hyperparameters with default values and optimized hyperparameters. We show that most of these machine learning algorithms perform very well, with the random forest leading to the best classification results with test accuracy values reaching 99.52\%. We also show that our FRE method outperforms all these algorithms, with test accuracy values reaching 99.8\%.} }
@inproceedings{10.1145/3677052.3698650, title = {Machine Learning-based Relative Valuation of Municipal Bonds}, booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance}, pages = {634--642}, year = {2024}, isbn = {9798400710810}, doi = {10.1145/3677052.3698650}, url = {https://doi.org/10.1145/3677052.3698650}, author = {Saha, Preetha and Lyu, Jasmine and Desai, Dhruv and Chauhan, Rishab and Jeyapaulraj, Jerinsh and Chu, Peter and Sommer, Philip and Mehta, Dhagash}, keywords = {CatBoost, Municipal Bonds, Relative Valuation, Similarity Learning, location = Brooklyn, NY, USA}, abstract = {The trading ecosystem of the Municipal (muni) bond is complex and unique. With nearly 2\% of securities from over a million securities outstanding trading daily, determining the value or relative value of a bond among its peers is challenging. Traditionally, relative value calculation has been done using rule-based or heuristics-driven approaches, which may introduce human biases and often fail to account for complex relationships between the bond characteristics. We propose a data-driven model to develop a supervised similarity framework for the muni bond market based on CatBoost algorithm. This algorithm learns from a large-scale dataset to identify bonds that are similar to each other based on their risk profiles. This allows us to evaluate the price of a muni bond relative to a cohort of bonds with a similar risk profile. We propose and deploy a back-testing methodology to compare various benchmarks and the proposed methods and show that the similarity-based method outperforms both rule-based and heuristic-based methods.} }
@inproceedings{10.1145/3600211.3604753, title = {Advancing Health Equity with Machine Learning}, booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {955--956}, year = {2023}, isbn = {9798400702310}, doi = {10.1145/3600211.3604753}, url = {https://doi.org/10.1145/3600211.3604753}, author = {Mhasawade, Vishwali}, keywords = {causal inference, fairness, health disparities, health equity, location = Montr\'eal, QC, Canada}, abstract = {Social privilege in terms of power, wealth, and prestige is the driver of avoidable health inequities. But today, machine learning systems in healthcare are largely focused on data and systems within hospitals and clinics, ignoring the factors that lead to health disparities across communities. The primary goal of my research is to understand the drivers of population health inequity and design fair and equitable machine learning systems for mitigating health disparities. In order to do this, I mainly focus on causal inference and machine learning methods using data from multiple environments, such as geographical locations and hospitals, to identify and address inequities in health and healthcare.} }
@inproceedings{10.1145/3690624.3709229, title = {On the Hyperparameter Loss Landscapes of Machine Learning Models: An Exploratory Study}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1}, pages = {555--564}, year = {2025}, isbn = {9798400712456}, doi = {10.1145/3690624.3709229}, url = {https://doi.org/10.1145/3690624.3709229}, author = {Huang, Mingyu and Li, Ke}, keywords = {exploratory data analysis, fitness landscape analysis, hyperparameter optimization, location = Toronto ON, Canada}, abstract = {Previous efforts on hyperparameter optimization (HPO) of machine learning (ML) models predominately focus on algorithmic advances, yet little is known about the topography of the underlying hyperparameter (HP) loss landscape, which plays a fundamental role in governing the search process of HPO. While several works have conducted fitness landscape analysis (FLA) on various ML systems, they are limited to properties of isolated landscape without interrogating the potential structural similarities among landscapes induced on different scenarios. The exploration of such similarities can provide a novel perspective for understanding the mechanism behind modern HPO methods, but has been missing. In this paper, we mapped 1,500 HP loss landscapes of 6 representative ML models on 63 datasets across different fidelity levels, with 11M+ configurations. By conducting exploratory analysis on these landscapes with fine-grained visualizations and dedicated FLA metrics, we observed a similar landscape topography across a wide range of models, datasets, and fidelities, and shed light on the mechanism behind the success of several popular methods in HPO. The artifacts associated with this paper is available at https://github.com/COLA-Laboratory/GraphFLA.} }
@article{10.1145/3633455, title = {Test-Driven Ethics for Machine Learning}, journal = {Commun. ACM}, volume = {67}, pages = {45--47}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3633455}, url = {https://doi.org/10.1145/3633455}, author = {Berente, Nicholas and Kormylo, Cameron and Rosenkranz, Christoph}, abstract = {Encouraging organizations to adapt a test-driven ethical development approach.} }
@inproceedings{10.1145/3638530.3664046, title = {Machine Learning for Evolutionary Computation - the Vehicle Routing Problems Competition}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {13--14}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3664046}, url = {https://doi.org/10.1145/3638530.3664046}, author = {Meng, Weiyao and Qu, Rong and Pillay, Nelishia}, keywords = {machine learning, evolutionary computation, meta-heuristics, vehicle routing, location = Melbourne, VIC, Australia}, abstract = {The Competition of Machine Learning for Evolutionary Computation for Solving Vehicle Routing Problems (ML4VRP) seeks to bring together machine learning and evolutionary computation communities to propose innovative techniques for vehicle routing problems (VRPs), aiming to advance machine learning-assisted evolutionary computation that works well across different instances of the VRPs. This paper overviews the key information of the competition.} }
@inproceedings{10.1145/3686081.3686118, title = {Application of CPU in AI and Machine Learning}, booktitle = {Proceedings of the International Conference on Decision Science \&amp; Management}, pages = {221--224}, year = {2024}, isbn = {9798400718151}, doi = {10.1145/3686081.3686118}, url = {https://doi.org/10.1145/3686081.3686118}, author = {Yu, Renchao}, keywords = {AI, CPU, Heterogeneous, Machine learning, Neural network, System architecture}, abstract = {In recent years, the burgeoning fields of Artificial Intelligence (AI) and Machine Learning (ML) have increasingly permeated daily life, catalyzing significant advancements in technology. This rapid evolution has, in turn, necessitated the continual adaptation and enhancement of Central Processing Units (CPUs), which remain at the forefront of computational hardware. Despite these advancements, there has been a noticeable dearth in scholarly literature focusing on training neural network models exclusively on CPUs. This gap presents a substantial impediment to the exploration and advancement of CPU applications within AI and ML domains. Upon extensive literature review and analysis, it becomes evident that the CPU's role in AI and ML has not only been pivotal but has also retained distinct advantages despite the relative deceleration in its development trajectory. Current trends suggest that modern CPUs are increasingly moving towards an integrated architecture, incorporating specialized modules and fostering synergies with heterogeneous computing units. This paradigm shift underscores a critical question: How can we stimulate CPU development further and leverage its inherent strengths effectively? Addressing this query is essential for pushing the boundaries of CPU technology in the ever-evolving landscape of AI and ML.} }
@inproceedings{10.1145/3647444.3647843, title = {Qualitative Analysis on Machine Learning Through Biblioshiny}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647843}, url = {https://doi.org/10.1145/3647444.3647843}, author = {M, Vidya and Anifa, Mansurali}, abstract = {Machine learning augments business firms by enhancing their business operations and by reduction in costs. It assists business houses to visualize the historical patterns and to envisage future decisions. Machine learning applies algorithms and models to assess the data patterns. Sentiment analysis is a form of Natural Language Processing for regulating the feedback from different ends. The study analysed 620 global publications pertaining to Machine learning and sentiment analysis indexed in Scopus database. The article demonstrated a Three field plot portrays the relationship between authors, countries, and keywords; authors' influence on sources; word counts and word growth; and a collaborative network of papers. The widely held articles of Machine Learning and Sentiment Analysis is published as journal articles, and the number of publications is continuously increasing. In a ranking most productive nations, India came out with the highest citations (1054), followed by USA (504), and Chine (354). The productive authors in the field of Machine Learning and Sentiment Analysis were Wang X has contributed 5 articles followed by Li Z , Wang Y, Yaqub U, with 4 articles. The Journal Information Processing and Management had 24 Publications around the world and also has the total citations of 763 followed by Artificial Intelligence Review (475 Citations). University of Florida was the contributing majority of 21 articles followed by The Hongkong Polytechnic University with 16 articles.} }
@inbook{10.1145/3745238.3745484, title = {Stock Decision-Making Based on Machine Learning Models and Z-Scores}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1570--1576}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745484}, author = {Wei, Jiawei}, abstract = {This study examines the performance of various machine learning models in stock decision-making based on the stock Z-scores. We employed four models—OLS linear regression, decision tree regression, support vector machine regression (SVM), and multilayer perceptron neural network (MLP). By leveraging Z-scores calculated by different models, stock trading strategies were constructed, and the models’ risk-adjusted returns and cumulative returns were assessed. The experimental results indicate that the decision tree model outperformed the others overall, especially in the unweighted case where it showed lower max drawdown and relatively lower risk. After weighting, the performances of the SVM and MLP models improved, with the decision tree regression model achieving the highest cumulative return.} }
@inproceedings{10.1145/3671151.3671253, title = {Using Machine Learning Methods to Select Stock Factors}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {575--580}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671253}, url = {https://doi.org/10.1145/3671151.3671253}, author = {Wang, Xiaoxi}, abstract = {In the intersection research of quantitative finance and big data, whether stock anomalies still exist under various statistical methods especially machine learning methods, and which characteristics could provide independent information have been widely discussed. To address these questions, this paper uses Fama-Macbeth and machine learning techniques to select stock characteristics and then examine them. Firstly, I construct 40 characteristics in finance and accounting from 1980 to 2016. Then perform Fama-Macbeth regressions to identify those with t\&gt;3. Next run Lasso regressions, and identify a few significant characteristics: B/M, Invest, Size, turnover, spread and illiquid. When loosen the penalty, profit, SUE, ROA, cash, mom1m, and IPO are selected. Other supervised learning methods are also implemented. Linear model, penalized models and boosting perform well out of sample. By using big data techniques and machine learning methods, this paper provides new evidence on the traditional finance research questions.} }
@inproceedings{10.1145/3660853.3660933, title = {Machine Learning Approaches for Botnet Detection in Network Traffic}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {310--315}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660933}, url = {https://doi.org/10.1145/3660853.3660933}, author = {Salih, Yousif Tareq and Fenjan, Ali and Ahmed, Saadaldeen Rashid and Ali, Hussein and Abdulwahab, Emad.N and Algruri, Sameer and Kurdi, Neesrin Ali and Al-Sarem, Mohammed and Tawfeq, Jamal Fadhil}, keywords = {Botnet Detection, Machine Learning, Network Security, Network Traffic, location = undefinedstanbul, Turkiye}, abstract = {Botnets pose a significant challenge to network security, continually evolving and threatening the integrity of digital infrastructure. Traditional botnet detection methodologies have limitations, prompting the need for innovative approaches. In this paper, we propose a machine learning-based method to effectively detect botnets within network traffic, with a particular focus on IoT devices. Our approach leverages support vector machine (SVM) and regularized logistic regression (rLR) algorithms. Experimental results demonstrate the efficacy of our model in detecting botnet attacks. This research serves as a precursor to countering the daily onslaught of botnet attacks and emphasizes the importance of integrating machine learning techniques into network security.} }
@inproceedings{10.1145/3721888.3722092, title = {Edge Acceleration of LiDAR Frame Transmission with In-network Machine Learning}, booktitle = {Proceedings of the 8th International Workshop on Edge Systems, Analytics and Networking}, pages = {13--18}, year = {2025}, isbn = {9798400715594}, doi = {10.1145/3721888.3722092}, url = {https://doi.org/10.1145/3721888.3722092}, author = {Qian, Peng and Zheng, Changgang and Zilberman, Noa}, keywords = {in-network machine learning, P4, edge computing, vehicle perception, location = World Trade Center, Rotterdam, Netherlands}, abstract = {In real-time vehicle perception scenarios, ensuring timely and stable transmission of LiDAR data between vehicles and the network edge is crucial for accurate object detection. However, the inherent variability of wireless links, coupled with the added impact of vehicle mobility, leads to inevitable packet loss and latency jitter, compromising both the timeliness and accuracy of vehicle perception. To address this challenge, we introduce a packet duplication mechanism on dual wireless links, improving LiDAR frame transmission performance. The solution is driven by an integrated In-Network Machine Learning module at a programmable edge device that dynamically detects performance degradation and controls packet duplication. Through practical implementation and extensive evaluation, it is demonstrated that the proposed packet duplication function can effectively address uncertainties in LiDAR frame transmission, while achieving 50\% reduction in transmission times.} }
@inproceedings{10.1145/3747227.3747276, title = {Research on the construction of the prediction model of economic management students' performance under machine learning technology}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {311--316}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747276}, url = {https://doi.org/10.1145/3747227.3747276}, author = {Wang, Qiankang and Yao, Jun}, keywords = {data mining techniques, economic management, student performance prediction}, abstract = {In order to explore the predictive model of economic management students' performance based on machine learning techniques, three algorithms, namely, Random Forest, Support Vector Regression (SVR) and Long Short-Term Memory Network (LSTM), are used for modeling. The dataset was obtained from 2,000 economics and management students from eight universities across China and contained 28-dimensional features, such as class attendance rate (92.5\% ± 5.8\%), assignment submission rate (94.2\% ± 4.1\%) and MOOC access frequency (2.7 ± 1.3 times per day). Through data preprocessing, feature selection and dimensionality reduction, the LSTM model was finally selected as the best prediction model, with a mean square error (MSE) of 2.73, a root mean square error (RMSE) of 1.65, and a coefficient of determination, R², of 0.89. Compared with the other models, the LSTM had a higher prediction accuracy in the high performance group (MSE = 2.51), and showed better stability and adaptability. The results of the study show that machine learning techniques can effectively capture the complex relationship between student performance and learning behaviors, and provide an accurate method for predicting student performance in the field of education.} }
@inproceedings{10.1145/3658644.3690304, title = {Analyzing Inference Privacy Risks Through Gradients In Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {3466--3480}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690304}, url = {https://doi.org/10.1145/3658644.3690304}, author = {Li, Zhuohang and Lowy, Andrew and Liu, Jing and Koike-Akino, Toshiaki and Parsons, Kieran and Malin, Bradley and Wang, Ye}, keywords = {inference privacy, machine learning, location = Salt Lake City, UT, USA}, abstract = {In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.} }
@inproceedings{10.1145/3718391.3718440, title = {Comparison of machine learning models for the identification of disc herniation}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {279--284}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3718440}, url = {https://doi.org/10.1145/3718391.3718440}, author = {Huapalla Garcia, Juan and Baldeon Canchaya, Walter and Ovalle, Christian}, keywords = {Classification Models, Disc Herniation, Machine Learning, Medical Diagnosis}, abstract = {The accurate identification of disc herniations is crucial for the proper diagnosis and treatment of low back pain, a health issue affecting millions of people worldwide. Therefore, this study proposes to develop an innovative approach by comparing machine learning models applied to data obtained from a smart belt equipped with advanced sensors. The adopted methodology encompasses data collection, preprocessing, and analysis, evaluating several models, including Logistic Regression, Decision Trees, Naive Bayes, Random Forest, and k-NN. The results reveal that Logistic Regression is the most effective model, demonstrating the best performance metrics in accuracy, AUC, recall, and MCC, with an AUC of 0.763. This model surpasses others in identifying disc herniations and offers balanced and consistent performance. Thus, the utility of machine learning models in clinical practice is validated, proposing a promising tool to improve the diagnosis and treatment of disc herniations. The findings establish a solid foundation for future research, highlighting the importance of exploring this technological approach to contribute to advancements in health and the management of low back pain} }
@inproceedings{10.1145/3745034.3745122, title = {Predicting Low Bone Density Based on Interpretable Machine Learning Models}, booktitle = {Proceedings of the 4th International Conference on Biomedical and Intelligent Systems}, pages = {576--581}, year = {2025}, isbn = {9798400714399}, doi = {10.1145/3745034.3745122}, url = {https://doi.org/10.1145/3745034.3745122}, author = {Zhang, Shuaiqiong and Lu, Jingbo}, keywords = {Low bone density, SHAP, machine learning, osteoporosis}, abstract = {Osteoporosis has an insidious onset and low bone density is an early stage of osteoporosis, in order to recognize and intervene in osteoporosis at an early stage, this study aimed to develop a predictive model for low bone density. This study used National Health and Nutrition Examination Survey (NHANES) data from 2013-2014 and 2017-2018 with Decision Tree (DT), Random Forest (RF), Gradient Boosting Decision Tree (GBDT) and XGBoost algorithms to construct the prediction model, and SHAP algorithm for visual interpretation. The results showed that the best prediction model, XGBoost, had an AUC value of 0.777, which ultimately determined that the characteristics of gender, age, obesity or not, race, marital status, number of co-morbidities, and dietary pattern had a good prediction effect. The prediction model of low BMD constructed in this study can help healthcare professionals to recognize low BMD as early as possible and develop targeted measures to prevent osteoporosis.} }
@inproceedings{10.1145/3757110.3757210, title = {Research on the Prediction Model of Occupational burnout Among Primary Healthcare Workers Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {598--603}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757210}, url = {https://doi.org/10.1145/3757110.3757210}, author = {Chen, Shuang and Li, Zeyi and Sun, Siyu and Hao, Zixu}, keywords = {Decision Tree, Logistic Regression, Prediction Model, Primary Healthcare Workers, Support Vector Machine}, abstract = {Objective: To conduct a questionnaire survey among primary-level medical personnel in Jinan City, Shandong Province, China, and to construct and evaluate machine learning models in predicting job burnout among this group. Methods: A stratified cluster sampling method was adopted in this study to select 1,710 primary-level medical personnel in Jinan City, Shandong Province, as study participants. A general information questionnaire, the Minnesota Satisfaction Questionnaire - Short Form (MSQ-SF), and the Maslach Burnout Inventory (MBI) were utilized to investigate demographic data, job satisfaction, and job burnout. Three machine learning algorithms—Support Vector Machine, Logistic Regression, and Decision Tree—were employed to construct prediction models. Model performance was evaluated, and predictive feature importance was analyzed. Results: The incidence of job burnout among primary-level medical personnel in Jinan City was relatively high. The Support Vector Machine model demonstrated superior performance, with job satisfaction identified as the most significant predictor. Conclusion: It is recommended to fully harness the practical value of the job burnout prediction model for primary-level medical personnel. Early identification of burnout should be prioritized, and targeted measures should be implemented to alleviate burnout and reduce its negative impacts.} }
@article{10.5555/3722479.3722527, title = {Studying Financial Data with Macroeconomic Factors Using Machine Learning}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {151--163}, year = {2024}, issn = {1937-4771}, author = {Anem, Sai Sravya and Amiruzzaman, Md and Bhuiyan, Ashik Ahmed}, abstract = {This paper focuses on the prediction of stock indices through machine learning, focusing on macroeconomic factors and market sentiment generation. It centers on major US stock index funds, notably the S\&amp;P 500, and their correlation with key economic indicators like GDP, unemployment, CPI, money supply, and retail sales. Utilizing economic data from diverse sources such as the Federal Reserve, NASDAQ, and news websites, the study cleans and transforms datasets to estimate quarterly fund returns. Employing tree-based algorithms, particularly XGBoost, enables accurate predictions. Moreover, the paper evaluates index forecast performance across various market cycles and geopolitical events. It also uses traditional NLP methods and large language models to explore market sentiment generation, offering comprehensive insights. In essence, this paper sheds light on the predictive power of macroeconomic factors on stock indices and the nuances of market sentiment analysis, leveraging both conventional and advanced techniques.} }
@inproceedings{10.1145/3700906.3700996, title = {Collaborative scheduling algorithm for full quantity materials based on process and machine learning}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {561--565}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700996}, url = {https://doi.org/10.1145/3700906.3700996}, author = {Gu, Sanlin}, keywords = {Artificial neural network, Full quantity of materials, Process design, Scheduling algorithm}, abstract = {Participants in the supply chain may have different information, leading to incomplete or inaccurate information when making decisions. To this end, a process and machine learning based collaborative scheduling algorithm for all materials is proposed. Design a health monitoring process for material supply chain based on R-tree dynamic indexing algorithm. Based on this, artificial neural networks in machine learning are applied to mine the data of the entire material supply chain. Through data mining, various data in the supply chain can be integrated and analyzed to improve information transparency and accuracy, and reduce information asymmetry. Adopting a dual layer scheduling model to achieve dual layer collaborative scheduling of materials. The experimental results show that the research method effectively improves the accuracy of data mining in the entire material supply chain, and the utilization rate of materials under this method is always higher than 95\%.} }
@inproceedings{10.1145/3647750.3647781, title = {Retailers' Order Decision with Setup Cost using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing}, pages = {37--44}, year = {2024}, isbn = {9798400716546}, doi = {10.1145/3647750.3647781}, url = {https://doi.org/10.1145/3647750.3647781}, author = {Jintanasonti, Pissacha and Dumrongsiri, Aussadavut and Tantiwattanakul, Phattarasaya}, keywords = {Artificial Neural Network, Machine Learning, Mathematical Model, Supply Chain, Wholesale Price Strategy, location = Singapore, Singapore}, abstract = {The objective of this study was to gain valuable insights into retailer behavior and develop a predictive model to inform their purchasing decisions. This process involved a comprehensive analysis of the various factors that influence retailers when they make choices regarding product or service purchases. To gather the data for the study, a simple random sampling method was employed to extract retailer purchase data from a mathematical model using Excel Solver. While Excel Solver can determine optimal solutions quickly but, with hundreds of retailers, reoptimizing the model every time a price is changed it is not practical and the manufacturer must contact trial and error many times to optimize has price. To address this challenge, Artificial Neural Network techniques were utilized to analyze the sample data. The resulting equations were subsequently integrated into manufacturer model to assist manufacturers in forecasting retailer decisions. With an understanding of the expected patterns of retailer behavior, manufacturers can strategically plan their purchase orders to align with different promotion and marketing strategies. The study demonstrated that the model achieved an average minimum cost increase of 5.02\% when tested with a new dataset consisting of 5,000 retailers. Based on these findings, it is recommended that manufacturers adjust their order policies by placing orders at the beginning of period 2 and making the most of the earliest discount period. Furthermore, manufacturers should consider a range of factors, including total holding cost, reorder cost, and expected demand, when formulating their order policies. This comprehensive approach will help manufacturers optimize their purchasing decisions and enhance their overall operational efficiency.} }
@inproceedings{10.1145/3745238.3745275, title = {A Comparative Machine Learning Framework for Stock Market Forecasting and Dynamic Portfolio Optimization}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {216--221}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745275}, url = {https://doi.org/10.1145/3745238.3745275}, author = {Chen, Zhuoyi}, keywords = {Gradient Boosting, LSTM, Machine Learning, Markowitz Model, Portfolio Optimization, Sliding Window, Turnover Limit}, abstract = {Predicting stock market trends has always been an important area of research, especially in the application of machine learning algorithms. This study aims to predict stock returns in the steel, electronics, food and beverage, and real estate industries using three different machine learning models: linear regression, random forest, and gradient boosting. The research utilizes historical data from 2005 to 2024, including key indicators such as MACD, RSI, and price-to-earnings ratio (PE/TTM), to forecast stock returns. Additionally, the study implements a portfolio optimization model based on the Markowitz model, incorporating different turnover limit to simulate a real trading environment and optimize asset allocation. The results indicate that as turnover limit increase, portfolios with dynamic rebalancing perform overall better across industries compared to others, reflecting the critical role of total fee rate management and turnover limits in maximizing portfolio returns. This study emphasizes the value of combining machine learning models with portfolio optimization strategies to enhance prediction accuracy and asset allocation efficiency.} }
@article{10.1145/3698831, title = {CtxPipe: Context-aware Data Preparation Pipeline Construction for Machine Learning}, journal = {Proc. ACM Manag. Data}, volume = {2}, year = {2024}, doi = {10.1145/3698831}, url = {https://doi.org/10.1145/3698831}, author = {Gao, Haotian and Cai, Shaofeng and Dinh, Tien Tuan Anh and Huang, Zhiyong and Ooi, Beng Chin}, keywords = {data analytics pipeline, data preparation}, abstract = {Machine learning models are only as good as their training data. Simple models trained on well-chosen features extracted from the raw data often outperform complex models trained directly on the raw data. Data preparation pipelines, which clean and derive features from the data, are therefore important for machine learning applications. However, constructing such pipelines is a resource-intensive process that involves deep human expertise.Our goal is to design an efficient framework for automatically finding high-quality data preparation pipelines. The main challenge is how to explore a large search space of pipeline components with the objective of computing features that maximize the performance of the downstream models. Existing solutions are limited in terms of feature quality, which results in low accuracies of the downstream models, while incurring significant runtime overhead. We present CtxPipe, a novel framework that addresses the limitations of previous works by leveraging contextual information to improve the pipeline construction process. Specifically, it uses pre-trained embedding models to capture the data semantics, which are then used to guide the selection of pipeline components. We implement CtxPipe with deep reinforcement learning and evaluate it against state-of-the-art automated pipeline construction solutions. Our comprehensive experiments demonstrate that CtxPipe outperforms all of the baselines in both model performance and runtime cost.} }
@inproceedings{10.1145/3643659.3643927, title = {Automated Boundary Identification for Machine Learning Classifiers}, booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing}, pages = {1--8}, year = {2024}, isbn = {9798400705625}, doi = {10.1145/3643659.3643927}, url = {https://doi.org/10.1145/3643659.3643927}, author = {Dobslaw, Felix and Feldt, Robert}, abstract = {AI and Machine Learning (ML) models are increasingly used as (critical) components in software systems, even safety-critical ones. This puts new demands on the degree to which we need to test them and requires new and expanded testing methods. Recent boundary-value identification methods have been developed and shown to automatically find boundary candidates for traditional, non-ML software: pairs of nearby inputs that result in (highly) differing outputs. These can be shown to developers and testers, who can judge if the boundary is where it is supposed to be.Here, we explore how this method can identify decision boundaries of ML classification models. The resulting ML Boundary Spanning Algorithm (ML-BSA) is a search-based method extending previous work in two main ways. We empirically evaluate ML-BSA on seven ML datasets and show that it better spans and thus better identifies the entire classification boundary(ies). The diversity objective helps spread out the boundary pairs more broadly and evenly. This, we argue, can help testers and developers better judge where a classification boundary actually is, compare to expectations, and then focus further testing, validation, and even further training and model refinement on parts of the boundary where behaviour is not ideal.} }
@article{10.1145/3764944.3764985, title = {MLAM: A Machine Learning-Aided Architectural BottleneckAnalysis Model for x86 Architectures}, journal = {SIGMETRICS Perform. Eval. Rev.}, volume = {53}, pages = {145--152}, year = {2025}, issn = {0163-5999}, doi = {10.1145/3764944.3764985}, url = {https://doi.org/10.1145/3764944.3764985}, author = {Ryoo, Jihyun and Gudukbay Akbulut, Gulsum and Jiang, Huaipan and Tang, Xulong and Akbulut, Suat and Sampson, John and Narayanan, Vijaykrishnan and Taylan Kandemir, Mahmut}, abstract = {The architectural analysis tools that output bottleneck information do not allow knowledge transfer to other applications or architectures. So, we propose a novel tool that can predict an application's bottlenecks for unavailable architectures. We (i) identify the bottleneck characteristics of 44 applications and use this as the dataset for our ML/DL model; (ii) identify the correlations between metrics and bottlenecks to create our tool's initial feature list; (iii) propose an architectural bottleneck analysis model -MLAM - that employs random forest regression (RFR) and multi-layer perceptron (MLP) regression; (iv) present results that indicate MLAM tool can achieve 0.70 (RFR) and 0.72 (MLP) R2 inference accuracy in predicting bottlenecks; (v) present five versions of MLAM, four of which are trained with single architecture data, and one of which is trained with multiple architecture data, to predict bottlenecks for new architectures.} }
@inproceedings{10.1145/3717664.3717668, title = {Exploring Explainable Machine Learning Models for Corporate Investment Decision Prediction}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {18--22}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717668}, url = {https://doi.org/10.1145/3717664.3717668}, author = {Chen, Yiwen}, keywords = {Decision Tree, Investment Forecasting, Machine Learning, Random Forest}, abstract = {In the current economic environment, the investment decisions of firms are of crucial importance to ensure continuous and stable growth. The present study explores the potential of explanatory problem machine learning models in predicting a firm's debt-to-asset ratio (D/A ratio) with the aim of providing better assistance to firms' investment decisions. The study utilizes a substantial dataset, comprising 57,522 corporate data points obtained from the CSMAR database between 2000 and 2022, encompassing over 80 diverse industries. The study employs a multifaceted approach to data preprocessing, incorporating the K-means clustering method, a random forest model to identify the most pertinent feature variables associated with transmission ratios, and a multistage decision tree with pruning for model optimization. The model's accuracy, as gauged by five cross-validations, approaches 99.95\%, suggesting its high precision. The study's findings indicate that the debt-to-equity ratio and enterprise size are pivotal factors influencing enterprise investment decisions. An in-depth examination of these factors offers valuable insights for the formulation of future corporate investment strategies and risk management. Furthermore, the methodology and findings of this study establish a framework for developing students' ability to utilize artificial intelligence to address future challenges in the field of educational technology. The dataset and code can be accessed at https://github.com/yuzengyi/CIDP.} }
@proceedings{10.1145/3630048, title = {DistributedML '23: Proceedings of the 4th International Workshop on Distributed Machine Learning}, year = {2023}, isbn = {9798400704475}, abstract = {Following up the prior three successful versions of DistributedML, it is our great honour and pleasure to welcome you again, this time physically in the 4rd edition of the Distributed Machine Learning Workshop (DistributedML '23). The workshop is co-located with the 19th International Conference on emerging Networking EXperiments and Technologies (CoNEXT '23) and held in Paris, France, on the 8th of December 2023.Distributed ML is a rapidly evolving, interdisciplinary field bringing together techniques from Distributed Systems, Networks and Machine Learning. With Deep Learning at the forefront, we are seeing an explosion of AI-driven technologies, from immersive VR experiences and smart digital assistants to advanced robotics and autonomous vehicles. These applications not only challenge the limits of local device capabilities but also many times necessitate a shift towards distributed models of computation to enhance performance and efficiency, while respecting privacy and sustainability. At the cornerstone of innovation, foundational models further push the boundaries of today's computational infrastructure. Therefore, scaling up to support the new training workloads and efficiently deploying Large Language or Vision Models become key research areas.} }
@inproceedings{10.1145/3708036.3708233, title = {Predictive Analysis of Vehicle Insurance Demand Using Machine Learning Techniques}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {1193--1197}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708233}, url = {https://doi.org/10.1145/3708036.3708233}, author = {Sun, Mingwei}, keywords = {Data Analyze, Health Insurance, Machine learning, Purchase Perception, Vehicle Insurance}, abstract = {Health insurance and vehicle insurance play a crucial role in reducing economic burdens, alleviating psychological stress, and maintaining social stability. Given the similarities in buyer characteristics for both types of insurance, predicting whether customers who purchase health insurance are interested in vehicle insurance is of significant importance for companies offering both products. By analyzing data from the Kaggle website, this paper employs machine learning techniques, including XGBoost, AdaBoost, and Multi-Layer Perceptron (MLP), to predict customer interest in purchasing vehicle insurance and compares the performance of different models. The study results indicate that AdaBoost performs best in predicting vehicle insurance demand, followed by XGBoost, while MLP performs relatively weaker in this task. These findings provide important insights for insurance companies to optimize marketing strategies, adjust pricing models, and better manage risks. However, future research should focus on integrating more advanced technologies and improving data quality to further enhance the performance of predictive models.} }
@inproceedings{10.1109/ICSE55347.2025.00107, title = {Testing and Understanding Deviation Behaviors in FHE-Hardened Machine Learning Models}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {2251--2263}, year = {2025}, isbn = {9798331505691}, doi = {10.1109/ICSE55347.2025.00107}, url = {https://doi.org/10.1109/ICSE55347.2025.00107}, author = {Peng, Yiteng and Wu, Daoyuan and Liu, Zhibo and Xiao, Dongwei and Ji, Zhenlan and Rahmel, Juergen and Wang, Shuai}, abstract = {Fully homomorphic encryption (FHE) is a promising cryptographic primitive that enables secure computation over encrypted data. A primary use of FHE is to support privacy-preserving machine learning (ML) on public cloud infrastructures. Despite the rapid development of FHE-based ML (or HE-ML), the community lacks a systematic understanding of their robustness.In this paper, we aim to systematically test and understand the deviation behaviors of HE-ML models, where the same input causes deviant outputs between FHE-hardened models and their plaintext versions, leading to completely incorrect model predictions. To effectively uncover deviation-triggering inputs under the constraints of expensive FHE computations, we design a novel differential testing tool called HEDiff, which leverages the margin metric on the plaintext model as guidance to drive targeted testing on FHE models. For the identified deviation inputs, we further analyze them to determine whether they exhibit general noise patterns that are transferable. We evaluate HEDiff using three popular HE-ML frameworks, covering 12 different combinations of models and datasets. HEDiff successfully detected hundreds of deviation inputs across almost every tested FHE framework and model. We also quantitatively show that the identified deviation inputs are (visually) meaningful in comparison to regular inputs. Further schematic analysis reveals the root cause of these deviant inputs and allows us to generalize their noise patterns for more directed testing. Our work sheds light on enabling robust HE-ML for real-world usage.} }
@inproceedings{10.1145/3689031.3717496, title = {Heimdall: Optimizing Storage I/O Admission with Extensive Machine Learning Pipeline}, booktitle = {Proceedings of the Twentieth European Conference on Computer Systems}, pages = {1109--1125}, year = {2025}, isbn = {9798400711961}, doi = {10.1145/3689031.3717496}, url = {https://doi.org/10.1145/3689031.3717496}, author = {Kurniawan, Daniar H. and Putri, Rani Ayu and Qin, Peiran and Zulkifli, Kahfi S. and Sinurat, Ray A. O. and Bhimani, Janki and Madireddy, Sandeep and Kistijantoro, Achmad Imam and Gunawi, Haryadi S.}, keywords = {Distributed systems, File and storage systems, I/O admission control, ML for systems, Operating systems, location = Rotterdam, Netherlands}, abstract = {This paper introduces Heimdall, a highly accurate and efficient machine learning-powered I/O admission policy for flash storage, designed to operate in a black-box manner. We make domain-specific innovations in various ML stages by introducing accurate period-based labeling, 3-stage noise filtering, in-depth feature engineering, and fine-grained tuning, which together improve the decision accuracy from 67\% up to 93\%. We perform various deployment optimizations to reach a sub-μs inference latency and a small, 28KB, memory overhead. With 500 unbiased random experiments derived from production traces, we show Heimdall delivers 15-35\% lower average I/O latency compared to the state of the art and up to 2x faster to a baseline. Heimdall is ready for user-level, in-kernel, and distributed deployments.} }
@inproceedings{10.1145/3651671.3651689, title = {Study on The Effect of Encoding Method in Quantum Machine Learning}, booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing}, pages = {94--99}, year = {2024}, isbn = {9798400709234}, doi = {10.1145/3651671.3651689}, url = {https://doi.org/10.1145/3651671.3651689}, author = {Xiong, Qingqing and Jiang, Jinzhe and Li, Chen and Zhang, Xin and Zhao, Yaqian}, keywords = {Data encoding, Metrics, Quantum machine learning, location = Shenzhen, China}, abstract = {Quantum machine learning algorithms use qubit encoding and quantum circuits to perform feature extraction and pattern recognition. However, the choice of data encoding methods can have a significant impact on the model’s performance. To evaluate the effect of different encoding methods in a systematic way, we propose two metrics: distribution distance and distribution radius. These metrics describe how the encoded data distribute in the Hilbert space. We show that there is a positive correlation between prediction accuracy and distribution distance, and a negative correlation between prediction accuracy and distribution radius, both theoretically and experimentally. Based on our findings, we suggest a comparative evaluation of data encoding methods for quantum machine learning, which can help improve the learning efficiency.} }
@inbook{10.1145/3718491.3718674, title = {Warehouse Demand Variation Prediction and Optimization Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {1136--1141}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718674}, author = {Lu, Yuting and Jomhari, Nazean and Liao, Shengshi and Goyal, Shyam Bihari}, abstract = {With the intensification of market competition and the diversification of customer demands, efficient inventory management has become a key factor for the success of enterprises. Warehouse demand forecasting, as a core aspect of inventory management, is of great significance for enterprises to reasonably arrange production, optimize inventory levels, and reduce costs. This paper delves into the prediction and optimization methods of warehouse demand changes based on machine learning algorithms, with a particular focus on three powerful algorithms: Random Forest, XGBoost, and LightGBM.the LightGBM model performed the best. The lowest MSE can provide a lower prediction error, and the highest R² value, which is close to 0.786, can better fit the data. The research results fully demonstrate that machine learning algorithms have significant application value and potential in the field of predicting changes in warehouse demand.} }
@proceedings{10.1145/3728199, title = {CNML '25: Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, year = {2025}, isbn = {9798400713231} }
@article{10.1145/3677119, title = {Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3677119}, url = {https://doi.org/10.1145/3677119}, author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag}, keywords = {Explainability in ML, counterfactual explanations, algorithmic recourse, interpretability in ML}, abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this article, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.} }
@inproceedings{10.1145/3712255.3726556, title = {Evaluating the Generalizability of Machine Learning Pipelines When Using Lexicase or Tournament Selection}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {283--286}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726556}, url = {https://doi.org/10.1145/3712255.3726556}, author = {Hernandez, Jose Guadalupe and Saini, Anil Kumar and Gupta, Ankit and Moore, Jason}, keywords = {AutoML, tournament selection, lexicase selection, parent selection, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Evolutionary Algorithms have been successfully applied in Automated Machine Learning (AutoML) to evolve effective machine learning (ML) pipelines. Here, we use 12 OpenML classification tasks and the AutoML tool TPOT2 to assess the impact of lexicase and tournament selection on the generalizability of pipelines. We use one of five stratified sampling splits to generate training and validation sets; pipelines are trained on the training set, and predictions are made on the validation set. Lexicase and tournament selection use these predictions to identify parents. At the end of a run, TPOT2 returns the pipeline that achieved the best validation accuracy while maintaining the lowest complexity. The generalizability of this pipeline is assessed using the test set provided for an OpenML task. We found that lexicase produced pipelines with higher validation accuracy than tournament selection in all tasks for at least one split. In contrast, tournament selection produced pipelines with greater generalizability for 10 of the 12 tasks on at least one split. For most cases where tournament selection outperformed lexicase on test accuracy, we detected differences in validation accuracy and pipeline complexity.} }
@inproceedings{10.1145/3626246.3656000, title = {The Limitations of Data, Machine Learning and Us}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {1--2}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3656000}, url = {https://doi.org/10.1145/3626246.3656000}, author = {Baeza-Yates, Ricardo}, keywords = {ai ethics, bias, legitimacy, ml evaluation, pseudoscience, location = Santiago AA, Chile}, abstract = {Machine learning (ML), particularly deep learning, is being used everywhere. However, not always is applied well or has ethical and/or scientific issues. In this keynote we first do a deep dive in the limitations of supervised ML and data, its key input. We cover small data, datification, bias, and evaluating success instead of harm, among other limitations. The second part is about ourselves using ML, including different types of social limitations and human incompetence such as cognitive biases, pseudoscience, or unethical applications. These limitations have harmful consequences such as discrimination, misinformation, and mental health issues, to mention just a few. In the final part we discuss regulation on the use of AI and responsible principles that can mitigate the problems outlined above.} }
@inbook{10.1145/3718491.3718587, title = {Extreme Weather Prediction and Prevention——Research Based on Machine Learning Method}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {595--600}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718587}, author = {Chao, Xiao}, abstract = {In this paper, machine learning method is used to predict and prevent extreme weather. Firstly, this paper introduces entropy weight -TOPSIS evaluation method and K-means clustering method, and combines LSTM model to evaluate and predict extreme weather conditions. By collecting a large number of eco-environmental data, this paper analyzes the trend and causes of extreme weather changes in China, and discusses it from the perspective of precipitation and land cover. The research results identify six areas with the most severe extreme weather in the future, and provide specific forecast scores. These findings are of great significance for preparing in advance and reducing the negative effects of extreme weather, which can help to take preventive measures, raise public awareness, strengthen infrastructure construction and optimize resource allocation.} }
@article{10.1145/3711699, title = {Exploring Function Granularity for Serverless Machine Learning Application with GPU Sharing}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, volume = {9}, year = {2025}, doi = {10.1145/3711699}, url = {https://doi.org/10.1145/3711699}, author = {Hui, Xinning and Xu, Yuanchao and Shen, Xipeng}, keywords = {cloud computing, deep learning, function granularity, function-as-a-service, machine learning for systems, quality of service, serverless computing}, abstract = {Recent years have witnessed increasing interest in machine learning (ML) inferences on serverless computing due to its auto-scaling and cost-effective properties. However, one critical aspect, function granularity, has been largely overlooked, limiting the potential of serverless ML. This paper explores the impact of function granularity on serverless ML, revealing its important effects on the SLO hit rates and resource costs of serverless applications. It further proposes adaptive granularity as an approach to addressing the phenomenon that no single granularity fits all applications and situations. It explores three predictive models and presents programming tools and runtime extensions to facilitate the integration of adaptive granularity into existing serverless platforms. Experiments show adaptive granularity produces up to a 29.2\% improvement in SLO hit rates and up to a 24.6\% reduction in resource costs over the state-of-the-art serverless ML which uses fixed granularity.} }
@inproceedings{10.1145/3746972.3746986, title = {Empirical Analysis and Trend Prediction of National Economic Development Based on Machine Learning Models}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {79--84}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3746986}, url = {https://doi.org/10.1145/3746972.3746986}, author = {Tu, Junhe}, keywords = {GDP forecasting, Support Vector Machine, economic development trends, machine learning, time series}, abstract = {Against the backdrop of increasing global economic uncertainties, accurately predicting the economic trends of a country is of vital importance for formulating effective macroeconomic policies. Gross domestic product (GDP), as a core indicator of economic health, plays a key role in decision-making. However, traditional economic forecasting models, such as autoregressive comprehensive moving average, show obvious limitations when dealing with complex nonlinear economic data. This study explores the application of machine learning models, including support vector machines and long short-term memory, to improve the accuracy of GDP prediction. This study used data from the World Bank's Open database (1960-2020) to compare the predictive performance of traditional models and machine learning models, with a focus on China and the United States. The results demonstrate that machine learning models outperform traditional methods in capturing nonlinear relationships and high-dimensional data, providing more reliable forecasts. The study also highlights the positive correlation between GDP growth and purchasing power parity, offering insights into the interaction between economic growth and monetary purchasing power. This research contributes to the field by providing empirical evidence for selecting economic forecasting methods and offering valuable references for policymakers to promote stable economic growth.} }
@inproceedings{10.1145/3639477.3639746, title = {Resolving Code Review Comments with Machine Learning}, booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice}, pages = {204--215}, year = {2024}, isbn = {9798400705014}, doi = {10.1145/3639477.3639746}, url = {https://doi.org/10.1145/3639477.3639746}, author = {Froemmgen, Alexander and Austin, Jacob and Choy, Peter and Ghelani, Nimesh and Kharatyan, Lera and Surita, Gabriela and Khrapko, Elena and Lamblin, Pascal and Manzagol, Pierre-Antoine and Revaj, Marcus and Tabachnyk, Maxim and Tarlow, Daniel and Villela, Kevin and Zheng, Daniel and Chandra, Satish and Maniatis, Petros}, abstract = {Code reviews are a critical part of the software development process, taking a significant amount of the code authors' and the code reviewers' time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see millions of reviewer comments per year, and authors require an average of ~60 minutes active shepherding time between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must devote to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code-review process, e.g., by proposing code changes based on a comment's text.We describe our application of recent advances in large sequence models in a real-world setting to automatically resolve code-review comments in the day-to-day development workflow at Google. We present the evolution of this feature from an asynchronous generation of suggested edits after the reviewer sends feedback, to an interactive experience that suggests code edits to the reviewer at review time. In deployment, code-change authors at Google address 7.5\% of all reviewer comments by applying an ML-suggested edit. The impact of this will be to reduce the time spent on code reviews by hundreds of thousands of engineer hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers' productivity and allows them to focus on more creative and complex tasks.} }
@inproceedings{10.1145/3732365.3732426, title = {Using machine learning for toxic text detection on Android}, booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering}, pages = {345--349}, year = {2025}, isbn = {9798400713613}, doi = {10.1145/3732365.3732426}, url = {https://doi.org/10.1145/3732365.3732426}, author = {Xue, Yue}, keywords = {Android platform, BERT model, harmful text}, abstract = {Harmful content is gradually emerging under the joint promotion of social media and instant messaging applications, such as hate speeches and fake news, which have a huge impact on social public opinion. Therefore, it is necessary to detect and filter harmful content through effective detection methods to prevent harmful content from causing harm to users. This detection technology is very important for social media, instant messaging applications, and content platforms. To this end, we mainly developed a harmful content detection tool for the Android platform, fine-tuned the BERT model to the Android platform, and finally got a robust scheme to detect harmful content, and got a 98.25\% F1 score.} }
@inproceedings{10.1145/3651671.3651683, title = {Automatic Machine Learning based Real Time Multi-Tasking Image Fusion}, booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing}, pages = {327--333}, year = {2024}, isbn = {9798400709234}, doi = {10.1145/3651671.3651683}, url = {https://doi.org/10.1145/3651671.3651683}, author = {Karim, Shahid and Tong, Geng and Li, Jinyang and Yu, Xiaochang and Hao, Jia and Yu, Yiting}, keywords = {automatic ML, imaging systems, multi-tasking image fusion, location = Shenzhen, China}, abstract = {Imaging systems work diversely in the image processing domain, and each system contains specific characteristics. We are developing models to fuse images from different sensors and environments to get promising outcomes for different computer vision applications. The multiple unified models have been developed for multiple tasks such as multi-focus (MF), multi-exposure (ME), and multi-modal (MM) image fusion. The careful tuning of such models is required to get optimal results, which are still not applicable to diverse applications. We propose an automatic machine learning (AML) based multi-tasking image fusion approach to overcome this problem. Initially, we evaluate source images with AML and feed them to the task-based models. Then, the source images are fused with the pre-trained and fine-tuned models. The experimental results authenticate the consequences of our proposed approach compared to generic approaches.} }
@inproceedings{10.1145/3742460.3742986, title = {Adaptive Water pH Sensing in Variable Conditions Using Near Infrared Imaging and Machine Learning}, booktitle = {Proceedings of the International Workshop on Environmental Sensing Systems for Smart Cities}, pages = {20--25}, year = {2025}, isbn = {9798400719868}, doi = {10.1145/3742460.3742986}, url = {https://doi.org/10.1145/3742460.3742986}, author = {Khmaissia, Fadoua and Ravi, Nirupama}, keywords = {water quality sensing, near-infrared imaging, machine learning, ubiquitous sensing, location = Hilton Anaheim, Anaheim, CA, USA}, abstract = {We present a preliminary study on non-invasive water pH sensing using consumer-grade near-infrared (NIR) imaging and machine learning, targeting mobile and field applications. Our main contribution is the NIR-pH dataset, which systematically captures NIR reflectance images of water samples with varying pH levels (4.01–9.18) under diverse environmental conditions, including changes in lighting, distance, volume, and container position. Employing an attention-based neural network, our approach achieves 85.8\% accuracy in classifying water pH, with interpretability analyses confirming the model's focus on relevant water surface features. This work aims to establish a foundation for portable, accessible water quality monitoring and introduces a new machine learning task and dataset to support further research in ubiquitous environmental sensing.} }
@inproceedings{10.1145/3703790.3703801, title = {Sensor-Guided Adaptive Machine Learning on Resource-Constrained Devices}, booktitle = {Proceedings of the 14th International Conference on the Internet of Things}, pages = {90--98}, year = {2025}, isbn = {9798400712852}, doi = {10.1145/3703790.3703801}, url = {https://doi.org/10.1145/3703790.3703801}, author = {Papst, Franz and Kraus, Daniel and Rechberger, Martin and Saukh, Olga}, keywords = {Machine Learning, Sensor Guided, Data Augmentation, Model Adaptation, Resource Efficient}, abstract = {In recent years, the deployment of deep learning models has extended beyond typical cloud environments to resource-constrained devices such as edge devices and smartphones. This shift is driven by their success in learning and detecting patterns in data. However, deep models are often excessively large and lack robustness to minor input transformations. To solve the challenge, deep learning models are often trained with data augmentation, which requires an even larger model to accommodate the additional knowledge. In this paper, we study ways to mitigate these problems by leveraging additional sensing modalities to a) adapt the input data and b) adapt the model for typical transformations. We show that both approaches increase the accuracy of deep learning models by up to 6.21\% and 7.57\% respectively, while using roughly the same number of parameters or even less at inference time. We furthermore study how well these approaches can handle noisy sensor readings.} }
@article{10.14778/3742728.3742753, title = {Robust Plan Evaluation Based on Approximate Probabilistic Machine Learning}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {2626--2638}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3742728.3742753}, url = {https://doi.org/10.14778/3742728.3742753}, author = {Kamali, Amin and Kantere, Verena and Zuzarte, Calisto and Corvinelli, Vincent}, abstract = {Query optimizers in RDBMSs search for execution plans expected to be optimal for given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select plans that are suboptimal at runtime, if estimates and assumptions are not valid. Therefore, they do not sufficiently support robust query optimization. Using ML to improve data systems has shown promising results for query optimization. Inspired by this, we propose Robust Query Optimizer, (Roq), a holistic framework based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq includes a novel learned cost model that is designed to predict the cost of query execution and the associated risks and performs query optimization accordingly. We demonstrate that Roq provides significant improvements in robust query optimization compared with the state-of-the-art.} }
@inproceedings{10.1145/3643796.3648464, title = {Detecting Security-Relevant Methods using Multi-label Machine Learning}, booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments}, pages = {101--106}, year = {2024}, isbn = {9798400705809}, doi = {10.1145/3643796.3648464}, url = {https://doi.org/10.1145/3643796.3648464}, author = {Johnson, Oshando and Piskachev, Goran and Krishnamurthy, Ranjith and Bodden, Eric}, keywords = {static analysis, software security, machine learning, vulnerability detection, multi-label learning, IntelliJ plugin development, location = Lisbon, Portugal}, abstract = {To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine learning approach has a higher F1-Measure than related approaches. Moreover, the plugin reduces and simplifies the manual effort required when configuring and using static analysis tools.} }
@inproceedings{10.1145/3676536.3676696, title = {CAMSHAP: Accelerating Machine Learning Model Explainability with Analog CAM}, booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design}, year = {2025}, isbn = {9798400710773}, doi = {10.1145/3676536.3676696}, url = {https://doi.org/10.1145/3676536.3676696}, author = {Moon, John and Pedretti, Giacomo and Bruel, Pedro and Serebryakov, Sergey and Eldash, Omar and Buonanno, Luca and Graves, Catherine E. and Faraboschi, Paolo and Ignowski, Jim}, keywords = {explainability, shapley additive explanation, XGBoost, content-addressable memory, location = Newark Liberty International Airport Marriott, New York, NY, USA}, abstract = {The recent success of machine learning (ML) models has led to increasing demands for model explanations - why a result was given - along with model predictions. Tree-based ML models are considered more explainable than deep neural networks and higher performers in several domains. However, algorithms computing model explanations are irregular and scale poorly with model size. While many custom accelerators for training and inference have been proposed, little attention has been paid to accelerating model explanations. This lack of explanatory capability has limited the use of these models for real-time decision-making systems in critical fields such as healthcare, autonomous operation and cybersecurity.In this paper, we propose an architecture called CAMSHAP based on analog content-addressable memory (CAM) for accelerating tree-based ML model explanations. Parallel search in CAMs and reduction through an H-Tree network on chip resolve load imbalance and reduction overhead challenges, which limit tree-based ML model efficiency on GPU. We build a cycle-accurate and functional simulator as well as custom instructions to demonstrate our approach. Studies on device variation also show the robustness of CAMSHAP and resilience to device noise. On evaluations across 5 different datasets, CAMSHAP shows 10 higher throughput, 191 smaller latency, and 41 smaller energy consumption compared to GPU for explaining tree-based ML model outputs.} }
@inproceedings{10.1145/3757749.3757764, title = {Machine Learning-Driven Optimization of Public Health Education Resources and Personalized Learning Systems}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {86--91}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757764}, url = {https://doi.org/10.1145/3757749.3757764}, author = {Wang, Ziquan and Jiang, Yunxia}, keywords = {Artificial intelligence, Machine learning, Optimization of public health education resources, Personalized learning, Support machine vector}, abstract = {As the artificial intelligence (AI) is developing swiftly, it is becoming more common in public health and education. The lack of egalitarian distribution of educational resources and the urgent requirement of the learning personalization has exposed that the intelligent, data-driven AI is desired now. We investigate the AI that has a better ability to ration public health education resources with personalized learning services. By using the mixed methods of case analysis and machine learning, an AI model based on Support Vector Machine (SVM) algorithms is used to analyze the user behavior data, and to calculate the effective educational programs recommendation models for each individual and in different regions based on their requirements. We find that the AI model, in special the one trained with SVM can leverage the efficiency and quality of public health education resources; it can bring high rewards for individual learning services and the early intervention of public health education. Finally, this work also exhibits how the AI model can help even the education resources by advancing the traditional education resource allocation in public health and learning personalization.} }
@inproceedings{10.1145/3634737.3657026, title = {Cloud-Based Machine Learning Models as Covert Communication Channels}, booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security}, pages = {141--157}, year = {2024}, isbn = {9798400704826}, doi = {10.1145/3634737.3657026}, url = {https://doi.org/10.1145/3634737.3657026}, author = {Krau, Torsten and Stang, Jasper and Dmitrienko, Alexandra}, keywords = {covert channel, machine learning, poisoning attacks, backdoors, location = Singapore, Singapore}, abstract = {While Machine Learning (ML) is one of the most promising technologies in our era, it is prone to a variety of attacks. One of them is covert channels, that enable two parties to stealthily transmit information through carriers intended for different purposes. Existing works only explore covert channels for federated ML. Thereby, communication is established among multiple entities that collaborate to train a model, while relying on access to model internals.This paper presents covert channels within ML models trained and publicly deployed in cloud-based (black-box) environments. The approach relies on targeted poisoning, or backdoor, attacks to encode messages into the model. It incorporates multiple well-chosen backdoors only through dataset poisoning and without requiring access to model internals or the training process. After model deployment, messages can be extracted via inference.We propose three covert channel versions with varying levels of message robustness and capacity while emphasizing minimal extraction effort, minimal pre-shared knowledge, or maximum message stealthiness. We investigate influencing factors affecting embedded backdoors and propose novel techniques to incorporate numerous backdoors simultaneously for message encoding. Experiments across various datasets and model architectures demonstrate message transmission of 20 to 66 bits with minimal error rates.} }
@inproceedings{10.1145/3631295.3631399, title = {Leveraging Intra-Function Parallelism in Serverless Machine Learning}, booktitle = {Proceedings of the 9th International Workshop on Serverless Computing}, pages = {36--41}, year = {2023}, isbn = {9798400704550}, doi = {10.1145/3631295.3631399}, url = {https://doi.org/10.1145/3631295.3631399}, author = {Predoaia, Ionut and Garc\'a-L\'opez, Pedro}, keywords = {Intra-Function Parallelism, Lithops, Machine Learning, Multicore Functions, Serverless, Stateful, location = Bologna, Italy}, abstract = {Running stateful machine learning algorithms with serverless architectures inherently induces overheads, as serverless functions are not directly network-addressable, hence one must rely on a remote storage service for storing the shared state. To hide the access latency to the remote storage, one can employ intra-function parallelism to take advantage of the multicore computing resources of the serverless functions. In this work, we port to serverless two stateful machine learning algorithms, k-means clustering and logistic regression, and then adopt intra-function parallelism to parallelize the execution of the serverless functions. Several experiments have demonstrated that intra-function parallelism delivers performance improvements in serverless machine learning. Improved performances of up to 68\% have been achieved when running k-means on serverless functions that employ intra-function parallelism. We demonstrate with k-means and logistic regression that from a performance perspective it is preferable to execute a smaller number of multiple-vCPUs workers than a larger number of single-vCPU workers, due to decreased synchronization overheads.} }
@article{10.1145/3706029, title = {A Machine Learning Approach to Resolving Conflicts in Physical Human–Robot Interaction}, journal = {J. Hum.-Robot Interact.}, volume = {14}, year = {2025}, doi = {10.1145/3706029}, url = {https://doi.org/10.1145/3706029}, author = {Dincer, Enes Ulas and Al-Saadi, Zaid and Hamad, Yahya M. and Aydin, Yusuf and Kucukyilmaz, Ayse and Basdogan, Cagatay}, keywords = {physical human–robot interaction, dyadic manipulation, conflict resolution, machine learning, classification of interaction behaviors, haptic features, subjective questionnaire, performance metrics}, abstract = {As artificial intelligence techniques become more sophisticated, we anticipate that robots collaborating with humans will develop their own intentions, leading to potential conflicts in interaction. This development calls for advanced conflict resolution strategies in physical human–robot interaction (pHRI), a key focus of our research. We use a machine learning (ML) classifier to detect conflicts during co-manipulation tasks to adapt the robot’s behavior accordingly using an admittance controller. In our approach, we focus on two groups of interactions, namely “harmonious” and “conflicting,” corresponding respectively to the cases of the human and the robot working in harmony to transport an object when they aim for the same target, and human and robot are in conflict when human changes the manipulation plan, e.g. due to a change in the direction of movement or parking location of the object.Co-manipulation scenarios were designed to investigate the efficacy of the proposed ML approach, involving 20 participants. Task performance achieved by the ML approach was compared against three alternative approaches: (a) a rule-based (RB) Approach, where interaction behaviors were rule-derived from statistical distributions of haptic features; (b) an unyielding robot that is proactive during harmonious interactions but does not resolve conflicts otherwise, and (c) a passive robot which always follows the human partner. This mode of cooperation is known as “hand guidance” in pHRI literature and is frequently used in industrial settings for so-called “teaching” a trajectory to a collaborative robot.The results show that the proposed ML approach is superior to the others in task performance. However, a detailed questionnaire administered after the experiments, which contains several metrics, covering a spectrum of dimensions to measure the subjective opinion of the participants, reveals that the most preferred mode of interaction with the robot is surprisingly passive. This preference indicates a strong inclination toward an interaction mode that gives more control to humans and offers less demanding interaction, even if it is not the most efficient in task performance. Hence, there is a clear trade-off between task performance and the preferred mode of interaction of humans with a robot, and a well-balanced approach is necessary for designing effective pHRI systems in the future.} }
@inproceedings{10.1109/ICSE55347.2025.00066, title = {Answering User Questions about Machine Learning Models through Standardized Model Cards}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {1488--1500}, year = {2025}, isbn = {9798331505691}, doi = {10.1109/ICSE55347.2025.00066}, url = {https://doi.org/10.1109/ICSE55347.2025.00066}, author = {Toma, Tajkia Rahman and Grewal, Balreet and Bezemer, Cor-Paul}, keywords = {machine learning model hubs, model cards, questions \&amp, answers, hugging face, location = Ottawa, Ontario, Canada}, abstract = {Reusing pre-trained machine learning models is becoming very popular due to model hubs such as Hugging Face (HF). However, similar to when reusing software, many issues may arise when reusing an ML model. In many cases, users resort to asking questions on discussion forums such as the HF community forum. In this paper, we study how we can reduce the community's workload in answering these questions and increase the likelihood that questions receive a quick answer. We analyze 11,278 discussions from the HF model community that contain user questions about ML models. We focus on the effort spent handling questions, the high-level topics of discussions, and the potential for standardizing responses in model cards based on a model card template. Our findings indicate that there is not much effort involved in responding to user questions, however, 40.1\% of the questions remain open without any response. A topic analysis shows that discussions are more centered around technical details on model development and troubleshooting, indicating that more input from model providers is required. We show that 42.5\% of the questions could have been answered if the model provider followed a standard model card template for the model card. Based on our analysis, we recommend that model providers add more development-related details on the model's architecture, algorithm, data preprocessing and training code in existing documentation (sub)sections and add new (sub)sections to the template to address common questions about model usage and hardware requirements.} }
@inproceedings{10.1145/3712255.3734248, title = {Evolutionary Co-Optimization of Rule Shape and Fuzziness in Rule-Based Machine Learning}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {71--72}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734248}, url = {https://doi.org/10.1145/3712255.3734248}, author = {Shiraishi, Hiroki and Hayamizu, Yohei and Hashiyama, Tomonori and Takadama, Keiki and Ishibuchi, Hisao and Nakata, Masaya}, keywords = {learning classifier systems, knowledge representation, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Rule-based machine learning systems face a fundamental representation challenge: traditional approaches require a priori selection between crisp intervals and/or fuzzy membership functions. This can result in either overly complex fuzzy rule sets or insufficiently expressive crisp rule sets. To address this limitation, we introduce a novel evolutionary approach for learning classifier system (LCS) machine learning algorithms that co-optimizes both rule shape and fuzziness using a four-parameter beta distribution. Our method integrates specialized genetic operators with generalization pressure mechanisms, such as subsumption and crispification operators, to favor crisp, interpretable rules when possible. Experiments on real-world classification tasks demonstrate competitive accuracy compared to state-of-the-art black-box models while maintaining superior interpretability. Our method can automatically determine appropriate rule representations for different feature space regions, evolving toward simpler crisp rules where possible while retaining fuzzy rules only where necessary for handling complex decision boundaries.This paper summarizes the following IEEE TEVC article: Hiroki Shiraishi, Yohei Hayamizu, Tomonori Hashiyama, Keiki Takadama, Hisao Ishibuchi, and Masaya Nakata. 2025. Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems. IEEE Transactions on Evolutionary Computation. https://doi.org/10.1109/TEVC.2025.3550915 [1]. Our implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS.} }
@inproceedings{10.1145/3626203.3670525, title = {A Comprehensive Cloud Architecture for Machine Learning-enabled Research}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670525}, url = {https://doi.org/10.1145/3626203.3670525}, author = {Stubbs, Joe and Indrakusuma, Dhanny and Garcia, Christian and Halbach, Francois and Hammock, Cody and Freeman, Nathan and Jamthe, Anagha and Packard, Michael and Fields, Alexander and Curbelo, Gilbert}, keywords = {Cloud Computing, GPUs, Machine Learning, location = Providence, RI, USA}, abstract = {The success of machine learning (ML) algorithms, and deep learning in particular, is having a transformative impact on a wide range of research disciplines, from astronomy, materials science, and climate change to bioinformatics, computational health, and animal ecology. At the same time, these new techniques introduce computational modalities that create challenges for academic computing centers and resource providers that have historically focused on asynchronous, batch-computing paradigms. In particular, there is an emergent need for computing models that enable efficient use of specialized hardware such as graphical processing units (GPUs) in the presence of interactive workloads. In this paper, we present a comprehensive, cloud-based architecture comprised of open-source software layers to better meet the needs of modern ML processes and workloads. This framework, deployed at the Texas Advanced Computing Center and in use by various research teams, provides different interfaces at varying levels of abstraction to support and simplify the tasks of users with different backgrounds and expertise, and to efficiently leverage limited GPU resources for these tasks. We present techniques and implementation details for overcoming challenges related to developing and maintaining such an infrastructure which will be of interest to service providers and infrastructure developers alike.} }
@inproceedings{10.1145/3727582.3728681, title = {Leveraging LLM Enhanced Commit Messages to Improve Machine Learning Based Test Case Prioritization}, booktitle = {Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering}, pages = {45--54}, year = {2025}, isbn = {9798400715945}, doi = {10.1145/3727582.3728681}, url = {https://doi.org/10.1145/3727582.3728681}, author = {Mahmoud, Yara and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta}, keywords = {commit messages, large language model, natural language processing, software testing, test case prioritization, location = Trondheim, Norway}, abstract = {In the rapidly evolving landscape of software development, software testing is critical for maintaining code quality and reducing defects. Effective test case prioritization employs techniques to identify defects early and ensure software quality. New avenues of research have explored using machine learning (ML) to automate the process, most current applications leverage a machine learning model using numerical features to prioritize the test cases. This study investigates the enhancement of this process by incorporating text-based features derived from git commit messages, which often include valuable information about code changes. Given that commit messages are often poorly written and inconsistent, we employ a large language model (LLM) to rewrite these messages based on code diffs, with the aim of improving the quality of their format and the information they contain. We then assess whether these refined commit messages, as an additional feature, contribute to better performance of the test case prioritization model. Our preliminary results indicate that the inclusion of LLM-enhanced commit messages leads to a noticeable improvement in prioritization effectiveness, suggesting a promising avenue for integrating natural language processing techniques in software testing workflows.} }
@inproceedings{10.1145/3626253.3635497, title = {Developing Interactive Exercise Materials for Machine Learning Using Spreadsheets}, booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1734--1735}, year = {2024}, isbn = {9798400704246}, doi = {10.1145/3626253.3635497}, url = {https://doi.org/10.1145/3626253.3635497}, author = {Maeda, Atsuhiko}, keywords = {exercise materials, machine learning, spreadsheets, location = Portland, OR, USA}, abstract = {This paper presents a new technique for creating machine-learning exercise materials using spreadsheets. Spreadsheets are often used in teaching machine learning for beginners and non-specialists. However, computation in machine learning models is divided into a learning phase and an inference phase, and conventional spreadsheet-based materials either rely on the software's extensions for the learning phase or can result in huge sheets, which is unsuitable for learners who want to observe it. We realize the learning phase on a spreadsheet that fits within a PC screen, including visualization for better understanding, without using any extensions. Also, this implementation technique makes it possible to execute or pause the learning process interactively. We will show an example of a neural network implementation and discuss the merits and limitations of this technique.} }
@inproceedings{10.1145/3595360.3595852, title = {Teaching Blue Elephants the Maths for Machine Learning}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595852}, url = {https://doi.org/10.1145/3595360.3595852}, author = {Ruck, Clemens and Sch\"ule, Maximilian Emanuel}, keywords = {automatic differentiation, in-database machine learning, location = Seattle, WA, USA}, abstract = {Code-generation suits well for reverse mode automatic differentiation as it stores each partial derivative as a virtual register. Since the introduction of just-in-time compilation in PostgreSQL, an efficient operator for automatic differentiation seems feasible. Such an operator would allow for in-database machine learning and thus eliminate the need for data extraction.In this paper, we propose automatic differentiation in PostgreSQL: We extend our proposed SQL lambda functions to compute the derivatives even for matrix operations by traversing the expression tree. The evaluation proves that the compiled execution is up to six times faster than the interpreted runs for regression tasks.} }
@proceedings{10.1145/3749566, title = {IoTML '25: Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, year = {2025}, isbn = {9798400713927} }
@article{10.1145/3636341.3636360, title = {Adversarial Machine Learning in Recommender Systems}, journal = {SIGIR Forum}, volume = {57}, year = {2023}, issn = {0163-5840}, doi = {10.1145/3636341.3636360}, url = {https://doi.org/10.1145/3636341.3636360}, author = {Merra, Felice Antonio}, abstract = {Recommender systems are ubiquitous. Our digital lives are influenced by their use when, for instance, we select the news to read, the product to buy, the friend to connect with, and the movie to watch. While enormous academic research efforts have been mainly focused on getting high-quality recommendations to reach maximum user satisfaction, little effort has been devoted to studying the integrity and security of these systems. Is there an underlying relationship between the characteristics of the historical user-item interactions and the efficacy of injection of false users/feedback strategies against collaborative models? Can public semantic data be used to perform attacks more potent in raising the recommendability of victim items? Can a malicious user poison or evade the image data of visual recommenders with adversarial perturbed product images? Is the family of model-based recommenders more vulnerable to multi-step gradient-based adversarial perturbations? Furthermore, is the adversarial training robustification still effective in the last scenario? Is this training defense influencing the beyond-accuracy and bias performance?This dissertation intends to pave the way towards more robust recommender systems, beginning with understanding how to robustify a model, what is the cost of robustness in terms of reduction of recommendation accuracy, and which are the novel adversarial risks of modern recommenders. This thesis, getting inspiration from the literature on the security of collaborative models against the insertion of hand-engineered fake profiles and the recent advances of adversarial machine learning methods in other research areas like computer vision, contributes to several directions: (i) the proposal of a practical framework to interpret the impact of data characteristics on the robustness of collaborative recommenders [Deldjoo et al., 2020], (ii) the design of powerful attack strategies using publicly available semantic data [Anelli et al., 2020], (iii) the identification of severe adversarial vulnerabilities of visual-based recommender models where adversaries can break the recommendation integrity by pushing products to the highest recommendation positions with a simple and human-imperceptible perturbation of products' images [Anelli et al., 2021b], (iv) the proposal of robust adversarial perturbation methods capable of completely breaking the accuracy of matrix factorization recommenders [Anelli et al., 2021a], and (v) a formal study that examines the effects of adversarial training in reducing the recommendation quality of state-of-the-art model-based recommenders Anelli et al. [2021c].Awarded by: Politecnico di Bari, Bari, Italy on 24 January 2022.Supervised by: Tommaso DI Noia.Available at: https://iris.poliba.it/retrieve/dd89f8a6-faa4-ccdd-e053-6605fe0a1b87/Adversarial_Machine_Learning_in_Recommender_Systems.pdf.} }
@inbook{10.1145/3711875.3736686, title = {SPATIUM: A Context-Aware Machine Learning Framework for Immersive Spatiotemporal Health Understanding}, booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services}, pages = {759--764}, year = {2025}, isbn = {9798400714535}, url = {https://doi.org/10.1145/3711875.3736686}, author = {Liu, Yang and Jang, SiYoung and Montanari, Alessandro and Kawsar, Fahim}, abstract = {Wearable devices have made significant progress in continuous health monitoring by providing time-series physiological data such as heart rate, respiration, and activity levels. However, most lack spatial awareness, limiting their ability to interpret physiological changes within environmental context—especially indoors where GPS is unreliable. Recent advances in indoor localization, such as Ultra-Wideband (UWB) and visual-inertial odometry, now allow precise spatial positioning in consumer devices. In this paper, we introduce SPATIUM, a context-aware machine learning framework designed to enable immersive spatiotemporal health understanding. SPATIUM integrates multimodal health signals with spatial and environmental data, such as furniture layout, lighting, and temperature, to support more contextually grounded health analysis. We present a proof-of-concept system that combines OmniBuds and UWB localization to collect and visualize spatiotemporal health data in 2D, 3D, and immersive formats. To evaluate the potential impact of spatial features, we conduct a simulation showing that incorporating spatial context improves physiological classification accuracy by up to 26\%. Our results highlight the importance of context-aware, spatially grounded modeling in achieving immersive spatiotemporal health understanding.} }
@inproceedings{10.1145/3747227.3747234, title = {Machine Learning-Driven Implicit Evaluation in College English Ideological Education: A Hybrid Corpus-Linguistic and Survey-Based Approach}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {42--47}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747234}, url = {https://doi.org/10.1145/3747227.3747234}, author = {Yang, Wei}, keywords = {Appraisal Theory, Engagement system, Ideological and Political Education in College English, Implicit evaluation, Machine Learning, Support Machine Vector, UAM Corpus Tool}, abstract = {In this paper, discourses in English have been analyzed by means of UAM Corpus Tool, in the light of Appraisal Theory. It counts the distribution of engagement resources and investigates their contributions to expressions of evaluation and intersubjectivity. The questionnaire survey gathers the feedbacks on implicit evaluation from 310 students in the College English Course. It processes the survey data by machine learning, in particular support vector machines (SVM). It presents quantitative perspective on the opinions of students toward the implicit evaluation. The adopted SVM model raises accuracy of students’ opinion, and it reveals hidden patterns in students’ opinions. The study indicates that implicit evaluation of the engagement system serves College English teachers. It makes their lectures more persuasive and promotes students’ critical thinking ability. It also guides students’ values and enhances their ability of international communication.} }
@article{10.1145/3712198, title = {Uncovering Community Smells in Machine Learning-Enabled Systems: Causes, Effects, and Mitigation Strategies}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3712198}, url = {https://doi.org/10.1145/3712198}, author = {Annunziata, Giusy and Lambiase, Stefano and Tamburri, Damian A. and van den Heuvel, Willem-Jan and Palomba, Fabio and Catolino, Gemma and Ferrucci, Filomena and De Lucia, Andrea}, keywords = {Socio-Technical Aspects, ML-Enabled Teams, Partial Least Squares Structural Equation Modeling, PLS-SEM}, abstract = {Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This article addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities.} }
@inproceedings{10.1145/3674399.3674469, title = {Access Structure Selection for Knowledge Graphs Based on Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024}, pages = {214--215}, year = {2024}, isbn = {9798400710117}, doi = {10.1145/3674399.3674469}, url = {https://doi.org/10.1145/3674399.3674469}, author = {Qi, Zhixin and Wang, Hongzhi}, keywords = {Index Selection, Knowledge Graph, Machine Learning, Performance Prediction, Physical Design Tuning, Storage Structure, location = Changsha, China}, abstract = {In recent years, the rapid development of machine learning technology has provided opportunities for the automatic access structure selection of knowledge graph data. Considering that machine learning is suitable to describe the complex patterns and solve the complex optimization problems, this paper adopts machine learning techniques to predict the performance of knowledge graph storage structures, tune the storage structure of a knowledge graph, and select the index configurations for a knowledge graph automatically.} }
@inproceedings{10.1145/3727353.3727501, title = {Machine Learning Analysis of Carbon Inclusion Drivers in Chinese Universities}, booktitle = {Proceedings of the 2025 4th International Conference on Big Data, Information and Computer Network}, pages = {409--414}, year = {2025}, isbn = {9798400712425}, doi = {10.1145/3727353.3727501}, url = {https://doi.org/10.1145/3727353.3727501}, author = {Tang, Dongping and Abdulraheem, Ahmed Ali Ahmed and Wu, Xiaoli}, keywords = {Chinese universities, carbon inclusion, ecological modernization theory, theory of planned behavior}, abstract = {Carbon inclusion is a recent idea where low-carbon and carbon-neutral principles are integrated into the operations and development strategies of Chinese universities. This is especially important since these organizations work to fulfill the national target of reaching carbon neutrality by 2060. In this paper, we investigate the main elements that influence the implementation of carbon inclusion in Chinese institutions in a bid to determine their driving mechanisms. We use Ecological Modernization Theory (EMT) and the Theory of Planned Behavior (TPB) to offer a theoretical framework clarifying both institutional and personal behavior. The EMT emphasizes collaboration between the state and market to achieve environmental sustainability, whereas the TPB highlights the influence of attitudes, social norms, and perceived control on behavior formation. We collected data from colleges in Guangdong province using a quantitative approach. The data was analyzed with advanced machine learning techniques such as clustering and classification models. The analyses revealed distinct university profiles based on sustainability practices, validated hypotheses regarding drivers and barriers, and highlighted critical interactions among factors such as energy efficiency, policy support, and stakeholder engagement. Our study provides actionable insights for university administrators, policymakers, sustainability practitioners and other relevant stakeholders. The results enhance academic discourse and bolsters governmental and societal initiatives that are aimed at attaining carbon neutrality in the Chinese higher education sector.} }
@article{10.1145/3603171, title = {TinyNS: Platform-aware Neurosymbolic Auto Tiny Machine Learning}, journal = {ACM Trans. Embed. Comput. Syst.}, volume = {23}, year = {2024}, issn = {1539-9087}, doi = {10.1145/3603171}, url = {https://doi.org/10.1145/3603171}, author = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Aggarwal, Mohit and Wang, Brian and Han, Liying and Briseno, Julian De Gortari and Srivastava, Mani}, keywords = {Neurosymbolic, neural architecture search, TinyML, AutoML, bayesian, platform-aware}, abstract = {Machine learning at the extreme edge has enabled a plethora of intelligent, time-critical, and remote applications. However, deploying interpretable artificial intelligence systems that can perform high-level symbolic reasoning and satisfy the underlying system rules and physics within the tight platform resource constraints is challenging. In this article, we introduce TinyNS, the first platform-aware neurosymbolic architecture search framework for joint optimization of symbolic and neural operators. TinyNS provides recipes and parsers to automatically write microcontroller code for five types of neurosymbolic models, combining the context awareness and integrity of symbolic techniques with the robustness and performance of machine learning models. TinyNS uses a fast, gradient-free, black-box Bayesian optimizer over discontinuous, conditional, numeric, and categorical search spaces to find the best synergy of symbolic code and neural networks within the hardware resource budget. To guarantee deployability, TinyNS talks to the target hardware during the optimization process. We showcase the utility of TinyNS by deploying microcontroller-class neurosymbolic models through several case studies. In all use cases, TinyNS outperforms purely neural or purely symbolic approaches while guaranteeing execution on real hardware.} }
@article{10.1145/3708320, title = {Adversarial Machine Learning Attacks and Defences in Multi-Agent Reinforcement Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708320}, url = {https://doi.org/10.1145/3708320}, author = {Standen, Maxwell and Kim, Junae and Szabo, Claudia}, keywords = {Security, robustness, perturbation, adversarial machine learning, deep reinforcement learning, defence, mitigation, communication, regularisation, detection, memory, ensemble}, abstract = {Multi-Agent Reinforcement Learning (MARL) is susceptible to Adversarial Machine Learning (AML) attacks. Execution-time AML attacks against MARL are complex due to effects that propagate across time and between agents. To understand the interaction between AML and MARL, this survey covers attacks and defences for MARL, Multi-Agent Learning (MAL), and Deep Reinforcement Learning (DRL). This survey proposes a novel perspective on AML attacks based on attack vectors. This survey also proposes a framework that addresses gaps in current modelling frameworks and enables the comparison of different attacks against MARL. Lastly, the survey identifies knowledge gaps and future avenues of research.} }
@inproceedings{10.1145/3724154.3724355, title = {Research on highway freight pricing based on machine learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {1235--1240}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724355}, url = {https://doi.org/10.1145/3724154.3724355}, author = {Jin, Chenyu and Lu, Xiaochun and Wu, Xiaoliang}, keywords = {Highway freight, Machine learning, Stacking combination}, abstract = {The logistics industry has become an important support for China's economy, and the road freight transportation business is the mainstay of the domestic logistics market. The highway network has created good conditions for highway freight transportation. With its advantages of flexibility and adaptability, highway transportation has assumed most of the transportation tasks in the domestic freight market and occupied most of the domestic freight market. At present, there are a large number of individual transporters in the road freight market, and there are problems such as oversupply of services and low freight rates. With the increase in transportation costs, the profit margin is getting smaller and smaller. What's more, there is still malicious price competition in the road freight market, which leads to the confusion of freight prices in the road freight market. Based on this, this paper extracts the factors and characteristics that affect the price of highway vehicle transportation, applies the tree model to train after correlation analysis, and Stacks the adjusted model. The results show that the average quotation deviation rate of the model after Stacking fusion is small and the effect is good, which provides an effective price reference for highway vehicle transportation and is conducive to promoting the orderly development of the highway freight market.} }
@inproceedings{10.1145/3674805.3686678, title = {An Investigation of How Software Developers Read Machine Learning Code}, booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement}, pages = {165--176}, year = {2024}, isbn = {9798400710476}, doi = {10.1145/3674805.3686678}, url = {https://doi.org/10.1145/3674805.3686678}, author = {Weber, Thomas and Winiker, Christina and Mayer, Sven}, keywords = {code reading, eye tracking, human computer interaction, software developers, location = Barcelona, Spain}, abstract = {Background\&nbsp;Machine Learning plays an ever-growing role in everyday software. This means a paradigmatic shift in how software operators from algorithm-centered software where the developers defines the functionality to data-driven development where behavior is inferred from data. Aims\&nbsp;The goal of our research is to determine how this paradigmatic shift materializes in the written code and whether developers are aware of these changes and how they affect their behavior. Method\&nbsp;To this end, we perform static analysis of 3,515 software repositories to determine structural differences in the code. Following this, we conducted a user study using eye tracking (N=18) to determine how the code reading of developers differs when reading Machine Learning source code versus traditional code. Results\&nbsp;The results show that there are structural differences in the code of this paradigmatically different software. Developers appear to adapt their mental models with growing experience resulting in distinctly different reading patterns. Conclusions\&nbsp;These difference highlight that we cannot treat all code the same but require paradigm-specific, empirically validated support mechanisms to help developers write high-quality code.} }
@inproceedings{10.1145/3715931.3715936, title = {Prediction of Anesthesia Resuscitation after Painless Gastroenteroscopy Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Intelligent Medicine and Health}, pages = {27--31}, year = {2025}, isbn = {9798400709616}, doi = {10.1145/3715931.3715936}, url = {https://doi.org/10.1145/3715931.3715936}, author = {Huang, Xueping and He, Qinghua and Zhou, Yun}, keywords = {General anesthesia, Machine learning, Painless gastroenteroscopy, Prediction model}, abstract = {Objective To construct a prediction model for post-anesthesia recovery after endoscopy and validate it. Methods Selection of 2400 surgical patients who underwent painless gastroenteroscopy at Sichuan Electric Power Hospital from September 2019 to September 2024. After data preprocessing, the Logistic regression, random forest (RF), support vector machine (SVM), decision tree (DT), and extreme gradient boosting (XGBoost) model methods were used to construct prediction models. The predictive accuracy of the machine learning models was evaluated by calculating the area under the receiver operating characteristic (ROC) curve, and the contribution of each feature to the prediction result was evaluated by using the Shapley additive explanations (SHAP) method. Results According to the performance indicators, the best model among the five models was the XGBoost regression model, and the nine indicators were found to be closely related to the recovery time of general anesthesia during the feature importance analysis. Conclusion The XGBoost model constructed using multiple data can analyze the complex relationships between variables and outcomes and has good predictive performance and clinical value.} }
@inproceedings{10.1145/3735014.3735909, title = {A Machine Learning-Based Prediction Model for High-Speed Railway Track Geometry}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {256--262}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735909}, url = {https://doi.org/10.1145/3735014.3735909}, author = {Zhou, Jing and Liang, Fang}, keywords = {Geometric state of orbit, High-speed railway, Machine learning, Prediction model}, abstract = {This work is devoted to developing a prediction scheme of the high-speed railway (HSR) track geometry state using machine learning technology to improve the efficiency of track maintenance. Many HSR orbit geometric data are systematically collected, and an efficient prediction framework is successfully established by using the maximum likelihood estimation method. Firstly, the research problems are clarified on the technical route, and a perfect data acquisition system is established to ensure the representativeness and reliability of data samples. Then, with the help of systematic feature engineering, the original dataset is deeply mined and preprocessed, and the most predictive feature subset is selected. In the modeling stage, many machine learning algorithms are compared horizontally, and the random forest (RF) algorithm is finally determined and optimized as the core prediction model. After rigorous experimental verification and performance evaluation, the constructed prediction system shows excellent performance in multiple evaluation dimensions, in which the accuracy rate is improved to 95\%, and it also shows excellent generalization performance and robustness. This research result confirms the value of machine learning in the HSR track geometry state prediction field and provides reliable technical support for track maintenance decisions.CCS CONCEPTS • Computing methodologies∼Machine learning∼Machine learning approaches∼Learning in probabilistic graphical models∼Maximum likelihood modeling} }
@inproceedings{10.1145/3716368.3735244, title = {Adversarial Data Poisoning Attack on Quantum Machine Learning in the NISQ Era}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {976--981}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735244}, url = {https://doi.org/10.1145/3716368.3735244}, author = {Kundu, Satwik and Ghosh, Swaroop}, keywords = {Quantum machine learning, noisy intermediate-scale quantum, data poisoning attack, quantum cloud security, noise resilience.}, abstract = {With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a Quantum Indiscriminate Data Poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM_Brisbane’s noise), across various architectures and datasets, QUID achieves up to ( 92\% ) accuracy degradation in model performance compared to baseline models and up to ( 75\% ) accuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding ( 50\% ), demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.} }
@article{10.1145/3770865.3770866, title = {DIF-PP: Threshold Optimization Informed by IRT Models for Group Fairness in Machine Learning}, journal = {SIGAPP Appl. Comput. Rev.}, volume = {25}, pages = {5--20}, year = {2025}, issn = {1559-6915}, doi = {10.1145/3770865.3770866}, url = {https://doi.org/10.1145/3770865.3770866}, author = {Minatel, Diego and Parmezan, Antonio R. S. and Santos, Nicolas Roque dos and C\'uri, Mariana and de Andrade Lopes, Alneu}, keywords = {bias, classification, DIF, item response theory, post-processing}, abstract = {Machine learning models risk reproducing social biases, so algorithmic decision-making must incorporate principles that prevent discrimination. Post-processing methods, such as threshold optimization, can support this goal, but striking an appropriate trade-off between predictive performance and group equity metrics remains challenging. The recent application of Differential Item Functioning (DIF) in model selection sheds light on its potential as a promising yet unexplored approach to threshold tuning in more unbiased machine learning systems. Building on this premise, this work introduces DIF-PP, a fairness-aware post-processing method that draws on concepts from Item Response Theory (IRT) and DIF for optimizing decision boundaries. DIF-PP represents these thresholds as test items, derives classification characteristic curves through IRT, and uses DIF to identify the most impartial cutoff point. Experimental results with 18 datasets show that DIF-PP consistently outperforms existing methods when we analyze group fairness metrics and predictive performance simultaneously. By combining IRT and DIF, our proposal effectively mitigates discriminatory effects in binary classification, marking a significant advance toward the development of responsible artificial intelligence solutions.} }
@inproceedings{10.1145/3704137.3704160, title = {Dynamic Hover Gesture Classification using Photovoltaic Sensor and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {268--275}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704160}, url = {https://doi.org/10.1145/3704137.3704160}, author = {Almania, Nora Abdullah and Alhouli, Sarah Yousef and Sahoo, Deepak Ranjan}, keywords = {Self-Powered Photovoltaic Sensor, Time-series Data, Dynamic Hover Hand Gestures, Machine Learning, Data Augmentation, Random Forest}, abstract = {Self-powered photovoltaic sensor technology has been presented as a gestural interface that could recognise time-series data of dynamic hover gestures using machine learning (ML). To further expand and improve the classification system of dynamic hover hand gestures, we discovered time-series data from multiple subjects and explored the systems’ performance through ML algorithms. In this paper, we collected our own time-series dataset of 3,696 hand gesture samples from 48 multiple subjects using an off-the-shelf photovoltaic sensor and analysed them by applying different preprocessing techniques such as smoothing, augmentation, normalisation, and resampling. Then, we used different ML algorithms such as K-Nearest Neighbors (KNN), Gradient Boosting (GB), Logistic Regression (LR), Decision Tree (DT), and Random Forest (RF) to classify 11 dynamic hover hand gestures. Our findings indicated that RF achieved the highest accuracy among all other ML classifiers, with more than 97\% accuracy. The implications of our study could inform researchers of the potential of this technology for recognising accurate hand gestures trained with multiple subjects, which could help them establish gesture recognition systems for improved accuracy and usability.} }
@inproceedings{10.1145/3644116.3644218, title = {Application of machine learning in bipolar disorder}, booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science}, pages = {621--625}, year = {2024}, isbn = {9798400708138}, doi = {10.1145/3644116.3644218}, url = {https://doi.org/10.1145/3644116.3644218}, author = {Hong, Xin and Hu, Maorong}, abstract = {Purpose of review: Machine learning, a hot area of research today, has been attempted to be applied in various fields of clinical medicine. This paper reviews the current status of research on machine learning in the filed of bipolar disorder, including applications in the diagnosis, identification and the accurate treatment of bipolar disorder, and the prediction of suicidal behaviors in bipolar disorder. It also discusses the advantages and shortcomings and prospects of the applications of machine learning in bipolar disorder, in order to provide references for related researches and clinical treatment.Research methods: The literature on the application of machine learning in the field of bipolar disorder in recent years was searched through literature navigation. The aspects of prevention, diagnosis and treatment interventions in bipolar disorder were analyzed.Research conclusion: In recent years, researches on the clinical diagnosis and treatment of bipolar disorder by machine learning have been increasing. However, there are still some limitations and it still needs to be cautious in clinical application. As mental health practitioners, we should actively adapt to and promote the further development of machine learning in the field of bipolar disorder.} }
@inproceedings{10.1145/3688671.3688790, title = {Unsupervised machine learning techniques for energy consumption tariff design}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688790}, url = {https://doi.org/10.1145/3688671.3688790}, author = {Tsakiris, Georgios and Virtsionis-Gkalinikis, Nikolaos and Mischos, Stavros and Vrakas, Dimitrios}, keywords = {Time-Of-Use pricing, residential energy consumption, fuzzy clustering, unsupervised machine learning}, abstract = {Dealing with electricity demand fluctuations throughout peak and off-peak periods is challenging for electricity companies. During peak demand times, the grid should be able to match the high consumer needs. Conversely, minimal usage during off-peak periods leads to underutilization of generation capacity. This imbalance challenges utilities to ensure sufficient capacity and devise fair pricing models. The Time-of-Use (ToU) pricing model has emerged as a viable solution in many countries, encouraging consumers to shift their energy consumption from expensive peak hours to more affordable off-peak periods. To this end, this paper proposes unsupervised machine learning methods for designing ToU tariffs using only energy consumption time series data. Additionally, a new metric is introduced to evaluate the adaptability of the ToU methods to fluctuations in energy consumption. To validate the implemented techniques, public datasets from different countries were used.} }
@inproceedings{10.1145/3651890.3672243, title = {m3: Accurate Flow-Level Performance Estimation using Machine Learning}, booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference}, pages = {813--827}, year = {2024}, isbn = {9798400706141}, doi = {10.1145/3651890.3672243}, url = {https://doi.org/10.1145/3651890.3672243}, author = {Li, Chenning and Nasr-Esfahany, Arash and Zhao, Kevin and Noorbakhsh, Kimia and Goyal, Prateesh and Alizadeh, Mohammad and Anderson, Thomas E.}, keywords = {network simulation, data center networks, approximation, machine learning, network modeling, location = Sydney, NSW, Australia}, abstract = {Data center network operators often need accurate estimates of aggregate network performance. Unfortunately, existing methods for estimating aggregate network statistics are either inaccurate or too slow to be practical at the data center scale.In this paper, we develop and evaluate a scale-free, fast, and accurate model for estimating data center network tail latency performance for a given workload, topology, and network configuration. First, we show that path-level simulations---simulations of traffic that intersects a given path---produce almost the same aggregate statistics as full network-wide packet-level simulations. We use a simple and fast flow-level fluid simulation in a novel way to capture and summarize essential elements of the path workload, including the effect of cross-traffic on flows on that path. We use this coarse simulation as input to a machine-learning model to predict path-level behavior, and run it on a sample of paths to produce accurate network-wide estimates. Our model generalizes over the choice of congestion control (CC) protocol, CC protocol parameters, and routing. Relative to Parsimon, a state-of-the-art system for rapidly estimating aggregate network tail latency, our approach is significantly faster (5.7), more accurate (45.9\% less error), and more robust.} }
@inproceedings{10.1145/3724154.3724158, title = {Construction and Application of E-commerce Recommendation Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {21--26}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724158}, url = {https://doi.org/10.1145/3724154.3724158}, author = {Gao, Jing}, keywords = {e-commerce recommendation system, hybrid model, machine learning, personalized recommendation}, abstract = {With the increasingly fierce market competition, e-commerce enterprises are facing many challenges, such as improving product quality, optimizing price strategy and deeply understanding customer needs. Among them, personalized recommendation based on user's historical behavior data has become an important means for many e-commerce companies to enhance user experience, enhance user stickiness and promote sales growth. However, in the face of the increasing data scale, the current e-commerce recommendation model shows a significant lag in feature recognition, recommendation quality and operational efficiency, which is difficult to meet the application needs of the current e-commerce platform. In this regard, this paper will analyze the application status of e-commerce recommendation model, comprehensively explore the application feasibility of machine learning technology in the field of recommendation system, and propose a set of recommendation model construction scheme based on machine learning to realize personalized recommendation and improve the accuracy and user satisfaction of recommendation system. Practice has proved that, compared with the control model, the hybrid e-commerce recommendation model based on machine learning has outstanding performance in three groups of indicators: accuracy, recall and mAP value, which provides new ideas and methods for the function optimization of recommendation system under e-commerce platform.} }
@inproceedings{10.1145/3662158.3662802, title = {Brief Announcement: A Case for Byzantine Machine Learning}, booktitle = {Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing}, pages = {131--134}, year = {2024}, isbn = {9798400706684}, doi = {10.1145/3662158.3662802}, url = {https://doi.org/10.1145/3662158.3662802}, author = {Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael}, keywords = {federated learning, byzantine failure, data poisoning, location = Nantes, France}, abstract = {The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called workers). Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against data poisoning and faulty workers. The problem of Byzantine ML formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm. Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear. It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable. We highlight here some important results on the efficacy of Byzantine robustness for tackling data poisoning. In particular, we discuss cases where, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are optimal even under the weaker threat model of data poisoning.} }
@inproceedings{10.1145/3701716.3715522, title = {Multi-Component Coarsened Graph Learning for Scaling Graph Machine Learning}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {1001--1004}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715522}, url = {https://doi.org/10.1145/3701716.3715522}, author = {Halder, Subhanu and Kumar, Manoj and Kumar, Sandeep}, keywords = {graph coarsening, graph neural networks, graph representation learning, location = Sydney NSW, Australia}, abstract = {Graph coarsening is a reduction technique that approximates a larger graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications. However, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, tree, bipartite structure, or multi-component structure. This work presents an optimization framework for learning coarsened graphs with desirable multi-component structure. The proposed methods are solved efficiently by leveraging block majorization-minimization, log determinant, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework's efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently outperforms state-of-the-art methods.} }
@article{10.14778/3705829.3705861, title = {cedar: Optimized and Unified Machine Learning Input Data Pipelines}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {488--502}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3705829.3705861}, url = {https://doi.org/10.14778/3705829.3705861}, author = {Zhao, Mark and Adamiak, Emanuel and Kozyrakis, Christos}, abstract = {The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical due to skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources - or worse - underutilize expensive accelerators.To address these demands, we present cedar, an optimized and unified programming framework for ML input data pipelines. cedar allows users to define a training job's data pipeline using composable operators that support arbitrary ML frameworks and libraries. cedar's extensible optimizer systematically combines and applies performance optimizations to the pipeline. cedar then orchestrates pipeline processing across configurable local and distributed compute resources to efficiently meet the training job's data throughput demands. Across eight pipelines, cedar improves performance by up to 1.87 to 10.65 compared to state-of-the-art input data systems.} }
@inproceedings{10.1145/3696271.3696277, title = {A More Precise Credit Score Computation Method Based on Big Data and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {34--38}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696277}, url = {https://doi.org/10.1145/3696271.3696277}, author = {Zhang, Liang and Yu, Rujie and Lin, Yifeng and Yang, Yuer}, keywords = {Big Data, Credit Score Computation, Machine Learning}, abstract = {Artificial intelligence is replacing traditional credit scoring systems, processing extensive datasets including financial history, transaction records, credit bureau data, and even the social media behavior of borrowers to promote more accurate credit scores. While traditional methods aimed to promote fair and transparent credit accessibility, there are still some concerns. As more and more factors should be considered to compute the credit score, traditional methods lack the capability of handling big data. Thus, this paper aims to solve the limitations of traditional credit scoring. A solution of using machine learning and big data to improve credit scoring, practical implementations, and challenges is proposed. A linear equation to perform credit score computation is presented.} }
@inproceedings{10.1145/3638529.3654043, title = {Machine Learning-Enhanced Ant Colony Optimization for Column Generation}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {1073--1081}, year = {2024}, isbn = {9798400704949}, doi = {10.1145/3638529.3654043}, url = {https://doi.org/10.1145/3638529.3654043}, author = {Xu, Hongjie and Shen, Yunzhuang and Sun, Yuan and Li, Xiaodong}, keywords = {ant colony optimization, machine learning, column generation, combinatorial optimization, location = Melbourne, VIC, Australia}, abstract = {Column generation (CG) is a powerful technique for solving optimization problems that involve a large number of variables or columns. This technique begins by solving a smaller problem with a subset of columns and gradually generates additional columns as needed. However, the generation of columns often requires solving difficult subproblems repeatedly, which can be a bottleneck for CG. To address this challenge, we propose a novel method called machine learning enhanced ant colony optimization (MLACO), to efficiently generate multiple high-quality columns from a sub-problem. Specifically, we train a ML model to predict the optimal solution of a subproblem, and then integrate this ML prediction into the probabilistic model of ACO to sample multiple high-quality columns. Our experimental results on the bin packing problem with conflicts show that the MLACO method significantly improves the performance of CG compared to several state-of-the-art methods. Furthermore, when our method is incorporated into a Branch-and-Price method, it leads to a significant reduction in solution time.} }
@inproceedings{10.1145/3681756.3697880, title = {A Multimodal LLM-based Assistant for User-Centric Interactive Machine Learning}, booktitle = {SIGGRAPH Asia 2024 Posters}, year = {2024}, isbn = {9798400711381}, doi = {10.1145/3681756.3697880}, url = {https://doi.org/10.1145/3681756.3697880}, author = {Kawabe, Wataru and Sugano, Yusuke}, keywords = {graphical user interface, machine learning, large language model}, abstract = {This paper proposes a system based on a multimodal large language model (MLLM) to assist non-expert users without prior experience in machine learning (ML) development. The MLLM assistant in our system interactively helps users compile their requirements and create appropriate training data while building an ML model. It has been reported that users often struggle to define training data that comprehensively covers all samples or aligns with their needs. To prevent such failures, the MLLM assistant monitors the user’s interaction process and translates users’ vague needs into concrete ML formulations through chat, ultimately facilitating the creation of appropriate training data.} }
@inproceedings{10.1145/3703847.3703865, title = {Systematic Evaluation of Machine Learning-Based Predictive Model for Diabetes}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {99--103}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703865}, url = {https://doi.org/10.1145/3703847.3703865}, author = {Zhang, Yiyuan}, keywords = {Diabetes, Machine Learning, Prediction Model}, abstract = {The paper aims to systematically evaluate the prediction model for diabetes based on machine learning (ML). It conducted literature research in Baidu Scholar, National Library of China, Web of Weipu, CNKI, and Wanfang databases for literature on prediction models for diabetes constructed by ML, and the search period spanned from January 2018 to February 2023. The paper completed literature screening and data extraction independently and used predictive models to construct a research data extraction and quality evaluation checklist (CHARMS) to evaluate the quality of the included literature and screened high-quality literature for discussion. Results: A total of 13 high-quality studies were collected, including 5 ML models, with an area under the ROC curve ranging from 0.720 to 0.97. Laboratory indicators such as age, BMI, blood sugar concentration, waistline, and diabetes history are the main predictive factors. Conclusions: Diabetes prediction models constructed using ML can accurately identify the risk of diabetes, and their predictive performance is superior to traditional risk prediction models. The available literature on the topic exhibits a low overall risk of bias, however, the applicability level of the prediction model is considered average.} }
@inproceedings{10.1145/3711403.3711480, title = {Research on Blended teaching of Artificial Intelligence and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management}, pages = {474--479}, year = {2025}, isbn = {9798400717468}, doi = {10.1145/3711403.3711480}, url = {https://doi.org/10.1145/3711403.3711480}, author = {Guo, Yunying and Li, Xiaofei and Zhang, Tianyu}, keywords = {OBE, achievement, blended teaching, expected learning effect}, abstract = {Based on the results-oriented OBE education concept, in order to cultivate application oriented innovative talents, artificial intelligence and machine learning courses adopt the blended teaching mode. The online learning platform is used to combine offline classroom and online independent learning, and the practical process of blended teaching is described from before, during and after class. Through the scientific setting of KT point and the expected learning effect mapping relationship to assess the level of achievement, we evaluate the impact of blended teaching reform and reflect on our teaching practices. Blended teaching in artificial intelligence courses presents the best scenario for learning, as it combines traditional and online teaching to enhance the understanding and engagement. It offers more personal and flexible learning, more interactions, and practical opportunities for the students to learn.} }
@inproceedings{10.1145/3718751.3718865, title = {Empirical Study on Optimizing Regional Economic Development Strategy Using Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {709--715}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718865}, url = {https://doi.org/10.1145/3718751.3718865}, author = {Yin, Yong and Zhang, Dongyu and Wen, Feiren and Huang, He and Xu, Yueran}, keywords = {Machine Learning, Regional Economic Development, Strategy Optimization}, abstract = {In the contemporary landscape of economic development, the integration of machine learning algorithms presents a promising avenue for refining regional economic strategies. This study embarks on an empirical investigation to explore the efficacy of machine learning algorithms in optimizing regional economic development strategies. By leveraging diverse datasets encompassing economic indicators, demographic information, and regional characteristics, this research employs a range of machine learning techniques including clustering, regression, and predictive modeling. Through rigorous analysis and evaluation, the study seeks to discern patterns, identify key determinants, and formulate actionable insights to enhance regional economic development strategies. The findings of this study contribute to a deeper understanding of the potential of machine learning in informing evidence-based policy decisions and fostering sustainable economic growth at the regional level.} }
@inproceedings{10.1145/3745238.3745505, title = {Enhancing E-commerce Logistics Insights through Machine Learning: A Real-Data Approach}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1709--1716}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745505}, url = {https://doi.org/10.1145/3745238.3745505}, author = {Wang, Xiangcheng}, keywords = {Big data, Data analysis, Distribution efficiency}, abstract = {This study employs real-world data from e-commerce logistics operations to conduct a comprehensive regression analysis on the benefits of e-commerce logistics. Utilizing linear regression and machine learning modeling techniques, the research identifies relevant factors influencing logistics performance. Furthermore, multiple machine learning models are developed to model and predict the risk of delayed order delivery. To enhance the interpretability of the prediction models, feature importance analysis and SHAP-based interpretability analysis are conducted. These analyses elucidate the impact of key factors such as transportation methods and order scheduling on the risk of delayed delivery, thereby offering a novel perspective for the analysis and research of e-commerce logistics.} }
@inbook{10.1145/3696630.3728600, title = {MANILA: A Low-Code Application to Benchmark Machine Learning Models and Fairness-Enhancing Methods}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {1153--1157}, year = {2025}, isbn = {9798400712760}, url = {https://doi.org/10.1145/3696630.3728600}, author = {d'Aloisio, Giordano}, abstract = {This paper presents MANILA, a web-based low-code application to benchmark machine learning models and fairness-enhancing methods and select the one achieving the best fairness and effectiveness trade-off. It is grounded on an Extended Feature Model that models a general fairness benchmarking workflow as a Software Product Line. The constraints defined among the features guide users in creating experiments that do not lead to execution errors. We describe the architecture and implementation of MANILA and evaluate it in terms of expressiveness and correctness.} }
@inproceedings{10.1145/3660853.3660879, title = {Advancing Cyber Threat Intelligence through Machine Learning Algorithms}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {111--115}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660879}, url = {https://doi.org/10.1145/3660853.3660879}, author = {Varbanov, Velizar}, keywords = {Cyber Threat Intelligence, Dark Web, Text Classification, location = undefinedstanbul, Turkiye}, abstract = {In the domain of Cyber Threat Intelligence (CTI) the enigmatic depths of the Dark Web are pivotal for the early identification of nascent cyber threats. Yet, the voluminous and sprawling data across hacker forums and illicit marketplaces pose a significant challenge, swamping analysts with a deluge of extraneous information. This paper advocates for the utilization of advanced Machine Learning (ML) algorithms as a strategic tool to distill and classify emergent threats from the Dark Web's vast data landscape. Through rigorous experiments employing a suite of ML models—Logistic Regression, Decision Tree Classification, Gradient Boosting Classifier, and Random Forest Classifier—we evaluated their performance across two meticulously curated datasets. One dataset was rich with data pertinent to cybersecurity, extracted from hacker forums and Dark Web marketplaces, while the other served as a control set. Our results illuminate the profound capability of ML algorithms to effectively navigate and filter through the data quagmire, highlighting threats with precision and thereby significantly optimizing the workflow of analysts. This automation paves the way for a more streamlined, focused approach to CTI, ensuring that cybersecurity operations remain agile and informed in the face of the dynamic and increasingly sophisticated landscape of cyber threats. Through this study, we underscore the transformative potential of ML in revolutionizing CTI methodologies, offering a beacon of efficiency and effectiveness in the ongoing battle against cybercrime.} }
@article{10.1145/3696352, title = {Machine Learning for Actionable Warning Identification: A Comprehensive Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3696352}, url = {https://doi.org/10.1145/3696352}, author = {Ge, Xiuting and Fang, Chunrong and Li, Xuanye and Sun, Weisong and Wu, Daoyuan and Zhai, Juan and Lin, Shang-Wei and Zhao, Zhihong and Liu, Yang and Chen, Zhenyu}, keywords = {Static analysis warnings, actionable warning identification, literature review}, abstract = {Actionable Warning Identification (AWI) plays a crucial role in improving the usability of static code analyzers. With recent advances in Machine Learning (ML), various approaches have been proposed to incorporate ML techniques into AWI. These ML-based AWI approaches, benefiting from ML’s strong ability to learn subtle and previously unseen patterns from historical data, have demonstrated superior performance. However, a comprehensive overview of these approaches is missing, which could hinder researchers and practitioners from understanding the current process and discovering potential for future improvement in the ML-based AWI community. In this article, we systematically review the state-of-the-art ML-based AWI approaches. First, we employ a meticulous survey methodology and gather 51 primary studies from January 1, 2000 to January 9, 2023. Then, we outline a typical ML-based AWI workflow, including warning dataset preparation, preprocessing, AWI model construction, and evaluation stages. In such a workflow, we categorize ML-based AWI approaches based on the warning output format. In addition, we analyze the key techniques used in each stage, along with their strengths, weaknesses, and distribution. Finally, we provide practical research directions for future ML-based AWI approaches, focusing on aspects such as data improvement (e.g., enhancing the warning labeling strategy) and model exploration (e.g., exploring large language models for AWI).} }
@inproceedings{10.1145/3719384.3719387, title = {Enhancing Process Yield Though Quality Prediction Using Machine Learning Techniques}, booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference}, pages = {19--25}, year = {2025}, isbn = {9798400717925}, doi = {10.1145/3719384.3719387}, url = {https://doi.org/10.1145/3719384.3719387}, author = {Wongngern, Nattapon and Phruksaphanrat, Busaba}, keywords = {Decision Analysis, Process improvement, Production Yield, Quality Control}, abstract = {This research investigates the application of Machine learning (ML) techniques to enhance yield of optical modules in manufacturing process of fiber optics. The study focuses on the quality process, which consists of four stages: FCAL, OPM, OPMT, and EXS. The objective is to improve process yield by identifying and removing parts likely to fail testing at an early stage to save time and resources. Initially, data collection and preprocessing were performed. Then, statistical hypothesis testing of mean and variance were utilized for feature selection. After that, various ML techniques were employed to classify parts as either “fail” or “pass,” including Random Forest, XGBoost, Gradient Boosting, Neural Networks, Logistic Regression, Decision Tree, Na\"ve Bayes, and Nearest Neighbors. The research found that Random Forest, XGBoost, and Gradient Boosting models outperformed other models in predicting the quality of part across different stages. Among these, Random Forest and XGBoost were the most effective in improving the cumulative yield of the overall processes. Implementing these models allowed for the early removal of defective parts, resulting in a significant increased in cumulative yield to approximately 97\%.} }
@article{10.1145/3589506, title = {A Survey on Machine Learning in Hardware Security}, journal = {J. Emerg. Technol. Comput. Syst.}, volume = {19}, year = {2023}, issn = {1550-4832}, doi = {10.1145/3589506}, url = {https://doi.org/10.1145/3589506}, author = {K\"oyl\"u, Troya Cagl and Wedig Reinbrecht, Cezar Rodolfo and Gebregiorgis, Anteneh and Hamdioui, Said and Taouil, Mottaqiallah}, keywords = {cybersecurity, neural networks, hardware countermeasures, hardware attacks, machine learning, Hardware security}, abstract = {Hardware security is currently a very influential domain, where each year countless works are published concerning attacks against hardware and countermeasures. A significant number of them use machine learning, which is proven to be very effective in other domains. This survey, as one of the early attempts, presents the usage of machine learning in hardware security in a full and organized manner. Our contributions include classification and introduction to the relevant fields of machine learning, a comprehensive and critical overview of machine learning usage in hardware security, and an investigation of the hardware attacks against machine learning (neural network) implementations.} }
@inproceedings{10.1145/3658271.3658330, title = {Machine Learning Applied To Fall Detection in the Elderly}, booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems}, year = {2024}, isbn = {9798400709968}, doi = {10.1145/3658271.3658330}, url = {https://doi.org/10.1145/3658271.3658330}, author = {Oliveira, Camila Pereira de and Colombo, Cristiano da Silveira and Nunes, Daniel Jos\'e Ventorim}, keywords = {Elderly, Fall detection, IOT, Machine learning, location = Juiz de Fora, Brazil}, abstract = {Context: With the increase in falls among the elderly, rapid detection becomes essential to reduce fatal risks. This challenge stimulates advancements in machine learning algorithms and IoT technologies in medicine, aiming to improve the safety and longevity of the elderly. Problem: An increase in falls among the elderly is a domestic severe health issue. Solution: Development of an efficient machine learning algorithm using accelerometer data from wearable devices to detect falls in the elderly. SI Theory: The study employs General Systems Theory, not to develop new hardware devices but to integrate and analyze data from accelerometers that already exist in wearable devices. It focuses on the synergy between this hardware data and software algorithms and their interaction with human behavior to enhance the detection of falls in the elderly. Method: This work uses the applied methodology to evaluate KNN, Decision Tree, and MLP algorithms applied to accelerometer data, focusing on accuracy and efficacy. Summary of Results: The MLP model stood out with high efficacy in fall detection, achieving a recall of 97.92\% during the testing and 100\% during the validation phases. This indicates the model’s strong ability to identify falls correctly, a crucial factor for the safety of the elderly. Contributions and Impact in the IS area: Presents an efficient solution for the health of the elderly, with the potential to reduce accidents and improve quality of life. Highlights the importance of validation in natural environments and with diverse individuals for future research.} }
@inproceedings{10.1145/3637732.3637744, title = {Breast Cancer Detection with Topological Machine Learning}, booktitle = {Proceedings of the 2023 10th International Conference on Biomedical and Bioinformatics Engineering}, pages = {217--222}, year = {2024}, isbn = {9798400708343}, doi = {10.1145/3637732.3637744}, url = {https://doi.org/10.1145/3637732.3637744}, author = {Yadav, Ankur and Nisha, Fnu and Coskunuzer, Baris}, keywords = {Breast Cancer Diagnosis, Cubical Persistence, Machine Learning, Mammogram, Topological Data Analysis, Ultrasound, location = Kyoto, Japan}, abstract = {Screening for breast cancer using mammograms and ultrasound images is an essential but time-consuming and expensive process that requires a trained clinician’s interpretation. To address this issue, machine learning (ML) methods have been developed in recent years as clinical decision-support tools. However, most of these algorithms face challenges related to computational feasibility, reliability, and interpretability. We present a new approach for feature extraction in mammograms and ultrasound images using topological data analysis (TDA) methods. The proposed method uses persistent homology to capture distinct topological patterns in healthy and unhealthy patient images, which are then transformed into powerful feature vectors. These vectors are combined with standard ML techniques to create the Topo-BRCA model, which provides competitive results with state-of-the-art deep learning (DL) models in several benchmark datasets. Unlike most DL models, Topo-BRCA does not require data augmentation or preprocessing and is effective for both small and large datasets. Additionally, the topological feature vectors can easily be integrated into future DL models to enhance their performance further.} }
@inproceedings{10.1145/3748825.3748935, title = {Multi-machine learning algorithms for the co-selection of biomarkers in hepatocellular carcinoma}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {709--715}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748935}, url = {https://doi.org/10.1145/3748825.3748935}, author = {Chen, Junjie and Li, Shuaicheng and Yang, Cheng and Hu, Huanjun}, keywords = {Drug prediction, Feature gene, Liver cancer, Machine learning}, abstract = {Hepatocellular carcinoma is the most common type of primary liver cancer, typically occurring on the basis of pre-existing severe liver diseases, such as hepatitis and cirrhosis. Given its status as a leading cause of mortality worldwide, identifying biomarkers and therapeutic targets associated with hepatocellular carcinoma is of utmost importance. In this study, we performed differential expression analysis on gene expression data from hepatocellular carcinoma patients and utilized a protein-protein interaction network to identify key genes related to the disease. Subsequently, we applied various machine learning algorithms to screen for feature genes, discovering that CYP2B6 and TOP2A are particularly crucial. We then validated our results and found that CYP2B6 and TOP2A are associated with increased mortality rates in hepatocellular carcinoma patients. Finally, we searched for potential drug compounds using the DsigDB database.} }
@proceedings{10.1145/3649403, title = {WiseML '24: Proceedings of the 2024 ACM Workshop on Wireless Security and Machine Learning}, year = {2024}, isbn = {9798400706028}, abstract = {It is our great pleasure to welcome you to the ACM Workshop on Wireless Security and Machine Learning (WiseML) 2024. This year's workshop continues its tradition of being a premier forum to bring together members of the ML, privacy, security, wireless communications, and networking communities from around the world. It provides a platform to share the latest research findings in these emerging and critical areas, fostering the exchange of ideas and promoting research collaborations to advance the state-of-the-art. This year's event will take place in Seoul, South Korea, and the program will feature a single track.} }
@article{10.5555/3648699.3648841, title = {On tilted losses in machine learning: theory and applications}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia}, keywords = {exponential tilting, empirical risk minimization, value-at-risk, superquantile optimization, fairness, robustness}, abstract = {Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM--tilted empirical risk minimization (TERM)--which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust optimization (DRO). We develop batch and stochastic first-order optimization methods for solving TERM, provide convergence guarantees for the solvers, and show that the framework can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications in machine learning, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. Despite the straightforward modification TERM makes to traditional ERM objectives, we find that the framework can consistently outperform ERM and deliver competitive performance with state-of-the-art, problem-specific approaches.} }
@inproceedings{10.1145/3605423.3605450, title = {Machine Learning on Insurance Premium Prediction}, booktitle = {Proceedings of the 2023 9th International Conference on Computer Technology Applications}, pages = {19--24}, year = {2023}, isbn = {9781450399579}, doi = {10.1145/3605423.3605450}, url = {https://doi.org/10.1145/3605423.3605450}, author = {Jesus, Rodrigo M. and Brito, Miguel A. and Duarte, Duarte N.}, keywords = {Insurance Premium Prediction, MLOPs, Machine Learning, location = Vienna, Austria}, abstract = {The insurance field is going through a phase of great transformation due to the growth of new technologies and techniques that are causing a change in the way data is handled and analyzed. The main perpetrator of this phenomenon is the introduction of Machine Learning (ML) in financial decision-making due to their efficiency and productivity. However, there is a new intervenient in the room, which will automate and support all steps of ML system development. Machine Learning Operations (MLOPs) will reduce technical friction, so that the model may move from an idea into production, in the shortest amount of time, and subsequently to market with the least possible risk. In this paper, a detailed review of the impacts of ML on insurance premium forecasting and the influence that MLOPs can have on forecasting outcomes is provided. Furthermore, a comprehensive summary is presented of crucial principles in the insurance industry, which are essential for comprehending the role that MLOPs will play in tailoring and individualizing insurance policies and premiums.} }
@inproceedings{10.1145/3626246.3654686, title = {Machine Learning for Databases: Foundations, Paradigms, and Open problems}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {622--629}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654686}, url = {https://doi.org/10.1145/3626246.3654686}, author = {Cong, Gao and Yang, Jingyi and Zhao, Yue}, keywords = {learned index, learned query optimization, machine learning for databases, location = Santiago AA, Chile}, abstract = {This tutorial delves into the burgeoning field of Machine Learning for Databases (ML4DB), highlighting its recent progress and the challenges impeding its integration into industrial-grade database management systems. We systematically explore three key themes: the exploration of foundations in ML4DB and their potential for diverse applications, the two paradigms in ML4DB, i.e., using machine learning as a replacement versus enhancement of traditional database components, and the critical open challenges such as improving model efficiency and addressing data shifts. Through an in-depth analysis, including a survey of recent works in major database conferences, this tutorial encapsulates the current state of ML4DB, as well as charts a roadmap for its future development and wider adoption in practical database environments.} }
@inproceedings{10.1145/3620666.3651367, title = {ProxiML: Building Machine Learning Classifiers for Photonic Quantum Computing}, booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages = {834--849}, year = {2024}, isbn = {9798400703867}, doi = {10.1145/3620666.3651367}, url = {https://doi.org/10.1145/3620666.3651367}, author = {Ranjan, Aditya and Patel, Tirthak and Silver, Daniel and Gandhi, Harshitta and Tiwari, Devesh}, abstract = {Quantum machine learning has shown early promise and potential for productivity improvements for machine learning classification tasks, but has not been systematically explored on photonics quantum computing platforms. Therefore, this paper presents the design and implementation of ProxiML - a novel quantum machine learning classifier for photonic quantum computing devices with multiple noise-aware design elements for effective model training and inference. Our extensive evaluation on a photonic device (Xanadu's X8 machine) demonstrates the effectiveness of ProxiML machine learning classifier (over 90\% accuracy on a real machine for challenging four-class classification tasks), and competitive classification accuracy compared to prior reported machine learning classifier accuracy on other quantum platforms - revealing the previously unexplored potential of Xanadu's X8 machine.} }
@inproceedings{10.1145/3747897.3747907, title = {Machine Learning-Based Approach for Android Spyware Detection An Analysis Using Logistic Regression}, booktitle = {Proceedings of the Sixth International Conference on Digital Age \&amp; Technological Advances for Sustainable Development}, pages = {55--59}, year = {2025}, isbn = {9798400713590}, doi = {10.1145/3747897.3747907}, url = {https://doi.org/10.1145/3747897.3747907}, author = {Fenjan, Ali and Md Desa, Dr. Jalil and Elaskari, Dr.Salah and Alsayafi, Eng. Athraa Saleh}, keywords = {Android spyware, Machine learning, Logistic Regression, Malware detection, Cybersecurity.}, abstract = {Android phones are fast becoming the targets of spyware, and the security and privacy consequences are severe. Signature-based methods are unable to deal with the malware dynamics, and therefore, machine learning (ML) techniques are employed. Logistic Regression is used in the identification of Android spyware in this paper with the help of static and dynamic analysis to improve classification accuracy. The model achieved 99.33\% classification accuracy with a precision of 97.50\% and recall of 98.62\%, effectively detecting spyware from ordinary programs. Results of the confusion matrix indicate minimal false positives and zero misclassification of genuine programs. Statistical measures like the Chi-Square and T-Test validate the model’s predictions and confirm its appropriateness. The study also explores issues such as adversarial evasion, computational efficiency, and privacy, suggesting optimizations for practical applications. Overall, the findings support ML-based Android spyware detection and offer scalable security solutions for the Android ecosystem.} }
@inproceedings{10.1145/3613905.3651116, title = {Machine Learning Insights Into Eating Disorder Twitter Communities}, booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703317}, doi = {10.1145/3613905.3651116}, url = {https://doi.org/10.1145/3613905.3651116}, author = {Kao, Hsien-Te and Erickson, Isabel and Chu, Minh Duc Hoang and He, Zihao and Lerman, Kristina and Volkova, Svitlana}, keywords = {Discussion Analysis, Eating Disorders, Social Media, location = Honolulu, HI, USA}, abstract = {Eating disorders have serious impacts on young population physical, psychological, and social functioning. Health agencies are calling for new psycho-therapeutic intervention considerations that take into account online communities, which is not possible if therapists know little about eating disorders discussions online. In this paper, we leverage machine learning analytics to understand what the eating disorder communities are talking about over time and how they are talking about it. By analyzing local and global community discussions, we discovered complex group dynamics underpinning collective identities offering emotional support but also potentially perpetuating harmful behaviors. Our analysis of four local subcommunities and four global theme evolutions revealed prevalent subjects, perspectives, motivations, and linguistic patterns. We found tight-knit communities grounded in shared membership, goals, cultures, and practices. Community voices highlighting recovery journeys were limited. Our computational assessment of invisible online spaces aims to inform personalized interventions accounting for community forces in youth mental health.} }
@inproceedings{10.1145/3664476.3670867, title = {RMF: A Risk Measurement Framework for Machine Learning Models}, booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security}, year = {2024}, isbn = {9798400717185}, doi = {10.1145/3664476.3670867}, url = {https://doi.org/10.1145/3664476.3670867}, author = {Schr\"oder, Jan and Breier, Jakub}, keywords = {Adversarial Machine Learning, Backdoor Attacks, ISO/IEC 27004:2016, Machine Learning Security, Risk Measurement, location = Vienna, Austria}, abstract = {Machine learning (ML) models are used in many safety- and security-critical applications nowadays. It is therefore important to measure the security of a system that uses ML as a component. This paper focuses on the field of ML, particularly the security of autonomous vehicles. For this purpose, a technical framework will be described, implemented, and evaluated in a case study. Based on ISO/IEC 27004:2016, risk indicators are utilized to measure and evaluate the extent of damage and the effort required by an attacker. It is not possible, however, to determine a single risk value that represents the attacker’s effort. Therefore, four different values must be interpreted individually.} }
@inproceedings{10.1145/3675249.3675337, title = {Quantitative Investment Based on Fundamental Analysis Using Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology}, pages = {509--515}, year = {2024}, isbn = {9798400718267}, doi = {10.1145/3675249.3675337}, url = {https://doi.org/10.1145/3675249.3675337}, author = {Zhang, Zhiruo}, abstract = {Forecasting financial markets has always been a focus of investors and researchers. This study aims to build a reliable financial market prediction model through factor selection and machine learning model construction. Traditional technical and fundamental analysis are limited in their predictive results due to subjective judgment and information asymmetry. Therefore, more people are turning to intelligent quantitative investment to enhance prediction accuracy and stability using artificial intelligence technology. Fundamental quantitative investment combines quantitative and value investment methods, utilizing the relationship between stock fundamentals and excess returns to achieve modern value investing. This paper employs supervised machine learning methods to analyze the relationship between stock factors and returns, choosing regression methods to solve prediction problems. The paper introduces the application of factor selection and machine learning models in financial market forecasting. Factor selection involves selecting key factors related to stock returns from a large amount of fundamental data. Machine learning models, including traditional linear regression, ridge regression, and Lasso regression, utilize patterns and rules in historical data for prediction. By utilizing the sliding window method to divide the dataset and retain its time series features, a reliable financial market prediction model is established by selecting appropriate sliding window lengths and machine learning models. Empirical results and analysis show that the selected model performs well in metrics such as annualized returns, maximum drawdown rate, and annualized volatility, demonstrating certain advantages over the Shanghai and Shenzhen 300 Index.} }
@article{10.1145/3688077, title = {Exploring the Frontiers of Machine Learning in Education}, journal = {XRDS}, volume = {31}, pages = {5}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3688077}, url = {https://doi.org/10.1145/3688077}, author = {Li, Jiayi} }
@article{10.1145/3637826, title = {A Machine Learning–Based Readability Model for Gujarati Texts}, journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.}, volume = {23}, year = {2024}, issn = {2375-4699}, doi = {10.1145/3637826}, url = {https://doi.org/10.1145/3637826}, author = {Bhogayata, Chandrakant K.}, keywords = {Readability model, readability rating and level of education, interrater agreement, model comparison, Gujarati texts}, abstract = {This study aims to develop a machine learning–based model to predict the readability of Gujarati texts. The dataset was 50 prose passages from Gujarati literature. Fourteen lexical and syntactic readability text features were extracted from the dataset using a machine learning algorithm of the unigram parts of speech tagger and three Python programming scripts. Two samples of native Gujarati speaking secondary and higher education students rated the Gujarati texts for readability judgment on a 10-point scale of “easy” to “difficult” with the interrater agreement. After dimensionality reduction, seven text features as the independent variables and the mean readability rating as the dependent variable were used to train the readability model. As the students' level of education and gender were related to their readability rating, four readability models for school students, university students, male students, and female students were trained with a backward stepwise multiple linear regression algorithm of supervised machine learning. The trained model is comparable across the raters’ groups. The best model is the university students’ readability rating model. The model is cross-validated. It explains 91\% and 88\% of the variance in readability ratings at training and cross-validation, respectively, and its effect size and power are large and high.} }
@inproceedings{10.1109/SCW63240.2024.00166, title = {Applying a Task-Based Approach to Distributed Machine Learning Workflows}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {1252--1261}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00166}, url = {https://doi.org/10.1109/SCW63240.2024.00166}, author = {V\'azquez-Novoa, Fernando and Lezzi, Daniele and Lordan, Francesc and Baghdadi, Fatemeh and Cirillo, Davide}, abstract = {The growing demands across various scientific fields have led to a significant shift in applications that consume data at the edge of the computing continuum. These applications require unified programming models for the composition of components and coordinating the execution of computational workloads, including training machine learning (ML) models on distributed resources. Personalized healthcare often leverages data generated from wearable devices used to train ML models, can be benefited from distributed computing approaches. Specifically, stroke care can be greatly benefited from distributed ML with modifiable risk factors that can be monitored using wearable devices. In this work, we present an implementation that leverages distributed techniques for large-scale ML workflows using electrocardiogram (ECG) recordings for atrial fibrillation (AF) classification. The application was evaluated using the PhysioNet database, showcasing the potential of distributed, ML in stroke care, opening the way for future creation of more advanced models embedded in edge devices.} }
@inproceedings{10.1145/3588155.3588181, title = {NFT Appraisal Using Machine Learning}, booktitle = {Proceedings of the 2023 5th Asia Pacific Information Technology Conference}, pages = {160--166}, year = {2023}, isbn = {9781450399500}, doi = {10.1145/3588155.3588181}, url = {https://doi.org/10.1145/3588155.3588181}, author = {Dawod, Ahmed Dawod Mohammed and Munkhdalai, Lkhagvadorj and Park, Kwang Ho and Ryu, Keun Ho and Pham, Van Huy}, keywords = {Blockchain, CatBoost, ElasticNet, Lasso, LightGBM, Linear and Polynomial Regression, Non-Fungible Tokens, Random Forest, Ridge, SVM, TabNet, XGBoost, location = Ho Chi Minh City, Vietnam}, abstract = {Non-Fungible Tokens (NFTs) are digital assets based on a blockchain and those are characterized as unique cryptographic tokens and non-interchangeable. To date, research into the NFT marketplace has been relatively limited. As it is an emerging platform with many unique elements, The NFT market has been impacted due to recent fluctuations in crypto-asset markets more broadly. This current bear market cycle has shed light on concerns around the value of NFTs, profit-based motivation, and environmental sustainability. However, periods of volatility and cyclicality are to be expected with any nascent technology as it develops a product-market fit. consequently, the appraisal of real-price for NFT collections is essential for individual financial security and investment making. In this study, we evaluate the machine learning algorithms to appraise their real-price based on NFT item's characteristics, market event information, and their rarity score data acquired by retrieved from the biggest marketplace OpenSea. Furthermore, the procedures were applied to meet the objectives of this study we built prediction models based on various machine-learning algorithms ranging from Random Forest, XGBoost, SVM, Lasso, ElasticNet, Ridge, Linear Polynomial Regression, TabNet, CatBoost, and LightGBM models. From the results, LightGBM regression model outperformed the other by RMSE around 0.905. The best R2 is only found in this model, which has a value of 0.917.} }
@inproceedings{10.1145/3638584.3638678, title = {Review on Research of Automated Machine Learning}, booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence}, pages = {526--532}, year = {2024}, isbn = {9798400708688}, doi = {10.1145/3638584.3638678}, url = {https://doi.org/10.1145/3638584.3638678}, author = {Zhong, Yuyanzhen and Yang, Chuan and Su, Xinyi and Li, Biao and Huang, Xiaoping and Shuai, Yong}, keywords = {Automated Machine Learning, Automatic Processing, Lifelong Learning, Neural Architecture Search, location = Beijing, China}, abstract = {Automated Machine Learning (AutoML) can automatically discover high-performance models to build deep learning systems without human assistance, with the ultimate goal of reducing the complexity and entry barriers of building deep learning systems. Although AutoML has achieved a certain degree of automation through four important steps: data preparation, feature engineering, model generation, and model evaluation, there is still a significant gap compared to the ultimate ideal of achieving truly intelligent lifelong learning. Therefore, a deep understanding of AutoML can help drive the development of artificial intelligence. Firstly, we comprehensively reviewed the latest technologies and achievements involved in these four steps, then we introduced their shortcomings and challenges. Secnodly, a detailed introduction was given to the existing AutoML libraries and the theoretical and practical applications of AutoML. Finally, we summarized AutoML models and Proposed an outlook.} }
@article{10.1145/3734866, title = {FuMi: A Runtime Fuzz-based Machine Learning Precision Measurement and Testing Framework}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3734866}, url = {https://doi.org/10.1145/3734866}, author = {Zhang, Peng and Papadakis, Mike and Zhou, Yuming}, keywords = {Deep learning, fuzzing testing, software measurement}, abstract = {The rapid evolution of machine learning model training has outpaced the development of corresponding measurement and testing tools, leading to two significant challenges. Firstly, developers of deep learning frameworks struggle to identify operators that fail to meet precision criteria, as these issues may only manifest in a few data points. Secondly, model trainers lack effective methods to estimate precision loss caused by operators during training. To address these issues, we introduce a Pythonic framework inspired by common network layer definitions in deep learning. Our framework includes two new layers: the Fuzz Layer and the Check Layer, designed to aid in measurement and testing. The Fuzz Layer introduces minor perturbations to tensor inputs for any deterministic layer under testing (LUT). The Check Layer then measures precision by analyzing the differences before and after the perturbation. This approach estimates a lower bound of the maximal relative error and alerts developers or trainers of potential bugs if the difference exceeds a predefined tolerance. Additionally, Check Layers can be used independently to conduct precision tests for specific layers, ensuring the precision of operators during runtime. Despite the additional memory and time requirements, this runtime testing ensures proper training of the original model. We demonstrate the utility of our framework, FuMi, through two experiments. First, we tested 21 torch operators across 9 popular machine learning models using PyTorch for various tasks, finding that the conv2d and linear operators often fail to meet precision requirements. Second, to showcase the generalizability of our framework, we tested the ATTENTION operator. By comparing different implementations of state-of-the-art ATTENTION operators, we found that the maximum relative error of the ATTENTION operator is not less than 1\%, which is 13 times larger than that measured by Predoo (a unit test tool). This framework provides a robust solution for identifying precision issues in deep learning models, ensuring reliable and accurate model training.} }
@inproceedings{10.1145/3617023.3617043, title = {Unfairness in Machine Learning for Web Systems Applications}, booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web}, pages = {144--153}, year = {2023}, isbn = {9798400709081}, doi = {10.1145/3617023.3617043}, url = {https://doi.org/10.1145/3617023.3617043}, author = {Minatel, Diego and dos Santos, N\'colas Roque and da Silva, Angelo Cesar Mendes and C\'uri, Mariana and Marcacini, Ricardo Marcondes and Lopes, Alneu de Andrade}, keywords = {Web Systems, Unfairness Examples, Machine Learning, Data Bias, location = Ribeir\~ao Preto, Brazil}, abstract = {Machine learning models are increasingly present in our society; many of these models integrate Web Systems and are directly related to the content we consume daily. Nonetheless, on several occasions, these models have been responsible for decisions that spread prejudices or even decisions, if committed by humans, that would be punishable. After several cases of this nature came to light, research and discussion topics such as Fairness in Machine Learning and Artificial Intelligence Ethics gained a boost of importance and urgency in our society. Thus, one way to make Web Systems fairer in the future is to show how they can currently be unfair. In order to support discussions and be a reference for unfairness cases in machine learning decisions, this work aims to organize in a single document known decision-making that was wholly or partially supported by machine learning models that propagated prejudices, stereotypes, and inequalities in Web Systems. We conceptualize relevant categories of unfairness (such as Web Search and Deep Fake), and when possible, we present the solution adopted by those involved. Furthermore, we discuss approaches to mitigate or prevent discriminatory effects in Web Systems decision-making based on machine learning.} }
@inproceedings{10.1145/3712255.3716531, title = {Automated Machine Learning Tools for Data Science, Modeling, and Algorithm Benchmarking}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1368--1395}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716531}, url = {https://doi.org/10.1145/3712255.3716531}, author = {Urbanowicz, Ryan J.} }
@article{10.1145/3771734, title = {Are There Exceptions to Goodhart's law? On the Moral Justification of Fairness-Aware Machine Learning}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3771734}, url = {https://doi.org/10.1145/3771734}, author = {Weerts, Hilde and Royakkers, Lamb\`er and Pechenizkiy, Mykola}, keywords = {algorithmic fairness, fairness-aware machine learning, ethics, egalitarianism}, abstract = {Fairness-aware machine learning (fair-ml) techniques are algorithmic interventions designed to ensure that individuals who are affected by the predictions of a machine learning model are treated fairly. The problem is often posed as an optimization problem, where the objective is to achieve high predictive performance under a quantitative fairness constraint. However, any attempt to design a fair-ml algorithm must assume a world where Goodhart’s law has an exception: when a fairness measure becomes an optimization constraint, it does not cease to be a good measure. In this paper, we argue that fairness measures are particularly sensitive to Goodhart’s law. Our main contributions are as follows. First, we present a framework for moral reasoning about the justification of fairness metrics. In contrast to existing work, our framework incorporates the belief that whether a distribution of outcomes is fair, depends not only on the cause of inequalities but also on what moral claims decision subjects have to receive a particular benefit or avoid a burden. We use the framework to distil moral and empirical assumptions under which particular fairness metrics correspond to a fair distribution of outcomes. Second, we explore the extent to which employing fairness metrics as a constraint in a fair-ml algorithm is morally justifiable, exemplified by the fair-ml algorithm introduced by Hardt et\&nbsp;al. [21]. We illustrate that enforcing a fairness metric through a fair-ml algorithm often does not result in the fair distribution of outcomes that motivated its use and can even harm the individuals the intervention was intended to protect.} }
@inproceedings{10.1145/3678957.3685723, title = {Understanding Non-Verbal Irony Markers: Machine Learning Insights Versus Human Judgment}, booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction}, pages = {164--172}, year = {2024}, isbn = {9798400704628}, doi = {10.1145/3678957.3685723}, url = {https://doi.org/10.1145/3678957.3685723}, author = {Spitale, Micol and Catania, Fabio and Panzeri, Francesca}, keywords = {Affective computing, Dataset, Irony detection, Multi-modal, location = San Jose, Costa Rica}, abstract = {Irony detection is a complex task that often stumps both humans, who frequently misinterpret ironic statements, and artificial intelligence (AI) systems. While the majority of AI research on irony detection has concentrated on linguistic cues, the role of non-verbal cues like facial expressions and auditory signals has been largely overlooked. This paper investigates the effectiveness of machine learning models in recognizing irony using solely non-verbal cues. To this end, we conducted the following experiments and analysis: (i) we trained and evaluated some machine-learning models to detect irony; (ii) we compared the results with human interpretations; and (iii) we analysed and identified multi-modal non-verbal irony markers. Our research demonstrates that machine learning models trained on nonverbal data have shown significant promise in detecting irony, outperforming human judgments in this task. Specifically, we found that certain facial action units and acoustic characteristics of speech are key indicators of irony expression. These non-verbal cues, often overlooked in traditional irony detection methods, were effectively identified by machine learning models, leading to improved accuracy in detecting irony.} }
@inproceedings{10.1145/3698038.3698548, title = {FaPES: Enabling Efficient Elastic Scaling for Serverless Machine Learning Platforms}, booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing}, pages = {443--459}, year = {2024}, isbn = {9798400712869}, doi = {10.1145/3698038.3698548}, url = {https://doi.org/10.1145/3698038.3698548}, author = {Zhao, Xiaoyang and Yang, Siran and Wang, Jiamang and Diao, Lansong and Qu, Lin and Wu, Chuan}, keywords = {Cluster Scheduling, Distributed System, location = Redmond, WA, USA}, abstract = {Serverless computing platforms have become increasingly popular for running machine learning (ML) tasks due to their user-friendliness and decoupling from underlying infrastructure. However, auto-scaling to efficiently serve incoming requests still remains a challenge, especially for distributed ML training or inference jobs in a serverless GPU cluster. Distributed training and inference jobs are highly sensitive to resource configurations, and demand high model efficiency throughout their lifecycle. We propose FaPES, a FaaS-oriented Performance-aware Elastic Scaling system to enable efficient resource allocation in serverless platforms for ML jobs. FaPES enables flexible resource loaning between virtual clusters for running training and inference jobs. For running inference jobs, servers are reclaimed on demand with minimal preemption overhead to guarantee service level objective (SLO); for training jobs, optimal GPU allocation and model hyperparameters are jointly adapted based on an ML-based performance model and a resource usage prediction board, alleviating users from model tuning and resource specification. Evaluation on a 128-GPU testbed demonstrates up to 24.8\% job completion time reduction and 1.8 Goodput improvement, as compared to representative elastic scaling schemes.} }
@proceedings{10.1145/3642970, title = {EuroMLSys '24: Proceedings of the 4th Workshop on Machine Learning and Systems}, year = {2024}, isbn = {9798400705410} }
@inproceedings{10.1145/3689236.3696749, title = {Application of Blockchain Equity System for Machine Learning and Network Security}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {277--286}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3696749}, url = {https://doi.org/10.1145/3689236.3696749}, author = {Peng, Kanghua and Shi, Jincheng and Qiao, Yifang}, keywords = {Blockchain technology, Cybersecurity, Equity system, Machine learning}, abstract = {This paper mainly studies how to apply machine learning and network security to the blockchain equity system to improve the efficiency and privacy of equity transactions of rural economic cooperatives and maintain the stability of rural revitalization. This paper analyzes the potential application value of machine learning and big data algorithms in equity transactions of economic cooperatives, including network security, risk assessment, and smart contracts. This paper proposes a framework scheme of blockchain equity system based on machine learning and big data algorithms, establishes a network security equity system with data tampering, provides a credible data management mechanism, has distributed storage functions, and realizes automatic network consensus and smart contract applications. Experimental results show that the proposed scheme can effectively improve the efficiency and privacy security of equity trading, and provide new ideas and network security support for the development of equity trading.} }
@inproceedings{10.1145/3716895.3717018, title = {Mining and analyzing potential customers of bank loans based on machine learning}, booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering}, pages = {694--699}, year = {2025}, isbn = {9798400718007}, doi = {10.1145/3716895.3717018}, url = {https://doi.org/10.1145/3716895.3717018}, author = {Cheng, Yuxian and Bai, Hua}, keywords = {Bank loan, Customer persona, Data mining, Machine learning}, abstract = {This paper predicts whether a customer will apply for a bank loan based on the customer persona created by machine learning. The data generated in the loan ac-tivities of a foreign commercial bank are pre-processed and the machine learning model parameters are tuned by grid search. Several machine learning algorithms such as logistic regression, random forest, KNN and XGBoost were evaluated by accuracy, AUC, F1, Kappa and other indicators, and it was concluded that Ran-dom forest under stratified sampling was the best. This paper uses the form of decision tree to construct detailed customer personas for precision marketing.} }
@inproceedings{10.1145/3757749.3757765, title = {Machine Learning Value Prediction based on a Soccer Player's Performance on the Field}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {92--101}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757765}, url = {https://doi.org/10.1145/3757749.3757765}, author = {Chen, Yunzhe}, keywords = {Machine Learning, On-field Performance Data, Player's value prediction, Web Crawling}, abstract = {Football is the most popular sport worldwide. With the development of the global economy, it has evolved beyond a competitive activity into a rapidly expanding market. Especially for frequent market transfers, the value of players has become a hot topic of concern to everyone. This paper employs machine learning to analyze the huge amount of player performance data on the field to predict the value of players. The data in this paper comes from a professional football data website, T Football Website. It mainly crawls various data including offense, organization, defense, etc. of players in the five major leagues in the 23/24 season. Feature selection is carried out by analyzing the correlation of the features, and then the target variable is logarithmically transformed to reduce the scale and alleviate skewness and other preprocessing. In terms of the model, this paper uses three different models, XGBoost, GBDT, and RF, to predict the player's value. Model performance is evaluated by comparing the predicted value with the true value scatter plot, as well as indicators such as R², MAE, and RMSE. The player's value is evaluated through 54 player instances, and the cause of the error is analyzed. The results indicate that the three different models are all feasible for this prediction to a certain extent, but due to factors such as the international reputation of the players, the rise and fall of their values during the growth and decline periods, etc., the models have a certain degree of error.} }
@article{10.14778/3750601.3750620, title = {Magnus: A Holistic Approach to Data Management for Large-Scale Machine Learning Workloads}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {4964--4977}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3750601.3750620}, url = {https://doi.org/10.14778/3750601.3750620}, author = {Song, Jun and Ding, Jingyi and Kandy, Irshad and Lin, Yanghao and Wei, Zhongjia and Zhou, Zilong and Peng, Zhiwei and Shan, Jixi and Mao, Hongyue and Huang, Xiuqi and Song, Xun and Chen, Cheng and Li, Yanjia and Yang, Tianhao and Jia, Wei and Dong, Xiaohong and Lei, Kang and Shi, Rui and Zhao, Pengwei and Chen, Wei}, abstract = {Machine learning (ML) has become a cornerstone of key applications at ByteDance. As model complexity and data volumes surge, data management for large-scale ML workloads faces substantial challenges, particularly with recent advances in large recommendation models (LRMs) and large multimodal models (LMMs). Traditional approaches exhibit limitations in storage efficiency, metadata scalability, update mechanisms, and integration with ML frameworks. To address these challenges, we propose Magnus, a holistic data management system built upon Apache Iceberg. Magnus integrates innovative optimizations across resource-efficient storage formats optimized for large wide tables and multimodal data, built-in support for vector and inverted indexes to accelerate data retrieval, scalable metadata planning with Git-like branching and tagging capabilities, and high-performance update/upsert based on lightweight merge-on-read (MOR) strategies. Additionally, Magnus provides native support and specialized enhancement for LRM and LMM training workloads. Experimental results demonstrate significant performance gains in real-world ML scenarios. Magnus has been deployed at ByteDance for over five years, enabling robust and efficient data infrastructure for large-scale ML workloads.} }
@inproceedings{10.1145/3678884.3681920, title = {Understanding the Perceptions and Practices of the Machine Learning Professionals in Bangladesh}, booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing}, pages = {647--652}, year = {2024}, isbn = {9798400711145}, doi = {10.1145/3678884.3681920}, url = {https://doi.org/10.1145/3678884.3681920}, author = {Nowshin, Afroza and Shiba, Shammi Akhter and Saha, Pratyasha and Sadeque, Farig and Haque, S M Taiabul}, keywords = {artificial intelligence, global south, human-computer interaction, ict4d, machine learning, location = San Jose, Costa Rica}, abstract = {A growing body of research focuses on the machine learning professionals in the Global North, but the perceptions and practices of the professionals in the Global South have remained understudied. For this work, we interviewed 15 AI/ML professionals from Bangladesh to gain insights into their motivations, perceptions, and practices when working with machine learning tools and technologies. Our findings reveal that the machine learning professionals in this region prefer labeled datasets for their work and have lack of knowledge regarding annotation, design, or representation bias. Their responses also indicate that resource constraints and systematic or organizational barriers limit their capacity to address environmental concerns as well as explainability and privacy issues. Our initial findings highlight the importance of a large-scale study with the machine learning professionals in the Global South.} }
@inproceedings{10.1145/3585088.3593929, title = {Developing Machine Learning Agency Among Youth: Investigating Youth Critical Use, Examination, and Production of Machine Learning Applications}, booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference}, pages = {781--784}, year = {2023}, isbn = {9798400701313}, doi = {10.1145/3585088.3593929}, url = {https://doi.org/10.1145/3585088.3593929}, author = {Adisa, Ibrahim Oluwajoba}, keywords = {agency, computational thinking, machine learning, youth, location = Chicago, IL, USA}, abstract = {Abstract. Young people are surrounded by machine learning (ML) devices and their lived experiences are increasingly shaped by the ML technologies that are ever-present in their lives. As innovations in machine learning technologies continue to shape society, it raises important implications for what young people learn, their career trajectories, and the required literacies they need to thrive in this changing occupational environment. Youth are particularly vulnerable to the impact of ML and very little has been done to empower them to critically engage in the discourse surrounding the next generation of technologies that have a marked potential to shape their lives for better or worse. My dissertation work seeks to develop youth autonomy and agency around ML by designing an intervention that supports youth critical use, examination, and production of ML applications in the context of promoting self-expression and social good. I will conduct a qualitative single case study research and collect multiple forms of data using interviews, story completions, digital artifacts, observations, and focus group discussions. These data sources will allow me to conduct an intensive analysis and investigation of how youth populations can be supported to develop the skills, practices and critical consciousness needed to effectively engage with machine learning technologies. Through my research, I also hope to advance the literature on how young people creatively collaborate with ML and use ML for self-expression.} }
@inproceedings{10.1145/3688671.3688780, title = {Driver Sleepiness Detection Using Machine Learning Models on EEG Data}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688780}, url = {https://doi.org/10.1145/3688671.3688780}, author = {Trigka, Maria and Dritsas, Elias and Mylonas, Phivos}, keywords = {Sleepiness Detection, EEG, Machine Learning}, abstract = {Driver sleepiness is a major cause of road accidents, necessitating effective detection systems to improve safety. This study investigates the use of machine learning (ML) models to automate the detection of driver sleepiness through electroencephalography (EEG) data collected in simulated environments. Various ML models, such as Random Forests (RF), Decision Trees (DT), Logistic Model Trees (LMT) and two Ensemble methods (Bagging and Stacking), were evaluated using 10-fold cross-validation. More specifically, the selected classifiers were trained and tested using EEG data acquired via the NeuroSky MindWave device, including band power, attention, and mediation features to differentiate effectively between Sleepy and Non-sleepy subjects. The Bagging approach demonstrated superior performance among the classifiers, achieving 84.99\% accuracy, 0.849 precision, 0.850 recall, and an Area Under the ROC Curve (AUC) of 0.935.} }
@inproceedings{10.1145/3626203.3670552, title = {NeedLR: Streamlining Point Cloud Annotation for Enhanced Machine Learning Integration}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670552}, url = {https://doi.org/10.1145/3626203.3670552}, author = {Stone, Gunner and Tavakkoli, Alireza}, keywords = {3D data processing, complex networks, machine learning, point cloud annotation, superpoint-graph, location = Providence, RI, USA}, abstract = {With the rising popularity of remote sensing across various scientific and engineering disciplines, the demand for efficient analysis of point cloud data has surged. However, the inherent complexity and volume of point cloud data pose considerable obstacles to human annotation efforts, which are an essential step within the machine learning pipeline for generating accurate training datasets. NeedLR emerges as a solution, offering a robust, user-friendly platform tailored for precise and streamlined point cloud annotation. By harnessing GPU-accelerated visualization, NeedLR facilitates interactive 3D manipulation of point clouds, granting users an intuitive means to dive into their data. Further optimized for efficient RAM usage and employing parallel computing strategies, NeedLR achieves great performance across varied computing environments. Its accessible interface broadens user engagement, rendering it a prime candidate for crowdsourced annotation initiatives and enhancing its utility in machine learning endeavors. This paper presents NeedLR, exploring its development, key features, and the user-centric philosophy that shapes its design. We highlight the potential for NeedLR’s role in enhancing current point cloud annotation techniques, merging operational efficiency with broad access to empower users across disciplines.} }
@inproceedings{10.1145/3628797.3628974, title = {Enhancing Intensive Care Patient Prognostics with Machine Learning}, booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology}, pages = {546--553}, year = {2023}, isbn = {9798400708916}, doi = {10.1145/3628797.3628974}, url = {https://doi.org/10.1145/3628797.3628974}, author = {Aissaoui, Wafae and Khennou, Fadoua and Abdellaoui, Abderrahim}, keywords = {Artificial Intelligence, Electronic health records., Machine Learning, Readmission \&amp, Discharge, location = Ho Chi Minh, Vietnam}, abstract = {This article delves into the challenge of foreseeing patient discharges and unplanned returns to the intensive care unit. Its primary objective is to enhance the decision-making process for healthcare providers and administrators, facilitate resource allocation, and elevate the quality of patient care. The focal point of our research involves the prediction of patient discharges and unforeseen readmissions to the intensive care unit, leveraging the comprehensive Medical Information Mart for Intensive Care (MIMIC-III) database. To do this, we evaluated seven distinct machine learning models, aiming to discern their predictive accuracy specifically for ICU readmissions and discharges using the MIMIC-III dataset. Our approach employs a repertoire of machine learning algorithms, encompassing logistic regression, support vector machines (SVM), random forest, and others. These algorithms undergo meticulous training to project the probabilities of both patient discharge and readmission. This training process is reliant on a diverse set of variables, encompassing vital signs, demographic information, and age. In the realm of patient readmissions, the ensemble of Neural Network, SVM, and Random Forest models demonstrates exceptional performance, achieving an impressive accuracy of 99\%. Meanwhile, the Gradient Boosting model showcases remarkably high accuracy, reaching 97\%. In the context of patient discharges, the Random Forest model emerges as the most proficient, boasting an accuracy of approximately 79\%. The findings underscore the effectiveness of these machine learning models in anticipating the likelihood of ICU patient readmissions and discharges. Nonetheless, it’s vital to acknowledge that the outcomes may vary contingent upon the implemented model. Nevertheless, these collective results hold the promise of enhancing patient care and refining administrative practices within medical clinics and hospitals.} }
@inproceedings{10.1145/3671151.3671363, title = {Research on depression diagnosis based on the Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {1213--1218}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671363}, url = {https://doi.org/10.1145/3671151.3671363}, author = {Li, Ningyu and Wang, Guixiang}, abstract = {Depression is a common mental disorder that has serious effects on patients' physical and mental health. Diagnosis of depression in clinical practice mainly relies on the experience and professional knowledge of physicians, which results in inaccurate diagnosis, misdiagnosis, and missed diagnosis. In recent years, the development of machine learning technology has provided new possibilities for automated diagnosis of depression. This paper proposes a machine learning-based diagnosis model for depression and validates the model through experiments. The model uses the deep neural network gcForest (multi-granularity cascade forest) to classify patients with depression and normal individuals, and uses multiple physiological features as input, including electroencephalogram data, heart rate variability, skin conductance activity, etc. The experimental results show that the model can effectively distinguish patients with depression from normal individuals and has high accuracy, precision, recall, and F1 scores. The machine learning-based diagnosis model for depression proposed in this study has certain clinical application value, can provide doctors with diagnostic assistance tools, and is expected to provide more effective means for early warning and treatment of depression in the future.} }
@inproceedings{10.1145/3604237.3626884, title = {Generative Machine Learning for Multivariate Equity Returns}, booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance}, pages = {159--166}, year = {2023}, isbn = {9798400702402}, doi = {10.1145/3604237.3626884}, url = {https://doi.org/10.1145/3604237.3626884}, author = {Tepelyan, Ruslan and Gopal, Achintya}, keywords = {Generative Modeling, Normalizing Flows, Portfolio Optimization, Risk Forecasting, Stock Returns, Variational Autoencoders, location = Brooklyn, NY, USA}, abstract = {The use of machine learning to generate synthetic data has grown in popularity with the proliferation of text-to-image models and especially large language models. The core methodology these models use is to learn the distribution of the underlying data, similar to the classical methods common in finance of fitting statistical models to data. In this work, we explore the efficacy of using modern machine learning methods, specifically conditional importance weighted autoencoders (a variant of variational autoencoders) and conditional normalizing flows, for the task of modeling the returns of equities. The main problem we work to address is modeling the joint distribution of all the members of the S\&amp;P 500, or, in other words, learning a 500-dimensional joint distribution. We show that this generative model has a broad range of applications in finance, including generating realistic synthetic data, volatility and correlation estimation, risk analysis (e.g., value at risk, or VaR, of portfolios), and portfolio optimization.} }
@inproceedings{10.1145/3644032.3644467, title = {Machine Learning-based Test Case Prioritization using Hyperparameter Optimization}, booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)}, pages = {125--135}, year = {2024}, isbn = {9798400705885}, doi = {10.1145/3644032.3644467}, url = {https://doi.org/10.1145/3644032.3644467}, author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta}, keywords = {hyperparameter optimization, test case prioritization, machine learning, continuous integration, location = Lisbon, Portugal}, abstract = {Continuous integration pipelines execute extensive automated test suites to validate new software builds. In this fast-paced development environment, delivering timely testing results to developers is critical to ensuring software quality. Test case prioritization (TCP) emerges as a pivotal solution, enabling the prioritization of fault-prone test cases for immediate attention. Recent advancements in machine learning have showcased promising results in TCP, offering the potential to revolutionize how we optimize testing workflows. Hyperparameter tuning plays a crucial role in enhancing the performance of ML models. However, there needs to be more work investigating the effects of hyperparameter tuning on TCP. Therefore, we explore how optimized hyperparameters influence the performance of various ML classifiers, focusing on the Average Percentage of Faults Detected (APFD) metric. Through empirical analysis of ten real-world, large-scale, diverse datasets, we conduct a grid search-based tuning with 885 hyperparameter combinations for four machine learning models. Our results provide model-specific insights and demonstrate an average 15\% improvement in model performance with hyperparameter tuning compared to default settings. We further explain how hyperparameter tuning improves precision (max = 1), recall (max = 0.9633), F1-score (max = 0.9662), and influences APFD value (max = 0.9835), indicating a direct connection between tuning and prioritization performance. Hence, this study underscores the importance of hyperparameter tuning in optimizing failure prediction models and their direct impact on prioritization performance.} }
@inproceedings{10.1145/3764924.3770889, title = {Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses}, booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Spatial Intelligence for Smart and Connected Communities}, pages = {1--4}, year = {2025}, isbn = {9798400721878}, doi = {10.1145/3764924.3770889}, url = {https://doi.org/10.1145/3764924.3770889}, author = {Shen, Bolin and Ozguven, Eren and Zhao, Yue and Wang, Guang and Xie, Yiqun and Dong, Yushun}, keywords = {machine learning, storm surge, economic losses, risk assessment, location = The Graduate Hotel Minneapolis, Minneapolis, MN, USA}, abstract = {Florida communities face recurring and severe hurricane impacts, leading to substantial and repeated economic losses, particularly in vulnerable coastal and low-income areas. While prior studies have examined individual drivers of hurricane-induced damage, few have developed a unified, community-relevant framework that integrates diverse factors to comprehensively assess sources of loss. Drawing on data that capture the unique environmental and socioeconomic conditions of Florida communities, we propose a modeling framework tailored to the Florida's disaster context. The framework categorizes contributing factors into three components: (1) hurricane characteristics, (2) water-related environmental conditions, and (3) socioeconomic attributes of affected areas. By integrating multi-source data and aggregating variables at the ZIP Code Tabulation Area (ZCTA) level, we use machine learning models to predict economic loss, with insurance claims serving as indicators of realized damage. Beyond accurate prediction, our approach systematically evaluates the relative importance of each factor category, providing actionable insights for disaster preparedness, equitable recovery, and adaptive urban planning in hurricane-exposed communities. Our code is available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction} }
@inproceedings{10.1145/3745238.3745265, title = {RESEARCH ON MACHINE LEARNING METHODS FOR PERSONALIZED LEARNING RECOMMENDATION SYSTEMS}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {153--157}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745265}, url = {https://doi.org/10.1145/3745238.3745265}, author = {Deng, YuJie}, keywords = {Adaptive Recommendation Framework, The Personalised Learning Platform, The application of machine learning methodologies}, abstract = {The present study proposes a personalised recommendation system for students, the function of which is to recommend teaching resources according to students' mastery and interest. Existing information platforms are challenging to utilise for the purpose of recommending teaching resources that align with students' individual conditions, and conventional recommendation technologies frequently encounter issues with accuracy and adaptability. It is evident that a combination of deep neural networks, reinforcement learning mechanisms and domain knowledge graphs is required. Firstly, deep learning methods are employed to extract multi-dimensional features from learning trajectories. Subsequently, reinforcement learning frameworks are utilised to optimise the recommendation decisions in order to facilitate dynamic planning of learning paths. The establishment of a structured knowledge graph enables the system to establish semantic associations between learning resources with a high degree of effectiveness, thereby improving the accuracy of recommendation results. In the face of the challenging issue posed by the limited number of new users, the system has been engineered to ensure the consistency of its recommendation quality in the majority of instances where the initial data is found to be inadequate. This is achieved through the implementation of a domain migration learning scheme.} }
@inproceedings{10.1145/3477495.3531722, title = {Retrieval-Enhanced Machine Learning}, booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {2875--2886}, year = {2022}, isbn = {9781450387323}, doi = {10.1145/3477495.3531722}, url = {https://doi.org/10.1145/3477495.3531722}, author = {Zamani, Hamed and Diaz, Fernando and Dehghani, Mostafa and Metzler, Donald and Bendersky, Michael}, keywords = {knowledge grounding, memory augmentation, retrieval augmentation, location = Madrid, Spain}, abstract = {Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.} }
@inproceedings{10.1145/3747227.3747279, title = {The Reflection of Traditional Architectural Features in Modern Residence using Machine Learning: A case study near ancient town in Shanghai}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {330--335}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747279}, url = {https://doi.org/10.1145/3747227.3747279}, author = {Fan, Yilun and Cai, Jun}, keywords = {Architectural styles, K-Means model, Machine learning, Quality of life, Satisfaction score}, abstract = {This study examines the role of the machine learning model in the clustering of groups having similarities in rating architectural styles. The survey approach was used to collect data from participants residing near Qibao Ancient Town in Shanghai, China. The collected data was preprocessed for extraction of significant architectural features. The extracted information was used to train and evaluate the K-Means model to cluster residents' satisfaction and identify their needs. Results show that moderate architectural similarity (0.4 - 0.8) was achieved by the machine learning model, while human-rated similarity (0.0 - 1.0) was obtained in this study. The K-Means clustering approach categorized participants into three clusters based on their satisfaction and their concerns on lighting and noise. This research study suggests redesigning fixtures with tactile finishes, ergonomic handles, and modular shelving. Furthermore, this work suggests installing LED lighting in cold regions and adding acoustic panels in high-traffic areas.} }
@inproceedings{10.1145/3658271.3658326, title = {Investigating Predicting Voluntary Resignation Program Participation with Machine Learning}, booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems}, year = {2024}, isbn = {9798400709968}, doi = {10.1145/3658271.3658326}, url = {https://doi.org/10.1145/3658271.3658326}, author = {Jorge, Ezequiel Mule and Barbieri, L\'ucio Tales and Escovedo, Tatiana and Kalinowski, Marcos}, keywords = {Algorithms, Machine Learning, Models, Prediction, Voluntary Resignation Programs, location = Juiz de Fora, Brazil}, abstract = {Context: The growing challenge in attracting employees to Voluntary Resignation Programs (VRP) lies in the need to balance the company’s cost control with the goal of increasing participation from the target audience. Problem: It is essential to ensure that the process occurs smoothly, reducing tension during the separation and fostering a more cooperative and responsible environment. Companies need to maximize attraction to VRP, minimize costs, and improve resource allocation. Solution: This article aims to construct a Machine Learning (ML) model to predict employee participation VRPs in an organizational context and to identify the key factors that influence employee participation in the program, identifying patterns and trends based on previous programs. IS Theory: This work is associated with the Theory of Computational Learning, which aims to understand the fundamental principles of learning and design better-automated methods. Method: This article constitutes a study of past data, aiming to identify patterns and develop trends related to employee participation through the utilization of ML algorithms. Summary of Results: The investigation into predicting VRP participation using ML revealed compelling correlations. The Bootstrap Aggregating with Logistic Regression model emerged as the most effective, demonstrating high F1-Score and Accuracy. Contributions and Impact in the IS area: The research significantly contributes to the IS field by showcasing ML’s application in predicting VRP participation, enriching our understanding of factors influencing employee decisions and highlights technology-driven solutions in workforce management. Insights from this investigation offer a valuable framework for future research, paving the way for predictive analytics integration in addressing complex HR challenges within the broader IS context.} }
@article{10.1145/3728887, title = {Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3728887}, url = {https://doi.org/10.1145/3728887}, author = {Risse, Niklas and Liu, Jing and B\"ohme, Marcel}, keywords = {LLM, ML4VD, benchmark, context, data quality, function, machine learning, software security, spurious correlations, vulnerability detection}, abstract = {According to our survey of machine learning for vulnerability detection (ML4VD), 9 in every 10 papers published in the past five years define ML4VD as a function-level binary classification problem: Given a function, does it contain a security flaw? From our experience as security researchers, faced with deciding whether a given function makes the program vulnerable to attacks, we would often first want to understand the context in which this function is called. In this paper, we study how often this decision can really be made without further context and study both vulnerable and non-vulnerable functions in the most popular ML4VD datasets. We call a function vulnerable if it was involved in a patch of an actual security flaw and confirmed to cause the program’s vulnerability. It is non-vulnerable otherwise. We find that in almost all cases this decision cannot be made without further context. Vulnerable functions are often vulnerable only because a corresponding vulnerability-inducing calling context exists while non-vulnerable functions would often be vulnerable if a corresponding context existed. But why do ML4VD techniques achieve high scores even though there is demonstrably not enough information in these samples? Spurious correlations: We find that high scores can be achieved even when only word counts are available. This shows that these datasets can be exploited to achieve high scores without actually detecting any security vulnerabilities. We conclude that the prevailing problem statement of ML4VD is ill-defined and call into question the internal validity of this growing body of work. Constructively, we call for more effective benchmarking methodologies to evaluate the true capabilities of ML4VD, propose alternative problem statements, and examine broader implications for the evaluation of machine learning and programming analysis research.} }
@inproceedings{10.1145/3704323.3704325, title = {Research on crime risk prediction model based on machine learning}, booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition}, pages = {361--365}, year = {2025}, isbn = {9798400717482}, doi = {10.1145/3704323.3704325}, url = {https://doi.org/10.1145/3704323.3704325}, author = {Hou, Xuehui and Rexiti, Kudelaiti and Aizezi, Yasen}, keywords = {Crime risk prediction, Naive Bayes, Random Forest, XGboost}, abstract = {Abstract: Crime prediction is an important method for public security departments to conduct crime early warning and investigation. According to the multidimensional characteristics of criminals, machine learning classification algorithm is used to establish a prediction model of whether a criminal commits a crime again, so as to quantitatively evaluate the possibility of the criminal commits a crime again, and provide a feasible scheme reference for preventing and reducing the recurrence of crime. Analysis of urban crime data in Boston, using Naive Bayes, Random Forest and XGboost algorithms to build a crime type prediction model, we can get the urban crime type tendency, help the public security department to predict the future crime, take effective measures to reduce the urban crime rate.} }
@inproceedings{10.1145/3735358.3737771, title = {A Communication-Efficient Paradigm for Decentralized Machine Learning with Model Logits}, booktitle = {Proceedings of the 9th Asia-Pacific Workshop on Networking}, pages = {301--302}, year = {2025}, isbn = {9798400714016}, doi = {10.1145/3735358.3737771}, url = {https://doi.org/10.1145/3735358.3737771}, author = {Cai, Yiwen and Du, Haizhou} }
@inproceedings{10.1145/3711542.3711585, title = {Machine Learning Approaches to the Exhaustive Auto-Identification of Japanese Clauses}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {235--241}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711585}, url = {https://doi.org/10.1145/3711542.3711585}, author = {Zhong, Yong}, keywords = {Japanese clause, Random Forest, XGBoost, exhaustive auto-identification, machine learning, morphological information}, abstract = {Automatic clause identification is important for both Natural Language Processing and Second Language Writing Research. In order to explore the optimal model for exhaustive auto-identification of Japanese clauses, this paper trained six distinct machine learning models using the morphological information from the BCCWJ corpus and its clause boundary annotation dataset, BCCWJ-CBL, and evaluated and compared the prediction performance of each model. Results indicated that both the XGBoost and Random Forest models achieved high F1 scores of 0.982 for the lenient metric and 0.976 for the stringent metric in the evaluation, and the two can be considered as the optimal tools for exhaustive auto-identification of Japanese clauses currently available. Additionally, this paper also found that the five types of morphological information (active form, lemma pronunciation, macro-categorization of parts of speech (POS), original form, and meso-categorization of POS) of current words are the most important features influencing the prediction performance of these models.} }
@inproceedings{10.1145/3589334.3645520, title = {ModelGo: A Practical Tool for Machine Learning License Analysis}, booktitle = {Proceedings of the ACM Web Conference 2024}, pages = {1158--1169}, year = {2024}, isbn = {9798400701719}, doi = {10.1145/3589334.3645520}, url = {https://doi.org/10.1145/3589334.3645520}, author = {Duan, Moming and Li, Qinbin and He, Bingsheng}, keywords = {ai licensing, license analysis, model mining, location = Singapore, Singapore}, abstract = {Productionizing machine learning projects is inherently complex, involving a multitude of interconnected components that are assembled like LEGO blocks and evolve throughout development lifecycle. These components encompass software, databases, and models, each subject to various licenses governing their reuse and redistribution. However, existing license analysis approaches for Open Source Software (OSS) are not well-suited for this context. For instance, some projects are licensed without explicitly granting sublicensing rights, or the granted rights can be revoked, potentially exposing their derivatives to legal risks. Indeed, the analysis of licenses in machine learning projects grows significantly more intricate as it involves interactions among diverse types of licenses and licensed materials. To the best of our knowledge, no prior research has delved into the exploration of license conflicts within this domain. In this paper, we introduce ModelGo, a practical tool for auditing potential legal risks in machine learning projects to enhance compliance and fairness. With ModelGo, we present license assessment reports based on five use cases with diverse model-reusing scenarios, rendered by real-world machine learning components. Finally, we summarize the reasons behind license conflicts and provide guidelines for minimizing them. Our code is publicly available at https://github.com/Xtra-Computing/ModelGo.} }
@inproceedings{10.5555/3615924.3623635, title = {First Workshop on Machine Learning Challenges in Cybersecurity}, booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering}, pages = {248--250}, year = {2023}, author = {Branco, Paula and Moniz, Nuno and Jourdan, Guy-Vincent}, keywords = {Cybersecurity, Machine Learning, Deep Learning, Imbalanced Data, Novelty Detection, location = Las Vegas, NV, USA}, abstract = {Cybersecurity events severely impact many individuals, infras-tructures, businesses, and institutions. Machine learning has been playing a key role in many aspects of our daily life and in particular as a cyber defense mechanism. However, the nature of cyber threats poses many unique challenges to machine learning models. The first workshop on Machine Learning Challenges in Cybersecurity held at the CASCON conference focused on the key challenges, recent developments, and open research issues of machine learning in cybersecurity. Two applications of deep learning to phishing and face verification systems are analyzed. Finally, an invited talk from Dr. Shirani covered a machine learning based solution for insider threat detection.} }
@inproceedings{10.1145/3658644.3690337, title = {Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {4480--4494}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690337}, url = {https://doi.org/10.1145/3658644.3690337}, author = {Bagdasarian, Eugene and Shmatikov, Vitaly}, keywords = {ML security, backdoors, hyperparameter search, location = Salt Lake City, UT, USA}, abstract = {Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a "backdoor" task (e.g., misclassify inputs with a certain feature) in addition to its main task. Recent research proposed many hypothetical backdoor attacks whose efficacy depends on the configuration and training hyperparameters of the target model. At the same time, state-of-the-art defenses require massive changes to the existing ML pipelines and protect only against some attacks.Given the variety of potential backdoor attacks, ML engineers who are not security experts have no way to measure how vulnerable their current training pipelines are, nor do they have a practical way to compare training configurations so as to pick the more resistant ones. Deploying a defense may not be a realistic option, either. It requires evaluating and choosing from among dozens of research papers, completely re-engineering the pipeline as required by the chosen defense, and then repeating the process if the defense disrupts normal model training (while providing theoretical protection against an unknown subset of hypothetical threats).In this paper, we aim to provide ML engineers with pragmatic tools to audit the backdoor resistance of their training pipelines and to compare different training configurations, to help choose the one that best balances accuracy and security.First, we propose a universal, attack-agnostic resistance metric based on the minimum number of training inputs that must be compromised before the model learns any backdoor.Second, we design, implement, and evaluate Mithridates, a multi-stage approach that integrates backdoor resistance into the training-configuration search. ML developers already rely on hyperparameter search to find configurations that maximize the model's accuracy. Mithridates extends this tool to also order configurations based on their backdoor resistance. We demonstrate that Mithridates discovers configurations whose resistance to multiple types of backdoor attacks increases by 3-5x with only a slight impact on accuracy. We also discuss extensions to AutoML and federated learning.} }
@inproceedings{10.1145/3690771.3690777, title = {Investigating the Factors Affecting Risky Levels of Alcohol Consumption among Students Using Machine Learning Approach}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {7--13}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690777}, url = {https://doi.org/10.1145/3690771.3690777}, author = {Shanchary, Sara Fariha and Meraz, Md Naved and Ibne Hakim, Ayman and Faiyaz, Chowdhury Nafis and Shafayat Oshman, Muhammad}, keywords = {alcohol consumption, alcohol use disorder, family relationships, machine learning}, abstract = {Addressing the pressing issue of alcohol consumption among students is crucial for the well-being of young individuals and the community. Understanding and mitigating alcohol related challenges faced by young people is essential for their healthy development. This study utilizes machine learning models to analyze data from a Portuguese school, aiming to link students' alcohol consumption levels to personal and familial factors. The primary objective is to identify key factors associated with high alcohol consumption among students. After data preprocessing on their dataset, we employed various machine learning algorithms, including hyperparameter-optimized Decision Tree, Random Forest, Boosting, and Ensemble Learning. Our findings revealed that the decision tree algorithm performed well in predicting risky alcohol consumption for our target research question. Our selected feature subset showed strong positive correlation with the target variable, achieving an accuracy of 80.9 percent on the test set and 98.43 percent on the train set. Notably, this project breaks new ground by using explainable artificial intelligence to add reason to our prediction based on students' familial relationships, expanding upon previous research that also focused on demographic factors.} }
@article{10.1145/3696430, title = {Leveraging Blockchain and Machine Learning to Promote Child Labor-Free Sustainable Development}, journal = {Distrib. Ledger Technol.}, volume = {4}, year = {2025}, doi = {10.1145/3696430}, url = {https://doi.org/10.1145/3696430}, author = {Musamih, Ahmad and Hassan, Abduraouf and Salah, Khaled and Jayaraman, Raja and Omar, Mohammed and Yaqoob, Ibrar}, keywords = {Child Labor, Social Change, Technological Innovation, Sustainable Development, Blockchain, Object Detection}, abstract = {Child labor has been on the rise in recent years, which necessitates improved identification and reporting mechanisms. Current labor management systems, which are often manual or article-based, lack traceability, audit, privacy, security, and trust features. This leads to challenges in detecting and reporting violations, particularly in large or remote areas. The persistence of this issue undermines the achievement of Sustainable Development Goals (SDGs) and highlights the important role of Corporate Social Responsibility (CSR) in addressing this challenge. Our article proposes a solution combining machine learning and blockchain to automate child labor detection and ensure traceable, auditable, private, and secure reporting. Utilizing Decentralized Proxy Re-Encryption (DPRE), Zero-Knowledge Proofs (ZKPs), and oracles on the Ethereum blockchain, with decentralized storage, our approach maintains privacy and transparency. We present a child labor detection model using Mask2Former and ResNet-18 Convolutional Neural Network (CNN) to achieve high accuracy and reliability. The model’s performance is evaluated using various metrics, achieving an accuracy rate of 89.45\%, a precision score of 0.906, and a recall score of 0.9332. Additionally, we assess smart contracts for cost-efficiency and security, and discuss the solution’s generalizability, challenges, and practical implications. We make the source code of our solution publicly available on GitHub.} }
@inproceedings{10.1145/3626232.3658637, title = {Machine Learning Techniques for Python Source Code Vulnerability Detection}, booktitle = {Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy}, pages = {151--153}, year = {2024}, isbn = {9798400704215}, doi = {10.1145/3626232.3658637}, url = {https://doi.org/10.1145/3626232.3658637}, author = {Farasat, Talaya and Posegga, Joachim}, keywords = {bilstm, python, software vulnerabilities, location = Porto, Portugal}, abstract = {Software vulnerabilities are a fundamental reason for the prevalence of cyber attacks and their identification is a crucial yet challenging problem in cyber security. In this paper, we apply and compare different machine learning algorithms for source code vulnerability detection specifically for Python programming language. Our experimental evaluation demonstrates that our Bidirectional Long Short-Term Memory (BiLSTM) model achieves a remarkable performance (average Accuracy = 98.6\%, average F-Score = 94.7\%, average Precision = 96.2\%, average Recall = 93.3\%, average ROC = 99.3\%), thereby, establishing a new benchmark for vulnerability detection in Python source code.} }
@inproceedings{10.1145/3736733.3736740, title = {Humans, Machine Learning, and Language Models in Union: A Cognitive Study on Table Unionability}, booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics}, year = {2025}, isbn = {9798400719592}, doi = {10.1145/3736733.3736740}, url = {https://doi.org/10.1145/3736733.3736740}, author = {Marimuthu, Sreeram and Klimenkova, Nina and Shraga, Roee}, keywords = {human-in-the-loop, table unionability, data discovery, machine learning, meta-cognitive analysis, large language models, location = Intercontinental Berlin, Berlin, Germany}, abstract = {Data discovery and table unionability in particular became key tasks in modern Data Science. However, the human perspective for these tasks is still under-explored. Thus, this research investigates the human behavior in determining table unionability within data discovery. We have designed an experimental survey and conducted a comprehensive analysis, in which we assess human decision-making for table unionability. We use the observations from the analysis to develop a machine learning framework to boost the (raw) performance of humans. Furthermore, we perform a preliminary study on how LLM performance is compared to humans indicating that it is typically better to consider a combination of both. We believe that this work lays the foundations for developing future Human-in-the-Loop systems for efficient data discovery.} }
@article{10.1145/3721977, title = {A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3721977}, url = {https://doi.org/10.1145/3721977}, author = {Casey, Beatrice and Santos, Joanna C. S. and Perry, George}, keywords = {Source code representation, machine learning for software security, systematic literature review}, abstract = {Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what is not there yet. This article presents a study of these existing machine learning based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall (e.g., AST and tokenizers are the representations with the highest count of papers, whereas graph-based representations is the category with the highest count of papers). We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines are the most popular model overall.} }
@article{10.1145/3607870, title = {SMaLL: Software for Rapidly Instantiating Machine Learning Libraries}, journal = {ACM Trans. Embed. Comput. Syst.}, volume = {23}, year = {2024}, issn = {1539-9087}, doi = {10.1145/3607870}, url = {https://doi.org/10.1145/3607870}, author = {Sridhar, Upasana and Tukanov, Nicholai and Binder, Elliott and Low, Tze Meng and McMillan, Scott and Schatz, Martin D.}, keywords = {Mathematical software, machine learning libraries, high-performance, portability, embedded systems}, abstract = {Interest in deploying deep neural network (DNN) inference on edge devices has resulted in an explosion of the number and types of hardware platforms that machine learning (ML) libraries must support. High-level programming interfaces, such as TensorFlow, can be readily ported across different devices; however, maintaining performance when porting the low-level implementation is more nuanced. High-performance inference implementations require an effective mapping of the high-level interface to the target hardware platform. Commonly, this mapping may use optimizing compilers to generate code at compile time or high-performance vendor libraries that have been specialized to the target platform. Both approaches rely on expert knowledge across levels to produce an efficient mapping. This makes supporting new architectures difficult and time-consuming.In this work, we present a DNN library framework, SMaLL, that is easily extensible to new architectures. The framework uses a unified loop structure and shared, cache-friendly data format across all intermediate layers, eliminating the time and memory overheads incurred by data transformation between layers. Each layer is implemented by specifying its dimensions and a kernel, the key computing operation of that layer. The unified loop structure and kernel abstraction allows the reuse of code across layers and computing platforms. New architectures only require a few hundred lines in the kernel to be redesigned. To show the benefits of our approach, we have developed software that supports a range of layer types and computing platforms; this software is easily extensible for rapidly instantiating high-performance DNN libraries.An evaluation of the portability of our framework is shown by instantiating end-to-end networks from the MLPerf:tiny benchmark suite on five ARM platforms and one x86 platform (an AMD Zen 2). We also show that the end-to-end performance is comparable to or better than ML frameworks such as TensorFlow, TVM, and LibTorch.} }
@inproceedings{10.1145/3747227.3747260, title = {Assessment of ship carbon emission reduction technology based on analytical hierarchy and machine learning models}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {197--204}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747260}, url = {https://doi.org/10.1145/3747227.3747260}, author = {Wang, Yutong and Tao, Wenmei and Liu, Yimeng}, keywords = {analytical hierarchy, carbon emission, comprehensive assessment, logistic regression, random forest, ship emission}, abstract = {The issue of ship carbon emissions is becoming increasingly prominent. Researchers have proposed various measures to reduce carbon emissions, achieving certain levels of reduction. However, each technique also has its own operational drawbacks. Currently, there is a lack of comprehensive evaluation methods to specifically compare the merits and drawbacks of different technologies. Therefore, this paper proposes the use of Random Forest and Logistic Regression models for prediction of carbon emission reduction technologies. Analytic Hierarchy Process (AHP) to quantitatively evaluate and analyze five typical ship carbon emission reduction technologies. Firstly, the AHP logic is applied to establish a comprehensive evaluation index system for ship carbon emission reduction technologies, encompassing three criteria layers of economy, environment and technology and eleven scheme layers. Secondly, by constructing a judgment matrix, the weight coefficients for each scheme layer are derived, and the overall weights for each criterion are obtained. Finally, three experts were invited to rate the carbon emission reduction technologies, resulting in a ranking of their merits and drawbacks as follows: ammonia fuel technology \&gt; carbon capture technology \&gt; wind-assisted propulsion technology \&gt; air layer drag reduction technology \&gt; hydrogen fuel technology. Random Forest model achieves better prediction accuracy compared to Logistic Regression model. The AHP quantitative research method applied in this paper can provide a valuable decision-making support for ship operators.} }
@inproceedings{10.1145/3641554.3701815, title = {PhysioML: A Web-Based Tool for Machine Learning Education with Real-Time Physiological Data}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {485--491}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701815}, url = {https://doi.org/10.1145/3641554.3701815}, author = {Hern\'andez-Cuevas, Bryan Y. and Lewis, Myles and Junkins, Wesley and Crawford, Chris S. and Denham, Andre and Luo, Feiya}, keywords = {computer science education, electromyography (emg), machine learning, muscle computer interfaces, physiological computing, location = Pittsburgh, PA, USA}, abstract = {Artificial Intelligence and Machine Learning continue to increase in popularity. As a result, several new approaches to machine learning education have emerged in recent years. Many existing interactive techniques utilize text, image, and video data to engage students with machine learning. However, the use of physiological sensors for machine learning education activities is significantly unexplored. This paper presents findings from a study exploring students' experiences learning basic machine learning concepts while using physiological sensors to control an interactive game. In particular, the sensors measured electrical activity generated from students' arm muscles. Activities featuring physiological sensors produced similar outcomes when compared to exercises that leveraged image data. While students' machine learning self-efficacy increased in both conditions, students seemed more curious about machine learning after working with the physiological sensor. These results suggest that PhysioML may provide learning support similar to traditional ML education approaches while engaging students with novel interactive physiological sensors. We discuss these findings and reflect on ways physiological sensors may be used to augment traditional data types during classroom activities focused on machine learning.} }
@inproceedings{10.1145/3655497.3655498, title = {The Knowledge Training System Based on Machine Learning Technology}, booktitle = {Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence}, pages = {38--44}, year = {2024}, isbn = {9798400709302}, doi = {10.1145/3655497.3655498}, url = {https://doi.org/10.1145/3655497.3655498}, author = {Gong, Zhansheng and Zhai, Chenggong and Ding, Shunjie and Zhang, Heng and Li, Yan and Huang, Yang}, keywords = {AI big mode, Knowledge graph, knowledge training, Machine learning, location = Tokyo, Japan}, abstract = {With the rapid development of information technology, people's demand for knowledge acquisition and application is becoming increasingly strong. However, with the explosive growth of information, it is difficult for ordinary people to effectively access and process a large amount of information, so knowledge training systems have become an increasingly important field. This article focuses on exploring knowledge training systems based on machine learning technology. By introducing the basic situation of machine learning technology and the requirements of knowledge training systems for machine learning technology, a knowledge graph concept based on machine learning technology is proposed, and a knowledge training system and its evaluation system are constructed on this basis. With the arrival of the big data era and the further development of technology, knowledge training systems based on machine learning technology will continue to evolve, enabling people to obtain the required knowledge more effectively.} }
@inproceedings{10.1145/3677182.3677290, title = {Effective e-commerce price prediction with machine learning technologies}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {603--608}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677290}, url = {https://doi.org/10.1145/3677182.3677290}, author = {Jiang, Kai}, abstract = {The development of e-commerce plays a significant role in improving market efficiency and promoting the optimization of industrial structures. Machine learning technology has been widely used in the exploration and prediction of data in this field. In this study, we aim to investigate the performance of machine learning models for commodity price prediction problems. We used eight different machine learning models to predict commodity prices and discounted prices, and added Bayesian optimization and sentiment analysis to improve the performance of the model. Experimental results showed that RandomForest model had the best comprehensive and prediction performance. In addition, we also found that Bayesian optimization could effectively improve the performance of the models, and the sentiment analysis method did not bring a positive effect in the experiment of predicting discounted prices.} }
@inproceedings{10.1145/3728199.3728216, title = {Research on Aircraft Delivery Forecasting and Supply-demand Matching Optimization Based on Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {115--119}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728216}, url = {https://doi.org/10.1145/3728199.3728216}, author = {Zhang, Yuehuan}, keywords = {Aircraft delivery forecasting, Deep learning, Machine learning, Production scheduling, Supply-demand matching optimization, neural network}, abstract = {This paper proposes a joint optimization model based on deep learning for aircraft delivery forecasting and supply-demand matching. The model processes historical delivery data and market demand characteristics through DNN to predict aircraft delivery, and combines the optimization algorithm to tune production scheduling and resource allocation, thereby achieving accurate matching of supply and demand. Experimental simulation results show that the joint model based on deep learning is significantly better than traditional statistical methods in prediction accuracy. Specifically, the prediction error of the deep learning model is reduced by about 18\%, and compared with the traditional prediction method based on time series, it can better capture the complex nonlinear relationship of market demand. In addition, in terms of supply and demand matching optimization, the model can effectively reduce resource waste, optimize production scheduling, and reduce inventory costs by about 12\%. Compared with traditional optimization methods, the deep learning joint model performs better in convergence speed and optimization effect.} }
@inproceedings{10.1145/3721145.3729514, title = {SmartNIC-GPU-CPU Heterogeneous System for Large Machine Learning Model with Software-Hardware Codesign}, booktitle = {Proceedings of the 39th ACM International Conference on Supercomputing}, pages = {837--852}, year = {2025}, isbn = {9798400715372}, doi = {10.1145/3721145.3729514}, url = {https://doi.org/10.1145/3721145.3729514}, author = {Guo, Anqi and Hao, Yuchen and Yao, Xiteng and Yang, Shining and Huang, Jianyu and Geng, Tony (Tong) and Herbordt, Martin}, keywords = {Heterogeneous System, SmartNIC, Machine learning}, abstract = {The rapid growth of large machine learning models, from billions to trillions of parameters, has led to powerful AI capabilities that increasingly impact everyday life. However, this expansion in model size has surpassed the capacity of GPU memory. As a result, GPU clusters—built by aggregating multiple GPUs—have scaled up significantly to accommodate these models. To address this scalability challenge, and to make large-model training more widely accessible, researchers have proposed heterogeneous systems. These systems leverage CPUs and secondary memory to offload storage and computation onto these devices, thereby reducing the total number of GPUs required for training. Despite their promise, such heterogeneous systems have so far faced challenges in achieving high efficiency and performance.In response, we address this problem with SmartNIC-GPU-CPU (SGC), a heterogeneous system, enhanced with SmartNICs, for training large machine learning models with software-hardware codesign. SGC increases system performance and efficiency while simultaneously reducing power consumption and overall costs.In SGC, SmartNICs serve as an intermediate layer that seamlessly connects the heterogeneous components. By implementing optimization techniques such as prefetching, buffering, and dynamic scheduling and control, SmartNICs streamline the data pipeline, minimizing idle times and overlapping communication with computation. In addition, system configuration software optimizes the system and model settings for maximal efficiency, given different system specifications. Experiments demonstrate that the SGC system achieves an improvement of over 1.6 in training throughput over the baseline for a 100B parameter model.} }
@inproceedings{10.1145/3696843.3696846, title = {SoK Paper: Security Concerns in Quantum Machine Learning as a Service}, booktitle = {Proceedings of the International Workshop on Hardware and Architectural Support for Security and Privacy 2024}, pages = {28--36}, year = {2024}, isbn = {9798400712210}, doi = {10.1145/3696843.3696846}, url = {https://doi.org/10.1145/3696843.3696846}, author = {Kundu, Satwik and Ghosh, Swaroop}, keywords = {Quantum machine learning, training, untrusted providers, security}, abstract = {Quantum machine learning (QML) is a category of algorithms that uses variational quantum circuits (VQCs) to solve machine learning tasks. Recent works have shown that QML models can effectively generalize from limited training data samples. This capability has led to an increased interest in deploying these models to address practical, real-world problems, resulting in the emergence of Quantum Machine Learning as a Service (QMLaaS). QMLaaS represents a hybrid model that utilizes both classical and quantum computing resources. Classical computers play a crucial role in this setup, handling initial pre-processing and subsequent post-processing of data to compensate for the current limitations of quantum hardware. Since this is a new area, very little work exists to paint the whole picture of QMLaaS in the context of known security threats in the domain of classical and quantum machine learning. This SoK paper is aimed to bridge this gap by outlining the complete QMLaaS workflow, which includes both the training and inference phases and highlighting security concerns involving untrusted classical and quantum providers. QML models contain several sensitive assets, such as the model architecture, training data, encoding techniques, and trained parameters. Unauthorized access to these components could compromise the model’s integrity and lead to intellectual property (IP) theft. We pinpoint the critical security issues that must be considered to pave the way for a secure QMLaaS deployment.} }
@inproceedings{10.1145/3746972.3747001, title = {Double Machine Learning - Enabled Analysis of How Digital Transformation Shapes ESG Performance}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {175--180}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747001}, url = {https://doi.org/10.1145/3746972.3747001}, author = {Wu, Yanting}, keywords = {Digital Transformation, Double Machine Learning, ESG Performance, Sustainable development}, abstract = {Digital transformation (DT), propelled by emerging technologies, signifies a strategic realignment of business operations, with the goal of enhancing organizational performance and competitive advantages. It involves the application of innovative digital technologies to comprehensively reshape business models and service delivery mechanisms. Key drivers of DT encompass big data, cloud computing, artificial intelligence, and block chain. In the context of global sustainability imperatives and China's dual carbon targets, this study empirically explores the mechanisms through which information technology - driven digital transformation improves corporate environmental, social, and governance (ESG) performance. Based on 11,800 firm - year observations of Chinese listed companies in heavy - polluting industries from 2010 to 2023, a double machine learning (DML) model is established, and tools such as Stata 19 are employed for data management, statistical analysis and programming. The findings demonstrate that digital transformation significantly enhances corporate ESG performance, with a more pronounced effect on state - owned enterprises. Replacing DML algorithms, including methods like lasso, gradient boosting, and neural network regression, shows that the research findings stay consistent across various model configurations. The present research stands out with its three pivotal contributions. Theoretically, it validates the positive impact of digital transformation on ESG performance for heavy - polluting listed companies in China, enriching the relevant theoretical system. Methodologically, the establishment of the DML model and the empirical analysis framework provide a new approach for similar research. Practically, it offers actionable strategies for emerging economies to integrate information technology into ESG governance, facilitating the construction of a sustainable development model characterized by “technology - enabled empowerment, market - oriented regulation, and environmental synergy,” and promoting the realization of China's “Dual Carbon” goals and broader sustainable development objectives.} }
@inproceedings{10.1145/3719384.3719386, title = {Towards Classification of Covariance Matrices via Bures-Wasserstein-Based Machine Learning}, booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference}, pages = {10--18}, year = {2025}, isbn = {9798400717925}, doi = {10.1145/3719384.3719386}, url = {https://doi.org/10.1145/3719384.3719386}, author = {Zirpoli, Michael and Yi, Yuyan and Lin, Shu-Chin and Ge, Linqiang and Zheng, Jingyi}, keywords = {Bures-Wasserstein distance, Fr\'echet Mean, Random Forest, Riemannian manifold, Support Vector Machine}, abstract = {Spatial-temporal data is a prevalent data type in biomedical domains, encompassing instances like multi-channel EEG and fMRI. In the analysis of such data, the connectivity matrix (e.g., functional connectivity derived from fMRI, covariance matrix derived from EEG) is widely extracted and analyzed. Rather than analyzing these matrices within the Euclidean space, this paper considers each matrix as a point situated on the manifold of positive semi-definite (PSD) matrices coupled with Bures-Wasserstein (BW) metric. Within this framework, two machine learning models based on the BW metric are proposed for the classification of PSD matrices on the manifold. Specifically, projection map techniques, based on the BW metric, have been introduced and integrated into machine learning models such as support vector machines and random forest. In comparison with Euclidean methods, our approach considers the geometry of the Riemannian manifold where PSD matrices reside. Moreover, compared with prevalent Affine-Invariant (AI) metrics, our framework does not require matrix regularization and is computationally efficient. To comprehensively evaluate the proposed methods, four fMRI datasets and three brain-computer interface datasets with varying dimensions and quantities have been utilized. The results demonstrate comparable and even superior performance of the proposed methods compared with Euclidean and AI-based approaches.} }
@article{10.1145/3695986, title = {A Survey of Machine Learning for Urban Decision Making: Applications in Planning, Transportation, and Healthcare}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3695986}, url = {https://doi.org/10.1145/3695986}, author = {Zheng, Yu and Hao, Qianyue and Wang, Jingwei and Gao, Changzheng and Chen, Jinwei and Jin, Depeng and Li, Yong}, keywords = {machine learning, urban planning, optimization, decision making}, abstract = {Developing smart cities is vital for ensuring sustainable development and improving human well-being. One critical aspect of building smart cities is designing intelligent methods to address various decision-making problems that arise in urban areas. As machine learning techniques continue to advance rapidly, a growing body of research has been focused on utilizing these methods to achieve intelligent urban decision-making. In this survey, we conduct a systematic literature review on the application of machine learning methods in urban decision-making, with a focus on planning, transportation, and healthcare. First, we provide a taxonomy based on typical applications of machine learning methods for urban decision-making. We then present background knowledge on these tasks and the machine learning techniques that have been adopted to solve them. Next, we examine the challenges and advantages of applying machine learning in urban decision-making, including issues related to urban complexity, urban heterogeneity, and computational cost. Afterward and primarily, we elaborate on the existing machine learning methods that aim at solving urban decision-making tasks in planning, transportation, and healthcare, highlighting their strengths and limitations. Finally, we discuss open problems and the future directions of applying machine learning to enable intelligent urban decision-making, such as developing foundation models and combining reinforcement learning algorithms with human feedback. We hope this survey can help researchers in related fields understand the recent progress made in existing works, and inspire novel applications of machine learning in smart cities.} }
@inproceedings{10.1145/3696687.3696698, title = {Analysis of vertical tank vertical deformation based on machine learning and point cloud data}, booktitle = {Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, pages = {60--63}, year = {2024}, isbn = {9798400709876}, doi = {10.1145/3696687.3696698}, url = {https://doi.org/10.1145/3696687.3696698}, author = {Liu, Haiyu and Cui, Lifu}, abstract = {As an important oil storage container, the vertical oil storage tank will be deformed due to the change of service age, wind load, pressure change, operation error and other reasons, which leaves a huge hidden danger for the instability or explosion caused by the storage tank collapse and oil leakage. Machine learning method to optimize the point cloud data processing process, the laboratory vibration table experiment tank model as the research object, the tank vertical deformation analysis, through 3 d laser scanning of point cloud data after downsampling, registration, extraction, slice, center fitting and reference point fitting, analyzed the deformation of the tank in the vertical direction (the maximum residual is about 2.21mm), prove that using machine learning method processing point cloud data analysis vertical tank in vertical deformation is fast and accurate.} }
@inproceedings{10.1145/3580305.3599574, title = {Trustworthy Machine Learning: Robustness, Generalization, and Interpretability}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5827--5828}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599574}, url = {https://doi.org/10.1145/3580305.3599574}, author = {Wang, Jindong and Li, Haoliang and Wang, Haohan and Pan, Sinno Jialin and Xie, Xing}, keywords = {adversarial learning, interpretability, out-of-distribution generalization, trustworthy machine learning, location = Long Beach, CA, USA}, abstract = {Machine learning is becoming increasingly important in today's world. Beyond its powerful performances, there has been an emerging concern about the trustworthiness of machine learning, including but not limited to: robustness to malicious attacks, generalization to unseen datasets, and interpretability to explain its outputs. Such concerns are even more urgent in some safety-critical applications such as medical diagnosis and autonomous driving. Trustworthy machine learning (TrustML) aims to tackle these challenges from the perspectives of theory, algorithm, and applications. In this tutorial, we will give a comprehensive introduction to the recent advance of trustworthy machine learning in robustness, generalization, and interpretability. We will cover their problem formulation, related research, popular algorithms, and successful applications. Additionally, we will also introduce some potential challenges for future research. We do hope that this tutorial will not only serve as a platform to understand TrustML, but also raise the awareness of everyone for more trustworthy applications.} }
@article{10.1145/3680463, title = {An Empirical Study of Testing Machine Learning in the Wild}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3680463}, url = {https://doi.org/10.1145/3680463}, author = {Openja, Moses and Khomh, Foutse and Foundjem, Armstrong and Jiang, Zhen Ming (Jack) and Abidi, Mouna and Hassan, Ahmed E.}, keywords = {Machine learning, Deep learning, Software Testing, Machine learning workflow, Testing strategies, Testing methods, ML properties, Test types/Types of testing}, abstract = {Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies.Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow.Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems.Results: Our findings reveal several key insights: (1) The most common testing strategies, accounting for less than 40\%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. (2) A wide range of (17) ML properties are tested, out of which only 20\% to 30\% are frequently tested, including Consistency, Correctness, and Efficiency. (3) Bias and Fairness is more tested in Recommendation (6\%) and Computer Vision (CV) (3.9\%) systems, while Security and Privacy is tested in CV (2\%), Application Platforms (0.9\%), and NLP (0.5\%). (4) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing.Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.} }
@inproceedings{10.1145/3562939.3565688, title = {Visualizing Machine Learning in 3D}, booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology}, year = {2022}, isbn = {9781450398893}, doi = {10.1145/3562939.3565688}, url = {https://doi.org/10.1145/3562939.3565688}, author = {Rivera, Diego}, keywords = {Interactive, Neural Networks, Transformers, location = Tsukuba, Japan}, abstract = {Understanding machine learning models can be difficult when the models at hand have many parts to them. Having a visual model can help aid in understanding how the model functions. A way to visualize these models is to use a 3D (three-dimensional) game development application. An application that will have an interactive element allowing the users to interact with the model (rotating and scaling it) and see changes at run-time. An interactive element will keep the users engaged, understanding, and seeing how a machine learning model looks and behaves. This paper describes the process of visualizing a machine learning model in a 3D application.} }
@article{10.1145/3631326, title = {Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey}, journal = {ACM J. Responsib. Comput.}, volume = {1}, year = {2024}, doi = {10.1145/3631326}, url = {https://doi.org/10.1145/3631326}, author = {Hort, Max and Chen, Zhenpeng and Zhang, Jie M. and Harman, Mark and Sarro, Federica}, keywords = {Fairness, bias mitigation, debiasing, fairness-aware machine learning, classification}, abstract = {This article provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics, and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.} }
@inproceedings{10.1145/3706594.3728869, title = {Leveraging gem5 for Hardware Trojan Research: Simulation for Machine-Learning-Based Detection}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {9--16}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3728869}, url = {https://doi.org/10.1145/3706594.3728869}, author = {Palumbo, Alessandro and Salvador, Ruben}, keywords = {Hardware Trojans, Gem5 Simulator, Microprocessor Security, Machine Learning-Based Anomaly Detection}, abstract = {Hardware Trojans (HTs) consist of malicious modifications intentionally embedded in hardware designs, capable of bypassing security measures, leaking sensitive information, or disrupting system operations. This work proposes a methodology for introducing and simulating HTs on microprocessor-based systems within the gem5 simulator, focusing on RISC-V architectures. By leveraging the gem5 System Emulation (SE) mode, we generate comprehensive datasets encompassing both benign and attack scenarios, automating the collection of 866 features from Hardware Performance Counters (HPC) dumped by gem5 after every program run. The dataset consists of 10,000 samples, evenly split into 5,000 benign and 5,000 attacked instances on different runs across 8 benchmarks. Contrary to previous work using processors with a very limited number of HPCs, we demonstrate how simulation provides access to a much larger number of HPCs and can, hence, enable new research on effective detection mechanisms. In particular, we show preliminary results for one HT and improve previous work on detection mechanisms using Machine Learning (ML). Using a Random Forest (RF) classifier and taking inspiration from the PIC16F84-T100 HT from the Trust-Hub platform as a case study, we achieve 100\% attack detection. Our approach facilitates the security evaluation of microprocessors under HT attacks and provides a framework for experimenting with and analyzing ML-based detection strategies.} }
@inproceedings{10.1145/3670474.3685947, title = {A Parallel Simulation Framework Incorporating Machine Learning-Based Hotspot Detection for Accelerated Power Grid Analysis}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685947}, url = {https://doi.org/10.1145/3670474.3685947}, author = {Jiang, Yangfan and Song, Jianfei and Yin, Xunzhao and Dong, Xiao and Sun, Songyu and Lin, Yibo and Jin, Zhou and Yang, Xiaoyu and Zhuo, Cheng}, keywords = {Power grid, hotspot, machine learning, parallel, location = Salt Lake City, UT, USA}, abstract = {Power grid analysis is essential for integrated circuit design, especially as the scale extends to billions of nodes, making precise calculations across the entire spatial domain prohibitively expensive. Domain decomposition methods (DDM) facilitate block-wise parallel analysis of power distribution networks (PDN) through segmentation, yet they still face bottlenecks in computational load balance during parallel processing. Considering the vastness of PDN structures and the emphasis on verifying areas prone to significant IR drop (i.e., hotspot) rather than those less affected, our research introduces a machine learning (ML)-based method for detecting PDN hotspots. This method focuses on accurately identifying areas within sub-blocks where the hotspot intensity is high. We then propose a novel parallel approximation computing approach that integrates hotspot detection, enabling approximations in regions not marked as hotspots. This strategy optimizes computational resources and effectively accelerates analysis. Our experimental results demonstrate that the hotspot detection model achieves an average accuracy of 94.39\%. The approximation strategy for non-hotspot sub-blocks results in 0.56-1.4\% mean relative error, while achieving an average computational speedup of 3.21 over the CK-TSO parallel solver and 2.21 over the conventional DDM-based solver.} }
@inproceedings{10.1145/3724363.3729028, title = {AfriML: An Interactive and Culturally-Infused Tool for Teaching Machine Learning in Schools}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {23--29}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729028}, url = {https://doi.org/10.1145/3724363.3729028}, author = {Okafor, David Odafe and Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday}, keywords = {afriml, culturally relevant pedagogy, machine learning education, no-code tools, location = Nijmegen, Netherlands}, abstract = {This study developed and assessed the impact of a machine learning (ML) tool called AfriML, a culturally infused, no-code tool designed to engage high school students in learning ML concepts. Inspired by Google's Teachable Machine, AfriML integrates African cultural elements, such as image, accent, and language detectors, to enhance student engagement and relatability. The platform's effectiveness was evaluated through classroom trials involving 40 students and 4 instructors across four schools, employing a mixed-methods approach grounded in Design Science Research (DSR). Results demonstrated significant improvements in student motivation and understanding, highlighting the importance of culturally relevant content in education. Despite resource constraints and technological challenges, AfriML shows promise in making ML education more effective and inclusive. Future work should focus on enhancing language processing capabilities, developing a mobile version for broader accessibility, and extending its application to diverse educational contexts.} }
@inproceedings{10.1145/3673038.3673106, title = {Scheduling Machine Learning Compressible Inference Tasks with Limited Energy Budget}, booktitle = {Proceedings of the 53rd International Conference on Parallel Processing}, pages = {961--970}, year = {2024}, isbn = {9798400717932}, doi = {10.1145/3673038.3673106}, url = {https://doi.org/10.1145/3673038.3673106}, author = {Da Silva Barros, Tiago and Ferre, Davide and Giroire, Frederic and Aparicio-Pardo, Ramon and Perennes, Stephane}, keywords = {deadlines, energy budget, neural network compression, scheduling, location = Gotland, Sweden}, abstract = {Advancements in cloud computing have boosted Machine Learning as a Service (MLaaS), highlighting the challenge of scheduling tasks under latency and deadline constraints. Neural network compression offers the latency and energy consumption reduction in data centers, aligning with efforts to minimize cloud computing’s carbon footprint, despite some accuracy loss. This paper investigates the Deadline Scheduling with Compressible Tasks - Energy Aware (DSCT-EA) problem, which addresses the scheduling of compressible machine learning tasks on several machines, with different speeds and energy efficiencies, under an energy budget constraint. Solving DSCT-EA involves determining both the machine on which each task will be processed and its processing time, a problem that has been proven to be NP-Hard. We formulate DSCT-EA as a Mixed-Integer Programming (MIP) problem and also provide an approximation algorithm for solving it. The efficacy of our approach is demonstrated through extensive experimentation, revealing its superiority over traditional scheduling techniques. It allows to save up to 70\% of the energy budget of image classification tasks, while only losing 2\% of accuracy compared to when not using compression.} }
@inproceedings{10.1145/3589335.3665841, title = {Leveraging Machine Learning Models for Trustworthy Prediction of Diabetes}, booktitle = {Companion Proceedings of the ACM Web Conference 2024}, pages = {1872--1875}, year = {2024}, isbn = {9798400701726}, doi = {10.1145/3589335.3665841}, url = {https://doi.org/10.1145/3589335.3665841}, author = {B, Aruna Devi and N, Karthik}, keywords = {anomalies, diabetes, imbalanced dataset, missing values, xai, location = Singapore, Singapore}, abstract = {A significant and widespread health issue among people of all ages is diabetes. Incorporating machine learning (ML) algorithms in clinical care can assist in the early detection of diabetes and preventing patients from experiencing significant health issues caused by the impact of diabetes. Furthermore, the most recent Explainable Artificial Intelligence (XAI) techniques may enable end users to understand and trust AI decisions. This research proposes an approach to detect diabetes by leveraging ML algorithms. The detection of diabetes allows for prompt medication, dietary, and lifestyle modifications, which enhance blood sugar regulation and lower the risk of complications from diabetes. Building a reliable machine learning model is influenced by several factors, including anomalies, data quality, and model selection. This work emphasizes building a trustworthy model to predict diabetes by overcoming the issues of missing data, anomalies, and appropriate model selection, along with an understanding of the results obtained by the model.} }
@inproceedings{10.1145/3576781.3608735, title = {Towards Molecular Machine Learning for the IoBNT}, booktitle = {Proceedings of the 10th ACM International Conference on Nanoscale Computing and Communication}, pages = {168--169}, year = {2023}, isbn = {9798400700347}, doi = {10.1145/3576781.3608735}, url = {https://doi.org/10.1145/3576781.3608735}, author = {Angerbauer, Stefan and Enzenhofer, Franz and Pankratz, Tobias and Haselmayr, Werner}, keywords = {Internet of Bio-Nano Things, Molecular Machine Learning, Molecular communications, location = Coventry, United Kingdom}, abstract = {In this paper, we present a reaction-diffusion-based architecture, which can be used as basic building block for the implementation of machine learning algorithms at the nano-scale. We present the principle and a first mathematical model and validated the proposed approach through particle-based simulations.} }
@inproceedings{10.1145/3747227.3747255, title = {Intelligent monitoring of insult management based on machine learning and multi-source big data — Application innovation for digital economy}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {167--172}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747255}, url = {https://doi.org/10.1145/3747227.3747255}, author = {Lu, Ling and Xie, Ling and Luo, Renjie and Yuan, Dan and Peng, Ling and Lin, Haibo}, keywords = {Abusive Supervision, Intelligent Monitoring, Machine Learning, Multi-Source Big Data, Organizational Behavior Management}, abstract = {In the context of the digital economy, this study explores an intelligent monitoring framework for abusive supervision based on machine learning and multi-source big data. By integrating support vector machines (SVM) and random forests (RF), the research constructs a fusion model to enhance the identification and early warning of abusive behaviors within organizations. Through data collection from employee behavior records, internal feedback, and social media, the system achieves real-time analysis and high-accuracy risk prediction. Experimental results show that the integrated model outperforms single models in accuracy, precision, recall, and F1 score, especially under imbalanced data conditions. Furthermore, key variables such as job satisfaction, emotional scores, and behavioral frequency were identified as critical indicators for early detection. This study not only validates the application potential of data-driven methods in organizational behavior management but also provides enterprises with a practical intelligent tool to optimize leadership behaviors and improve employee experience.} }
@inproceedings{10.1145/3686081.3686100, title = {Machine Learning-Based Market Segmentation and Consumer Behavior Prediction Models}, booktitle = {Proceedings of the International Conference on Decision Science \&amp; Management}, pages = {122--126}, year = {2024}, isbn = {9798400718151}, doi = {10.1145/3686081.3686100}, url = {https://doi.org/10.1145/3686081.3686100}, author = {Liu, Sheng and Yang, Shixun}, keywords = {Consumer behavior prediction, Feature engineering, K-means clustering, Machine learning, Random Forest}, abstract = {This study aims to explore the application of machine learning technology in market segmentation and consumer behavior prediction. By utilizing large-scale data sets from e-commerce platforms, this paper builds two models: a market segmentation model based on K-means clustering and a consumer behavior prediction model based on random forest. Data preprocessing includes data cleaning, feature engineering, and data standardization, aiming to optimize the quality of model inputs. The market segmentation model divides consumers into different market segments by analyzing their purchasing behavior, age, gender and other characteristics. Consumer behavior prediction models use users’ historical purchase data and personal characteristics to predict their future purchase behavior. Model evaluation is based on precision, recall and F1 scores, while cross-validation and parameter optimization techniques are used to improve the generalization ability of the model. The results show that the random forest model performs better than the K-means clustering model in predicting consumer behavior, proving the effectiveness of machine learning technology in precise market analysis and consumer behavior prediction. This research provides enterprises with a practical framework for using machine learning technology to conduct market analysis and consumer behavior prediction, helping enterprises to better understand market dynamics and consumer needs, thereby formulating more effective market strategies.} }
@inproceedings{10.1145/3580305.3599571, title = {Socially Responsible Machine Learning: A Causal Perspective}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5819--5820}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599571}, url = {https://doi.org/10.1145/3580305.3599571}, author = {Moraffah, Raha and Karimi, Amir-Hossein and Raglin, Adrienne and Liu, Huan}, keywords = {causality, fairness, interpretability, responsible ml, robustness, location = Long Beach, CA, USA}, abstract = {The evergrowing reliance of humans and society on machine learning methods has raised concerns about their trustworthiness and liability. As a response to these concerns, Socially Responsible Machine Learning (SRML) aims at developing fair, transparent, and robust machine learning algorithms. However, traditional approaches to SRML do not incorporate human perspectives, and therefore are not sufficient to build long-lasting trust between machines and human being. Causality as the key to human intelligence plays a vital role in achieving socially responsible machine learning algorithms which are compatible with human notions. Bridging the gap between traditional SRML and causality, in this tutorial, we aim at providing a holistic overview of SRML through the lens of causality. In particular, we will focus on state-of-the-art techniques on causal socially responsible ML in terms of fairness, interpretability, and robustness. The objectives of this tutorial are as follows: (1) we provide a taxonomy of existing literature on causal socially responsible ML from fairness, interpretability, and robustness perspective; (2) we review the state-of-the-art techniques for each task; and (3) we elucidate open questions and future research directions. We believe this tutorial is beneficial to researchers and practitioners from the areas of data mining, machine learning, and social sciences.} }
@inproceedings{10.1145/3650215.3650280, title = {Rethinking on Misleading of Machine Learning in Bank Risk Warning System}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {367--373}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650280}, url = {https://doi.org/10.1145/3650215.3650280}, author = {Xiao, Yan and He, Zehao and Liu, Yuyang}, abstract = {Machine learning is widely used in the bank risk warning system more and more popular. It has been able to provide early internal bank risk warning nowadays. However, its ability of external risk warning is limited and need a long way to go to meet the needs of bank risk warning system. This study shows that external bank risk warning information, especially the subtle information provided by the machine learning would mislead the bank risk warning system. Analyses indicated that machine learning relied heavily on the accuracy of data, typically provided predictive rather than deterministic results, overlooked or underestimated unexpected events during the learning process, and it might provide opposite predictive results when dealing with external subtle information. With the supports of external subtle information filtering criteria and bank simulation training center, machine learning could jointly improve its ability to process external subtle information and reduce the possibilities of misleading in bank risk warning system.} }
@article{10.1145/3722215, title = {I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3722215}, url = {https://doi.org/10.1145/3722215}, author = {Lewis, Noah and Bez, Jean Luca and Byna, Suren}, keywords = {I/O access pattern, HPC I/O, storage, machine learning}, abstract = {Growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference. This demand for speed has prompted the use of high performance computing (HPC) systems that excel in managing distributed workloads. Because data is the main fuel for AI applications, the performance of the storage and I/O subsystem of HPC systems is critical. In the past, HPC applications accessed large portions of data written by simulations or experiments or ingested data for visualizations or analysis tasks. ML workloads perform small reads spread across a large number of random files. This shift of I/O access patterns poses several challenges to modern parallel storage systems. In this article, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024. We define the scope of the survey, provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during offline data preparation, training, and inference, and explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature. Lastly, we seek to expose research gaps that could spawn further R\&amp;D.} }
@inproceedings{10.1145/3715020.3715058, title = {Redefined Classification of Hepatitis Variants through Arima, Signal Processing and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computational Biology and Bioinformatics}, pages = {136--143}, year = {2025}, isbn = {9798400709623}, doi = {10.1145/3715020.3715058}, url = {https://doi.org/10.1145/3715020.3715058}, author = {Shah, Vatsalkumar Vipulkumar and Fadia, Love and Hassanzadeh, Mohammad and Ahmadi, Majid and Wu, Jonathan}, keywords = {Genomic Data Analysis, Hepatitis Virus, Machine learning, ARIMA and Finite Impulse Response Filters}, abstract = {This study introduces a novel feature selection method using the Auto Regressive Moving Average model to classify four hepatitis virus types: Hepatitis B, C, D, and E. Firstly, DNA sequences are transformed from characters into numerical representations using Electron-Ion Interaction Potential coding. Then discrete sine transform is applied to extract features from the sequences. Our proposed feature selection technique, using the inverse integrated moving average (ARIMA) as a statistical filter, refines the features for optimal classification. We train two machine learning models Light Gradient Boosting Machine and Random Forest on the selected features. We compare the results of our approach with another 5 window-based finite impulse response filters (FIR) as they extract significant features by reducing noise and emphasizing key patterns. Our findings indicate that ARIMA, with an order of (1,1,5), achieves 98\% accuracy, while the Bartlett window-based filter yields 93.25\% accuracy. This approach highlights the potential of ARIMA for effective feature selection in the classification of hepatitis viruses using machine learning.} }
@inproceedings{10.1145/3731806.3731834, title = {Enhancing Global Air Quality Classification Using Efficient Machine Learning Techniques}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {284--289}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731834}, url = {https://doi.org/10.1145/3731806.3731834}, author = {Putra Mulyana, Muhammad Haikel Nur Kamil and Hikmawati, Erna and Mandasari, Rizza Indah Mega}, keywords = {Air Quality Prediction, Decision Tree, Feature Selection, Information Gain, K-Nearest Neighbors}, abstract = {Air quality problems have become a global problem due to an important impact on human and environmental health. The goal of this research is to improve the accuracy of the air quality prediction using the method of learning with the machine. How to select the features that are used to receive data are used to identify the most involved parameters such as carbon monoxide (CO), sulfur dioxide (SO2), nitrogen dioxide (NO2), the temperature is similar to the industrial area and measurement. Efficiency, such as AUC and MCC K-Nearest, two, the main algorithm called neighbors (KNN) is used to show that the decision-making plan is the best results with AUC 0.9380 and CA at 0.8820 while KNN also. There is significant improvement after selecting this method of choosing this method not only But improving the analysis of efficiency data but still has more accurate predictions This research is a trend for further development, including the application of all the methods or the combination of additional parameters to solve more complex challenges in the future.} }
@inproceedings{10.1145/3748825.3748912, title = {Factors Influencing Rural Migrants' Intentions for Urban Household Registration: An Interpretable Machine Learning Analysis}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {558--563}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748912}, url = {https://doi.org/10.1145/3748825.3748912}, author = {Rao, Jiawen}, keywords = {SHAP value, intentions of urban household registration, machine learning, rural migrants}, abstract = {Leveraging 2017 China Migrants Dynamic Survey (CMDS) data and machine learning algorithms (KNN, GBDT, XGBoost, LightGBM, BP neural networks) with SHAP interpretability, this study investigates determinants of rural migrants' urban household registration intentions. Findings reveal heightened registration intentions in proximity to hometowns and in a state of high psychological identification, but constrained by rural land tenure and housing affordability challenges. Notably, stable urban housing does not uniformly translate to registration transfer. Informed by these insights, policy recommendations emphasize achieving people-centered urbanization: (a) fostering localized urbanization; (b) strengthening socio-psychological integration; (c) establishing flexible land-rights mechanisms; and (d) enhancing housing security.} }
@inproceedings{10.1145/3625007.3632288, title = {pyStudio: An Open-Source Machine Learning Platform}, booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining}, pages = {436--440}, year = {2024}, isbn = {9798400704093}, doi = {10.1145/3625007.3632288}, url = {https://doi.org/10.1145/3625007.3632288}, author = {Gomicia-Murccia, Enrique and Souissi, Riad and AL-Qurishi, Muhammad and Bordel S\'anchez, Borja}, abstract = {Data analytics has emerged as a critical capability for businesses and organizations in the modern era. The abundance of data necessitates a deep understanding and the exploitation of its potential to gain insights into current and future scenarios.This paper introduces an integrated platform designed to streamline data acquisition, storage, management, processing, and visualization. The primary objective is to facilitate data analysis by offering a machine learning studio equipped with pre-built algorithms. Remarkably, this platform eliminates the need for coding, allowing users to effortlessly generate AI models. Furthermore [19], it provides a secure environment for sharing these models without compromising data privacy---a noteworthy contribution in the realm of federated learning (FL).The platform's significance lies in its ability to empower nontechnical users to perform advanced tasks without requiring specialized expertise.} }
@inproceedings{10.1145/3689236.3689890, title = {Personalized Online Training Method for Power Dispatchers Based on Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {677--685}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3689890}, url = {https://doi.org/10.1145/3689236.3689890}, author = {Liu, Guiqing and Chen, Manqi and Zhang, Lei and Lei, Xiaoyu and Wang, Xunshi and Ling, Xinglong}, keywords = {Learning Path, Machine Learning, Online Training, Personalized Learning, Power Dispatcher}, abstract = {To fully utilize online course resources and optimize the job training for power dispatchers, this paper proposes a personalized learning path recommendation algorithm based on machine learning. The paper employs the Deep Q-learning algorithm to construct a recommendation model aimed at suggesting a series of knowledge points to learners, with the ultimate goal of improving learning efficiency. By analyzing personalized data such as the learner's profession, job requirements, and skill level, the process involves job and skill analysis, course module construction, and personalized learning path design. This approach achieves a more efficient and scientific personalized education model, helping learners to achieve their learning goals with minimal cost.} }
@article{10.1145/3649841, title = {TorchQL: A Programming Framework for Integrity Constraints in Machine Learning}, journal = {Proc. ACM Program. Lang.}, volume = {8}, year = {2024}, doi = {10.1145/3649841}, url = {https://doi.org/10.1145/3649841}, author = {Naik, Aaditya and Stein, Adam and Wu, Yinjun and Naik, Mayur and Wong, Eric}, keywords = {Machine Learning, Integrity Constraints, Query Languages}, abstract = {Finding errors in machine learning applications requires a thorough exploration of their behavior over data. Existing approaches used by practitioners are often ad-hoc and lack the abstractions needed to scale this process. We present TorchQL, a programming framework to evaluate and improve the correctness of machine learning applications. TorchQL allows users to write queries to specify and check integrity constraints over machine learning models and datasets. It seamlessly integrates relational algebra with functional programming to allow for highly expressive queries using only eight intuitive operators. We evaluate TorchQL on diverse use-cases including finding critical temporal inconsistencies in objects detected across video frames in autonomous driving, finding data imputation errors in time-series medical records, finding data labeling errors in realworld images, and evaluating biases and constraining outputs of language models. Our experiments show that TorchQL enables up to 13x faster query executions than baselines like Pandas and MongoDB, and up to 40\% shorter queries than native Python. We also conduct a user study and find that TorchQL is natural enough for developers familiar with Python to specify complex integrity constraints.} }
@inproceedings{10.1145/3716554.3716579, title = {Accelerating Edge Intelligence: Challenges and Future Directions in Hardware-Driven Machine Learning}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {168--173}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716579}, url = {https://doi.org/10.1145/3716554.3716579}, author = {Batzolis, Eleftherios and Karampatzakis, Dimitris}, keywords = {Hardware Accelerators, Machine Learning, Artificial Intelligence, Embedded Systems, Neural Network Optimization}, abstract = {The joining of hardware speed-up for machine learning (ML) in small devices shows a very important step in technology growth. This ability to run complicated ML models on low-power systems opens up many uses, like Internet of Things (IoT) gadgets, robots and different edge computing tasks. This paper studies the present condition of hardware speed-up for ML in small devices, clarifies difficulties and looks at future paths for progress.} }
@inproceedings{10.1145/3674912.3674915, title = {Machine Learning Models for Advanced Air Quality Prediction}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {51--56}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674915}, url = {https://doi.org/10.1145/3674912.3674915}, author = {Sadriddin, Zuhra and Mekuria, Remudin Reshid and Gaso, Mekia Shigute}, keywords = {AQI, Data Analysis, Long Short-Term Memory (LSTM), Machine Learning, Random Forest Regression (RFR), location = Ruse, Bulgaria}, abstract = {Air quality stands as a pivotal factor influencing public health and well-being, shaping urban planning strategies, health management practices, and environmental policies. Having meticulously scrutinized PM2.5 concentration data from 42 monitoring stations across Bishkek, we have employed meteorological factors, including temperature and humidity from these stations to model air quality. We have thus implemented two distinct prediction methodologies namely, Random Forest Regression (RFR) and Long Short-Term Memory (LSTM). Our findings unequivocally favored RFR as the superior predictor for PM2.5 values, achieving accuracy levels of up to 90\%.} }
@inproceedings{10.1145/3757749.3757766, title = {Research on the Prediction Model of Basketball Player Rehabilitation Efficiency Based on Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {102--106}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757766}, url = {https://doi.org/10.1145/3757749.3757766}, author = {Yan, Sheng and Liu, Linjun}, keywords = {basketball, injury prediction, machine learning, rehabilitation efficiency, ridge regression}, abstract = {This paper proposes a prediction model of injury rehabilitation efficiency based on machine learning for the common sports injury problems in high-intensity basketball. The study uses a data set containing athlete biomechanical data, injury information and rehabilitation results, and establishes an effective rehabilitation efficiency prediction system by comparing the performance of multiple regression algorithms. Experimental analysis shows that the ridge regression model achieves the best effect in predicting rehabilitation efficiency, with a root mean square error (RMSE) of 0.157 and a mean absolute error (MAE) of 0.136. SHAP (SHapley Additive exPlanations) analysis further reveals the important influence of biomechanical factors such as knee angle, height, weight and jump height on rehabilitation efficiency. The results of this study can provide decision support for sports medicine experts to formulate personalized rehabilitation plans, which is helpful to improve the efficiency of injury rehabilitation and reduce the risk of injury recurrence.} }
@inproceedings{10.1145/3701100.3701121, title = {Machine learning methods for identifying destructive texts in the Kazakh language}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {99--103}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701121}, url = {https://doi.org/10.1145/3701100.3701121}, author = {Bolatbek, Milana and Sagynay, Moldir and Mussiraliyeva, Shynar and Zhastay, Yeltay}, keywords = {Bag of Words, NLTK, TF-IDF, bullying, cyber security, destructive messages, machine learning, national extremism, racism, text classification, text corpora, violent extremism, word2vec}, abstract = {In the digital age, society is faced with an increasing number of destructive messages on the Internet, including insults, racism, violent extremism and national extremism. This paper presents a machine learning-based approach to classify destructive texts in the Kazakh language, collected from social networks. During the study a text corpora has been accumulated on social networks: YouTube, Vkontakte, Telegram. The study uses advanced text vectorization methods, including Bag of Words, TF-IDF, and Word2Vec, to preprocess text data. Our findings demonstrate that Logistic Regression outperforms other models, achieving an accuracy of 86\%. These results contribute to the development of automated systems for identifying destructive content, promoting safer online communication."} }
@inproceedings{10.1145/3674912.3674937, title = {Machine Learning Algorithms for Cyber Attack Detection And Classification}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {29--36}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674937}, url = {https://doi.org/10.1145/3674912.3674937}, author = {Note, Johan and Mullalli, Erind and CICO, BETIM}, abstract = {As a result of the accelerated development and expansion of technology in the present day, a new concern has emerged: cyberattacks. This has generated significant concern across various domains globally, leading to considerable disruption in networks and presenting PC users with a multitude of challenges. Presently, a multitude of organisations are striving to combat these types of cyber-attacks through the implementation of novel detection and subsequent destruction methods. The domain of machine learning enables computers to acquire knowledge and skills without requiring explicit programming. There is an abundance of implementation strategies for this technology. This study aims to demonstrate a diverse array of algorithms utilised in the defense against various cyber-attacks. This paper will examine various classification algorithms utilised to defend against diverse cyber-attacks, as well as the methods of defense against these attacks. The implementation, accuracy, and testing time of these algorithms will vary depending on the classification of the attack. This thesis will discuss various varieties of these algorithms.} }
@inproceedings{10.1109/WSESE66602.2025.00016, title = {Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?}, booktitle = {Proceedings of the 2025 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering}, pages = {56--63}, year = {2025}, isbn = {9798331502256}, doi = {10.1109/WSESE66602.2025.00016}, url = {https://doi.org/10.1109/WSESE66602.2025.00016}, author = {Costalonga, Marcelo and Napole\~ao, Bianca Minetto and Baldassarre, Maria Teresa and Felizardo, Katia Romero and Steinmacher, Igor and Kalinowski, Marcos}, keywords = {systematic review automation, selection of studies, machine learning, systematic literature review update, location = Ottawa, Ontario, Canada}, abstract = {[Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort. Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence. [Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates. [Method] We reproduce the study selection of an SLR update performed by three SE researchers. We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR. We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure. We also compared the performance of human-ML pairs with human-only pairs when selecting studies. [Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation. However, we found that such models can reduce the study selection effort by 33.9\% without loss of evidence (keeping a 100\% recall). Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result. [Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase.} }
@inproceedings{10.1145/3700906.3700998, title = {Applying Machine Learning Technology for Weather Forecasting: A Case Study of the Logistic Regression Model}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {572--576}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700998}, url = {https://doi.org/10.1145/3700906.3700998}, author = {Shen, Kaiwei}, keywords = {Artificial intelligence, Data preprocessing, Logistic regression model, Weather forecasting}, abstract = {This study is focused on improving the dependability and precision of weather forecasting by employing the capabilities of Artificial Intelligence. Specifically, this study utilizes Logistic Regression and Machine Learning techniques to forecast weather, demonstrating the potential in optimizing weather-related activities and disaster management strategies. The study relies on comprehensive weather data observed over several years, sourced from Kaggle, and handles missing data and outliers during its pre-processing stages. The primary machine learning tool applied is Logistic Regression, followed by a stepwise feature selection to identify influential features for accurate weather prediction. The workflow also involves data collection, pre-processing, model building, training, and testing, with provisions for handling both numeric and categorical features along with imputations. The accuracy, precision, and recall of the prediction module are tested using appropriate statistical tools. The Logistic Regression model, upon implementation, demonstrated considerable accuracy, with an ability to predict rainy days and non-rainy days efficiently. An analytical approach was used to examine the model's sensitivity towards the removal of each feature, thereby ascertaining the relative importance of each. Critical predictors like 'Rainfall', 'Pressure9am', and 'WindGustSpeed' exhibited significant effects on the probability of rain. Overall, the use of Logistic Regression and Machine Learning techniques notably improved rain prediction, offering potential for further advancements in the field of weather forecasting.} }
@inbook{10.1145/3658617.3697772, title = {A Hybrid Machine Learning and Numeric Optimization Approach to Analog Circuit Deobfuscation}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {801--807}, year = {2025}, isbn = {9798400706356}, url = {https://doi.org/10.1145/3658617.3697772}, author = {Jain, Dipali Deepak and Zhao, Guangwei and Datta, Rajesh Kumar and Shamsi, Kaveh}, abstract = {Oracle-guided circuit deobfuscation (or learning) is the problem of disambiguating an obfuscated (partially hidden) circuit given black-box access to it. This has applications in various hardware security areas such as analyzing the security of circuit obfuscation defense schemes, side-channel analysis, reverse engineering, and hardware Trojan detection. Generic deobfuscation of analog circuits has received less attention than the digital counterpart with existing methods relying on manual expert work to extract closed-form equations from the circuit. In this work, we move towards a significantly more automated process by using a combination of machine learning and Newton-method-based analog circuit optimization. We showcase how this hybrid scheme is superior to either standalone approach in terms of runtime and accuracy on a set of analog circuits that include amplifiers, filters, and oscillators. We achieve \&gt;98\% average accuracy without any manual expert equation extraction in addition to demonstrating a superior resilience to process variation.} }
@inproceedings{10.1145/3712256.3726462, title = {Machine Learning-Assisted Constraint Handling Under Variable Uncertainty for Preference-based Multi-Objective Optimization}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {508--516}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3726462}, url = {https://doi.org/10.1145/3712256.3726462}, author = {Yadav, Deepanshu and Ramu, Palaniappan and Deb, Kalyanmoy}, keywords = {evolutionary algorithms, multi-criteria decision-making, machine learning, reference point, reliability, uncertainty, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Evolutionary Multi-objective Optimization (EMO) algorithms are widely used to solve real-world multi-objective optimization problems, aiming to obtain a set of non-dominated solutions close to the Pareto front. However, most EMO methods assume deterministic decision variables, ignoring inherent uncertainties in engineering applications, which can lead to design failures, especially in reliability-based designs. Reliability-based Multi-objective Optimization (ReMOO) addresses this issue by incorporating variable uncertainty and probabilistic constraints to generate a Reliable Front. ReMOO operates using a bi-level framework: the outer level optimizes objective functions, while the inner level estimates reliability through computationally intensive methods, like Monte Carlo Simulation (MCS) or the Performance Measure Approach (PMA). Additionally, decision-makers (DMs) often select only a subset of reliable solutions, limiting computational efficiency. To overcome these challenges, this paper proposes a Machine Learning-assisted reliability-based Multi-Criteria Decision-Making (ML-ReMCDM) technique. ML models are trained on reliability-based constraints within the decision space before an EMO execution. In the inner loop, ML models predict probabilistic constraints and reliability indices, significantly reducing computational costs. Moreover, the outer loop computes only the DM-preferred segment of the reliable front, further enhancing efficiency. The ML-ReMCDM approach, implemented on several benchmark and real-world examples, demonstrates substantial improvements in computational efficiency as well as practical applicability.} }
@inproceedings{10.1145/3664647.3680665, title = {AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models}, booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia}, pages = {8586--8594}, year = {2024}, isbn = {9798400706868}, doi = {10.1145/3664647.3680665}, url = {https://doi.org/10.1145/3664647.3680665}, author = {Luo, Daqin and Feng, Chengjian and Nong, Yuxuan and Shen, Yiqing}, keywords = {automated machine learning, human-ai interaction, large language model, usability, user study, location = Melbourne VIC, Australia}, abstract = {Automated Machine Learning (AutoML) offers a promising approach to streamline the training of machine learning models. However, existing AutoML frameworks are often limited to unimodal scenarios and require extensive manual configuration. Recent advancements in Large Language Models (LLMs) have showcased their exceptional abilities in reasoning, interaction, and code generation, presenting an opportunity to develop a more automated and user-friendly framework. To this end, we introduce AutoM3L, an innovative Automated Multimodal Machine Learning framework that leverages LLMs as controllers to automatically construct multimodal training pipelines. AutoM3L comprehends data modalities and selects appropriate models based on user requirements, providing automation and interactivity. By eliminating the need for manual feature engineering and hyperparameter optimization, our framework simplifies user engagement and enables customization through directives, addressing the limitations of previous rule-based AutoML approaches. We evaluate the performance of AutoM3L on six diverse multimodal datasets spanning classification, regression, and retrieval tasks, as well as a comprehensive set of unimodal datasets. The results demonstrate that AutoM3L achieves competitive or superior performance compared to traditional rule-based AutoML methods. Furthermore, a user study highlights the user-friendliness and usability of our framework, compared to the rule-based AutoML methods.} }
@inproceedings{10.1145/3647444.3652464, title = {Detection of Pneumonia using Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652464}, url = {https://doi.org/10.1145/3647444.3652464}, author = {Bhattarai, Pankaj and K a, Varun Kumar and Th, Balachander and Od, Rashmi}, keywords = {Deep Learning, Image Segmentation, Medical Imaging, Pneumonia, X-Ray Images, location = Jaipur, India}, abstract = {Pneumonia is a severe respiratory infection that can cause life-threatening complications if left untreated. Although an early and accurate diagnosis is essential for successful treatment, conventional techniques of diagnosis can be expensive and time-consuming. For the automated detection of pneumonia from medical images, deep learning algorithms and computer vision methods have recently been investigated. In this study, we suggest a system for automatically identifying pneumonia from chest X-ray images using deep learning algorithms. In order to categorize chest X-ray pictures as either pneumonia-positive or pneumonia-negative, we will create a convolutional neural network (CNN) model that will be trained on a dataset of chest X-ray images. Using different assessment metrics, such as accuracy, sensitivity, and specificity, we will assess the model's performance. Particularly in re-source-constrained settings with a shortage of skilled medical personnel, the suggested system has the potential to greatly improve the effectiveness and accuracy of pneumonia diagnosis. The system is a useful instrument for the medical community because of its capacity to provide early and accurate diagnosis, which may be able to save lives and enhance patient outcomes to a considerable degree.} }
@inproceedings{10.1145/3724154.3724317, title = {A Comparative Research on Stock Selection Strategy based on Machine Learning Models}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {997--1001}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724317}, url = {https://doi.org/10.1145/3724154.3724317}, author = {Jiang, Yulai}, keywords = {Machine learning, Quantitative Investment, Stock price forecasting}, abstract = {This paper aims to develop efficient prediction techniques using machine learning models. In this paper, We have extensive access to market trading and technical indicator data, and a sliding window method is used to better capture more time series information. To identify the precise forecasting approach, we employed multiple algorithms like Linear Regression, Random Forest and XGBoost, etc,. After the daily rotation stock selection models were constructed, we use the investing performance indicators such as annualized return, sharpe ratio, and maximum drawdown to evaluate and compare the efficiency of difference models.} }
@inproceedings{10.1145/3671151.3671158, title = {Progress of machine learning potentials for material atomic simulation}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {33--38}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671158}, url = {https://doi.org/10.1145/3671151.3671158}, author = {Zheng, Guikai and Zhu, Min and Xu, Zijian and Liu, Chao and Yin, Hao and Sun, Peng}, abstract = {How to simulate material phenomena of large time-scales and system with high accuracy at low computational cost is the driving force for the development of simulation techniques in computational materials science. The study of complex large-scale natural phenomena needs the support of fast and accurate computational methods. Since the data-driven machine learning potential has been proposed in 2007, it has attracted wide attention because of its advantages of high precision and high computational efficiency. In this review, we present the key construction process of machine learning potential and its application in materials science, discuss the current challenges, and point out future directions.} }
@article{10.1145/3511543, title = {Steampunk Machine Learning: Victorian contrivances for modern data science}, journal = {Queue}, volume = {19}, pages = {5--17}, year = {2022}, issn = {1542-7730}, doi = {10.1145/3511543}, url = {https://doi.org/10.1145/3511543}, author = {Kelly, Terence}, abstract = {Fitting models to data is all the rage nowadays but has long been an essential skill of engineers. Veterans know that real-world systems foil textbook techniques by interleaving routine operating conditions with bouts of overload and failure; to be practical, a method must model the former without distortion by the latter. Surprisingly effective aid comes from an unlikely quarter: a simple and intuitive model-fitting approach that predates the Babbage Engine. The foundation of industrial-strength decision support and anomaly detection for production datacenters, this approach yields accurate yet intelligible models without hand-holding or fuss. It is easy to practice with modern analytics software and is widely applicable to computing systems and beyond.} }
@inproceedings{10.1145/3728985.3728997, title = {Modified Particle Swarm Optimization and Machine Learning for Solving Handwritten English Characters}, booktitle = {Proceedings of the 2024 10th International Conference on Robotics and Artificial Intelligence}, pages = {45--50}, year = {2025}, isbn = {9798400717451}, doi = {10.1145/3728985.3728997}, url = {https://doi.org/10.1145/3728985.3728997}, author = {Ratanavilisagul, Chiabwoot}, keywords = {Handwritten English Characters, Machine Learning, Particle Swarm Optimization, Support Vector Machine}, abstract = {The Handwritten English Recognition Problem (HERP) involves interpreting handwritten English text from sources such as documents and answer sheets. This problem is particularly challenging and complex due to the uniqueness of each person's handwriting, along with several factors that influence the interpretation process. Recently, many researchers have proposed using the Histogram of Oriented Gradients (HOG) technique or the combination of Discrete Wavelet Transform and Discrete Cosine Transform (DWT-DCT) for feature extraction. Additionally, researchers have explored machine learning techniques to address this problem, with experimental results showing that machine learning can effectively solve it. Therefore, this research proposes using machine learning methods such as Support Vector Machine (SVM), Random Forest (RF), and Logistic Regression (LR), combined with HOG or DWT-DCT for feature extraction, to tackle this challenge. Experimental results from this research indicate that SVM achieves the best outcomes. Consequently, this research enhances SVM by using Particle Swarm Optimization (PSO) to optimize SVM parameters, improving model performance and achieving a higher recognition rate. Experiments were conducted on two datasets: the EMNIST dataset and a personalized handwriting dataset from 30 participants. The results demonstrate that the proposed method, which employs HOG for feature extraction and PSO-optimized SVM, achieves superior accuracy compared to other methods, underscoring its effectiveness in HERP.} }
@inproceedings{10.1145/3615366.3622793, title = {Advanced Machine Learning for Runtime Data Generation}, booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing}, pages = {182--187}, year = {2023}, isbn = {9798400708442}, doi = {10.1145/3615366.3622793}, url = {https://doi.org/10.1145/3615366.3622793}, author = {Zamir, Bukhtawar and Campos, Jo\~ao R. and Vieira, Marco}, keywords = {Artificial Intelligence, Generative Models, Machine Learning, location = La Paz, Bolivia}, abstract = {Given the ubiquity of software in everyday critical tasks, ensuring its dependability is of utmost importance. Software faults, which can lead to errors and vulnerabilities, can significantly comprise the target system. Various techniques have been developed to improve the dependability of software-intensive systems, from fault avoidance to fault tolerance. Machine Learning (ML) techniques have been playing a vital role in improving the dependability of systems. Nonetheless, such techniques require significant amounts of data, which are not typically available. To overcome this, various techniques, such as fault injection or intrusion injection, have been proposed to generate realistic data. Still, they are computationally expensive and require considerable expertise. At the same time, a recent growing sub-field of ML is generative models. Generative models offer an innovative solution by creating synthetic data that closely resemble real-world samples. If such models could be used to generate realistic synthetic failure or intrusion data on demand, their value would be significant. Notwithstanding, the feasibility of such an approach has not yet been researched. Generative models have only mostly been used for sequential data (e.g., text or music) or data with high spatial dependency (e.g., images). On the other hand, dependability problems often have high dimensional tabular data, for which generative models are yet to excel, and for which it is also considerably more difficult to assess the representativeness of the generated data. This research will focus on determining the feasibility of using generative techniques to generate runtime data to support dependability research.} }
@inproceedings{10.1145/3675417.3675484, title = {Analysis of Random Initialization Methods in Machine Learning}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {405--409}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675484}, url = {https://doi.org/10.1145/3675417.3675484}, author = {Wei, Yucheng and Li, Xiangdong and Lang, Fengyong and Wang, Yi and Ma, Teng}, abstract = {Machine learning is an important field in artificial intelligence. In machine learning, the process of randomly initializing model parameters during model training is very important because good random initialization can help the model converge to better solutions faster. The choice of initialization method depends on factors such as the structure of the model, the nature of the data, and the activation function used. In practice, people usually try different initialization methods and choose the best initialization strategy based on the performance of the model. Through analysis, it was found that in most cases, random initialization methods use normal distribution to initialize parameters, while uniform distribution is used to initialize parameters when each value has an equal probability.} }
@inproceedings{10.1145/3611643.3616352, title = {Can Machine Learning Pipelines Be Better Configured?}, booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages = {463--475}, year = {2023}, isbn = {9798400703270}, doi = {10.1145/3611643.3616352}, url = {https://doi.org/10.1145/3611643.3616352}, author = {Wang, Yibo and Wang, Ying and Zhang, Tingwei and Yu, Yue and Cheung, Shing-Chi and Yu, Hai and Zhu, Zhiliang}, keywords = {Empirical Study, Machine Learning Libraries, location = San Francisco, CA, USA}, abstract = {A Machine Learning (ML) pipeline configures the workflow of a learning task using the APIs provided by ML libraries. However, a pipeline’s performance can vary significantly across different configurations of ML library versions. Misconfigured pipelines can result in inferior performance, such as poor execution time and memory usage, numeric errors and even crashes. A pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries. We refer to such performance inconsistency as a pipeline configuration (PLC) issue. There is no prior systematic study on the pervasiveness, impact and root causes of PLC issues. A systematic understanding of these issues helps configure effective ML pipelines and identify misconfigured ones. In this paper, we conduct the first empirical study of PLC issues. To better dig into the problem, we propose Piecer, an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ML libraries and compares their performance inconsistencies. We apply Piecer to the 3,380 pipelines that can be deployed out of the 11,363 ML pipelines collected from multiple ML competitions at Kaggle platform. The empirical study results show that 1,092 (32.3} }
@proceedings{10.1145/3757110, title = {CMNM '25: Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, year = {2025}, isbn = {9798400714344} }
@inbook{10.1145/3757749.3757776, title = {Machine Learning-XGBoost Analysis of Subjective Well-being Among Chronic Hepatitis B Patients}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {165--170}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757776}, author = {Li, Lala and Zhou, Jie}, abstract = {This study examined the relationships among objective social support, subjective social support, social participation, self-efficacy, and subjective well-being in chronic hepatitis B (CHB) patients using the XGBoost machine learning algorithm. Data were collected from 253 CHB patients. Using an optimal hyperparameter search, the XGBoost model achieved a classification accuracy of 98.04\%. The results indicated that objective support, subjective support, self-efficacy, and social participation significantly predicted subjective well-being. XGBoost also highlighted self-efficacy as the most crucial predictive factor. These findings emphasize targeted psychological interventions to enhance self-efficacy and social support among CHB patients.} }
@inproceedings{10.1145/3643651.3659899, title = {Modeling and Security Analysis of Attacks on Machine Learning Systems}, booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics}, pages = {1--2}, year = {2024}, isbn = {9798400705564}, doi = {10.1145/3643651.3659899}, url = {https://doi.org/10.1145/3643651.3659899}, author = {Singhal, Anoop}, keywords = {deep learning. security analysis, machine learning, location = Porto, Portugal}, abstract = {The past several years have witnessed rapidly increasing use of machine learning (ML) systems in multiple industry sectors. Since security analysis is one of the most essential parts of the real-world ML system protection practice, there is an urgent need to conduct systematic security analysis of ML systems. However, it is widely recognized that the existing security analysis approaches and techniques, which were developed to analyze enterprise (software) systems and networks, are no longer very suitable for analyzing ML systems. In this paper, we present a methodology for ML-system-specific security analysis.} }
@inproceedings{10.1145/3641525.3663629, title = {Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines}, booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability}, pages = {101--109}, year = {2024}, isbn = {9798400705304}, doi = {10.1145/3641525.3663629}, url = {https://doi.org/10.1145/3641525.3663629}, author = {Bhaskar, Adhithya and Stodden, Victoria}, keywords = {Computational Reproducibility, CyberInfrastructure, Machine Learning, Open Code, Open Data, ReproScore, Reproducibility Policy, Reproscreener, location = Rennes, France}, abstract = {The increasing reliance on machine learning models in scientific research and day-to-day applications – and the near-opacity of their associated computational methods – creates a widely recognized need to enable others to verify results coming from Machine Learning Pipelines. In this work we use an empirical approach to build on efforts to define and deploy structured publication standards that allow machine learning research to be automatically assessed and verified, enabling greater reliability and trust in results. To automate the assessment of a set of publication standards for Machine Learning Pipelines we developed Reproscreener; a novel, open-source software tool (see https://reproscreener.org/). We benchmark Reproscreener’s automatic reproducibility assessment against a novel manually labeled “gold standard” dataset of machine learning arXiv preprints. Our empirical evaluation has a dual goal: to assess Reproscreener’s performance; and to uncover gaps and opportunities in current reproducibility standards. We develop reproducibility assessment metrics we called the Repo Metrics to provide a novel overall assessment of the re-executability potential of the Machine Learning Pipeline, called the ReproScore. We used two approaches to the automatic identification of reproducibility metrics, keywords and LLM tools, and found the reproducibility metric evaluation performance of Large Language Model (LLM) tools superior to keyword associations.} }
@inproceedings{10.1145/3653724.3653776, title = {Data feature analysis for blast furnace temperature prediction from machine learning perspective}, booktitle = {Proceedings of the International Conference on Mathematics and Machine Learning}, pages = {298--303}, year = {2024}, isbn = {9798400716973}, doi = {10.1145/3653724.3653776}, url = {https://doi.org/10.1145/3653724.3653776}, author = {Duan, Junyi}, abstract = {Modern blast furnace ironmaking technology mainly uses the thermal state of the furnace cylinder to reflect the furnace temperature conditions. However, due to the complexity of the blast furnace smelting process, it is very difficult to modelling and control this process effectively. Therefore, it is important to carry out research on blast furnace temperature prediction modelling in order to realize early warning of furnace health condition in production systems, where nowadays more and more attentions are paid to technologies of machine learning and deep learning. Taking the actual application scenario of a large iron and steel production enterprise as a case study, this paper focuses on the lack of robustness when training machine learning models, due to the noise in the original collected business data. The experimental results show that the application of feature engineering, including feature construction, key feature analysis, and feature ranking within different data analysis stages, is able to improve the quality of the collected raw business data, consequently it is helpful in solving practical engineering problems from machine learning perspective.} }
@inproceedings{10.1145/3759928.3759946, title = {Research on the Construction of Digital Art Graphics Classification and Retrieval Framework under Machine Learning Technology}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {101--106}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759946}, url = {https://doi.org/10.1145/3759928.3759946}, author = {Pan, Lina}, keywords = {digital art graphics, image classification, structure perception, style embedding}, abstract = {To improve the classification and retrieval accuracy of digital art graphics with complex style attributes, this study proposes a joint framework integrating style and structural embeddings. A multi-scale residual network with channel attention is employed for feature extraction, and dual-branch representations are constructed for stylistic semantics and geometric structure. A multitask classification model and a dual-tower cross-modal retrieval structure are designed. A two-level retrieval strategy—style-based indexing and structure-based matching—supports efficient similarity fusion, while contrastive learning enhances embedding alignment. Experimental results show that the framework surpasses existing methods in multi-label classification, embedding clustering, and cross-modal retrieval, particularly in distinguishing similar styles and supporting text-guided queries. These findings validate the effectiveness and scalability of the dual-branch embedding approach for complex art graphic semantics.} }
@inproceedings{10.1145/3711129.3711303, title = {Research on Large-scale Ocean Date Analysis based on Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering}, pages = {1023--1028}, year = {2025}, isbn = {9798400710094}, doi = {10.1145/3711129.3711303}, url = {https://doi.org/10.1145/3711129.3711303}, author = {Zhou, Tong and Meng, Wenzheng and Yu, Dingfeng}, keywords = {Machine learning, Marine data analysis, Spatiotemporal convolutional neural networks, Variational autoencoders}, abstract = {The rapid development of electronic information and communication technology has led to the widespread use of a variety of marine monitoring technologies, including aerospace remote sensors, automatic buoys, multi-beam echo-sounders, and underwater detection equipment. This has resulted in a significant increase in the volume of marine big data. This work puts forth an innovative model that is based on the most recent machine learning techniques. In consideration of the distinctive attributes of high-dimensional, nonlinear, and strongly spatiotemporally correlated marine data, the model employs an enhanced deep neural network architecture. Initially, the Variational Autoencoder (VAE) was employed for the purpose of reducing the dimensionality of the data set and extracting the most pertinent features, thus enabling the effective handling of the noise and redundant information inherent to marine data. Subsequently, the spatiotemporal dependencies in the data were mined using a spatiotemporal convolutional neural network (ST-CNN) with a self-attention mechanism, enabling high-precision prediction and classification of ocean parameters. The model has been optimised to account for the distinctive characteristics of marine data, thereby enhancing its capacity to adapt to complex marine environments.} }
@inbook{10.5555/3716662.3716707, title = {A Conceptual Framework for Ethical Evaluation of Machine Learning Systems}, booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {534--546}, year = {2025}, author = {Gupta, Neha R. and Hullman, Jessica and Subramonyam, Hari}, abstract = {Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.} }
@inproceedings{10.1145/3759928.3759957, title = {In-depth Analysis, Model Optimization, and Interpretability of Heart Disease Risk Factors Using Machine Learning and Stacking Ensembles}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {177--185}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759957}, url = {https://doi.org/10.1145/3759928.3759957}, author = {Lin, Jintian}, keywords = {Ensemble learning, Heart disease prediction, Machine learning, SMOTE, Tomek Links}, abstract = {Heart disease, as a major threat to human health, imposes critical significance on early prediction for clinical prevention and control. To enhance prediction accuracy, this study leverages the Heart Failure Prediction Dataset from Kaggle, addressing data imbalance via SMOTE combined with Tomek Links algorithm, normalizing dimensions and handling outliers through Z-score standardization and capping method. Logistic Regression, Random Forest, XGBoost, and Stacking ensemble learning models are constructed, with their prediction performances systematically compared. Key influencing factors including ST-segment slope, chest pain type, and exercise-induced angina are identified via Random Forest feature importance analysis. Using SHAP values to interpret model decision logic reveals a strong correlation between abnormal ST-segment slope and heart disease incidence. The Stacking model demonstrates optimal comprehensive performance, achieving an accuracy of 0.9057 and an AUC value of 0.9581, outperforming single models. This study provides efficient modeling tools and interpretable risk assessment basis for early heart disease prediction, facilitating the formulation of clinical precision prevention strategies and public health interventions.} }
@article{10.1145/3772367, title = {Leveraging Machine Learning Models to Improve Smart Contract Security: A Survey of Vulnerabilities and Detection Methods}, journal = {ACM Comput. Surv.}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3772367}, url = {https://doi.org/10.1145/3772367}, author = {Alsunaidi, Shikah J. and Aljamaan, Hamoud and Hammoudeh, Mohammad}, keywords = {Blockchain, Ethereum, machine learning, security, smart contracts, smart contract vulnerabilities, software security}, abstract = {Smart Contracts (SCs), self-executing programs on blockchain platforms, are transforming industries such as banking, healthcare, and supply chains through automated, trustless transactions. However, their inherent vulnerabilities have led to severe financial and operational losses, with large-scale exploits causing substantial economic damage. Machine Learning (ML) has emerged as a promising approach for SC vulnerability detection, yet its effectiveness, adaptability, and generalizability remain insufficiently explored. This article comprehensively classifies current Ethereum SC vulnerabilities and attacks. It also surveys 108 ML-based detection methods, covering both traditional models and a structured taxonomy of advanced approaches such as GNN-based, LLM-based, contrastive learning, ensemble, hybrid, meta-learning, and transfer learning techniques. The strengths, limitations, and practical challenges of these methods are systematically analyzed, with particular attention to factors such as detection stages, classification problems, dataset characteristics, feature engineering, performance evaluation, generalizability, detection capability, model aging, and ethical and privacy implications. Additionally, existing datasets on SC vulnerabilities are reviewed and consolidated. By integrating these insights, this work provides actionable guidelines and a foundation for building secure, resilient, and trustworthy SC ecosystems.} }
@inproceedings{10.1145/3670474.3685961, title = {HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685961}, url = {https://doi.org/10.1145/3670474.3685961}, author = {Abi-Karam, Stefan and Sarkar, Rishov and Seigler, Allison and Lowe, Sean and Wei, Zhigang and Chen, Hanqiu and Rao, Nanditha and John, Lizy and Arora, Aman and Hao, Cong}, abstract = {Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present great challenges to FPGA and ML researchers. Existing datasets either cover only a subset of previously published benchmarks, provide no way to enumerate optimization design spaces, are limited to a specific vendor, or have no reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs to existing datasets, limiting wider adoption and sustainability of such datasets.In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture not only ensures broad coverage of data points via design space expansion but also supports interoperability with tools from multiple vendors. Users can contribute to each stage easily by submitting their own HLS designs or synthesis results via provided user APIs. The framework is also flexible, allowing extensions at every step via user APIs with custom frontends, synthesis tools, and scripts.To demonstrate the framework functionality, we include an initial set of built-in base designs from PolyBench, MachSuite, Rosetta, CHStone, Kastner et al.'s Parallel Programming for FPGAs, and curated kernels from existing open-source HLS designs. We report the statistical analyses and design space visualizations to demonstrate the completed end-to-end compilation flow, and to highlight the effectiveness of our design space expansion beyond the initial base dataset, which greatly contributes to dataset diversity and coverage.In addition to its evident application in ML, we showcase the versatility and multi-functionality of our framework through seven case studies:I) Building an ML model for post-implementation QoR predictionII) Using design space sampling in stage 1 to expand the design space covered from a small base set of HLS designs; III) Demonstrating the speedup from the fine-grained design parallelism backend; IV) Extending HLSFactory to target Intel's HLS flow across all stages; V) Adding and running new auxiliary designs using HLSFactory; VI) Integration of previously published HLS data in stage 3; VII) Using HLSFactory to perform HLS tool version regression benchmarking.Code available at https://github.com/sharc-lab/HLSFactory.} }
@inproceedings{10.1109/SCW63240.2024.00103, title = {Predicting Compute Node Unavailability in HPC: A Graph-Based Machine Learning Approach}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {737--740}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00103}, url = {https://doi.org/10.1109/SCW63240.2024.00103}, author = {Krumpak, Roy and Rozanec, Joze M. and Molan, Martin and Angelinelli, Matteo and Bartolini, Andrea}, keywords = {Anomalies Forecasting, Artificial Intelligence, Data Center, Graphs, HPC, Machine Learning, location = Atlanta, GA, USA}, abstract = {As high-performance computing (HPC) systems advance towards Exascale computing, their size and complexity increase, introducing new maintenance challenges. Modern HPC systems feature data monitoring infrastructures that provide insights into the system's state. This data can be leveraged to train machine learning models to anticipate anomalies that require compute nodes to undergo maintenance procedures. This paper presents a novel approach to predicting such anomalies by creating a graph per measurement that encodes current and past sensor readings and information related to the compute node sensors. The experiments were performed with data collected from Marconi 100, a tier-0 production supercomputer at CINECA in Bologna, Italy. Our results show that the machine learning model can accurately predict anomalies and surpass current State-Of-The-Art (SOTA) models regarding the quality of predictions and the time horizon considered to forecast them.} }
@article{10.14778/3641204.3641209, title = {PilotScope: Steering Databases with Machine Learning Drivers}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {980--993}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3641204.3641209}, url = {https://doi.org/10.14778/3641204.3641209}, author = {Zhu, Rong and Weng, Lianggui and Wei, Wenqing and Wu, Di and Peng, Jiazhen and Wang, Yifan and Ding, Bolin and Lian, Defu and Zheng, Bolong and Zhou, Jingren}, abstract = {Learned databases, or AI4DB techniques, have rapidly developed in the last decade. Deploying machine learning (ML) and AI4DB algorithms into actual databases is the gold standard to examine their performance in practice. However, due to the complexity of database systems, the difference between ML and DB programming paradigms, and the diversity of ML models, the tasks of developing and deploying AI4DB algorithms into databases are prohibitively difficult. Most previous works focus on specific AI4DB algorithms and ML models whose deployment requires close cooperation between ML and DB developers and heavy engineering cost.In this paper, we design and implement PilotScope, an AI4DB middleware with a programming model that largely reduces such difficulties. With a novel abstraction of AI4DB algorithms for, e.g., knob tuning and query optimization, PilotScope consists of two classes of components, AI4DB drivers and DB interactors, with different programming paradigms and roles in AI4DB tasks. ML developers focus on designing and implementing AI4DB drivers, which are algorithmic workflows that collect statistics from databases, train ML models, make decisions and optimize databases using learned models. AI4DB drivers interact with databases via DB interactors (e.g., for collecting data and enforcing actions in databases). DB developers focus on implementing these interactors on one or more database engines, with the interaction details hindered from ML developers. PilotScope supports a variety of AI4DB tasks, and the implementation of an AI4DB algorithm on PilotScope can be deployed in different databases with only minimum modifications. PilotScope is effective in benchmarking these AI4DB algorithms in real-world scenarios. We hope that PilotScope could significantly accelerate iterating AI4DB research and make AI4DB techniques truly applicable in production.} }
@inproceedings{10.1145/3637528.3671551, title = {Causal Machine Learning for Cost-Effective Allocation of Development Aid}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5283--5294}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671551}, url = {https://doi.org/10.1145/3637528.3671551}, author = {Kuzmanovic, Milan and Frauen, Dennis and Hatt, Tobias and Feuerriegel, Stefan}, keywords = {causal machine learning, development aid, heterogeneous treatment effects, medicine, treatment effect estimation, location = Barcelona, Spain}, abstract = {The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3\% (~50,000 cases) compared to the current allocation practice.} }
@inproceedings{10.1145/3747227.3747272, title = {An Empirical and Machine Learning Analysis of Emotional Experiences in College English Translation Courses with Online Platforms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {285--291}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747272}, url = {https://doi.org/10.1145/3747227.3747272}, author = {Tang, Jinfeng}, keywords = {College English Translation Course, English learning emotion, Machine Learning, Support Machine Vector, Yicat online translation platform, correlation analysis, difference analysis}, abstract = {Emotion experience in foreign language learning. Emotional experience directly influences students’ learning outcomes. College English translation is a high-level course. This course is highly complicated and professional. The students also have different emotional experiences. So this study takes Yicat online translation platform for teaching. Questionnaire surveys are carried out to collect data in this study. Non-English major students in a Wuhan university in China are the analysis targets. Anxiety, enjoyment and boredom are the targeted emotional factors for analysis. The study purpose is to figure out which emotional factors significantly influence the translation learning result. To ensure the study objective and novelty, support vector machine (SVM) method is applied in this study to analyze the data and get the main emotion factors that influence the translation performance. And it is to show the connections among the anxiety, the enjoyment and the boredom. From the final results, some valuable suggestions are provided for improving the teaching approach and making the translation courses more effective.} }
@inproceedings{10.1145/3607822.3616410, title = {Machine Learning on Topological Constraint for Mismatching Removal}, booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction}, year = {2023}, isbn = {9798400702815}, doi = {10.1145/3607822.3616410}, url = {https://doi.org/10.1145/3607822.3616410}, author = {Shen, Chentao and He, Zaixing and Zhao, Xinyue and Qiu, Mengyu}, keywords = {Machine Learning, Mismatch Removal, Sampling, Topological Constraints, location = Sydney, NSW, Australia}, abstract = {Image feature matching is a critical task in human-computer interaction, such as driverless cars, collaborative medical robots, aiming to establish correspondence between two images inputted from human or computer. To ensure the accuracy of matches, removing mismatches is of utmost importance. In recent years, machine learning has emerged as a new perspective for achieving effective mismatch removal. However, current learning-based methods suffer from a lack of generalizability due to their heavy reliance on extensive image data for training. Consequently, handling cases with a high mismatch ratio becomes challenging. In this paper, a novel approach is proposed that incorporates the triangular topology constraint into machine learning, which we called LTM. By summarizing topology constraints around the matching points and integrating the idea of sampling, we successfully address the mismatch removal task. Notably, our method stands out by requiring few parameters as input, enabling us to train it using just several image pairs from four sets. Remarkably, our method achieves outstanding results on diverse datasets using various machine learning techniques compared with many existing methods.} }
@inproceedings{10.1145/3759928.3759953, title = {Social-psychological Dual-dimensional Clustering Modeling: Heterogeneity Analysis of Smoking Cessation Success Based on Machine Learning}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {148--156}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759953}, url = {https://doi.org/10.1145/3759928.3759953}, author = {Wu, Yuting and Sun, Xiao}, keywords = {Smoking cessation intervention, cluster analysis, principal component analysis, random forest model, working population}, abstract = {Smoking poses a serious threat to public health worldwide, with significant differences in smoking behavior and quit success rates among different occupational groups. We propose an "Occupational Clustering - Dynamic Modeling" framework, which involves finely classifying occupations into seven major categories and analyzing them from both the "social-psychological" dimensions to construct a differentiated intervention model. From a methodological standpoint, we apply Principal Component Analysis (PCA) to perform dimensionality reduction and extract the most informative features. K-Means clustering is employed to reveal latent socio-psychological groupings within each occupational category, thereby enabling a stratified examination of populations. Random Forest classifier is built and assessed across multiple combinations of input variables: the configuration yielding the highest validation performance is chosen, and its feature‐importance metrics pinpoint the principal drivers of quitting success. To rectify the imbalance between cessation outcomes, the Borderline-SMOTE oversampling approach is used, which enhances the model's ability to detect and correctly classify minority‐class instances. We find significant differences in quit success rates and feature influences among different occupational groups, with the group model performing better in specific occupational groups and the Random Forest model accurately identifying key influencing factors. The use of grouping-based smoking cessation prediction models and feature importance analysis helps in formulating precise intervention strategies, improving quit success rates, advancing the field of smoking cessation interventions, and enhancing public health.} }
@inproceedings{10.1145/3587716.3587732, title = {Ensemble Two Stage Machine Learning for Network Abnormal Detection}, booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing}, pages = {97--102}, year = {2023}, isbn = {9781450398411}, doi = {10.1145/3587716.3587732}, url = {https://doi.org/10.1145/3587716.3587732}, author = {Du, Runze and Li, Runzhi and Zhang, Zijiao}, keywords = {CNN-CatBoost, intrusion detection, machine learning, network traffic, location = Zhuhai, China}, abstract = {With the endless emergence of network security problems, it also brings the corresponding security threats to the society. IDS is an effective means to deal with network threats, but in the modern large-scale network environment, the traditional IDS has a high false positive rate. Researchers apply machine learning method to intrusion detection and get good results. In this work, based on the network traffic features, we proposed an ensemble two stage machine learning method CNN-CatBoost to detect network attacking behavior. To solve imbalance problem between normal and anomaly traffic, we construct a CNN model with multi-scale convolutional adopted to extract relations among traffic features for anomaly traffic recognition. Next for a multi-class classification problem, we use classic CatBoost model to identify different attacking types. In the experiments, datasets is from CICIDS2017. We deploy comparison experiments among proposed method and other methods. The experimental results show that CNN-CatBoost outperforms others in performance. Lastly, we give feature analysis by SHAP, understand how features influence on the model.} }
@inproceedings{10.1145/3591197.3591306, title = {Privacy-Preserving Distributed Machine Learning Made Faster}, booktitle = {Proceedings of the 2023 Secure and Trustworthy Deep Learning Systems Workshop}, year = {2023}, isbn = {9798400701818}, doi = {10.1145/3591197.3591306}, url = {https://doi.org/10.1145/3591197.3591306}, author = {Jiang, Zoe L. and Gu, Jiajing and Wang, Hongxiao and Wu, Yulin and Fang, Junbin and Yiu, Siu-Ming and Luo, Wenjian and Wang, Xuan}, keywords = {Privacy-preserving machine learning, distributed machine learning, multi-key fully homomorphic encryption, location = Melbourne, VIC, Australia}, abstract = {With the development of machine learning, it is difficult for a single server to process all the data. So machine learning tasks need to be spread across multiple servers, turning the centralized machine learning into a distributed one. Multi-key homomorphic encryption is one of the suitable candidates to solve the problem. However, the most recent result of the Multi-key homomorphic encryption scheme (MKTFHE) only supports the NAND gate. Although it is Turing complete, it requires efficient encapsulation of the NAND gate to further support mathematical calculation. This paper designs and implements a series of operations on positive and negative integers accurately. First, we design basic bootstrapped gates, the efficiency of which is times that the number of using NAND to build. Second, we construct practical k-bit complement mathematical operators based on our basic binary bootstrapped gates. The constructed created can perform addition, subtraction, multiplication, and division on both positive and negative integers. Finally, we demonstrated the generality of the designed operators by achieving a distributed privacy-preserving machine learning algorithm, i.e. linear regression with two different solutions. Experiments show that the consumption time of the operators built with our gate is about 50 ∼ 70\% shorter than built directly with NAND gate and the iteration time of linear regression with our gates is 66.7\% shorter than with NAND gate directly.} }
@inproceedings{10.1145/3746972.3747010, title = {AI-Driven Marketing Strategy Optimization in the Digital Economy: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {239--242}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747010}, url = {https://doi.org/10.1145/3746972.3747010}, author = {Chen, Yuezhang and Liu, Jiacheng}, keywords = {Customer Segmentation, Marketing Strategy Optimization, Predictive Analytics, XGBoost}, abstract = {During digital transformation, AI-powered marketing plan optimization gives the organization a competitive edge. Machine learning models improve consumer targeting, campaign targeting, ROI, and campaign likelihood in this study. We tested these methods using two real-world datasets and supervised learning algorithms like Logistic Regression, Decision Trees, Random Forest, and XGBoost. The data was imputed, one-hot encoded, normalized, and feature-selected to improve model accuracy and relevance. With an accuracy of 90.3\% and an AUC-ROC score of 0.927, XGBoost surpasses all models in high-dimensional, difficult-to-characterize marketing data. The report also examines AI adoption's ethical issues, including data protection and model interpretation.} }
@inproceedings{10.1145/3718751.3718924, title = {Analysis of Chronic Disease Severity Using Machine Learning Approaches}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {1053--1059}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718924}, url = {https://doi.org/10.1145/3718751.3718924}, author = {Zhu, Yiwen}, keywords = {Chronic Disease Severity, Neural Networks, Random Forest, TfidfVectorizer}, abstract = {With the rising global prevalence of chronic diseases each year, there is a growing economic burden impacting social and economic development worldwide. In response, patient data pertaining to chronic illnesses were gathered and transformed into feature representations using the TfidfVectorizer. Neural Networks, Random Forest, GBDT, and SVM algorithms were then employed to predict the severity of these conditions accurately. Upon training and testing the models, it became evident that the Random Forest model outperformed the other models, showcasing superior predictive capabilities. Furthermore, by combining models through fusion techniques, there was potential for further enhancing predictive accuracy. This study has the potential to equip healthcare professionals with more precise and effective tools for analyzing chronic conditions, ultimately enhancing diagnostic efficiency and patient care.} }
@inproceedings{10.1145/3731715.3733275, title = {An Explainable Machine Learning Approach for Cognitive Load Detection in Virtual Reality Using Eye Tracking Data}, booktitle = {Proceedings of the 2025 International Conference on Multimedia Retrieval}, pages = {340--348}, year = {2025}, isbn = {9798400718779}, doi = {10.1145/3731715.3733275}, url = {https://doi.org/10.1145/3731715.3733275}, author = {Gao, Hong and Gao, Yapeng and Kasneci, Enkelejda}, keywords = {cognitive load, explainable ai, eye tracking, machine learning, virtual reality, location = Chicago, IL, USA}, abstract = {Accurate cognitive load (CL) detection during virtual reality (VR) locomotion is critical for enhancing user experience and improving interaction design. Traditional CL assessment methods, such as self-reports and physiological measures, face challenges in VR environments. Eye tracking has shown potential as a reliable indicator of CL across various human-computer interaction (HCI) tasks. It offers significant promise as a discriminative feature for predictive models in VR. This study explores the feasibility of detecting CL induced by VR locomotion using an explainable machine-learning approach along with eye-tracking techniques. A comparative user study employing a within-subjects design evaluated five unique gait-free locomotion techniques. Statistical analysis revealed distinct CL levels across these locomotion techniques. Several machine learning models were developed for CL detection using eye-tracking data, with the Light Gradient Boosting Machine (LightGBM) achieving the highest accuracy of 0.78. The SHAP approach was employed to analyze the importance of features to provide interpretability, offering insights into the machine learning model's decision-making process. Our findings highlight the potential of using eye-tracking-based machine learning techniques as a practical approach for cognitive load detection in VR, contributing to the growing research in multimedia analytics, human perception, and user intent within immersive environments. Additionally, our work demonstrates how eye-tracking data can be leveraged to improve user interactions and optimize immersive multimedia experiences based on cognitive load analysis.} }
@article{10.1145/3757892.3757893, title = {Causal Machine Learning Approaches for Modelling Data Center Heat Recovery: A Physical Testbed Study}, journal = {SIGENERGY Energy Inform. Rev.}, volume = {5}, pages = {4--10}, year = {2025}, doi = {10.1145/3757892.3757893}, url = {https://doi.org/10.1145/3757892.3757893}, author = {Gonzalez, David Zapata and Meyer, Marcel and M\"uller, Oliver}, keywords = {data center operations, heat recovery, causal machine learning}, abstract = {Data centers (DCs) form the backbone of our growing digital economy, but their rising energy demands pose challenges to our environment. At the same time, reusing waste heat from DCs also represents an opportunity, for example, for more sustainable heating of residential buildings. Modeling and optimizing these coupled and dynamic systems of heat generation and reuse is complex. On the one hand, physical simulations can be used to model these systems, but they are time-consuming to develop and run. Machine learning (ML), on the other hand, allows efficient data-driven modeling, but conventional correlation-based approaches struggle with the prediction of interventions and out-of-distribution generalization. Recent advances in causal ML, which combine principles from causal inference with flexible ML methods, are a promising approach for more robust predictions. Due to their focus on modeling interventions and cause-and-effect relationships, it is difficult to evaluate causal ML approaches rigorously. To address this challenge, we built a testbed of a miniature DC with an integrated waste heat network, equipped with sensors and actuators. This testbed allows conducting controlled experiments and automatic collection of realistic data, which can then be used to benchmark conventional and causal ML methods. Our experimental results highlight the strengths and weaknesses of each modeling approach, providing valuable insights on how to appropriately apply different types of machine learning to optimize data center operations and enhance their sustainability.} }
@inproceedings{10.1145/3677619.3678114, title = {Identifying Secondary School Students' Misconceptions about Machine Learning: An Interview Study}, booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research}, year = {2024}, isbn = {9798400710056}, doi = {10.1145/3677619.3678114}, url = {https://doi.org/10.1145/3677619.3678114}, author = {Marx, Erik and Witt, Clemens and Leonhardt, Thiemo}, keywords = {artificial intelligence, interview study, machine learning, mental models, misconceptions, qualitative research, students conceptions, location = Munich, Germany}, abstract = {Since students are familiar with machine learning (ML)-based applications in their everyday lives, they already construct mental models of how these systems work. This can result in misconceptions that influence the learning of correct ML concepts. Therefore, this study investigates the misconceptions students hold about the functionality of ML-based applications. To this end, we conducted semi-structured interviews with five students, focusing on their understanding of facial recognition and ChatGPT. The interviews were analyzed using an inductively developed code system and qualitative content analysis. This process identified six key misconceptions held by students: “Programmed Behavior,” “Exactness,” “Data Storage,” “Continuous Learning,” “User-trained Model,” and “Autonomous Data Acquisition”. These misconceptions include the notion that AI learns continuously during application, or that training data is saved and reused later. This paper presents the identified misconceptions and discusses their implication for the design and evaluation of effective learning activities in the context of ML.} }
@inproceedings{10.1145/3665689.3665699, title = {A Machine Learning Approach for Medical Imaging Evaluation}, booktitle = {Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing}, pages = {54--58}, year = {2024}, isbn = {9798400716645}, doi = {10.1145/3665689.3665699}, url = {https://doi.org/10.1145/3665689.3665699}, author = {Qiu, Wei and Shi, Dongxiao and Li, Qiang and Zheng, Yating and Shen, Xiaohui}, abstract = {Collaborating efficiently on medical imaging presents hurdles stemming from data privacy concerns and collaboration barriers. In response to these challenges, federated learning emerges as a decentralized machine learning method showing considerable promise. Its notable advantage lies in preserving data privacy robustly, enabling collaborative efforts without compromising sensitive medical information confidentiality. This study meticulously examines federated learning's application in medical imaging, thoroughly assessing its strengths and limitations. Our experimental results show that federated learning has potential in the field of medical images, achieving cooperative improvement in model performance while protecting data privacy.} }
@article{10.5555/3648699.3648824, title = {Statistical robustness of empirical risks in machine learning}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Guo, Shaoyan and Xu, Huifu and Zhang, Liwei}, keywords = {empirical risks, stability analysis, asymptotic qualitative statistical robustness, non-asymptotic quantitative statistical robustness, uniform consistency}, abstract = {This paper studies convergence of empirical risks in reproducing kernel Hilbert spaces (RKHS). A conventional assumption in the existing research is that empirical training data are generated by the unknown true probability distribution but this may not be satisfied in some practical circumstances. Consequently the existing convergence results may not provide a guarantee as to whether the empirical risks are reliable or not when the data are potentially corrupted (generated by a distribution perturbed from the true). In this paper, we fill out the gap from robust statistics perspective (Kr\"atschmer, Schied and Z\"ahle (2012); Kr\"atschmer, Schied and Z\"ahle (2014); Guo and Xu (2020)). First, we derive moderate sufficient conditions under which the expected risk changes stably (continuously) against small perturbation of the probability distributions of the underlying random variables and demonstrate how the cost function and kernel affect the stability. Second, we examine the difference between laws of the statistical estimators of the expected optimal loss based on pure data and contaminated data using Prokhorov metric and Kantorovich metric, and derive some asymptotic qualitative and non-asymptotic quantitative statistical robustness results. Third, we identify appropriate metrics under which the statistical estimators are uniformly asymptotically consistent. These results provide theoretical grounding for analysing asymptotic convergence and examining reliability of the statistical estimators in a number of regression models.} }
@inproceedings{10.1145/3767624.3767626, title = {Improving Crop Yield Prediction Accuracy: A Composite Machine Learning Model Specially Designed for Sub-frigid Regions}, booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence}, pages = {10--16}, year = {2025}, isbn = {9798400715907}, doi = {10.1145/3767624.3767626}, url = {https://doi.org/10.1145/3767624.3767626}, author = {Xu, Qingquan and Zhang, Xiao and Qiu, Tingyu}, keywords = {Crop yield prediction, composite model, machine learning, sub-frigid regions}, abstract = {This study introduces a novel approach for crop yield prediction in Heilongjiang, filling the gap in crop yield forecasting in sub-frigid countries. We developed a composite model, which ensured its applicability by double filtration and combined the strength of several machine learning models with hard vote and soft vote. The summary of our prediction results showed excellent predictive power of our model in general, and in the cases of four major crops, our model demonstrated superior performance over non-filtration machine learning models (ANN, LR, RF, SVR, RR), evidenced by RMSE. Our findings not only contribute to the improvement of yield forecasting in sub-frigid regions, but also solve the underlying instability of machine learning models to make it better applied in reality.} }
@inproceedings{10.1145/3717664.3717671, title = {Research on enterprise internal audit risk prediction model based on machine learning}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {37--40}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717671}, url = {https://doi.org/10.1145/3717664.3717671}, author = {Li, Na}, keywords = {Audit Risk Prediction, Machine Learning, Random Forest}, abstract = {This study constructs a machine learning-based internal audit risk prediction model to address the complex and dynamic financial risks faced by modern enterprises. By utilizing algorithms such as Random Forest, Support Vector Machine (SVM), and Logistic Regression, we analyze five years of internal financial data and perform feature selection, coupled with grid search and cross-validation to optimize model parameters. The experimental results indicate that the Random Forest model performs best in terms of accuracy, F1 score, and AUC value, effectively identifying high-risk transactions. The research conclusions provide an intelligent tool for enterprise audit risk management and offer insights for future model optimization and application research.} }
@inproceedings{10.1145/3551901.3556484, title = {A Thermal Machine Learning Solver For Chip Simulation}, booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD}, pages = {111--117}, year = {2022}, isbn = {9781450394864}, doi = {10.1145/3551901.3556484}, url = {https://doi.org/10.1145/3551901.3556484}, author = {Ranade, Rishikesh and He, Haiyang and Pathak, Jay and Chang, Norman and Kumar, Akhilesh and Wen, Jimin}, keywords = {autoencoders, chip thermal simulation, machine learning, location = Virtual Event, China}, abstract = {Thermal analysis provides deeper insights into electronic chips' behavior under different temperature scenarios and enables faster design exploration. However, obtaining detailed and accurate thermal profile on chip is very time-consuming using FEM or CFD. Therefore, there is an urgent need for speeding up the on-chip thermal solution to address various system scenarios. In this paper, we propose a thermal machine-learning (ML) solver to speed-up thermal simulations of chips. The thermal ML-Solver is an extension of the recent novel approach, CoAEMLSim (Composable Autoencoder Machine Learning Simulator) with modifications to the solution algorithm to handle constant and distributed HTC. The proposed method is validated against commercial solvers, such as Ansys MAPDL, as well as a latest ML baseline, UNet, under different scenarios to demonstrate its enhanced accuracy, scalability, and generalizability.} }
@inproceedings{10.1145/3715020.3715044, title = {Predicting Protein Interactions with BteA in Bordetella pertussis Pathogenesis Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computational Biology and Bioinformatics}, pages = {42--47}, year = {2025}, isbn = {9798400709623}, doi = {10.1145/3715020.3715044}, url = {https://doi.org/10.1145/3715020.3715044}, author = {Koshiba, Toshiki and Sudo, Chisato and Ogawa, Toshinobu and Iuchi, Toshihiko and Martono, Niken Prasasti and Kuwae, Asaomi and Ohwada, Hayato}, keywords = {Protein-Protein Interactions (PPIs), Machine Leargning, BteA, Bordetella pertussis}, abstract = {This study employed machine learning techniques to identify protein interactions with BteA in Bordetella pertussis, aiming to enhance our understanding of whooping cough pathogenesis and discover potential targets for new drug. Whooping cough (pertussis), caused by B. pertussis, remains a significant global health challenge despite widespread vaccine availability. B. pertussis protein BteA, delivered into host cells through the type III secretion system, plays a crucial role in promoting host cell death and pathogenesis. We utilized machine learning to predict proteins that interact with BteA. First, the STRING database was used to learn protein interactions, and Biopython was employed to extract four biological features for each protein: isoelectric point, total number of positively and negatively charged amino acid residues, and GRAVY. Six machine learning models, including XGBoost, Random Forest, and LightGBM, were trained using these features. The top models were applied to predict proteins likely to interact with BteA. RpoB, RpID, BP2755 and RpsK were identified with high interaction probability. Feature importance analysis using Shapley values showed that isoelectric points and GRAVY contributed significantly to the prediction models. Our findings demonstrate the effectiveness of machine learning in predicting protein-protein interactions within B. pertussis, providing valuable insights into the interaction network of BteA. The identified proteins offer promising targets for new drug, potentially contributing to the development of new treatments for whooping cough.} }
@proceedings{10.1145/3674029, title = {ICMLT '24: Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, year = {2024}, isbn = {9798400716379} }
@inproceedings{10.1145/3706598.3713482, title = {Preventing Harmful Data Practices by using Participatory Input to Navigate the Machine Learning Multiverse}, booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, year = {2025}, isbn = {9798400713941}, doi = {10.1145/3706598.3713482}, url = {https://doi.org/10.1145/3706598.3713482}, author = {Simson, Jan and Draxler, Fiona and Mehr, Samuel and Kern, Christoph}, keywords = {Participatory Design, Machine Learning, Algorithmic Fairness, Multiverse Analysis, Citizen Science, Garden of Forking Paths}, abstract = {In light of inherent trade-offs regarding fairness, privacy, interpretability and performance, as well as normative questions, the machine learning (ML) pipeline needs to be made accessible for public input, critical reflection and engagement of diverse stakeholders.In this work, we introduce a participatory approach to gather input from the general public on the design of an ML pipeline. We show how people’s input can be used to navigate and constrain the multiverse of decisions during both model development and evaluation. We highlight that central design decisions should be democratized rather than “optimized” to acknowledge their critical impact on the system’s output downstream. We describe the iterative development of our approach and its exemplary implementation on a citizen science platform. Our results demonstrate how public participation can inform critical design decisions along the model-building pipeline and combat widespread lazy data practices.} }
@inproceedings{10.1145/3731806.3731829, title = {Leveraging Machine Learning Techniques to Obtain Data for Virtual Sensors}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {290--294}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731829}, url = {https://doi.org/10.1145/3731806.3731829}, author = {Zhao, Ge-Zhi and Tan, Yi-Fei and Abdul Karim, Hezerul and Cheeng, Tze-Hang and Chia, Ching-King}, keywords = {Internet of Things (IoT), data fusion, physical sensors, predictive model, virtual sensors}, abstract = {The Internet-of-Things (IoT) has revolutionised smart devices by enabling real-time monitoring through remote sensors. It is the most essential element particularly in smart sensing industrial applications such as environmental monitoring and industrial automation. These sensors provide crucial raw data to be analysed and accurate prediction of events of equipment breakdowns or preventive maintenance is required. However, if a physical sensor fails to function normally, virtual sensors can facilitate the missing data during downtime. Virtual sensors utilise predictive models to forecast the missing data, leveraging historical data and patterns from previously trained events to forecast sensor readings under the same conditions. In this research, the authors build a predictive model to generate data for a malfunctioned sensor by using actual data from other functional sensors. The hybrid setup between physical and virtual sensors will complement each other during operations to ensure fail-safe operation. In the research methodology, data from five sensors were analysed with predictive models of random forest. Data were trained on four of the sensors to predict the next day's readings of the fifth sensor. The experiment examined the impact of training various data durations (5, 10, and 15 days). The results revealed promising outcomes across all three training data sizes. Notably, the random forest regression model achieved better performance with larger training datasets, highlighting the impact of dataset size on model effectiveness.} }
@inproceedings{10.1145/3701100.3701144, title = {Research on a Bridge Health Monitoring System Based on Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {214--218}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701144}, url = {https://doi.org/10.1145/3701100.3701144}, author = {Zhang, DaYong}, keywords = {Bridge health monitoring, data analysis, system design, performance optimization, machine learning}, abstract = {This study designs and implements a bridge health monitoring system based on machine learning. The system adopts a four-layer architecture, including data acquisition, processing, analysis and decision-making, and user interaction layers. Core functions encompass data preprocessing, feature extraction, model training, health assessment, and early warning. It integrates algorithms such as Support Vector Machines, Random Forests, and Deep Neural Networks to enhance the accuracy of damage identification and status prediction. The development utilizes modern tools to improve efficiency. Test results show that the system is stable, responsive, and has strong processing capabilities. This system provides an intelligent solution for bridge health monitoring, improving the automation level and management efficiency of monitoring, and offers strong support for bridge safety management.} }
@article{10.1145/3686973, title = {Articulation Work and Tinkering for Fairness in Machine Learning}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {8}, year = {2024}, doi = {10.1145/3686973}, url = {https://doi.org/10.1145/3686973}, author = {Fahimi, Miriam and Russo, Mayra and Scott, Kristen M. and Vidal, Maria-Esther and Berendt, Bettina and Kinder-Kurlanda, Katharina}, keywords = {articulation work, doability, fair machine learning, interview study}, abstract = {The field of fair AI aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory, and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI research to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research for them. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve.} }
@inbook{10.1145/3760023.3760053, title = {Comparison and Optimization of Social Media Text Sentiment Analysis Models Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {179--184}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760053}, author = {Zhu, Jiemin and Liang, Yan and Lv, Le}, abstract = {In this paper, a hybrid sentiment analysis framework combining lexical rules and machine learning is proposed, aiming to improve the performance of sentiment classification for English social media texts. The study first preprocesses the raw data through a multi-stage text cleaning process (including denoising, stemming extraction, and deactivated word filtering) and generates preliminary sentiment labels by calculating text sentiment scores with the VADER sentiment lexicon; further, the text is vectorized using the TF-IDF method to construct the feature space, and the passive-aggressive classifiers, logistic regression, random forests, support vector machines, and 14 models such as Simple Bayes for the classification task. In order to optimize the model performance, the study tunes the key hyperparameters by random search cross-validation, and finally compares the accuracy, precision, recall and F1 value of each model on the test set.The experimental results show that the optimized passive-aggressive classifier performs best in the emotion classification task, with an improvement of about 1.2\% in accuracy, 0.7\% in precision, 1.2\% in recall, 1.1\% in F1 score, and a significant enhancement in the recognition of neutral emotions. In addition, the study reveals the high-frequency sentiment expression patterns in social media texts through word frequency statistics and visualization, and finds that negative sentiments are mostly focused on topics such as service complaints and product defects.} }
@article{10.1145/3665795, title = {Self-Supervised Machine Learning Framework for Online Container Security Attack Detection}, journal = {ACM Trans. Auton. Adapt. Syst.}, volume = {19}, year = {2024}, issn = {1556-4665}, doi = {10.1145/3665795}, url = {https://doi.org/10.1145/3665795}, author = {Tunde-Onadele, Olufogorehan and Lin, Yuhang and Gu, Xiaohui and He, Jingzhu and Latapie, Hugo}, keywords = {Performance debugging, microservices, causal analysis}, abstract = {Container security has received much research attention recently. Previous work has proposed to apply various machine learning techniques to detect security attacks in containerized applications. On one hand, supervised machine learning schemes require sufficient labeled training data to achieve good attack detection accuracy. On the other hand, unsupervised machine learning methods are more practical by avoiding training data labeling requirements, but they often suffer from high false alarm rates. In this article, we present a generic self-supervised hybrid learning (SHIL) framework for achieving efficient online security attack detection in containerized systems. SHIL can effectively combine both unsupervised and supervised learning algorithms but does not require any manual data labeling. We have implemented a prototype of SHIL and conducted experiments over 46 real-world security attacks in 29 commonly used server applications. Our experimental results show that SHIL can reduce false alarms by 33\%–93\% compared to existing supervised, unsupervised, or semi-supervised machine learning schemes while achieving a higher or similar detection rate.} }
@inproceedings{10.1145/3718491.3718638, title = {A Comparative Study of Linear and Nonlinear Machine Learning Algorithms in Quantitative Investment}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {909--913}, year = {2025}, isbn = {9798400710865}, doi = {10.1145/3718491.3718638}, url = {https://doi.org/10.1145/3718491.3718638}, author = {Zheng, Zengji and Sun, Haocheng and Chen, Sihan and Cai, Jiayi}, keywords = {investment portfolio, machine learning, multi-factor modeling, quantitative investment}, abstract = {In quantitative investing, multi-factor models are widely used, but the growing number of factors presents challenges for traditional models in factor selection and model structure. Machine learning algorithms, however, offer advantages in feature selection and return prediction, enabling multi-factor strategies to handle more data and achieve better performance. Linear and nonlinear machine learning algorithms have distinct feature selection mechanisms, each with its strengths and weaknesses in return prediction.This paper compares two linear algorithms (Lasso and Elastic Net) with two nonlinear algorithms (Random Forest and Gradient Boosting Decision Tree) in terms of factor selection and investment performance. Using a sample of CSI 300 index constituents from January 2007 to November 2021, 244 factors across eight categories—including quality, value, and momentum—were studied. The results show that linear models excel at overcoming factor correlation and noise, while nonlinear models are better at capturing complex, long-term relationships between factors and returns. Linear models tend to favor momentum factors related to price movements, whereas nonlinear models emphasize value and quality factors based on company fundamentals.To leverage the strengths of both linear and nonlinear models, we applied Simple Average Integration and Stacking Integration, resulting in enhanced portfolio performance, including higher monthly returns and Sharpe ratios, compared to both traditional linear regression and the Shanghai-Shenzhen-300 Index. This integrated approach offers a more robust and reliable investment strategy.In conclusion, this study demonstrates the potential of machine learning algorithms in improving multi-factor quantitative investment strategies, providing valuable insights for future investment practices.} }
@inproceedings{10.1109/ICSE-Companion58688.2023.00065, title = {Towards Machine Learning Guided by Best Practices}, booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings}, pages = {240--244}, year = {2023}, isbn = {9798350322637}, doi = {10.1109/ICSE-Companion58688.2023.00065}, url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00065}, author = {Mojica-Hanke, Anamaria}, keywords = {machine learning, good practices, software engineering, location = Melbourne, Victoria, Australia}, abstract = {Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where multiple studies related to the use of Machine Learning in Software Engineering have been published in conferences and journals. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional software, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and focused on their domain (e.g., biomechanics), but for the best of our knowledge, there is no guideline focused on the SE community. This thesis aims to reduce the gap of not having clear guidelines in the SE community by using possible sources of practices such as question-and-answer communities and also previous research studies. As a result, we will present a set of practices with an SE perspective, for researchers and practitioners, including a tool for searching them.} }
@inbook{10.1145/3718491.3718672, title = {Environmental Ecological Analysis Based on Machine Learning and Big Data--Evidence from China}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {1125--1130}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718672}, author = {Chao, Xiao and Zhang, Qiang}, abstract = {Based on machine learning and big data technology, this paper analyzes the environmental ecology of China. Firstly, this paper collected and analyzed a large number of eco-environmental data sets, and discussed the trend and reasons of eco-environmental changes in China from the perspective of precipitation and land cover. Then the article uses LGBM and SHAP models to evaluate the contribution of influencing factors to the ecological environment, and evaluates the effect of the model. The key indicators affecting the ecological environment are obtained and the corresponding analysis is given. It provides an empirical reference for the use of machine learning methods in geography.} }
@article{10.1109/TNET.2022.3190797, title = {Machine Learning Feature Based Job Scheduling for Distributed Machine Learning Clusters}, journal = {IEEE/ACM Trans. Netw.}, volume = {31}, pages = {58--73}, year = {2022}, issn = {1063-6692}, doi = {10.1109/TNET.2022.3190797}, url = {https://doi.org/10.1109/TNET.2022.3190797}, author = {Wang, Haoyu and Liu, Zetian and Shen, Haiying}, abstract = {With the rapid proliferation of Machine Learning (ML) and Deep learning (DL) applications running on modern platforms, it is crucial to satisfy application performance requirements such as meeting deadline and ensuring accuracy. To this end, researchers have proposed several job schedulers for ML clusters. However, none of the previously proposed schedulers consider ML model parallelism, though it has been proposed as an approach to increase the efficiency of running large-scale ML and DL jobs. Thus, in this paper, we propose an ML job Feature based job Scheduling system (MLFS) for ML clusters running both data parallelism and model parallelism ML jobs. MLFS first uses a heuristic scheduling method that considers an ML job’s spatial and temporal features to determine task priority for job queue ordering in order to improve job completion time (JCT) and accuracy performance. It uses the data from the heuristic scheduling method for training a deep reinforcement learning (RL) model. After the RL model is well trained, it then switches to the RL method to automatically make decisions on job scheduling. In addition, MLFS has a system load control method that selects tasks from overloaded servers to move to underloaded servers based on task priority, and also intelligently removes the tasks that generate little or no improvement on the desired accuracy performance when the system is overloaded to improve JCT and accuracy by job deadline. Furthermore, we propose Optimal ML iteration stopping method that determines the proper time to stop training ML model when this model reaches the minimum loss value. Our real experiments and large-scale simulation based on real trace show that MLFS reduces JCT by up to 53\% and makespan by up to 52\%, and improves accuracy by up to 64\% when compared with existing ML job schedulers. We also open sourced our code.} }
@article{10.14778/3659437.3659441, title = {InferDB: In-Database Machine Learning Inference Using Indexes}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {1830--1842}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3659437.3659441}, url = {https://doi.org/10.14778/3659437.3659441}, author = {Salazar-D\'az, Ricardo and Glavic, Boris and Rabl, Tilmann}, abstract = {The performance of inference with machine learning (ML) models and its integration with analytical query processing have become critical bottlenecks for data analysis in many organizations. An ML inference pipeline typically consists of a preprocessing workflow followed by prediction with an ML model. Current approaches for in-database inference implement preprocessing operators and ML algorithms in the database either natively, by transpiling code to SQL, or by executing user-defined functions in guest languages such as Python. In this work, we present a radically different approach that approximates an end-to-end inference pipeline (preprocessing plus prediction) using a light-weight embedding that discretizes a carefully selected subset of the input features and an index that maps data points in the embedding space to aggregated predictions of an ML model. We replace a complex preprocessing workflow and model-based inference with a simple feature transformation and an index lookup. Our framework improves inference latency by several orders of magnitude while maintaining similar prediction accuracy compared to the pipeline it approximates.} }
@inproceedings{10.1145/3703323.3703749, title = {Road traffic accident severity prediction using causal inference and machine learning}, booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)}, pages = {292--300}, year = {2025}, isbn = {9798400711244}, doi = {10.1145/3703323.3703749}, url = {https://doi.org/10.1145/3703323.3703749}, author = {Srivastava, Nishtha and Gohil, Bhavesh N. and Ray, Suprio}, abstract = {The global rise in road traffic accidents presents substantial challenges across economic, societal, and public health domains, leading to millions of injuries and fatalities annually. Current studies on modeling and analyzing traffic accident frequency largely treat the issue as a classification task, primarily utilizing learning-based or ensemble methods. However, these approaches frequently neglect the intricate relationships among the multifaceted factors—such as road complexity, environmental conditions, driver behavior, and contextual elements—that contribute to traffic accidents and hazardous scenarios. We propose an approach that employs causal inference and causal Machine Learning (ML) techniques to predict accident severity and identify key causal factors. We evaluate our proposed approach with two datasets, from Ethiopia and UK. Given the inherent imbalance in these datasets, the Synthetic Minority Oversampling Technique (SMOTE) is utilized to achieve balanced data representation. Uplift modeling and causal inference methods are employed for severity prediction. Individual Treatment Effect (ITE) and Average Treatment Effect (ATE) are used to make interpretations of the predictions. Our research contributes to understanding and mitigating the impact of road traffic accidents through advanced causal analysis techniques, offering actionable insights for policymakers, urban planners, and public health officials globally.} }
@inproceedings{10.1145/3638530.3648413, title = {Evolutionary Computation meets Machine Learning for Combinatorial Optimisation}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1328--1351}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3648413}, url = {https://doi.org/10.1145/3638530.3648413}, author = {Mei, Yi and Raidl, G\"unther} }
@inproceedings{10.1145/3704137.3704138, title = {Heart Rate Arrhythmia Identification with Internet of Things and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {1--7}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704138}, url = {https://doi.org/10.1145/3704137.3704138}, author = {Chze Xin, Loo and Yogarayan, Sumendra and Abdul Razak, Siti Fatimah and Azman, Afizan}, keywords = {Heart Rate Classification, Internet of Things, Machine Learning}, abstract = {Heart rate classification is a critical task in health monitoring and diagnosis, particularly facilitated by advancements in Internet of Things (IoT) and wearable technology. This study evaluates the performance of various machine learning models, with a specific focus on Support Vector Machine (SVM) with a linear kernel, in classifying heart rate data. The testing indicates that SVM with a linear kernel achieves a testing accuracy of 100\%, surpassing other models such as Random Forest (99.6\%) and Decision Tree (99.3\%). This exceptional performance is attributed to the linear separability of the heart rate data, where SVM with a linear kernel effectively identifies the optimal hyperplane for class separation. Additionally, SVM's linear kernel demonstrates robustness against noise, which is common in real-world heart rate data, thereby enhancing its reliability. The study also highlights the interpretability of linear models through feature weights, providing insights into the physiological factors influencing heart rate. This research bridges significant gaps in existing literature by demonstrating the accuracy and practical applicability of linear kernel SVMs in heart rate classification, especially in the context of IoT and wearable technologies. The findings suggest that simpler models should be considered before resorting to complex non-linear models, offering a balance of high performance, computational efficiency, and interpretability in medical applications.} }
@article{10.1145/3725835, title = {Evaluating Gaze Event Detection Algorithms: Impacts on Machine Learning-based Classification and Psycholinguistic Statistical Modeling}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3725835}, url = {https://doi.org/10.1145/3725835}, author = {Reich, David R. and Prasse, Paul and J\"ager, Lena A.}, keywords = {I-DT, I-VT, eye movements, eye tracking, eye-tracking, machine learning, psycholinguistic analysis, psycholinguistic statistical modeling}, abstract = {Eye movements offer valuable, non-invasive insights into cognitive processes and are widely used in both psycholinguistic research and machine-learning applications, such as assessing reading comprehension and cognitive load. These applications typically rely on fixations and saccades detected through gaze event algorithms, which may be either proprietary or open-source. The impact of different gaze event detection algorithms on subsequent analysis is underexplored and often overlooked. This study investigates how two threshold-based algorithms, I-DT and I-VT, influence both machine-learning classification tasks and psycholinguistic statistical modeling. Using diverse datasets-including stationary, remote, and VR eye-tracking data across multiple sampling frequencies-our findings show significant differences in downstream performance. For ML tasks, I-DT generally outperforms I-VT, with I-VT being highly sensitive to threshold choices. In psycholinguistic analysis, results confirm established findings only when thresholds align with established fixation metrics, emphasizing the importance of appropriate threshold selection for meaningful analysis. Our code is publicly available: https://github.com/aeye-lab/eye-movement-preprocessing.} }
@inproceedings{10.1145/3712255.3726701, title = {Machine Learning and Genetic Algorithms: An Intricate Relationship for Locating Methane in Satellite Images}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {931--934}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726701}, url = {https://doi.org/10.1145/3712255.3726701}, author = {Wijata, Agata M. and Long\'ep\'e, Nicolas and Przewozniczek, Michal W. and Nalepa, Jakub}, keywords = {earth observation, hyperspectral image, methane detection, onboard machine learning, feature selection, genetic algorithm, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Detecting methane in remotely-sensed hyperspectral images is a crucial task in environmental monitoring. It aids in identifying methane leaks and emissions, which is essential for mitigating their impact on the climate change. We tackle this challenge and propose an end-to-end and flexible approach for detecting and locating methane plumes in satellite hyperspectral images. It couples classic machine learning with a genetic algorithm to identify the most important image features that contribute to methane localization. We extract features from superpixels—locally-coherent image areas—and then perform feature selection to identify the most discriminative ones, pruning unnecessary (or even noisy) features. Our experiments performed over the large-scale methane dataset (STAR-COP) indicated that feature subsets optimized using a genetic algorithm allow us to build effective methane detectors that outperform supervised models trained over full feature sets. For reproducibility, we refer to our GitHub repository: https://github.com/smile-research/GeneticAlgorithms4MethaneDetection.git.} }
@article{10.1145/3674849, title = {Empirical Architecture Comparison of Two-input Machine Learning Systems for Vision Tasks}, journal = {Form. Asp. Comput.}, volume = {36}, year = {2024}, issn = {0934-5043}, doi = {10.1145/3674849}, url = {https://doi.org/10.1145/3674849}, author = {Wakigami, Kazuya and Machida, Fumio and Phung-Duc, Tuan}, keywords = {Energy consumption, machine learning system, performance, reliability, simulation}, abstract = {As machine learning models have been deployed in many vision systems, including autonomous vehicles and robots, designing architectures for machine learning systems (MLSs) has emerged as a critical concern. Previous studies have shown that enhancing the reliability of MLS outputs can be achieved by comparing multiple inference results on distinct inputs. Nevertheless, the architectures facilitating multiple inferences incur non-negligible performance overhead and energy consumption that have been less investigated. This article delves into the trade-offs among reliability, performance, and energy efficiency of architectures for two-input MLSs through real experiments conducted on image classification and object detection tasks. Specifically, we scrutinize the comparison between parallel- and shared-type architectures of two-input MLSs for vision tasks. The experiments confirm that the shared-type architecture can achieve a shorter response time and smaller energy consumption by using a shared machine learning module for both image classification and object detection tasks. However, the parallel-type architecture can benefit the redundant machine learning modules for improving throughput and fault tolerance. Our empirical results also show the service time distributions of image classification and object detection tasks fit well with a log-normal distribution and a mixture of the Gaussian model, respectively.} }
@inproceedings{10.1145/3697467.3697640, title = {Identifying and predicting key factors of customer satisfaction based on machine learning}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {200--204}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697640}, url = {https://doi.org/10.1145/3697467.3697640}, author = {Li, Beiling and Lai, Yuxuan and Ling, Yangbo and Li, Ganxiang}, keywords = {AdaBoost, Customer satisfaction, Feature engineering, LightGBM, SVM, Stacking, XGBoost}, abstract = {With the advancement of mobile communication technology and the continuous development of network infrastructure, people increasingly enjoy the convenience brought by mobile communication and the internet. The user's network experience is also increasingly valued by mobile operators. To help further enhance the quality of network services and market operations, the current study focuses on customer satisfaction as an important indicator. Firstly, feature engineering is used to better anchor the key factors and improve the performance of the model. Secondly, various models such as Tree-based models and Support Vector Machine (SVM) model were used for data processing, and Bayesian parameter tuning methods for model optimization. Finally, a predictive model is established using the Stacking ensemble method to integrate the models. It turns out that the ensemble model has a better performance to predict customers’ satisfaction than any single model.} }
@article{10.1613/jair.1.14340, title = {Towards Green Automated Machine Learning: Status Quo and Future Directions}, journal = {J. Artif. Int. Res.}, volume = {77}, year = {2023}, issn = {1076-9757}, doi = {10.1613/jair.1.14340}, url = {https://doi.org/10.1613/jair.1.14340}, author = {Tornede, Tanja and Tornede, Alexander and Hanselle, Jonas and Mohr, Felix and Wever, Marcel and H\"ullermeier, Eyke}, abstract = {Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution — a machine learning pipeline — tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticized for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large-scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool w.r.t. their “greenness”, i.e., sustainability, are summarized. Finally, we elaborate on how to be transparent about the environmental footprint and what kind of research incentives could direct the community in a more sustainable AutoML research direction. As part of this, we propose a sustainability checklist to be attached to every AutoML paper featuring all core aspects of Green AutoML.} }
@proceedings{10.1145/3748382, title = {FAIML '25: Proceedings of the 2025 4th International Conference on Frontiers of Artificial Intelligence and Machine Learning}, year = {2025}, isbn = {9798400713217} }
@inproceedings{10.1145/3644713.3644804, title = {Machine Learning for Malware Detection in Network Traffic}, booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems}, pages = {605--610}, year = {2024}, isbn = {9798400709036}, doi = {10.1145/3644713.3644804}, url = {https://doi.org/10.1145/3644713.3644804}, author = {Omopintemi, Ayorinde Henry and Ghafir, Ibrahim and Eltanani, Shadi and Kabir, Sohag and Lefoane, Moemedi}, keywords = {Intrusion Detection, K-Nearest Neighbor Algorithm, Machine learning, Malware Analysis, Malware Detection, Random Forest, The Adaboost Algorithm, location = Dubai, United Arab Emirates}, abstract = {Developing advanced and efficient malware detection systems is becoming significant in light of the growing threat landscape in cybersecurity. This work aims to tackle the enduring problem of identifying malware and protecting digital assets from cyber-attacks. Conventional methods frequently prove ineffective in adjusting to the ever-evolving field of harmful activity. As such, novel approaches that improve precision while simultaneously taking into account the ever-changing landscape of modern cybersecurity problems are needed. To address this problem this research focuses on the detection of malware in network traffic. This work proposes a machine-learning-based approach for malware detection, with particular attention to the Random Forest (RF), Support Vector Machine (SVM), and Adaboost algorithms. In this paper, the model’s performance was evaluated using an assessment matrix. Included the Accuracy (AC) for overall performance, Precision (PC) for positive predicted values, Recall Score (RS) for genuine positives, and the F1 Score (SC) for a balanced viewpoint. A performance comparison has been performed and the results reveal that the built model utilizing Adaboost has the best performance. The TPR for the three classifiers performs over 97\% and the FPR performs \&lt; 4\% for each of the classifiers. The created model in this paper has the potential to help organizations or experts anticipate and handle malware. The proposed model can be used to make forecasts and provide management solutions in the network’s everyday operational activities.} }
@inproceedings{10.1145/3709016.3737807, title = {A Context-Aware Framework for Dynamic NFT Updates Using a Machine Learning Driven Adaptive Smart Contract}, booktitle = {Proceedings of the 7th ACM International Symposium on Blockchain and Secure Critical Infrastructure}, year = {2025}, isbn = {9798400714122}, doi = {10.1145/3709016.3737807}, url = {https://doi.org/10.1145/3709016.3737807}, author = {Mabarani, Samukeliso and Rahman, Mohammad Saidur and Gondal, Iqbal and Bandara, H. M. N. Dilum}, keywords = {NFT, tokenization, context-aware updates, static attributes, dynamic attributes, machine learning, adaptive smart-contract, location = Meli\'a Hanoi, Hanoi, Vietnam}, abstract = {Updating dynamic attributes of Real-World Asset (RWA)-backed Non-Fungible Tokens (NFTs) presents challenges due to blockchain immutability. To address this, we propose a context-aware framework that integrates a machine learning (ML) model with an adaptive smart contract (ASC). The ML model intelligently determines when and which dynamic attributes require updates, while the ASC ensures security by locking static attributes and enabling automatic updates for dynamic ones. A specialized locking mechanism prevents unauthorized modifications, preserving NFT integrity. Experimental results validate the framework's effectiveness in securing static attributes, dynamically updating valid attributes, and preventing unauthorized transactions. Security evaluations further demonstrate robust access control, input validation, and data integrity, ensuring resilience against unauthorized modifications.} }
@inproceedings{10.1145/3539618.3591872, title = {On the "Rough Use" of Machine Learning Techniques}, booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {2}, year = {2023}, isbn = {9781450394086}, doi = {10.1145/3539618.3591872}, url = {https://doi.org/10.1145/3539618.3591872}, author = {Lin, Chih-Jen}, keywords = {machine learning, validation and prediction, location = Taipei, Taiwan}, abstract = {Machine learning is everywhere, but unfortunately, we are not experts of every method. Sometimes we "inappropriately'' use machine learning techniques. Examples include reporting training instead of test performance and comparing two methods without suitable hyper-parameter searches. However, the reality is that there are more sophisticated or more subtle examples, which we broadly call the "rough use'' of machine learning techniques. The setting may be roughly fine, but seriously speaking, is inappropriate. We briefly discuss two intriguing examples.- In the topic of graph representation learning, to evaluate the quality of the obtained representations, the multi-label problem of node classification is often considered. An unrealistic setting was used in almost the entire area by assuming that the number of labels of each test instance is known in the prediction stage. In practice, such ground truth information is rarely available. Details of this interesting story are in Lin et al. (2021).- In training deep neural networks, the optimization process often relies on the validation performance for termination or selecting the best epoch. Thus in many public repositories, training, validation, and test sets are explicitly provided. Many think this setting is standard in applying any machine learning technique. However, except that the test set should be completely independent, users can do whatever the best setting on all the available labeled data (i.e., training and validation sets combined). Through real stories, we show that many did not clearly see the relation between training, validation, and test sets.The rough use of machine learning methods is common and sometimes unavoidable. The reason is that nothing is called a perfect use of a machine learning method. Further, it is not easy to assess the seriousness of the situation. We argue that having high-quality and easy-to-use software is an important way to improve the practical use of machine learning techniques.} }
@inproceedings{10.1145/3638530.3658378, title = {Evolutionary Machine Learning for Interpretable and eXplainable AI}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1038--1068}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3658378}, url = {https://doi.org/10.1145/3638530.3658378}, author = {Siddique, Abubakar and Browne, Will N. and Urbanowicz, Ryan J.} }
@inproceedings{10.1145/3675888.3676104, title = {Customer Opinion Analysis and Topic Extraction Using Machine Learning}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {502--507}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676104}, url = {https://doi.org/10.1145/3675888.3676104}, author = {Moza, Sidhant and Ashraf, Ehtesham and Agrawal, Himani and .. Vikash}, keywords = {Clustering, Deep Learning, Dimensionality Reduction, Feature Extraction, Machine Learning, NLP, location = Noida, India}, abstract = {In today’s competitive market landscape, understanding customer sentiments through online reviews is imperative for businesses. However, the unstructured nature of this data poses challenges for traditional supervised learning methods. This paper proposes a methodology integrating semi-supervised learning and clustering techniques to analyze software reviews. Through extensive preprocessing and feature extraction, including Word2Vec embeddings and keyword extraction, the approach achieves nuanced language understanding. Clustering algorithms like HDBSCAN partition the data into meaningful clusters, facilitating insightful classification. Hyperparameter tuning ensures optimal performance, with findings indicating the effectiveness of the approach in discerning product-specific opinions. Evaluation metrics confirm the quality of obtained clusters, revealing distinct sentiments towards various software products. This research contributes to enhancing customer understanding and decision-making in software development and marketing strategies.} }
@article{10.1145/3766539, title = {What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3766539}, url = {https://doi.org/10.1145/3766539}, author = {Kong, Youjin}, keywords = {Machine Learning Fairness, Normative Foundation, Egalitarianism, Distributive Equality (DE), Equality of Opportunity (EOP), Relational Equality (RE), Allocative Harm, Representational Harm}, abstract = {Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable benefits and goods, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation, especially in the context of text- and image-generation models. Grounding ML fairness in egalitarianism—the view that equality is a fundamental moral and social ideal—requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong—that is, why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups—and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a novel multifaceted egalitarian framework for ML fairness that integrates both distributive and relational notions of equality. Drawing on critical social and political philosophy, including the work of Anderson, Young, and Fraser, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the entire ML pipeline.} }
@inproceedings{10.1145/3627673.3679218, title = {Introducing CausalBench: A Flexible Benchmark Framework for Causal Analysis and Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5220--5224}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679218}, url = {https://doi.org/10.1145/3627673.3679218}, author = {Kapkic, Ahmet and Mandal, Pratanu and Wan, Shu and Sheth, Paras and Gorantla, Abhinav and Choi, Yoonhyuk and Liu, Huan and Candan, K. Selcuk}, keywords = {benchmark, causality, dataset, machine learning, metric, model, location = Boise, ID, USA}, abstract = {While witnessing the exceptional success of machine learning (ML) technologies in many applications, users are starting to notice a critical shortcoming of ML: correlation is a poor substitute for causation. The conventional way to discover causal relationships is to use randomized controlled experiments (RCT); in many situations, however, these are impractical or sometimes unethical. Causal learning from observational data offers a promising alternative. While being relatively recent, causal learning aims to go far beyond conventional machine learning, yet several major challenges remain. Unfortunately, advances are hampered due to the lack of unified benchmark datasets, algorithms, metrics, and evaluation service interfaces for causal learning. In this paper, we introduce CausalBench, a transparent, fair, and easy-to-use evaluation platform, aiming to (a) enable the advancement of research in causal learning by facilitating scientific collaboration in novel algorithms, datasets, and metrics and (b) promote scientific objectivity, reproducibility, fairness, and awareness of bias in causal learning research. CausalBench provides services for benchmarking data, algorithms, models, and metrics, impacting the needs of a broad of scientific and engineering disciplines.} }
@article{10.1145/3655635, title = {Machine Learning in Computer Security is Difficult to Fix}, journal = {Commun. ACM}, volume = {67}, pages = {103}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3655635}, url = {https://doi.org/10.1145/3655635}, author = {Biggio, Battista} }
@inproceedings{10.1145/3697090.3699798, title = {Comparison of Machine Learning Algorithms for Detecting Software Aging in SQL Server}, booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing}, pages = {159--164}, year = {2024}, isbn = {9798400717406}, doi = {10.1145/3697090.3699798}, url = {https://doi.org/10.1145/3697090.3699798}, author = {Nascimento, Maria Gizele and Moura, Rafael Jos\'e and Machida, Fumio and Andrade, Ermeson}, keywords = {Software Aging Detection, Machine Learning, Memory Exhaustion.}, abstract = {Software aging is a phenomenon characterized by the progressive degradation of system performance, resulting from the accumulation of internal erros, such as memory leaks and resource exhaustion. Efficient detection of this process is essential to prevent critical failures in production environments. Although several studies use Machine Learning (ML) algorithms to detect software aging, systematic comparison between these algorithms is still limited, especially in terms of their ability to predict resource exhaustion. This paper aims to fill this gap by comparing ML algorithms for detecting software aging, focusing on RAM memory exhaustion as the main indicator. The analysis was conducted using a dataset on RAM memory usage in SQL Server, applying the algorithms K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF). The performance of the models was evaluated using the metrics Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and coefficient of determination (R2). Based on these indicators, it was possible to identify the most accurate algorithm and predict the time until memory exhaustion.} }
@inproceedings{10.1145/3689939.3695783, title = {The Quantum Imitation Game: Reverse Engineering of Quantum Machine Learning Models}, booktitle = {Proceedings of the 2024 Workshop on Attacks and Solutions in Hardware Security}, pages = {48--57}, year = {2024}, isbn = {9798400712357}, doi = {10.1145/3689939.3695783}, url = {https://doi.org/10.1145/3689939.3695783}, author = {Ghosh, Archisman and Ghosh, Swaroop}, keywords = {quantum machine learning, quantum security, reverse engineering, location = Salt Lake City, UT, USA}, abstract = {Quantum Machine Learning (QML) is an amalgamation of quantum computing paradigms with machine learning models, providing significant prospects for solving complex problems. However, with the expansion of numerous third-party vendors in the Noisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security of QML models is of prime importance, particularly against reverse engineering, which could expose sensitive parameters and proprietary algorithms embedded within the models. We assume the untrusted third-party quantum cloud provider is an adversary having white-box access to the transpiled version of the user-designed trained QML model during inference. Although the adversary can steal and use the model without any modification, reverse engineering (RE) to extract the pre-transpiled copy of the QML circuit will enable re-transpilation and usage of the model for various hardware with completely different native gate sets and even different qubit technology. The information about the parameters (e.g., number of parameters, their placements, and optimized values) can allow further training of the QML model if the adversary plans to alter the QML model to tamper with the watermark and/or embed their own watermark or refine the model for other purposes. In this first effort to investigate the RE of QML circuits, we examine quantum classifiers by comparing the training accuracy of original and reverse-engineered models across various sizes (i.e., number of qubits and number of parametric layers) of Quantum Neural Networks (QNNs). We note that multi-qubit classifiers can be reverse-engineered under specific conditions with a mean error of order 10^-2 in a reasonable time. We also propose adding dummy rotation gates in the QML model with fixed parameters to increase the RE overhead for defense. For instance, an addition of 2 dummy qubits and 2 layers increases the overhead by ~1.76 times for a classifier with 2 qubits and 3 layers with a performance overhead of less than 9\%. We note that RE is a very powerful attack model which warrants further efforts on defenses.} }
@article{10.1145/3611383, title = {Machine Learning and Physics: A Survey of Integrated Models}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3611383}, url = {https://doi.org/10.1145/3611383}, author = {Seyyedi, Azra and Bohlouli, Mahdi and Oskoee, Seyedehsan Nedaaee}, keywords = {Machine learning-guided physics, physics-guided machine learning, modeling, neural networks, physics-based models, deep learning, machine learning}, abstract = {Predictive modeling of various systems around the world is extremely essential from the physics and engineering perspectives. The recognition of different systems and the capacity to predict their future behavior can lead to numerous significant applications. For the most part, physics is frequently used to model different systems. Using physical modeling can also very well help the resolution of complexity and achieve superior performance with the emerging field of novel artificial intelligence and the challenges associated with it. Physical modeling provides data and knowledge that offer a meaningful and complementary understanding about the system. So, by using enriched data and training phases, the overall general integrated model achieves enhanced accuracy. The effectiveness of hybrid physics-guided or machine learning-guided models has been validated by experimental results of diverse use cases. Increased accuracy, interpretability, and transparency are the results of such hybrid models. In this article, we provide a detailed overview of how machine learning and physics can be integrated into an interactive approach. Regarding this, we propose a classification of possible interactions between physical modeling and machine learning techniques. Our classification includes three types of approaches: (1)\&nbsp;physics-guided machine learning (2)\&nbsp;machine learning-guided physics, and (3)\&nbsp;mutually-guided physics and ML. We studied the models and specifications for each of these three approaches in-depth for this survey.} }
@inproceedings{10.1145/3767052.3767054, title = {Machine Learning Models for Bank Customer Churn Prediction: A Comparative Study of LightGBM, CatBoost, and XGBoost}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {6--16}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767054}, url = {https://doi.org/10.1145/3767052.3767054}, author = {Zhang, Hanyuan and Wang, Yankai and Li, Zexuan and Wang, Xiaoyin}, keywords = {bank customer churn prediction, exploratory data analysis, machine learning}, abstract = {This research is dedicated to forecasting bank customer churn by using machine learning methods. In the highly competitive landscape of the banking industry, retaining customers has emerged as a critical factor for a bank's success. Eleven distinct machine learning models were utilized in the analysis of a bank customer dataset. Prior to model implementation, comprehensive data preprocessing was conducted, encompassing the management of missing values, encoding of categorical variables, and detection of outliers. The models were then evaluated under various data split ratios and sampling methods, with recall serving as the primary evaluation metric.The study's findings indicate that LightGBM, CatBoost, and XGBoost demonstrate superior performance in predicting customer churn. Additionally, Gradient Boost, Ada Boost, and Bagging prove to be effective in handling imbalanced data. Several key predictors of customer churn, such as Total_Trans_Amt and Total Trans ct, were identified. These results offer valuable practical implications for banks in formulating customer retention strategies. However, it is necessary to recognize the research's limitations. There may be deficiencies in data representation. What's more, external factors that could influence customer churn have been excluded.} }
@inproceedings{10.1145/3639478.3639797, title = {Enhancing Model-Driven Reverse Engineering Using Machine Learning}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {173--175}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3639797}, url = {https://doi.org/10.1145/3639478.3639797}, author = {Siala, Hanan Abdulwahab}, keywords = {application programs, model driven reverse engineering (MDRE), unified modeling language (UML), object constraint language (OCL), machine learning, large language models (LLMS), program comprehension, location = Lisbon, Portugal}, abstract = {Organizations often rely on large applications that are classified as legacy systems due to their dependence on outdated programming languages or platforms. To modernize these systems, it is necessary to understand their architecture, functionality, and business rules. Our research aims to define a novel model-driven reverse engineering (MDRE) approach to extract Unified Modeling Language (UML) and Object Constraint Language (OCL) representations from source code using Large Language Models (LLMs).} }
@inproceedings{10.1145/3675888.3676144, title = {Automated Examination System using Machine Learning and Natural Language Processing}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {752--761}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676144}, url = {https://doi.org/10.1145/3675888.3676144}, author = {Bhadouria, Aman and Gupta, Pranav and Bindal, Parish and Madan, Kapil and Sonal, Sonal}, keywords = {Examination System, NLP, OpenSource, Question Generation, location = Noida, India}, abstract = {Traditional examination processes are burdened with inefficiencies and vulnerabilities that hinder the educational process. Manual tasks such as question formulation and answer grading consume significant time and resources, while security concerns regarding physical exam materials persist. In response, this paper proposes an automated examination system leveraging advanced technological tools such as machine learning and natural language processing. By addressing the shortcomings of traditional methods and enhancing efficiency and security, this system aims to revolutionize academic evaluation.} }
@inproceedings{10.1145/3658644.3691366, title = {Demo: FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {5075--5077}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3691366}, url = {https://doi.org/10.1145/3658644.3691366}, author = {Gu, Yuechun and He, Jiajie and Chen, Keke}, keywords = {differential privacy, membership inference attack, location = Salt Lake City, UT, USA}, abstract = {Data privacy has been a top concern in the AI era. Despite the recent development of differentially private learning methods, controlled data access remains a mainstream method for protecting data privacy in many industrial and research environments. In controlled data access, authorized model builders work in a restricted environment to access sensitive data, which can fully preserve data utility with reduced risk of data leak. However, unlike differential privacy, there is no quantitative measure for individual data contributors to tell their privacy risk before participating in a machine learning task. We developed the demo prototype FT-PrivacyScore to show that it's possible to efficiently and quantitatively estimate the privacy risk of participating in a model fine-tuning task. The demo source code will be available at https://github.com/RhincodonE/demo_privacy_scoring.} }
@inproceedings{10.1145/3589806.3600039, title = {Integrated Reproducibility with Self-describing Machine Learning Models}, booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability}, pages = {1--14}, year = {2023}, isbn = {9798400701764}, doi = {10.1145/3589806.3600039}, url = {https://doi.org/10.1145/3589806.3600039}, author = {Wonsil, Joseph and Sullivan, Jack and Seltzer, Margo and Pocock, Adam}, keywords = {Machine Learning, provenance, reproducibility, location = Santa Cruz, CA, USA}, abstract = {Researchers and data scientists frequently want to collaborate on machine learning models. However, in the presence of sharing and simultaneous experimentation, it is challenging both to determine if two models were trained identically and to reproduce precisely someone else’s training process. We demonstrate how provenance collection that is tightly integrated into a machine learning library facilitates reproducibility. We present MERIT, a reproducibility system that leverages a robust configuration system and extensive provenance collection to exactly reproduce models, given only a model object. We integrate MERIT with Tribuo, an open-source Java-based machine learning library. Key features of this integrated reproducibility framework include controlling for sources of non-determinism in a multi-threaded environment and exposing the training differences between two models in a human-readable form. Our system allows simple reproduction of deployed Tribuo models without any additional information, ensuring data science research is reproducible. Our framework is open-source and available under an Apache 2.0 license.} }
@inproceedings{10.1145/3624062.3626083, title = {Machine Learning Applied to Single-Molecule Activity Prediction}, booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {66--72}, year = {2023}, isbn = {9798400707858}, doi = {10.1145/3624062.3626083}, url = {https://doi.org/10.1145/3624062.3626083}, author = {Hood, Kendric and Guan, Qiang}, keywords = {chemistry, data sets, machine learning, neural networks, single-molecule, location = Denver, CO, USA}, abstract = {Catalytic processes are used in about 1/3 of US manufacturing, from the field of chemical engineering to renewable energy. Assessing the activity of single-molecules, or individual molecules, is necessary to the development of efficient catalysts. Their heterogeneity structure leads to particle-specific catalytic activity. Experimentation with single-molecules can be time consuming and difficult. We purpose a Machine learning (ML) model that allows chemical researchers to run shorter single-molecule experiments to obtain the same level of results. We use common and widely understood ML methods to reduce complexity and enable accessibility to the chemical engineering community. We reduce the experiment time by up to 83\%. Our evaluation shows that a small data set is sufficient to train an acceptable model. 300 experiments are needed, including the validation set. We use a well understood multilayer perceptron (MLP) model. We show that more complex models are not necessary and simpler methods are not sufficient.} }
@inproceedings{10.1145/3705927.3705947, title = {Optimizing Medical Imaging: High-Performance Hardware for Image Processing, Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Digital Medicine and Image Processing}, pages = {112--116}, year = {2025}, isbn = {9798400709586}, doi = {10.1145/3705927.3705947}, url = {https://doi.org/10.1145/3705927.3705947}, author = {N S, Kalyan Chakravarthy and Balapanur, Mouli Chandra and Pandian, Arun Nambi and Pulikanti, Jyothi and D, Prasad and Syed Masood, Jafar Ali Ibrahim}, keywords = {GPUS, High-performance hardware, Machine learning and image processing, Medical imaging}, abstract = {The demand for high-performance hardware solutions for machine learning tasks is growing as medical imaging evolves. In this paper, we will focus on the latest hardware advanced technologies: GPUs, TPUs and FPGAs that can be used for image processing optimization in medical imaging such as brain tumor segmentation or chest X-ray classification. We test these devices on the BraTS and ChestX-ray14 datasets to show how running models quicker increases diagnostic performance. The study offers a comprehensive performance and energy efficiency evaluation of all the considered hardware solutions when training deep learning models for medical imaging, providing clinical guidelines to choose appropriate new-generation accelerators before engaging in expensive leasing contracts.} }
@inbook{10.1145/3730436.3730494, title = {Leveraging Machine Learning for Employee Resignation Prediction in HR Analytics}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {342--346}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730494}, author = {Fang, Xiang}, abstract = {A major issue in human resource management, employee attrition prediction offers insightful information for corporate decision-making. In this regard, conventional approaches including decision trees have only shown modest success. These techniques usually presume feature independence, though, and struggle to fit the relationships in the data. Using Graph Attention Networks (GAT), we present a novel method for estimating employee attrition that makes use of the linkages and similarities among employees to raise prediction performance. In this study, we represent each employee as a node in a graph, with their personal attributes (such as age, salary, job satisfaction, etc.) serving as node features. We build an adjacency matrix based on employee similarity computed with cosine similarity or Euclidean distance. We develop a GAT model using this graph structure that aggregates surrounding node features based on attention-based criteria therefore enabling the model to weigh the significance of various relationships between employees. The GAT-based model beats conventional logistic regression according to experimental data, therefore greatly enhancing the accuracy of employee attrition prediction. Improving prediction performance depends critically on the model's capacity to replicate dependencies between characteristics and include relational information from adjacent nodes. This paper shows the possibilities of GAT in employee attrition prediction and emphasizes its capacity to model complex interactions inside employee data, therefore providing a fresh approach for strategic workforce management and human resource analytics.} }
@inproceedings{10.1145/3641512.3686393, title = {In-Sensor Machine Learning: Radio Frequency Neural Networks for Wireless Sensing}, booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing}, pages = {261--270}, year = {2024}, isbn = {9798400705212}, doi = {10.1145/3641512.3686393}, url = {https://doi.org/10.1145/3641512.3686393}, author = {Tong, Jingyu and An, Zhenlin and Zhao, Xiaopeng and Liao, Sicong and Yang, Lei}, keywords = {in-sensor machine learning, physical neural network, location = Athens, Greece}, abstract = {Growing interest in wireless sensing, a cornerstone of the Artificial Intelligence of Things (AIoT), stems from its ability to gauge target states through nearby wireless signals. However, the escalating count of AIoT nodes escalates redundant data flow and exacerbates energy usage in AI cloud infrastructures. This amplifies the urgency for machine learning techniques that function in proximity to, or directly within, sensors. In light of this, we present the Radio-Frequency Neural Network (RFNN), a novel architecture that uses cost-effective transmissive intelligent surfaces to mimic the functions of a traditional neural network near (or in) sensors, transforming sensory nodes into intelligent terminals primed for machine learning. We first devised a unique training algorithm to mitigate the issues arising from unmodelable error-backward propagation; secondly, we incorporated contrastive learning to address the issue of blind labels stemming from environmental uncertainties. Our RFNN prototype, resonating at a 5 GHz WiFi bandwidth, has been honed across nine varied sensing tasks. The rigorous evaluation shows that it achieves a mean accuracy of 91.5\% while consuming only 67.2 μJ of energy. This positions RFNN as a match in inferencing prowess to its electronic neural network counterparts but with significantly diminished energy demands.} }
@inproceedings{10.1145/3718751.3718877, title = {Optimizing Vegetable Pricing and Replenishment Decisions Using Hybrid Machine Learning Models}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {770--774}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718877}, url = {https://doi.org/10.1145/3718751.3718877}, author = {Han, Yiyou and Huang, Ziheng and Xu, Yihong}, keywords = {ARIMA, Genetic Algorithm, Linear Regression, PSO, Replenishment}, abstract = {In the modern business environment, the automatic pricing and replenishment decision of vegetable commodities has become the core to ensure the smooth supply chain and the profit of merchants. This study combined ARIMA, particle swarm optimization (PSO), linear regression and genetic algorithm to propose and implement an automatic pricing and replenishment strategy based on a mixed machine learning model and a large number of historical sale data. The average (R^2) of different categories of vegetables was 0.992, indicating a high accuracy of the model. At the same time, it provided a pricing and purchase strategy for vegetables for merchants.} }
@inproceedings{10.1145/3665320.3670994, title = {Implementing a Machine Learning Deformer for CG Crowds: Our Journey}, booktitle = {Proceedings of the 2024 Digital Production Symposium}, year = {2024}, isbn = {9798400706905}, doi = {10.1145/3665320.3670994}, url = {https://doi.org/10.1145/3665320.3670994}, author = {Arcelin, Bastien and S\'ebastien, Maraux and Chaverou, Nicolas}, keywords = {Animation, Crowds, Neural Networks, Rigging, location = Denver, CO, USA}, abstract = {CG crowds have become increasingly popular this last decade in the VFX and animation industry: formerly reserved to only a few high end studios and blockbusters, they are now widely used in TV shows or commercials. Yet, there is still one major limitation: in order to be ingested properly in crowd software, studio rigs have to comply with specific prerequisites, especially in terms of deformations. Usually only skinning, blend shapes and geometry caches are supported preventing close-up shots with facial performances on crowd characters. We envisioned two approaches to tackle this: either reverse engineer the hundreds of deformer nodes available in the major DCCs/plugins and incorporate them in our crowd package, or surf the machine learning wave to compress the deformations of a rig using a neural network architecture. Considering we could not commit 5+ man/years of development into this problem, and that we were excited to dip our toes in the machine learning pool, we went for the latter. From our first tests to a minimum viable product, we went through hopes and disappointments: we hit multiple pitfalls, took false shortcuts and dead ends before reaching our destination. With this paper, we hope to provide a valuable feedback by sharing the lessons we learnt from this experience.} }
@inproceedings{10.1145/3702468.3702478, title = {A Comparative Analysis of Machine Learning Techniques for Therapeutic Warfarin Dose Prediction}, booktitle = {Proceedings of the 2024 7th International Conference on Robot Systems and Applications}, pages = {59--63}, year = {2024}, isbn = {9798400717031}, doi = {10.1145/3702468.3702478}, url = {https://doi.org/10.1145/3702468.3702478}, author = {Khianchainat, Khatadet and Kanjanawattana, Sarunya}, keywords = {Machine learning, Warfarin, Grid search, Artificial intelligence, Dose Estimation}, abstract = {Warfarin is a crucial anticoagulant medication used for the prevention and treatment of thromboembolic disorders. This study aims to develop an effective machine learning model for predicting appropriate warfarin dosing for patients in Thailand, considering their Asian origin and based on Thailand medication process. Our dataset for this preliminary investigation was derived from IWPC. We conducted the necessary data preprocessing, feature selection, and balancing procedures to acquire the suitable dataset. Nine machine learning algorithms that have been refined include the following: Multilayer Perceptron Classifier, Support Vector Machines Classifier, K Nearest Neighbours, Extreme Gradient Boosting, Categorical Boosting Classifier, Gradient Boosting Tree, Random Forest, Decision Tree Classifier, and Logistic Regression. The experiment yielded the finding that, in comparison to the other algorithms, the Categorical Boosting Classifier exhibited the maximum accuracy (72\%).} }
@inproceedings{10.1145/3652037.3663949, title = {Predicting Stress among Students via Psychometric Assessments and Machine Learning}, booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {662--669}, year = {2024}, isbn = {9798400717604}, doi = {10.1145/3652037.3663949}, url = {https://doi.org/10.1145/3652037.3663949}, author = {Ghosh, Sagnik and Tripathi, Kirti and Garg, Ankita and Singh, Dinesh and Prasad, Amit and Bhavsar, Arnav and Dutt, Varun}, keywords = {Artificial Neural Networks, Common Yoga Protocol, Mental Health, R-squared Score, RMSPE, Stress, location = Crete, Greece}, abstract = {Yoga is recommended as a method for managing stress among students. Despite its widespread acceptance, however, there exists a gap in research concerning the prediction of changes in psychological stress among students practicing yoga. The main objective of this study was to assess the immediate consequences of practicing yoga among students on their psychological stress levels and to predict changes in stress levels via machine learning. A total of 166 participants were recruited in this study and were randomly divided into two groups: intervention (N = 110) and control N =56). The intervention group engaged in regular, structured sessions of the common yoga protocol by the Indian Government thrice a week in 45 minutes sessions over a period of 6 weeks. The self-reported questionnaire such as the Depression, Anxiety, Stress Scales (DASS 21), was employed to assess the psychological stress change before and after the intervention. Additionally, machine learning (ML) algorithms such as Linear Regression (LR), Random Forest (RF), Support Vector Regression (SVR), and Artificial Neural Networks (ANN) were developed to predict changes in stress levels as a result of the intervention from data collected before the intervention. These models were evaluated on metrics such as R-squared and root mean square percent error (RMSPE), with the RF algorithm showing the lowest RMSPE of 1.23 units and R-squared of 0.71 by relying on top 7 features. This research not only affirms the positive effects of yoga on psychological health but also highlights the utility of machine learning in predicting stress changes, offering new perspectives on yoga's role in stress management.} }
@inproceedings{10.1145/3597638.3608421, title = {Reimagining Machine Learning's Role in Assistive Technology by Co-Designing Exergames with Children Using a Participatory Machine Learning Design Probe}, booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility}, year = {2023}, isbn = {9798400702204}, doi = {10.1145/3597638.3608421}, url = {https://doi.org/10.1145/3597638.3608421}, author = {Duval, Jared and Turmo Vidal, Laia and M\'arquez Segura, Elena and Li, Yinchu and Waern, Annika}, keywords = {Designing with Children, Games, Participatory Machine Learning, Physical Therapy, Play, Sensory Based Motor Disorder, location = New York, NY, USA}, abstract = {The paramount measure of success for a machine learning model has historically been predictive power and accuracy, but even a gold-standard accuracy benchmark fails when it inappropriately misrepresents a disabled or minority body. In this work, we reframe the role of machine learning as a provocation through a case study of participatory work co-creating exergames by employing machine learning and its training as a source of play and motivation rather than an accurate diagnostic tool for children with and without Sensory Based Motor Disorder. We created a design probe, Cirkus, that supports nearly any aminal locomotion exergame while collecting movement data for training a bespoke machine learning model. During 5 participatory workshops with a total of 30 children using Cirkus, we co-created a catalog of 17 exergames and a resulting machine-learning model. We discuss the potential implications of reframing machine learning’s role in Assistive Technology for values other than accuracy, share the challenges of using “messy” movement data from children with disabilities in an ever-changing co-creation context for training machine learning, and present broader implications of using machine learning in therapy games.} }
@inproceedings{10.1145/3708036.3708152, title = {Research on Car Insurance Compensation Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {687--691}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708152}, url = {https://doi.org/10.1145/3708036.3708152}, author = {Han, Yichen}, keywords = {Car Insurance, Logistic Regression, Multilayer Perceptron, Random Forest}, abstract = {In the field of insurance claims, it is important to understand the impact of each variable on the amount of the claim. Accurate prediction of whether a car insurance claim will be paid out and the amount of the claim will be paid out is of great significance to increase the company's profit, optimize the premium pricing, reduce the financial risk of the insurance company, and enhance the competitiveness of the insurance company in the market. In this study, we analyze industry reports, market data and relevant literature to study the key factors affecting the amount of insurance claims, firstly, we construct the ‘Insurance Claims Prediction Model’ and select five variables that have the greatest impact on the amount of car insurance claims, and then we predict the amount of car insurance claims through the selected variables. By analyzing the intrinsic connection of the data, based on the random forest model, it predicts whether the insurance is paid or not, and selects the variables that affect the insurance compensation amount, and predicts whether the insurance is paid or not; in addition, in order to accurately predict the amount of the insurance compensation, based on the theory of neural network, it constructs a large-data neural network prediction model, and achieves good experimental results, and obtains a low error. This study conducted the preliminary research on the prediction of car insurance compensation amount, which provides an important reference value for the subsequent formulation of the premium amount of car insurance.} }
@inproceedings{10.1145/3715669.3726802, title = {Predicting Mental Demand of Teammates Using Eye Tracking Metrics: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726802}, url = {https://doi.org/10.1145/3715669.3726802}, author = {Atweh, Jad and Riggs, Sara}, keywords = {Eye Tracking, Teams, Mental Demand, Complex Systems, ML}, abstract = {Analyzing eye tracking data using machine learning (ML) offers new insights into cognitive states such as mental workload. However, most research has focused on individuals rather than teams, where workload emerges from shared attention and coordination. Additionally, traditional workload assessment methods like the NASA Task Load Index (NASA-TLX) face limitations in aggregating subscales, raising concerns about their accuracy. This study explores the feasibility of using gaze-based ML models to classify mental demand in Unmanned Aerial Vehicle (UAV) command-and-control (C2) teams. Eye tracking and workload data were collected from four experimental studies involving UAV C2 tasks. Using eight ML classifiers, we evaluated whether gaze features could predict workload levels. k-Nearest Neighbors (kNN) achieved the highest accuracy (81\%) and precision (90\%), outperforming other models. These findings demonstrate the potential of real-time gaze-based workload monitoring of teams in high-stakes environments, paving the way for adaptive systems that support team performance and situation awareness.} }
@proceedings{10.1145/3690771, title = {ACMLC '24: Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, year = {2024}, isbn = {9798400710018} }
@article{10.1145/3545574, title = {The Role of Machine Learning in Cybersecurity}, journal = {Digital Threats}, volume = {4}, year = {2023}, doi = {10.1145/3545574}, url = {https://doi.org/10.1145/3545574}, author = {Apruzzese, Giovanni and Laskov, Pavel and Montes de Oca, Edgardo and Mallouli, Wissam and Brdalo Rapa, Luis and Grammatopoulos, Athanasios Vasileios and Di Franco, Fabio}, keywords = {Cybersecurity, incident detection, machine learning, artificial intelligence}, abstract = {Machine Learning (ML) represents a pivotal technology for current and future information systems, and many domains already leverage the capabilities of ML. However, deployment of ML in cybersecurity is still at an early stage, revealing a significant discrepancy between research and practice. Such a discrepancy has its root cause in the current state of the art, which does not allow us to identify the role of ML in cybersecurity. The full potential of ML will never be unleashed unless its pros and cons are understood by a broad audience.This article is the first attempt to provide a holistic understanding of the role of ML in the entire cybersecurity domain—to any potential reader with an interest in this topic. We highlight the advantages of ML with respect to human-driven detection methods, as well as the additional tasks that can be addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic problems affecting real ML deployments in cybersecurity. Finally, we present how various stakeholders can contribute to future developments of ML in cybersecurity, which is essential for further progress in this field. Our contributions are complemented with two real case studies describing industrial applications of ML as defense against cyber-threats.} }
@inproceedings{10.1145/3597512.3597522, title = {MACAIF: Machine Learning Auditing for Clinical AI Fairness}, booktitle = {Proceedings of the First International Symposium on Trustworthy Autonomous Systems}, year = {2023}, isbn = {9798400707346}, doi = {10.1145/3597512.3597522}, url = {https://doi.org/10.1145/3597512.3597522}, author = {Barnard, Pepita and Bautista, John Robert and Krook, Joshua and Liu, Anqi and Men\'endez, H\'ector and Schmidt, Aurora and Sookoor, Tamim}, keywords = {Auditing, Dashboard, Doctor-centred, Healthcare, MLighter, Machine Learning, location = Edinburgh, United Kingdom}, abstract = {Artificial intelligence in the form of machine learning algorithms is driving the latest industrial revolution, leading to disruptive changes in the ways we communicate, interact, design, collect information, and express ourselves. While these changes offer new possibilities for our societies, they may also introduce biases that can lead to unfair decisions. This issue is particularly critical in the context of medical diagnosis, as bias can jeopardize patient treatment and health. To mitigate these biases, it is essential to such biases and involve all relevant stakeholders in the design of fair machine learning algorithms. In this context, the MACAIF project aims to develop user-centred interfaces that allow stakeholders, including doctors, to challenge the fairness of machine learning algorithms based on demographics, such as gender or race. Our project proposes a methodology to engage with stakeholders and incorporate their concerns during the design of a dashboard based on MLighter - an adversarial tool which is applied to identify fairness-related issues in machine learning models.} }
@inproceedings{10.1145/3713043.3728853, title = {"It’s Just a Machine that Predicts" - Demystifying Artificial Intelligence / Machine Learning with Teenagers}, booktitle = {Proceedings of the 24th Interaction Design and Children}, pages = {168--182}, year = {2025}, isbn = {9798400714733}, doi = {10.1145/3713043.3728853}, url = {https://doi.org/10.1145/3713043.3728853}, author = {Klemettil\"a, Pauli Aleksi and Sharma, Sumita and Mochiyama, Fumika and Iivari, Netta and Iwata, Megumi and Koivisto, Jussi}, keywords = {AI/ML Literacy, Youth Perceptions, Cross-cultural studies}, abstract = {Teenagers today face an expanding and unpredictable role of Artificial Intelligence (AI) in society, yet educational interventions are still catching up. AI literacy is crucial for teenagers (and others) to help them make informed decisions about their futures. We present our work on Artificial Intelligence / Machine Learning (AI/ML) literacy with 43 Japanese and 20 Finnish high school students, exploring their pre-existing perceptions of AI/ML and how those perceptions evolved after they trained image classification models. Our findings indicate that while a substantial number of teenagers still have limited or contradictory understanding of the topic, even short-term workshops can be effective in demystifying core AI/ML concepts when the activities are tailored to their interests. Furthermore, we find that cultural background and language may factor into how teenagers perceive AI. Our study contributes to growing research on AI/ML literacy by focusing on teenagers and including cross-cultural perspectives.} }
@article{10.1145/3665929, title = {IoT Video Delivery Optimization through Machine Learning-Based Frame Resolution Adjustment}, journal = {ACM Trans. Multimedia Comput. Commun. Appl.}, volume = {20}, year = {2024}, issn = {1551-6857}, doi = {10.1145/3665929}, url = {https://doi.org/10.1145/3665929}, author = {Bandung, Yoanes and Wicaksono, Mokhamad Arfan and Pribadi, Sean and Langi, Armein Z. R. and Tanjung, Dion}, keywords = {Internet of video things, machine learning, time series forecasting, throughput prediction, file size estimator}, abstract = {Providing acceptable video quality in the Internet of Things (IoT) implementation poses a significant challenge, mainly when the application is performed on low-cost and low-power devices. This research focuses on developing a frame resolution adjustment system that maintains the frame rate value of video delivery in wireless IoT environments with resource-constrained devices. Consistent frame rates prevent motion lag and data loss, improving user experience. The system works by predicting the upcoming throughput values using machine learning methods to adjust the sensing parameter, which is the resolution of the video frame to be captured by camera nodes. Hence, the proposed system is equipped with a file size estimator to estimate the size of the next video frame and then adjust the resolution in accordance with the throughput prediction. In this research, we conducted extensive experiments to evaluate the accuracy of the file size estimator and the throughput prediction. The experiment generated a dataset to evaluate throughput prediction and file size estimator model. The evaluation results for the file size estimator showed a mean absolute percentage error (MAPE) of 6.73\% in the experiment using 317 frames with video resolutions between 72p and 720p. Experiments were also conducted to compare several machine learning methods for predicting throughput values. Compared to long short-term memory (LSTM) and autoregressive integrated moving average (ARIMA), simple exponential smoothing (SES) outperforms the others with the lowest root mean squared error (RMSE) and mean absolute error (MAE) values. Building upon these findings, we implemented the frame resolution adjustment system using SES as the method for predicting the upcoming throughput values. Finally, we demonstrated that the proposed system can maintain the frame rate according to the threshold set by the system while the resolution is being maximized, thereby addressing the challenges of maintaining video quality in resource-constrained IoT environments.} }
@proceedings{10.1145/3701047, title = {CNML '24: Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, year = {2024}, isbn = {9798400711688} }
@inproceedings{10.1145/3605764.3623905, title = {Information Leakage from Data Updates in Machine Learning Models}, booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security}, pages = {35--41}, year = {2023}, isbn = {9798400702600}, doi = {10.1145/3605764.3623905}, url = {https://doi.org/10.1145/3605764.3623905}, author = {Hui, Tian and Farokhi, Farhad and Ohrimenko, Olga}, keywords = {attribute inference, data update, machine learning, privacy, location = Copenhagen, Denmark}, abstract = {In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the updated model. Moreover, we observe that data records with rare values are more vulnerable to attacks, which points to the disparate vulnerability of privacy attacks in the update setting. When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model. These observations point to vulnerability of machine learning models to attribute inference attacks in the update setting.} }
@inproceedings{10.1145/3638782.3638790, title = {Tackling Disinformation: Machine Learning Solutions for Fake News Detection}, booktitle = {Proceedings of the 2023 13th International Conference on Communication and Network Security}, pages = {46--51}, year = {2024}, isbn = {9798400707964}, doi = {10.1145/3638782.3638790}, url = {https://doi.org/10.1145/3638782.3638790}, author = {Sangi, Abdur Rashid and Nagaram, Jyothish and Sudulagunta, Akshara and Talari, Sai Sandeep and Malla, Vinay and Enduri, Muralikrishna and Anamalamudi, Satish}, keywords = {Fake news, decision tree, machine learning, medical issues, naive bayes, performance measure, political issues, location = Fuzhou, China}, abstract = {Fake news is termed as the news that spreads via the internet very fast which is not true i.e., false news. Since we are in a society of modern living culture, we will be attracted to the trend easily. So, taking this as an advantage some business traders make this news as their profit by clicking on that fake news. We can observe these types of issues in areas like Political issues, medical issues, Job rackets, etc. The tremendous increase in the spreading of fake news may result in less hope for real news. The main goal is to create a resilient and effective system with the ability to automatically differentiate between authentic and falsified news articles. We can find whether the news is fake or real through machine learning algorithms with greater methodology. We have selected and implemented a few datasets using machine learning algorithms like Decision Tree, Naive Bayes, SVM, Random Forest, Logistic Regression, and Passive aggressive classifier. Further, we come up with the algorithm which gives the highest performance measures. The results from the experiments exhibit encouraging performance metrics in identifying fake news, highlighting machine learning’s potential in countering misinformation. The outcomes imply that blending various feature types and advanced algorithms leads to better performance when contrasted with individual methods. We have applied these ML methods on two datasets and achieved accuracy of 99.69\% with SVM, 99.06\% with Logistic Regression and 99.64} }
@inproceedings{10.1145/3690771.3690779, title = {A study of relationship between business performance and stock prices using machine learning techniques}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {14--19}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690779}, url = {https://doi.org/10.1145/3690771.3690779}, author = {Rukpanichsiri, Vorapat and Soonthornphisaj, Nuanwan}, keywords = {AdaBoost, Feature Importance, Gradient Boosting, Lasso regression, Neural Networks, Random Forest, Regressing Tree, Stock price}, abstract = {This research investigates the relationship between fundamental factors and stock prices. The study uses 50 stocks listed on the SET100 index in 2022. The fundamental factors of interest are earnings per share (EPS), price-to-book value (P/BV), net profit margin, dividend yield, return on assets (ROA), return on equity (ROE), and debt-to-equity ratio (D/E). The study uses quarterly data for 10 years, from 2012 to 2021. This results in 40 data points per stock. The relationship between fundamental factors and stock prices is investigated using machine learning methods, including Lasso, Regression Tree, Random Forest, AdaBoost, Xgboost, Gradient Boosting, and Neural Network. The relationship between fundamental factors and stock prices is evaluated using k-fold cross-validation. This method divides the data into training and testing sets. The performance of each model is measured using the R-squared value. The findings of the study suggest that some fundamental factors are significant predictors of stock prices. The strongest relationships are found in P/BV. The results also suggest that Neural Network methods can be used to identify the relationships between fundamental factors and stock prices.} }
@inproceedings{10.1145/3701571.3703393, title = {Decoding Fatigue: Analyzing Offline Handwriting with Machine Learning to Detect Perceived Exhaustion}, booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia}, pages = {487--489}, year = {2024}, isbn = {9798400712838}, doi = {10.1145/3701571.3703393}, url = {https://doi.org/10.1145/3701571.3703393}, author = {Schoen, Dominik and Kosch, Thomas and Becker, Till and Antwi-Boasiako, Godfred and Jung, Merret and Chioca Vieira, Ana Laura and M\"uhlh\"auser, Max and M\"uller, Florian}, keywords = {Exertion, Fatigue, Machine Learning, Drawing, Exhaustion, Discomfort}, abstract = {The quality and readability of an individual’s handwriting and drawing can be influenced by various factors, including their level of physical exertion. This enables us to explore the quantification of exertion by observing an individual’s handwriting. To test this hypothesis, we collected data from 17 participants, building a database of handwriting and drawing samples and their corresponding Borg 10 exertion ratings at the time of drawing. In this paper, we investigate using machine learning techniques to estimate perceived exertion before, during, and after physical activity based on handwriting and drawings. We apply a regression model to compare different drawing tasks and demonstrate that perceived exertion can be predicted using simple line drawings. However, more complex sketches and handwriting demand further research. Our findings suggest that interactive systems could use handwriting and drawing to intervene when users experience excessive discomfort.} }
@inproceedings{10.1145/3698062.3698088, title = {Exploring Machine Learning for Credit Card Fraud Detection from a Philippine Perspective}, booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)}, pages = {177--182}, year = {2024}, isbn = {9798400717086}, doi = {10.1145/3698062.3698088}, url = {https://doi.org/10.1145/3698062.3698088}, author = {Blancaflor, Eric and Asuncion, Keziah Dawn and Reyes, Hailie Jade and Verzosa, Michaela}, keywords = {Artificial Neural Network, Credit Card, Detection, Fraud Detection, Machine Learning, Philippines, Prevention, Support Vector Machines}, abstract = {This study examines how machine learning (ML) techniques are applied in the Philippine setting to identify credit card fraud. This research aims to provide insights into the effectiveness of ML techniques in fraud detection, focusing on customizing ML algorithms to the distinct patterns and dynamics of credit card fraud in the Philippines, considering the nation's unique economic, technological, and social milieu. The research assesses the efficacy of different machine learning (ML) models using available data on credit card fraud occurrences and suggests improving fraud detection systems in Philippine financial institutions through ML integration. It also examines the opportunities and difficulties of using ML-driven fraud detection techniques in the Philippine financial industry.} }
@article{10.5555/3586589.3586678, title = {Machine learning on graphs: a model and comprehensive taxonomy}, journal = {J. Mach. Learn. Res.}, volume = {23}, year = {2022}, issn = {1532-4435}, author = {Chami, Ines and Abu-El-Haija, Sami and Perozzi, Bryan and R\'e, Christopher and Murphy, Kevin}, keywords = {network embedding, graph neural networks, geometric deep learning, manifold learning, relational learning}, abstract = {There has been a surge of recent interest in graph representation learning (GRL). GRL methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding, focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between network embedding, graph regularization and graph neural networks. We propose a comprehensive taxonomy of GRL methods, aiming to unify several disparate bodies of work. Specifically, we propose the GRAPHEDM framework, which generalizes popular algorithms for semi-supervised learning (e.g. GraphSage, GCN, GAT), and unsupervised learning (e.g. DeepWalk, node2vec) of graph representations into a single consistent approach. To illustrate the generality of GRAPHEDM, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.} }
@inproceedings{10.1145/3745812.3745818, title = {Sustainable Vertical Farming: Leveraging Machine Learning and IoT for Energy Efficiency and Productivity}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745818}, url = {https://doi.org/10.1145/3745812.3745818}, author = {Gupta, Shruti and Das, Arnab and Sinha, Sanjay Kumar}, keywords = {Vertical farming, energy efficiency, machine learning, sustainability, water management}, abstract = {Vertical farming as an idea has emerged as a helpful strategy to mitigate problems related to food production throughout the world, less land available, and climate changes. Through the use of controlled environment agriculture, vertical farming consists of growing food in layers and within the vicinity of consumers, hence decreasing the need for conventional agriculture. One of the precision framing efficient crop growth practices that easily integrates machine learning (ML) technologies, and the Internet of Things (IoT) is vertical framing. Various technologies are used, including robotics, machine learning, big data analytics, drones, and IoT sensors in gathering data on crop growth, environmental conditions, and weather patterns. For this reason, energy efficiency is regarded as a significant parameter that determines the economic sustainability of vertical farms if not their environmental impacts. This article explores energy efficiency within vertical farming systems highlighting how technology, better practices, and the use of clean energy sources help counter the high energy needs of vertical farms. The paper also looks into energy-efficient systems and designs such as LED lights which consume lower electric power and are designed to produce light like natural sunlight, energy-efficient HVAC systems which operate with less energy, systems for recycling water and nutrients and minimize usage of these resources.} }
@inproceedings{10.1145/3726854.3727297, title = {NetJIT: Bridging the Gap from Traffic Prediction to Preknowledge for Distributed Machine Learning}, booktitle = {Abstracts of the 2025 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems}, pages = {145--147}, year = {2025}, isbn = {9798400715938}, doi = {10.1145/3726854.3727297}, url = {https://doi.org/10.1145/3726854.3727297}, author = {Ai, Xin and Li, Zijian and Zhu, Yuanyi and Chen, Zixuan and Liu, Sen and Xu, Yang}, keywords = {JIT, distributed machine learning, just-in-time program analysis, network optimization, traffic prediction, location = Stony Brook, NY, USA}, abstract = {Today's distributed machine learning (DML) introduces heavy traffic load, making the network one of the primary bottlenecks. To mitigate this bottleneck, existing state-of-the-art network optimization methods, such as traffic or topology engineering, are proposed to adapt to real-time traffic. However, current traffic measurement and prediction methods struggle to collect sufficiently fine-grained and accurate traffic patterns. This limitation impedes the ability of cutting-edge network optimization techniques to react agilely to the ever-changing traffic demands of DML jobs.This paper proposes NetJIT, a novel program-behavior-aware toolkit for foreseeing the traffic pattern of DML. To the best of our knowledge, this is the first work proposing the use of just-in-time (JIT) program analysis for real-time traffic measurement. In DML applications, communication behavior is primarily determined by the previously computed results. NetJIT leverages this characteristic to anticipate communication details by tracing and analyzing the data relations in the computation process. This capability enables the deployment of optimization strategies in advance.We deploy NetJIT in real-world network optimization for traffic preknowledge. Evaluation with the self-built testbed demonstrates that NetJIT can achieve up to about 97\% less error of detecting communication events compared with other methods. Simulations with real-world DML workloads further illustrate that NetJIT enables more precise network optimization, leading to approximately 50\% better network performance w.r.t the metrics including average iteration time, throughput, and average packet delay.} }
@inproceedings{10.1145/3627673.3679101, title = {Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5547--5550}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679101}, url = {https://doi.org/10.1145/3627673.3679101}, author = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Adhikari, Saugat}, keywords = {GNN, graph, graph neural network, structure, subgraph, system, vertex, location = Boise, ID, USA}, abstract = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand.In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems.} }
@inproceedings{10.1145/3627106.3627175, title = {Secure Softmax/Sigmoid for Machine-learning Computation}, booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference}, pages = {463--476}, year = {2023}, isbn = {9798400708862}, doi = {10.1145/3627106.3627175}, url = {https://doi.org/10.1145/3627106.3627175}, author = {Zheng, Yu and Zhang, Qizhi and Chow, Sherman S. M. and Peng, Yuxiang and Tan, Sijun and Li, Lichun and Yin, Shan}, keywords = {Crypto, Machine Learning, Secure Computation, Sigmoid, Softmax, location = Austin, TX, USA}, abstract = {Softmax and sigmoid, composing exponential functions (ex) and division (1/x), are activation functions often required in training. Secure computation on non-linear, unbounded 1/x and ex is already challenging, let alone their composition. Prior works aim to compute softmax by its exact formula via iteration (CrypTen, NeurIPS\&nbsp;’21) or with ASM approximation\&nbsp;(Falcon, PoPETS\&nbsp;’21). They fall short in efficiency and/or accuracy. For sigmoid, existing solutions such as ABY2.0 (Usenix Security\&nbsp;’21) compute it via piecewise functions, incurring logarithmic communication rounds. We study a rarely-explored approach to secure computation using ordinary differential equations and Fourier series for numerical approximation of rational/trigonometric polynomials over composition rings. Our results include 1) the first constant-round protocol for softmax and 2) the first 1-round error-bounded protocol for approximating sigmoid. They reduce communication by and , respectively, shortening the private training process of state-of-the-art frameworks or platforms, namely, CryptGPU (S\&amp;P\&nbsp;’21), Piranha (Usenix Security\&nbsp;’22), and quantized training from MP-SPDZ (ICML\&nbsp;’22), while maintaining competitive accuracy.} }
@inproceedings{10.1145/3674029.3674036, title = {Comparative Study of Machine Learning Techniques for Inventory Classification Based on Multi-Criteria Decision-Making}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {36--40}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674036}, url = {https://doi.org/10.1145/3674029.3674036}, author = {Phruksaphanrat, Busaba}, keywords = {ABC, AHP, Inventory classification, Machine learning, Multi-attribute decision-making, location = Oslo, Norway}, abstract = {Various multicriteria inventory classification methods have been developed to overcome the limitations of conventional ABC analysis. Commonly used techniques include the analytic hierarchy process (AHP) and data envelopment analysis (DEA). However, these methods are mainly focused on classifying existing items in the inventory. Furthermore, both total inventory costs and the similarity of each group should be of concern. To address the challenge of assigning groups to new, unclassified items in the warehouse, this research proposes integrating machine learning (ML) techniques with multicriteria inventory classification. The combined approach considers both similarity and total costs, thereby improving the accuracy of inventory classification for both existing and new items based on the existing groups classified using the multicriteria approach. The result has shown that among ABC analysis, DEA, and AHP; AHP outperforms in the classification of the current inventory items of the case study factory based on the minimum total inventory cost and similarity index. To achieve the highest accuracy in inventory classification, firstly discriminant analysis (DA) and artificial neural network (ANN) were identified as the most suitable machine learning (ML) techniques to be integrated. After tuning some parameters, the best adjusted ANN model was found with the highest accuracy at 97.70\% of testing data and F1 at 100\%, 94.74\%, and 98.25\% for classes A, B, and C, respectively.} }
@inproceedings{10.1145/3615979.3656064, title = {Detecting Emergent Behavior in Complex Systems: A Machine Learning Approach}, booktitle = {Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation}, pages = {81--87}, year = {2024}, isbn = {9798400703638}, doi = {10.1145/3615979.3656064}, url = {https://doi.org/10.1145/3615979.3656064}, author = {Dahia, Simranjeet Singh and Szabo, Claudia}, keywords = {Complex systems, Emergent behavior, location = Atlanta, GA, USA}, abstract = {The live identification of emergent behavior in complex systems with little a-priori information is a challenging task and existing approaches are either applicable to a small subset of models or do not scale well. In contrast, post-mortem approaches that have a more in-depth understanding of the characteristics of emergent properties often struggle with analyzing a large amount of data to extract relationships between the variables, events, and entities whose interaction eventually leads to emergent behavior. Machine learning approaches have been promoted as potential replacements of existing approaches, due to their ability to analyze large amounts of data without a-priori knowledge of existing relationships. In this paper, we present a first step towards the use of supervised learning approaches to identify and predict emergent behavior. Our hybrid approach unifies live and post-mortem perspectives by relying on a visual inspection of the simulation run and the simulation data set to identify a set of features that are more likely to generate emergent behavior (post-mortem) which are then used by a machine learning module to predict emergent behavior (live). Our analysis shows the potential of such approaches but also highlights challenges and future avenues of research.} }
@inproceedings{10.1145/3647444.3647926, title = {Early diagnosis of Parkinson disease using Machine Learning Techniques}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647926}, url = {https://doi.org/10.1145/3647444.3647926}, author = {Srivastava, Atul and Rana, Harshita and Dixit, Prashant and Singh, Reecha}, keywords = {Gradient Boosting, K-NN, Machine Learning, Parkinson Disease Prediction, Random Forest, SVM, location = Jaipur, India}, abstract = {Approximately two in every thousand people suffer from Parkinson disease. The symptoms of this neurological disorder can be motor or non-motor. However, it is significantly difficult to determine the gravity and appropriate classification of the disease. The diagnosis of PD majorly depends on the clinical examination and neurological examinations. Recently, machine learning techniques have proved to be an alternate method to detect the disease in a very early stage of it. Machine learning techniques use motor symptoms (gait analysis, handwriting, etc.) and non-motor symptoms (voice characteristics) to classify the people suffering and not suffering from PD. This study evaluated classifiers such as K-Nearest Neighbours (K-NN), Random Forest (RF), Gradient Boosting, Support Vector Machine (SVM), Boosting, and Bagging.} }
@inproceedings{10.1145/3570361.3615740, title = {Runtime WCET Estimation Using Machine Learning}, booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking}, year = {2023}, isbn = {9781450399906}, doi = {10.1145/3570361.3615740}, url = {https://doi.org/10.1145/3570361.3615740}, author = {Yun, Sangwoon and Kang, Kyungtae}, keywords = {embedded systems, real-time systems, neural networks, location = Madrid, Spain}, abstract = {Accurate task execution time estimation is vital for efficient and dependable operation of safety-critical systems. However, modern automotive functions' complexity challenges conventional estimation methods. To address this, we propose a novel technique that combines execution time and job sequence data using a multi-layer perceptron (MLP) neural network. Leveraging MLP's capabilities, our approach achieves impressive 99.7\% prediction accuracy with a mere 38.33 μs latency. Integrating our technique into safety-critical systems optimizes resource allocation and scheduling, enhancing performance and reliability. Importantly, our method extends beyond automotive systems, finding potential in diverse safety-critical domains. By precisely estimating task execution time, we enhance operational efficiency and decision-making in complex systems.} }
@article{10.1145/3539783, title = {Software Engineering of Machine Learning Systems}, journal = {Commun. ACM}, volume = {66}, pages = {35--37}, year = {2023}, issn = {0001-0782}, doi = {10.1145/3539783}, url = {https://doi.org/10.1145/3539783}, author = {Isbell, Charles and Littman, Michael L. and Norvig, Peter}, abstract = {Seeking to make machine learning more dependable.} }
@inproceedings{10.1145/3594806.3596591, title = {Predicting Adverse Childhood Experiences via Machine Learning Ensembles}, booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {773--779}, year = {2023}, isbn = {9798400700699}, doi = {10.1145/3594806.3596591}, url = {https://doi.org/10.1145/3594806.3596591}, author = {K Rao, Akash and Y Trivedi, Gunjan and Bajpai, Anshika and Singh Chouhan, Gajraj and G Trivedi, Riri and Kumar, Anita and Dutt, Varun and Soundappan, Kathirvel and Ramani, Hemalatha}, keywords = {Adverse Childhood Experiences, Childhood trauma, Depression, Insomnia, Machine learning, Random Forest, Suicidal Behavior, location = Corfu, Greece}, abstract = {Adverse Childhood Experiences (ACEs) have been linked to negative health outcomes later in life, including depression, anxiety, insomnia, and suicidal behavior. Recent studies have explored machine learning methods to classify individuals based on their ACE scores and predict their mental health outcomes. However, an extensive prediction of ACE via novel machine-learning ensembles based on several measures is yet to be undertaken. In this study, we used machine learning algorithms to classify individuals into high and low ACE groups and predict their mental health outcomes using various measures, including the Major Depressive Inventory, Generalized Anxiety Disorder, Insomnia Severity Index, World Health Organization Well-Being Index (WHO-5), suicide behavior, irrational decisions, self-harm, ability to focus, and suicidal thoughts. The study results showed that novel machine learning ensemble algorithms like a support-vector-decision tree ensemble and a support-vector-decision tree-random forest ensemble could accurately classify individuals into high and low ACE groups and predict their mental health outcomes. The study highlights the potential of using machine learning methods to identify individuals at high risk for mental health issues and provide targeted interventions to prevent the long-term negative consequences of ACEs.} }
@inproceedings{10.1145/3735014.3735893, title = {Risk Prediction of Container Cargo Loss and Damage Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {168--172}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735893}, url = {https://doi.org/10.1145/3735014.3735893}, author = {Ting, Nong}, keywords = {Container Cargo, Gradient Boosting Decision Tree, Loss and Damage, Machine Learning, Random Forest}, abstract = {With the rapid development of globalization, container cargo transportation occupies a core position in international trade, but the loss and damage of its goods have brought huge economic losses and reputation risks to both sides of the trade. To meet this challenge, this paper proposes a fusion model combining Random Forest (RF) and Gradient Boosting Decision Tree (GBDT) to predict the risk of loss and damage of container goods during transportation. Firstly, through collecting about 15,000 cargo transportation records of a company in the last three years, covering multi-dimensional data such as cargo attributes, transportation environment and whether it is lost or damaged, and cleaning and preprocessing the data. Subsequently, an integrated learning model based on RF and GBDT is constructed, and the model is trained and optimized by cross-validation and feature selection methods. The experimental results show that the fusion model is superior to the single RF and GBDT models in accuracy, precision, recall, and F1 score, with an accuracy of about 90\%, which shows high prediction accuracy and robustness. The research in this paper not only verifies the effectiveness of machine learning in the risk prediction of container cargo loss and damage but also puts forward a strategy to improve prediction accuracy through model fusion. In addition, according to the experimental results and the analysis of the importance of characteristics, this paper puts forward a series of measures to reduce the risk of goods transportation, such as strengthening the classification of goods and customizing protection measures, monitoring key environmental factors, optimizing transportation routes, etc., which provides scientific basis and practical suggestions for the risk management of goods transportation industry.} }
@inproceedings{10.1145/3691573.3691582, title = {Analysis of Cybersickness through Biosignals: an approach with Symbolic Machine Learning}, booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality}, pages = {11--20}, year = {2024}, isbn = {9798400709791}, doi = {10.1145/3691573.3691582}, url = {https://doi.org/10.1145/3691573.3691582}, author = {Nunes da Silva, Wedrey and Porcino, Thiago Malheiros and Castanho, Carla Denise and Jacobi, Ricardo Pezzuol}, keywords = {Biosignals, Cybersickness, Decision Tree, HMD Devices, Random Forest., Symbolic Machine Learning, Virtual Reality, location = Manaus, Brazil}, abstract = {Cybersickness represents one of the main obstacles to the use of Virtual Reality, often triggered by the use of Head-mounted Display devices. The symptoms associated with cybersickness can vary among individuals and include nausea, dizziness, eye strain, and headache, which may persist for minutes or even hours after exposure to Virtual Reality. According to the literature, cybersickness has a considerable impact on physiological signals such as delta waves in the Electroencephalogram; Heart Rate and Heart Rate Variability, derived from the Electrocardiogram; Electrodermal Activity; and Electrogastrography, all of which show a significant correlation with this condition. In this study, we investigated the use of biosignals to identify the possible causes associated with cybersickness in Virtual Reality. Our main hypothesis is that the combination of quantitative and subjective assessments, combined with Symbolic Machine Learning techniques, is effective in creating a ranking of the main causative/indicative factors of this condition. The results of this study highlight significant contributions to the understanding of factors influencing cybersickness symptoms. Statistical analyses confirmed the relationship between physiological changes and cybersickness symptoms. By including biosignals in our model, we achieved a significant gain, with an AUC of 0.95. The rankings of the main factors, both for the model without and with the inclusion of biosignals, confirmed previous research described in the literature. To the best of our knowledge, this is the first work to employ Symbolic Machine Learning models combining data from user profiles, game, and biosignals to detect the causes of cybersickness and generate a ranking of the most relevant factors.} }
@inproceedings{10.1145/3674029.3674054, title = {Degree of Difference in Clinical Data and Imaging Based on Machine Learning and Complex Network}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {153--157}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674054}, url = {https://doi.org/10.1145/3674029.3674054}, author = {Kong, Guanqing and Li, Xiuxu and Wu, Chuanfu and Zhang, Lanhua}, keywords = {Complex network, Data driven, Machine learning, Multimodal imaging, location = Oslo, Norway}, abstract = {Clinical patients should have corresponding clinical indicators to characterize the disease. Multimodal data and physiological indicators provide a basis for patient diagnosis and assessment, but small sample data pose statistical difficulties. In order to better support the clinical conclusions, from a data-driven perspective, using machine learning algorithms, we explored the support of physiological indicator data for multimodal data in the case of insufficient samples, and according to the results of the model, it is shown that the data-driven results can better support the final conclusions, and therefore, integrating the multimodal data and the clinical indicators can better provide the diagnosis and assessment conclusions for the clinical patients.} }
@inproceedings{10.1145/3697467.3697604, title = {Construction of a News Event Influence Assessment Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {76--79}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697604}, url = {https://doi.org/10.1145/3697467.3697604}, author = {Jiang, Junnan}, keywords = {Deep Learning, Multimodal Feature Fusion, News Event Influence}, abstract = {This study proposes a deep learning-based model for assessing the influence of news events by integrating multimodal features such as text, time series, and social networks to construct a comprehensive influence score metric. The model utilizes Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) to process feature data, and incorporates attention mechanisms to optimize feature weights. Experimental results demonstrate that the model performs excellently in short-term predictions, with a Mean Squared Error (MSE) of 0.022 and a Mean Absolute Error (MAE) of 0.015. However, the model's performance in long-term predictions is less satisfactory, with higher MSE and MAE, indicating that there is room for improvement in long-term prediction accuracy.} }
@inproceedings{10.1145/3675417.3675432, title = {Machine Learning in the Chinese Corporate Bond Market}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {84--90}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675432}, url = {https://doi.org/10.1145/3675417.3675432}, author = {Geng, Yiyang}, abstract = {Research on the factors influencing corporate bond yields has consistently been a focal point in the financial field. However, there is currently insufficient attention directed toward Chinese corporate bond yields, especially neglecting the consideration of the nonlinear and interactive relationships between variables and yields. This paper utilizes seven machine learning algorithms, including random forest and feed-forward neural networks, to compare and analyze the predictive effectiveness of each method on China's corporate bond yields. Additionally, it evaluates the importance of feature variables and variable combinations. The findings indicate that these methods demonstrate certain applicability in predicting Chinese corporate bond yields, revealing distinct nonlinear and interactive effects between these yields and the feature variables. Liquidity risk and downside risk emerge as the most critical predictive factors, highlighting the heightened sensitivity of market participants to both liquidity conditions and extreme risk scenarios. Notably, within the portfolio of liquidity risk, Amihud and ILLIQ, as well as CVaR10 and ES10 within the downside risk portfolio, along with the timing of bond issuance, emerge as the feature variables making the most significant predictive contributions.} }
@article{10.1613/jair.1.14238, title = {Generalizing Group Fairness in Machine Learning via Utilities}, journal = {J. Artif. Int. Res.}, volume = {78}, year = {2024}, issn = {1076-9757}, doi = {10.1613/jair.1.14238}, url = {https://doi.org/10.1613/jair.1.14238}, author = {Blandin, Jack and Kash, Ian A.}, abstract = {Group fairness definitions such as Demographic Parity and Equal Opportunity make assumptions about the underlying decision-problem that restrict them to classification problems. Prior work has translated these definitions to other machine learning environments, such as unsupervised learning and reinforcement learning, by implementing their closest mathematical equivalent. As a result, there are numerous bespoke interpretations of these definitions. This work aims to unify the shared aspects of each of these bespoke definitions, and to this end we provide a group fairness framework that generalizes beyond just classification problems. We leverage two fairness principles that enable this generalization. First, our framework measures outcomes in terms of utilities, rather than predictions, and does so for both the decision-maker and the individual. Second, our framework can consider counterfactual outcomes, rather than just observed outcomes, thus preventing loopholes where fairness criteria are satisfied through self-fulfilling prophecies. We provide concrete examples of how our utility fairness framework avoids these assumptions and thus naturally integrates with classification, clustering, and reinforcement learning fairness problems. We also show that many of the bespoke interpretations of Demographic Parity and Equal Opportunity fit nicely as special cases of our framework.} }
@inproceedings{10.1145/3723936.3723959, title = {Analysis of Momentum in Tennis Matches Based on Machine Learning Models}, booktitle = {Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis}, pages = {148--155}, year = {2025}, isbn = {9798400712234}, doi = {10.1145/3723936.3723959}, url = {https://doi.org/10.1145/3723936.3723959}, author = {Lin, Shengbo and Wu, Bingheng and Yuan, Feijie and Wang, Yuqi and Wen, Quan and Zhang, Puzhao}, keywords = {Binary Logistic Regression, LightGBM, Momentum, Tennis Matches, XGBoost}, abstract = {Momentum, as one of the critical factors affecting the outcomes of sports competitions, has long posed challenges in its quantification and mechanism of action. This study focuses on Wimbledon tennis matches, constructing and validating a multitask analytical framework to systematically explore momentum and its impact on match outcomes. Firstly, the study filters and normalizes predictive indicators affecting point wins and losses from Wimbledon match data. By combining binary logistic regression with various machine learning model optimizations, it designs an optimal model to capture scoring dynamics, enabling dynamic visual analysis of match processes. Secondly, momentum decomposition analysis is conducted based on data training results, and the potential influence of momentum on match outcomes is verified using the Kruskal-Wallis test (P = 0.27). The study further validates the model using data from the 2023 Wimbledon Men's Singles Final, achieving a predictive accuracy of 72\% through aggregate processing and the Hosmer-Lemeshow test. A LightGBM model is employed to visualize momentum transitions, while analysis of variance (ANOVA) identifies key factors affecting momentum changes. Finally, the model's performance and potential error sources are evaluated using PRC curves and Pearson analysis. Comparative analyses across different sports validate the model's generalizability and cross-discipline applicability. The findings provide novel insights into the quantification of momentum and its application in match prediction, demonstrating the model's extensive potential for dynamic performance analysis in sports.} }
@inproceedings{10.1145/3698062.3698091, title = {IPL Cricket Fantasy Team Prediction for Dream11 using Machine Learning}, booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)}, pages = {196--202}, year = {2024}, isbn = {9798400717086}, doi = {10.1145/3698062.3698091}, url = {https://doi.org/10.1145/3698062.3698091}, author = {Raju, K Bhavish and M, Govindarajan and Thakur, Kritin and Kumar, Ishan and Paladugula, Lakshmi Snigdha and Rajapurohit, Prarthana and K S, Srinivas}, keywords = {CatBoost (CB), Cricket, Dream11, Fantasy Contests, Grid Search CV, \&nbsp, RandomSearch CV, Indian Premier League (IPL), Player prediction, Random forest Classifier (RF), eXtreme Gradient Boosting (XGB)}, abstract = {This study aims to pick the best fantasy cricket team for IPL matches (T20) to participate in fantasy contests on platforms like Dream11 and My11 Circle. This is done by predicting the top eleven players from both the participating teams for a particular match in line with the requirements. A dataset of ball-by-ball details of each IPL match from 2016 to 2023 was obtained from the Cricsheet website [10]. This data was then preprocessed into the required format followed by the addition of important columns such as bowler faced by each batsman and their bowling style. Most importantly we obtained individual batsman-bowler match-ups data in each IPL match along with a fielding dataset as well. We used several Machine Learning models which included regression models such as Random Forest (RF), Extreme Gradient Boosting (XGB) and a baseline Linear Regression Model for the batting predictions, and classification models such as Random Forest, Gradient Boosting, CatBoost (CB), etc for bowling and fielding predictions. Based on the predictions we used a standardized formula to calculate the Dream11 fantasy points scored by the batsman, bowlers and fielders. From the calculated fantasy points, the top 11 players are picked into the fantasy team while the players with the highest and second highest points are selected as captain and vice-captain for bonus points.\&nbsp; The model was able to predict at least 7 players correctly as top performers in a match accurately 85\% of the time.} }
@inproceedings{10.1145/3584371.3612946, title = {Optimizing K-Mer Fingerprint Generation for Machine Learning}, booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics}, year = {2023}, isbn = {9798400701269}, doi = {10.1145/3584371.3612946}, url = {https://doi.org/10.1145/3584371.3612946}, author = {Kromer-Edwards, Cory}, keywords = {k-mer, cuda, fingerprint, hashmap, algorithm, extension, python, fasta, optimize, machine learning, bacteria, location = Houston, TX, USA}, abstract = {With the increasing availability of genomic data obtained through Whole-Genome Sequencing (WGS), Machine Learning (ML) algorithms are being used to analyze this data. However, processing large datasets or files poses challenges. One approach is to count K-Mers, which has been used in ML studies. However, larger K-Mer sizes may lead to decreased accuracy and training difficulties. Alternatively, combining multiple K-Mers of smaller sizes into fingerprints has shown promise in predicting species and antibiotic resistance. This study compares existing fingerprint generation techniques with a new algorithm called GPU K-Mer Fingerprinting (GKF), which utilizes a GPU for parallel processing. GKF demonstrates similar memory utilization compared to other approaches but achieves a speedup of 5,546X.} }
@inproceedings{10.1145/3647444.3647923, title = {Analysis on Machine Learning Strategies for Carcinoma Detection Biomarker}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647923}, url = {https://doi.org/10.1145/3647444.3647923}, author = {Verma, Shairal and Verma, Prem Kumari and Singh, Nagendra Pratap}, keywords = {Biomarkers, and carcinoma, machine learning, location = Jaipur, India}, abstract = {Biomarkers are substances that are identifiable and can potentially be used to show if a disease is present or is progressing. Biomarkers have the potential to diagnose cancer, forecast its course, and direct decisions regarding treatment. It has enabled the discovery of genes, plasma metabolites, and miRNA biomarkers for various cancers. A potent method for finding and analyzing biomarkers in data sets is machine learning. A component of artificial intelligence, machine learning is still a crucial and significant step in the diagnosis of several illnesses in the human body. In this rapidly expanding research, an increasing number of medical professionals are depending on artificial intelligence to identify and diagnose illnesses inside the body. The only issue is that they are striving to improve both precision and accuracy. Thus, we have provided an overview of the technologies used in the case of biomarkers used to detect cancer in the body in this article. This article will provide an overview of the work completed so far and assist future researchers in discovering new avenues for exploration within the specific research areas they cover.} }
@inproceedings{10.1145/3706468.3706482, title = {Towards Fair Assessments: A Machine Learning-based Approach for Detecting Cheating in Online Assessments}, booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference}, pages = {104--114}, year = {2025}, isbn = {9798400707018}, doi = {10.1145/3706468.3706482}, url = {https://doi.org/10.1145/3706468.3706482}, author = {Garg, Manika and Goel, Anita}, keywords = {Academic dishonesty, Cheating, Feature engineering, Integrity, Machine learning, Online education}, abstract = {Academic cheating poses a significant challenge to conducting fair online assessments. One common way is collusion, where students unethically share answers during the assessment. While several researchers proposed solutions, there is lack of clarity regarding the specific types they target among the different types of collusion. Researchers have used statistical techniques to analyze basic attributes collected by the platforms, for collusion detection. Only few works have used machine learning, considering two or three attributes only; the use of limited features leading to reduced accuracy and increased risk of false accusations.In this work, we focus on In-Parallel Collusion, where students simultaneously work together on an assessment. For data collection, a quiz tool is improvised to capture clickstream data at a finer level of granularity. We use feature engineering to derive seven features and create a machine learning model for collusion detection. The results show: 1) Random Forest exhibits the best accuracy (98.8\%), and 2) In contrast to less features as used in earlier works, the full feature set provides the best result; showing that considering multiple facets of similarity enhance the model accuracy. The findings provide platform designers and teachers with insights into optimizing quiz platforms and creating cheat-proof assessments.} }
@inproceedings{10.1145/3627673.3679925, title = {Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {3637--3641}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679925}, url = {https://doi.org/10.1145/3627673.3679925}, author = {Briscoe, Jarren and Gebremedhin, Assefaw}, keywords = {bias, bias metric, discrimination law, disparate impact, fairness, legal policy, objective fairness index, protected classes, location = Boise, ID, USA}, abstract = {Leveraging current legal standards, we define bias through the lens of marginal benefits and objective testing with the novel metric "Objective Fairness Index". This index combines the contextual nuances of objective testing with metric stability, providing a legally consistent and reliable measure. Utilizing the Objective Fairness Index, we provide fresh insights into sensitive machine learning applications, such as COMPAS (recidivism prediction), highlighting the metric's practical and theoretical significance. The Objective Fairness Index allows one to differentiate between discriminatory tests and systemic disparities.} }
@inproceedings{10.1145/3703935.3704116, title = {Research on Intention Recognition of Air Target Based on Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Artificial Intelligence and Pattern Recognition}, pages = {386--395}, year = {2025}, isbn = {9798400717178}, doi = {10.1145/3703935.3704116}, url = {https://doi.org/10.1145/3703935.3704116}, author = {Niu, QianRu and Ren, ShuangYin and Gao, Wei and Wang, ChunJiang}, keywords = {Air Target, Combat Intention, Deep Learning, Intention Recognition, Machine Learning}, abstract = {With the evolution and enhancement of air combat forces in modern warfare, accurate recognition of the combat intentions of air targets plays a vital role in battlefield situation assessment and command decision-making. This paper comprehensively sorts out and deeply analyzes the research on air target intention recognition through a systematic literature review method. The article first introduces the relevant concepts of intention recognition, summarizes the application of the two key elements of air target intention recognition, and explores its basic problems in the military field. Then, the research progress of traditional and machine learning-based intention recognition approaches is summarized in chronological order, outlining the main progress and shortcomings of current research, besides, the performance of various methods is compared. Since deep learning technology has shown great application potential in the field of intention recognition in complex battlefields, this paper constructs an intention recognition framework using deep learning methods. Finally, the future research direction of air target intention recognition is prospected, and targeted research suggestions are put forward.} }
@inproceedings{10.1145/3589883.3589884, title = {AI2: a novel explainable machine learning framework using an NLP interface}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {1--7}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589884}, url = {https://doi.org/10.1145/3589883.3589884}, author = {Dessureault, Jean-S\'ebastien and Massicotte, Daniel}, keywords = {NLP, accessibility, explainability, framework, machine learning, location = Stockholm, Sweden}, abstract = {This paper proposes a novel machine learning framework that encapsulates recent concerns of the data scientists community: accessibility and explainability. This framework, called AI2, proposes a natural language interface, making the framework accessible even to a non-expert. Traditionally, machine learning frameworks are accessible using a programming language. Python is one of the most common programming language for coding different machine learning methods. The AI2 framework, although made with Python scripts, is made to be accessed in a natural language, namely, English. Hence, the first contribution is about accessibility, allowing a non-data scientist to exploit a machine learning framework without knowing how to code. For decades, the data scientists community has known that one of the drawbacks in the machine learning field is the black-box problem. Data scientists have to create different methods to explain their results. The second contribution of this paper is to encapsulate the principle of explainability in the framework, systematically proposing not only the results but also the explanations of the results for every included machine learning algorithm.} }
@article{10.1145/3771766, title = {Saga++: A Scalable Framework for Optimizing Data Cleaning Pipelines for Machine Learning Applications}, journal = {ACM Trans. Database Syst.}, year = {2025}, issn = {0362-5915}, doi = {10.1145/3771766}, url = {https://doi.org/10.1145/3771766}, author = {Siddiqi, Shafaq and Phani, Arnab and Kern, Roman and Boehm, Matthias}, keywords = {Data Cleaning for ML, Linear-algebra-based Primitives, Data Cleaning Pipelines, Evolutionary Algorithms, Hyper-parameter Tuning, Data- and Task-parallel Execution}, abstract = {In the exploratory data science lifecycle, data scientists often spent the majority of their time finding, integrating, validating and cleaning relevant datasets. Despite recent work on data validation, and numerous error detection and correction algorithms, in practice, data cleaning for ML remains largely a manual, unpleasant, and labor-intensive trial and error process, especially in large-scale, distributed computation settings. The target ML application—such as classification or regression models—can be used as a signal of valuable feedback though, for selecting effective data cleaning strategies. In this paper, we introduce Saga++, a framework for automatically generating the top-K most effective data cleaning pipelines. Saga++ adopts ideas from Auto-ML, feature selection, and hyper-parameter tuning. Our framework is extensible for user-provided constraints, new data cleaning primitives, and ML applications; automatically generates hybrid runtime plans of local and distributed operations; and performs pruning by interesting properties (e.g., monotonicity). Furthermore, we exploit guided sampling on the input dataset to enable enumeration on a smaller subset, reducing the time required to discover the top-K pipelines. As a post-processing step, we also perform pipeline pruning on the selected top-K pipelines, removing redundant and less effective cleaning primitives. Instead of full automation—which is rather unrealistic—Saga++ simplifies the mechanical aspects of data cleaning. Our experiments show that Saga++ yields robust accuracy improvements over state-of-the-art, and good scalability regarding increasing data sizes and number of evaluated pipelines.} }
@inproceedings{10.1145/3700906.3701007, title = {Design and Python Simulation of Cross-border E-commerce Logistics Pathways Based on Machine Learning Algorithms}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {627--633}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3701007}, url = {https://doi.org/10.1145/3700906.3701007}, author = {Liu, Jian and Lei, Fei N/A and Liu, Qi and Yuan, Huan}, keywords = {Cross-border e-commerce logistics path design, Machine learning algorithm, Python Simulation, Random Forest}, abstract = {In the era of globalization, cross-border e-commerce has emerged as a significant driver of international trade, necessitating efficient logistics pathways to enhance delivery speed and reduce costs. This study presents a comprehensive approach to designing and simulating cross-border e-commerce logistics pathways using machine learning algorithms. We first identify the key factors influencing logistics efficiency, including geographical, regulatory, and demand variability aspects. Utilizing a dataset comprising historical shipping data, we apply various machine learning techniques, such as decision trees, random forests, and neural networks, to model and predict optimal logistics routes. The proposed model integrates real-time data processing and predictive analytics to dynamically adapt to changing conditions and optimize the logistics pathways in real-time. A Python-based simulation framework is developed to visualize and test the effectiveness of the proposed logistics pathways under different scenarios. The results demonstrate significant improvements in delivery times and cost reductions when compared to traditional logistics strategies. This research not only contributes to the field of logistics and supply chain management but also provides practical insights for e-commerce businesses seeking to enhance their operational efficiency in the competitive cross-border market. Future work will focus on refining the model with additional variables and exploring the integration of blockchain technology to further enhance transparency and traceability in cross-border logistics operations.} }
@inproceedings{10.1145/3650400.3650563, title = {Exploring Heart Disease Prediction through Machine Learning Techniques}, booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering}, pages = {964--969}, year = {2024}, isbn = {9798400708305}, doi = {10.1145/3650400.3650563}, url = {https://doi.org/10.1145/3650400.3650563}, author = {Lin, Zhicong and Chen, Shujing and Chen, Jichang}, abstract = {Currently, heart disease stands as the most formidable threat to human life. The application of machine learning in scrutinizing data holds the promise of augmenting early detection and prevention strategies for this ailment. Within this study, a suite of six distinctive and classical machine learning models—Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbor, Support Vector Classifier, and Neural Network—are introduced and meticulously evaluated. These models leverage data gathered from heart patients across four distinct regions, contributing to a comprehensive assessment. The investigative process encompasses five pivotal stages: data collection, preprocessing, K-Means clustering, application of classification algorithms models for heart disease prediction, and rigorous evaluation. At the end of the study, a comprehensive summary of heart disease prediction was presented.} }
@inproceedings{10.1145/3661167.3661268, title = {Insights Into Test Code Quality Prediction: Managing Machine Learning Techniques}, booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering}, pages = {2}, year = {2024}, isbn = {9798400717017}, doi = {10.1145/3661167.3661268}, url = {https://doi.org/10.1145/3661167.3661268}, author = {Pontillo, Valeria}, keywords = {Empirical Studies., Machine Learning, Test Code Quality Prediction, location = Salerno, Italy}, abstract = {Test cases represent the first line of defence against the introduction of software faults, especially when testing for regressions. They must be constantly maintained and updated as part of software components to keep them useful. With the help of testing frameworks, developers create test methods and run them periodically on their code. The entire team relies on the results from these tests to decide whether to merge a pull request or deploy the system. Unfortunately, tests are not immune to bugs or technical debts: indeed, they often suffer from issues that can preclude their effectiveness. Typical problems in test cases are called flaky tests and test smells. Over the last decades, the software engineering research community has been proposing a number of static and dynamic approaches to assist developers with the (semi-)automatic detection and removal of these problems. Despite this, most of these approaches rely on expensive dynamic steps and depend on tunable thresholds. These limitations have been partially targeted through machine learning solutions that could predict test quality issues using various features, like source code vocabulary or a mixture of static and dynamic metrics. In this tutorial, I will discuss our experience building prediction models to detect quality issues in test code. The tutorial will discuss the design choices to make in the context of test code quality prediction and the implications these choices have for the reliability of the resulting models.} }
@article{10.1145/3696354, title = {Federated In-Network Machine Learning for Privacy-Preserving IoT Traffic Analysis}, journal = {ACM Trans. Internet Technol.}, volume = {24}, year = {2024}, issn = {1533-5399}, doi = {10.1145/3696354}, url = {https://doi.org/10.1145/3696354}, author = {Zang, Mingyuan and Zheng, Changgang and Koziak, Tomasz and Zilberman, Noa and Dittmann, Lars}, keywords = {In-network computing, federated learning, security, internet of things, P4}, abstract = {The expanding use of Internet-of-Things (IoT) has driven machine learning (ML)-based traffic analysis. 5G networks’ standards, requiring low-latency communications for time-critical services, pose new challenges to traffic analysis. They necessitate fast analysis and response, preventing service disruption or security impact on network infrastructure. Distributed intelligence on IoT edge has been studied to analyze traffic, but introduces delays and raises privacy concerns. Federated learning can address privacy concerns, but does not meet latency requirements. In this article, we propose FLIP4: an efficient federated learning-based framework for in-network traffic analysis. Our solution introduces a lightweight federated tree-based model, offloaded and running within network devices. FLIP4 consumes less resources than previous solutions and reduces communication overheads, making it well-suited for IoT edge traffic analysis. It ensures prompt mitigation and minimal impact on services in the presence of false alerts using two approaches (metering and dropping), thereby balancing learning accuracy and privacy requirements.} }
@inproceedings{10.1145/3613904.3642628, title = {Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference}, booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703300}, doi = {10.1145/3613904.3642628}, url = {https://doi.org/10.1145/3613904.3642628}, author = {Hohman, Fred and Wang, Chaoqun and Lee, Jinmook and G\"ortler, Jochen and Moritz, Dominik and Bigham, Jeffrey P and Ren, Zhile and Foret, Cecile and Shan, Qi and Zhang, Xiaoyi}, keywords = {Efficient machine learning, interactive systems, model compression, on-device machine learning, visual analytics, location = Honolulu, HI, USA}, abstract = {On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria : a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.} }
@inproceedings{10.1145/3625343.3625348, title = {Enhancing Phishing URL Detection: A Comparative Study of Machine Learning Algorithms}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625348}, url = {https://doi.org/10.1145/3625343.3625348}, author = {Alsarhan, Ayoub and Igried, Bashar and Bani Saleem, Raad Mohammad and Alauthman, Mohammad and Aljaidi, Mohammad}, keywords = {Cybersecurity, Machine Learning Algorithms, Phishing Detection, Server-side Analysis, URL Analysis, location = Bangkok, Thailand}, abstract = {Phishing constitutes a significant threat in the digital world, often exploiting human vulnerabilities to illicitly obtain sensitive data such as personal credentials, financial details, and private information. Misusing this information results in substantial financial loss and personal harm to victims. This study introduces an innovative approach to mitigate the risk of phishing attacks by employing machine learning algorithms to detect phishing URLs. The proposed method applies a suite of algorithms, including j48, Na\"ve Bayes, JRip, and Decision Table, to a robust dataset of 11,430 URLs, each with 87 extracted features. The results reveal the considerable potential of machine learning in identifying phishing threats. Furthermore, the study explores a novel server-side analysis concept where the server scrutinizes links transmitted via emails or social communication platforms such as WhatsApp, Messenger, and Instagram. The application of phishing detection algorithms filters and prevents the delivery of phishing links, thus reducing the potential harm to users. This research is poised to significantly contribute to cybersecurity by enhancing phishing detection mechanisms' accuracy and efficiency.} }
@inproceedings{10.1145/3727993.3728063, title = {Research on a Machine Learning-Based Prediction Model for Tear Gas Smoke Dispersion}, booktitle = {Proceedings of the 2024 4th International Conference on Computational Modeling, Simulation and Data Analysis}, pages = {415--421}, year = {2025}, isbn = {9798400711831}, doi = {10.1145/3727993.3728063}, url = {https://doi.org/10.1145/3727993.3728063}, author = {He, Yipeng and Cui, Xiaoping and Tian, Ye and Zhang, Mengxing}, keywords = {Machine learning, Prediction model, Smoke diffusion, Tear gas}, abstract = {In this study, a machine learning-based prediction model for tear smoke dispersion was constructed, incorporating Random Forest, Gradient Boosting Tree and LSTM networks. Detailed meteorological, geographical and topographical data were collected, and data cleaning and feature engineering were performed. The model was found to outperform traditional models in terms of prediction accuracy, computational efficiency and adaptability, and to show excellent performance in different time scales, spatial scales and environmental conditions.} }
@inproceedings{10.1145/3745812.3745853, title = {Diagnosis of cardiac diseases using spectral features and machine learning}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745853}, url = {https://doi.org/10.1145/3745812.3745853}, author = {Tiwari, Shashi Kant and Sinha, Shweta and Sekhon, Karamjit Kaur and Singh, Ram Sewak}, keywords = {Congestive Heart Failure (CHF), Generalized Discriminant Analysis (GDA), Heart Rate Variability (HRV), coronary artery disease (CAD)}, abstract = {This study presents combination of spectral analysis method, generalized discriminant analysis (GDA), and the extreme learning machine (LELM) classifier algorithms which provide a robust method for prediction of cardiac diseases. Firstly, spectral approaches, has been applied for features extraction from heart rate variability (HRV) signal. The proposed GDA +ELM model was fitted with the features derived from approaches for the classification model of congestive heart disease and coronary artery disease. The standard database of HRV signal was collected from Physio net ATM's HRV source for training and validating predictive models for cardiac diseases.} }
@article{10.1145/3687267, title = {Understanding the performance of machine learning models from data- to patient-level}, journal = {J. Data and Information Quality}, volume = {16}, year = {2024}, issn = {1936-1955}, doi = {10.1145/3687267}, url = {https://doi.org/10.1145/3687267}, author = {Valeriano, Maria Gabriela and Matran-Fernandez, Ana and Kiffer, Carlos and Lorena, Ana Carolina}, keywords = {Machine learning, Instance Hardness, Data-centric, Healthcare}, abstract = {Machine Learning (ML) models have the potential to support decision-making in healthcare by grasping complex patterns within data. However, decisions in this domain are sensitive and require active involvement of domain specialists with deep knowledge of the data. To address this task, clinicians need to understand how predictions are generated so they can provide feedback for model refinement. There is usually a gap in the communication between data scientists and domain specialists that needs to be addressed. Specifically, many ML studies are only concerned with presenting average accuracies over an entire dataset, losing valuable insights that can be obtained at a more fine-grained patient-level analysis of classification performance. In this article, we present a case study aimed at explaining the factors that contribute to specific predictions for individual patients. Our approach takes a data-centric perspective, focusing on the structure of the data and its correlation with ML model performance. We utilize the concept of Instance Hardness, which measures the level of difficulty an instance poses in being correctly classified. By selecting the hardest and easiest to classify instances, we analyze and contrast the distributions of specific input features and extract meta-features to describe each instance. Furthermore, we individually examine certain instances, offering valuable insights into why they offer challenges for classification, enabling a better understanding of both the successes and failures of the ML models. This opens up the possibility for discussions between data scientists and domain specialists, supporting collaborative decision-making.} }
@inproceedings{10.1145/3676581.3676583, title = {Smart Emergency Alerting System: A Machine Learning Approach}, booktitle = {Proceedings of the 2024 2nd International Conference on Communications, Computing and Artificial Intelligence}, pages = {8--15}, year = {2024}, isbn = {9798400716898}, doi = {10.1145/3676581.3676583}, url = {https://doi.org/10.1145/3676581.3676583}, author = {Alrowaily, Majed Abdullah}, keywords = {BLE, GPS, IoT, beacons, camera analysis, deep learning, metadata, location = Jeju, Republic of Korea}, abstract = {Abstract: Recently, Saudi Arabia has hosted significant sports and technology events. Saudi Arabia has also successfully secured the bid to host Expo 2030 and declared its intention to host the FIFA World Cup in 2034. These crowds pertain to the elderly and special needs groups, which constitute the most vulnerable category in general, and they usually require some form of assistance. This study proposes a framework that integrates beacon technology with deep learning and real-time camera feed analysis to help crowds with special needs request emergency assistance by training a prioritization model for a patient based on this labeled dataset of 6962 records that consists of demographic and clinical details. In this, the random forest gives a nearly perfect classification and is the most successful model compared to the logistic regression and the SVM models. Logistic regression and the SVM model were not good with the minority classes. Once again, the model has been used by the application of that integrated application, and the result will be better in giving more priority to the medical emergency request and making a sound response. Another disadvantage could be, for example, accuracy in GPS coordinates, illumination conditions, and crowd density in indoor situations.} }
@inproceedings{10.1145/3583780.3614786, title = {Automatic and Precise Data Validation for Machine Learning}, booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, pages = {2198--2207}, year = {2023}, isbn = {9798400701245}, doi = {10.1145/3583780.3614786}, url = {https://doi.org/10.1145/3583780.3614786}, author = {Shankar, Shreya and Fawaz, Labib and Gyllstrom, Karl and Parameswaran, Aditya}, keywords = {data validation, machine learning, location = Birmingham, United Kingdom}, abstract = {Machine learning (ML) models in production pipelines are frequently retrained on the latest partitions of large, continually- growing datasets. Due to engineering bugs, partitions in such datasets almost always have some corrupted features; thus, it's critical to find data issues and block retraining before downstream ML accuracy decreases. However, current ML data validation methods are difficult to operationalize: they yield too many false positive alerts, require manual tuning, or are infeasible at scale. In this pa- per, we present an automatic, precise, and scalable data validation system for ML pipelines, employing a simple idea that we call a Partition Summarization (PS) approach to data validation: each timestamp-based partition of data is summarized with data quality metrics, and summaries are compared to detect corrupted partitions. We demonstrate how to adapt PS for any data validation method in a robust manner and evaluate several adaptations-which by themselves provide limited precision. Finally, we present gate, our data validation method that leverages these adaptations, giving a 2.1 average improvement in precision over the baseline from prior work on a case study within our large tech company.} }
@inproceedings{10.1145/3708635.3708653, title = {Comparing the Performance of a CDC Questionnaire with Machine Learning Models in Predicting Diabetes}, booktitle = {Proceedings of the 2024 13th International Conference on Software and Information Engineering}, pages = {119--124}, year = {2025}, isbn = {9798400717765}, doi = {10.1145/3708635.3708653}, url = {https://doi.org/10.1145/3708635.3708653}, author = {Alatawi, Yusuf and Kadhem, Hasan}, keywords = {Machine Learning, Diabetes, Classification, Nutrition and Dietary Habits, NHANES}, abstract = {This paper addresses the pressing issue of diabetes, a primary global health concern, by comparing machine learning models and a Centers for Disease Control and Prevention (CDC) questionnaire for diabetes diagnosis based on nutritional data. Utilizing the National Health and Nutrition Examination Survey (NHANES) dataset, the study highlights the limited of Machine Learning (ML) research focusing solely on nutritional features. The machine learning models, including K nearest neighbor (KNN) and support vector machine (SVM), are evaluated for recall, precision, and other performance metrics. Notably, KNN and SVM outperform the CDC questionnaire, obtaining F1 scores of more than 60\% compared to about 40\% for the questionnaire. In addition to that, they achieved recall scores of more than 80\%, while the questionnaire only managed a recall of about 30\%. These results suggest that machine learning models based on nutrition and diet could serve as more effective diabetes screening tools.} }
@inproceedings{10.1145/3630106.3659043, title = {Understanding Disparities in Post Hoc Machine Learning Explanation}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {2374--2388}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3659043}, url = {https://doi.org/10.1145/3630106.3659043}, author = {Mhasawade, Vishwali and Rahman, Salman and Haskell-Craig, Zo\'e and Chunara, Rumi}, keywords = {explainability, fairness, post hoc explanation methods, location = Rio de Janeiro, Brazil}, abstract = {Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across “race” and “gender” as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.} }
@inproceedings{10.1145/3706594.3726967, title = {HEEPstor: an Open-Hardware Co-design Framework for Quantized Machine Learning at the Edge}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {22--25}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3726967}, url = {https://doi.org/10.1145/3706594.3726967}, author = {Palacios, Pedro and Medina, Rafael and Ansaloni, Giovanni and Atienza, David}, keywords = {Machine learning framework, hardware-software co-design, open-hardware, RISC-V, edge AI.}, abstract = {Edge-AI applications necessitate the joint application of hardware acceleration and software optimization to meet energy and area constraints. However, the co-design of these systems is hindered by the lack of integration options for novel hardware prototypes offered by commonly employed Machine Learning frameworks. Bridging this gap, we present HEEPstor, an open-hardware co-design framework that enables the deployment of quantized, PyTorch-defined models on heterogeneous RISC-V systems interfacing custom accelerators. We demonstrate its application by executing quantized ML models for image classification on a X-HEEP platform integrating tailored systolic arrays, showcasing its flexibility for hardware-software explorations.} }
@inproceedings{10.1145/3589883.3589889, title = {HE-MAN – Homomorphically Encrypted MAchine learning with oNnx models}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {35--45}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589889}, url = {https://doi.org/10.1145/3589883.3589889}, author = {Nocker, Martin and Drexel, David and Rader, Michael and Montuoro, Alessio and Sch\"ottle, Pascal}, keywords = {Homomorphic Encryption, Machine Learning as a Service, Secure and Privacy-Preserving Machine Learning, location = Stockholm, Sweden}, abstract = {Machine learning (ML) algorithms are increasingly important for the success of products and services, especially considering the growing amount and availability of data. This also holds for areas handling sensitive data, e.g. applications processing medical data or facial images. However, people are reluctant to pass their personal sensitive data to a ML service provider. At the same time, service providers have a strong interest in protecting their intellectual property and therefore refrain from publicly sharing their ML model. Fully homomorphic encryption (FHE) is a promising technique to enable individuals using ML services without giving up privacy and protecting the ML model of service providers at the same time. Despite steady improvements, FHE is still hardly integrated in today’s ML applications. Reasons for that are, among others, that existing implementations either require the user to possess expertise in FHE, do not feature an easy ML framework integration, or have to approximate non-polynomial activations. We introduce HE-MAN, an open-source two-party machine learning toolset for privacy preserving inference with ONNX models and homomorphically encrypted data. Both the model and the input data do not have to be disclosed. HE-MAN abstracts cryptographic details away from the users, thus expertise in FHE is not required for either party. HE-MAN’s security relies on its underlying FHE schemes. For now, we integrate two different homomorphic encryption schemes, namely Concrete and TenSEAL. Compared to prior work, HE-MAN supports a broad range of ML models in ONNX format out of the box without sacrificing accuracy. We evaluate the performance of our implementation on different network architectures classifying handwritten digits and performing face recognition and report accuracy and latency of the homomorphically encrypted inference. Cryptographic parameters are automatically derived by the tools. We show that the accuracy of HE-MAN is on par with models using plaintext input while inference latency is several orders of magnitude higher compared to the plaintext case.} }
@inproceedings{10.1145/3735452.3735538, title = {Multi-level Machine Learning-Guided Autotuning for Efficient Code Generation on a Deep Learning Accelerator}, booktitle = {Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems}, pages = {134--145}, year = {2025}, isbn = {9798400719219}, doi = {10.1145/3735452.3735538}, url = {https://doi.org/10.1145/3735452.3735538}, author = {Cha, JooHyoung and Lee, Munyoung and Kwon, Jinse and Lee, Jemin and Kwon, Yongin}, keywords = {Auto-tuning, Deep learning accelerator, Hardware-aware optimization, Machine learning for systems, Performance prediction, location = Seoul, Republic of Korea}, abstract = {The growing complexity of deep learning models necessitates specialized hardware and software optimizations, particularly for deep learning accelerators. While machine learning-based autotuning methods have emerged as a promising solution to reduce manual effort, both template-based and template-free approaches suffer from prolonged tuning times due to the profiling of invalid configurations, which may result in runtime errors. To address this issue, we propose ML2Tuner, a multi-level machine learning-guided autotuning technique designed to improve efficiency and robustness. ML2Tuner introduces two key ideas: (1) a validity prediction model to filter out invalid configurations prior to profiling, and (2) an advanced performance prediction model that leverages hidden features extracted during the compilation process. Experimental results on an extended VTA accelerator demonstrate that ML2Tuner achieves equivalent performance improvements using only 12.3\% of the samples required by a TVM-like approach and reduces invalid profiling attempts by an average of 60.8\%, highlighting its potential to enhance autotuning performance by filtering out invalid configurations.} }
@inproceedings{10.1145/3586209.3591395, title = {Machine Learning-Based Jamming Detection and Classification in Wireless Networks}, booktitle = {Proceedings of the 2023 ACM Workshop on Wireless Security and Machine Learning}, pages = {39--44}, year = {2023}, isbn = {9798400701337}, doi = {10.1145/3586209.3591395}, url = {https://doi.org/10.1145/3586209.3591395}, author = {Testi, Enrico and Arcangeloni, Luca and Giorgetti, Andrea}, keywords = {internet of things, jammer classification, jamming detection, machine learning, privacy preservation, location = Guildford, United Kingdom}, abstract = {The development of novel tools to detect, classify and counteract the new generation of smart jammers in Internet of Things (IoT) is of paramount importance. Detection and classification have to be performed in a short time, with high reliability, and preserving the privacy of network users. In this work, we propose a novel machine learning (ML)-based jamming detection and classification algorithm which can be implemented in the network gateway (GW). The proposed method is based on energy detector (ED), the extraction of specific problem-tailored features, dimensionality reduction, and multi-class classification. Extensive numerical results have been carried out to evaluate the performance of detection and classification, varying the number of principal components selected through dimensionality reduction, the observation window length, the shadowing intensity, and the signal-to-jammer ratio (SJR). Our solution reaches remarkably high accuracy, i.e., up to 99\%, outperforming a state-of-the-art solution. That is a very promising result considering that the approach does not need to inspect the decoded information, thus preserving the privacy of the network users.} }
@inproceedings{10.1145/3726122.3726142, title = {The Impact of AI and Machine Learning on E commerce Personalization}, booktitle = {Proceedings of the 8th International Conference on Future Networks \&amp; Distributed Systems}, pages = {115--128}, year = {2025}, isbn = {9798400711701}, doi = {10.1145/3726122.3726142}, url = {https://doi.org/10.1145/3726122.3726142}, author = {Khamdamov, Shoh Jakhon and Shahbaz, Muhammad and Mamadiyarov, Zokir and Usmanov, Anvar and Xonturayev, Bobur and Rashidov, Sharofjon and Izzatillayev, Alisher}, abstract = {The rapid evolution of e-commerce has propelled personalization to the forefront of digital marketing strategies. This study investigates the transformative impact of Artificial Intelligence (AI) and Machine Learning (ML) on e-commerce personalization, examining their effects on customer engagement, sales performance, and long-term market dynamics. We conducted a comprehensive econometric analysis using panel data from diverse e-commerce platforms over multiple years. Employing difference-in-differences models and instrumental variable approaches, we isolated the specific impact of AI and ML-driven personalization on key performance metrics. Our research encompassed various product categories and market segments to assess heterogeneous effects across the e-commerce landscape. The implementation of AI and ML-driven personalization strategies led to statistically significant increases in conversion rates (10-15\%) and customer lifetime value (20-30\%). These insights have significant implications for e-commerce strategy, investment decisions, and regulatory frameworks in an increasingly AI-driven economy. This research contributes to the growing body of literature on AI economics, providing empirical evidence of its impact in e-commerce. It offers valuable guidance for practitioners in optimizing personalization strategies and informs policy discussions surrounding digital market regulation, data privacy, and technological competition.} }
@inproceedings{10.1145/3723178.3723247, title = {Diabetes Prediction: A Comprehensive Study Integrating Deep Learning and Machine Learning Approaches}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {520--526}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723247}, url = {https://doi.org/10.1145/3723178.3723247}, author = {Z Waughfa, Mohammed and Adnan, Imranul Islam and Sultana, Syeda Samia and Mumu, Sumaiya Siddiqua}, keywords = {Diabetes prediction, machine learning, Pima Indian Diabetic dataset, classification, Random Forest, SVM, Decision Tree, MLP, KNN, Healthcare.}, abstract = {Diabetes, a prevalent chronic disease, affects a significant portion of the global population, with early detection and management being crucial for patient health. Despite the lack of a cure, advancements in machine learning (ML) and deep learning (DL) offer promising avenues for diabetes prediction. This study leverages the Pima Indian dataset to explore and compare the effectiveness of various ML and DL techniques, including Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), Random Forest, K-Nearest Neighbors (KNN), and Decision Tree. Our comprehensive evaluation, based on accuracy, precision, and recall, identified MLP as the most effective algorithm, achieving an accuracy rate of 85\%. This research addresses current challenges in diabetes prediction by highlighting the superior performance of MLP in early detection, thereby underscoring its potential in improving diabetes management. The findings contribute to the ongoing discourse in healthcare technology, advocating for the integration of advanced ML techniques to enhance predictive accuracy and patient outcomes.} }
@proceedings{10.1145/3653724, title = {ICMML '23: Proceedings of the International Conference on Mathematics and Machine Learning}, year = {2023}, isbn = {9798400716973} }
@inbook{10.1145/3718491.3718572, title = {Damage identification and evaluation model of blast-resistant structure based on machine learning}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {500--505}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718572}, author = {Wang, Chongyu and Ding, Jianguo}, abstract = {Explosion-proof structure plays a vital role in protecting the safety of people and property. However, it is a complex and challenging task to identify and evaluate the damage caused by explosion to these structures. Traditional detection methods are time-consuming, laborious and ineffective in detecting hidden damage. In this paper, a damage identification and evaluation model of blast-resistant structure based on machine learning is proposed. The dynamic response data of the structure under the action of explosion is automatically analyzed by using the convolutional neural network (CNN) in deep learning, so as to quickly and accurately identify the location, degree and type of damage. CNN is selected because of its powerful feature extraction ability and efficient recognition ability of complex patterns. It can automatically learn and extract high-level features from original data, which simplifies the complicated process of manually designing features in traditional methods. The effectiveness of the proposed model is verified by the actual explosion experiment. The experimental results show that the CNN model maintains a high level of accuracy in identifying the three types of damage under different explosion intensities, especially in the medium-intensity explosion. In addition, CNN model shows remarkable advantages in damage identification and assessment tasks, and its accuracy reaches 93.87\%, far exceeding the methods based on vibration modal analysis and machine learning. This study not only improves the detection efficiency and reduces the risk of human misjudgment, but also provides a scientific basis for the safety management of anti-explosion structures and contributes a new technical means to improve the level of public safety.} }
@inproceedings{10.1145/3644815.3644972, title = {Software Design Decisions for Greener Machine Learning-based Systems}, booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI}, pages = {256--258}, year = {2024}, isbn = {9798400705915}, doi = {10.1145/3644815.3644972}, url = {https://doi.org/10.1145/3644815.3644972}, author = {del Rey, Santiago}, keywords = {green AI, energy efficiency, software engineering, sustainable computing, location = Lisbon, Portugal}, abstract = {The widespread integration of Machine Learning (ML) in software systems has brought forth unprecedented advancements, yet the surge in energy consumption raises ecological concerns. This research addresses the environmental impact of ML development, focusing on the energy implications of design decisions in ML-based systems. This thesis aims to offer insights into the energy consumption patterns influenced by deployment architecture and training environment. Different case studies on ML-based systems will be conducted to validate and demonstrate the implications of these design choices. The expected outcomes encompass actionable insights, validated through rigorous evaluations, and the development of an energy prediction tool for ML-based system development, to help in the decision-making process. This work contributes to the broader field of Green AI by addressing a critical gap and guiding the transition towards a more sustainable AI landscape.} }
@inproceedings{10.1145/3712335.3712431, title = {Research on the application of machine learning in corporate public opinion monitoring and management}, booktitle = {Proceedings of the 3rd International Conference on Signal Processing, Computer Networks and Communications}, pages = {557--562}, year = {2025}, isbn = {9798400710834}, doi = {10.1145/3712335.3712431}, url = {https://doi.org/10.1145/3712335.3712431}, author = {Ma, Xiaohui}, keywords = {machine learning, natural language processing, public opinion monitoring, sentiment analysis}, abstract = {This article makes a point of the overall application of corporate public opinion monitoring and management technology based on machine learning. This work proves that by means of case analysis, the basic theories and methods of machine learning, it elaborates on how to use effective machine learning for large-scale public opinion data collection, processing, and analysis, and prediction. Considerable attention is paid to the way natural language processing techniques and sentiment analysis are applied in processing text data and how the models of machine learning, among other support vector machines, decision trees, and neural networks, bear upon implementation in real enterprise environments. Research results show that the level of efficiency and prediction accuracy will be greatly improved if the machine learning technique is applied during public opinion data processing, therefore effectively optimizing the strategy of public opinion response and management of the company. The effectiveness and application scenarios of each prediction model in public opinion analysis were verified through comparative analysis with multiple prediction models, which provided the scientific data support for the decision-making reference of the company.} }
@inproceedings{10.1145/3690771.3690785, title = {Machine Learning Regression Model Development and Data Visualization of Road Accident in Urdaneta City, Pangasinan, Philippines}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {27--32}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690785}, url = {https://doi.org/10.1145/3690771.3690785}, author = {Dorado, Danilo and Aviles, Joey}, keywords = {Data Visualization, Machine Learning, Model Development, Road Accident, Urdaneta City}, abstract = {Road accidents contribute significantly to annual fatalities. Various agencies engaged in road safety management are consistently striving for quality improvement to predict and mitigate accidents through the aid of information technology. As machine learning emerged, its data analysis, visualization, and prediction capabilities have proven to help policymakers. The main objective of this study is to develop a machine-learning model for predicting road accidents in Urdaneta City, Philippines. The dataset used consists of three years (2021–2023) of road accident records from the Emergency Medical Services (EMS). Various preprocessing methods are employed, and a grid search technique is implemented to identify optimal features for the model. Random Forest, XGBoost, AdaBoost, Decision Tree, and LGBM algorithms are developed to predict and visualize road accident occurrences. Upon development, it was found that XGBoost performs well among other models, acquiring a total Rsquared of 0.9996 in training and 0.9898 during testing. To validate the findings of the study, the model's interpretability was assessed using a William Plot. The SHAP and LIME methodologies offer valuable insights into how various variables affect the model's forecast of the monthly number of road accidents. The Permutation Feature Importance approach provides insight into the specific impact of each feature on the model's predictions, emphasizing the significant influence of time on accident frequency, particularly during peak hours or late-night accidents. The Box Plot offers valuable insights, with the median (Q2) indicated by a line showing a value of around 10. Leveraging this model can significantly reduce the likelihood of accidents in real-time, resulting in a safer and more intelligent transportation ecosystem.} }
@inproceedings{10.1145/3700838.3703661, title = {Postprandial Blood Glucose Level Prediction through combined Machine Learning, Meta-Learning and XAI}, booktitle = {Proceedings of the 26th International Conference on Distributed Computing and Networking}, pages = {284--286}, year = {2025}, isbn = {9798400710629}, doi = {10.1145/3700838.3703661}, url = {https://doi.org/10.1145/3700838.3703661}, author = {B, Aruna Devi and N, Karthik}, keywords = {Feature Engineering, Generalizability, Interpretability, Machine Learning, Meta-Learning, PPBG, T2DM, XAI}, abstract = {Postprandial blood glucose (PPBG) is the glucose level measured after a meal and it is one of the main factors for Type 2 Diabetes Mellitus (T2DM) management. This work aims to predict PPBG level of T2DM patients by addressing the problems of clinical interpretability, limited number of patient dataset and generalizability. The proposed work aims to build a machine learning (ML) model that integrates appropriate feature selection, meta-learning to improve the generalizability and Explainable Artificial Intelligence (XAI) for interpretability. The preliminary work employed Random Forest (RF) feature selection, Independent Component Analysis (ICA) feature extraction technique and followed by training of five ML models such as Support Vector Regression (SVR), Xtreme Gradient Boosting (XGB), Random Forest (RF), Convolutional Neural Network (CNN) and Multilayer Perceptron (MLP). RF produced low Root Mean Square Error (RMSE) of 30.91 followed by CNN and MLP.} }
@inproceedings{10.5555/3535850.3536121, title = {Manipulation of Machine Learning Algoirhtms}, booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems}, pages = {1833--1835}, year = {2022}, isbn = {9781450392136}, author = {Bishop, Nicholas}, keywords = {computational social choice, machine learning, location = Virtual Event, New Zealand}, abstract = {As data becomes increasingly available, individuals, organisations and companies are increasingly applying machine learning algorithms to make decisions. In many cases, those decisions have a direct effect on those who provided the data to the decision maker. In other words, data providers often have a vested interest in the decisions made based on the data provided. Therefore, decision makers should anticipate that data providers may alter or change the data they provide in order to achieve a preferential outcome. Such strategic behaviour is not adequately modelled by classical machine learning settings in the literature. As a result, new machine learning algorithms are required, which take into the account the incentives and capabilities of data providers when making decisions. This paper summarises a PhD project which attempts to address this problem in a number of contexts.} }
@article{10.1145/3729358, title = {RegTrieve: Reducing System-Level Regression Errors for Machine Learning Systems via Retrieval-Enhanced Ensemble}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729358}, url = {https://doi.org/10.1145/3729358}, author = {Cao, Junming and Xiang, Xuwen and Cheng, Mingfei and Chen, Bihuan and Wang, Xinyan and Lu, You and Sha, Chaofeng and Xie, Xiaofei and Peng, Xin}, keywords = {Ensemble Model, Regression Error, Spoken QA System}, abstract = {Multiple machine learning (ML) models are often incorporated into real-world ML systems. However, updating an individual model in these ML systems frequently results in regression errors, where the new model performs worse than the old model for some inputs. While model-level regression errors have been widely studied, little is known about how regression errors propagate at system level. To address this gap, we propose RegTrieve, a novel retrieval-enhanced ensemble approach to reduce regression errors at both model and system level. Our evaluation across various model update scenarios shows that RegTrieve reduces system-level regression errors with almost no impact on system accuracy, outperforming all baselines by 20.43\% on average.} }
@inproceedings{10.1145/3708036.3708221, title = {"The Enterprise Performance Puzzle - Predictive Modeling Based on Interpretable Machine Learning"}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {1119--1124}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708221}, url = {https://doi.org/10.1145/3708036.3708221}, author = {Yang, Nan}, keywords = {Business performance, Executive characteristics, Firm characteristics, SHAP algorithm, Tree model}, abstract = {Corporate performance is a mature and popular research topic, and this study focuses on the influence of internal factors on corporate performance, and constructs a systematic corporate performance prediction model through the tree model and SHAP algorithm in machine learning. Unlike previous studies that explored corporate performance in a single dimension, this study covers corporate characteristics and executive characteristics, and explores the contribution of internal factors in corporate performance prediction. We compare and analyze firm-level and executive-level characteristics to discover the importance and mode of their roles in predicting firm performance. The results show that firm-level characteristics have a more direct and significant impact than executive-level characteristics in predicting firm performance. Among them, total corporate assets play the most important role in predicting corporate performance, and this study not only overcomes the limitations of traditional measurement models, but also provides new perspectives and tools for corporate performance prediction and management practices.} }
@inproceedings{10.1145/3701100.3701104, title = {A Tennis Momentum Analysis Method Based on Gaussian Dynamics and Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {13--17}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701104}, url = {https://doi.org/10.1145/3701100.3701104}, author = {Long, Zheng and Li, Na and Sun, Mengqing and Luo, Jiawen and Pan, Ting and Yin, Yujie and Yang, Xinhua}, keywords = {Gaussian dynamics model, Machine learning, Tennis match momentum analysis, prediction accuracy}, abstract = {Momentum analysis in tennis matches has predominantly focused on qualitative methods, lacking systematic quantitative approaches. This study proposes a novel method that integrates Gaussian dynamics models with machine learning techniques to quantitatively analyze and predict momentum changes in tennis matches. By selecting and defining key indicators such as serve success rate and scoring rate, we developed an algorithm to identify and predict momentum shifts. The proposed method employs ensemble learning techniques, specifically Stacking, to combine multiple machine learning models, enhancing prediction accuracy and execution efficiency. Experimental validation using data from major tennis tournaments, including the 2023 Wimbledon Championships, demonstrates the effectiveness of the proposed approach. The results show that the Stacking model outperforms individual models in accuracy and robustness, providing scientific decision support for coaches and players. This method has low computational requirements, is simple to implement, and considers a wide range of variables, making it highly efficient. Future research will focus on further optimizing the model and applying it to other sports and match types.} }
@article{10.5555/3722479.3722480, title = {10, 23, 81 --- Stacking up the LLM Risks: Applied Machine Learning Security}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {17--18}, year = {2024}, issn = {1937-4771}, author = {McGraw, Gary}, abstract = {I present the results of an architectural risk analysis (ARA) of large language models (LLMs), guided by an understanding of standard machine learning (ML) risks previously identified by BIML in 2020. After a brief level-set, I cover the top 10 LLM risks, then detail 23 black box LLM foundation model risks screaming out for regulation, finally providing a bird's eye view of all 81 LLM risks BIML identified. BIML's first work, published in January 2020 presented an in-depth ARA of a generic machine learning process model, identifying 78 risks. In this talk, I consider a more specific type of machine learning use case---large language models---and report the results of a detailed ARA of LLMs. This ARA serves two purposes: 1) it shows how our original BIML-78 can be adapted to a more particular ML use case, and 2) it provides a detailed accounting of LLM risks. At BIML, we are interested in "building security in" to ML systems from a security engineering perspective. Securing a modern LLM system (even if what's under scrutiny is only an application involving LLM technology) must involve diving into the engineering and design of the specific LLM system itself. This ARA is intended to make that kind of detailed work easier and more consistent by providing a baseline and a set of risks to consider.} }
@article{10.5555/3648699.3649089, title = {A scalable and efficient iterative method for copying machine learning classifiers}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Statuto, Nahuel and Unceta, Irene and Nin, Jordi and Pujol, Oriol}, keywords = {sustainable AI, transfer learning, environmental adaptation, optimization, model enhancement}, abstract = {Differential replication through copying refers to the process of replicating the decision behavior of a machine learning model using another model that possesses enhanced features and attributes. This process is relevant when external constraints limit the performance of an industrial predictive system. Under such circumstances, copying enables the retention of original prediction capabilities while adapting to new demands. Previous research has focused on the single-pass implementation for copying. This paper introduces a novel sequential approach that significantly reduces the amount of computational resources needed to train or maintain a copy, leading to reduced maintenance costs for companies using machine learning models in production. The effectiveness of the sequential approach is demonstrated through experiments with synthetic and real-world datasets, showing significant reductions in time and resources, while maintaining or improving accuracy.} }
@article{10.1145/3582575, title = {Estimating Software Functional Size via Machine Learning}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {32}, year = {2023}, issn = {1049-331X}, doi = {10.1145/3582575}, url = {https://doi.org/10.1145/3582575}, author = {Lavazza, Luigi and Locoro, Angela and Liu, Geng and Meli, Roberto}, keywords = {Function Points, functional size measurement, NESMA Estimated, early size estimation, Function Point Analysis, High-level FPA, simple function points, SFP, SiFP, machine learning estimation, Neural Networks, Support Vector Regression, Random Forests}, abstract = {Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods\&nbsp;for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the “High-level FPA” method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression\&nbsp;, and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data.} }
@inproceedings{10.1145/3675417.3675564, title = {Machine Learning-Based Crack Detection Methods in Ancient Buildings}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {885--890}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675564}, url = {https://doi.org/10.1145/3675417.3675564}, author = {Fang, Tianke and Hui, Zhenxing and P.Rey, William and Yang, Aihua and Liu, Bin and He, Yuanrong}, abstract = {This research paper introduces an innovative machine learning-based methodology for detecting cracks in ancient buildings, a vital aspect of architectural heritage conservation. Recognizing the limitations of conventional techniques in addressing the complex nature of historical structures, this study explores the integration of deep learning algorithms with enhanced image processing methods tailored for aged and varied materials typical of ancient architecture. The paper includes a comprehensive process involving meticulous data collection from diverse ancient structures, employing specialized preprocessing methods to handle the unique challenges of old surfaces. The development of a custom machine learning model, designed to adapt to the intricacies of historical construction, is thoroughly detailed. Experimental results demonstrate the model's superior performance in accurately identifying structural cracks compared to traditional methods. This research not only significantly contributes to the preservation of historical buildings but also exemplifies the versatile application of machine learning in the field of architectural conservation. The findings hold promise for revolutionizing the approach to maintaining and restoring our architectural heritage, ensuring longevity and integrity.} }
@inproceedings{10.1145/3655497.3655515, title = {Machine Learning-Based Web Application for ADHD Detection in Children}, booktitle = {Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence}, pages = {92--98}, year = {2024}, isbn = {9798400709302}, doi = {10.1145/3655497.3655515}, url = {https://doi.org/10.1145/3655497.3655515}, author = {Porras, Diego Oscar Alexander and Mejia, Gerson Antonio and Casta\~neda, Pedro Segundo}, keywords = {ADHD detection, Child mental health, Computing methodologies, Machine learning, location = Tokyo, Japan}, abstract = {Attention deficit hyperactivity disorder (ADHD) represents a medical condition characterized by the presence of inattention, hyperactivity, and impulsivity, which affects the academic development of students globally. In Peru, it affects a proportion of the pediatric population ranging from 2\% to 12\%, with a prevalence of 12.1\% in South Lima, particularly in public schools. This research presents an online application with machine learning to improve the detection of ADHD in elementary school children. Several machine learning algorithms were reviewed and Random Forest was selected as the best-performing model with an accuracy of 96.08\%. The model uses 27 selected variables, optimizing data collection and training. The child answers the questionnaire within the app and psychologists can access the app to visualize the results, aiding in the early detection of ADHD. The experiment involved 189 participants, resulting in a high accuracy of the Random Forest model. This innovative solution can have a significant impact on the early identification of ADHD, benefiting children's health and educational process.} }
@inproceedings{10.1145/3640900.3640910, title = {Automatic CDT Scoring Using Machine Learning with Interpretable Feature}, booktitle = {Proceedings of the 2024 14th International Conference on Bioscience, Biochemistry and Bioinformatics}, pages = {55--59}, year = {2024}, isbn = {9798400716768}, doi = {10.1145/3640900.3640910}, url = {https://doi.org/10.1145/3640900.3640910}, author = {Chen, Bo-Lin and Hu, Kuan-Ting and Cheng, Kuo-Sheng and Chen, Chien-Yu}, abstract = {The Clock Drawing Test (CDT) is a widely used cognitive assessment tool in clinical practice. However, it requires a trained neuropsychologist to evaluate the drawings, and the scoring process may be subjective due to the experience of the neuropsychologist. In this paper, we propose a novel automatic CDT scoring method based on interpretable features using machine learning. First, we use image processing techniques to extract features associated with the scoring guideline. Then, we combine these features as an input vector for training the machine learning classifier. Our experimental results demonstrate that our method achieves an accuracy of 82\%, which is superior to that of deep learning methods.} }
@inproceedings{10.1145/3675249.3675313, title = {A machine learning-based diabetes risk prediction modeling study}, booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology}, pages = {363--369}, year = {2024}, isbn = {9798400718267}, doi = {10.1145/3675249.3675313}, url = {https://doi.org/10.1145/3675249.3675313}, author = {Ming, Jiexiu and Xu, Junyi and Zhang, Miaomiao and Li, Ningyu and Yan, Xu}, abstract = {Diabetes mellitus is a chronic metabolic disease, mainly characterized by insufficient insulin secretion or impaired insulin action in the body, resulting in elevated blood glucose. According to the World Health Organization (WHO), the number of diabetes patients worldwide has been on the rise in recent years, and has become an important public health problem worldwide today. In this paper, we used the Random Forest-based feature importance screening method to retain the variables with larger variable feature weights, performed Spearman correlation analysis, selected the top 10 operational variables with lower correlations, and used information entropy theory and correlation analysis to test the representativeness and independence of the main variables, and finally screened out the main variables as platelet volume distribution width, HDL cholesterol, and the proportion of white globules, platelet specific volume, platelet count, red blood cell count, lymphocyte \%, albumin, neutrophil \%, and leukocyte count. Blood glucose prediction models were established through data mining techniques, in this paper five machine learning were selected for prediction, namely Extreme Gradient Boosted Tree (XGBoost), Random Forest Regression, Support Vector Machine Regression SVR, LightGBM, Gradient Boosted Decision Tree (GBDT). The training set was put into each model for training, and the test set was inputted into the model to get the root mean squared error produced by the five models ( MSE), Mean Absolute Error (MAE), and Maximum Absolute Error (MAS), comparing the five models, in general, the Support Vector Machine regression SVR has the highest accuracy. To establish a support vector machine SVR blood sugar prediction model based on Bayesian optimization, the sample data are normalized, the parameters are initially corrected using Bayesian principles, and then the support vector machine estimation algorithm is selected to initialize the model, the parameters are inferred using the Bayesian evidence framework, and the optimal model is established after several iterations, and the support vector machine regression SVR trained using the optimal hyperparameters obtained from Bayesian optimization model has improved accuracy in all three evaluation metrics.} }
@inproceedings{10.1145/3626246.3654745, title = {PLUTUS: Understanding Data Distribution Tailoring for Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {528--531}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654745}, url = {https://doi.org/10.1145/3626246.3654745}, author = {Chang, Jiwon and Dionysio, Christina and Nargesian, Fatemeh and Boehm, Matthias}, keywords = {data acquisition, distribution tailoring, model debugging, location = Santiago AA, Chile}, abstract = {Existing data debugging tools allow users to trace model performance problems all the way to the data by efficiently identifying slices (conjunctions of features and values) for which a trained model performs significantly worse than the entire dataset. To ensure accurate and fair models, one solution is to acquire enough data for these slices. In addition to crowdsourcing, recent data acquisition techniques design cost-effective algorithms to obtain such data from a union of external sources such as data lakes and data markets. We demonstrate PLUTUS, a tool for human-in-the-loop and model-aware data acquisition pipeline, on SystemDS, as an open source ML system for the end-to-end data science lifecycle. In PLUTUS, a user can efficiently identify problematic slices, connect to external data sources, and acquire the right amount of data for these slices in a cost-effective manner.} }
@article{10.5555/3648699.3649030, title = {Fair data representation for machine learning at the pareto frontier}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Xu, Shizhou and Strohmer, Thomas}, keywords = {statistical parity, equalized odds, Wasserstein barycenter, Wasserstein geodesics, conditional expectation estimation}, abstract = {As machine learning powered decision-making becomes increasingly important in our daily lives, it is imperative to strive for fairness in the underlying data processing. We propose a pre-processing algorithm for fair data representation via which supervised learning results in estimations of the Pareto frontier between prediction error and statistical disparity. In particular, the present work applies the optimal affine transport to approach the post-processing Wasserstein barycenter characterization of the optimal fair L2-objective supervised learning via a pre-processing data deformation. Furthermore, we show that the Wasserstein geodesics from the conditional (on sensitive information) distributions of the learning outcome to their barycenter characterize the Pareto frontier between L2-loss and the average pairwise Wasserstein distance among sensitive groups on the learning outcome. Numerical simulations underscore the advantages: (1) the pre-processing step is compositive with arbitrary conditional expectation estimation supervised learning methods and unseen data; (2) the fair representation protects the sensitive information by limiting the inference capability of the remaining data with respect to the sensitive data; (3) the optimal affine maps are computationally efficient even for high-dimensional data.} }
@inbook{10.1145/3718491.3718545, title = {Machine Learning Based River Segmentation with GF-2 Satellite Imagery}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {325--328}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718545}, author = {Cui, Hanwen and Li, Cheng}, abstract = {Rivers constitute a crucial element of the natural environment, playing an indispensable role in ecosystem dynamics. Investigating river systems not only facilitates the sustainable management of water resources but also advances ecological preservation, alongside social and economic development. Considering escalating global challenges such as climate change, population pressures, and environmental degradation, acquiring a comprehensive understanding of river resources is imperative for addressing both current and future ecological dilemmas. This study employs high-spatial-resolution remote sensing imagery Gaofen-2 (GF-2) alongside two deep learning models: k-Nearest Neighbor (kNN) and Decision Tree (DT), to conduct river segmentation within the context of Zhuhai, a city characterized by abundant river resources. The research utilizes a 10-fold cross-validation approach and the grid search method to ascertain the optimal parameters for each model, aiming to enhance training accuracy. Comparative analysis indicates that the kNN model outperforms the DT model across various evaluation metrics and segmentation outcomes. The findings of this research contribute valuable theoretical insights and technical support for effective river management strategies, enabling governmental bodies to formulate more scientifically grounded and rational policies and measures for sustainable development.} }
@inproceedings{10.1145/3677182.3677236, title = {Machine learning algorithm based airborne LiDAR point cloud classification method}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {301--305}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677236}, url = {https://doi.org/10.1145/3677182.3677236}, author = {Zhang, Yanwen and Wang, Xiaosong and Fu, Yuwen and Wang, Miao and Liu, Haoguang and Wang, Zhoujie}, abstract = {For decades, airborne LiDAR measurement technology has gradually matured and achieved high measurement results, obtaining detailed and accurate data. The airborne Lidar has a capability to capture high accuracy and high-resolution point clouds from the object surface efficiently. However, it is inefficient and labor-intensive to distinguish the information on land use by manual classification. Deep learning, as a part of machine learning research, plays a significant role in point cloud data classification, such as unstructured and chaotic data. At present, many scholars have proposed various algorithms in the processing of airborne LiDAR point cloud data. However, overall, there are relatively few literature reviews on radar point clouds. Therefore, this study introduces machine learning from three different stages of development, starting from the historical development of time. After that, a large number of application examples in point clouds classification are compared. The analysis results demonstrate that rule-based classification is tedious, but can help users understand internal structure, classic machine learning methods can achieve a higher classification accuracy and deep learning has broad prospects in airborne Lidar point clouds.} }
@inproceedings{10.1145/3627673.3680272, title = {Towards Making Effective Machine Learning Decisions Against Out-of-Distribution Data}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5479--5482}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3680272}, url = {https://doi.org/10.1145/3627673.3680272}, author = {Tamang, Lakpa D.}, keywords = {OOD detection, OOD generalization, out-of-distribution, location = Boise, ID, USA}, abstract = {Conventional machine learning systems operate on the assumption of independent and identical distribution (i.i.d), where both the training and test data share a similar sample space, and no distribution shift exists between them. However, this assumption does not hold in practical deployment scenarios, making it crucial to develop methodologies that address the non-trivial task of data distribution shift. In our research, we aim to address this problem by developing ML algorithms that explicitly achieve promising performance when subjected to various types of out-of-distribution (OOD) data. Specifically, we approach the problem by categorizing the data distribution shifts into two types: covariate shifts and semantic shifts, and proposing effective methodologies to tackle each type independently and conjointly while validating them with different types of datasets. We aim to propose ideas that are compatible with existing deep neural networks to perform detection and/or generalization of the test instances that are shifted in semantic and covariate space, respectively.} }
@proceedings{10.1145/3670474, title = {MLCAD '24: Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998} }
@inproceedings{10.1145/3658835.3658838, title = {Machine Learning-based Composition Analysis of Ancient Glass Objects}, booktitle = {Proceedings of the 2024 5th International Conference on Artificial Intelligence in Electronics Engineering}, pages = {9--19}, year = {2024}, isbn = {9798400716850}, doi = {10.1145/3658835.3658838}, url = {https://doi.org/10.1145/3658835.3658838}, author = {Li, Ying and Tang, Jierong and Rao, Junreng and Wang, Yuhan and Li, Le and Tan, Zhen and Xiao, Weidong}, keywords = {Ancient glass components, Decision trees, Fisher discriminant method, K-means clustering, Machine learning, location = Bangkok, Thailand}, abstract = {Ancient glass is one of the objects studied in archaeology, and the study of ancient Chinese glass has attracted much attention from scholars at home and abroad in recent years. Ancient glass is similar in appearance to exotic glass objects, but the chemical composition is different. Moreover, ancient glass is highly susceptible to weathered by the burial environment. During the weathered process, a large number of internal elements are exchanged with environmental elements, resulting in changes in their composition ratios, which affects scientists' correct judgement of their categories. This paper implements a variety of machine learning methods to analyze the chemical composition of ancient glass, predict the chemical composition content of ancient glass before weathered, correctly determine the type of ancient glass based on its chemical composition, and sub-classify ancient glass. The final results of this paper provide a scientific basis for archaeological research work and lay the foundation for subsequent studies on the origins, systems, manufacturing dates and preparation processes of ancient glass.} }
@inproceedings{10.1145/3631802.3631808, title = {Common Errors in Machine Learning Projects: A Second Look}, booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research}, year = {2024}, isbn = {9798400716539}, doi = {10.1145/3631802.3631808}, url = {https://doi.org/10.1145/3631802.3631808}, author = {Zimmermann, Renato Magela and Allin, Sonya and Zhang, Lisa}, keywords = {AI, Common Errors, Data science, Machine learning, Misconceptions, location = Koli, Finland}, abstract = {While machine learning (ML) has proved impactful in many disciplines, design decisions involved in building ML models are difficult for novices to make, and mistakes can cause harm. Prior work by Skripchuk et\&nbsp;al. [35] identified common errors made by ML students via qualitative analysis of open-ended ML assessments, but their sample was limited to a single institution, course, and assessment setting. Our work is an extended, conceptual replication of this work to understand the common errors made by machine learning students. We use a mixed-method approach to analyze errors in 30 final project reports in an undergraduate machine learning course. The final reports describe the model-building process for a classification task, where students build models on a complex data set with numerical, categorical, ordinal and text features. Our choice to analyze project reports (rather than code) allows us to uncover design errors via how students justify their methodology. Our project task is to achieve the best test accuracy on an unseen test set; thus, as a way to validate these common errors, we identify the association between these errors and the model’s test accuracy performance. Common errors we find include those consistent with Skripchuk et\&nbsp;al. [35], for example issues with data processing, hyperparameter tuning, and model selection. In addition, our focus on design error exposes other common errors, for example where students use certain kinds of features (e.g., bag of words representations) only with particular models (e.g., Naive Bayes). We call these latter types of errors model misconceptions, and such errors are associated with lower test accuracy. Some of these errors are also present in work by practitioners. Others reflect a difficulty by students to make correct connections between ML concepts and achieve the relational level of the SOLO taxonomy. We identify areas of opportunity to improve machine learning pedagogy, particularly related to data processing, data leakage, hyperparameters, nonsensical outputs, and disentangling data decisions from model decisions.} }
@inproceedings{10.1145/3748825.3748936, title = {Constructing a hepatocellular carcinoma diagnosis model based on multiple machine learning algorithms and interpretable models}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {716--722}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748936}, url = {https://doi.org/10.1145/3748825.3748936}, author = {Chen, Junjie and Wen, Xianghua and Liu, Zhao and Hu, Huanjun}, keywords = {Diagnostic model, Liver cancer, Machine learning}, abstract = {Hepatocellular carcinoma is the most common type of primary liver cancer and ranks third in terms of cancer impact, making it one of the significant causes of cancer-related mortality. Identifying biomarkers and therapeutic targets associated with hepatocellular carcinoma is crucial. The effective screening and construction of diagnostic models to enhance the early diagnosis of hepatocellular carcinoma has been a major issue. To effectively address these challenges and improve the applicability of the models, our study performed differential expression analysis on sepsis-related data, constructed a protein-protein interaction network, and identified core genes associated with sepsis. We applied various machine learning algorithms to analyze the expression data of these key genes and selected effective models based on ROC curves and residual value metrics. Using the best model, we developed a diagnostic nomogram containing five genes with the highest importance scores, achieving an accuracy rate of up to 98.5\%, making it particularly suitable for predicting a risk rate of 60\%-95\% for hepatocellular carcinoma.} }
@inproceedings{10.1145/3688671.3688757, title = {Machine Learning Methods for Emulating Personality Traits in a Gamified Environment}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688757}, url = {https://doi.org/10.1145/3688671.3688757}, author = {Liapis, Georgios and Vordou, Anna and Vlahavas, Ioannis}, keywords = {Machine Learning, OCEAN 5, Gamified Environment}, abstract = {Personality traits are regarded as a significant factor of competency for job candidates, for example, evaluating the capacity to work efficiently within a team. However, there is a gap in the traditional assessment system for these cases since they typically rely on self-answered questionnaires that are biased or easily exploitable. Artificial Intelligence techniques can fill this gap by generating objective data to define standard personality template profiles, utilizing trained Reinforcement Learning agents. In this paper, we propose a gamified framework that employs Machine Learning methods to emulate personality traits based on the players’ play styles, with the purpose of creating standard team profiles. The OCEAN Five personality model is used as a basis for this attempt, which characterizes personality as a synthesis of the five components: openness, conscientiousness, extraversion, agreeableness, and neuroticism. After generating gameplay data through self-play, we examine how various personality qualities, actions, and modes of communication impact the team performance of the agents, with respect to the different personality traits. Results indicate that the personality traits of the agents individually and as a team do impact their performance and efficiency. This can be used as a methodology for creating efficient individual bot agents or teams of agents in many game environments.} }
@inproceedings{10.1145/3639478.3643069, title = {Interpretable Software Maintenance and Support Effort Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {288--289}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3643069}, url = {https://doi.org/10.1145/3639478.3643069}, author = {Haldar, Susmita and Capretz, Luiz Fernando}, keywords = {maintenance and support effort prediction, explainable machine learning models, model agnostic interpretation, location = Lisbon, Portugal}, abstract = {Software maintenance and support efforts consume a significant amount of the software project budget to operate the software system in its expected quality. Manually estimating the total hours required for this phase can be very time-consuming, and often differs from the actual cost that is incurred. The automation of these estimation processes can be implemented with the aid of machine learning algorithms. The maintenance and support effort prediction models need to be explainable so that project managers can understand which features contributed to the model outcome. This study contributes to the development of the maintenance and support effort prediction model using various tree-based regression machine-learning techniques from cross-company project information. The developed models were explained using the state-of-the-art model agnostic technique SHapley Additive Explanations (SHAP) to understand the significance of features from the developed model. This study concluded that staff size, application size, and number of defects are major contributors to the maintenance and support effort prediction models.} }
@inproceedings{10.1145/3695080.3695141, title = {A Method for Wal-Mart Sales Forecasting Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Cloud Computing and Big Data}, pages = {350--355}, year = {2024}, isbn = {9798400710223}, doi = {10.1145/3695080.3695141}, url = {https://doi.org/10.1145/3695080.3695141}, author = {Xu, Ruiheng}, abstract = {With the widespread use of big data science, there is a growing need for data-driven decision-making in the retail industry. As one of the largest retailers in the world, Walmart's sales forecasting through machine learning methods is crucial to its business operations, which directly affects inventory management, supply chain optimization, and promotional strategy development and helps the company to save huge amounts of costs. In this paper, a prediction method combining multiple machine-learning techniques is proposed using historical sales data. The experimental results verify the method's effectiveness and significantly improve the accuracy of sales forecasting, which can provide an important reference for Walmart's inventory management and supply chain optimization.} }
@inproceedings{10.1145/3715669.3726785, title = {Learning Disorder Detection Using Eye Tracking: Are Large Language Models Better Than Machine Learning?}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726785}, url = {https://doi.org/10.1145/3715669.3726785}, author = {Nguyen, Quoc-Toan and Nguyen, Hy and Tang, Quang-Hieu and Truong, Tien and Pham, Van-Tuan and Le, Linh and Williams-King, David}, keywords = {Eye Tracking, LLMs, Dyslexia, Specific Learning Disorder, Human-centered computing}, abstract = {Early detection of learning disorders is essential for timely intervention, fostering improved academic performance and overall well-being. This research explores the potential of Large Language Models (LLMs) combined with cost-effective eye-tracking data for detecting dyslexia, one of the most prevalent learning disorders. The prominent LLMs included are DeepSeek-V3, Llama3.3-70B, GPT-4o, GPT-o3-mini, and GPT-o1-mini. Leveraging data from 70 participants across three distinct tasks, our findings reveal that LLMs can outperform traditional Machine Learning (ML) models. Notably, few-shot prompting significantly enhances accuracy, demonstrating the adaptability and efficiency of LLM-driven approaches. In summary, this study presents a novel approach to dyslexia detection by integrating eye-tracking data with LLMs. By outperforming specialised ML models, this scalable approach optimises resources and expands early detection, making dyslexia assessment more accessible. It enables timely support, enhancing academic performance and overall well-being for affected individuals.} }
@inproceedings{10.1145/3716554.3716617, title = {Optimizing Sample Size in Machine Learning: Balancing Complexity and Generalization}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {413--418}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716617}, url = {https://doi.org/10.1145/3716554.3716617}, author = {Karapiperis, Dimitrios and Feretzakis, Georgios and Kalles, Dimitris and Verykios, Vassilios}, abstract = {This paper introduces a new direction in estimating an optimal sample size for a binary classification problem using two theoretical frameworks, namely Rademacher complexity and the PAC-Bayesian generalization bound. First, the Rademacher complexity provides a measure of the capacity of a hypothesis class, characterizing its risk of overfitting, while the second-the PAC-Bayesian generalization bound-provides a probabilistic upper bound on the generalization error from a Bayesian viewpoint. We unify these two bounds to get a tighter generalization error bound that will enable a closer estimate of the optimal sample size in achieving model performance that one can count on. The experimental results showed that the combination of both bounds allows a more thorough understanding of the generalization performance of the classifiers.} }
@inproceedings{10.1145/3640115.3640191, title = {Human Resources Analysis Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering}, pages = {470--477}, year = {2024}, isbn = {9798400708299}, doi = {10.1145/3640115.3640191}, url = {https://doi.org/10.1145/3640115.3640191}, author = {Guo, Ning}, keywords = {Data Analysis, Human Resources Management, Machine learning, Optimized Decision Support, PSO-PB Model, location = Changde, Hunan, China}, abstract = {This study is grounded in a human resource dataset, examining the correlation between employee performance and various factors, encompassing age, and educational background. The dataset comprises 13 dimensions, and meticulous data cleaning and processing were applied to rectify missing and incongruous data issues. Subsequent to the preparatory steps, data mining techniques were employed to scrutinize the influence of age and education on employee value. The paper then details the methodology for establishing and optimizing the model, utilizing the Particle Swarm Optimization (PSO) algorithm and BP00 neural network. Validation of the model's efficacy follows, with results indicating its robust capability to predict employee value accurately. These findings offer valuable insights for enhancing human resources management practices. The paper concludes by summarizing research outcomes and underscored the pivotal role of a judicious performance assessment system in human resources management. Future research directions are also delineated.} }
@inproceedings{10.1145/3576915.3616593, title = {DPMLBench: Holistic Evaluation of Differentially Private Machine Learning}, booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security}, pages = {2621--2635}, year = {2023}, isbn = {9798400700507}, doi = {10.1145/3576915.3616593}, url = {https://doi.org/10.1145/3576915.3616593}, author = {Wei, Chengkun and Zhao, Minghu and Zhang, Zhikun and Chen, Min and Meng, Wenlong and Liu, Bo and Fan, Yuan and Chen, Wenzhi}, keywords = {deep learning, differential privacy, privacy-preserving machine learning, location = Copenhagen, Denmark}, abstract = {Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning (ML) techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We first present a taxonomy of where improvements are located in the ML life cycle. Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms, over twelve algorithms, four model architectures, four datasets, two attacks, and various privacy budget configurations. We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation. According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense. We also explore some improvements that can maintain model utility and defend against MIAs more effectively. Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs. ML practitioners may benefit from these evaluations to select appropriate algorithms. To support our evaluation, we implement a modular re-usable software, DPMLBench,1. We open-source the tool in https://github.com/DmsKinson/DPMLBench which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners.} }
@inproceedings{10.1145/3607947.3607967, title = {Prediction of Algae Growth: A Machine Learning Perspective}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {109--114}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3607967}, url = {https://doi.org/10.1145/3607947.3607967}, author = {Tiwary, Sanjeeb and Darshana, Subhashree and Mohanty, Debabrata and Dash, Adyasha and Rupsa, Potnuru and Barik, Rabindra K}, keywords = {Algae, Artificial Algae Algorithm, Artificial Neural Network, Blooms, Harmful Algal, Machine Learning, Support Vector, location = Noida, India}, abstract = {Algal blooms pose a significant threat to aquatic ecosystems and human health. To address this issue, this paper proposes a machine learning-based approach for predicting harmful algal blooms (HABs) by analyzing environmental features. Algae, as primary organic matter and oxygen producers, play a vital role in the biosphere. However, the exponential increase in algal growth worldwide poses significant challenges to economic development and long-term sustainability. The paper employs three popular machine learning algorithms: Artificial Neural Network (ANN), Gradient Boosting Decision Tree (GBDT), and Support Vector Machine (SVM) to predict algal blooms. The research utilizes real-time data from two locations: the Sassafras River in the United States Chesapeake Bay and Lake Okeechobee in Florida, USA. These locations have experienced frequent HABs due to factors like chemical runoff and nutrient-rich conditions. By analyzing the collected data, the paper identifies and selects the most important features to optimize the prediction models’ accuracy. Preliminary results demonstrate promising accuracy in predicting algal growth and identifying key characteristics associated with HABs. These findings contribute to a better understanding of algal blooms and pave the way for effective mitigation strategies to combat this global environmental challenge.} }
@inproceedings{10.1145/3691620.3695532, title = {Constructing Surrogate Models in Machine Learning Using Combinatorial Testing and Active Learning}, booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering}, pages = {1645--1654}, year = {2024}, isbn = {9798400712487}, doi = {10.1145/3691620.3695532}, url = {https://doi.org/10.1145/3691620.3695532}, author = {Shree, Sunny and Khadka, Krishna and Lei, Yu and Kacker, Raghu N. and Kuhn, D. Richard}, keywords = {machine learning, surrogate model, proxy model, model extraction attack, combinatorial testing, feature interactions, test case generation, location = Sacramento, CA, USA}, abstract = {Machine learning (ML)-based models are often black box, making it challenging to understand and interpret their decision-making processes. Surrogate models are constructed to approximate the behavior of a target model and are an essential tool for analyzing black-box models. The construction of a surrogate model typically includes querying the target model with carefully selected data points and using the responses from the target model to infer information about its structure and parameters.In this paper, we propose an approach to surrogate model construction using combinatorial testing and active learning, aiming to efficiently capture the essential interactions between features that drive the target model's predictions. Our approach first leverages t-way testing to generate data points that capture all the t-way feature interactions. We then use an iterative process to isolate the essential feature interactions, i.e., those that can determine a model prediction. In the iterative process, we remove nonessential feature interactions, generate additional data points to contain the remaining interactions, and employ active learning techniques to select a subset of the data points to update the surrogate model. This process is continued until we construct a surrogate model that closely mirrors the target model's behavior. We evaluate our approach on 4 public datasets and 12 ML models and compare the results with the state-of-the-art (SOTA) approaches. Our experimental results show that our approach can perform in most cases better than the SOTA approaches in terms of accuracy and efficiency.} }
@inproceedings{10.1145/3637528.3671472, title = {Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6627--6632}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671472}, url = {https://doi.org/10.1145/3637528.3671472}, author = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Zheng, Chenguang and Chen, Hongzhi and Cheng, James}, keywords = {gnn, graph, graph neural network, structure, subgraph, system, vertex, location = Barcelona, Spain}, abstract = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems.} }
@inproceedings{10.1145/3671016.3674816, title = {CLUE: Customizing clustering techniques using machine learning for software modularization}, booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware}, pages = {189--198}, year = {2024}, isbn = {9798400707056}, doi = {10.1145/3671016.3674816}, url = {https://doi.org/10.1145/3671016.3674816}, author = {Meng, Fanyi and Wang, Ying and Chong, Chun Yong and Yu, Hai and Zhu, Zhiliang}, keywords = {Empirical study, Software architecture, Software clustering, Software modularization, location = Macau, China}, abstract = {Software clustering is often used as a remodularization and architecture recovery technique to help developers simplify software maintenance tasks and ease the burden of software comprehension. While the choice of clustering technique can significantly influence the outcomes of remodularization, it is noteworthy that existing works have yet to conduct an exhaustive exploration of the suitability of various clustering techniques for different software projects. Although many prior works introduce new clustering techniques, their validations often focus on specific domains, which may restrict the generalizability of their findings. In this paper, we conduct an empirical study aimed at understanding the impact of software features and architectural problems on the effectiveness of various software clustering techniques. Leveraging our empirical findings, we propose an approach, CLUE, which leverages Machine Learning (ML) models to customize a suitable software clustering technique for a given software. Our approach focuses on eight types of software clustering techniques and offers insights into their suitability based on features and architectural problems of software. This comprehensive analysis helps developers to select the suitable clustering technique that can achieve the best MoJoFM, c2ccvg, or TurboMQ value from the chosen pool of software clustering techniques for specific software. We evaluate CLUE by analyzing 100 open-source software projects. The experiment results demonstrate that CLUE achieves highly accurate clustering technique customization, with an accuracy exceeding 90\%.} }
@inproceedings{10.5555/3643142.3643363, title = {Transforming Discrete Event Models to Machine Learning Models}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {2662--2673}, year = {2024}, isbn = {9798350369663}, author = {Sarjoughian, Hessam S. and Fallah, Forouzan and Saeidi, Seyyedamirhossein and Yellig, Edward J.}, abstract = {Discrete event simulation, formalized as deductive modeling, has been shown to be effective for studying dynamical systems. Development of models, however, is challenging when numerous interacting components are involved and should operate under different conditions. Machine Learning (ML) holds the promise to help reduce the effort needed to develop models. Toward this goal, a collection of ML algorithms, including Automatic Relevance Determination is used. Parallel Discrete Event System Specification (PDEVS) models are developed for Single-stage and Two-stage cascade factories. Each model is simulated under different demand profiles. The simulated data sets are partitioned into subsets, each for one or more model components. The ML algorithms are applied to the data sets for generating models. The throughputs predicted by the ML models closely match those in the PDEVS simulated data. This study contributes to modeling by demonstrating the potential benefits and complications of utilizing ML for discrete-event systems.} }
@article{10.1109/TCBB.2024.3434340, title = {Machine Learning-Assisted High-Throughput Screening for Anti-MRSA Compounds}, journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics}, volume = {21}, pages = {1911--1921}, year = {2024}, issn = {1545-5963}, doi = {10.1109/TCBB.2024.3434340}, url = {https://doi.org/10.1109/TCBB.2024.3434340}, author = {Shehadeh, Fadi and Felix, LewisOscar and Kalligeros, Markos and Shehadeh, Adnan and Fuchs, Beth Burgwyn and Ausubel, Frederick M. and Sotiriadis, Paul P. and Mylonakis, Eleftherios}, abstract = {Background: Antimicrobial resistance is a major public health threat, and new agents are needed. Computational approaches have been proposed to reduce the cost and time needed for compound screening. Aims: A machine learning (ML) model was developed for the \&lt;italic\&gt;in silico\&lt;/italic\&gt; screening of low molecular weight molecules. Methods: We used the results of a high-throughput \&lt;italic\&gt;Caenorhabditis elegans\&lt;/italic\&gt; methicillin-resistant \&lt;italic\&gt;Staphylococcus aureus\&lt;/italic\&gt; (MRSA) liquid infection assay to develop ML models for compound prioritization and quality control. Results: The compound prioritization model achieved an AUC of 0.795 with a sensitivity of 81\% and a specificity of 70\%. When applied to a validation set of 22,768 compounds, the model identified 81\% of the active compounds identified by high-throughput screening (HTS) among only 30.6\% of the total 22,768 compounds, resulting in a 2.67-fold increase in hit rate. When we retrained the model on all the compounds of the HTS dataset, it further identified 45 discordant molecules classified as non-hits by the HTS, with 42/45 (93\%) having known antimicrobial activity. Conclusion: Our ML approach can be used to increase HTS efficiency by reducing the number of compounds that need to be physically screened and identifying potential missed hits, making HTS more accessible and reducing barriers to entry.} }
@inproceedings{10.1145/3708394.3708433, title = {Research on Learning Outcomes Prediction in Mechanical Engineering Courses based on Machine Learning}, booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education}, pages = {225--229}, year = {2025}, isbn = {9798400710650}, doi = {10.1145/3708394.3708433}, url = {https://doi.org/10.1145/3708394.3708433}, author = {Zhu, Zhongming and Wu, Jingyi and Wang, Mingxiang}, keywords = {Attention Mechanism, Deep Learning Hybrid Model, Educational Data Mining, Learning Outcomes Prediction, Machine Learning}, abstract = {The prediction of learning outcomes is becoming an increasingly crucial aspect of educational quality assurance and the provision of targeted student support. Conventional assessment techniques frequently prove inadequate for identifying students who are at risk of failing to learn in a timely manner. This makes it challenging to intervene in a timely manner. This work proposes an innovative predictive model based on the combination of machine learning and deep learning, which aims to accurately predict the learning outcomes in mechanical engineering courses by analyzing students' multi-dimensional data. Initially, classical principal component analysis (PCA) is employed for data preprocessing purposes. This enables the extraction of key features, reduction of data dimensions, and identification of the core factors influencing academic performance. Subsequently, the preprocessed features are fed into a deep learning model based on a long short-term memory network (LSTM) architecture. LSTM is designed to process time series data, thereby enabling the capture of dynamic changes in students' learning behaviors, including performance trends over time and the time-dependence of learning activities. Furthermore, the model incorporates an attention mechanism to dynamically assign weights to different features, thereby facilitating more accurate identification of the factors with the greatest impact on learning outcomes. The experimental results demonstrate that the hybrid model exhibits superior performance in terms of accuracy and generalization ability when compared to a single machine learning algorithm and traditional statistical methods.} }
@inproceedings{10.1145/3677182.3677256, title = {SmartAuth: A Behavioral Biometric Authentication System based on Machine Learning}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {415--421}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677256}, url = {https://doi.org/10.1145/3677182.3677256}, author = {Mi, Rongxin}, abstract = {Smartphone is an essential part of people's lives, which are often used to store highly sensitive and private information. The information leakage will cause vital security risks for smartphone users. User authentication is a key technology to guarantee smartphone security. Compared with traditional password authentication and face authentication, behavioral bio- metric authentication can keep authenticating the user's identity during use and does not require the user's collaboration in the authentication process. This paper proposes a behavioral bio- metric authentication scheme based on machine learning named SmartAuth, which is a mobile application designed to protect private data on the smartphone by combining touchscreen-based authentication and motion-based authentication. The paper de- scribes the design and implementation of SmartAuth on Android, designs three groups of experiments to verify the performance of the software, and discusses the impact of different machine learning algorithms on the performance.} }
@inproceedings{10.1145/3677525.3678633, title = {JoinDetect: A Data-Led Machine Learning Approach to Detection of Coinjoin Transactions}, booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good}, pages = {5--13}, year = {2024}, isbn = {9798400710940}, doi = {10.1145/3677525.3678633}, url = {https://doi.org/10.1145/3677525.3678633}, author = {O'Meara, John William and Taneja, Mohit and Nicholls, Jack and Kothale, Nitish and Flinter, Steve and Jurcut, Anca Delia}, keywords = {Anti-Money Laundering, Blockchain Technology, CoinJoin, Cryptocurrency, Forensic Analysis, Machine Learning, Privacy Enhancing Technologies, Social Good, location = Bremen, Germany}, abstract = {CoinJoin is a privacy-enhancing technique used in cryptocurrency transactions, designed to obfuscate the ownership of funds by amalgamating multiple inputs, thereby complicating the task of linking identities to addresses. While traditional methods for detecting such transactions have relied on rule-based and heuristic approaches, these often fall short in addressing the complexities of advanced CoinJoin techniques. Recent advancements have shown that machine learning methods significantly outperform these traditional approaches in identifying these types of transactions. This paper introduces a robust data engineering and machine learning framework to effectively distinguish CoinJoin transactions from standard ones. Our approach tackles class imbalance, and accounts for varying frequency distributions of target class transactions. We detail a two-part process: initially demonstrating the approach’s utility in identifying classic Chaumian CoinJoin transactions, then reapplying the methodology for model retraining to adapt to concept drift, enabling the detection of newer, more complex CoinJoin variants. Leveraging ground truth labelled data and proprietary blockchain analytics, our model adeptly navigates the evolving challenges of privacy technologies. Beyond mere identification, this process enhances blockchain forensic applications, improving blockchain address clustering and the efficiency of attribution tracing. This not only streamlines forensic investigations, but also supports regulatory compliance initiatives, ensuring that the application of blockchain technology aligns with broader societal values without compromising user privacy. By integrating blockchain technology in this manner, we contribute to a framework where digital finance meets social good, providing tools that ensure both innovation and integrity in blockchain applications.} }
@article{10.1145/3630104, title = {A Comprehensive Survey on Automated Machine Learning for Recommendations}, journal = {ACM Trans. Recomm. Syst.}, volume = {2}, year = {2024}, doi = {10.1145/3630104}, url = {https://doi.org/10.1145/3630104}, author = {Chen, Bo and Zhao, Xiangyu and Wang, Yejing and Fan, Wenqi and Guo, Huifeng and Tang, Ruiming}, keywords = {Automated machine learning, recommender systems, neural networks}, abstract = {Deep recommender systems (DRS) are critical for current commercial online service providers, which address the issue of information overload by recommending items that are tailored to the user’s interests and preferences. They have unprecedented feature representations effectiveness and the capacity of modeling the non-linear relationships between users and items. Despite their advancements, DRS models, like other deep learning models, employ sophisticated neural network architectures and other vital components that are typically designed and tuned by human experts. This article will give a comprehensive summary of automated machine learning (AutoML) for developing DRS models. We first provide an overview of AutoML for DRS models and the related techniques. Then we discuss the state-of-the-art AutoML approaches that automate the feature selection, feature embeddings, feature interactions, and model training in DRS. We point out that the existing AutoML-based recommender systems are developing to a multi-component joint search with abstract search space and efficient search algorithm. Finally, we discuss appealing research directions and summarize the survey.} }
@proceedings{10.1145/3700906, title = {IPMLP '24: Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, year = {2024}, isbn = {9798400707032} }
@inproceedings{10.1145/3715669.3726844, title = {Predicting Children’s Reading Comprehension Through Eye Movements: Insights from Visual Search and Interpretable Machine Learning}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726844}, url = {https://doi.org/10.1145/3715669.3726844}, author = {Brasser, Jan and Tschirner, Chiara and Stegenwallner-Sch\"utz, Maja and Jakobi, Deborah Noemie and J\"ager, Lena A.}, keywords = {Eye Movements, Reading Comprehension, Visual Search, Reading Development, Beginning Readers, Machine Learning, Interpretability}, abstract = {Early identification of children at risk of reading difficulties is paramount for promoting educational success and equity, as earlier interventions are more effective. Traditional assessment methods of early reading abilities, however, are resource-intensive, and often require basic reading abilities. Predicting reading acquisition from non-reading tasks, particularly during the early stages of formal reading instruction, overcomes these limitations. In this paper, we investigate to what extent eye movements recorded during a visual search task that is hypothesized to correlate with reading ability allows to predict children’s reading comprehension scores at the time of recording as well as one year later. Using machine learning methods that allow for an evaluation of feature importance, namely Neural Additive Models and Random Forests, we explore what eye movement features obtained from a visual search task are predictive of reading comprehension, thus laying the groundwork for future research in early assessment systems based on eye movements.} }
@inproceedings{10.1145/3731806.3731818, title = {Prediction of Standard Minute Value Using Machine Learning in the Garment Industry}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {148--153}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731818}, url = {https://doi.org/10.1145/3731806.3731818}, author = {Hemal, MD Tasadul Islam and Sin, Yew Keong and Ahmad Osman, Ahmad Farimin and Tan, Yi-fei}, keywords = {Garment industry, Mean square error (MSE), Regression, Squared correlation coefficient (R2), Standard minute value (SMV)}, abstract = {The garment industry is a critical component of the global economy, and it has been a major driver of economic growth. The industry faces various challenges, including labour practices, one of which involves the estimation of the standard minute value (SMV). The SMV, representing the time required for a qualified operator to complete a task under standard conditions with appropriate allowances, is often estimated primarily based on engineers' experience. Different individuals may predict the SMV differently. Advancements in technology are expected to standardize SMV prediction and make the production processes more efficient. By knowing SMV accurately in advance, the garment production processes can be improved, thereby reducing cost of producing clothes. In this research, data are collected from a ready-made garment (RMG) industry, with the aim to apply machine learning (ML) based regression models to predict SMV outcomes without depending on industrial engineers. Among the regression models, linear regression (LR), decision tree regression (DTR), and random forest regression (RFR) are chosen for predicting SMV. For the model performance evaluation, mean square error (MSE) and squared correlation coefficient (R2) are calculated. The testing results showed that MSE values fall within 0.004 to 0.006 and R2 range from 0.77 to 0.86, indicating that ML-based regression models are quite accurate in predicting SMV. In addition to providing an efficient method for predicting SMV, this research helps in reducing manpower requirements, enhancing productivity, and minimizing losses.} }
@inproceedings{10.1145/3733155.3734911, title = {Rent Price Prediction Using Machine Learning with Public Land Price, Geospatial, and Demographic Projection Data}, booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {439--445}, year = {2025}, isbn = {9798400714023}, doi = {10.1145/3733155.3734911}, url = {https://doi.org/10.1145/3733155.3734911}, author = {Sato, Goei and Nishiyama, Hiroyuki}, keywords = {Rent Prediction, Machine Learning, National Land Numerical Information}, abstract = {This study investigated the use of “National Land Numerical Information” provided by the Ministry of Land, Infrastructure, Transport and Tourism together with rental property data to improve rent price forecasting through machine learning. We integrated public land price data, geospatial information, and demographic projections (including future population by age group) with basic property attributes (for example, floor area, building age, and amenities). Experimental results using a LightGBM model showed that including land price data and population projections significantly enhanced predictive accuracy, reducing the root mean squared error (RMSE) from 14,319.50 (basic features only) to 13,072.48, a performance improvement of approximately 8.7\%. Feature importance and SHAP analyses revealed that well-known individual property factors (for example, floor area and building age) remained central to rent formation; however, macro-level indicators such as land price and future demographic composition also played critical roles, particularly in capturing mid- to long-term market dynamics. In contrast, including detailed facility information (for example, distances to schools or medical centers) did not yield substantial gains, suggesting that land prices might already encapsulate regional convenience. These findings highlighted a practical yet powerful approach to rent prediction that centered on easily updated public data sources. They also underscored the importance of model interpretability through methods such as SHAP for real-world applications in real estate technology and urban planning. Future research directions include tailoring models to local conditions, incorporating time-series data to capture ongoing changes, and enhancing explainability to support diverse stakeholders.} }
@article{10.5555/3722577.3722849, title = {Boundary constrained Gaussian processes for robust physics-informed machine learning of linear partial differential equations}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Dalton, David and Lazarus, Alan and Gao, Hao and Husmeier, Dirk}, keywords = {physics-informed machine learning, Gaussian processes, partial differential equations, boundary-value problems, inverse problems}, abstract = {We introduce a framework for designing boundary constrained Gaussian process (BCGP) priors for exact enforcement of linear boundary conditions, and apply it to the machine learning of (initial) boundary value problems involving linear partial differential equations (PDEs). In contrast to existing work, we illustrate how to design boundary constrained mean and kernel functions for all classes of boundary conditions typically used in PDE modelling, namely Dirichlet, Neumann, Robin and mixed conditions. Importantly, this is done in a manner which allows for both forward and inverse problems to be naturally accommodated. We prove that the BCGP kernel has a universal representational capacity under Dirichlet conditions, and establish a formal equivalence between BCGPs and boundary-constrained neural networks (BCNNs) of infinite width. Finally, extensive numerical experiments are performed involving several linear PDEs, the results of which demonstrate the effectiveness and robustness of BCGP inference in the presence of sparse, noisy data.} }
@article{10.1145/3561381, title = {Open-world Machine Learning: Applications, Challenges, and Opportunities}, journal = {ACM Comput. Surv.}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3561381}, url = {https://doi.org/10.1145/3561381}, author = {Parmar, Jitendra and Chouhan, Satyendra and Raychoudhury, Vaskar and Rathore, Santosh}, keywords = {Open-world Machine Learning, continual machine learning, incremental learning, open-world image and text classification}, abstract = {Traditional machine learning, mainly supervised learning, follows the assumptions of closed-world learning, i.e., for each testing class, a training class is available. However, such machine learning models fail to identify the classes, which were not available during training time. These classes can be referred to as unseen classes. Open-world Machine Learning (OWML) is a novel technique, which deals with unseen classes. Although OWML is around for a few years and many significant research works have been carried out in this domain, there is no comprehensive survey of the characteristics, applications, and impact of OWML on the major research areas. In this article, we aimed to capture the different dimensions of OWML with respect to other traditional machine learning models. We have thoroughly analyzed the existing literature and provided a novel taxonomy of OWML considering its two major application domains: Computer Vision and Natural Language Processing. We listed the available software packages and open datasets in OWML for future researchers. Finally, the article concludes with a set of research gaps, open challenges, and future directions.} }
@article{10.14778/3648160.3648172, title = {Optimizing Data Acquisition to Enhance Machine Learning Performance}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {1310--1323}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3648160.3648172}, url = {https://doi.org/10.14778/3648160.3648172}, author = {Wang, Tingting and Huang, Shixun and Bao, Zhifeng and Culpepper, J. Shane and Dedeoglu, Volkan and Arablouei, Reza}, abstract = {In this paper, we study how to acquire labeled data points from a large data pool to enrich a training set for enhancing supervised machine learning (ML) performance. The state-of-the-art solution is the clustering-based training set selection (CTS) algorithm, which initially clusters the data points in a data pool and subsequently selects new data points from clusters. The efficiency of CTS is constrained by its frequent retraining of the target ML model, and the effectiveness is limited by the selection criteria, which represent the state of data points within each cluster and impose a restriction of selecting only one cluster in each iteration. To overcome these limitations, we propose a new algorithm, called CTS with incremental estimation of adaptive score (IAS). IAS employs online learning, enabling incremental model updates by using new data, and eliminating the need to fully retrain the target model, and hence improves the efficiency. To enhance the effectiveness of IAS, we introduce adaptive score estimation, which serves as novel selection criteria to identify clusters and select new data points by balancing trade-offs between exploitation and exploration during data acquisition. To further enhance the effectiveness of IAS, we introduce a new adaptive mini-batch selection method that, in each iteration, selects data points from multiple clusters rather than a single cluster, hence eliminating the potential bias due to using only one cluster. By integrating this method into the IAS algorithm, we propose a novel algorithm termed IAS with adaptive mini-batch selection (IAS-AMS). Experimental results highlight the superior effectiveness of IAS-AMS, with IAS also outperforming other competing algorithms. In terms of efficiency, IAS takes the lead, while the efficiency of IAS-AMS is on par with that of the existing CTS algorithm.} }
@inproceedings{10.1145/3620678.3624790, title = {Is Machine Learning Necessary for Cloud Resource Usage Forecasting?}, booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing}, pages = {544--554}, year = {2023}, isbn = {9798400703874}, doi = {10.1145/3620678.3624790}, url = {https://doi.org/10.1145/3620678.3624790}, author = {Christofidi, Georgia and Papaioannou, Konstantinos and Doudali, Thaleia Dimitra}, keywords = {Cloud Computing, Data Persistence, Forecasting, Long Short Term Memory, Machine Learning, Persistent Forecast, Prediction, Resource Usage, location = Santa Cruz, CA, USA}, abstract = {Robust forecasts of future resource usage in cloud computing environments enable high efficiency in resource management solutions, such as autoscaling and overcommitment policies. Production-level systems use lightweight combinations of historical information to enable practical deployments. Recently, Machine Learning (ML) models, in particular Long Short Term Memory (LSTM) neural networks, have been proposed by various works, for their improved predictive capabilities. Following this trend, we train LSTM models and observe high levels of prediction accuracy, even on unseen data. Upon meticulous visual inspection of the results, we notice that although the predicted values seem highly accurate, they are nothing but versions of the original data shifted by one time step into the future. Yet, this clear shift seems to be enough to produce a robust forecast, because the values are highly correlated across time. We investigate time series data of various resource usage metrics (CPU, memory, network, disk I/O) across different cloud providers and levels, such as at the physical or virtual machine-level and at the application job-level. We observe that resource utilization displays very small variations in consecutive time steps. This insight can enable very simple solutions, such as data shifts, to be used for cloud resource forecasting and deliver highly accurate predictions. This is the reason why we ask whether complex machine learning models are even necessary to use. We envision that practical resource management systems need to first identify the extent to which simple solutions can be effective, and resort to using machine learning to the extent that enables its practical use.} }
@article{10.5555/3648699.3648940, title = {Improved powered stochastic optimization algorithms for large-scale machine learning}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Yang, Zhuang}, keywords = {powerball function, stochastic optimization, variance reduction, adaptive learning rate, non-convex optimization}, abstract = {Stochastic optimization, especially stochastic gradient descent (SGD), is now the workhorse for the vast majority of problems in machine learning. Various strategies, e.g., control variates, adaptive learning rate, momentum technique, etc., have been developed to improve canonical SGD that is of a low convergence rate and the poor generalization in practice. Most of these strategies improve SGD that can be attributed to control the updating direction (e.g., gradient descent or gradient ascent direction), or manipulate the learning rate. Along these two lines, this work first develops and analyzes a novel type of improved powered stochastic gradient descent algorithms from the perspectives of variance reduction, where the updating direction was determined by the Powerball function. Additionally, to bridge the gap between powered stochastic optimization (PSO) and the learning rate, which is now still an open problem for PSO, we propose an adaptive mechanism of updating the learning rate that resorts the Barzilai-Borwein (BB) like scheme, not only for the proposed algorithm, but also for classical PSO algorithms. The theoretical properties of the resulting algorithms for non-convex optimization problems are technically analyzed. Empirical tests using various benchmark data sets indicate the efficiency and robustness of our proposed algorithms.} }
@article{10.1145/3656580, title = {Foundations \&amp; Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3656580}, url = {https://doi.org/10.1145/3656580}, author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe}, keywords = {Multimodal machine learning, representation learning, data heterogeneity, feature interactions, language and vision, multimedia}, abstract = {Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this article is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.} }
@inproceedings{10.1145/3723178.3723239, title = {Advancements in Machine Learning for Adaptive Intrusion Detection: A Comprehensive Review}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {460--465}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723239}, url = {https://doi.org/10.1145/3723178.3723239}, author = {Mahmud, Mirza Asif and Hasan, Khandaker Tabin and Karim, Razuan}, keywords = {Intrusion Detection Systems, Cyber Security, Adaptive Defense Mechanisms, Resilient Cyber Defense, Emerging Threats, Feature Selection Methods, Ensemble Learning, Cyber Threat Detection, Data Preprocessing, Model Evaluation, Hybrid Approaches, Classifier Subset Evaluation, Correlation-Based Feature Selection, Security Analytics.}, abstract = {The stability and adaptability of the intrusion detection systems (IDS) are of prime importance in the current scenario of continuous combat between cyber attackers and defenders in cyber-crime activity. Those dedicated rule-based systems have made considerable advancements in detecting established patterns from attacks but severely lag behind when it comes to detection of emerging and evolving threats. Thus, this review article attempts to fill this research gap by synthesizing the recent methodologies and input from different studies regarding adapting intrusion detection today using machine learning techniques. The idea is to take into consideration the effectiveness of various ML algorithms along with feature selection methods and use of ensemble learning techniques for improving the IDS capability. The proposed methodology includes important procedures such as data loading and preprocessing, feature selection, exploratory data analysis, training and evaluating the model, applying a Bayesian Gaussian Mixture Model, performing threshold comparisons, and analyzing outcomes. Observed results from training six models, including Random Forest, MLP, LSTM, Logistic Regression, KNN, and Decision Tree, are reported. Among the six models trained, the Random Forest model emerged the best with an accuracy of 95.10\%, an F1 score of 95.10\%, precision of 95.11\%, and recall of 95.10\% while accruing a loss of 1.77. The models were trained and evaluated on the UNSW-NB15 dataset, a well-known network intrusion detection dataset. For each model, performance metrics such as accuracy, F1 score, precision, recall, and loss are reported. In addition, average Receiver Operating Characteristic Area under Curve analysis, optimal threshold analysis, and performance metric comparisons at various thresholds are included. The findings highlight the effectiveness of ML methods for adaptive intrusion detection and the importance of threshold selection for the optimal detection performance.} }
@inproceedings{10.1145/3724154.3724270, title = {Research and Application of Human Resources Demand Forecasting Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {696--701}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724270}, url = {https://doi.org/10.1145/3724154.3724270}, author = {Ma, Xuanke}, keywords = {Employee turnover prediction, Human resources management, Machine learning, Random forest model}, abstract = {This study demonstrates the potential of applying machine learning techniques in the field of human resource management by applying the random forest model to human resource demand forecasting, specifically employee turnover forecasting. The study used a dataset containing several features such as age, education level, department, gender and years of service to construct and train the model. The feature importance analysis shows that seniority is a key factor in prediction accuracy. The model achieved an accuracy of 85\% on the test dataset with an AUC-ROC value of 0.90, demonstrating its efficiency and accuracy in predicting employee turnover behaviour. In addition, the study further validated the reliability of the model through error analysis and performance evaluation. The results show that using the random forest model to predict employee turnover can provide strong data support for corporate human resource management and help companies better understand and predict changes in human resource needs.} }
@inproceedings{10.1145/3703323.3703698, title = {Imbalanced Multi-Class Research Article Classification using Sentence Transformers and Machine Learning Algorithms}, booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)}, pages = {309--310}, year = {2025}, isbn = {9798400711244}, doi = {10.1145/3703323.3703698}, url = {https://doi.org/10.1145/3703323.3703698}, author = {Gowhar, Saliq and Kempaiah, Praveen and Kamath S, Sowmya and Sugumaran, Vijayan}, keywords = {Document classification, Machine Learning, Sentence Transformers, Natural Language Processing}, abstract = {Categorizing scientific articles into specific research fields is a challenging problem, considering the volume and variety of published literature. However, existing classification systems often suffer from limitations regarding taxonomy or the models used for classification. This article explores approaches built on Sentence Transformer embeddings combined with Machine Learning algorithms to classify articles into 123 predefined classes, with the dataset being heavily imbalanced in nature. The effectiveness of Large Language Models (LLMs) for generating synthetic data is also experimented with, along with synonym augmentation and SMOTE. The best-performing model, the One vs Rest classifier trained on MP-Net sentence embeddings with SMOTE, achieved an accuracy of 77\%, and outperformed all the other models.} }
@article{10.14778/3685800.3685859, title = {Demonstration of MaskSearch: Efficiently Querying Image Masks for Machine Learning Workflows}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {4297--4300}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3685800.3685859}, url = {https://doi.org/10.14778/3685800.3685859}, author = {Wei, Lindsey Linxi and Yeung, Chung Yik Edward and Yu, Hongjian and Zhou, Jingchuan and He, Dong and Balazinska, Magdalena}, abstract = {We demonstrate MaskSearch, a system designed to accelerate queries over databases of image masks generated by machine learning models. MaskSearch formalizes and accelerates a new category of queries for retrieving images and their corresponding masks based on mask properties, which support various applications, from identifying spurious correlations learned by models to exploring discrepancies between model saliency and human attention. This demonstration makes the following contributions: (1) the introduction of MaskSearch's graphical user interface (GUI), which enables interactive exploration of image databases through mask properties, (2) hands-on opportunities for users to explore MaskSearch's capabilities and constraints within machine learning workflows, and (3) an opportunity for conference attendees to understand how MaskSearch accelerates queries over image masks.} }
@inproceedings{10.1145/3723178.3723278, title = {Prediction of IMDb Rating in Netflix Shows Using Machine Learning Algorithms}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {754--761}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723278}, url = {https://doi.org/10.1145/3723178.3723278}, author = {Fariha, Farzana and Irom Rushee, Kawser and Sharier Khan Arnop, Md. Fahim}, abstract = {Netflix is a streaming online service that contains TV shows, series and movies from various countries and categories. People of all ages can seamlessly watch their desired tv series or movies according to various genres. Regarding the prevailing circumstance, controversies arise while choosing the preferred show. This paper attempts to predict the rating of Netflix shows using Machine Learning Algorithms by relating Netflix movies with Internet Movie Database (IMDb) ratings. We performed research regarding the rating of Netflix which will portray the IMDb rating for every show. We have applied three algorithms in our database with a standard accuracy, which are Artificial Neural Network (ANN) = 0.97(97\%) Random Forest = 0.96(96\%) and Decision Tree =1.0(100\%). Among these three algorithms, the decision tree was the most accurate algorithm to predict the ratings. The purpose of our research is to assist people with watching movies and shows without any trouble. Moreover, people can easily choose according to their preferred categories. This research aims to improve the user experience of Netflix.} }
@inproceedings{10.1145/3628096.3628740, title = {An Interface Design Methodology for Serving Machine Learning Models}, booktitle = {Proceedings of the 4th African Human Computer Interaction Conference}, pages = {12--14}, year = {2024}, isbn = {9798400708879}, doi = {10.1145/3628096.3628740}, url = {https://doi.org/10.1145/3628096.3628740}, author = {Ogbuju, Emeka Emmanuel and Ihinkalu, Olalekan and Oladipo, Francisca}, keywords = {AI, Interface, ML, Machine Learning, Model, location = East London, South Africa}, abstract = {Interface design is an essential part of the machine learning (ML) modelling process, which enable users to interact effectively with developed models for solutions/decision making. This paper proposes an interface design methodology for serving ML models by taking into account specific user requirements of the model. The aim of the proposed methodology is to close the gap between the interaction of the ML developers and the end-users by providing user-friendly ML model interfaces.} }
@inproceedings{10.1145/3746469.3746509, title = {Research on the Teaching System Combining Virtual Reality Technology and Machine Learning in Civil Engineering Education}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area Education Digitalization and Computer Science International Conference}, pages = {242--246}, year = {2025}, isbn = {9798400713811}, doi = {10.1145/3746469.3746509}, url = {https://doi.org/10.1145/3746469.3746509}, author = {Han, Luyue and Ren, Xiaoli and Zhang, Xiaolin and Zhou, Song and Wang, Yuxin}, keywords = {Civil Engineering, Intelligent Interaction, Machine Learning, Teaching System, Virtual Reality}, abstract = {The combination of virtual reality technology and machine learning technology brings new opportunities and challenges to civil engineering education. Based on the teaching practices of Chongqing University and Northeastern University, a teaching system combining virtual reality technology and machine learning algorithms is constructed. The system relies on 3D modeling and deep learning algorithms to achieve visualization and intelligent interaction of engineering entities. Experiments show that students can understand complex engineering concepts more intuitively, master design principles and participate in virtual construction processes with the help of this system. Compared with traditional teaching, the combined system significantly improves students' learning interest and practical ability. At the same time, it is found that the degree of virtual reality immersion and the accuracy of machine learning algorithms are key factors affecting teaching effectiveness.} }
@inproceedings{10.1145/3615834.3615845, title = {Sensor-Based Detection of Food Hypersensitivity Using Machine Learning}, booktitle = {Proceedings of the 8th International Workshop on Sensor-Based Activity Recognition and Artificial Intelligence}, year = {2023}, isbn = {9798400708169}, doi = {10.1145/3615834.3615845}, url = {https://doi.org/10.1145/3615834.3615845}, author = {Jablonski, Lennart and Jensen, Torge and Ahlemann, Greta M. and Huang, Xinyu and Tetzlaff-Lelleck, Vivian V. and Piet, Artur and Schmelter, Franziska and Dinkler, Valerie S. and Sina, Christian and Grzegorzek, Marcin}, keywords = {adverse reaction to food, carbohydrate malassimilation, classification, explainable AI, feature engineering, food hypersensitivity, machine learning, precision nutrition, random forest, sensor-based, time series analysis, location = L\"ubeck, Germany}, abstract = {The recognition of physiological reactions with the help of machine learning methods already plays a major role in many research areas, but is still little represented in the field of food hypersensitivity recognition. The present work addresses the question of how food hypersensitivity can be detected by analysing sensor data with explainable machine learning algorithms. In a first step, the Empatica E4 wristband, a wearable device that can be easily integrated into everyday life, collects raw data on various physiological patterns, and algorithms are implemented to extract a variety of features from the raw data. Subsequently, machine learning methods are used to target this classification problem and examine how food hypersensitivity can be detected using these objectively measurable features. In a subject-independent setup, an accuracy of 91\% could be achieved, which provides a promising basis for a new non-invasive and objectively measurable method to detect food hypersensitivity.} }
@inproceedings{10.1145/3697355.3697359, title = {Enhancing Dynamic Hand Gesture Recognition through Optimized Feature Selection using Double Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Big Data and Internet of Things}, pages = {19--25}, year = {2024}, isbn = {9798400717529}, doi = {10.1145/3697355.3697359}, url = {https://doi.org/10.1145/3697355.3697359}, author = {Yan, Keyue and Lam, Chi Fai and Fong, Simon and Marques, Jo\~ao Alexandre Lobo and Song, Qun and Qin, Huafeng}, keywords = {Causal Effect, Double Machine Learning, Feature Selection, Hand Gesture Recognition, Leap Motion Controller}, abstract = {Causal machine learning combines causal inference and machine learning to understand and utilize causal relationships in data. While traditional machine learning focuses on missions of prediction and pattern recognition, causal machine learning goes a step further by revealing causal relationships between variables. In this research, we employ the double machine learning method to identify variables in the gesture recognition problem where independent variables have causal relationships with the final gesture. These variables are then selected for further classification and analysis. By comparing this approach with traditional feature selection methods, we find that the variables selected using double machine learning are more useful for classification and yield excellent results across different machine learning classification models. This new double machine learning based approach provides a valuable reference for researchers during the feature selection stage.} }
@inproceedings{10.1145/3514094.3539530, title = {Privacy Preserving Machine Learning Systems}, booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {898}, year = {2022}, isbn = {9781450392471}, doi = {10.1145/3514094.3539530}, url = {https://doi.org/10.1145/3514094.3539530}, author = {EL MESTARI, Soumia Zohra}, keywords = {differential privacy, homomorphic encryption, secure multi-party computation, location = Oxford, United Kingdom}, abstract = {Machine learning(ML) tools are among the promising data-driven techniques that can help solve many real-life problems. However these tools rely on the collection of large volumes of data, which raises many privacy concerns and more broadly trustworthiness concerns. Privacy Preserving technologies aim at solving the issue by integrating privacy enhancing technologies (PETs) within the machine learning pipelines.} }
@inproceedings{10.1145/3654823.3654870, title = {Machine Learning Based Early Rejection of Low Performance Cells in Li Ion Battery Production}, booktitle = {Proceedings of the 2024 3rd Asia Conference on Algorithms, Computing and Machine Learning}, pages = {251--256}, year = {2024}, isbn = {9798400716416}, doi = {10.1145/3654823.3654870}, url = {https://doi.org/10.1145/3654823.3654870}, author = {Xu, Xukuan and Moeckel, Michael J.}, keywords = {LIB cell production, Machine learning, flexible production, quality control, location = Shanghai, China}, abstract = {Lithium-ion battery cell production is conducted through a multistep production process which suffers from a notable scrap rate. Machine learning (ML) based process monitoring provides solutions to mitigate the impact of substantial scrap rates by repeated multifactorial quality predictions (virtual quality gates) along the process line. This enables an early rejection of battery cells which are unlikely to reach required specifications, avoids further waste of resources at later process steps and simplifies recycling of rejected cells. A hierarchical architecture is used to apply ML algorithms first for process-adapted feature extraction which is guided by a priori knowledge on typical production anomalies. In a second step, these features are correlated with end-of-line quality control data using explainable ML methods. The resulting predictions may lead to pass or fail of a battery cell, or -in the context of flexible production- may also trigger adjustments of later process steps to compensate for detected deficiencies. An example ML based quality control concept is illustrated for a pilot battery cell production line.} }
@inproceedings{10.1145/3759275.3759290, title = {Data‐Driven Insights into Water Art: A Machine Learning Approach to the Power of Natural Spirit}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Systems and Design Innovation}, pages = {92--97}, year = {2025}, isbn = {9798400719554}, doi = {10.1145/3759275.3759290}, url = {https://doi.org/10.1145/3759275.3759290}, author = {Li, Yanhui}, keywords = {Audience Segmentation, Clustering, Contemporary Art, Cross-Cultural Communication, Machine Learning, Natural Spirit, Predictive Modeling, Visual Art, Water Art}, abstract = {In the field of visual arts, water is a special artistic element which symbolizes a profound understanding of nature. Through the examination of its expressive techniques, cultural significance, and close links to human emotions and philosophical ideas comprising painting, sculpture, installation art, photography, and digital media, this article explores the different types of water art and the power of its natural spirit. We employ machine learning methods to improve our traditional case studies and derive deeper, data-driven insights by utilizing a comprehensive dataset on audience viewpoints. First, based on emotional responses, ecological concern, and preferred art forms, unsupervised clustering reveals multiple audience archetypes, including aesthetics-driven and ecologically motivated. Next, classification models identify important demographic and ideological determinants by predicting people's likelihood to support or participate in water-themed events. By measuring hidden trends in viewer involvement, these computational methods work in combination to enhance our qualitative research and deepen our comprehension of water's function as a medium of natural spirit. In addition to providing insight into how water art promotes intercultural dialogue and environmental consciousness in a globalized setting, the results also highlight the usefulness of machine learning in enhancing empirical art studies.} }
@inproceedings{10.1145/3704137.3704142, title = {Understanding the relevance of parallelising machine learning algorithms using CUDA for sentiment analysis}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {28--38}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704142}, url = {https://doi.org/10.1145/3704137.3704142}, author = {Chai, Dakun Mang and Moulitsas, Irene and Bisandu, Desmond Bala}, keywords = {CUDA, Machine Learning, Sentiment Analysis, Word Embedding}, abstract = {Sentiment classification is essential in natural language processing, leveraging machine learning algorithms to understand the sentiment expressed in textual data. Over the years, advancements in machine learning, particularly with Naive Bayes (NB) and Support Vector Machines (SVM), have tremendously improved sentiment classification. These models benefit from word embedding techniques such as Word2Vec and GloVe, which provide dense vector representations of words, capturing their semantic and syntactic relationships. This paper explores the parallelisation of NB and SVM models using CUDA on GPUs to enhance computational efficiency and performance. Despite the computational power offered by GPUs, the literature on parallelising machine learning methods, especially for sentiment classification, remains limited. Our work aims to fill this gap by comparing the performance of NB and SVM on CPU and GPU platforms, focusing on execution time and model accuracy. Our experiments demonstrate that NB outperforms SVM in execution time and overall efficiency, mainly when using GPU acceleration. The NB model consistently achieves higher accuracy, precision, and F1 scores with Word2Vec and GloVe embeddings. The results show the importance of leveraging GPU acceleration using varying numbers of threads per block for large-scale sentiment analysis and laying the foundation for parallelising sentiment classification tasks.} }
@inproceedings{10.1145/3698038.3698519, title = {Vista: Machine Learning based Database Performance Troubleshooting Framework in Amazon RDS}, booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing}, pages = {83--98}, year = {2024}, isbn = {9798400712869}, doi = {10.1145/3698038.3698519}, url = {https://doi.org/10.1145/3698038.3698519}, author = {Singh, Vikramank and Song, Zhao and Narayanaswamy, Balakrishnan Murali and Vaidya, Kapil Eknath and Kraska, Tim}, keywords = {Cloud Databases, ML for Systems, Performance Troubleshooting, location = Redmond, WA, USA}, abstract = {Database performance troubleshooting is a complex multi-step process that broadly involves three key stages- (a) Detection: determining what's wrong and when; (b) Root Cause Analysis (RCA): reasoning about why is the performance poor; (c) Resolution: identifying a fix. A plethora of techniques exist to address each of these problems, but they hardly work in real-world at scale. First, real-world customer workloads are noisy, non-stationary and quasi-periodic in nature rendering traditional detectors ineffective. Second, real-world production databases execute a highly diverse set of queries that skew the database statistics into long-tail distributions causing traditional RCA methods to fail. Third, these databases typically execute millions of such diverse queries every minute rendering traditional methods inefficient when deployed at scale.In this paper we describe Vista, a machine learning based performance troubleshooting framework for databases, and dive-deep into how it addresses the 3 real-world problems outlined above. Vista deploys a deep auto-regressive model trained on a large and diverse Amazon Relational Database Service (RDS) fleet with custom skip connections and periodicity alignment features to model long range and varying periodicity in customer workloads, and detects performance bottlenecks in the form of outliers. Furthermore, it efficiently filters only a top few dominating SQL queries from millions in a problematic workload, and uses a robust causal inference framework to identify the culprit queries and their statistics leading to a low false-positive and false-negative rate. Currently, Vista runs on hundreds of thousands of RDS databases, analyzes millions of workloads every day bringing down the troubleshooting time for RDS customers from hours to seconds. At the end, we also describe several challenges and learnings from implementing and deploying Vista at Amazon scale.} }
@inproceedings{10.1145/3696348.3696878, title = {MLTCP: A Distributed Technique to Approximate Centralized Flow Scheduling For Machine Learning}, booktitle = {Proceedings of the 23rd ACM Workshop on Hot Topics in Networks}, pages = {167--176}, year = {2024}, isbn = {9798400712722}, doi = {10.1145/3696348.3696878}, url = {https://doi.org/10.1145/3696348.3696878}, author = {Rajasekaran, Sudarsanan and Narang, Sanjoli and Zabreyko, Anton A. and Ghobadi, Manya}, keywords = {Congestion control, DNN training, Datacenters for ML, Networks for ML, Resource allocation, Transport layer, location = Irvine, CA, USA}, abstract = {This paper argues that congestion control protocols in machine learning datacenters sit at a sweet spot between centralized and distributed flow scheduling solutions. We present MLTCP, a technique to augment today's congestion control algorithms to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. We demonstrate that MLTCP uses a gradient descent trend with a step taken at every training (or fine-tuning) iteration towards reducing network congestion among competing jobs.} }
@article{10.1145/3717063, title = {Informing the Design of Individualized Self-Management Regimens from the Human, Data, and Machine Learning Perspectives}, journal = {ACM Trans. Comput.-Hum. Interact.}, volume = {32}, year = {2025}, issn = {1073-0516}, doi = {10.1145/3717063}, url = {https://doi.org/10.1145/3717063}, author = {Pichon, Adrienne and Urteaga, I\~nigo and Mamykina, Lena and Elhadad, No\'emie}, keywords = {reinforcement learning, self-management, chronic illness}, abstract = {Intelligent systems for self-management can help patients and improve quality of life. However, designing AI-based systems is challenging because designers need to account not only for user needs, but also for capabilities and practical constraints of underlying algorithms. We propose and implement a human-centered AI framework to align human and technological requirements and constraints that can guide design of intelligent systems for personal health. We use concepts from a machine learning technique, reinforcement learning, to elicit user needs, through directed content analysis of user interviews, and uncover practical data constraints, through analysis of “in the wild” user engagement logs from a self-monitoring app. We gather and triangulate human-machine-data requirements for a self-management tool for individuals with endometriosis—a poorly understood, complex chronic condition with no reliable treatment. We present recommendations for developing a system that aligns with needs, capabilities, and constraints from human user, data, and machine learning perspectives.} }
@inproceedings{10.1145/3627106.3627188, title = {PAVUDI: Patch-based Vulnerability Discovery using Machine Learning}, booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference}, pages = {704--717}, year = {2023}, isbn = {9798400708862}, doi = {10.1145/3627106.3627188}, url = {https://doi.org/10.1145/3627106.3627188}, author = {Ganz, Tom and Imgrund, Erik and H\"arterich, Martin and Rieck, Konrad}, keywords = {Patches, Program Representations, Vulnerability Discovery, location = Austin, TX, USA}, abstract = {Machine learning has been increasingly adopted for automatic security vulnerability discovery in research and industry. The ability to automatically identify and prioritize bugs in patches is crucial to organizations seeking to defend against potential threats. Previous works, however only consider bug discovery on statement, function or file level. How one would apply them to patches in realistic scenarios remains unclear. This paper presents a novel deep learning-based approach leveraging an interprocedural patch graph representation and graph neural networks to analyze software patches for identifying and locating potential security vulnerabilities. We modify current state-of-the-art learning-based static analyzers to be applicable to patches and show that our patch-based vulnerability discovery method, a context and flow-sensitive learning-based model, has a more than increased detection performance, is twice as robust against concept drift after model deployment and is particularly better suited for analyzing large patches. In comparison, other methods already lose their efficiency when a patch touches more than five methods.} }
@inproceedings{10.1145/3649476.3658775, title = {IR drop Prediction Based on Machine Learning and Pattern Reduction}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024}, pages = {516--519}, year = {2024}, isbn = {9798400706059}, doi = {10.1145/3649476.3658775}, url = {https://doi.org/10.1145/3649476.3658775}, author = {Chang, Yong-Fong and Chen, Yung-Chih and Cheng, Yu-Chen and Lin, Shu-Hong and Lin, Che-Hsu and Chen, Chun-Yuan and Chen, Yu-Hsuan and Lee, Yu-Che and Lin, Jia-Wei and Pao, Hsun-Wei and Chang, Shih-Chieh and Li, Yi-Ting and Wang, Chun-Yao}, keywords = {Dynamic IR drop analysis, IR drop prediction, pattern reduction, location = Clearwater, FL, USA}, abstract = {With the advances in semiconductor technology, the sizes of transistors are getting smaller, which has led to an increasingly severe impact of IR drop. Consequently, this trend has amplified the significance of IR drop analysis within the realm of chip design. However, analyzing IR drop is resource-intensive and time-consuming, since numerous simulation patterns are required to verify the power integrity of circuits. Additionally, with every engineering change order (ECO) step, a reevaluation is necessary. In this paper, we propose a machine learning-based method to predict IR drop levels and present an algorithm for reducing simulation patterns, which could reduce the time and computing resources required for IR drop analysis within the ECO flow. Experimental results show that our approach can reduce the number of patterns by approximately 50\%, thereby decreasing the analysis time while maintaining accuracy.} }
@inproceedings{10.1145/3625343.3625356, title = {Machine Learning based Intrusion Detection System for IoT Applications using Explainable AI}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625356}, url = {https://doi.org/10.1145/3625343.3625356}, author = {Mukhtar Bhatti, Muhammad Asim and Awais, Muhammad and Iqtidar, Aamna}, keywords = {Artificial Intelligence (AI), Decision Tree, Internet of things, Intrusion detection system (IDS), Machine learning (ML), Multilayer Perceptron (MLP), XGBoost classifier, location = Bangkok, Thailand}, abstract = {This research focuses on studying the classification performance of a Machine Learning-based Intrusion Detection System (IDS) using the UNSW-NB15 dataset. The effectiveness of three classifiers - Decision Tree, Multilayer Perceptron (MLP), and XGBoost - was analyzed to determine their accuracy in identifying attacks and normal network traffic. The experimental results revealed that Decision Tree achieved an accuracy of 96.5\%, MLP achieved an accuracy of 89.83\%, and XGBoost achieved an accuracy of 89.9\%. Additionally, the Explanability of the machine learning models was analyzed, highlighting the differences in interpretability among the classifiers. It was observed that Decision Tree provided better Explanability, but lower accuracy compared to MLP and XGBoost. Overall, this research contributes to our comprehension of the performance and Explanability of three different machine learning classifiers for intrusion detection. The findings can provide valuable insights for choosing suitable classifiers that align with the specific priorities and requirements of the IDS system.} }
@inproceedings{10.1145/3718751.3718820, title = {A Machine Learning Approach to Detecting Financial Anomalies in Large Global Companies}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {433--438}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718820}, url = {https://doi.org/10.1145/3718751.3718820}, author = {Ji, Jiexin}, keywords = {Financial Anomaly Detection, Machine Learning, Isolation Forest, Market Capitalization, P/E Ratio, Financial Metrics Analysis}, abstract = {Financial analysis is essential for assessing the health and performance of large global companies, particularly in identifying anomalies that may signal overvaluation, undervaluation, or operational inefficiencies. With the growing complexity of financial markets and the volume of data, traditional analysis methods are often insufficient for detecting hidden patterns or irregularities. In the present work, the Isolation Forest algorithm is utilized to analyzing financial metrics from the world's largest companies. The dataset includes key metrics that reflect both the market valuation and operational performance, such as Market Capitalization, Revenue, Earnings, and the Price-to-Earnings (P/E) Ratio. After data preprocessing, the Isolation Forest algorithm was applied to detect outliers in the dataset, revealing companies with discrepancies between market expectations and actual financial performance. As the research results reveal, the proposed machine learning solution performs well in identifying financial anomalies and thereby provide a statistical basis for investors and analysts in data-driven decision-making.} }
@inproceedings{10.1145/3630106.3658955, title = {Machine learning data practices through a data curation lens: An evaluation framework}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {1055--1067}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658955}, url = {https://doi.org/10.1145/3630106.3658955}, author = {Bhardwaj, Eshta and Gujral, Harshit and Wu, Siyi and Zogheib, Ciara and Maharaj, Tegan and Becker, Christoph}, keywords = {data practices, dataset creation, datasets, datasheets, documentation, evaluation, machine learning, rubric, location = Rio de Janeiro, Brazil}, abstract = {Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.} }
@inbook{10.1145/3724504.3724517, title = {Extract Information to Improve Teaching Effectiveness Using Machine Learning Methods}, booktitle = {Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence}, pages = {73--77}, year = {2025}, isbn = {9798400711732}, url = {https://doi.org/10.1145/3724504.3724517}, author = {Feng, Changli and Wei, Haiyan and Li, Xin and Qiao, Sai}, abstract = {In traditional curriculum evaluations, educators often suggest improvements based on their impressions of teaching effectiveness and student achievement. However, this approach is not based on empirical evidence and can lead to subjective and potentially biased initiatives. To address this problem, our research introduces a data-driven approach to unearthing teaching reform strategies using the Random Forest classifier. Under this framework, students provide a comprehensive self-assessment score, as well as ratings in eight different areas. These scores are then used to calculate the achievement of each course objective, as outlined in the syllabus, which helps to identify areas that need enhancement and suggest targeted interventions. In addition, a threshold-based approach is used to classify the students' comprehensive assessment scores. These data, together with the self-assessment scores, make up the dataset of the Random Forest algorithm, which produces key features that influence classification. By examining these key characteristics, we can identify key teaching areas that need attention in future curricula and develop practical measures to be integrate into subsequent curricula.} }
@inproceedings{10.1145/3534678.3542902, title = {Machine Learning for Materials Science (MLMS)}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4902--4903}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542902}, url = {https://doi.org/10.1145/3534678.3542902}, author = {Sardeshmukh, Avadhut and Reddy, Sreedhar and Gautham, B P. and Agrawal, Ankit}, keywords = {machine learning, materials informatics, materials science, microstructure informatics, location = Washington DC, USA}, abstract = {Artificial intelligence and machine learning are being increasingly used in scientific domains such as computational fluid dynamics and chemistry. Particularly notable is a recently renewed interest in solving partial differential equations using machine learning models, especially deep neural networks, as partial differential equations arise in many scientific problems of interest. Within materials science literature, there has been a surge in publications on AI-enabled materials discovery, in the last five years. Despite this, the interaction between machine learning researchers and materials scientists (especially, scientists working on structural materials, their microstructures, textures and so on) has been very sparse. On the other hand, AI/ML techniques can clearly be integrated into materials design frameworks (e.g., MGI efforts) to support accelerated materials development, novel simulation methodologies and advanced data analytics. Hence there is an immediate need for exchange of ideas and collaborations between machine learning and materials science communities. We believe a workshop dedicated to this theme would be well- suited to foster such collaborations. The aim of this workshop is to bring together the computer science and materials science communities and foster deeper collaborations between the two to accelerate the adoption of AI/ML in materials science. We hope and envision this workshop to facilitate in building a community of researchers in this interdisciplinar area in the years ahead.} }
@inproceedings{10.1145/3639592.3639594, title = {Conventional Machine Learning Approach for Waste Classification}, booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference}, pages = {7--12}, year = {2024}, isbn = {9798400716225}, doi = {10.1145/3639592.3639594}, url = {https://doi.org/10.1145/3639592.3639594}, author = {Jangsamsi, Kharittha}, keywords = {Fourier descriptors, feature extraction, histogram of oriented gradients, local binary pattern, waste classification, location = Kyoto, Japan}, abstract = {Waste management is a complex and challenging process, especially waste classification to sort waste by categories. The paper aims to overcome these challenges by proposing a waste classification approach that uses various feature extraction algorithms along with a support vector machine (SVM). The purpose is to identify the most effective feature for building a classification model, even with a low number of samples and high intra-class variance. SVM was used for classification while Fourier descriptors (FDs), histogram of oriented gradients (HOG), and local binary pattern (LBP) were used for feature extraction. The dataset used in this paper was obtained from Kaggle.com and Google.com with different types of vision problems. The experimental results showed that classification with LBP feature extraction achieves the highest accuracy. This accuracy is higher than the experiments with other feature extractions.} }
@inproceedings{10.1145/3671151.3671249, title = {Applications of Machine Learning in Recognizing Chinese Calligrapher's Handwriting Styles}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {548--552}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671249}, url = {https://doi.org/10.1145/3671151.3671249}, author = {Hu, Xiangkun and Wu, Fei}, keywords = {CNN, Calligraphic Stroke Features, Chinese Calligraphy Recognition, Deep Learning, location = Wuhan, China}, abstract = {This study explores the potential of machine learning (ML) techniques in recognizing the handwriting styles of Chinese calligraphers. By analyzing a dataset comprising diverse samples of calligraphy from renowned artists, we implement and compare various ML models, including Convolutional Neural Networks (CNNs) and Support Vector Machines (SVMs), to identify unique style markers[1]. Our findings demonstrate the efficacy of CNNs in capturing intricate patterns, yielding a promising accuracy rate in style classification. This research not only contributes to the digitization and preservation of cultural heritage but also offers insights into the fusion of technology and art.} }
@inbook{10.1145/3729706.3729773, title = {Analyzing Phase-Specific Drivers of Digital Transformation in SMEs: A Machine Learning-Based Approach}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {425--430}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729773}, author = {Zhang, Fan and Liu, Chongyu and Meng, Haiting and Du, Gangqiang and Wang, Xiangyu and Zhou, Yanling}, abstract = {This study explores the evolving drivers of digital transformation (DT) in small and medium-sized enterprises (SMEs) by analyzing data from two phases: the shift from non-digitalization to initial digitalization, and from initial to advanced digitalization. Using a combination of Logit, linear regression and machine learning techniques, specifically XGBoost and SHAP value analysis, this research identifies key incentives in each phase. The external factors, such as the business environment and resource availability, are crucial in the early phase of digitalization. The internal factors, including digital organizational design and digital literacy are assuming an increasingly critical role in the second phase of DT. The findings offer insights into the challenges SMEs face during digital transformation journey, and underscore the importance of organizational restructuring and capacity-building for advancing digitalization.} }
@inproceedings{10.1145/3610978.3640598, title = {Design Principles for Building Robust Human-Robot Interaction Machine Learning Models}, booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction}, pages = {247--251}, year = {2024}, isbn = {9798400703232}, doi = {10.1145/3610978.3640598}, url = {https://doi.org/10.1145/3610978.3640598}, author = {Bhagat Smith, Josh and Mallampati, Vivek and Baskaran, Prakash and Giolando, Mark-Robin and Adams, Julie A.}, keywords = {design principles, human-robot interaction, machine learning, location = Boulder, CO, USA}, abstract = {Effective collaboration between humans and robots hinges on the robot's ability to comprehend its human teammate. This collaboration demands the development of machine learning models that bridge the gap between human physiological signals and their mental states. However, the challenge lies in developing generalizable machine learning models using data collected in controlled experimental conditions. This manuscript proposes a set of principles for designing human subject evaluations, emphasizing the crucial balance between experimental control and ecological validity while also balancing fundamental machine learning trade-offs.} }
@inproceedings{10.1145/3688671.3688740, title = {Machine Learning Applications in Nanotechnology Manufacturing: From Etching Accuracy to Deposition Prediction}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688740}, url = {https://doi.org/10.1145/3688671.3688740}, author = {Kondi, Alex and Papia, Efi-Maria and Constantoudis, Vassilios}, keywords = {Line Edge Roughness (LER), nanofabrication, deposition, pattern transfer, machine learning, transfer learning, regression}, abstract = {The integration of machine learning (ML) within the realm of nanomanufacturing processes, specifically through the applications of thickness prediction during deposition and pattern fidelity during etching, may present transformative potential. This paper discusses two ML-based methodologies that enhance the precision and efficiency of these critical processes. In the context of etching, we revisit the 3D geometrical modeling of etch-induced Line Edge Roughness (LER) transfer from photoresist lines to the substrate, refining previous models to account for realistic three-dimensional surface topographies. Meanwhile, in the deposition segment, we employ a deep neural network (DNN) to predict the thickness of films deposited on rough substrates using binarized top-down Scanning Electron Microscopy (SEM) images. The outcomes demonstrate how ML-based methodologies not only predict but also potentially control the nanofabrication parameters, leading to improved manufacturing outcomes.} }
@inproceedings{10.1145/3580305.3599206, title = {The Second Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5859--5860}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599206}, url = {https://doi.org/10.1145/3580305.3599206}, author = {Goldenberg, Dmitri and Ross, Chana and Meir Lador, Shir and Cheong, Lin Lee and Xu, Panpan and Sokolova, Elena and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit and Weil Modlinger, Amit and Potdar, Saloni}, keywords = {data science management, machine learning management, ml product development, location = Long Beach, CA, USA}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Second KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@inproceedings{10.1145/3637528.3672497, title = {Machine Learning for Clinical Management: From the Lab to the Hospital}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4736}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3672497}, url = {https://doi.org/10.1145/3637528.3672497}, author = {Gavald\`a, Ricard}, keywords = {chronic disease, fairness in health, healthcare analytics, healthcare resources, machine learning, patient safety, location = Barcelona, Spain}, abstract = {Population aging, increasing social demands, and rising costs of treatments are stressing healthcare systems to the point of risking the sustainability of universal and accessible healthcare. A hope in this dismal panorama is that there are large inefficiencies, and so opportunities for getting more from the same resources. To name a few, avoidable hospitalizations, unnecessary medication and tests, and lack of coordination among healthcare agents are estimated to cost several hundred billion euros per year in the EU. Technology can be useful for locating and reducing these inefficiencies, and within technology, the full exploitation of the data that the system collects to record its activity. In this talk, I will review the case for activity data analytics in healthcare, with two main considerations: 1) The need to include information about resources and costs in the models, in addition to clinical knowledge and patient outcomes, and 2) the need to use mostly data that healthcare organizations already collect and is not locked and distributed in silos. Fortunately, data collected for administrative and billing purposes, even though imperfect, partial, and low resolution, can be used to improve efficiency and safety, as well as fairness and equity.I will focus on the work carried out at Amalfi Analytics, a spin-off of my research group at UPC in Barcelona. On the one hand, we have addressed predictive management in hospitals, from influx to the emergency room to availability of surgical areas, beds, and staff. Anticipating activity, needs, and resource availability lets managers improve critical KPIs, e.g. waiting times, but also reduce staff stress, which leads to fewer medical errors and accidents. On the other hand, we have developed a patient cohort analyzer, based mostly on a recent clustering algorithm, that gives experts a fresh view of their patient population and lets them refine protocols and identify high-risk patient groups. This tool has also been used to support territorial planning and resource allocation.These problems have been extensively addressed in the past, but actual penetration of solutions in hospitals is smaller than one could expect. For example, one can find hundreds of papers on predicting influx to emergency rooms or bed demands, but many of them conclude after producing an AUC figure, and even fewer describe a working system that can be exported from the hospital where they were developed to others at an affordable cost. I will describe the approach taken at Amalfi so that hospitals can have such a solution up and running in a few days of work for their IT departments, in what I think is an interesting combination of software engineering and automatic Machine Learning.} }
@inproceedings{10.1145/3708359.3712105, title = {Robust Relatable Explanations of Machine Learning with Disentangled Cue-specific Saliency}, booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces}, pages = {1203--1231}, year = {2025}, isbn = {9798400713064}, doi = {10.1145/3708359.3712105}, url = {https://doi.org/10.1145/3708359.3712105}, author = {Abichandani, Harshavardhan Sunil and Zhang, Wencan and Lim, Brian Y}, keywords = {Explainable AI, misleading explanations, robust machine learning, vocal emotion}, abstract = {Concept-based explanations help users understand the relation between model predictions and meaningful cues. However, under noisy real-world conditions, data perturbations lead to distorted and deviated explanations. We hypothesize that these corruptions affect specific cues rather than all, so disentangling them may help reduce model dependency on degraded cues. For the application of explaining emotional speech recognition, we propose RobustRexNet to explain with disentangled and discretized saliency maps for separate speech cues (e.g., loudness, pitch) to improve robustness against noise. Modeling evaluations show that RobustRexNet improved both model performance and explanation faithfulness in noisy and privacy-preserving distortions. User studies further indicate that the robust explanations aligned better with human intuition and improved user emotion labeling under noise. This work contributes toward robust explainable AI to improve user trust under real-world conditions.} }
@inproceedings{10.1145/3650215.3650398, title = {Analysis of Machine Learning Methods for Water Quality Evaluation of Penaeus Vannamei}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {1032--1041}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650398}, url = {https://doi.org/10.1145/3650215.3650398}, author = {Peng, Xiaohong and Li, Zixin and Ma, Zebin and Zhang, Ying}, abstract = {In global aquaculture, Penaeus vannamei stands out due to its immense economic importance. Water quality, being pivotal for its successful cultivation, demands precise evaluation techniques. This research undertook a meticulous systematic review and, leveraging data mining, crafted a rich dataset of 50,000 water quality samples pertinent to Penaeus vannamei. Diving deep into machine learning, we assessed four key algorithms: decision tree, Bayesian, k-nearest neighbor, and support vector machine, each tailored for intricate multi-feature multi-classification challenges. Of these, the Gaussian Parsimonious Bayes-based model was distinguished by its superior accuracy and efficiency. This study successfully applied machine learning techniques to develop a reliable and efficient water quality evaluation model for Penaeus vannamei farming, offering a scientific tool for the aquaculture industry and facilitating more efficient, scientifically informed farming management. This research contributes an innovative scientific approach and theoretical foundation for the sustainable growth of the aquaculture industry.} }
@inproceedings{10.1145/3706890.3707039, title = {Machine Learning-based Algorithm for Screening Drug Candidates in Breast Cancer Treatment}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {868--874}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3707039}, url = {https://doi.org/10.1145/3706890.3707039}, author = {Zhou, Yuxuan and Ye, Yuhan and Fan, Caiyun}, keywords = {Breast cancer drug discovery, heuristic algorithms, machine learning, multi-objective optimization}, abstract = {Breast cancer is one of the most prevalent malignant tumors globally. This study aims to develop and optimize a model for predicting biological activity and estimating pharmacokinetic (ADMET) properties using machine learning algorithms. The goal is to screen potential drug candidates for breast cancer. Based on this model, we propose a molecular screening model for breast cancer drug molecules utilizing a greedy-genetic-LGB algorithm to enhance the efficiency of drug molecule screening.First, two rounds of screening of potential drug molecules using LightGBM and the Permutation importance algorithm were performed to identify the 20 most relevant molecular descriptors for biological activity.Secondly, the PSO-LGB algorithm was proposed. Based on this model, regression prediction models for compound bioactivity and classification prediction models for pharmacokinetic properties were established and compared with popular industry machine models, demonstrating the superiority of the proposed models.Finally, the greedy-genetic-LGB algorithm was proposed to transform the drug molecule screening problem into a multi-objective optimization problem. The objective function is the prediction result of the two models, and the optimization objective is the descriptor value of the drug molecule, based on the PSO-LGB compound bioactivity and PSO-LGB pharmacokinetic property prediction models. To verify the superiority of the proposed algorithm, experiments were conducted on the public dataset of breast cancer drug molecule screening provided by DrugBank. A set of optimal molecule values was obtained, resulting in an excellent active pIC50 value (8.7792) with this molecule combination along with good ADMET performance.In summary, the machine learning approach proposed in this study can effectively predict the biological activity and ADMET properties of compounds and improve the speed and effectiveness of screening potential breast cancer drug molecules, as well as potentially facilitate their clinical studies.} }
@inproceedings{10.1145/3712255.3726776, title = {An Evolutionary Approach to Interpretable Machine Learning for ICU Length of Stay Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {291--294}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726776}, url = {https://doi.org/10.1145/3712255.3726776}, author = {Kar, Reshma and Ilangovan, Eslin Kiran and Bizel, Gulhan and Patra, Braja Gopal}, keywords = {interpretable machine learning, multiple gaussian model, ICU length of stay prediction, MIMIC-IV dataset, gravitational search algorithm, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Neural networks have repeatedly demonstrated the ability to accurately model complex nonlinear relationships. However, the black-box nature of such models has led to concerns about bias and trustworthiness especially in settings like healthcare where it impacts multiple aspects of patient care including prognosis and resource-allocation. Existing model-agnostic methods to enhance neural network interpretability only offer an external perspective on the working of neural networks. To solve this issue, we designed an Evolutionary Multiple Gaussian Model that replaces the activation function of a perceptron with an aggregate of multiple Gaussian functions. The parameters of this model were evaluated using the gravitational search algorithm with the objective of minimizing a function of the mean absolute percentage error (MAPE) and mean squared error between the predicted and actual values. We performed multiple experiments on the Medical Information Mart for Intensive Care-IV dataset to predict the patient length of stay in Intensive Care Units. Results indicate that the proposed algorithm outperforms it's popular counterparts in terms of MAPE while offering better interpretability.} }
@inproceedings{10.1145/3748825.3748931, title = {Construction of a Diagnostic Model for Sepsis Based on Multiple Machine Learning Algorithms and Bioinformatics}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {684--689}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748931}, url = {https://doi.org/10.1145/3748825.3748931}, author = {Chen, Junjie and Liu, Zhao and Hu, Huanjun}, keywords = {Bioinformatics, Machine learning, Nomogram, Sepsis}, abstract = {Sepsis is a severe clinical condition with an increasing demand for the identification of biomarkers and therapeutic targets associated with the disease. The current challenge in research lies in how to effectively extract key genes closely related to sepsis from complex biological data and to construct diagnostic models that improve early diagnosis and treatment outcomes. To address this, this study performed differential expression analysis on sepsis-related data, constructed a protein-protein interaction network, and identified core genes associated with sepsis. We applied various machine learning algorithms to analyze the expression data of these key genes and selected effective models based on ROC curves and residual value metrics. Using the best model, we developed a diagnostic nomogram containing five genes with the highest importance scores, which was validated in an external dataset, achieving an accuracy rate of up to 97.3\%. The individual prediction accuracy of the model genes IL10RB, FCER1G, FARS2, FHIT, and CCDC88C was lower than that of the overall diagnostic model, reflecting the effectiveness of our model. These findings provide new perspectives for improving the early identification and treatment of sepsis and hold significant implications for enhancing patient prognosis.} }
@proceedings{10.1145/3696687, title = {MLPRAE '24: Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, year = {2024}, isbn = {9798400709876} }
@article{10.1145/3568429, title = {Improving Storage Systems Using Machine Learning}, journal = {ACM Trans. Storage}, volume = {19}, year = {2023}, issn = {1553-3077}, doi = {10.1145/3568429}, url = {https://doi.org/10.1145/3568429}, author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Burford, Andrew and McNeill, Michael and Arkhangelskiy, Michael and Zadok, Erez}, keywords = {Operating systems, storage systems, Machine Learning, storage performance optimization}, abstract = {Operating systems include many heuristic algorithms designed to improve overall storage performance and throughput. Because such heuristics cannot work well for all conditions and workloads, system designers resorted to exposing numerous tunable parameters to users—thus burdening users with continually optimizing their own storage systems and applications. Storage systems are usually responsible for most latency in I/O-heavy applications, so even a small latency improvement can be significant. Machine learning (ML) techniques promise to learn patterns, generalize from them, and enable optimal solutions that adapt to changing workloads. We propose that ML solutions become a first-class component in OSs and replace manual heuristics to optimize storage systems dynamically. In this article, we describe our proposed ML architecture, called KML. We developed a prototype KML architecture and applied it to two case studies: optimizing readahead and NFS read-size values. Our experiments show that KML consumes less than 4 KB of dynamic kernel memory, has a CPU overhead smaller than 0.2\%, and yet can learn patterns and improve I/O throughput by as much as 2.3 and 15 for two case studies—even for complex, never-seen-before, concurrently running mixed workloads on different storage devices.} }
@inproceedings{10.1145/3701625.3701695, title = {Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding}, booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality}, pages = {703--705}, year = {2024}, isbn = {9798400717772}, doi = {10.1145/3701625.3701695}, url = {https://doi.org/10.1145/3701625.3701695}, author = {Cabral, Raphael and Kalinowski, Marcos}, keywords = {SOLID Design Principles, Machine Learning, Code Understanding}, abstract = {[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects. These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms. However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn’t adhere to software design best practices. [Goal] To better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding. [Method] We conducted a controlled experiment with three independent trials involving 100 data scientists. We restructured real industrial ML code that did not use SOLID principles. Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles. Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding. [Results] The dissertation results provide statistically significant evidence that adopting the SOLID design principles can improve code understanding within ML projects. [Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the quality and maintainability of ML code.} }
@article{10.1145/3711095, title = {Understanding Interaction with Machine Learning through a Thematic Analysis Coding Assistant: A User Study}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3711095}, url = {https://doi.org/10.1145/3711095}, author = {Milana, Federico and Costanza, Enrico and Musolesi, Mirco and Ayobi, Amid}, keywords = {interactive machine learning, thematic analysis}, abstract = {Interactive Machine Learning (IML) enables users, including non-experts in ML, to iteratively train and improve ML models. However, limited research has been reported on how non-experts interact with these systems. Focusing on thematic analysis as a practical application, we report on a user study where 20 participants interacted with TACA, a functioning IML tool. Thematic analysis involves individual interpretation of ambiguous data, hence it is suited for and can benefit from the iterative customization of models supported by IML. Through a combination of interaction logs and semi-structured interviews, our findings revealed that, by using TACA, participants critically reflected on their analysis, gained new thematic insights, and adapted their interpretative stance. We also document misconceptions of ML concepts, positivist views, and personal blame for poor model performance. We then discuss how applications could be designed to improve the understanding of IML concepts and foster reflexive work practices.} }
@inproceedings{10.1145/3748777.3748792, title = {ImPORTance - Machine Learning-Driven Analysis of Global Port Significance and Network Dynamics for Improved Operational Efficiency}, booktitle = {Proceedings of the 19th International Symposium on Spatial and Temporal Data}, pages = {66--75}, year = {2025}, doi = {10.1145/3748777.3748792}, url = {https://doi.org/10.1145/3748777.3748792}, author = {Carlini, Emanuele and Di Gangi, Domenico and de Lira, Vinicius Monteiro and Kavalionak, Hanna and Soares, Amilcar and Spadon, Gabriel}, keywords = {AIS, Ports Network, Port Centrality, Port Importance, Connectivity}, abstract = {Seaports play a crucial role in the global economy, and researchers have sought to understand their significance through various studies. In this paper, we aim to explore the common characteristics shared by important ports by analyzing the network of connections formed by vessel movement among them. To accomplish this task, we adopt a bottom-up network construction approach that combines three years’ worth of AIS (Automatic Identification System) data from around the world, constructing a Ports Network that represents the connections between different ports. Through this representation, we utilize machine learning to assess the relative significance of various port features. Our model examined such features and revealed that geographical characteristics and the port’s depth are indicators of a port’s importance to the Ports Network. Accordingly, this study employs a data-driven approach and utilizes machine learning to provide a comprehensive understanding of the factors contributing to the extent of ports. Our work aims to inform decision-making processes related to port development, resource allocation, and infrastructure planning within the industry.} }
@inproceedings{10.1145/3659677.3659738, title = {Machine Learning to Predict Law Graduates of Universit Thomas Sankara}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659738}, url = {https://doi.org/10.1145/3659677.3659738}, author = {Konate, Brahima and Ouedraogo, Tounwendyam Fr\'ed\'eric}, keywords = {Campusfaso, Machine Learning, Prediction, guidance, law, location = Meknes, AA, Morocco}, abstract = {Improving the academic success rate of students is of interest in public universities in Burkina Faso. The guidance of new student is an important step in this process. Predicting the success of students graduating in the Faculty of Law of Universite Thomas Sankara (UTS) aims ultimately to build an intelligent guidance process to help improve the success rate of students. This intelligent process will improve the algorithm currently used for the guidance of new students in public universities in Burkina Faso. For this first step of our research a literature review on similar problems allowed us to select 4 models, including logistic regression, KNN, decision tree and SVM. In this article, data from 2632 new students guided to UTS in 2018 by Campusfaso were used. The features focused the grades obtained in the high school diploma exam and on the demographic data of new students. The article describes our methodological approach and presents the results following the implementation of the 4 machine learning models used. The results show that the SVM model gives the best score in predicting success in obtaining a bachelor’s degree for a student guided to UTS by Campusfaso. Implementing this study could help to improve the success rate of new students in the faculty of law of UTS.} }
@inproceedings{10.1145/3718751.3718839, title = {Binary Classification of Algae and Non-algae Microorganisms: A Comparison of Machine Learning Models}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {552--557}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718839}, url = {https://doi.org/10.1145/3718751.3718839}, author = {Xu, Jing}, keywords = {Decision tree, Logistic regression, Machine learning, Microorganisms classification, Random forest, Support vector machine}, abstract = {Microorganisms are the most numerous group of organisms in the world and play an indispensable role on earth. They affect soil fertility, water cleanliness, atmospheric composition, and biological health. Among them, algae microorganisms have unique ecological functions and biological characteristics compared with other microbes. For this reason, the distinction between algae and non-algae microbes is essential to protect the stability of ecosystems. The purpose of this work is to investigate a 30527 x 25-field microbial dataset using various machine learning models to tackle the binary classification problem of algae and non-algae microorganisms. To address this research question, the performance of decision trees, random forests, logistic regression, and support vector machine models in machine learning is assessed in terms of accuracy, F1 scores, and AUC values. With the maximum accuracy of 0.829, the findings demonstrate how effectively the machine learning decision tree performs in the analysis of this dataset. For classification analysis, the outcomes can be used to other datasets that are similar.} }
@inproceedings{10.1145/3681777.3698469, title = {Comparing Associations Of Chronic Health Outcomes with SDoH Indices Using Machine Learning}, booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Spatial Computing for Epidemiology}, pages = {9--18}, year = {2024}, isbn = {9798400711534}, doi = {10.1145/3681777.3698469}, url = {https://doi.org/10.1145/3681777.3698469}, author = {Gupta, Vandana and Gokhale, Swapna}, keywords = {chronic health outcomes, social vulnerability index, social deprivation index, machine learning, random forests, geography, racial minority, location = Atlanta, GA, USA}, abstract = {Chronic health outcomes require ongoing medical attention and have a significant impact on a person's quality of life. It is widely accepted that social determinants of health (SDoH) shape the onset and management of chronic health outcomes. Among the many composite indices that assess SDoH, there is no consensus on which index best explains these associations between health outcomes and social determinants. Furthermore, chronic outcomes may be modulated by place or geography both through cultural, social, and political forces and spatial correlations. The novelty of this paper lies in building a machine learning (ML) methodology to compare the strengths of SDoH indices in explaining the associations between chronic health outcomes and social determinants while adjusting for geography. The methodology is illustrated by studying the relative strengths of the Social Vulnerability Index (SVI) and Social Deprivation Index (SDI) in explaining age-adjusted prevalence rates of 12 chronic health outcomes obtained from the CDC PLACES project. Results suggest that the SVI is more strongly associated with all 12 chronic health outcomes, however, the increase in the strength of SVI over SDI varies across the health outcomes. For each outcome, importance scores of all SVI measures are then normalized according to its four sub themes, while introducing geography/place as the fifth sub theme. Comparing the relative importance of these five sub themes leads to a grouping of the outcomes into three clusters depending on whether geography/place, racial minority status, or socio-economic measures shows the greatest impact. The emergence of geography as a dominant sub theme alongside conventional social determinants underscores the value of our approach in providing the capability to consider the modulating effect of geography on understanding the relationships between social determinants and health.} }
@inproceedings{10.1145/3650215.3650326, title = {Research on Financial Fraud Identification Model Based on Privacy-Preserving Machine Learning}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {628--632}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650326}, url = {https://doi.org/10.1145/3650215.3650326}, author = {Jiao, Li and Li, Hui}, abstract = {Financial fraud will not only mislead investors and reduce investor confidence, but also disrupt market order and hinder the orderly development of the capital market. In order to solve the problem of financial fraud, based on the data mining process, a process framework of the financial fraud identification method based on machine learning is proposed, which includes three steps: sample and feature selection, data preparation, and model construction and analysis. Based on the process framework of the financial fraud identification method based on machine learning, GBDT methods are used to build models respectively, and the model effect is evaluated based on indicators such as recall rate. Due to the particularity of financial reporting fraud, the model should aim to identify fraud samples as much as possible, so the recall rate is a major evaluation index in this article. The recall rate of the GBDT model is better when based on fewer indicators. Therefore, after comprehensively considering factors such as data acquisition costs, this article recommends using the GBDT model to identify financial fraud.} }
@article{10.1145/3575637.3575644, title = {Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications}, journal = {SIGKDD Explor. Newsl.}, volume = {24}, pages = {32--47}, year = {2022}, issn = {1931-0145}, doi = {10.1145/3575637.3575644}, url = {https://doi.org/10.1145/3575637.3575644}, author = {Fu, Xingbo and Zhang, Binchi and Dong, Yushun and Chen, Chen and Li, Jundong}, abstract = {Graph machine learning has gained great attention in both academia and industry recently. Most of the graph machine learning models, such as Graph Neural Networks (GNNs), are trained over massive graph data. However, in many realworld scenarios, such as hospitalization prediction in healthcare systems, the graph data is usually stored at multiple data owners and cannot be directly accessed by any other parties due to privacy concerns and regulation restrictions. Federated Graph Machine Learning (FGML) is a promising solution to tackle this challenge by training graph machine learning models in a federated manner. In this survey, we conduct a comprehensive review of the literature in FGML. Specifically, we first provide a new taxonomy to divide the existing problems in FGML into two settings, namely, FL with structured data and structured FL. Then, we review the mainstream techniques in each setting and elaborate on how they address the challenges under FGML. In addition, we summarize the real-world applications of FGML from different domains and introduce open graph datasets and platforms adopted in FGML. Finally, we present several limitations in the existing studies with promising research directions in this field.} }
@article{10.1145/3565271, title = {Machine Learning Optimization of Quantum Circuit Layouts}, journal = {ACM Transactions on Quantum Computing}, volume = {4}, year = {2023}, doi = {10.1145/3565271}, url = {https://doi.org/10.1145/3565271}, author = {Paler, Alexandru and Sasu, Lucian and Florea, Adrian-Catalin and Andonie, Razvan}, keywords = {Machine learning, quantum circuits, optimization}, abstract = {The quantum circuit layout (QCL) problem involves mapping out a quantum circuit such that the constraints of the device are satisfied. We introduce a quantum circuit mapping heuristic, QXX, and its machine learning version, QXX-MLP. The latter automatically infers the optimal QXX parameter values such that the laid out circuit has a reduced depth. In order to speed up circuit compilation, before laying the circuits out, we use a Gaussian function to estimate the depth of the compiled circuits. This Gaussian also informs the compiler about the circuit region that influences most the resulting circuit’s depth. We present empiric evidence for the feasibility of learning the layout method using approximation. QXX and QXX-MLP open the path to feasible large-scale QCL methods.} }
@article{10.1145/3730577, title = {Less is More: Feature Engineering for Fairness and Performance of Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3730577}, url = {https://doi.org/10.1145/3730577}, author = {Meng, Linghan and Li, Yanhui and Chen, Lin and Ma, Mingliang and Zhou, Yuming and Xu, Baowen}, keywords = {Fairness, ML Software, Feature Engineering, Performance}, abstract = {Machine learning (ML) software employs statistical algorithms to perform high-stake tasks in our daily lives, whose results are usually discriminatory due to protected features (e.g., gender), i.e., one part (called privileged, e.g., male) may be more likely to obtain beneficial decisions than the other part (called unprivileged, e.g., female). In alleviating the unfairness, developers have obtained widely-held beliefs about the trade-off between performance and fairness for ML software. Surprisingly, recent research on feature engineering suggests that enlarging the feature set is the perfect way to kill two birds with one stone, i.e., achieving both higher performance and fairness. However, the experiments used in the prior study did not remove the effect of protected features, which have been suggested to be excluded in both industrial applications and academic studies. As a result, the study did not fully explore the trade-off between performance and fairness.In this paper, we first conduct an empirical study to replicate this prior study after excluding the protected features and observe that there is still a trade-off between performance and fairness with enlarging the features, i.e., more features are not perfect, which would lead to higher performance and lower fairness. Due to more features causing more collection and preprocessing budgets, we aim to search for an effective alternative. Inspired by the “less is more” principle, we propose a novel feature ranking method, Hybrid-importance and Early-validation based Feature Ranking (HEFR), to find an efficient subset to replace the full feature set with comparable performance and fairness. Our method, HEFR, employs hybrid feature importances to combine performance and fairness and conducts early validation to check the effectiveness of hybrid importances. We conduct experiments on seven datasets and three classifiers to evaluate our method with five baselines. The results have shown that (a) HEFR is efficient for ML software feature engineering: applying HEFR to choose about 10\% of features would construct ML software with better or comparable performance and fairness, and (b) HEFR is actionable with small dataset sizes: applying HEFR with only 10\% data size would still help choose the proper feature subset.} }
@article{10.1145/3654663, title = {Machine Learning in Metaverse Security: Current Solutions and Future Challenges}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3654663}, url = {https://doi.org/10.1145/3654663}, author = {Otoum, Yazan and Gottimukkala, Navya and Kumar, Neeraj and Nayak, Amiya}, keywords = {Metaverse Security, Digital Twin, Machine Learning, Extended Reality, Generative AI, Blockchain}, abstract = {The Metaverse, positioned as the next frontier of the Internet, has the ambition to forge a virtual shared realm characterized by immersion, hyper-spatiotemporal dynamics, and self-sustainability. Recent technological strides in AI, Extended Reality, 6G, and blockchain propel the Metaverse closer to realization, gradually transforming it from science fiction into an imminent reality. Nevertheless, the extensive deployment of the Metaverse faces substantial obstacles, primarily stemming from its potential to infringe on privacy and be susceptible to security breaches, whether inherent in its underlying technologies or arising from the evolving digital landscape. Metaverse security provisioning is poised to confront various foundational challenges owing to its distinctive attributes, encompassing immersive realism, hyper-spatiotemporally, sustainability, and heterogeneity. This article undertakes a comprehensive study of the security and privacy challenges facing the Metaverse, leveraging machine learning models for this purpose. In particular, our focus centers on an innovative distributed Metaverse architecture characterized by interactions across 3D worlds. Subsequently, we conduct a thorough review of the existing cutting-edge measures designed for Metaverse systems while also delving into the discourse surrounding security and privacy threats. As we contemplate the future of Metaverse systems, we outline directions for open research pursuits in this evolving landscape.} }
@inproceedings{10.1145/3640310.3674092, title = {Towards Runtime Monitoring for Responsible Machine Learning using Model-driven Engineering}, booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems}, pages = {195--202}, year = {2024}, isbn = {9798400705045}, doi = {10.1145/3640310.3674092}, url = {https://doi.org/10.1145/3640310.3674092}, author = {Naveed, Hira and Grundy, John and Arora, Chetan and Khalajzadeh, Hourieh and Haggag, Omar}, keywords = {Human-centric requirements, Machine learning components, Model-driven engineering, Responsible ML, Runtime monitoring, location = Linz, Austria}, abstract = {Machine learning (ML) components are used heavily in many current software systems, but developing them responsibly in practice remains challenging. 'Responsible ML' refers to developing, deploying and maintaining ML-based systems that adhere to human-centric requirements, such as fairness, privacy, transparency, safety, accessibility, and human values. Meeting these requirements is essential for maintaining public trust and ensuring the success of ML-based systems. However, as changes are likely in production environments and requirements often evolve, design-time quality assurance practices are insufficient to ensure such systems' responsible behavior. Runtime monitoring approaches for ML-based systems can potentially offer valuable solutions to address this problem. Many currently available ML monitoring solutions overlook human-centric requirements due to a lack of awareness and tool support, the complexity of monitoring human-centric requirements, and the effort required to develop and manage monitors for changing requirements. We believe that many of these challenges can be addressed by model-driven engineering. In this new ideas paper, we present an initial meta-model, model-driven approach, and proof of concept prototype for runtime monitoring of human-centric requirements violations, thereby ensuring responsible ML behavior. We discuss our prototype, current limitations and propose some directions for future work.} }
@inproceedings{10.1145/3628797.3628930, title = {A Machine Learning-Based Anomaly Packets Detection for Smart Home}, booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology}, pages = {816--823}, year = {2023}, isbn = {9798400708916}, doi = {10.1145/3628797.3628930}, url = {https://doi.org/10.1145/3628797.3628930}, author = {Nguyen, Thanh Binh and Nguyen, Duc Dang Khoi and Le Nguyen, Binh Nguyen and Le, Tan}, keywords = {Anomaly Detection, Cybersecurity, IoT-23 Dataset, Machine Learning, Smart Homes, location = Ho Chi Minh, Vietnam}, abstract = {The advent of smart homes has revolutionized residential living, integrating advanced technologies and intelligent devices to create secure, comfortable, and efficient environments. However, this integration of diverse smart devices has brought significant cybersecurity challenges. Detecting and analyzing abnormal network packets have become paramount, signifying potential intrusions, malicious activities, or system errors and ensuring the security and stability of smart home systems. Machine learning techniques, such as Decision Trees, Support Vector Machines (SVM), Convolutional Neural Networks (CNN), K-Nearest Neighbors (KNN), Recurrent Neural Networks (RNN), and Random Forests, have shown promise in addressing these challenges. However, most research has concentrated on anomaly detection rather than malicious activity in smart homes. The vast datasets collected from various scenarios pose methodological and algorithmic challenges for applying machine learning techniques. To fill these research gaps, our study introduces traditional machine-learning methods for detecting abnormal network packets in smart homes using the IoT-23 dataset. It involves preprocessing the dataset, extracting relevant features, and training various machine learning models. The correlation matrix helps validate the feature selection of the best models based on performance metrics like precision, F1-score, recall, accuracy ratio, training score, and training time cost. Additionally, the study classifies 12 types of malicious malware across different machine learning models, considering performance within the context of smart home devices. This study implements real-time anomaly detection on the Raspberry Pi using packet captures and Zeek flowmeter methods. The findings contribute insights into models suitable for smart home security. In addition, our research enhances the understanding and application of machine learning methods for bolstering security in smart homes.} }
@inbook{10.1145/3730436.3730465, title = {Anomaly Detection in Human Activity Logs Using Wearable Inertial Sensors and Machine Learning Techniques}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {176--180}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730465}, author = {Ahmed, Muhammad Raisuddin and Srimal, Woshan and Aseeri, Mohammed A and bin Marhaban, Mohammad Hamiruce and Alabdullah, Ahmed A and Kaiser, M Shamim}, abstract = {The wearable inertial sensors enhanced by real-time monitoring employed to monitor and improve human activities relating to health, safety, and well-being. This paper provides a framework for detecting anomalies in activity data, based on records and collected values from accelerometers and gyroscopes with the help of machine learning. By jointly considering advanced signal processing, feature extraction, and Random Forest classification technique optimized with SMO, it solves acute noise, computational costs, and contextual ambiguity issues. The extracted features, like mean and max values, are capable of capturing static and dynamic patterns quite efficiently. The proposed system has shown good improvements in the detection of anomalies in critical events, such as falls and abnormal target behaviour. The framework increases the prospects of safety and improvement in living standards and could evolve into the mainstream market with reliable monitoring systems.} }
@inproceedings{10.1145/3723936.3723949, title = {Application of machine learning in motion capture and technical movement diagnosis of football players}, booktitle = {Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis}, pages = {79--85}, year = {2025}, isbn = {9798400712234}, doi = {10.1145/3723936.3723949}, url = {https://doi.org/10.1145/3723936.3723949}, author = {Yang, Jie and Hu, Juanjuan}, keywords = {Football motion capture, deep learning model, machine learning, sports data analysis, technical motion diagnosis}, abstract = {This study combines motion capture systems and machine learning methods to explore the construction of high-precision capture and diagnosis models and their applications in football. Motion capture technology relies on optical sensors and inertial sensors to obtain multi-dimensional motion data of athletes. Through data preprocessing and feature extraction, machine learning algorithms classify and analyze movements. This paper designs a technical motion diagnosis model based on deep learning to identify detail deviations in football shooting movements and optimize the training process. Experimental verification shows that the model is superior to traditional methods in terms of data capture accuracy and diagnostic accuracy, and can effectively improve the standardization and expressiveness of football players' movements. The study summarizes the advantages of machine learning in technical motion capture and diagnosis, proposes future improvement directions, and provides intelligent solutions for football training and technical analysis.} }
@inproceedings{10.1145/3703847.3703853, title = {Advancing Disease Diagnosis and Biomarker Discovery: The Role of Machine Learning in ncRNA Analysis}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {29--35}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703853}, url = {https://doi.org/10.1145/3703847.3703853}, author = {Wang, Dan and Jin, Xin and Li, Shanshan}, keywords = {biomarker, diagnosis, machine learning, ncRNAs}, abstract = {Non-coding RNAs (ncRNAs) are critical regulators in diverse biological processes and pathological mechanisms. Their prospective utility as biomarkers for early disease detection and prognostic evaluation has attracted substantial scholarly interest. However, the complexity and volume of ncRNA data pose challenges in biomarker discovery. Recent advancements in machine learning have provided powerful tools to address these challenges, offering novel approaches for the analysis of ncRNA data. This review explores the integration of machine learning techniques in ncRNA research, focusing on miRNA, circRNA, and lncRNA. We discuss the biological functions of these ncRNAs, traditional methods for their analysis, and how machine learning enhances the discovery of disease biomarkers. Through a detailed examination of case studies, we highlight successful applications of machine learning in identifying ncRNA biomarkers. Additionally, we compare the effectiveness of various machine learning methods across different ncRNA types and address current challenges and future directions in the field. Our review underscores the transformative potential of machine learning in advancing ncRNA biomarker discovery, ultimately contributing to improved diagnostic and therapeutic strategies.} }
@article{10.1145/3653319, title = {ForestEyes: Citizen Scientists and Machine Learning-Assisting Rainforest Conservation}, journal = {Commun. ACM}, volume = {67}, pages = {95--96}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3653319}, url = {https://doi.org/10.1145/3653319}, author = {Fazenda, \'Alvaro L. and Faria, Fabio A.} }
@inproceedings{10.1145/3697467.3697644, title = {A System for the Prediction of Fire Pump Failure Based on Internet of Things and Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {216--221}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697644}, url = {https://doi.org/10.1145/3697467.3697644}, author = {Yu, Chenguang and Li, Shengli and Liu, Yanqian}, keywords = {Internet of things, STM32, fire pump failure prediction system, machine learning}, abstract = {In order to facilitate the early detection of potential failure in fire pumps, a fire pump failure prediction system has been designed which incorporates Internet of Things (IoT) and machine learning techniques. The system employs an STM32F103RCT6 microcontroller to acquire data from temperature, pressure and water flow sensors. The data is then transmitted to the Internet of Things (IoT) cloud platform via a Wi-Fi module, allowing for real-time monitoring of the fire pump environment. The data from the IoT cloud platform is also subjected to analysis and learning through the application of machine learning algorithms. Three machine learning algorithms, namely k-nearest neighbour, logistic regression and extreme gradient boosting, were employed for the purposes of modelling, training and prediction. It was determined that the accuracy, Kappa coefficient and AUC value of the XGBoost algorithm were superior to those of the other algorithms, and that it demonstrated an excellent capacity for predicting the occurrence of fire pump failure. The system is capable of meeting the real-time monitoring of fire pumps, as well as fault prediction, and of enhancing the reliability of fire pumps.} }
@inproceedings{10.1145/3662739.3669983, title = {Research on Transient Stability of Power Systems Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications}, pages = {383--391}, year = {2024}, isbn = {9798400718144}, doi = {10.1145/3662739.3669983}, url = {https://doi.org/10.1145/3662739.3669983}, author = {Luan, Jing and Yang, Yawen}, keywords = {Deep learning, Machine learning, Power system, Transient stability, location = Ningbo, China}, abstract = {The safe and stable operation of electric power system is an important foundation and support for the stable development of modern society. With the continuous expansion of the scale of the power system, the access of various new energy, new loads and the wide application of power electronic devices have increased the uncertainty and complexity of the network operation, making it face a severe test. The power system is often disturbed, when a variety of short circuit, broken line or switch without fault trip and other large disturbances, the need to study the transient stability of the system. This paper is based on the machine learning view and the transient stability evaluation process, the deep learning model is used to learn a more abstract representation of the data feature rules. On the basis of constructing the system feature quantity data, the stacking auto encoder and support vector machine algorithm are combined to learn and train the sample set and test the accuracy of the model. Considering the stacking automatic encoder belongs to the most basic deep learning model, further introduce deeper complex convolution neural network as a power system transient stability assessment model, and at the same time the support vector machine algorithm introduced output layer of discriminant mechanism, through the example verified the effectiveness of the two models, improve the performance of the transient stability assessment.} }
@article{10.1145/3660801, title = {MirrorFair: Fixing Fairness Bugs in Machine Learning Software via Counterfactual Predictions}, journal = {Proc. ACM Softw. Eng.}, volume = {1}, year = {2024}, doi = {10.1145/3660801}, url = {https://doi.org/10.1145/3660801}, author = {Xiao, Ying and Zhang, Jie M. and Liu, Yepang and Mousavi, Mohammad Reza and Liu, Sicen and Xue, Dingyuan}, keywords = {Bias Mitigation, Fairness Bugs, Machine Learning, Software Discrimination}, abstract = {With the increasing utilization of Machine Learning (ML) software in critical domains such as employee hiring, college admission, and credit evaluation, ensuring fairness in the decision-making processes of underlying models has emerged as a paramount ethical concern. Nonetheless, existing methods for rectifying fairness issues can hardly strike a consistent trade-off between performance and fairness across diverse tasks and algorithms. Informed by the principles of counterfactual inference, this paper introduces MirrorFair, an innovative adaptive ensemble approach designed to mitigate fairness concerns. MirrorFair initially constructs a counterfactual dataset derived from the original data, training two distinct models—one on the original dataset and the other on the counterfactual dataset. Subsequently, MirrorFair adaptively combines these model predictions to generate fairer final decisions. We conduct an extensive evaluation of MirrorFair and compare it with 15 existing methods across a diverse range of decision-making scenarios. Our findings reveal that MirrorFair outperforms all the baselines in every measurement (i.e., fairness improvement, performance preservation, and trade-off metrics). Specifically, in 93\% of cases, MirrorFair surpasses the fairness and performance trade-off baseline proposed by the benchmarking tool Fairea, whereas the state-of-the-art method achieves this in only 88\% of cases. Furthermore, MirrorFair consistently demonstrates its superiority across various tasks and algorithms, ranking first in balancing model performance and fairness in 83\% of scenarios. To foster replicability and future research, we have made our code, data, and results openly accessible to the research community.} }
@inproceedings{10.1145/3607947.3608032, title = {Sales Analysis and Forecasting using Machine Learning Approach}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {375--377}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608032}, url = {https://doi.org/10.1145/3607947.3608032}, author = {Rajpoot, Dharmveer Singh and Mittal, Bhavey and Dudani, Harshit and Singhal, Ujjwal}, abstract = {In this paper, we examine how machine learning models are used in sales predictive analytics. This paper's main objective is to examine the key methods and case studies of using machine learning to sales forecasting. It has been thought about how machine learning generalization will affect things. When there is only a little quantity of historical data available for a certain sales time series, such as when a new store or product is released, this impact can be utilized to make sales predictions. Regression ensembles of single models have been built using a regressor technique. The findings demonstrate that we may enhance the performance of predictive models for sales forecasting by utilizing linear regression and XG boost regressor.} }
@inproceedings{10.1145/3733155.3734916, title = {Classification of mild cognitive impairment using machine learning with dynamic functional connectivity from resting-state functional MRI}, booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {458--467}, year = {2025}, isbn = {9798400714023}, doi = {10.1145/3733155.3734916}, url = {https://doi.org/10.1145/3733155.3734916}, author = {Minami, Ryosuke and Hatano, Ryo and Nishiyama, Hiroyuki}, keywords = {rs-fMRI, machine learning, MCI, window-based FC analysis}, abstract = {The early diagnosis of mild cognitive impairment (MCI) is crucial for effective treatment. Resting-state functional magnetic resonance imaging (rs-fMRI) combined with machine learning has shown promise for the diagnosis of MCI. However, because rs-fMRI data tend to include substantial noise and the limited amount of available rs-fMRI data especially for MCI, it is important to develop a robust model to counter the effects of noise and data imbalance. Therefore, we propose a preprocessing method and classify preprocessed rs-fMRI data into cognitively normal and MCI groups using a machine learning model. Specifically, during preprocessing, we perform principal component analysis, window-based functional connectivity analysis, and feature selection based on hypothesis testing for differences. The highest classification performance from the fivefold cross-validation was an accuracy of 0.847, recall of 0.670, precision of 0.635, and F1 score of 0.633.} }
@inproceedings{10.1145/3647444.3647916, title = {Exploring Relatonship between music and mood through machine learning technique}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647916}, url = {https://doi.org/10.1145/3647444.3647916}, author = {Gujar, Shital Shankar and Reha, Ali Yawar}, keywords = {Machine Learning, Mood Classification, Music, Neural Network, location = Jaipur, India}, abstract = {The importance of music in our lives is acknowledged, as it can help us feel happy and put a smile on our faces. Studies have shown that music therapy can improve mental health, and the link between mood and music is a crucial area of research. Machine learning is becoming more prevalent in the analysis of large datasets, and it can provide valuable insights. The goal of this study is to analyze the relationship between mood and music by using machine learning techniques. The findings of this research will be used to develop recommendations for individuals seeking therapeutic music. The paper uses machine learning methods to analyze the link between mood and music. It shows how this connection can be utilized to enhance mental health. The study analyzed a large dataset of music tracks and its associated moods. The results of the analysis revealed that certain musical elements, such as key, tempo, and mode, were associated with specific mood. The findings of this study provide valuable insight into the link between music and mood. They can also help develop recommendations for individuals who seek therapeutic music.} }
@inproceedings{10.1145/3555776.3578591, title = {SOTERIA: Preserving Privacy in Distributed Machine Learning}, booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing}, pages = {135--142}, year = {2023}, isbn = {9781450395175}, doi = {10.1145/3555776.3578591}, url = {https://doi.org/10.1145/3555776.3578591}, author = {Brito, Cl\'audia and Ferreira, Pedro and Portela, Bernardo and Oliveira, Rui and Paulo, Jo\~ao}, keywords = {apache spark, machine learning, Intel SGX, privacy-preserving, location = Tallinn, Estonia}, abstract = {We propose Soteria, a system for distributed privacy-preserving Machine Learning (ML) that leverages Trusted Execution Environments (e.g. Intel SGX) to run code in isolated containers (enclaves). Unlike previous work, where all ML-related computation is performed at trusted enclaves, we introduce a hybrid scheme, combining computation done inside and outside these enclaves. The conducted experimental evaluation validates that our approach reduces the runtime of ML algorithms by up to 41\%, when compared to previous related work. Our protocol is accompanied by a security proof, as well as a discussion regarding resilience against a wide spectrum of ML attacks.} }
@inproceedings{10.1145/3688671.3688744, title = {Inverse design of Hexagonal Moir\'e Materials: Machine Learning for tunable pore properties}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688744}, url = {https://doi.org/10.1145/3688671.3688744}, author = {Papia, Efi-Maria and Kondi, Alex and Constantoudis, Vassilios}, keywords = {porous materials, Moir\'e patterns, inverse design, machine learning, classification}, abstract = {Moir\'e patterns, emerging from the overlay of periodic structures, produce distinctive interference effects. Hexagonal Moir\'e patterns formed by superimposing hexagonal lattices with rotational misalignment or varying lattice constants are particularly notable. These patterns have unveiled extraordinary electronic properties in two-dimensional materials like graphene, significantly impacting fields such as quantum computing and nanoelectronics, while also holding promise in materials engineering for filtration purposes. This study employs machine learning to achieve inverse design of hexagonal Moir\'e lattices, predicting lattice configurations for specific pore characteristics. Using computational modeling, datasets of simulated lattices are generated, and pore size distribution data are extracted. Neural networks are trained on these datasets to predict the baseline mesh width while also classifying the number of meshes required for the desired pore size distribution. The results, evaluated through accuracy metrics and ROC curves, are compared to well-known classifiers, highlighting the approach’s potential to revolutionize materials design with tunable properties and advance materials science.} }
@inbook{10.1145/3729706.3729749, title = {State-owned Capital and Green M\&amp;A: A Machine Learning Analysis of Private Firms}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {272--277}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729749}, author = {Xu, Haili}, abstract = {This study examines the impact of state-owned capital participation on private enterprises’ green mergers and acquisitions (M\&amp;A) in China, using the data on green M\&amp;A events of Chinese listed organization between 2010 and 2023. An empirical analysis is conducted to investigate the moderating role of media attention in this relationship. Machine learning model such as Random Forest (RF) and Logistic Regression (LR) are used to validate the results. The results show that state-owned capital participation in private enterprises significantly facilitates green M\&amp;A by improving internal and external supervision and providing resource synergies. RF received (85.7\%) accuracy followed by LR with (83.1\&amp;) in predicting the success of green M\&amp;A success. Results also reveal that the media attention plays a positive and moderating role in the relationship between state-owned capital participation and green M\&amp;A of private enterprises. This study contributes to a deeper understanding of the impact of state-owned capital participation on private enterprises’ green M\&amp;A, empowering high-quality economic development and supporting the achievement of the national “dual carbon” goals.} }
@inproceedings{10.1145/3711896.3736851, title = {Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms and Hyperparameters on Shared Latent Space}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {1025--1036}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3736851}, url = {https://doi.org/10.1145/3711896.3736851}, author = {Ishikawa, Kazuki and Ozaki, Ryota and Kanzaki, Yohei and Takeuchi, Ichiro and Karasuyama, Masayuki}, keywords = {automl, bayesian optimization, gaussian process, location = Toronto ON, Canada}, abstract = {Selecting the optimal combination of a machine learning (ML) algorithm and its hyper-parameters is crucial for the development of high-performance ML systems. However, since the combination of ML algorithms and hyper-parameters is enormous, the exhaustive validation requires a significant amount of time. Many existing studies use Bayesian optimization (BO) for accelerating the search. On the other hand, a significant difficulty is that, in general, there exists a different hyper-parameter space for each one of candidate ML algorithms. BO-based approaches typically build a surrogate model independently for each hyper-parameter space, by which sufficient observations are required for all candidate ML algorithms. In this study, our proposed method embeds different hyper-parameter spaces into a shared latent space, in which a surrogate multi-task model for BO is estimated. This approach can share information of observations from different ML algorithms by which efficient optimization is expected with a smaller number of total observations. We further propose the pre-training of the latent space embedding with an adversarial regularization, and a ranking model for selecting an effective pre-trained embedding for a given target dataset. Our empirical study demonstrates effectiveness of the proposed method through datasets from OpenML.} }
@article{10.1145/3678168, title = {Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3678168}, url = {https://doi.org/10.1145/3678168}, author = {Li, Hao and Rajbahadur, Gopi Krishnan and Bezemer, Cor-Paul}, keywords = {Software engineering for machine learning, software quality, deep learning, binding, TensorFlow, PyTorch}, abstract = {Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework’s functionality using a programming language different from the framework’s default language (usually Python). In this article, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.} }
@article{10.1145/3708497, title = {Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708497}, url = {https://doi.org/10.1145/3708497}, author = {Bayram, Firas and Ahmed, Bestoun S.}, keywords = {Artificial intelligence, machine learning, Trustworthy AI, robustness, MLOps systems, DataOps, ModelOps, model performance}, abstract = {Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.} }
@inproceedings{10.1145/3607888.3608962, title = {Special Session: Machine Learning for Embedded System Design}, booktitle = {Proceedings of the 2023 International Conference on Hardware/Software Codesign and System Synthesis}, pages = {28--37}, year = {2024}, isbn = {9798400702891}, doi = {10.1145/3607888.3608962}, url = {https://doi.org/10.1145/3607888.3608962}, author = {Alcorta Lozano, Erika Susana and Gerstlauer, Andreas and Deng, Chenhui and Sun, Qi and Zhang, Zhiru and Xu, Ceyu and Wills, Lisa Wu and Sanchez Lopera, Daniela and Ecker, Wolfgang and Garg, Siddharth and Hu, Jiang}, keywords = {machine learning, embedded system design, location = Hamburg, Germany}, abstract = {Embedded systems are becoming increasingly complex, which has led to a productivity crisis in their design and verification. Although conventional design automation coupled with IP and platform reuse techniques have led to leaps in design productivity improvement, they face fundamental limits given that most design optimization and verification problems remain NP-hard and that reuse of pre-designed IP blocks and platforms inherently limits flexibility and optimality. At the same time, machine learning (ML) has recently made unprecedented advances and created phenomenal impact in various computing applications. In particular, application of ML techniques as a way to extract knowledge and learn from existing design, optimization and verification data has recently seen a lot of excitement and promise at lower physical and integrated circuit levels of abstraction. Using ML has the potential to similarly close the complexity gap in embedded system design, but corresponding ML-based approaches for embedded system optimization and verification at higher levels of abstraction are still at their infancy.This paper presents the current state of the art, along with opportunities and open challenges, in the application of ML methods for embedded system design and optimization. We discuss design and optimization at different levels of abstraction ranging from system-level modeling and optimization and high-level synthesis to RTL and micro-architecture design, bringing together perspectives from different communities in both academia and industry.} }
@inproceedings{10.1145/3639592.3639617, title = {IoT Based Accident Prevention System using Machine Learning techniques}, booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference}, pages = {179--188}, year = {2024}, isbn = {9798400716225}, doi = {10.1145/3639592.3639617}, url = {https://doi.org/10.1145/3639592.3639617}, author = {Alnashwan, Raghad and Mashaabi, Malak and Alotaibi, Areej and Qudaih, Hala and Albraheem, Lamya}, abstract = {The likelihood of car accidents increases during extreme weather conditions, such as fog, winds, snow, rain, etc. While it may not be possible to prevent all such accidents, their incidence can be reduced by taking proper measures. Therefore, an intelligent accident-avoidance system is necessary to predict the severity of accidents based on weather and road conditions. This research paper suggests three machine learning (ML) methods for an Internet of Things (IoT)-based accident severity prediction system. The methods are Random Forest, LightGBM, and XGBoost.The aim is to predict the severity of car accidents based on various weather features using a machine learning model. However, considering the previous work, we observed that the size of datasets is frequently minimal, and some of the research discusses the influence of the weather on the number of accidents. Therefore, we used the Countrywide Traffic Accident Dataset, which covers 2.8 million vehicle accidents in the United States from 2016 to 2021. In conclusion, our methodology appears to be efficient in predicting the severity of car accidents. Among the three methods, LightGBM achieved the highest prediction accuracy (72\%), precision (70\%), recall (70\%), F1-scores (70\%), and area curve (AUC) (0.86) of the receiver operating characteristic (ROC) curve.} }
@inproceedings{10.1145/3729605.3729694, title = {Research on Personal Credit Risk Assessment Model Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Big Data and Informatization Education}, pages = {512--516}, year = {2025}, isbn = {9798400714405}, doi = {10.1145/3729605.3729694}, url = {https://doi.org/10.1145/3729605.3729694}, author = {Zhang, Jiaoxia}, keywords = {AUC, Credit Evaluation, Logistic, Random Forest, Smote Algorithm}, abstract = {As the personal credit business continues to grow and expand, credit assessment has become a key task in risk management for the entire credit industry. In order to reduce the number of non-performing loan cases and their probability of occurrence, financial institutions need to fully understand the creditworthiness of their customers before lending, assess the quality of their customers, and reduce non-performing loans by reducing the potential risk borne by the financial institutions. In this paper, based on default data, the SMOTE algorithm was first applied to solve the problem of unbalanced classification samples, and then logistic regression, random forest and SVM algorithms were selected to build the credit evaluation model, and the logistic regression model was selected as the optimal model using the AUC value as the judgment criterion.} }
@inproceedings{10.1145/3696271.3696285, title = {Machine Learning-Driven Optimization of Livestock Management: Classification of Cattle Behaviors for Enhanced Monitoring Efficiency}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {85--91}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696285}, url = {https://doi.org/10.1145/3696271.3696285}, author = {Zhao, Zhuqing and Shehada, Halah and Ha, Dong and Dos Reis, Barbara and White, Robin and Shin, Sook}, keywords = {HGBDT, RF, RFE, SVM}, abstract = {Monitoring cattle health in remote and expansive pastures poses significant challenges that necessitate automated, continuous, and real-time behavior monitoring. This paper investigates the effectiveness and reliability sensor-based cattle behavior classification for such monitoring, emphasizing the impact of intelligent feature selection in enhancing classification performance. To achieve this, we developed Wireless Sensor Nodes (WSN) affixed to individual cattle, enabling the capture of 3-axis acceleration data from five cows across varying seasons, spanning from summer to winter. Initially, we extracted a comprehensive set of 52 features, representing a broad spectrum of cow behaviors alongside statistical attributes. To enhance computational efficiency, we employed the Recursive Feature Elimination (RFE) method to distill 30 critical features by discarding redundant or less significant ones. Subsequently, these optimized features were utilized to train four machine learning (ML) models: Support Vector Machine (SVM), k-Nearest Neighbors (k-NN), Random Forest (RF), and Histogram-based Gradient Boosted Decision Trees (HGBDT). Notably, the HGBDT model demonstrated superior performance, achieving remarkable F1-scores of 99.01\% for 'grazing', 98.74\% for 'ruminating', 89.62\% for 'lying', 84.06\% for 'standing', and 91.87\% for 'walking'. These findings underscore the potential of our approach to serve as a robust framework for precision livestock farming, offering valuable insights into enhancing cattle health monitoring in remote environments.} }
@inproceedings{10.1145/3639233.3639250, title = {Explainable Machine Learning Models for Swahili News Classification}, booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval}, pages = {12--18}, year = {2024}, isbn = {9798400709227}, doi = {10.1145/3639233.3639250}, url = {https://doi.org/10.1145/3639233.3639250}, author = {Murindanyi, Sudi and Yiiki, Brian Afedra and Katumba, Andrew and Nakatumba-Nabende, Joyce}, keywords = {Explainability., Interpretability, location = Seoul, Republic of Korea}, abstract = {Although Swahili is considered a well-resourced language, challenges persist in utilizing it for Natural Language Processing (NLP) tasks, primarily due to the limited data availability required for these systems. For instance, obtaining sufficient Swahili news data for classification remains a significant obstacle. This paper addresses the problem of accurate Swahili news classification by leveraging classical machine learning (ML) models and deep neural networks (DNN). Our proposed method involves data acquisition, Exploratory Data Analysis (EDA), and employing modelling techniques using classical ML models, such as Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes, Random Forest, Gradient Boosting, Hard Voting, and Bagging, as well as DNN models including Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (Bi-LSTM), and CNN-Bi-LSTM + Attention. The models were evaluated using Area Under the Curve (AUC) metrics and accuracy. Our results demonstrate commendable performance for classical ML classifiers and DNN models, with accuracies above 75\%. Notably, the CNN-Bi-LSTM + Attention model achieved an impressive AUC score of 97\%. Additionally, explainability using LIME (Local Interpretable Model-agnostic Explanations) provided valuable insights into model decisions. This research contributes to Swahili natural language processing and lays the foundation for further explorations into transformer-based models for improved classification.} }
@inproceedings{10.1145/3626246.3654680, title = {Applications and Computation of the Shapley Value in Databases and Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {630--635}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654680}, url = {https://doi.org/10.1145/3626246.3654680}, author = {Luo, Xuan and Pei, Jian}, keywords = {Shapley value, cooperative game theory, data market, databases, machine learning, location = Santiago AA, Chile}, abstract = {Recently, the Shapley value, a concept rooted in cooperative game theory, has found more and more applications in databases and machine learning. Due to its combinatoric nature, the computation of the Shapley value is #P-hard. To address this challenge, numerous studies are actively engaged in developing efficient computation methods or exploring alternative solutions in specific application contexts. Applications of the Shapley value in databases and machine learning as well as fast computation or approximation of the Shapley value in those applications are becoming a new research frontier in the database community. This tutorial presents a comprehensive and systematic overview of Shapley value applications and computation within both database and machine learning domains. We survey the existing methods from a unique perspective that diverges from the current literature. Unlike most reviews, which mainly focus on applications, our approach focuses on the underlying algorithmic mechanisms and application specific assumptions in these methods. This approach allows us to highlight the similarities and differences among the various Shapley value applications and computation techniques more effectively. Our tutorial categorizes these methods based on their intrinsic processes, cutting across different applications. The tutorial begins with an introduction to the Shapley value and its diverse applications in databases and machine learning. Subsequently, it delves into the computational challenges of the Shapley value, presents cutting-edge solutions for its efficient computation, and explores alternative solutions.} }
@inproceedings{10.1145/3627673.3680021, title = {GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {4514--4521}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3680021}, url = {https://doi.org/10.1145/3627673.3680021}, author = {Gupta, Vipul and Chen, Xin and Huang, Ruoyun and Meng, Fanlong and Chen, Jianjun and Yan, Yujun}, keywords = {billion-node graphs, distributed graph learning, node embedding, location = Boise, ID, USA}, abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools for supervised machine learning over graph-structured data, while sampling-based node representation learning is widely utilized in unsupervised learning. However, scalability remains a major challenge in both supervised and unsupervised learning for large graphs (e.g., those with over 1 billion nodes). The scalability bottleneck largely stems from the mini-batch sampling phase in GNNs and the random walk sampling phase in unsupervised methods. These processes often require storing features or embeddings in memory. In the context of distributed training, they require frequent, inefficient random access to data stored across different workers. Such repeated inter-worker communication for each mini-batch leads to high communication overhead and computational inefficiency.We propose GraphScale, a unified framework for both supervised and unsupervised learning to store and process large graph data distributedly. The key insight in our design is the separation of workers who store data and those who perform the training. This separation allows us to decouple computing and storage in graph training, thus effectively building a pipeline where data fetching and data computation can overlap asynchronously. Our experiments show that GraphScale outperforms state-of-the-art methods for distributed training of both GNNs and node embeddings. We evaluate GraphScale both on public and proprietary graph datasets and observe a reduction of at least 40\% in end-to-end training times compared to popular distributed frameworks, without any loss in performance. While most existing methods don't support billion-node graphs for training node embeddings, GraphScale is currently deployed in production at TikTok enabling efficient learning over such large graphs.} }
@inproceedings{10.1145/3747227.3747266, title = {A Study on Optimizing the Integration of Police Work into Community Security Governance in Inner Mongolia with Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {241--247}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747266}, url = {https://doi.org/10.1145/3747227.3747266}, author = {Zhu, Guangqin and Zhao, Shuhui}, keywords = {community policing, community security governance, machine learning algorithms, multi-center governance, occupational burnout}, abstract = {Inner Mongolia police have adopted new models like "police grids + community grids" and "community police + grid members + N" to integrate into community governance, achieving certain results. However, issues such as insufficient community police force allocation, inadequate resources for community police work, and a shortage of participating entities in multi - center governance still exist. This study constructs a "data - algorithm - application" closed - loop system using machine learning algorithms, specifically the K-Means clustering algorithm. By preprocessing data, training and verifying the model, and analyzing various relationships like those between community security satisfaction and participation activities, and community policing input and performance, the research aims to optimize the integration of police and civilians in community security governance. The system was piloted in communities in Inner Mongolia to verify the model's effectiveness, shorten the response time to security incidents, and form a replicable "Inner Mongolia smart policing" framework for border areas across the country.} }
@inproceedings{10.1145/3607947.3608045, title = {Review of Machine Learning Techniques for Crop Recommendation}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {443--449}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608045}, url = {https://doi.org/10.1145/3607947.3608045}, author = {Kansal, Liza and Pandey, Anoushka and Shukla, Sanidhya Madhav and Dhaliwal, Parneeta}, keywords = {Agriculture, Crop Prediction, Machine Learning, Predictive Analytics, location = Noida, India}, abstract = {Agriculture is an important sector in India, and about 58\% of the Indian population depends on it. This is why it is paramount it remains profitable and provides a high yield. One of the problems that lead to reduced productivity is the selection of the wrong crop. For maximum productivity, every crop needs specific environmental conditions like soil quality, water, etc. In our work, we have used various Machine learning techniques and based on their comparative analysis adopted the best model to predict the most suitable crop for a particular soil sample based on parameters like Nitrogen, Potassium, Phosphorus, ph. level, rainfall, temperature, and humidity. The dataset is pre-processed and optimized using pre-processing techniques. We have reviewed existing algorithms such as Decision Trees, Naive Bayes, Support Vector Machine (SVM), K Nearest Neighbor (KNN), and Random Forest to predict the most suitable crop and found Naive Bayes Classifier to be the best model, based on performance metrics of precision, recall, accuracy and F1 score.} }
@inproceedings{10.1145/3712256.3726461, title = {Rule-based Machine Learning: Separating Rule and Rule-Set Pareto-Optimization for Interpretable Noise-Agnostic Modeling}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {407--415}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3726461}, url = {https://doi.org/10.1145/3712256.3726461}, author = {Lipschutz-Villa, Gabriel and Bandhey, Harsh and Yin, Ruonan and Kamoun, Malek and Urbanowicz, Ryan}, keywords = {rule-based machine learning, learning classifier systems, supervised learning, interpretability, multi-objective optimization, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Rule-based machine learning (RBML) algorithms, e.g. learning classifier systems (LCSs), can capture complex relationships while yielding more interpretable models than most other machine learning algorithms. Traditional LCSs rely on a single fitness function for both rule and/or rule-set optimization. However, ideal rule vs. rule-set discovery often requires distinct and multiple objectives. Recently, hybrid-LCSs were proposed that explicitly separated the task of rule vs. rule-set discovery but relied on distinct single-objective or weighted multi-objective fitness functions. This study introduces a newly developed Heuristic Evolutionary Rule Optimization System (HEROS) that combines previous LCS innovations aimed at tackling noisy, larger-scale, classification tasks, while adopting separation of rule vs. rule-set evolution. Uniquely, HEROS employs a custom Pareto-front-based multi-objective fitness function (for rule discovery) and NSGA-II-style multi-objective optimization (for rule-set discovery) to solve both clean and noisy-signal classification problems agnostically. Rule discovery is driven by rule-accuracy and instance coverage objectives, while rule-set discovery is driven by prediction accuracy and rule-set size objectives. Using diverse simulated benchmark datasets, i.e. noisy (GAMETES) and clean (MUX), we demonstrate proof-of-principle that HEROS can directly discover accurate, highly-compact, interpretable, and ideal solutions when compared to the established 'ExSTraCS' RBML algorithm, without objective weightings or adjusting hyperparameters.} }
@inproceedings{10.1145/3587716.3587731, title = {Baileys: An Efficient Distributed Machine Learning Framework by Dynamic Grouping}, booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing}, pages = {92--96}, year = {2023}, isbn = {9781450398411}, doi = {10.1145/3587716.3587731}, url = {https://doi.org/10.1145/3587716.3587731}, author = {Ni, Chengdong and Du, Haizhou}, keywords = {Distributed machine learning, Dynamic grouping, Multi-server architecture, System heterogeneity, location = Zhuhai, China}, abstract = {Many machine-learning applications rely on distributed machine learning (DML) systems to train models from massive datasets using massive computing resources (e.g., GPUs and TPUs). However, given a DML system in most applications, its parameter synchronization mechanism is fixed and independent from the types and amounts of resources available to the system. In this paper, we argue that given an application, the synchronization mechanism in its DML system should be co-designed with the available resources in the heterogeneous cluster. To this end, we design an efficient parameter synchronization framework called Baileys. First, Baileys retrieves resource information from the heterogeneous clusters and uses such information to partition heterogeneous workers into multiple groups dynamically. Second, Baileys develops efficient group-based parameter synchronization mechanisms to converge model parameters quickly and accurately. We implement a prototype of Baileys and demonstrate its efficiency and efficacy through experiments. Results show that Baileys can reduce block time up to 33.2\% and decrease the test error up to 39.7\%.} }
@inproceedings{10.1145/3655038.3665943, title = {Rethinking Erasure-Coding Libraries in the Age of Optimized Machine Learning}, booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems}, pages = {23--30}, year = {2024}, isbn = {9798400706301}, doi = {10.1145/3655038.3665943}, url = {https://doi.org/10.1145/3655038.3665943}, author = {Hu, Jiyu and Kosaian, Jack and Rashmi, K. V.}, keywords = {erasure coding, machine learning, redundancy, location = Santa Clara, CA, USA}, abstract = {Erasure codes are critical tools for building fault-tolerant and resource-efficient storage systems. However developing and maintaining optimized erasure-coding libraries are challenging. We make the case that the growth of fast machine-learning (ML) libraries may serve as a lifeboat for easing the development of current and future optimized erasure-coding libraries: fast erasure-coding libraries for various hardware platforms can be easily implemented by using existing optimized ML libraries. We show that the computation structure of many erasure codes mirrors that common to matrix multiplication, which is heavily optimized in ML libraries. Due to this similarity, one can implement erasure codes using ML libraries in few lines of code and with little knowledge of erasure codes, while immediately adopting the many optimizations within these libraries, without requiring expertise in high-performance programming. We develop prototypes of our proposed approach using an existing ML library. Our prototypes are up to 1.75 faster than state-of-the-art custom erasure-coding libraries.} }
@inproceedings{10.5555/3635637.3662903, title = {Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework}, booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems}, pages = {525--533}, year = {2024}, isbn = {9798400704864}, author = {Esmaeili, Ahmad and Ghorrati, Zahra and Matson, Eric T.}, keywords = {collaborative learning, distributed learning, edge computing, holonic learning, location = Auckland, New Zealand}, abstract = {Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements HoloAvg, a special variant of HoL that employs weighted averaging for model aggregation across all holons. The convergence of the proposed method is validated through experiments on both identically and independently distributed (IID) and Non-IID settings of the standard MNIST dataset. Furthermore, the performance behaviors of HoL are investigated under various holarchical designs and data distribution scenarios. The presented results affirm HoL's prowess in delivering competitive performance particularly, in the context of the Non-IID data distribution.} }
@inproceedings{10.1145/3595360.3595855, title = {Transactional Python for Durable Machine Learning: Vision, Challenges, and Feasibility}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595855}, url = {https://doi.org/10.1145/3595360.3595855}, author = {Chockchowwat, Supawit and Li, Zhaoheng and Park, Yongjoo}, abstract = {In machine learning (ML), Python serves as a convenient abstraction for working with key libraries such as PyTorch, scikit-learn, and others. Unlike DBMS, however, Python applications may lose important data, such as trained models and extracted features, due to machine failures or human errors, leading to a waste of time and resources. Specifically, they lack four essential properties that could make ML more reliable and user-friendly---durability, atomicity, replicability, and time-versioning (DART).This paper presents our vision of Transactional Python that provides DART without any code modifications to user programs or the Python kernel, by non-intrusively monitoring application states at the object level and determining a minimal amount of information sufficient to reconstruct a whole application. Our evaluation of a proof-of-concept implementation with public PyTorch and scikit-learn applications shows that DART can be offered with overheads ranging 1.5\%--15.6\%.} }
@inproceedings{10.1145/3715931.3715955, title = {Classification of Depression and Anxiety with Machine Learning Applying Random Forest Models}, booktitle = {Proceedings of the 2024 5th International Conference on Intelligent Medicine and Health}, pages = {128--132}, year = {2025}, isbn = {9798400709616}, doi = {10.1145/3715931.3715955}, url = {https://doi.org/10.1145/3715931.3715955}, author = {Alvarez Espezua, Camila Britany and Cruz de la Cruz, Jose Emmanuel and Apaza Davila, Fabiana Alexandra and Cruz de la Cruz, Trinidad Dorotea and Huaquipaco Encinas, Saul and Mamani Machaca, Wilson Antony}, keywords = {Anxiety, Classification, Depression, Random Forest}, abstract = {Anxiety and depression have a strong correlation; although they often differ in their symptoms, both disorders frequently coexist in individuals, leading to a significant impairment of their daily functioning. Anxiety is among the most common conditions that, over time, can develop into a depressive disorder, or vice versa. The simultaneous presence of these disorders contributes to a range of symptoms, including mood disturbances, anhedonia, sleep problems, persistent fear, suicidal thoughts or ideation, distress, irritability, and constant worry, which result in substantial suffering for affected individuals. Recognizing this complex interplay, the present research applied Machine Learning models, particularly Random Forest, which is renowned for its robustness in classification tasks and its ability to identify both linear and non-linear relationships. The study involved the use of 17 predictor variables to train two models, each focused on predicting different outcomes: “Depression Severity” and “Anxiety Severity.” By leveraging the strengths of Random Forest, the research achieved highly accurate classifications. Specifically, the model for “Depression Severity” attained an accuracy of 99.35\% and an F1 Score of 99.34\%, while the model for “Anxiety Severity” reached an accuracy of 98.04\% and an F1 Score of 98.05\%. These results underscore the effectiveness and precision of the Random Forest algorithm in accurately identifying the severity of these mental health conditions.} }
@inproceedings{10.1145/3426826.3426837, title = {Machine Learning in Tourism}, booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence}, pages = {53--57}, year = {2020}, isbn = {9781450388344}, doi = {10.1145/3426826.3426837}, url = {https://doi.org/10.1145/3426826.3426837}, author = {Afsahhosseini, Fatemehalsadat and Al-Mulla, Yaseen}, keywords = {Demand Forecasting, Machine Learning, Recommender System, Sentiment Analysis, Tourism, location = Hangzhou, China}, abstract = {Machine Learning is a subset of Artificial Intelligence, which is a process of learning from different types of data to make accurate predictions. Data in tourism is various such as Statistics, Photos, Maps, and Texts. Also, each tourism cycle has different stages: Pre, During, and After Trip. In this paper application of machine learning in tourism related data and trip stages are introduced in detailed.} }
@inbook{10.1145/3745238.3745498, title = {Analysis and Prediction of Cycling Behavior's Effect on Sleep Quality by Means of Machine Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1655--1668}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745498}, author = {Huang, Jinqiang}, abstract = {Sleep quality is quite important for health nowadays, influencing how the body works and how it feels mentally. Physical activity can be an alternative without medication to make sleep better. But various kinds of exercise have unique effects on how sleep is helped. Cycling is common as moderate aerobic activity having special body and mind effects giving a big influence on sleep. We examined how cycling frequency, intensity, and duration affect sleep duration, trouble falling asleep, and how content one feels using surveys mixed with machine learning, developing many prediction models. Results are positive for cycling improving sleep factors, with random forest model being best for predicting cycling's sleep effect. Strong data for health and policy is provided by this study, showing why cycling matters for healthy life now.} }
@inproceedings{10.1145/3745238.3745386, title = {A-Share Quantitative Investment Strategy for Multiple Objectives Driven by Machine Learning and Deep Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {945--950}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745386}, url = {https://doi.org/10.1145/3745238.3745386}, author = {Li, Xiang and Hong, Ruixin}, keywords = {Investment portfolio, Machine learning, Quantization method}, abstract = {This article uses the real market data of A-shares, selects price data, financial data and technical indicators, uses a variety of machine learning models to predict future returns, and updates the model monthly. At the same time, in order to consider different economic goals, a yield maximization strategy is set, and a risk-return equilibrium strategy based on the modified Markowitz model is set. It attempts to use stocks in the stock pool to track the market index, control risks, and set adjustable mean-variance parameters. This article aims to use advanced computer technology and quantitative methods to provide more reference value for investment groups with different needs and risk preferences.} }
@inproceedings{10.1145/3555041.3589682, title = {Proactively Screening Machine Learning Pipelines with ARGUSEYES}, booktitle = {Companion of the 2023 International Conference on Management of Data}, pages = {91--94}, year = {2023}, isbn = {9781450395076}, doi = {10.1145/3555041.3589682}, url = {https://doi.org/10.1145/3555041.3589682}, author = {Schelter, Sebastian and Grafberger, Stefan and Guha, Shubha and Karlas, Bojan and Zhang, Ce}, keywords = {data validation, machine learning pipelines, provenance tracking, location = Seattle, WA, USA}, abstract = {Software systems that learn from data with machine learning (ML) are ubiquitous. ML pipelines in these applications often suffer from a variety of data-related issues, such as data leakage, label errors or fairness violations, which require reasoning about complex dependencies between their inputs and outputs. These issues are usually only detected in hindsight after deployment, after they caused harm in production. We demonstrate ArgusEyes, a system which enables data scientists to proactively screen their ML pipelines for data-related issues as part of continuous integration. ArgusEyes instruments, executes and screens ML pipelines for declaratively specified pipeline issues, and analyzes data artifacts and their provenance to catch potential problems early before deployment to production. We demonstrate our system for three scenarios: detecting mislabeled images in a computer vision pipeline, spotting data leakage in a price prediction pipeline, and addressing fairness violations in a credit scoring pipeline.} }
@inproceedings{10.1145/3747357.3747382, title = {A method combining finite element simulation and machine learning for performing lower extremity firearm injury diagnosis}, booktitle = {Proceedings of the 2025 International Symposium on Bioinformatics and Computational Biology}, pages = {156--162}, year = {2025}, isbn = {9798400714368}, doi = {10.1145/3747357.3747382}, url = {https://doi.org/10.1145/3747357.3747382}, author = {Mao, Yucheng and Zhang, Linxuan}, keywords = {FEA, Firearm Injury, Lower Limbs, Machine Learning, Wound Ballistics}, abstract = {As global conflicts escalate, addressing potential human injuries on the battlefield becomes increasingly critical. However, ethical considerations preclude the use of human experimentation in injury research. Fortunately, advancements in computer simulation, particularly through numerical methods and machine learning algorithms, offer promising avenues for understanding mechanisms of injury without resorting to unethical practices. In this study, we explore the integration of the finite element analysis method with machine learning algorithms to expedite the diagnosis of human lower extremity firearm injuries. Initially,we will establish a finite element model of the human lower limb bones based on CT images and validated material properties of the fibula and tibia. Subsequently, we will simulate various combat scenarios by altering different loading conditions. By scripting parameterized simulations, we will generate a large dataset of simulation results. Upon obtaining finite element results for each simulated case, we employ supervised machine learning algorithms to classify wound characteristics, including entrance velocity, projectile type, and angle of incidence. Our findings demonstrate high diagnostic accuracies, with k nearest neighbor (KNN), multilayer perceptron(MLP) ,support vector machine (SVM) and random forest classifiers achieving accuracies of 85.5\%, 86.5\%, 86.5\% and 86\%, respectively.} }
@inproceedings{10.1145/3603166.3632535, title = {Machine Learning Inference on Serverless Platforms Using Model Decomposition}, booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing}, year = {2024}, isbn = {9798400702341}, doi = {10.1145/3603166.3632535}, url = {https://doi.org/10.1145/3603166.3632535}, author = {Gallego, Adrien and Odyurt, Uraz and Cheng, Yi and Wang, Yuandou and Zhao, Zhiming}, keywords = {serverless computing, machine learning, model decomposition, inference, location = Taormina (Messina), Italy}, abstract = {Serverless offers a scalable and cost-effective service model for users to run applications without focusing on underlying infrastructure or physical servers. While the Serverless architecture is not designed to address the unique challenges posed by resource-intensive workloads, e.g., Machine Learning (ML) tasks, it is highly scalable. Due to the limitations of Serverless function deployment and resource provisioning, the combination of ML and Serverless is a complex undertaking. We tackle this problem through decomposition of large ML models into smaller sub-models, referred to as slices. We set up ML inference tasks using these slices as a Serverless workflow, i.e., sequence of functions. Our experimental evaluations are performed on the Serverless offering by AWS for demonstration purposes, considering an open-source format for ML model representation, Open Neural Network Exchange. Achieved results portray that our decomposition method enables the execution of ML inference tasks on Serverless, regardless of the model size, benefiting from the high scalability of this architecture while lowering the strain on computing resources, such as required run-time memory.} }
@inproceedings{10.1145/3708360.3708389, title = {A machine learning based Anti-fraud approach for life insurance company : A Case Study in a life insurance company}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {179--185}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708389}, url = {https://doi.org/10.1145/3708360.3708389}, author = {Jiang, Yuwen and Feng, Jiangang and Jiang, Chengxuan}, keywords = {Life Insurance, Logistic Regression, Machine Learning, Policy Management}, abstract = {In China, when life insurance companies tried to develop agent channels, they often applied aggressive incentive policies to encourage agents to sell more products and enlarge sales team. Agents can easily utilize improper incentive policies to forge false insurance policies and defraud the company of incentive fees. This article proposed a “false policy” detection approach based on machine learning algorithm, which can effectively detect the suspected arbitrage policies of agents by combining the information of policyholder and insured, agent information and sales department information of newly underwritten policies. Firstly, the data of policy holders, policy features, agents and business unit were integrates as an analytical flat table, than Logistic Regression was applied the predict the probability of being a “false policy”. This approach has been applied to one domestic life insurance company, it has showed perfect performance and saved considerable losses for the company.} }
@inproceedings{10.1145/3732801.3732805, title = {Design and Application of a University Student Behavior Analysis Model Based on Machine Learning Technology}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {17--20}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732805}, url = {https://doi.org/10.1145/3732801.3732805}, author = {Li, Xiangyun}, keywords = {Learning Outcome Prediction, Machine Learning, Personalized Learning Path, Student Behavior Analysis}, abstract = {With the acceleration of the process of information technology in education, the use of machine learning methods for the study of student learning behaviour has increasingly become an effective method. On this basis, this paper proposes a method for analysing college students' behaviour based on machine learning. On this basis, this paper proposes an approach based on the combination of mathematical modelling and mathematical modelling. This project intends to accurately identify students‘ learning patterns and needs through deep mining of multiple sources of information such as students’ online behaviour, homework, classroom behaviour, etc., so as to achieve personalized education and precise intervention.} }
@inproceedings{10.1145/3637528.3671603, title = {GraphStorm: All-in-one Graph Machine Learning Framework for Industry Applications}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6356--6367}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671603}, url = {https://doi.org/10.1145/3637528.3671603}, author = {Zheng, Da and Song, Xiang and Zhu, Qi and Zhang, Jian and Vasiloudis, Theodore and Ma, Runjie and Zhang, Houyu and Wang, Zichen and Adeshina, Soji and Nisa, Israt and Mottini, Alejandro and Cui, Qingjun and Rangwala, Huzefa and Zeng, Belinda and Faloutsos, Christos and Karypis, George}, keywords = {graph machine learning, industry scale, location = Barcelona, Spain}, abstract = {Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a \&lt;u\&gt;dozen\&lt;/u\&gt; \&lt;u\&gt;billion-scale\&lt;/u\&gt; industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.} }
@proceedings{10.1145/3698263, title = {MLPR '24: Proceedings of the 2024 2nd International Conference on Machine Learning and Pattern Recognition}, year = {2024}, isbn = {9798400710001} }
@inproceedings{10.1145/3616855.3635725, title = {The 5th International Workshop on Machine Learning on Graphs (MLoG)}, booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining}, pages = {1210--1211}, year = {2024}, isbn = {9798400703713}, doi = {10.1145/3616855.3635725}, url = {https://doi.org/10.1145/3616855.3635725}, author = {Derr, Tyler and Ma, Yao and Ding, Kaize and Zhao, Tong and Ahmed, Nesreen K.}, abstract = {Graphs, which encode pairwise relations between entities, are a kind of universal data structure for a lot of real-world data, including social networks, transportation networks, and chemical molecules. Many important applications on these data can be treated as computational tasks on graphs. Recently, machine learning techniques are widely developed and utilized to effectively tame graphs for discovering actionable patterns and harnessing them for advancing various graph-related computational tasks. Huge success has been achieved and numerous real-world applications have benefited from it. However, since in today's world, we are generating and gathering data in a much faster and more diverse way, real-world graphs are becoming increasingly large-scale and complex. More dedicated efforts are needed to propose more advanced machine learning techniques and properly deploy them for real-world applications in a scalable way. Thus, we organize The 5th International Workshop on Machine Learning on Graphs (MLoG) (https://mlog-workshop.github.io/wsdm24.html), held in conjunction with the 17th ACM Conference on Web Search and Data Mining (WSDM), which provides a venue to gather academia researchers and industry researchers/practitioners to present the recent progress on machine learning on graphs.} }
@inproceedings{10.1145/3709026.3709102, title = {Poor Posture Detection Based on Behavioral and Environmental Data Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence}, pages = {1--7}, year = {2025}, isbn = {9798400718182}, doi = {10.1145/3709026.3709102}, url = {https://doi.org/10.1145/3709026.3709102}, author = {Gu, Feng and Li, Jianwei and Deng, Shunrong and Yang, Qin}, keywords = {Behavioral and environmental factors, Poor posture, Prevention and intervention, Random forests}, abstract = {Poor spinal posture is a common problem among primary school students, which may lead to long-term health issues. It is essential to detect poor posture and identify the key behavioral and environmental factors for its preventions and intervention strategies, such as the sitting time, the backpack weight, the physical activities, and the sleep habits. We develop a machine learning method using random forests to predict the incorrect posture occurrences based on the collected behavioral and environmental data of primary school students and identify important influential factors. The data collected from 782 primary school students include six categories of behavioral factors and five categories of environmental factors. Experimental results show that the prediction accuracy reaches 75.16\%, and some of the most important factors for poor posture include carrying backpacks on two shoulders, the mattress firmness, backpack weights, and the excessive use of the electronic devices, which are able to facilitate the early detection and prevention of scoliosis for school children and provide guidelines and insights for families and schools to develop intervention strategies, such as using the right backpack, cutting down on the screen time, increasing physical activities, and making the sleep surface more supportive.} }
@article{10.1145/3715154, title = {Leveraging Incremental Machine Learning for Reconfigurable Systems Modeling under Dynamic Workloads}, journal = {ACM Trans. Reconfigurable Technol. Syst.}, volume = {18}, year = {2025}, issn = {1936-7406}, doi = {10.1145/3715154}, url = {https://doi.org/10.1145/3715154}, author = {Encinas, Juan and Rodr\'guez, Alfonso and Otero, Andr\'es}, keywords = {Multi-Accelerator Systems, Reconfigurable Computing, Dynamic Workloads, Incremental Machine Learning, System Modeling}, abstract = {Dynamic workload orchestration is one of the main concerns when working with heterogeneous computing infrastructures in the edge-cloud continuum. In this context, FPGA-based computing nodes can take advantage of their improved flexibility, performance, and energy efficiency provided that they use proper resource management strategies. In this regard, many state-of-the-art systems rely on proactive power management techniques and task scheduling decisions, which in turn require deep knowledge about the applications to be accelerated and the actual response of the target reconfigurable fabrics when executing them. While acquiring this knowledge at design time was more or less feasible in the past, with applications mostly being static task graphs that did not change at run time, the highly dynamic nature of current workloads in the edge-cloud continuum, where tasks can be deployed on any node and at any time, has removed this possibility. As a result, being able to derive such information at run time to make informed decisions has become a must. This article presents an infrastructure to build incremental ML models that can be used to obtain run-time power consumption and performance estimations in FPGA-based reconfigurable multi-accelerator systems operating under dynamic workloads. The proposed infrastructure features a novel stop-and-restart resource-aware mechanism to monitor and control the model training and evaluation stages during normal system operation, enabling low-overhead updates in the models to account for either unexpected acceleration requests (i.e., tasks not considered previously by the models) or model drift (e.g., fabric degradation). Experimental results show that the proposed approach induces a maximal additional error of 3.66\% compared to a continuous training alternative. Furthermore, the proposed approach incurs only a 4.49\% execution time overhead, compared to the 20.91\% overhead induced by the continuous training alternative. The proposed modeling strategy enables innovative scheduling approaches in reconfigurable systems. This is exemplified by the conflict-aware scheduler introduced in this work, which achieves up to a 1.35 times speedup in executing the experimental workload. Additionally, the proposed approach demonstrates superior adaptability compared to other methods in the literature, particularly in response to significant changes in workload and to mitigate the effects of model overfitting. The portability of the proposed modeling methodology and monitoring infrastructure is also shown through their application to both Zynq-7000 and Zynq UltraScale+ devices.} }
@inproceedings{10.1145/3640115.3640189, title = {Research on Network Security Technology Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering}, pages = {455--461}, year = {2024}, isbn = {9798400708299}, doi = {10.1145/3640115.3640189}, url = {https://doi.org/10.1145/3640115.3640189}, author = {Han, Fei}, keywords = {Machine learning, Network intrusion, Network security, Random Forest, location = Changde, Hunan, China}, abstract = {In recent years, with the popularization of the Internet, the network has penetrated into all aspects of individuals and society. Internet technology has made people's daily lives more intelligent and convenient, but the accompanying network security issues cannot be ignored. Network vulnerabilities and illegal intrusions can affect internet security, and even affect the construction of national infrastructure, critical defense systems, and even military systems. This article selects 50000 sets of data from the KDD99 cybersecurity dataset provided by the National Security Administration of the United States. Each sample in the dataset includes 41 feature attributes and 1 decision attribute. We constructed a model based on the random forest algorithm, using 30\% of the total sample data as the test set and the remaining 70\% of the data as the training set. Then, 100 sets of data were taken for model training. After the training was completed, the detection model successfully predicted 98 intrusions out of 100 detections, and the final score of the test set was 0.982, basically achieving the effect of intrusion detection. A machine learning algorithm model is provided for detecting network intrusions.} }
@inproceedings{10.1145/3674912.3674950, title = {Localization in cellular network using machine learning and fingerprint methods}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {87--94}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674950}, url = {https://doi.org/10.1145/3674912.3674950}, author = {Pham, Ngoc Hung and Nguyen, Huy Dinh and Nguyen, Dinh Thuan and La, The Vinh and Ta, Hai Tung and Hoang, Van Hiep}, keywords = {Cellular Network, Localization, Location-based services, Received Signal Strength, Signal Fingerprint, location = Ruse, Bulgaria}, abstract = {Location-based services (LBS) are widely used in many applications for daily life. The localization of mobile devices has attracted a great deal of research interest with many different techniques to provide precision and high effective positioning capabilities for users. Global Position System (GPS) has good results of location information and is popularly used in many cases. However, there is lack of some abilities compared with the localization techniques based on cellular network. Cellular network is the most used mobile technology for providing communication between user devices themselves, between user device and network operators, as well as enabling the localization ability of user device using cellular network information. In this study, we proposed a solution for collecting and processing a dataset of cell information that consists of cell identification, cell location, and the received signal strength indicator by using a mobile device, then evaluating the localization methods of the mobile device from this dataset. We conducted experiments for localization methods including Centroid, Weighted centroid, Linear Regression, Support Vector Regression, Multilayer Perceptron, and Fingerprint. We analyze and compare the accuracy of these localization methods from the experimental results. Centroid methods are simple and able to provide acceptable accuracy, machine learning methods provide quite good accuracy comparing with Centroid methods, and Fingerprint method provides the best accuracy among investigated localization methods.} }
@inproceedings{10.1145/3646547.3689656, title = {Poster: Toward Achieving Inter-protocol Friendliness through Estimation of BBR Behaviors using Machine Learning}, booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference}, pages = {739--740}, year = {2024}, isbn = {9798400705922}, doi = {10.1145/3646547.3689656}, url = {https://doi.org/10.1145/3646547.3689656}, author = {Utsumi, Satoshi and Ishikawa, Junya and Ohba, Fuya and Zabir, Salahuddin Muhammad Salim and Atiquzzaman, Mohammed}, keywords = {bbr, congestion control algorithm, machine learning, location = Madrid, Spain}, abstract = {In this paper, we present a new mechanism for estimating the data sending rates of BBR congestion control algorithm using machine learning. Results show that machine learning can estimate the data sending rates of BBR with high accuracy.} }
@inproceedings{10.1145/3626641.3627146, title = {Exploring Machine Learning Techniques for Male Infertility Prediction: A Review}, booktitle = {Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology}, pages = {235--240}, year = {2023}, isbn = {9798400708503}, doi = {10.1145/3626641.3627146}, url = {https://doi.org/10.1145/3626641.3627146}, author = {Shofiyah, Shofiyah and Mahmudy, Wayan Firdaus}, keywords = {Literature Review, Machine Learning, Male Infertility, location = Badung, Bali, Indonesia}, abstract = {Infertility, also known as sterility in both men and women, is a global health problem that affects the quality of life of couples who want to have children. In recent decades, technological developments in medicine and computer science have inspired the exploration of machine learning techniques to support early prediction of male infertility. In this paper, the authors present a comprehensive review of the various machine learning techniques that have been applied to male infertility prediction. The authors begin by outlining the background of male infertility and the complexity of its clinical diagnosis. We then detail the advantages of machine learning techniques in processing and analyzing complex health data, as well as their potential to provide new insights into the causative factors of male infertility. Through in-depth analysis, we identify several machine learning approaches commonly used in the literature, such as regression and classification. We also review a series of recent studies that applied these techniques in diagnosing male infertility. In addition, the authors highlight the challenges faced by researchers in using machine learning for infertility prediction, including the lack of high-quality data and the interpretability of complex models. Nevertheless, many studies have shown positive results in using machine learning techniques to contribute to the development of decision support systems in this field. To support the quality of research, future research that can be done by conducting a comprehensive review of various techniques in deep learning in detecting infertility diseases, especially in men.} }
@inproceedings{10.1145/3603165.3607418, title = {Auxiliary Diagnosing Coronary Stenosis based on Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023}, pages = {98--99}, year = {2023}, isbn = {9798400702334}, doi = {10.1145/3603165.3607418}, url = {https://doi.org/10.1145/3603165.3607418}, author = {Zhu, Weijun and Liu, Yang}, keywords = {Classification, Coronary stenosis, Machine learning, location = Wuhan, China}, abstract = {How to accurately classify and diagnose whether an individual has Coronary Stenosis (CS) without invasive physical examination? This problem has not been solved satisfactorily. To this end, the four Machine Learning (ML) algorithms, i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and Random Forest (RF) are employed in this paper. First, eleven features including basic information of an individual, symptoms and results of routine physical examination are selected, as well as one label is specified, indicating whether an individual suffers from different severity of coronary artery stenosis or not. On the basis of it, a set containing one thousand samples is constructed. Second, each of these four ML algorithms learns from the sample set to obtain the corresponding optimal classified results, respectively. The experimental results show that: RF performs better than other three algorithms, and it classifies whether an individual has CS with an accuracy of 95.7\%.} }
@inproceedings{10.1145/3718751.3718892, title = {Stock Return Prediction Using Hybrid Machine Learning Method Based on Optiver Company Data}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {866--870}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718892}, url = {https://doi.org/10.1145/3718751.3718892}, author = {Yang, Jiahua and Guo, Yutao and Meng, Lingdi and Zhu, Yi and Geng, Weiyi}, keywords = {Hybrid Machine Learning, Quantitative Finance, Stock Return Prediction}, abstract = {In this paper, we present a novel approach for stock return prediction based on the optiver company stock data using a hybrid machine learning method. In this project, we utilize the advanced models like LightGBM, CatBoost, neural networks and we engineered a powerful ensemble technique to improve the quality of prediction and strengthen the accuracy and robustness. We leverage the best of each and complement their shortcomings by developing a hybrid model to improve performance. Extensive experimental results show that the hybrid method we proposed is superior to both the conventional method and all individual methods. More specifically, the hybrid model with a mean absolute error (MAE) of 5.466 is lower than the MAE for the models on their own: XGBoost (MAE: 5.602), support vector machine (MAE: 5.910), LightGBM (MAE: 5.512), CatBoost (MAE: 5.566). These outcomes highlight the successfulness of the Hybrid Method as it accurately predicts market trends by capturing complex relationships within stock return data. These contributions are important for the domain of financial machine learning because they provide a simple yet powerful approach for improving the performance of stock return forecasting based on ensemble learning methods.} }
@inproceedings{10.1145/3660395.3660455, title = {Research on 3D Animation Simulation Based on Machine Learning}, booktitle = {Proceedings of the 2023 3rd Guangdong-Hong Kong-Macao Greater Bay Area Artificial Intelligence and Big Data Forum}, pages = {352--356}, year = {2024}, isbn = {9798400716362}, doi = {10.1145/3660395.3660455}, url = {https://doi.org/10.1145/3660395.3660455}, author = {Huang, Xinran and Li, Panpan and Tan, Yupeng and Zhang, Yining and Deng, Mengyuan}, abstract = {The provided document is a research paper on 3D animation simulation based on machine learning, focusing on the application of machine learning in three-dimensional human animation technology. The paper discusses the importance of virtual reality technology based on computer three-dimensional animation in various fields such as film production, game development, sports simulation, and simulated training. The authors emphasize the significance of three-dimensional human animation technology in portraying subtle changes in human movement and simulating human emotions contained in movement. The paper also explores the application of machine learning methods in processing human body movement data and synthesizing new realistic movement sequences. Various machine learning theories and methods, including regression, function approximation, dimension reduction, classification, clustering, and decision-making, are discussed in relation to their application in three-dimensional human animation technology. The document also reviews the research status of human body movement synthesis technology at home and abroad, highlighting the contributions of various researchers in the field. The authors conclude that machine learning methods have unique advantages in processing human body movement data and can significantly improve the utilization rate of movement capture samples, thereby reducing the cost and time cycle of three-dimensional human animation production..} }
@inproceedings{10.1145/3724154.3724282, title = {Machine Learning-XGBoost Analysis of Re-employment Intention of Full-time Mothers}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {771--774}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724282}, url = {https://doi.org/10.1145/3724154.3724282}, author = {Wu, Junbi and Zhou, Jie}, keywords = {Artificial intelligence algorithm, Re-employment intention, XGBoost}, abstract = {This study focused on the effects of social support from strong and weak relationships on the re-employment intention of full-time mothers after the implementation of the three-child birth policy in China. This study received 358 valid questionnaires, and utilized the XGBoost machine learning algorithm to create model. After conducting an optimal search for the network parameters, we achieved an accuracy of 97.22\% on the test set. Furthermore, XGBoost can effectively rank the importance of independent variables, facilitating timely and effective prediction and adjustment of re-employment intention based on significant factors.} }
@inproceedings{10.1109/SCW63240.2024.00117, title = {Framework for Integrating Machine Learning Methods for Path-Aware Source Routing}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {829--838}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00117}, url = {https://doi.org/10.1109/SCW63240.2024.00117}, author = {Al-Najjar, Anees and Paraiso, Domingos and Kiran, Mariam and Dominicini, Cristina and Borges, Everson and Guimaraes, Rafael and Martinello, Magnos and Newman, Harvey}, keywords = {congestion minimization, machine learning, network optimization, segment routing, traffic engineering, location = Atlanta, GA, USA}, abstract = {Since the advent of software-defined networking (SDN), Traffic Engineering (TE) has been highlighted as one of the key applications that can be achieved through software-controlled protocols (e.g. PCEP and MPLS). Being one of the most complex challenges in networking, TE problems involve difficult decisions such as allocating flows, either via splitting them among multiple paths or by using a reservation system, to minimize congestion. However, creating an optimized solution is cumbersome and difficult as traffic patterns vary and change with network scale, capacity, and demand. AI methods can help alleviate this by finding optimized TE solutions for the best network performance. SDN-based TE tools such as Teal, Hecate and more, use classification techniques or deep reinforcement learning to find optimal network TE solutions that are demonstrated in simulation. Routing control conducted via source routing tools, e.g., PolKA, can help dynamically divert network flows. In this paper, we propose a novel framework that leverages Hecate to practically demonstrate TE on a real network, collaborating with PolKA, a source routing tool. With real-time traffic statistics, Hecate uses this data to compute optimal paths that are then communicated to PolKA to allocate flows. Several contributions are made to show a practical implementation of how this framework is tested using an emulated ecosystem mimicking a real P4 testbed scenario. This work proves valuable for truly engineered self-driving networks helping translate theory to practice.} }
@inproceedings{10.1145/3715335.3735485, title = {Machine Learning Fairness in House Price Prediction: A Case Study of America’s Expanding Metropolises}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {473--480}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735485}, url = {https://doi.org/10.1145/3715335.3735485}, author = {Almajed, Abdalwahab and Tabar, Maryam and Najafirad, Peyman}, keywords = {Machine Learning for Sustainable Societies, Machine Learning Fairness, House Price Prediction}, abstract = {As a basic human need, housing plays a key role in enhancing health, well-being, and educational outcome in society, and the housing market is a major factor for promoting quality of life and ensuring social equity. To improve the housing conditions, there has been extensive research on building Machine Learning (ML)-driven house price prediction solutions to accurately forecast the future conditions, and help inform actions and policies in the field. In spite of their success in developing high-accuracy models, there is a gap in our understanding of the extent to which various ML-driven house price prediction approaches show ethnic and/or racial bias, which in turn is essential for the responsible use of ML, and ensuring that the ML-driven solutions do not exacerbate inequity. To fill this gap, this paper develops several ML models from a combination of structural and neighborhood-level attributes, and conducts comprehensive assessments on the fairness of ML models under various definitions of privileged groups. As a result, it finds that the ML-driven house price prediction models show various levels of bias towards protected attributes (i.e., race and ethnicity in this study). Then, it investigates the performance of different bias mitigation solutions, and the experimental results show their various levels of effectiveness on different ML-driven methods. However, in general, the in-processing bias mitigation approach tends to be more effective than the pre-processing one in this problem domain. Our code is available at https://github.com/wahab1412/housing_fairness.} }
@proceedings{10.1145/3686490, title = {SPML '24: Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning}, year = {2024}, isbn = {9798400717192} }
@inproceedings{10.1145/3701625.3701696, title = {Identifying Concerns When Specifying Machine Learning-Enabled Systems: A Perspective-Based Approach}, booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality}, pages = {673--675}, year = {2024}, isbn = {9798400717772}, doi = {10.1145/3701625.3701696}, url = {https://doi.org/10.1145/3701625.3701696}, author = {Villamizar, Hugo and Kalinowski, Marcos}, keywords = {Requirements Specification, Machine Learning, Software Quality}, abstract = {Engineering successful machine learning (ML)-enabled systems poses various challenges from both a theoretical and a practical side. Among those challenges are how to effectively address unrealistic expectations of ML capabilities from customers, managers and even other team members, and how to connect business value to engineering and data science activities composed by interdisciplinary teams. In this thesis, we studied the state of the practice and literature of requirements engineering (RE) for ML to propose\&nbsp;PerSpecML, a perspective-based approach for specifying ML-enabled systems that helps practitioners identify which attributes, including ML and non-ML components, are important to contribute to the overall system’s quality. The approach involves analyzing 60 concerns related to 28 tasks that practitioners typically face in ML projects, grouping them into five perspectives: system objectives, user experience, infrastructure, model, and data. The conception of \&nbsp;PerSpecML involved a series of validations conducted in different contexts: (i) in academia, (ii) with industry representatives, and (iii) in two real industrial case studies. As a result of the diverse validations and continuous improvements,\&nbsp;PerSpecML showed a positive impact to the specification of ML-enabled systems, particularly helping to specify key quality components that would have been otherwise missed without using\&nbsp;PerSpecML.} }
@article{10.1145/3605153, title = {Offloading Machine Learning to Programmable Data Planes: A Systematic Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3605153}, url = {https://doi.org/10.1145/3605153}, author = {Parizotto, Ricardo and Coelho, Bruno Loureiro and Nunes, Diego Cardoso and Haque, Israat and Schaeffer-Filho, Alberto}, keywords = {In-network computing, Programmable NICs, programmable switches, offloading, ML training, ML inference}, abstract = {The demand for machine learning (ML) has increased significantly in recent decades, enabling several applications, such as speech recognition, computer vision, and recommendation engines. As applications become more sophisticated, the models trained become more complex while also increasing the amount of data used for training. Several domain-specific techniques can be helpful to scale machine learning to large amounts of data and more complex models. Among the methods employed, of particular interest is offloading machine learning functionality to the network infrastructure, which is enabled by the use of emerging programmable data plane hardware, such as SmartNICs and programmable switches. As such, offloading machine learning to programmable network hardware has attracted considerable attention from the research community in the last few years. This survey presents a study of programmable data planes applied to machine learning, also highlighting how in-network computing is helping to speed up machine learning applications. In this article, we provide various concepts and propose a taxonomy to classify existing research. Next, we systematically review the literature that offloads machine learning functionality to programmable data plane devices, classifying it based on our proposed taxonomy. Finally, we discuss open challenges in the field and suggest directions for future research.} }
@inproceedings{10.1145/3640429.3640437, title = {Design Thinking Using Qualitative Data Analysis and Machine Learning}, booktitle = {Proceedings of the 2023 13th International Conference on Information Communication and Management}, pages = {40--47}, year = {2024}, isbn = {9798400708114}, doi = {10.1145/3640429.3640437}, url = {https://doi.org/10.1145/3640429.3640437}, author = {Hanan, Moussa and Galal, Galal-Edeen H.}, keywords = {Agile Development, Design Thinking, Machine Learning, Persona creation, Qualitative Data Analysis, location = Cairo, Egypt}, abstract = {Design Thinking is a human-centered approach that allows continuous feedback by the user through Empathizing, Defining, Testing, Ideating and Prototyping. It mainly focuses on user needs, aspirations, wishes, concerns and frustrations in attempting to solve their problems. The Persona Creation approach follows the process of collecting data from multiple sources including social media platforms or the traditional methods including interviews of different users to cover the different types of behaviors, interactions and goals, questionnaires, or surveys. Condensing gathered data using qualitative data analysis renders assessable domain models that can be shared among and modified by stakeholders, so as to agree on user needs and issues. After agreeing on useful needs and user issues, they are used to generate Personas that represent the different types of users of a specific software product. When both approaches Design Thinking and Persona Creation are incorporated during Agile software development, this would lead to the creation of a successful software product. Successful software products are ones that cover all the needs as mentioned by the product user, also known as user perspectives.A user perspective refers to the perception of a given user and how they would use the final product. Those are\&nbsp;the people who would interact with the software product created and, therefore, the people for whom\&nbsp;the software is designed. For this reason, if an application, a website or a functionality that does not meet the final user's needs, this would ultimately result in a failure for the business. Inducing pain points/insights (what is needed by users) should not left totally to the skills of the analyst with little guidance. A systematic and more guidance is needed in this situation. Agile Software Development lack a coherent and explicit technique or open architecture [1] that can accommodate changes mandated by experiments on the ground. In addition, there has to be a method for objectively evaluating resultant prototypes/releases/deliverables at the end of each sprint in a way that can effectively guide path adjustments.Therefore, in this research, we make use of Design Thinking with software products. Through creating a framework that includes Design Thinking as an elicitation technique. We propose a framework composed of two phases: The first phase is the use of a robust qualitative data analysis method, to achieve models that are rich, and at the same time concise and traceable to their origins. We propose the use of the Grounded Theory method in the analysis and integration of the qualitative data that can characterize user needs, pain points and system requirements, in addition to second layer requirements that are often hard to spot. Second layer requirements are those requirements that are not immediately visible or perceivable by the end-user of a system, or those working with or observing him or her, such as systems and requirements analysts. The source of data for generating grounded theoretical formulations include interviews (of whatever type), observations, online chatter, and documents relating to the immediate and wider contexts of the need phenomenon under study.The second part of our proposed framework is applying Machine Learning on the data resulting from the first phase so that we are able to automate the Persona creation using Machine learning. Automatic Persona creation via machine learning is used to represent potential users, as an attempt to enhance the requirements of software products since they will necessarily include user perspectives.} }
@inproceedings{10.1145/3643786.3648022, title = {Data vs. Model Machine Learning Fairness Testing: An Empirical Study}, booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning}, pages = {1--8}, year = {2024}, isbn = {9798400705748}, doi = {10.1145/3643786.3648022}, url = {https://doi.org/10.1145/3643786.3648022}, author = {Shome, Arumoy and Cruz, Luis and Van Deursen, Arie}, keywords = {SE4ML, ML fairness testing, empirical software engineering, data-centric AI, location = Lisbon, Portugal}, abstract = {Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a "cheap" and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.} }
@inproceedings{10.1145/3550082.3564214, title = {Time-Dependent Machine Learning for Volumetric Simulation}, booktitle = {SIGGRAPH Asia 2022 Posters}, year = {2022}, isbn = {9781450394628}, doi = {10.1145/3550082.3564214}, url = {https://doi.org/10.1145/3550082.3564214}, author = {Giraud-Carrier, Samuel and Holladay, Seth and Egbert, Parris}, keywords = {machine learning, ode networks, volumetric simulation, location = Daegu, Republic of Korea}, abstract = {We explore the application of a time-dependent machine learning framework to art direction of volumetric simulations. We show the benefit of the time dependency inherent to the ODE-net model when used in conjunction with simulation sequences. Unlike other machine learning methods which maintain a uniform timestep constraint during evaluation, the ODE-net framework is able to generate results for arbitrary time samples. We demonstrate how this non-uniform time step evaluation can be leveraged for use in artistic direction tasks. We specifically apply the model to the retiming of volumetric simulations to showcase the ability of the machine learning method to properly predict arbitrary time steps. We show that with minimal training data, the model is able to generalize over several simulation sequences with similar parameters.} }
@inproceedings{10.1145/3644116.3644174, title = {Study on Machine Learning based Heart Disease Prediction Model}, booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science}, pages = {346--352}, year = {2024}, isbn = {9798400708138}, doi = {10.1145/3644116.3644174}, url = {https://doi.org/10.1145/3644116.3644174}, author = {Zhang, Shihan}, abstract = {In this study, this paper examined and compared the performance of two different machine learning models (logistic regression and random forest) in predicting the presence of heart disease using a dataset with 13 characteristics. While the logistic regression model provided reasonable performance with an accuracy of 82.47\%, the random forest model significantly outperformed logistic regression, achieving an impressive 99.03\% accuracy. In all models, features such as "cp" "thal" and "oldpeak" have been consistently recognized as important predictors of heart disease. These findings highlight the potential of advanced machine learning techniques, in particular random forest, to improve predictive accuracy in health risk assessments. The study also highlights the importance of considering various machine learning models to determine the most efficient way to predict specific data sets and tasks. Future research should focus on validating these models with different, larger datasets and exploring the integration of these models into clinical decision support systems.} }
@inproceedings{10.1145/3503161.3548549, title = {CurML: A Curriculum Machine Learning Library}, booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, pages = {7359--7363}, year = {2022}, isbn = {9781450392037}, doi = {10.1145/3503161.3548549}, url = {https://doi.org/10.1145/3503161.3548549}, author = {Zhou, Yuwei and Chen, Hong and Pan, Zirui and Yan, Chuanhao and Lin, Fanqi and Wang, Xin and Zhu, Wenwu}, keywords = {curriculum learning, machine learning, training strategy, location = Lisboa, Portugal}, abstract = {Curriculum learning (CL) is a machine learning paradigm gradually learning from easy to hard, which is inspired by human curricula. As an easy-to-use and general training strategy, CL has been widely applied to various multimedia tasks covering images, texts, audios, videos, etc. The effectiveness of CL has recently facilitated an increasing number of new CL algorithms. However, there has been no open-source library for curriculum learning, making it hard to reproduce, evaluate and compare the numerous CL algorithms on fair benchmarks and settings. To ease and promote future research on CL, we develop CurML, the first \&lt;u\&gt;Cur\&lt;/u\&gt;riculum \&lt;u\&gt;M\&lt;/u\&gt;achine \&lt;u\&gt;L\&lt;/u\&gt; earning library to integrate existing CL algorithms into a unified framework. It is convenient to use and flexible to customize by calling the provided five APIs, which are designed for easily plugging into a general training process and conducting the data-oriented, model-oriented and loss-oriented curricula. Furthermore, we present empirical results obtained by CurML to demonstrate the advantages of our library. The code is available online at https://github.com/THUMNLab/CurML.} }
@inproceedings{10.1145/3607947.3608023, title = {Heart Failure Prediction Using Different Machine Learning Algorithms}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {352--360}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608023}, url = {https://doi.org/10.1145/3607947.3608023}, author = {Aggarwal, Ananya and Gupta, Samarth and Varshney, Vanshita and Jaiswal, Shruti}, keywords = {Accuracy, Cardiovascular disease, Machine Learning Algorithms, UCI dataset, location = Noida, India}, abstract = {A doctor can benefit greatly from an early diagnosis of an illness and lower their patient's mortality risk. Machine learning algorithms play a significant role in the initial stages of illness detection, which would help in providing effective treatment for patients. We are using the dataset from the University of California, Irvine (UCI) repository to train and test our model. We will be using a total of twelve machine-learning algorithms. To improve their performance, Hyperparameter tuning of each algorithm would be done. For the comparative analysis of all algorithms, we are using accuracy on the testing set as the performance measure. For simulation, Jupyter Notebook 6.5.2 is used.} }
@inproceedings{10.1145/3727993.3728015, title = {A Multi-Modal Machine Learning Framework for Wind Power Forecasting}, booktitle = {Proceedings of the 2024 4th International Conference on Computational Modeling, Simulation and Data Analysis}, pages = {129--134}, year = {2025}, isbn = {9798400711831}, doi = {10.1145/3727993.3728015}, url = {https://doi.org/10.1145/3727993.3728015}, author = {Yang, Zexiang and Yin, Han and Wen, Yongjia}, keywords = {Conv-BiLSTM, Feature dimensionality reduction, LightGBM SIM, New quality productive power}, abstract = {The report of the 2024 National People's Congress and the National People's Congress clearly pointed out that the primary task of high-quality development should be grasped, relating with the new quality productive forces. Meanwhile. wind power, as the new quality productive forces of renewable energy, has significantly played the vital role in the global power supply. So it's important for us to explore the ways and likelihoods about how to accurately predict wind power. Nevertheless, on the one hand,it's numerous features involved in wind power that possibly create the dimensions collapse and waste the unnecessary computational resource. On the other hand, Most of the features which involve with too manyt invalid information are easily to lead to the overfittting model. Thus, we made a analysis on a wind power dateset with 140,000 samples and 76 features. The steps are as follows:①In the first place, it's initial dimensionality reduction that we applied LightGBM to analyse the importance of wind power features. Based on the result, we remove the features with low importance.②Subsequently, it's secondary dimensionality reduction we combined MIC and VIF to construct the SIM matrix so as to measure the linear and nonlinear relationships of each feature.③Consequently, Based on the former dimensionality reduction results, we combined convolution and bidirectional long short-term memory network to build the Conv-BiLSTM model is utilized to forecast the wind power. It shows that the method can effectively achieve a high feature dimensionality reduction rate while achieving more efficient power prediction.} }
@inproceedings{10.1145/3615366.3615375, title = {Evaluation of Machine Learning for Intrusion Detection in Microservice Applications}, booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing}, pages = {126--135}, year = {2023}, isbn = {9798400708442}, doi = {10.1145/3615366.3615375}, url = {https://doi.org/10.1145/3615366.3615375}, author = {Araujo, Iury and Antunes, Nuno and Vieira, Marco}, keywords = {Intrusion Detection, Machine Learning, Microservices, System Calls, location = La Paz, Bolivia}, abstract = {Microservices have thrived recently as an approach for service design, development, and delivery. It provides several benefits to the systems as an architecture, such as faster delivery, improved scalability, and greater autonomy. Although microservice architectures are popular, security characteristics of these architectures impair the deployment of security, such as sizable attack surface, network complexity, heterogeneity, and others. For years, intrusion detection has been a practical security approach for many applications. Recently, machine learning provided improved functionality for intrusion detection systems with exciting results in overall tests. This paper presents the evaluation of machine learning techniques for intrusion detection in a microservice scenario. System call data was collected from containers simulating microservice applications; these containers were submitted to attacks that exploited different vulnerabilities. The data was used to train and test machine learning techniques, and the test results provided us with exciting possibilities for this approach. Some of the tested attacks were very well detected by the techniques, while some were not, attesting that machine-learning-based intrusion detection is usable in this environment. However, to enhance detection, it is required to improve data processing and representation for this type of scenario.} }
@inproceedings{10.1145/3669947.3669963, title = {Multi-Label Emotion Classification of Online Learners' Reviews Using Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Education Development and Studies}, pages = {59--64}, year = {2024}, isbn = {9798400718083}, doi = {10.1145/3669947.3669963}, url = {https://doi.org/10.1145/3669947.3669963}, author = {Makhoukhi, Hajar and Roubi, Sarra}, keywords = {Emotions Recognition, Machine Learning, Multi-label Classification, Online Learning, location = Cambridge, United Kingdom}, abstract = {Text- based emotion recognition is one of research areas widely developed in applied computing, but it is highly limited when dealing with online learners. In this study, we evaluate the performances of 13 multi-label classification machine learning-based methods for automatic recognizing of online learners’ emotions, 12 of them are problem transformation methods and 1 is an adaptation algorithm method. The experiments are carried out using a dataset of online learners’ reviews sourced from Coursera and manually multi-labeled with the emotions: Enjoyment, Excitement, Satisfaction, Frustration, Boredom, and Confusion. Our best results in term of Hamming Loss and Micro-averaged F1 Score are obtained using Random Forest classifier and classifier chains approach, while the best Macro-averaged F1 Score was obtained using Decision Tree classifier and binary relevance approach.} }
@inproceedings{10.1145/3638530.3648408, title = {Evolutionary Art and Design in the Machine Learning Era}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1460--1501}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3648408}, url = {https://doi.org/10.1145/3638530.3648408}, author = {Machado, Penousal and Correia, Jo\~ao} }
@inproceedings{10.1145/3715275.3732046, title = {Measuring Machine Learning Harms from Stereotypes Requires Understanding Who Is Harmed by Which Errors in What Ways}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {746--762}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732046}, url = {https://doi.org/10.1145/3715275.3732046}, author = {Wang, Angelina and Bai, Xuechunzi and Barocas, Solon and Blodgett, Su Lin}, keywords = {stereotypes, machine learning fairness, harms from biases}, abstract = {Despite a proliferation of research on the ways that machine learning models can propagate harmful stereotypes, very little of this work is grounded in the psychological experiences of people exposed to such stereotypes. We use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use surveys to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experiential harm, while having minimal impact on participants’ cognitive beliefs, attitudes, or behaviors. This experiential harm impacts participants who are women more than those who are men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conclude by proposing a more nuanced perspective on the harms of machine learning errors—one that depends on who is experiencing what harm and why.} }
@inproceedings{10.1145/3718677.3718710, title = {Perioperative Immune-Inflammatory Analysis Method for Colorectal Cancer Based on Explainable Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Public Health and Data Science}, pages = {204--208}, year = {2025}, isbn = {9798400711671}, doi = {10.1145/3718677.3718710}, url = {https://doi.org/10.1145/3718677.3718710}, author = {Tian, Naiyuan}, keywords = {Colorectal Cancer, Game Theory, Interpretability, Machine Learning, Perioperative Immune Inflammatory Responses}, abstract = {The single-controlled experimental method plays an important role in medical research, while it has a lot of limitations. With the rapid development of artificial intelligence, machine learning, and game theory are playing increasingly important roles. The research is about what factors infect the immune inflammatory response\&nbsp;of colorectal cancer surgery. We propose an innovative\&nbsp;model to predict immunity inflammatory response, and the model has more accuracy and more interpretability. Ensemble learning improves accuracy and reduces errors. The shap value explains which features are important and how the features explain the result. Many factors are considered, and the result shows that the anesthesia method\&nbsp;has a significant effect. The time of the surgery has no significant effect on the immune inflammatory response, which means that the surgical schedule can be more flexible.} }
@proceedings{10.1145/3661725, title = {CMLDS '24: Proceedings of the International Conference on Computing, Machine Learning and Data Science}, year = {2024}, isbn = {9798400716393} }
@inproceedings{10.1145/3672608.3707815, title = {Quantitative Assessment of Explainability in Machine Learning Models : A Study on the OULA Dataset}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {101--103}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707815}, url = {https://doi.org/10.1145/3672608.3707815}, author = {Gunasekara, Sachini and Saarela, Mirka}, keywords = {fidelity, stability, ANN, feature importance, LIME, location = Catania International Airport, Catania, Italy}, abstract = {Many studies on AI in education compare model performance and fairness, but few focus on explainability. To address this gap, we evaluate two machine learning models—Artificial Neural Network (ANN) and Decision Tree (DT)—focusing on performance and explainability in predicting student performance using the OULA dataset. The DT, being inherently explainable, struggles with complex data relationships and misclassification, while ANN, although more accurate and stable, lacks transparency. Using the LIME method, the ANN outperforms the DT in accuracy and stability, but enhancing the interpretability of ANN models remains a key challenge for future research.} }
@inproceedings{10.1145/3716895.3717016, title = {Urban Waterlogging Risk Assessment Based on Multivariate Data and Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering}, pages = {680--687}, year = {2025}, isbn = {9798400718007}, doi = {10.1145/3716895.3717016}, url = {https://doi.org/10.1145/3716895.3717016}, author = {Zang, Feng and Fu, Jianyu and Chen, Quan}, keywords = {Machine learning, Multivariate data, Risk assessment, Urban waterlogging}, abstract = {With the acceleration of climate change and urbanization, urban waterlogging has become an increasingly severe issue, posing a threat to residents' safety. This study focuses on Nanjing City, selecting 12 risk indicators and integrating diverse data such as Weibo and remote sensing. Eight traditional and deep learning models were compared, with the best-performing Random Forest model selected for waterlogging risk assessment. The results show that the waterlogging risk is higher in the central areas of Nanjing, where vulnerability indicators such as POI density and population density, as well as probability indicators like surface impermeability and topography, significantly impact the risk. This study provides practical strategies for urban managers to prevent and mitigate waterlogging.} }
@inproceedings{10.1145/3696673.3723080, title = {Using Machine Learning for Air Quality Prediction in Alabama: An Environmental Justice Case Study}, booktitle = {Proceedings of the 2025 ACM Southeast Conference}, pages = {251--256}, year = {2025}, isbn = {9798400712777}, doi = {10.1145/3696673.3723080}, url = {https://doi.org/10.1145/3696673.3723080}, author = {Maskey, Arnav and Shinde, Rajat}, keywords = {neural networks, machine learning datasets, air quality, environmental justice, location = Southeast Missouri State University, Cape Girardeau, MO, USA}, abstract = {Environmental justice encompasses the fair treatment of all people regardless of race, color, national origin, education level, or income, in the context of environmental impacts. This paper investigates the application of machine learning (ML) to address environmental justice issues arising from air quality disparities in Jefferson County, Alabama, particularly around Birmingham. The study makes several key contributions including curation of ML-ready datasets from environmental sensor data for air quality analysis; fusion of domain specific datasets for analyzing the effects of air quality degradation across demographics; utilization of novel advanced ML approaches using a Convolutional Neural Network and Long Short-Term Memory (CNN-LSTM) architecture to identify, predict, and analyze areas with significant environmental injustice; and projection for air quality degradation with demographics to assist in decision making and future policy making. This research demonstrates the utility of ML in rapidly identifying environmental justice hotspots and offers predictive capabilities for timely mitigation measures. Additionally, this study provides a pathway for extending the methodology to similar issues globally. The work also offers social value by providing policymakers with actionable insights for mitigating air quality disparities and empowering marginalized communities with access to precise environmental predictions.} }
@inproceedings{10.1145/3640543.3645170, title = {Comparing Teaching Strategies of a Machine Learning-based Prosthetic Arm}, booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces}, pages = {715--730}, year = {2024}, isbn = {9798400705083}, doi = {10.1145/3640543.3645170}, url = {https://doi.org/10.1145/3640543.3645170}, author = {Sungeelee, Vaynee and Jarrass\'e, Nathana\"el and Sanchez, T\'eo and Caramiaux, Baptiste}, keywords = {Interactive Machine Teaching, Machine Learning, Mental model, Myoelectric prosthesis, Training curriculum, location = Greenville, SC, USA}, abstract = {Pattern-recognition-based arm prostheses rely on recognizing muscle activation to trigger movements. The effectiveness of this approach depends not only on the performance of the machine learner but also on the user’s understanding of its recognition capabilities, allowing them to adapt and work around recognition failures. We investigate how different model training strategies to select gesture classes and record respective muscle contractions impact model accuracy and user comprehension. We report on a lab experiment where participants performed hand gestures to train a classifier under three conditions: (1) the system cues gesture classes randomly (control), (2) the user selects gesture classes (teacher-led), (3) the system queries gesture classes based on their separability (learner-led). After training, we compare the models’ accuracy and test participants’ predictive understanding of the prosthesis’ behavior. We found that teacher-led and learner-led strategies yield faster and greater performance increases, respectively. Combining two evaluation methods, we found that participants developed a more accurate mental model when the system queried the least separable gesture class (learner-led). Our results conclude that, in the context of machine learning-based myoelectric prosthesis control, guiding the user to focus on class separability during training can improve recognition performances and support users’ mental models about the system’s behavior. We discuss our results in light of several research fields : myoelectric prosthesis control, motor learning, human-robot interaction, and interactive machine teaching.} }
@inproceedings{10.1145/3637528.3671461, title = {Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6470--6479}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671461}, url = {https://doi.org/10.1145/3637528.3671461}, author = {Derezi\'nski, Micha and Mahoney, Michael W.}, keywords = {matrix computations, optimization, randomization, statistics, location = Barcelona, Spain}, abstract = {Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.} }
@article{10.1145/3610536, title = {Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview}, journal = {ACM Trans. Evol. Learn. Optim.}, volume = {3}, year = {2023}, doi = {10.1145/3610536}, url = {https://doi.org/10.1145/3610536}, author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merch\'an, Eduardo C. and Branke, Juergen and Bischl, Bernd}, keywords = {Multi-objective hyperparameter optimization, neural architecture search, Bayesian optimization}, abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.} }
@article{10.1145/3711702, title = {NetJIT: Bridging the Gap from Traffic Prediction to Preknowledge for Distributed Machine Learning}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, volume = {9}, year = {2025}, doi = {10.1145/3711702}, url = {https://doi.org/10.1145/3711702}, author = {Ai, Xin and Li, Zijian and Zhu, Yuanyi and Chen, Zixuan and Liu, Sen and Xu, Yang}, keywords = {distributed machine learning, jit, just-in-time program analysis, network optimization, traffic prediction}, abstract = {Today's distributed machine learning (DML) introduces heavy traffic load, making the interconnection network one of the primary bottlenecks. To mitigate this bottleneck, existing state-of-the-art network optimization methods, such as traffic or topology engineering, are proposed to adapt to real-time traffic. However, current traffic measurement and prediction methods struggle to collect sufficiently fine-grained and accurate traffic patterns. This limitation impedes the ability of cutting-edge network optimization techniques to react agilely to the ever-changing traffic demands of DML jobs.This paper proposes NetJIT, a novel program-behavior-aware toolkit for accurately foreseeing the traffic pattern of DML. To the best of our knowledge, this is the first work proposing the use of just-in-time (JIT) program analysis for real-time traffic measurement. In DML applications, communication behavior is primarily determined by the previously computed results. NetJIT leverages this characteristic to anticipate communication details by tracing and analyzing the data relations in the computation process. This capability enables the deployment of optimization strategies in advance.We deploy NetJIT in real-world network optimization for traffic preknowledge. Evaluation with the self-built testbed prototype demonstrates that NetJIT can achieve up to about 97\% less error of detecting communication events compared with other methods. Simulations with real-world DML workloads further illustrate that NetJIT enables more precise network optimization, leading to approximately 50\% better network performance w.r.t the metrics including average iteration time, throughput, and average packet delay.} }
@article{10.1145/3744749, title = {Localization of Data Compromised by Hardware Attacks in Machine Learning Enabled Cyber-Physical Edge Devices}, journal = {ACM Trans. Cyber-Phys. Syst.}, volume = {9}, year = {2025}, issn = {2378-962X}, doi = {10.1145/3744749}, url = {https://doi.org/10.1145/3744749}, author = {Edara, Pravineeth and Banerjee, Sanmitra and Joardar, Biresh Kumar}, keywords = {Cyber-physical systems, Fault-injection attacks, Diagnosis, Convolutional Neural Networks, Rowhammer}, abstract = {Hardware attacks present a new and easy way for malicious actors to compromise model parameters in machine learning (ML) enabled cyber-physical systems (CPS). This can have severe consequences for many safety-critical cyber-physical applications such as power systems, self-driving cars, healthcare, security, and so on. Prior works have proposed several pre-emptive mitigation approaches for hardware attacks that can be adopted. However, adversarial attacks can bypass existing pre-emptive attack detection methods. Existing defense setups offer no further protection once the detection is bypassed. The attacker can then cause damage without getting noticed easily. In this work, we propose a new diagnosis method to search for compromised weights in real-time even when detection is bypassed considering fault-injection attacks. The proposed methodology provides an additional level of protection, which can rapidly identify and localize more than 99\% of affected weights in ML models, even when thousands of model parameters are affected simultaneously, with low power, performance, and area (PPA) overheads. In addition, we also propose a method to ensure that the CPS remains functional, even when undergoing attack diagnosis.} }
@inproceedings{10.1145/3639478.3643531, title = {Improving Fairness in Machine Learning Software via Counterfactual Fairness Thinking}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {420--421}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3643531}, url = {https://doi.org/10.1145/3639478.3643531}, author = {Yin, Zhipeng and Wang, Zichong and Zhang, Wenbin}, abstract = {Machine Learning (ML) software is increasingly influencing decisions that impact individuals' lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social subgroups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works enhance software fairness, a non-functional software property, at the cost of software performance. To this end, we proposed a novel framework, namely Group Equality Counterfactual Fairness (GECF), which aims to mitigate sensitive attribute bias and labeling bias using counterfactual fairness while reducing the resulting performance loss based on ensemble learning. Experimental results on 6 real-world datasets show the superiority of our proposed framework from different aspects.} }
@inproceedings{10.1145/3626246.3655014, title = {Eighth Workshop on Data Management for End-to-End Machine Learning (DEEM)}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {651--652}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3655014}, url = {https://doi.org/10.1145/3626246.3655014}, author = {Hulsebos, Madelon and Interlandi, Matteo and Shankar, Shreya}, keywords = {data management, machine learning, systems, location = Santiago AA, Chile}, abstract = {The DEEM'24 workshop (Data Management for End-to-End Machine Learning) is held on Sunday June 9th, in conjunction with SIGMOD/PODS 2024. DEEM brings together researchers and practitioners at the intersection of applied machine learning, data management and systems research, with the goal to discuss the arising data management issues in ML application scenarios. The workshop solicits regular research papers (8 pages) describing preliminary and ongoing research results, including industrial experience reports of end-to-end ML deployments, related to DEEM topics. In addition, DEEM 2023 has a category for short papers (4 pages) as a forum for sharing interesting use cases, problems, datasets, benchmarks, visionary ideas, system designs, preliminary results, and descriptions of system components and tools related to end-to-end ML pipelines. This year, the workshop received 16 high-quality submissions on diverse topics relevant to DEEM, of which 8 regular papers and 8 short papers.} }
@inproceedings{10.1145/3745676.3745695, title = {Malicious Website Detection Optimization in Enterprise Cybersecurity Management: A Machine Learning-Based Decision Support Approach}, booktitle = {Proceedings of the 2025 2nd International Conference on Innovation Management and Information System}, pages = {121--127}, year = {2025}, isbn = {9798400715150}, doi = {10.1145/3745676.3745695}, url = {https://doi.org/10.1145/3745676.3745695}, author = {Zhao, Yiyi and Chen, Yingying and Zhang, Runnan}, keywords = {Enterprise cybersecurity management, Machine learning, Malicious website detection, Strategy optimization}, abstract = {The increasing proliferation of malicious web platforms presents critical risks to organizational cybersecurity, necessitating the creation of advanced detection systems that adapt to evolving cyber threats. Conventional identification approaches frequently prove inadequate against sophisticated attack patterns. This paper proposes an innovative website threat analysis framework combining structured feature selection with t-SNE visualization in data-centric methodologies. Through targeted feature optimization, we improve model transparency and processing effectiveness while minimizing noise in complex datasets. The analytical framework evaluates five widely-adopted classification algorithms, XGBoost, KNN, Naive Bayes, Random Forest, and SVM, to assess detection capabilities. Validation tests using the Malicious/Benign Websites dataset reveal that Random Forest and XGBoost achieved superior performance in identifying malicious domains, maintaining high accuracy and balanced F1-scores. These outcomes highlight the value of embedding adaptive machine learning solutions into corporate security infrastructures to optimize threat recognition, operational responsiveness, and system resource allocation. The study offers practical guidance for enhancing cybersecurity budgeting decisions and developing preemptive protection frameworks within enterprise risk management contexts.} }
@inproceedings{10.1145/3580305.3599223, title = {KDD Workshop on Machine Learning in Finance}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5863--5864}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599223}, url = {https://doi.org/10.1145/3580305.3599223}, author = {Akoglu, Leman and Chawla, Nitesh and Kumar, Senthil and Nagrecha, Saurabh and Das, Mahashweta and Naware, Vidyut M. and Faruquie, Tanveer}, keywords = {finance, graph mining, machine learning, nlp, location = Long Beach, CA, USA}, abstract = {The finance industry is constantly faced with an ever evolving set of challenges including credit card fraud, identity theft, network intrusion, money laundering, human trafficking, and illegal sales of firearms. There is also the newly emerging threat of fake news in financial media that can lead to distortions in trading strategies and investment decisions. In addition, traditional problems such as customer analytics, forecasting, and recommendations take on a unique flavor when applied to financial data. A number of new ideas are emerging to tackle all these problems including self-supervised learning methods, deep learning algorithms, network/graph based solutions as well as linguistic approaches. These methods must often be able to work in real-time and be able handle large volumes of data. The purpose of this workshop is to bring together researchers and practitioners to discuss both the problems faced by the financial industry and potential solutions. We plan to invite regular papers, positional papers and extended abstracts of work in progress. We will also encourage short papers from financial industry practitioners that introduce domain specific problems and challenges to academic researchers.} }
@inproceedings{10.1145/3724154.3724354, title = {Research on Financial Risk Management and Investment Strategies Based on Bayesian Machine Learning Models}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {1228--1234}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724354}, url = {https://doi.org/10.1145/3724154.3724354}, author = {Liu, Chang}, keywords = {Bayesian Risk Model, Machine Learning, Python, Quantitative Finance, Risk Allocation}, abstract = {Traditional quantitative investment tactics are facing limitations by the growing of data and the increasing complexity of financial markets. Machine learning (ML) has been a transformative approach in quantitative finance. This paper explores ML-based strategies, focusing on adding more risk considerations into the quantitative stock selection strategy, and back tested and compared, and reached the relevant conclusions, to provide some help for the increasing market uncertainty faced by investors.} }
@inproceedings{10.1145/3700906.3700995, title = {A Methodology for Assessing Carbon Emission Indicators Based on Incorporates Machine Learning and Multi-modal data}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {556--560}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700995}, url = {https://doi.org/10.1145/3700906.3700995}, author = {Zhang, Shuang and Liu, Jia and Ma, Rui and Sha, Jiangbo and Kang, Wenni and Qu, Fanghao and Kang, Yibin}, keywords = {BP neural network, Carbon emission costs, Carbon emission indicators, Fusion machine learning, Linear relationship, Multi-modal data}, abstract = {Since the status of carbon emission indicators involves many factors, there is often a problem of large errors when evaluating them. For this reason, this paper proposes a study on a carbon emission indicator evaluation method that integrates machine learning and multi-modal data. After comprehensively analyzing the influencing factors of the value of carbon emission rights from the four perspectives of macroeconomics, energy prices, climate change, policy, and the international carbon market, we integrated multi-modal influencing factor data, calculated the corresponding carbon emission costs, and combined them as the input characteristic parameter of the BP neural network, through forward propagation and backward propagation, the linear relationship between it and the value of carbon emission rights is determined, and the corresponding model is constructed to realize the value evaluation of carbon emission rights. In the test results, the fit between the carbon emission rights value assessment results and the actual transaction price has always remained relatively stable, and the specific error has always been within 0.70 yuan/ton, with the maximum error being only 0.66 yuan/ton. Compared with the control group, it has obvious advantages in assessment accuracy and effectiveness respectively.} }
@inproceedings{10.1145/3486001.3486248, title = {Machine Learning for Databases}, booktitle = {Proceedings of the First International Conference on AI-ML Systems}, year = {2021}, isbn = {9781450385947}, doi = {10.1145/3486001.3486248}, url = {https://doi.org/10.1145/3486001.3486248}, author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei}, abstract = {Machine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously learning the optimization strategy from historical data or explorations. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including (i) NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), (ii) regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and (iii) prediction problems (e.g., transaction scheduling, trend prediction). We review existing machine learning based techniques to address these problems and provide research challenges.} }
@inproceedings{10.1145/3629296.3629368, title = {A Systematic Analysis of Machine Learning Studies in Education}, booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers}, pages = {451--455}, year = {2024}, isbn = {9798400709111}, doi = {10.1145/3629296.3629368}, url = {https://doi.org/10.1145/3629296.3629368}, author = {Pektas, Sule Tasl}, keywords = {Bibliometric Analysis, Education, Keyword Co-occurrence Network, Machine Learning, location = Barcelona, Spain}, abstract = {Machine learning has been transforming education and changing learning, teaching, and administration processes. However, studies analyzing the existing body of work and emerging research foci are lacking. To fill in the re-search gap, this paper presents a bibliometric analysis of articles on machine learning in education that were indexed by Web of Science Core Citation In-dices from 1979 to 2023. The study investigates publication patterns (articles per year and journals) and key research areas. A keyword co-occurrence analysis was conducted to identify the clusters of keywords which often co-exist in articles. The analysis revealed six clusters which correspond to the main research themes: profiling and prediction, assessment, intelligent tutoring systems, MOOCs, natural language processing, and prediction in distance learning. It is discussed that the newly emerging and rapidly developing research area focuses merely on applications of the technology, while ethical, pedagogical, socio-cultural, and administrative is-sues regarding machine learning in education need further attention.} }
@article{10.1145/3652579, title = {Machine Learning-Based Kernel Selector for SpMV Optimization in Graph Analysis}, journal = {ACM Trans. Parallel Comput.}, volume = {11}, year = {2024}, issn = {2329-4949}, doi = {10.1145/3652579}, url = {https://doi.org/10.1145/3652579}, author = {Xiao, Guoqing and Zhou, Tao and Chen, Yuedan and Hu, Yikun and Li, Kenli}, keywords = {Adaptive framework, GPU computing, graph analysis, machine learning, SpMV}, abstract = {Sparse Matrix and Vector multiplication (SpMV) is one of the core algorithms in various large-scale scientific computing and real-world applications. With the rapid development of AI and big data, the input vector in SpMV becomes sparse in many application fields. Especially in some graph analysis calculations, the sparsity of the input vector will change with the running of the program, and the non-zero element distribution of the adjacency matrix of some graph data has the power law property, leading to serious load imbalance, which requires additional optimization means. Therefore, the optimal SpMV kernel may be different, and a single SpMV kernel can no longer meet the acceleration requirements. In this article, we propose a decision tree-based adaptive SpMV framework, named DTSpMV, that can automatically select appropriate SpMV kernels according to different input data in iterations of graph computation. Based on the analysis of computing patterns, bit-array compression algorithms, and serial and parallel algorithms, we encapsulate nine SpMV kernels within the framework. We explore machine learning-based kernel selectors in terms of both accuracy and runtime overhead. Experimental results on NVIDIA Tesla T4 GPU show that our adaptive framework achieves the arithmetic average performance improvement of 152 compared to the SpMV kernel in cuSPARSE.} }
@inbook{10.1109/ICSE55347.2025.00122, title = {Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {743--755}, year = {2025}, isbn = {9798331505691}, url = {https://doi.org/10.1109/ICSE55347.2025.00122}, author = {Chen, Zhenpeng and Li, Xinyue and Zhang, Jie M. and Sarro, Federica and Liu, Yang}, abstract = {Intersectional fairness is a critical requirement for Machine Learning (ML) software, demanding fairness across subgroups defined by multiple protected attributes. This paper introduces FairHOME, a novel ensemble approach using higher order mutation of inputs to enhance intersectional fairness of ML software during the inference phase. Inspired by social science theories highlighting the benefits of diversity, FairHOME generates mutants representing diverse subgroups for each input instance, thus broadening the array of perspectives to foster a fairer decision-making process. Unlike conventional ensemble methods that combine predictions made by different models, FairHOME combines predictions for the original input and its mutants, all generated by the same ML model, to reach a final decision. Notably, FairHOME is even applicable to deployed ML software as it bypasses the need for training new models. We extensively evaluate FairHOME against seven state-of-the-art fairness improvement methods across 24 decision-making tasks using widely adopted metrics. FairHOME consistently outperforms existing methods across all metrics considered. On average, it enhances intersectional fairness by 47.5\%, surpassing the currently best-performing method by 9.6 percentage points.} }
@article{10.1145/3475167, title = {Declarative machine learning systems}, journal = {Commun. ACM}, volume = {65}, pages = {42--49}, year = {2021}, issn = {0001-0782}, doi = {10.1145/3475167}, url = {https://doi.org/10.1145/3475167}, author = {Molino, Piero and R\'e, Christopher}, abstract = {The future of machine learning will depend on it being in the hands of the rest of us.} }
@inproceedings{10.1145/3583740.3626617, title = {SODA: Protecting Proprietary Information in On-Device Machine Learning Models}, booktitle = {Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing}, pages = {121--132}, year = {2024}, isbn = {9798400701238}, doi = {10.1145/3583740.3626617}, url = {https://doi.org/10.1145/3583740.3626617}, author = {Atrey, Akanksha and Sinha, Ritwik and Mitra, Saayan and Shenoy, Prashant}, keywords = {on-device, machine learning, proprietary information, privacy, location = Wilmington, DE, USA}, abstract = {The growth of low-end hardware has led to a proliferation of machine learning-based services in edge applications. These applications gather contextual information about users and provide some services, such as personalized offers, through a machine learning (ML) model. A growing practice has been to deploy such ML models on the user's device to reduce latency, maintain user privacy, and minimize continuous reliance on a centralized source. However, deploying ML models on the user's edge device can leak proprietary information about the service provider. In this work, we investigate on-device ML models that are used to provide mobile services and demonstrate how simple attacks can leak proprietary information of the service provider. We show that different adversaries can easily exploit such models to maximize their profit and accomplish content theft. Motivated by the need to thwart such attacks, we present an end-to-end framework, SODA, for deploying and serving on edge devices while defending against adversarial usage. Our results demonstrate that SODA can detect adversarial usage with 89\% accuracy in less than 50 queries with minimal impact on service performance, latency, and storage.} }
@inproceedings{10.1145/3677404.3677410, title = {Optimization of Beer Yeast Centrifuge Separation Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 International Academic Conference on Edge Computing, Parallel and Distributed Computing}, pages = {35--40}, year = {2024}, isbn = {9798400718168}, doi = {10.1145/3677404.3677410}, url = {https://doi.org/10.1145/3677404.3677410}, author = {Li, Huabin and Cui, Yunqian}, abstract = {Beer yeast is one of the indispensable microorganisms in the beer brewing process, and its centrifugal separation is a crucial step in the brewing industry. The traditional centrifugal separation method relies on experience and trial and error, and has problems of low efficiency and resource waste. With the development of machine learning algorithms, utilizing their advantages to optimize the centrifugal separation process has become possible. This study aims to optimize the centrifugal separation process of beer yeast using machine learning algorithms. This article proposes a centrifugal separation optimization method based on machine learning algorithms. Firstly, this article collected a large amount of experimental data, including key parameters such as yeast concentration, centrifugation speed, and centrifugation time. Then, this article used these data to train a machine learning model to predict the optimal centrifugal conditions. Compared with traditional methods, the results of this article indicate that machine learning algorithms can significantly improve the efficiency and quality of beer yeast centrifugal separation. This study provides a reference for optimizing methods in the brewing industry, with the potential to reduce production costs and improve product quality. The experimental results showed that the separation efficiency before optimization was only 0.80, but after optimization, it was increased to 0.92, and the yeast purity was also increased from 0.80 to 0.88.} }
@inproceedings{10.1145/3663529.3663831, title = {Costs and Benefits of Machine Learning Software Defect Prediction: Industrial Case Study}, booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering}, pages = {92--103}, year = {2024}, isbn = {9798400706585}, doi = {10.1145/3663529.3663831}, url = {https://doi.org/10.1145/3663529.3663831}, author = {Stradowski, Szymon and Madeyski, Lech}, keywords = {case study, cost-benefit analysis, industry, machine learning, software defect prediction, software testing, location = Porto de Galinhas, Brazil}, abstract = {Context: Our research is set in the industrial context of Nokia 5G and the introduction of Machine Learning Software Defect Prediction (ML SDP) to the existing quality assurance process within the company. Objective: We aim to support or undermine the profitability of the proposed ML SDP solution designed to complement the system-level black-box testing at Nokia, as cost-effectiveness is the main success criterion for further feasibility studies leading to a potential commercial introduction. Method: To evaluate the expected cost-effectiveness, we utilize one of the available cost models for software defect prediction formulated by previous studies on the subject. Second, we calculate the standard Return on Investment (ROI) and Benefit-Cost Ratio (BCR) financial ratios to demonstrate the profitability of the developed approach based on real-world, business-driven examples. Third, we build an MS Excel-based tool to automate the evaluation of similar scenarios that other researchers and practitioners can use. Results: We considered different periods of operation and varying efficiency of predictions, depending on which of the two proposed scenarios were selected (lightweight or advanced). Performed ROI and BCR calculations have shown that the implemented ML SDP can have a positive monetary impact and be cost-effective in both scenarios. Conclusions: The cost of adopting new technology is rarely analyzed and discussed in the existing scientific literature, while it is vital for many software companies worldwide. Accordingly, we bridge emerging technology (machine learning software defect prediction) with a software engineering domain (5G system-level testing) and business considerations (cost efficiency) in an industrial environment of one of the leaders in 5G wireless technology.} }
@article{10.1145/3617380, title = {Multi-objective Feature Attribution Explanation For Explainable Machine Learning}, journal = {ACM Trans. Evol. Learn. Optim.}, volume = {4}, year = {2024}, doi = {10.1145/3617380}, url = {https://doi.org/10.1145/3617380}, author = {Wang, Ziming and Huang, Changwu and Li, Yun and Yao, Xin}, keywords = {Explainable machine learning, feature attribution explanations, multi-objective learning, multi-objective evolutionary algorithms}, abstract = {The feature attribution-based explanation (FAE) methods, which indicate how much each input feature contributes to the model’s output for a given data point, are one of the most popular categories of explainable machine learning techniques. Although various metrics have been proposed to evaluate the explanation quality, no single metric could capture different aspects of the explanations. Different conclusions might be drawn using different metrics. Moreover, during the processes of generating explanations, existing FAE methods either do not consider any evaluation metric or only consider the faithfulness of the explanation, failing to consider multiple metrics simultaneously. To address this issue, we formulate the problem of creating FAE explainable models as a multi-objective learning problem that considers multiple explanation quality metrics simultaneously. We first reveal conflicts between various explanation quality metrics, including faithfulness, sensitivity, and complexity. Then, we define the considered multi-objective explanation problem and propose a multi-objective feature attribution explanation (MOFAE) framework to address this newly defined problem. Subsequently, we instantiate the framework by simultaneously considering the explanation’s faithfulness, sensitivity, and complexity. Experimental results comparing with six state-of-the-art FAE methods on eight datasets demonstrate that our method can optimize multiple conflicting metrics simultaneously and can provide explanations with higher faithfulness, lower sensitivity, and lower complexity than the compared methods. Moreover, the results have shown that our method has better diversity, i.e., it provides various explanations that achieve different tradeoffs between multiple conflicting explanation quality metrics. Therefore, it can provide tailored explanations to different stakeholders based on their specific requirements.} }
@inproceedings{10.1145/3745238.3745503, title = {Research on Business-finance Integration, Digital Transformation and Corporate Performance based on Dual Machine Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1698--1702}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745503}, url = {https://doi.org/10.1145/3745238.3745503}, author = {Zheng, Qidong and Zhang, Xin and Ding, Mingyi}, keywords = {Business-finance integration, Causal inference, Digital transformation, Dual machine learning, Enterprise performance}, abstract = {With the rapid development of digital economy, business-financial integration and digital transformation have become an important means for enterprises to improve operational efficiency and enhance competitiveness. Based on the pairwise machine learning method, this paper explores the impact mechanism of industry-finance integration and digital transformation on enterprise performance, analyzes the mediating role of industry-finance integration in digital transformation through causal inference framework, big data analysis and machine learning algorithms, and further explores its long-term effect on enterprise performance. The results of the study show that business-financial integration can effectively promote the implementation of digital transformation, thereby improving enterprise financial performance and market competitiveness.} }
@inproceedings{10.1145/3650215.3650382, title = {Automatic Protein Sequences Classification Using Machine Learning Methods based on N-Gram Model}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {936--940}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650382}, url = {https://doi.org/10.1145/3650215.3650382}, author = {Zou, Chengen}, abstract = {Protein sequence classification is a crucial task in the field of bioinformatics as it helps to reveal various types of properties of proteins. Machine learning and deep learning algorithms have great application value in the problem of protein classification prediction. In this study, we propose a novel approach for extracting sequence features based on N-Gram model. We then utilize this approach to train and evaluate machine learning and deep learning algorithms on the same dataset. The experimental results show that the Random Forest method based on N-Gram features significantly outperforms other types of algorithmic models on this dataset, with a further increase in accuracy.} }
@inproceedings{10.1145/3675888.3676040, title = {PQTD-TM: Integrating Information Retrieval with Machine Learning for effective Placement Topic Discovery}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {122--127}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676040}, url = {https://doi.org/10.1145/3675888.3676040}, author = {Sardana, Neetu and Kumar, Prabudh and Jain, Nehal and Goyal, Kritank Rishi and Arora, Anuja and Varshney, Deepika}, keywords = {Campus Placement, Information Retrieval, Machine Learning, TFIDF, location = Noida, India}, abstract = {Technical interviews, particularly coding rounds, are pivotal in shaping career trajectories, especially for roles in top-tier companies like Amazon, Google, and Microsoft. Understanding the patterns and nuances of coding problems and their associated tags is essential for effective interview preparation. To aid candidates in excelling in these assessments, we analyzed coding problems encountered in such interviews. We compile a comprehensive dataset of coding problems and their associated tags, sourced from real technical interviews, ensuring reliability and relevance. We propose a topic prediction approach for placement related questions, PQTD-TM (Placement question topic discovery using TF IDF and Machine Learning). The proposed technique integrates information retrieval technique (TF-IDF) with six machine learning classifiers for prediction. We evaluated the predictive performance of various machine learning models, including Logistic Regression, Decision Trees, Naive Bayes, KNN, One-vs-Rest Classifier, and Random Forest. Random forest attained the best prediction accuracy of 93\%} }
@inproceedings{10.1145/3638584.3638667, title = {Scaling Machine Learning with a Ring-based Distributed Framework}, booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence}, pages = {23--32}, year = {2024}, isbn = {9798400708688}, doi = {10.1145/3638584.3638667}, url = {https://doi.org/10.1145/3638584.3638667}, author = {Zhao, Kankan and Leng, Youfang and Zhang, Hui}, keywords = {Artificial Intelligence, Machine Learning, Parameter Server, Ring-based Distributed Framework, location = Beijing, China}, abstract = {In centralized distributed machine learning systems, communication overhead between servers and computing nodes has always been an important issue affecting the training efficiency. Although existing research has proposed various measures to reduce communication overhead between nodes in parameter server frameworks, the communication pressure and overhead inherited from centralized architectures are still significant. To address the above issue, this paper proposes a ring-based parameter server framework that is distinct from node division and model training mechanism in the standard p/s framework. The ring-based architecture cancels the global model stored on the server side, and each computing node stores a local copy of the model. During model training, computing nodes can asynchronously train local models based on local or remote training data. After all nodes finish learning, the ensemble learning method can predict test data based on all local models. To avoid the negative impact of remote data reading on model training efficiency, a producer-consumer data reading strategy is proposed. This strategy can reduce data reading overhead in a pipeline manner. To make rational use of the input and output bandwidths of all nodes, a circular data scheduling mechanism is proposed. At any given time, this mechanism ensures each node has at most one input stream and one output stream, thereby dispersing communication pressure. The experimental results show that the proposed distributed architecture achieves significantly better performance (1.7\%-2.1\% RMSE) than the state-of-the-art baselines and also achieves a 2.2x-3.4x speedup when reaching a comparable RMSE performance.} }
@inproceedings{10.1145/3576915.3623076, title = {SalsaPicante: A Machine Learning Attack on LWE with Binary Secrets}, booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security}, pages = {2606--2620}, year = {2023}, isbn = {9798400700507}, doi = {10.1145/3576915.3623076}, url = {https://doi.org/10.1145/3576915.3623076}, author = {Li, Cathy Yuanchen and Sot\'akov\'a, Jana and Wenger, Emily and Malhou, Mohamed and Garcelon, Evrard and Charton, Francois and Lauter, Kristin}, keywords = {cryptanalysis, machine learning, post-quantum cryptography, location = Copenhagen, Denmark}, abstract = {Learning with Errors (LWE) is a hard math problem underpinning many proposed post-quantum cryptographic (PQC) systems. The only PQC Key Exchange Mechanism (KEM) standardized by NIST [13] is based on module LWE [2], and current publicly available PQ Homomorphic Encryption (HE) libraries are based on ring LWE. The security of LWE-based PQ cryptosystems is critical, but certain implementation choices could weaken them. One such choice is sparse binary secrets, desirable for PQ HE schemes for efficiency reasons. Prior work SALSA[51] demonstrated a machine learning-based attack on LWE with sparse binary secrets in small dimensions (n ≤ = 128) and low Hamming weights (h ≤ = 4). However, this attack assumes access to millions of eavesdropped LWE samples and fails at higher Hamming weights or dimensions.We present PICANTE, an enhanced machine learning attack on LWE with sparse binary secrets, which recovers secrets in much larger dimensions (up to n=350) and with larger Hamming weights (roughly n/10, and up to h=60 for n=350). We achieve this dramatic improvement via a novel preprocessing step, which allows us to generate training data from a linear number of eavesdropped LWE samples (4n) and changes the distribution of the data to improve transformer training. We also improve the secret recovery methods of SALSA and introduce a novel cross-attention recovery mechanism allowing us to read off the secret directly from the trained models. While PICANTE does not threaten NIST's proposed LWE standards, it demonstrates significant improvement over SALSA and could scale further, highlighting the need for future investigation into machine learning attacks on LWE with sparse binary secrets.} }
@inproceedings{10.1145/3603287.3656163, title = {An Augmented Machine Learning-Based Course Enrollment Recommender System}, booktitle = {Proceedings of the 2024 ACM Southeast Conference}, pages = {319--320}, year = {2024}, isbn = {9798400702372}, doi = {10.1145/3603287.3656163}, url = {https://doi.org/10.1145/3603287.3656163}, author = {Zhu, Lizi and Perchyk, Oleg and Wang, Xiwei}, keywords = {Collaborative Filtering, Contextual Information, Machine Learning, Matrix Factorization, Students Demographics, location = Marietta, GA, USA}, abstract = {Higher education has been undergoing a transformation in many aspects such as course reorganization and technology adoption. Many universities keep updating their curriculum to account for changes. This, however, poses a great challenge to both students and advisors. This paper proposes a new approach to course recommender system that takes into consideration the contextual information such as students demographics and courses description.} }
@article{10.1145/3544013, title = {A Survey on Dynamic Fuzzy Machine Learning}, journal = {ACM Comput. Surv.}, volume = {55}, year = {2022}, issn = {0360-0300}, doi = {10.1145/3544013}, url = {https://doi.org/10.1145/3544013}, author = {Liu, Li and Li, Fanzhang}, keywords = {Dynamic fuzzy sets, dynamic fuzzy logic, dynamic fuzzy machine learning}, abstract = {Dynamic fuzzy characteristics are ubiquitous in a lot of scientific and engineering problems. Specifically, the physical systems and learning processes in machine learning are dynamic and fuzzy in general. This fact has driven researchers to integrate dynamic elements into fuzzy theory and proposed dynamic fuzzy sets and dynamic fuzzy logic. Based on these pioneering theoretical works and various theories for uncertain datasets, an innovative machine learning paradigm that is referred to as dynamic fuzzy machine learning (DFML) was proposed in the early 2000s. DFML extends existing fuzzy machine learning paradigms to deal with dynamic fuzzy problems in machine learning activities. This article provides an insightful overview of DFML by surveying the field from basics to advances in five aspects: (1) the theoretical basics; (2) the system and the learning model; (3) typical DFML methods and categorization of the methods; (4) the open challenges; and (5) the research frontiers. As the first survey addressing the topic, this article intends to help more researchers better understand the basics and state-of-the-art in this field, find the most appropriate tools for a particular application, and identify possible directions for future research.} }
@article{10.1145/3648682, title = {Self-adapting Machine Learning-based Systems via a Probabilistic Model Checking Framework}, journal = {ACM Trans. Auton. Adapt. Syst.}, volume = {19}, year = {2024}, issn = {1556-4665}, doi = {10.1145/3648682}, url = {https://doi.org/10.1145/3648682}, author = {Casimiro, Maria and Soares, Diogo and Garlan, David and Rodrigues, Lu\'s and Romano, Paolo}, keywords = {Self-adaptation, machine learning, model retrain, fraud detection system}, abstract = {This article focuses on the problem of optimizing the system utility of Machine Learning (ML)-based systems in the presence of ML mispredictions. This is achieved via the use of self-adaptive systems and through the execution of adaptation tactics, such as model retraining, which operate at the level of individual ML components.To address this problem, we propose a probabilistic modeling framework that reasons about the cost/benefit tradeoffs associated with adapting ML components. The key idea of the proposed approach is to decouple the problems of estimating (1) the expected performance improvement after adaptation and (2) the impact of ML adaptation on overall system utility.We apply the proposed framework to engineer a self-adaptive ML-based fraud detection system, which we evaluate using a publicly available, real fraud detection dataset. We initially consider a scenario in which information on the model’s quality is immediately available. Next, we relax this assumption by integrating (and extending) state-of-the-art techniques for estimating the model’s quality in the proposed framework. We show that by predicting the system utility stemming from retraining an ML component, the probabilistic model checker can generate adaptation strategies that are significantly closer to the optimal, as compared against baselines such as periodic or reactive retraining.} }
@inproceedings{10.1145/3704522.3704553, title = {Short Paper: Predicting and Analyzing EV Energy Consumption in Bangladesh : A Machine Learning Approach}, booktitle = {Proceedings of the 11th International Conference on Networking, Systems, and Security}, pages = {222--227}, year = {2025}, isbn = {9798400711589}, doi = {10.1145/3704522.3704553}, url = {https://doi.org/10.1145/3704522.3704553}, author = {Haque, F.M. Mahmudul and Tabassum, Humayra and Amin, Md Fazal and Jony, Md Nazrul Islam and Dey, Shamik and Quaium, Adnan}, keywords = {Machine learning, Energy, EV charging, EV, Electric Power, Bangladesh}, abstract = {The increasing adoption of EVs in Bangladesh poses challenges to the existing power grid [15]. This study develops a machine learning model to accurately predict EV energy consumption at charging stations [23]. using California City data as a proxy [8]. Evaluating algorithms like Random Forest, Gradient Boosting Regression (GBR), and Decision Tree [3] [7], Gradient Boosting outperformed with an R² of 0.9999965 and an MAE of 0.0071. Beyond predictions, this research optimizes energy management, assesses grid impacts, and supports sustainable infrastructure development, offering crucial insights for integrating EVs into Bangladesh’s transportation sector.} }
@inproceedings{10.1145/3641555.3705269, title = {Three-stage Learning with Portable Online Hands-on Labware for Quantum-based Machine Learning Development}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1619--1620}, year = {2025}, isbn = {9798400705328}, doi = {10.1145/3641555.3705269}, url = {https://doi.org/10.1145/3641555.3705269}, author = {Shi, Yong and Lo, Dan and Chi, Hongmei and Polisetty, Andrew and Suo, Kun and Nguyen, Tu}, abstract = {Quantum-based Machine Learning (QML) combines quantum computing (QC) with machine learning (ML), which can be applied in various sectors, and there is a high demand for QML professionals. However, QML is not yet in many schools' curricula. We design labware for the basic concepts of QC, ML, and QML and their applications in science and engineering fields in Google Colab, applying a three-stage learning strategy for efficient and effective student learning.} }
@inproceedings{10.1145/3746709.3746795, title = {Efficient Cross-Rack Data Updates Based on Intelligent Dynamic Data Layout Using Machine-learning}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {500--505}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746795}, url = {https://doi.org/10.1145/3746709.3746795}, author = {Song, Zengyuan and Wei, Bing and Song, Yao and Wu, Yi}, keywords = {Big data storage, Dynamic stripe distribution, Erasure-coded data update, Inter-rack traffic, Machine-learning}, abstract = {In large-scale storage systems, erasure-coded data updates often cause performance issues due to high inter-rack traffic. This paper introduces CRUD, an efficient method to reduce inter-rack traffic. CRUD uses combinatorial mathematics to create a pairwise balanced design (PBD), distributing stripes across racks to exploit data update locality. It applies a machine-learning model to classify data blocks into repeated and non-repeated updates. CRUD considers update categories, strategies, block numbers, and rack involvement to optimize stripe distribution, dynamically adjusting it to further reduce traffic. Experiments show CRUD cuts inter-rack traffic by up to 42.31\% and saves 31.95\% in update time compared to the best current method.} }
@inproceedings{10.1145/3647444.3652479, title = {Machine Learning approach for Diabetes Prediction using Pima Dataset}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652479}, url = {https://doi.org/10.1145/3647444.3652479}, author = {Moon, Pradnya Sumit and Bainalwar, Prachi A. and Borkar, Shubhangi M. and Shambharkar, Shubhangii S.}, abstract = {The rising cases of diabetes globally have called for effective prediction and early detection techniques. This study explores the use of machine learning methods to identify this condition in the PIMA diabetes dataset. The research is focused on analyzing the algorithms K-Nearest Neighbors, Logistic Regression, Decision Trees, Random Forest, and XGBoost. The objective of this study is to analyze the performance of the various algorithms used in predicting diabetes. The data collected from PIMA, which consists of diagnostic and clinical measurements, is the primary source of the study. All of the algorithms are then tested and trained on this dataset to improve their precision, F1-Score, recall, and accuracy. Moreover, cross-validation and hyperparameter tuning techniques are utilized to enhance the performance of the algorithms. The findings of this study provide valuable information on the algorithms' effectiveness when it comes to identifying diabetes. Among the tested algorithms, XGBoost performed well and consistently achieved high precision, F1-Score, recall, and accuracy. It has been concluded that this algorithm is the most suitable for identifying diabetes in the PIMA dataset. Different levels of performance were exhibited by the different algorithms. This provided a comprehensive analysis of their weaknesses and strengths in predicting diabetes. The findings of this study highlight the significance of utilizing machine learning methods in predicting diabetes. It also identified XGBoost as the best performer among the evaluated systems. The findings prove valuable in helping develop effective tools for early detection of diabetes, ultimately leading to better healthcare outcomes for those at risk.} }
@article{10.1145/3709742, title = {Sequoia: An Accessible and Extensible Framework for Privacy-Preserving Machine Learning over Distributed Data}, journal = {Proc. ACM Manag. Data}, volume = {3}, year = {2025}, doi = {10.1145/3709742}, url = {https://doi.org/10.1145/3709742}, author = {Xu, Kaiqiang and Chai, Di and Zhang, Junxue and Lai, Fan and Chen, Kai}, keywords = {compiler-executor architecture, distributed data, privacy-preserving machine learning (PPML), secure computation, training throughput}, abstract = {Privacy-preserving machine learning (PPML) algorithms use secure computation protocols to allow multiple data parties to collaboratively train machine learning (ML) models while maintaining their data confidentiality. However, current PPML frameworks couple secure protocols with ML models in PPML algorithm implementations, making it challenging for non-experts to develop and optimize PPML applications, limiting their accessibility and performance.We propose Sequoia, a novel PPML framework that decouples ML models and secure protocols to optimize the development and execution of PPML applications across data parties. Sequoia offers JAX-compatible APIs for users to program their ML models, while using a compiler-executor architecture to automatically apply PPML algorithms and system optimizations for model execution over distributed data. The compiler in Sequoia incorporates cross-party PPML processes into user-defined ML models by transparently adding computation, encryption, and communication steps with extensible policies, and the executor efficiently schedules code execution across multiple data parties, considering data dependencies and device heterogeneity.Compared to existing PPML frameworks, Sequoia requires 64\%-92\% fewer lines of code for users to implement the same PPML algorithms, and achieves 88\% speedup of training throughput in horizontal PPML.} }
@inproceedings{10.1145/3611643.3617845, title = {Detecting Overfitting of Machine Learning Techniques for Automatic Vulnerability Detection}, booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages = {2189--2191}, year = {2023}, isbn = {9798400703270}, doi = {10.1145/3611643.3617845}, url = {https://doi.org/10.1145/3611643.3617845}, author = {Risse, Niklas}, keywords = {automatic vulnerability detection, large language models, machine learning, semantic preserving transformations, location = San Francisco, CA, USA}, abstract = {Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function f, models trained by machine learning techniques can decide if f contains a security flaw with up to 70\% accuracy. But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model’s accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels. In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose a cross validation algorithm, where a semantic preserving transformation is applied during the amplification of either the training set or the testing set. Using 11 transformations and 3 ML techniques, we find that the improved robustness only applies to the specific transformations used during training data amplification. In other words, the robustified models still rely on unrelated features for predicting the vulnerabilities in the testing data.} }
@inproceedings{10.1145/3676536.3697114, title = {Co-Designing NVM-based Systems for Machine Learning and In-memory Search Applications}, booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design}, year = {2025}, isbn = {9798400710773}, doi = {10.1145/3676536.3697114}, url = {https://doi.org/10.1145/3676536.3697114}, author = {Henkel, J\"org and Siddhu, Lokesh and Nassar, Hassan and Bauer, Lars and Chen, Jian-Jia and Hakert, Christian and Seidl, Tristan and Chen, Kuan Hsun and Hu, Xiaobo Sharon and Li, Mengyuan and Yang, Chia-Lin and Wei, Ming-Liang}, abstract = {With the rapid development of the Internet of Things, machine learning applications on edge devices with limited resources face challenges due to large data scales and irregular memory access patterns. Non-volatile memory (NVM) technologies provide promising solutions by offering larger capacity, low leakage power, and data persistence. In this paper, we discuss the potential of NVM technology in enhancing machine learning applications by improving energy efficiency and reducing latency through in-memory computation and different NVM write modes. The insights from this analysis provide valuable guidance to device researchers and system architects working to develop highperformance systems for machine learning and accelerators in large-scale search applications using NVMs.} }
@inproceedings{10.1145/3743642.3743649, title = {Invited Paper: Rethinking Benchmarks for Parallel Machine Learning Techniques: Integrating Qualitative and Quantitative Evaluation Metrics}, booktitle = {Proceedings of the 7th Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems}, pages = {1--10}, year = {2025}, isbn = {9798400720062}, doi = {10.1145/3743642.3743649}, url = {https://doi.org/10.1145/3743642.3743649}, author = {Bahbouh, Abdulfatah and Ahmad, Ishfaq}, keywords = {Benchmarking, Parallel Machine Learning, Scalability, Sustainability, location = Huatulco, Mexico}, abstract = {Growing model complexity and data volumes in Machine Learning (ML), especially Deep Learning (DL), necessitate parallel processing for efficient, scalable computation. Benchmarking is critical for evaluating parallel ML techniques. Current methodologies predominantly emphasize quantitative metrics like throughput and accuracy, exemplified by MLPerf and HPC-AI500. Such quantitative focus neglects key qualitative factors-portability, robustness, deployment complexity, and usefulness-essential for practical parallel ML. This paper argues for a paradigm shift in benchmarking, advocating for a holistic evaluation framework that inherently integrates qualitative assessments alongside traditional quantitative measures. To this end, we introduce NNoPP (Neural Network on-top-of Parallel Processing), a structured evaluation approach designed to complement existing benchmarks. NNoPP proposes criteria---novelty, portability, performance, scalability, complexity, usefulness, and sustainability---methodically defined for real-world ML solution viability, sustainability, and impact. Beyond performance numbers, these metrics promote sustainable solutions: computationally efficient, practically deployable, broadly applicable, and robust across diverse contexts. Through illustrative case studies across key DL model families, we demonstrate the application of NNoPP and its capacity to provide nuanced insights beyond conventional benchmark rankings. Future directions for parallel ML benchmarking emphasize integrated metrics, dynamic workloads, domain-specific relevance, and collaborative community evolution. Ultimately, this paper emphasizes a re-envisioned holistic benchmarking approach that fosters the adoption of parallel ML solutions that are both quantitatively superior and qualitatively robust, useful, and sustainably impactful for continuous advancement.} }
@inproceedings{10.1145/3723178.3723179, title = {Machine Learning-Enhanced Cross-Tier Security and Anomaly Detection in Wireless Body Area Networks}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {1--8}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723179}, url = {https://doi.org/10.1145/3723178.3723179}, author = {Shamshuzzoha, Md and Islam, Md. Motaharul}, keywords = {Machine Learning, Cross-Tier Security, Wireless Body area Networks, secured Healthcare, Anomaly Detection}, abstract = {Nowadays, in the dynamic landscape of e-health, the relentless pace of information technology advances brings both transformative possibilities and heightened security concerns, particularly within WBANs. At the forefront of this challenge is the critical imperative to fortify security measures across healthcare Internet of Things environments. This research addresses this pressing issue by championing a pioneering three-layered defense system, thoughtfully integrating the potency of machine learning techniques. The paramount emphasis is on achieving cross-tier security, strategically targeting the device, Hub, and Cloud layers. The groundbreaking SenseGuard anomaly detection system, fueled by sophisticated machine learning algorithms, not only elevates security at the Device layer but strategically reinforces patient wellness and the entire WBAN network. Simultaneously, an innovative intrusion detection algorithm fortifies the Hub layer, while adaptive machine learning models in the Cloud layer ensure a comprehensive defense. This approach provides an effective solution to the security challenges inherent in healthcare IoT as a remarkable achievement in advancing the state-of-the-art, placing cross-tier security at the forefront of WBAN innovation.} }
@article{10.1145/3702646, title = {Exploring Dataset Bias and Scaling Techniques in Multi-Source Gait Biomechanics: An Explainable Machine Learning Approach}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {16}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3702646}, url = {https://doi.org/10.1145/3702646}, author = {Fleischmann, Sophie and Dietz, Simon and Shanbhag, Julian and Wuensch, Annika and Nitschke, Marlies and Miehling, J\"org and Wartzack, Sandro and Leyendecker, Sigrid and Eskofier, Bjoern M. and Koelewijn, Anne D.}, keywords = {datasets, dataset combination, neural networks, explainable AI, scaling, biomechanics, motion capture, machine learning, LRP}, abstract = {Machine learning has become increasingly important in biomechanics. It allows to unveil hidden patterns from large and complex data, which leads to a more comprehensive understanding of biomechanical processes and deeper insights into human movement. However, machine learning models are often trained on a single dataset with a limited number of participants, which negatively affects their robustness and generalizability. Combining data from multiple existing sources provides an opportunity to overcome these limitations without spending more time on recruiting participants and recording new data. It is furthermore an opportunity for researchers who lack the financial requirements or laboratory equipment to conduct expensive motion capture studies themselves. At the same time, subtle interlaboratory differences can be problematic in an analysis due to the bias that they introduce. In our study, we investigated differences in motion capture datasets in the context of machine learning, for which we combined overground walking trials from four existing studies. Specifically, our goal was to examine whether a machine learning model was able to predict the original data source based on marker and GRF trajectories of single strides and how different scaling methods and pooling procedures affected the outcome. Layer-wise relevance propagation was applied to understand which factors were influential to distinguish the original data sources. We found that the model could predict the original data source with a very high accuracy (up to (gt) 99\%), which decreased by about 15 percentage points when we scaled every dataset individually prior to pooling. However, none of the proposed scaling methods could fully remove the dataset bias. Layer-wise relevance propagation revealed that there was not only one single factor that differed between all datasets. Instead, every dataset had its unique characteristics that were picked up by the model. These variables differed between the scaling and pooling approaches but were mostly consistent between trials belonging to the same dataset. Our results show that motion capture data is sensitive even to small deviations in marker placement and experimental setup and that small inter-group differences should not be overinterpreted during data analysis, especially when the data was collected in different labs. Furthermore, we recommend scaling datasets individually prior to pooling them which led to the lowest accuracy. We want to raise awareness that differences in datasets always exist and are recognizable by machine learning models. Researchers should thus think about how these differences might affect their results when combining data from different studies.} }
@inproceedings{10.1145/3589883.3589886, title = {Energy-aware Tiny Machine Learning for Sensor-based Hand-washing Recognition}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {15--22}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589886}, url = {https://doi.org/10.1145/3589883.3589886}, author = {Lattanzi, Emanuele and Calisti, Lorenzo}, keywords = {Energy aware, Human Activity Recognition, Long Short-Term Memory networks, Support Vector Machine, Tiny Machine Learning, location = Stockholm, Sweden}, abstract = {Tiny wearable devices are nowadays one of the most popular and used devices in everyday life. At the same time, machine learning techniques have reached a level of maturity such that they can be used in the most varied fields. The union of these two technologies represents a valuable opportunity for the development of pervasive computing applications. On the other hand, pushing the machine learning inference on a wearable device can lead to nontrivial issues. In fact, devices with small size and low-energy availability, like those dedicated to wearable platforms, pose strict computational, memory, and power requirements which result in challenging issues to be addressed by designers. The main purpose of this study is to empirically explore the trade-off between energy consumption and classification accuracy of a machine learning-based hand-washing recognition task deployed on a real wearable device. Through extensive experimental results, obtained on a public human activity recognition dataset, we demonstrated that given an identical level of classification performance, a classic SVM classifier can save about 40\% of energy with respect to a more complex LSTM network. Moreover, reducing the LSTM complexity, by lowering the number of its internal unit, can make the LSTM network energy cost-effective (with a savings of about 30\%) at the cost of a reduction in accuracy of only 2\%.} }
@inproceedings{10.1145/3669721.3674555, title = {Data-Driven Modeling of Miniature Hall Thrusters: A Machine Learning Approach}, booktitle = {Proceedings of the 2024 3rd International Symposium on Intelligent Unmanned Systems and Artificial Intelligence}, pages = {216--220}, year = {2024}, isbn = {9798400710025}, doi = {10.1145/3669721.3674555}, url = {https://doi.org/10.1145/3669721.3674555}, author = {Zine el abidine, Hebboul and Tang, Hai-Bin and Wang, Zixiang}, abstract = {New data-driven and physics-based modeling approaches have been applied synergistically to advance the design of sub-kilowatt Hall thrusters. An extensive literature database was compiled and cleaned using a custom program to impute missing values resulting from patents and confidentiality restrictions. Generative adversarial networks (GANs) augmented the limited dataset, covering various thruster geometries and operating conditions. A correlation analysis identified the most influential performance parameters, which served as inputs to a surrogate artificial neural network (ANN) model for rapid design prediction. The ANN predictions were closely aligned with conventional linear regression models, thus validating the approach. Numerical simulations of the ANN-generated designs demonstrated accurate performance projections, highlighting the potential of these new machine-learning techniques in the design of electric propulsion systems. This framework synergized data resources, machine learning models, and high-fidelity simulations to realize next-generation, low-power Hall thrusters.} }
@inproceedings{10.1145/3477495.3531678, title = {Table Enrichment System for Machine Learning}, booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {3267--3271}, year = {2022}, isbn = {9781450387323}, doi = {10.1145/3477495.3531678}, url = {https://doi.org/10.1145/3477495.3531678}, author = {Dong, Yuyang and Oyamada, Masafumi}, keywords = {machine learning, table augmentation, table discovery, table enrichment, location = Madrid, Spain}, abstract = {Data scientists are constantly facing the problem of how to improve prediction accuracy with insufficient tabular data. We propose a table enrichment system that enriches a query table by adding external attributes (columns) from data lakes and improves the accuracy of machine learning predictive models. Our system has four stages, join row search, task-related table selection, row and column alignment, and feature selection and evaluation, to efficiently create an enriched table for a given query table and a specified machine learning task. We demonstrate our system with a web UI to show the use cases of table enrichment.} }
@inproceedings{10.1145/3529399.3529400, title = {Machine Learning in Textual Criticism: An examination of the performance of supervised machine learning algorithms in reconstructing the text of the Greek New Testament}, booktitle = {Proceedings of the 2022 7th International Conference on Machine Learning Technologies}, pages = {1--5}, year = {2022}, isbn = {9781450395748}, doi = {10.1145/3529399.3529400}, url = {https://doi.org/10.1145/3529399.3529400}, author = {Jones, Mason and Romano, Francesco and Mohd, Abidalrahman}, keywords = {Koine Greek, New Testament, Textual criticism, location = Rome, Italy}, abstract = {New Testament textual criticism is a field of research that seeks to confidently establish the text of the New Testament by comparing thousands of manuscripts, finding the variants between these manuscripts, and making informed decisions as to the original text based on the internal and external features of those manuscripts. By examining such features as date of composition, textual family, and variant length, scholars are able to determine the correct reading of a text with a high degree of confidence. The use of computing in this field has been documented since at least the 1970’s, but they have not been applied to the task of textual criticism itself. Rather, computers have been used primarily to classify manuscripts and determine their relationships to each other. Our research in this paper takes a new approach by applying machine learning algorithms to the task of textual criticism. After testing multiple supervised learning algorithms, our research finds that support vector machines and decision trees outperform the other tested and achieve 98.8\% accuracy.} }
@article{10.1145/3652918, title = {Unveiling Patterns of the Earth through Machine Learning and Geospatial Analysis}, journal = {XRDS}, volume = {30}, pages = {32--33}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3652918}, url = {https://doi.org/10.1145/3652918}, author = {Li, Jiayi and Klemmer, Konstantin}, abstract = {Konstantin Klemmer is a researcher at Microsoft Research New England, where he works on the representation of geospatial phenomena in machine learning methods.} }
@inproceedings{10.1145/3522664.3528620, title = {Code smells for machine learning applications}, booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI}, pages = {217--228}, year = {2022}, isbn = {9781450392754}, doi = {10.1145/3522664.3528620}, url = {https://doi.org/10.1145/3522664.3528620}, author = {Zhang, Haiyin and Cruz, Lu\'s and van Deursen, Arie}, keywords = {anti-pattern, code quality, code smell, machine learning, technical debt, location = Pittsburgh, Pennsylvania}, abstract = {The popularity of machine learning has wildly expanded in recent years. Machine learning techniques have been heatedly studied in academia and applied in the industry to create business value. However, there is a lack of guidelines for code quality in machine learning applications. In particular, code smells have rarely been studied in this domain. Although machine learning code is usually integrated as a small part of an overarching system, it usually plays an important role in its core functionality. Hence ensuring code quality is quintessential to avoid issues in the long run. This paper proposes and identifies a list of 22 machine learning-specific code smells collected from various sources, including papers, grey literature, GitHub commits, and Stack Overflow posts. We pinpoint each smell with a description of its context, potential issues in the long run, and proposed solutions. In addition, we link them to their respective pipeline stage and the evidence from both academic and grey literature. The code smell catalog helps data scientists and developers produce and maintain high-quality machine learning application code.} }
@article{10.1145/3648608, title = {Resilient Machine Learning: Advancement, Barriers, and Opportunities in the Nuclear Industry}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3648608}, url = {https://doi.org/10.1145/3648608}, author = {Khadka, Anita and Sthapit, Saurav and Epiphaniou, Gregory and Maple, Carsten}, keywords = {Resilient machine learning, nuclear industry, adversaries, defences, resilience, robustness, survey}, abstract = {The widespread adoption and success of Machine Learning (ML) technologies depend on thorough testing of the resilience and robustness to adversarial attacks. The testing should focus on both the model and the data. It is necessary to build robust and resilient systems to withstand disruptions and remain functional despite the action of adversaries, specifically in the security-sensitive Nuclear Industry (NI), where consequences can be fatal in terms of both human lives and assets. We analyse ML-based research works that have investigated adversaries and defence strategies in the NI. We then present the progress in the adoption of ML techniques, identify use cases where adversaries can threaten the ML-enabled systems, and finally identify the progress on building Resilient Machine Learning (rML) systems entirely focusing on the NI domain.} }
@inproceedings{10.1145/3656019.3676952, title = {FriendlyFoe: Adversarial Machine Learning as a Practical Architectural Defense against Side Channel Attacks}, booktitle = {Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques}, pages = {338--350}, year = {2024}, isbn = {9798400706318}, doi = {10.1145/3656019.3676952}, url = {https://doi.org/10.1145/3656019.3676952}, author = {Nam, Hyoungwook and Pothukuchi, Raghavendra Pradyumna and Li, Bo and Kim, Nam Sung and Torrellas, Josep}, keywords = {Hardware security, Machine learning, Side-channel analysis, location = Long Beach, CA, USA}, abstract = {Machine learning (ML)-based side channel attacks have become prominent threats to computer security. These attacks are often powerful, as ML models easily find patterns in signals. To address this problem, this paper proposes dynamically applying Adversarial Machine Learning (AML) to obfuscate side channels. The rationale is that it has been shown that intelligently injecting an adversarial perturbation can confuse ML classifiers. We call this approach FriendlyFoe and the neural network we introduce to perturb signals FriendlyFoe Defender. FriendlyFoe is a practical, effective, and general architectural technique to obfuscate signals. We show a workflow to design Defenders with low overhead and information leakage, and to customize them for different environments. Defenders are transferable, i.e., they thwart attacker classifiers that are different from those used to train the Defenders. They also resist adaptive attacks, where attackers train using the obfuscated signals collected while the Defender is active. Finally, the approach is general enough to be applicable to different environments. We demonstrate FriendlyFoe against two side channel attacks: one based on memory contention and one on system power. The first example uses a hardware Defender with ns-level response time that, for the same level of security as a Pad-to-Constant scheme, has 27\% and 64\% lower performance overhead for single- and multi-threaded workloads, respectively. The second example uses a software Defender with ms-level response time that reduces leakage by 3.7 over a state-of-the-art scheme while reducing the energy overhead by 22.5\%.} }
@inproceedings{10.1145/3625343.3625344, title = {Practical and Efficient Secure Aggregation for Privacy-Preserving Machine Learning}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625344}, url = {https://doi.org/10.1145/3625343.3625344}, author = {Zhang, Yuqi and Li, Xiangyang and Luo, Qingcai and Wang, Yang and Shen, Yanzhao}, keywords = {Federal Learning, Homomorphic Encryption, Information Security, Secure Aggregation, Secure Multi-party Computation, location = Bangkok, Thailand}, abstract = {In recent years, Federal Learning has received much attention becourse it can train models by updating gradients without contacting users’ true data. However, adversaries also can track users’ privacy from the shared gradient. In this paper, we aim to solve three major issues during the process of federated learning: 1) how to protect users’ privacy during training; 2) How to verify the correctness of the aggregation results returned from the server; 3) How to reduce communication costs while ensuring training security. So we propose a verifiable aggregation scheme that can effectively verify the results of server aggregation. Specifically, we follow the classic double mask aggregation scheme, and use Paillier homomorphic encryption algorithm to implement the message authentication code with additive homomorphic property. Users can compare their local codes with server’s aggregation results to verify the correctness of the aggregation results and improve model’s accuracy. In our framework, we adopt a Top-k gradient selection scheme to reduce models’ communication and computing overhead. Experimental results indicate that our training framework is feasible and efficient.} }
@inproceedings{10.1145/3626252.3630759, title = {A Fast and Accurate Machine Learning Autograder for the Breakout Assignment}, booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1}, pages = {736--742}, year = {2024}, isbn = {9798400704239}, doi = {10.1145/3626252.3630759}, url = {https://doi.org/10.1145/3626252.3630759}, author = {Liu, Evan Zheran and Yuan, David and Ahmed, Ahmed and Cornwall, Elyse and Woodrow, Juliette and Burns, Kaylee and Nie, Allen and Brunskill, Emma and Piech, Chris and Finn, Chelsea}, keywords = {autograder, cs1, feedback, grading support, graphics, machine learning, location = Portland, OR, USA}, abstract = {In this paper, we detail the successful deployment of a machine learning autograder that significantly decreases the grading labor required in the Breakout computer science assignment. This assignment - which tasks students with programming a game consisting of a controllable paddle and a ball that bounces off the paddle to break bricks - is popular for engaging students with introductory computer science concepts, but creates a large grading burden. Due to the game's interactive nature, grading defies traditional unit tests and instead typically requires 8+ minutes of manually playing each student's game to search for bugs. This amounts to 45+ hours of grading in a standard course offering and prevents further widespread adoption of the assignment. Our autograder alleviates this burden by playing each student's game with a reinforcement learning agent and providing videos of discovered bugs to instructors. In an A/B test with manual grading, we find that our human-in-the-loop AI autograder reduces grading time by 44\%, while slightly improving grading accuracy by 6\%, ultimately saving roughly 30 hours over our deployment in two offerings of the assignment. Our results further suggest the practicality of grading other interactive assignments (e.g., other games or building websites) via similar machine learning techniques. Live demo at https://ezliu.github.io/breakoutgrader.} }
@inproceedings{10.1145/3654522.3654577, title = {ChainSniper: A Machine Learning Approach for Auditing Cross-Chain Smart Contracts}, booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology}, pages = {223--230}, year = {2024}, isbn = {9798400716713}, doi = {10.1145/3654522.3654577}, url = {https://doi.org/10.1145/3654522.3654577}, author = {Tran, Tuan-Dung and Vo, Kiet Anh and Phan, Duy The and Tan, Cam Nguyen and Pham, Van-Hau}, keywords = {Cross-chain, Machine Learning, Smart Contract, Vulnerability, location = Ho Chi Minh City, Vietnam}, abstract = {Smart contracts are autonomous programs stored on blockchain networks that self-execute agreed terms in a transparent and accurate manner. Within cross-chain platforms, smart contracts facilitate interaction and exchange of data between diverse blockchains. However, the presence of vulnerabilities in smart contracts renders them susceptible to exploitation, jeopardizing security. Considerable research has focused on identifying and detecting such vulnerabilities, though existing approaches have yet to achieve comprehensive coverage. This paper presents ChainSniper, a sidechain-based framework integrating machine learning to automatically appraise vulnerabilities in cross-chain smart contracts. A comprehensive dataset, denoted "CrossChainSentinel", was compiled comprising 300 manually labeled code snippets. This dataset was leveraged to train machine learning models discerning vulnerable versus secure smart contracts. Experimental findings demonstrate the viability of machine learning methodologies for enhancing smart contract auditing within decentralized applications spanning multiple networks. Notable detection precision was achieved, substantiating ChainSniper’s potential to strengthen security analysis through an automated and expansive evaluation of smart contract code.} }
@inproceedings{10.1145/3671127.3699688, title = {Improving Cyber-Physical Building Energy System via Large-Scale Machine Learning Evaluation}, booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {262--263}, year = {2024}, isbn = {9798400707063}, doi = {10.1145/3671127.3699688}, url = {https://doi.org/10.1145/3671127.3699688}, author = {Deng, Yang}, keywords = {ML evaluation, automation, cyber-physical, smart building, location = Hangzhou, China}, abstract = {Machine learning (ML) is playing a crucial role in almost every business sector. In the building automation sector, the Industries (e.g., Siemens, Honeywell) depend on AI to introduce new services, e.g., ML-based HVAC control. Despite recent advances in this area and many ML-based solutions have been developed, however, the building practitioner argued that published ML research is sometimes not driving sufficient real-world impact and is hard to deploy. This triggered us to promote ML deployment through evaluation instead of developing new ML models. In our current work, we propose a behavior testing methodology to systematically evaluate the ML building energy forecasting model; and two data augmentation solutions to solve the data shortage problem while testing ML models; and we developed a visualization tool contains these research outputs, which is user-friendly to the practitioners.} }
@inproceedings{10.1145/3703847.3703885, title = {Research on Wearable Sensors Data Collection and Health Assessment based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {227--231}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703885}, url = {https://doi.org/10.1145/3703847.3703885}, author = {Chen, Runhua and Liu, Qigang and Chen, Zinuo}, keywords = {Health Assessment, Machine Learning, Self-attention Mechanism, Wearable Sensors}, abstract = {With the rapid development of economy and culture, more and more people are starting to use wearable devices for health monitoring. However, as the health monitoring industry is still in its infancy, there are many problems in the professional knowledge, skill reserves, equipment use and logistics support of users. These issues lead to suboptimal health monitoring and can pose a threat to the health and safety of users. In this work, the health assessment of wearable sensor data acquisition and machine learning studied the discrimination method of integrating multi-source sensor signals and expert experience, and designed a two-stage anomaly recognition scheme. In the first stage, the discrimination rules containing expert experience were used to complete the preliminary discrimination of anomalies, and in the research of expert knowledge base, the representation and use of expert experience in wearable systems were discussed, and the construction method of anomaly discrimination knowledge base based on problems was designed and implemented. The Transformer model is used to process multi-source sensor signals, and its self-attention mechanism and timing expression characteristics are fully utilized, which greatly reduces the false negative rate of abnormal states. Experimental analysis shows that the proposed method significantly improves the accuracy and reliability of health assessment.} }
@inproceedings{10.1145/3632410.3633290, title = {Unveiling Graph Structures for Machine Learning: Learning, Structuring, and Coarsening}, booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)}, pages = {484--488}, year = {2024}, isbn = {9798400716348}, doi = {10.1145/3632410.3633290}, url = {https://doi.org/10.1145/3632410.3633290}, author = {Kumar, Manoj and Kumar, Sandeep}, keywords = {Graph learning, graph coarsening, node and classification, structured graph learning, location = Bangalore, India}, abstract = {Graph structure is an important element in the realm of machine learning tasks. This tutorial centers on the art of deriving graph representations from data. It unfolds through three pivotal themes: Each theme is dissected thoroughly, combining theory with practical application. The tutorial demonstrates how these graph representations drive various applications, including clustering, node and graph classification, and edge prediction. In essence, this tutorial arms participants with the tools to unleash the potential of graph structures in the realm of machine learning.} }
@proceedings{10.1145/3630050, title = {SAFE '23: Proceedings of the 2023 on Explainable and Safety Bounded, Fidelitous, Machine Learning for Networking}, year = {2023}, isbn = {9798400704499}, abstract = {It is with great pleasure that we welcome you to the 2023 ACM CoNEXT Workshop on 'Explainable and Safety Bounded, Fidelitous, Machine Learning for Networking' - SAFE'23. We are excited to be hosting the first edition of this workshop, and it brings us pleasure to see the growing interest and enthusiasm surrounding the convergence of machine learning and networking. Machine learning offers promising solutions for network optimization, security, and management. Control and decision-making algorithms are critical for the operation of networks, hence we believe that the solutions should be safety bounded and interpretable. Understanding the decisions and behaviors of machine learning models is crucial for optimizing network performance, enhancing security, and ensuring reliable network operations. This is a very crucial topic which needs to be addressed, as network operators, managers or administrators are reluctant to use ML based solutions which are black box in nature. The production networks have a critical and sensitive nature, where outages or performance degradations can be very costly.} }
@inproceedings{10.1145/3603719.3603726, title = {Accelerating Machine Learning Queries with Linear Algebra Query Processing}, booktitle = {Proceedings of the 35th International Conference on Scientific and Statistical Database Management}, year = {2023}, isbn = {9798400707469}, doi = {10.1145/3603719.3603726}, url = {https://doi.org/10.1145/3603719.3603726}, author = {Sun, Wenbo and Katsifodimos, Asterios and Hai, Rihan}, keywords = {database, machine learning, operator fusion, query optimization, location = Los Angeles, CA, USA}, abstract = {The rapid growth of large-scale machine learning (ML) models has led numerous commercial companies to utilize ML models for generating predictive results to help business decision-making. As two primary components in traditional predictive pipelines, data processing, and model predictions often operate in separate execution environments, leading to redundant engineering and computations. Additionally, the diverging mathematical foundations of data processing and machine learning hinder cross-optimizations by combining these two components, thereby overlooking potential opportunities to expedite predictive pipelines. In this paper, we propose an operator fusing method based on GPU-accelerated linear algebraic evaluation of relational queries. Our method leverages linear algebra computation properties to merge operators in machine learning predictions and data processing, significantly accelerating predictive pipelines by up to 317x. We perform a complexity analysis to deliver quantitative insights into the advantages of operator fusion, considering various data and model dimensions. Furthermore, we extensively evaluate matrix multiplication query processing utilizing the widely-used Star Schema Benchmark. Through comprehensive evaluations, we demonstrate the effectiveness and potential of our approach in improving the efficiency of data processing and machine learning workloads on modern hardware.} }
@inproceedings{10.1145/3669754.3669781, title = {Leukaemia Detection and Classification of its Sub-types using Machine Learning}, booktitle = {Proceedings of the 2024 10th International Conference on Computing and Artificial Intelligence}, pages = {180--183}, year = {2024}, isbn = {9798400717055}, doi = {10.1145/3669754.3669781}, url = {https://doi.org/10.1145/3669754.3669781}, author = {V, Harsha and P R, Chandan and C, Niharika and Siddesh Loni, Ankit and U, Purushotham}, keywords = {Additional Key Words and Phrases: Machine Learning, Leukaemia Diagnosis, Vision Transformers, location = Bali Island, Indonesia}, abstract = {Leukemia, a group of blood cancers originating from abnormal white blood cells as show in Fig 1, poses a significant threat to global health. Early detection and accurate classification of leukemia subtypes are crucial for effective treatment planning and patient management. This research focuses on developing a comprehensive approach for leukemia detection and subtype classification using advanced machine learning techniques. The objectives of this research are to efficiently detect leukemia and classify its subtypes using machine learning techniques. The successful implementation of this research project has the potential to significantly improve the efficiency of leukemia diagnosis by providing a reliable, automated system for early detection and accurate classification of leukemia subtypes. This, in turn, can contribute to more timely and targeted treatment strategies, ultimately improving patient outcomes in the battle against leukemia.} }
@inproceedings{10.1145/3691620.3695258, title = {MLOLET - Machine Learning Optimized Load and Endurance Testing: An industrial experience report}, booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering}, pages = {1956--1966}, year = {2024}, isbn = {9798400712487}, doi = {10.1145/3691620.3695258}, url = {https://doi.org/10.1145/3691620.3695258}, author = {Vitui, Arthur and Chen, Tse-Hsun}, abstract = {Load testing is essential for ensuring the performance and stability of modern large-scale systems, which must handle vast numbers of concurrent requests. Traditional load tests, often requiring extensive execution times, are costly and impractical within the short release cycles typical of contemporary software development. In this paper, we present our experience deploying MLOLET, a machine learning optimized load testing framework, at Ericsson. MLOLET addresses key challenges in load testing by determining early stop points for tests and forecasting throughput and response time trends in production environments. By training a time-series model on key performance indicators (KPIs) collected from load tests, MLOLET enables early detection of abnormal system behavior and provides accurate performance forecasting. This capability allows load test engineers to make informed decisions on resource allocation, enhancing both testing efficiency and system reliability. We document the design of MLOLET, its application in industrial settings, and the feedback received from its implementation, highlighting its impact on improving load testing processes and operational performance.} }
@article{10.1145/3688086, title = {Harnessing Machine Learning and Generative AI: A New Era in Online Tutoring Systems}, journal = {XRDS}, volume = {31}, pages = {40--45}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3688086}, url = {https://doi.org/10.1145/3688086}, author = {Schmucker, Robin}, abstract = {Discover how the convergence of machine learning and generative AI is revolutionizing online tutoring, enabling systems that evolve to become better teachers--continuously refining their instructional methods based on student data and feedback.} }
@inproceedings{10.1145/3592149.3592157, title = {O-RAN with Machine Learning in ns-3}, booktitle = {Proceedings of the 2023 Workshop on Ns-3}, pages = {60--68}, year = {2023}, isbn = {9798400707476}, doi = {10.1145/3592149.3592157}, url = {https://doi.org/10.1145/3592149.3592157}, author = {Garey, Wesley and Ropitault, Tanguy and Rouil, Richard and Black, Evan and Gao, Weichao}, keywords = {LTE, Machine Learning, Mobile Networks, Modeling and Simulation, O-RAN, ONNX, ns-3, location = Arlington, VA, USA}, abstract = {The Open Radio Access Network (O-RAN) Alliance is an industry-led standardization effort, with the main objective of evolving the Radio Access Network (RAN) to be open, intelligent, interoperable, and autonomous to support the ever growing need of improved performance and flexibility in mobile networks. This paper introduces an extension to Network Simulator 3 (ns-3) which mimics the behavior and components of the O-RAN Alliance’s O-RAN architecture. In this paper, we will describe the O-RAN architecture, our model in ns-3, and a Long Term Evolution (LTE) case study that utilizes Machine Learning (ML) and its integration with ns-3. At the end of this paper, the reader will have a general understanding of O-RAN and the capabilities of our fully simulated contribution so it can be leveraged to design and evaluate O-RAN-based solutions.} }
@inproceedings{10.1145/3577193.3593710, title = {CMLCompiler: A Unified Compiler for Classical Machine Learning}, booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing}, pages = {63--74}, year = {2023}, isbn = {9798400700569}, doi = {10.1145/3577193.3593710}, url = {https://doi.org/10.1145/3577193.3593710}, author = {Wen, Xu and Gao, Wanling and Li, Anzheng and Wang, Lei and Jiang, Zihan and Zhan, Jianfeng}, keywords = {classical machine learning, deep learning, compiler, location = Orlando, FL, USA}, abstract = {Classical machine learning (CML) occupies nearly half of machine learning pipelines in production applications. Unfortunately, it fails to utilize the state-of-the-practice devices fully and performs poorly. Without a unified framework, the hybrid deployments of deep learning (DL) and CML also suffer from severe performance and portability issues. This paper presents the design of a unified compiler, called CMLCompiler, for CML inference. We propose two unified abstractions: operator representations and extended computational graphs. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to DL compilers or frameworks. We implement CMLCompiler on TVM. The evaluation shows CMLCompiler's portability and superior performance. It achieves up to 4.38 speedup on CPU, 3.31 speedup on GPU, and 5.09 speedup on IoT devices, compared to the state-of-the-art solutions --- scikit-learn, intel sklearn, and hummingbird. Our performance of CML and DL mixed pipelines achieves up to 3.04x speedup compared with cross-framework implementations. The project documents and source code are available at https://www.computercouncil.org/cmlcompiler.} }
@inproceedings{10.1145/3563766.3564115, title = {Congestion control in machine learning clusters}, booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks}, pages = {235--242}, year = {2022}, isbn = {9781450398992}, doi = {10.1145/3563766.3564115}, url = {https://doi.org/10.1145/3563766.3564115}, author = {Rajasekaran, Sudarsanan and Ghobadi, Manya and Kumar, Gautam and Akella, Aditya}, keywords = {DNN training, congestion control, datacenters for ML, networks for ML, resource allocation, transport layer, location = Austin, Texas}, abstract = {This paper argues that fair-sharing, the holy grail of congestion control algorithms for decades, is not necessarily a desirable property in Machine Learning (ML) training clusters. We demonstrate that for a specific combination of jobs, introducing unfairness improves the training time for all competing jobs. We call this specific combination of jobs compatible and define the compatibility criterion using a novel geometric abstraction. Our abstraction rolls time around a circle and rotates the communication phases of jobs to identify fully compatible jobs. Using this abstraction, we demonstrate up to 1.3 improvement in the average training iteration time of popular ML models. We advocate that resource management algorithms should take job compatibility on network links into account. We then propose three directions to ameliorate the impact of network congestion in ML training clusters: (i) an adaptively unfair congestion control scheme, (ii) priority queues on switches, and (iii) precise flow scheduling.} }
@inproceedings{10.1145/3723178.3723280, title = {Skin Cancer Detection: Leveraging Hybrid Deep Learning Models and Traditional Machine Learning Classifiers}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {770--778}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723280}, url = {https://doi.org/10.1145/3723178.3723280}, author = {Nusrat Prome, Jannatun and Sultana, Fariha and Anika, Saraf}, keywords = {Deep learning, K-Nearest Neighbors, Machine Learning, MobileNet, Random Forest, ResNet-50, ShuffleNet, Skin Cancer Detection, Support Vector Machine, VGG16, and DenseNet-201}, abstract = {Skin cancer is a significant worldwide health concern, necessitating efficient observation and treatment methods. We explore the potential of deep neural network techniques to automate skin cancer identification, addressing challenges like classification complexity and dataset scarcity. The research makes use of two publicly accessible datasets of skin cancer photographs, featuring 3311 and 5000 photos, respectively. Our approach combines established deep learning models like ResNet-50, VGG16, MobileNet, ShuffleNet and DenseNet-201 with novel hybrid structures to leverage their complementary strengths. The methodology initiated with data pre-processing, encompassing steps such as data splitting, normalization, reshaping, and encoding. Augmentation techniques were subsequently applied to augment the dataset volume. Following pre-processing, traditional deep learning models were independently evaluated from scratch. Subsequently, these models were re-evaluated in conjunction with machine learning classifiers. Hybrid models were then constructed from the individual models and subjected to assessment. Additionally, hybrid models paired with Support Vector Machine (SVM) classifiers were assessed. Throughout the experimentation, the Adam optimizer consistently demonstrated effective performance. Notably, SVM classifiers exhibited superior performance among the classifiers measured. Accuracy-Loss plots, Confusion Matrix, and Receiver Operating Characteristic (ROC) curves were used for a comprehensive analysis. The suggested hybrid model of VGG16 and ResNet-50 with SVM classifier outperformed the conventional DL models and ML classifiers during the anatomizing process with consistent and dependable performance. Accordingly, the suggested model demonstrated accuracy on the corresponding datasets of 90.64\% and 93.61\%, indicating its capability as a tool aimed at skin cancer early detection} }
@article{10.1145/3699711, title = {A Systematic Literature Review on Automated Software Vulnerability Detection Using Machine Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3699711}, url = {https://doi.org/10.1145/3699711}, author = {Shiri Harzevili, Nima and Boaye Belle, Alvine and Wang, Junjie and Wang, Song and Jiang, Zhen Ming (Jack) and Nagappan, Nachiappan}, keywords = {Source code, software security, software vulnerability detection, software bug detection, machine learning, deep learning}, abstract = {In recent years, numerous Machine Learning (ML) models, including Deep Learning (DL) and classic ML models, have been developed to detect software vulnerabilities. However, there is a notable lack of comprehensive and systematic surveys that summarize, classify, and analyze the applications of these ML models in software vulnerability detection. This absence may lead to critical research areas being overlooked or under-represented, resulting in a skewed understanding of the current state of the art in software vulnerability detection. To close this gap, we propose a comprehensive and systematic literature review that characterizes the different properties of ML-based software vulnerability detection systems using six major Research Questions (RQs).Using a custom web scraper, our systematic approach involves extracting a set of studies from four widely used online digital libraries: ACM Digital Library, IEEE Xplore, ScienceDirect, and Google Scholar. We manually analyzed the extracted studies to filter out irrelevant work unrelated to software vulnerability detection, followed by creating taxonomies and addressing RQs. Our analysis indicates a significant upward trend in applying ML techniques for software vulnerability detection over the past few years, with many studies published in recent years. Prominent conference venues include the International Conference on Software Engineering (ICSE), the International Symposium on Software Reliability Engineering (ISSRE), the Mining Software Repositories (MSR) conference, and the ACM International Conference on the Foundations of Software Engineering (FSE), whereas Information and Software Technology (IST), Computers \&amp; Security (C\&amp;S), and Journal of Systems and Software (JSS) are the leading journal venues.Our results reveal that 39.1\% of the subject studies use hybrid sources, whereas 37.6\% of the subject studies utilize benchmark data for software vulnerability detection. Code-based data are the most commonly used data type among subject studies, with source code being the predominant subtype. Graph-based and token-based input representations are the most popular techniques, accounting for 57.2\% and 24.6\% of the subject studies, respectively. Among the input embedding techniques, graph embedding and token vector embedding are the most frequently used techniques, accounting for 32.6\% and 29.7\% of the subject studies. Additionally, 88.4\% of the subject studies use DL models, with recurrent neural networks and graph neural networks being the most popular subcategories, whereas only 7.2\% use classic ML models. Among the vulnerability types covered by the subject studies, CWE-119, CWE-20, and CWE-190 are the most frequent ones. In terms of tools used for software vulnerability detection, Keras with TensorFlow backend and PyTorch libraries are the most frequently used model-building tools, accounting for 42 studies for each. In addition, Joern is the most popular tool used for code representation, accounting for 24 studies.Finally, we summarize the challenges and future directions in the context of software vulnerability detection, providing valuable insights for researchers and practitioners in the field.} }
@inbook{10.1145/3745238.3745497, title = {The Application of Big Data Screening and Machine Learning in Developing Investment Strategies for the Stock Market}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1650--1654}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745497}, author = {Zou, Chengyu}, abstract = {Substantial investors often rely on valuation and financial indices to forecast stock prices, but no method guarantees profits, as even financially strong companies can see their stock prices decline. In this case, more research should be done to look for some specific characteristics of the rising of stock price and verify of what degree can these characteristics help forecasting the changes of stock price. This study will shift out the relatively valuable data with the help of a system function which represent the difficulty to predict a result for polytomized variable. For continuous variable, using reduced classification method or calculating the variance of data to evaluate its value. Moreover, this study will explain how to evaluate machine learning models with cost matrix and how to improve the accuracy of machine learning models with bagging method and Adaboost method. With the theories introduced in this study, investors can shift out more valuable message from applying the difficulty of prediction function or calculating variance of continuous variable data and update their invest strategies more efficiently and earn more profit with the advanced machine learning models.} }
@proceedings{10.1145/3696271, title = {MLMI '24: Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, year = {2024}, isbn = {9798400717833} }
@inproceedings{10.1145/3687311.3687398, title = {Study on Exam Paper Generation Methods and Experiments based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology}, pages = {483--488}, year = {2024}, isbn = {9798400709920}, doi = {10.1145/3687311.3687398}, url = {https://doi.org/10.1145/3687311.3687398}, author = {Liu, Mingyang}, abstract = {This study aims to address issues in traditional composing a test paper, including subjectivity, uncertainty, and inconsistency in quality, through machine learning techniques. We developed a new method to enhance efficiency and rationality. Our analysis showed the machine learning-based method outperforms manual assembly in knowledge point coverage, test question similarity, and diversity. However, manual assembly excels in differentiation. Our contributions include a reliable evaluation model and a test paper creation system, which were experimentally validated. We aim to further optimize our model and explore effective combinations of machine learning and manual assembly for improved educational support.} }
@inproceedings{10.1145/3643915.3644085, title = {Explanation-driven Self-adaptation using Model-agnostic Interpretable Machine Learning}, booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems}, pages = {189--199}, year = {2024}, isbn = {9798400705854}, doi = {10.1145/3643915.3644085}, url = {https://doi.org/10.1145/3643915.3644085}, author = {Negri, Francesco Renato and Nicolosi, Niccolo and Camilli, Matteo and Mirandola, Raffaela}, keywords = {explainable self-adaptation, model-agnostic explanations, interpretable machine learning, location = Lisbon, AA, Portugal}, abstract = {Self-adaptive systems increasingly rely on black-box predictive models (e.g., Neural Networks) to make decisions and steer adaptations. The lack of transparency of these models makes it hard to explain adaptation decisions and their possible effects on the surrounding environment. Furthermore, adaptation decisions in this context are typically the outcome of expensive optimization processes. The complexity arises from the inability to directly observe or comprehend the internal mechanisms of the black-box predictive models, which requires employing iterative methods to explore a possibly large search space and optimize according to many goals. Here, balancing the trade-off between effectiveness and cost becomes a crucial challenge. In this paper, we propose explanation-driven self-adaptation, a novel approach that embeds model-agnostic interpretable machine learning techniques into the feedback loop to enhance the transparency of the predictive models and gain insights that help drive adaptation decisions effectively by significantly reducing the cost of planning them. Our empirical evaluation demonstrates the cost-effectiveness of our approach using two evaluation subjects in the robotics domain.} }
@inproceedings{10.1145/3746972.3746983, title = {Modeling and Forecasting Global GDP Trends Using PCA and Machine Learning: Evidence from 1960 to 2020}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {60--65}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3746983}, url = {https://doi.org/10.1145/3746972.3746983}, author = {Wu, Junxi}, keywords = {China, GDP, Machine Learning, PPP, Principal Component Analysis}, abstract = {This paper analyzes the GDP (PPP) data of more than 200 countries from 1960 to 2020, and explores the economic growth trend and its influencing factors. This study reveals the structural patterns behind GDP data and predicts future economic trends through methods such as data cleaning, PCA reduction and machine learning. The results show that the linear regression model has the best prediction effect on GDP, and its R² is close to 1, indicating that it effectively captures the main trends in the data. The article also analyzed the impact of the COVID-19 pandemic on China's GDP, pointing out that China's GDP grew by 8.1\% in 2021, with GDP (purchasing power parity) increasing by 8.1\%, while in 2022, GDP growth slowed down to 3.0\%, with GDP (purchasing power parity) increasing by 4.9\%. The research emphasizes the significance of GDP forecasting for policy-making and economic analysis, pointing out that GDP forecasting can assist the government in optimizing resource allocation, formulating scientific and reasonable economic policies, and providing references for enterprise risk hedging, supply chain optimization, and market access strategies.} }
@inproceedings{10.1145/3686592.3686600, title = {Comparison of Machine Learning Methods for Binary Classification of Multicollinearity Data}, booktitle = {Proceedings of the 2024 7th International Conference on Mathematics and Statistics}, pages = {44--49}, year = {2024}, isbn = {9798400707223}, doi = {10.1145/3686592.3686600}, url = {https://doi.org/10.1145/3686592.3686600}, author = {Araveeporn, Autcha and Wanitjirattikal, Puntipa}, keywords = {Backpropagation Neural Network, Na\"ve Bayes, Random Forest, Support Vector Machine}, abstract = {This study examines the effectiveness of binary classification performance in multicollinearity. Four machine learning methods, namely backpropagation neural network, Na\"ve Bayes, support vector machine, and random forest, are compared in terms of their efficiency in handling multicollinear data. The evaluation of binary classification performance efficiency considers multicollinearity in independent variables, considering both a constant correlation model and the Toeplitz correlation. Correlation coefficients of 0.1 and 0.9 are explored in the analysis. The independent variables in this study are simulated from a multivariate normal distribution with 10, 20, 30, and 40 variables, respectively. The dependent variable is constructed using the logit function with sample sizes of 100 and 200. The simulation and data analysis are performed using the R Studio program and repeated 1,000 times for each scenario. The findings of this research reveal that the backpropagation neural network and Na\"ve Bayes methods exhibit superior performance in determining the mean accuracy percentage under constant correlation. On the other hand, the backpropagation neural network and support vector machine are the most effective methods in determining the mean accuracy percentage when dealing with multicollinearity in the form of Toeplitz correlation.} }
@inproceedings{10.1145/3546918.3546921, title = {Machine-Learning-Based Self-Optimizing Compiler Heuristics✱}, booktitle = {Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes}, pages = {98--111}, year = {2022}, isbn = {9781450396967}, doi = {10.1145/3546918.3546921}, url = {https://doi.org/10.1145/3546918.3546921}, author = {Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M\"ossenb\"ock, Hanspeter}, keywords = {Dynamic Compilation, Heuristics, Loop Peeling, Machine Learning, Neural Networks, Optimization, Performance, location = Brussels, Belgium}, abstract = {Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30\% for several benchmarks and only few slowdowns of up to 7\%.} }
@inproceedings{10.1145/3706890.3706901, title = {Intelligent Diagnosis and Progression Analysis of Alzheimer's Disease Using Machine Learning}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {66--72}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3706901}, url = {https://doi.org/10.1145/3706890.3706901}, author = {Yu, Kexin and Luo, Xin}, keywords = {Alzheimer's disease, K-means++ clustering, fixed effect model, logistic regression}, abstract = {AD is a progressive neurodegenerative disease with insidious onset. The disease is usually progressive in the elderly, with a gradual loss of independent living skills and death from complications 10 to 20 years after the onset of the disease. However, due to the limited cognition of patients and their families for such diseases, most patients are often diagnosed to the middle and late stages of the disease, missing the best early intervention time. Therefore, timely and accurate identification of AD and its early stages is critical for improving treatment outcomes. In this paper, we first employs four machine learning algorithms: logistic regression, decision trees, random forests, and SVM to build different intelligent diagnostic models. The results show that the logistic regression model has the best performance, with an F1 score of 0.96. Our study also used the method of K-Means ++ clustering and hierarchical clustering to further subdivide MCI into three categories: SMC, EMCI and LMCI. Finally, we use fixed effects models and the least squares method to reveal the temporal progression patterns of various disease categories.} }
@inproceedings{10.1145/3603287.3651186, title = {Benchmarking Machine Learning Techniques for Bankruptcy Prediction under Benign and Adversarial Behaviors}, booktitle = {Proceedings of the 2024 ACM Southeast Conference}, pages = {259--265}, year = {2024}, isbn = {9798400702372}, doi = {10.1145/3603287.3651186}, url = {https://doi.org/10.1145/3603287.3651186}, author = {Yin, Xing and Le, Thai}, keywords = {10K Financial Report, Adversarial Machine Learning, Data Balancing Techniques, Datasets, Machine Learning, Neural Networks, location = Marietta, GA, USA}, abstract = {This research uses machine learning methods to perform company bankruptcy prediction. 10K Financial reports are audited by public auditing firms and submitted by companies to the Security Exchange Commission (SEC). 10K Financial reports are the appropriate and reasonable financial statements including financial variables (numbers) and texts to evaluate a company's financial status yearly. However, people who don't have a business background would find it difficult to read and understand the 10K Financial reports. Therefore, it is useful to have machine learning methods to analyze the financial status or predict bankruptcy on 10K Financial reports datasets. This research uses different time frames 10K Financial reports datasets and compares several machine learning models to classify and predict bankruptcy companies. Based on different dataset's experimental results, this research concluded that the Random Forest model or XGBoost model generates a better performance. The outcomes of comparing adversarial model performance showed that MLP (Multi-layer Perceptron) + FGSM (Fast Gradient Sign Method) generate better performance, dropping the prediction performance in F1 score from 81.63\% to 79.01\%.} }
@inproceedings{10.1145/3511808.3557501, title = {Fairness of Machine Learning in Search Engines}, booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management}, pages = {5132--5135}, year = {2022}, isbn = {9781450392365}, doi = {10.1145/3511808.3557501}, url = {https://doi.org/10.1145/3511808.3557501}, author = {Fang, Yi and Liu, Hongfu and Tao, Zhiqiang and Yurochkin, Mikhail}, keywords = {fairness, machine learning, search engines, location = Atlanta, GA, USA}, abstract = {Fairness has gained increasing importance in a variety of AI and machine learning contexts. As one of the most ubiquitous applications of machine learning, search engines mediate much of the information experiences of members of society. Consequently, understanding and mitigating potential algorithmic unfairness in search have become crucial for both users and systems. In this tutorial, we will introduce the fundamentals of fairness in machine learning, for both supervised learning such as classification and ranking, and unsupervised learning such as clustering. We will then present the existing work on fairness in search engines, including the fairness definitions, evaluation metrics, and taxonomies of methodologies. This tutorial will help orient information retrieval researchers to algorithmic fairness, provide an introduction to the growing literature on this topic, and gathering researchers and practitioners interested in this research direction.} }
@inproceedings{10.1145/3718751.3718847, title = {Momentum on Sports Performance: Machine Learning and Multi-Aspect Analysis}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {602--611}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718847}, url = {https://doi.org/10.1145/3718751.3718847}, author = {Li, Junxiao and Tan, Xin and Yi, Lanfang and Ma, Qianting}, keywords = {IEW-TOPSIS, Momentum, Multilayer Perceptro, correlation coefficient}, abstract = {This study examines the multidimensional influence of momentum on competitive sports performance through a holistic analytical lens. Commencing with the application of the Information Entropy Weight Technique for Order Preference by Similarity to Ideal Solution (IEW-TOPSIS), it meticulously measures momentum by integrating psychological components, scoring trends, and skill indicators. This refined measurement approach enables an exhaustive exploration of athletes' variable performance throughout competitive engagements. Expanding on this groundwork, a Multilayer Perceptron (MLP) model is employed to forecast player performance with notable precision, reinforcing its efficacy as a predictive instrument. The research extends to strategy formulation, leveraging opponents' historical data, athletes' self-reflections, and pivotal momentum transitions to inform tactical strategies, thereby augmenting decision-making acumen. Model generalizability is demonstrated through successful application to additional tennis matches, albeit suggesting calibration with specific match category data for enhanced accuracy. Sensitivity analyses confirm the robustness of the IEW-TOPSIS and MLP models across varied parameters and data distributions, positioning them as valuable tools for coaching and athlete enhancement in diverse competitive scenarios.} }
@inproceedings{10.1145/3675417.3675455, title = {Revenue Prediction Research of Vegetable Superstores Based on Machine Learning}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {235--239}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675455}, url = {https://doi.org/10.1145/3675417.3675455}, author = {Zhang, Yuqing and Wang, Chunping}, abstract = {Due to the instability of vegetable commodity pricing and replenishment decisions, it‘s difficult to accurately predict their superstore revenues. In this paper, a machine learning-based revenue prediction method is proposed for vegetable superstores. Firstly, based on the historical unit price and wholesale price data, an XGBoost sales volume prediction model is established to obtain various types of sales volume in the future. Secondly, Multilayer Perceptron Model is applied to gain a time series prediction of the wholesale price. Finally, according to the constraints, a particle swarm algorithm optimized objective planning model is presented to optimize the key parameters and achieve the maximum superstore revenue. The simulation results show that the proposed machine learning prediction method can get better prediction results of superstore revenue.} }
@inproceedings{10.1145/3638529.3654154, title = {Survival-LCS: A Rule-Based Machine Learning Approach to Survival Analysis}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {431--439}, year = {2024}, isbn = {9798400704949}, doi = {10.1145/3638529.3654154}, url = {https://doi.org/10.1145/3638529.3654154}, author = {Woodward, Alexa and Bandhey, Harsh and Moore, Jason H. and Urbanowicz, Ryan J.}, keywords = {learning classifier systems, rule-based machine learning, survival analysis, genetic algorithm, location = Melbourne, VIC, Australia}, abstract = {Survival analysis is a critical aspect of modeling time-to-event data in fields such as epidemiology, engineering, and econometrics. Traditional survival methods rely heavily on assumptions and are limited in their application to real-world datasets. To overcome these challenges, we introduce the survival learning classifier system (Survival-LCS) as a more flexible approach. Survival-LCS extends the capabilities of ExSTraCS, a rule-based machine learning algorithm optimized for biomedical applications, to handle survival (time-to-event) data. In addition to accounting for right-censored observations, Survival-LCS handles multiple feature types and missing data, and makes no assumptions about baseline hazard or survival distributions.As proof of concept, we evaluated the Survival-LCS on simulated genetic survival datasets of increasing complexity derived from the GAMETES software. The four genetic models included univariate, epistatic, additive, and heterogeneous models, simulated across a range of censoring proportions, minor allele frequencies, and number of features. The results of this sensitivity analysis demonstrated the ability of Survival-LCS to identify complex patterns of association in survival data. Using the integrated Brier score as the key performance metric, Survival-LCS demonstrated reliable survival time and distribution predictions, potentially useful for clinical applications such as informing self-controls in clinical trials.} }
@inproceedings{10.1145/3747227.3747270, title = {A Machine Learning-Based Design Case Study on the Influencing Factors of Classroom Silence Among Chinese University Students: The “Reconciliation Classroom” Conceptual Healing Space}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {268--275}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747270}, url = {https://doi.org/10.1145/3747227.3747270}, author = {Liu, Qiman and Xie, Wanying and Guo, Nuoya and Shen, Wenxin and Yang, Yan}, keywords = {Chinese university students, Classroom Silence, Healing Space, Machine Learning, Quantitative Research}, abstract = {Classroom interaction among students is an externalization and expression of knowledge, playing a crucial role in facilitating learning. However, university students in China tend to remain silent in class, rarely initiating or responding to questions. This study investigates the phenomenon of classroom silence among university students by employing quantitative statistical methods to explore the influence of classroom-related factors on students’ level of classroom engagement. A total of 449 questionnaires were distributed and collected, and after verifying the reliability and validity of the questionnaire, data cleaning, and the exclusion of outliers, 357 valid samples were obtained for multiple regression and machine learning analyses. Results show that the multiple regression model performs better in predicting the independent variables and the random forest (RF) model outperforms in predicting the most influencing factor on the classroom interactive activity level. Performance accuracy achieved by the multiple regression model is 100\% compared to the RF model with 80\% accuracy. The findings indicate that (1) students’ personality traits exhibit a significant positive correlation with students' classroom engagement; (2) students' abilities have a notable positive impact on their participation in classroom interaction; and (3) teachers’ teaching styles and response benefits demonstrate a weak positive correlation with classroom engagement. Based on the implementation of machine learning models, a new idea emerges to tackle students’ silence in classrooms by proposing and implementing a predictive modeling approach to identify students at risk in learning in universities and colleges.} }
@inproceedings{10.1145/3639478.3643121, title = {Data vs. Model Machine Learning Fairness Testing: An Empirical Study}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {366--367}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3643121}, url = {https://doi.org/10.1145/3639478.3643121}, author = {Shome, Arumoy and Cruz, Lu\'s and Van Deursen, Arie}, keywords = {SE4ML, ML fairness testing, empirical software engineering, data-centric AI, location = Lisbon, Portugal}, abstract = {Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a "cheap" and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.} }
@inproceedings{10.1145/3756580.3756612, title = {Nonlinear Mediation of Parental Phubbing on Child Loneliness: SHAP Interpretability and Double Machine Learning Insights}, booktitle = {Proceedings of the 2025 6th International Conference on Education, Knowledge and Information Management}, pages = {196--201}, year = {2025}, isbn = {9798400715624}, doi = {10.1145/3756580.3756612}, url = {https://doi.org/10.1145/3756580.3756612}, author = {Hao, Jiabao and Li, Huijie}, abstract = {Objective Grounded in social-ecological theory and parent-child system theory, this study investigates the mechanism by which parental phubbing (excessive smartphone use during parent-child interactions) influences children's loneliness, with a focus on the chained mediation effects of parent-child cohesion and children's social anxiety. Existing research predominantly employs parallel mediation models, overlooking the sequential transmission between parent-child interactions and social development, while traditional regression analyses fail to capture nonlinear relationships in behavioral data. Methods Utilizing scales for parental phubbing, children's loneliness, parent-child cohesion, and social anxiety, we conducted a questionnaire survey with 300 fourth- to sixth-grade students from a primary school in Shanxi Province. Predictive models were constructed using Gradient Boosted Regression Trees (GBRT) and a double machine learning framework, with variable contributions analyzed via SHAP (Shapley Additive Explanations) values, and chained pathways validated through mediation random forests. Results Parental phubbing exhibited a direct effect on loneliness (β = 0.29, 95\% CI: 0.21–0.37). The chained mediation effects of parent-child cohesion (β = −0.24) and social anxiety (β = 0.18) were significant (total indirect effect = 0.041), with a nonlinear threshold effect: when parent-child cohesion scores fell below 3.2, the mediation effect intensity increased by 1.7-fold. Conclusion This study reveals a nonlinear dose-response relationship between parental smartphone use and children's psychological distress, providing quantitative evidence for targeted interventions.} }
@inproceedings{10.1145/3686625.3686630, title = {An Effectual Image based Authentication Scheme for Mobile Device using Machine Learning}, booktitle = {Proceedings of the 2024 6th International Electronics Communication Conference}, pages = {24--30}, year = {2024}, isbn = {9798400717598}, doi = {10.1145/3686625.3686630}, url = {https://doi.org/10.1145/3686625.3686630}, author = {Kumar, Kota Lokesh and Ray, Sangram and Das, Priyanka}, keywords = {Image, Machine Learning, Mobile Authentication, Privacy, Security, location = Fukuoka, Japan}, abstract = {In today's digital world, the integration of mobile device authentication has revolutionized the way we access services and applications. But, due to the growing amount of sensitive data that is stored in mobile devices, a strong authentication system is essential to protect user privacy and prevent unauthorized access. To address this aspect the incorporation of password, biometric, etc., to authentication scheme has increased owing to its significant features such as - high security, fast verification, etc. However, there are several weaknesses of using these mechanisms as it cannot be reset or reissued effortlessly. Therefore, to overcome these disadvantages a novel effectual Image based authentication scheme for mobile device using Machine Learning is proposed. Further, the performance efficiency of the proposed scheme is evaluated in terms of processing time. Thus, our scheme is comparable efficient than other existing password and/or biometric based schemes.} }
@inproceedings{10.1145/3600160.3605004, title = {User Acceptance Criteria for Privacy Preserving Machine Learning Techniques}, booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security}, year = {2023}, isbn = {9798400707728}, doi = {10.1145/3600160.3605004}, url = {https://doi.org/10.1145/3600160.3605004}, author = {L\"obner, Sascha and Pape, Sebastian and Bracamonte, Vanessa}, keywords = {Privacy Preserving Machine Learning, Privacy-by-design, User Acceptance, location = Benevento, Italy}, abstract = {Users are confronted with a variety of different machine learning applications in many domains. To make this possible especially for applications relying on sensitive data, companies and developers are implementing Privacy Preserving Machine Learning (PPML) techniques what is already a challenge in itself. This study provides the first step for answering the question how to include the user’s preferences for a PPML technique into the privacy by design process, when developing a new application. The goal is to support developers and AI service providers when choosing a PPML technique that best reflects the users’ preferences. Based on discussions with privacy and PPML experts, we derived a framework that maps the characteristics of PPML to user acceptance criteria.} }
@inproceedings{10.1145/3583668.3600025, title = {From Distributed Algorithms to Machine Learning and Back}, booktitle = {Proceedings of the 2023 ACM Symposium on Principles of Distributed Computing}, pages = {1}, year = {2023}, isbn = {9798400701214}, doi = {10.1145/3583668.3600025}, url = {https://doi.org/10.1145/3583668.3600025}, author = {Wattenhofer, Roger}, keywords = {graph neural networks, distributed computing, networks algorithms, location = Orlando, FL, USA}, abstract = {In the realm of computer science, it may seem that distributed computing and machine learning exist on opposite ends of the spectrum. However, there are many connections between the two domains, both in theory and practice.Recently, machine learning research has become excited about graphs. And when machine learning meets graphs, researchers familiar with distributed algorithms may experience a sense of d\'ej\`a vu, as many classic distributed computing paradigms are being rediscovered. It feels a bit like "machine learning + graphs = distributed algorithms." In my talk, I am going to introduce some key concepts in graph machine learning such as underreaching and oversquashing. These concepts have been known in the distributed computing community as local and congest, respectively.In the main part of the talk, I am going to present some recent breakthroughs in this exciting intersection of fields. Finally, I will also present some intriguing open problems.} }
@inproceedings{10.1145/3613904.3642855, title = {Machine Learning Processes As Sources of Ambiguity: Insights from AI Art}, booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703300}, doi = {10.1145/3613904.3642855}, url = {https://doi.org/10.1145/3613904.3642855}, author = {Sivertsen, Christian and Salimbeni, Guido and Lvlie, Anders Sundnes and Benford, Steven David and Zhu, Jichen}, keywords = {ambiguity, art, artificial intelligence, computer vision, generative art, machine learning, location = Honolulu, HI, USA}, abstract = {Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success. This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work. Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis. Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes. Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details. Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one.} }
@inproceedings{10.1145/3534678.3542914, title = {Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4874--4875}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542914}, url = {https://doi.org/10.1145/3534678.3542914}, author = {Goldenberg, Dmitri and Sokolova, Elena and Meir Lador, Shir and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit}, keywords = {data science management, machine learning management, ml product development, location = Washington DC, USA}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@inproceedings{10.1145/3716554.3716590, title = {Integration of Evolutionary Algorithms and Machine Learning techniques in routing-related problems: A review}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {237--243}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716590}, url = {https://doi.org/10.1145/3716554.3716590}, author = {Giannopoulos, Panagiotis G. and Malamas, Vangelis and Dasaklis, Thomas K.}, keywords = {Evolutionary Algorithms, Machine Learning, Routing problems, Operations Research}, abstract = {This paper provides a systematic review of hybrid algorithms combining Evolutionary Algorithms (EAs) with Machine Learning (ML) techniques, focusing on routing-related problems. It explores the integration of various ML methods, such as Reinforcement Learning (RL), Supervised Learning (SL), and Unsupervised Learning (UL), with EAs, aiming to address complex, multi-objective optimization challenges often encountered in logistics, robotics, and network routing. The review identifies RL-based methods, particularly Q-Learning (QL) and Deep RL (DRL), as the most prominent approaches due to their adaptability and capability to dynamically adjust solutions. SL techniques, including Decision Trees (DTs) and Artificial Neural Networks (ANNs), and UL approaches, such as clustering, also play significant roles in enhancing EAs’ performance.} }
@inproceedings{10.1145/3718491.3718639, title = {Classification Prediction of ADMET Properties for Anti-Pancreatic Cancer Drug Candidates Using Machine Learning Approaches}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {914--919}, year = {2025}, isbn = {9798400710865}, doi = {10.1145/3718491.3718639}, url = {https://doi.org/10.1145/3718491.3718639}, author = {Wu, Chao and Huang, Zongxiang and Feng, Yanghui and Wang, Yukun and Xie, Zongsheng and Chen, Jinsheng}, keywords = {Pancreatic cancer, classification prediction, ensemble learning model, machine learning}, abstract = {Utilizing machine learning algorithms, this study aimed to predict the pharmacokinetic properties (ADMET properties) of anti-pancreatic cancer drug candidates, focusing on five key indicators: Absorption, Distribution, Metabolism, Excretion, and Toxicity. Based on 729 molecular descriptors derived from 1,974 compounds, classification prediction models were developed for intestinal epithelial cell permeability (Caco-2), cytochrome P450 3A4 subtype (CYP3A4) inhibition, cardiac safety (hERG), oral bioavailability (HOB), and micronucleus (MN) test. Various algorithms, including decision trees, discriminant analysis, support vector machines (SVMs), K-nearest neighbors, and ensemble learning, were applied. The Quadratic SVM algorithm emerged as the best performer in predicting Caco-2, CYP3A4, and hERG, achieving accuracies of 91.6\%, 95.0\%, and 90.5\%, respectively. In contrast, the Boosted Tree model within ensemble learning showed superior performance in predicting HOB and MN, with accuracies of 90.5\% and 96.1\%. These findings demonstrate that machine learning algorithms significantly enhance the efficiency and success rate of drug development in the context of anti-pancreatic cancer candidates.} }
@inproceedings{10.1145/3736426.3736474, title = {Data-Driven Supplier Management Framework: Integrating Machine Learning Models into Planning and Assessment Strategies}, booktitle = {Proceedings of the 2025 International Conference on Digital Management and Information Technology}, pages = {304--309}, year = {2025}, isbn = {9798400714238}, doi = {10.1145/3736426.3736474}, url = {https://doi.org/10.1145/3736426.3736474}, author = {Lin, Nuo and Wang, Shiyin and Gao, Yaguxun}, keywords = {Algorithms, Civil Aircraft, Machine Learning Models, Plan Control}, abstract = {Civil aircraft programmes are distinguished by their considerable scale and duration. They also involve synergies with numerous suppliers. By establishing an effective evaluation mechanism for supplier programme management, the main manufacturer can regularly audit and monitor the quality of the supplier's work and the progress of the development to identify and correct problems in the supplier's project development process in good time.This paper presents in-depth applications of computer technology, such as machine learning-based prediction models and optimisation algorithms, to further improve the efficiency and accuracy of supplier planning control. In addition, this paper proposes a practical and efficient appraisal strategy for supplier program control, aiming to improve suppliers' program execution capability and project delivery quality.By establishing three rating levels, A, B and C, in the programme evaluation, the rating level directly affects the allocation of research and development funds to stimulate supplier enthusiasm and initiative.} }
@inproceedings{10.1145/3665601.3669849, title = {On Integrating the Data-Science and Machine-Learning Pipelines for Responsible AI}, booktitle = {Proceedings of the Conference on Governance, Understanding and Integration of Data for Effective and Responsible AI}, pages = {50--53}, year = {2024}, isbn = {9798400706943}, doi = {10.1145/3665601.3669849}, url = {https://doi.org/10.1145/3665601.3669849}, author = {Esmaelizadeh, Armin and Rorseth, Joel and Yu, Andy and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw and Taghva, Kazem}, keywords = {Data Science, Explainable AI, Machine Learning Model Diagnostics, location = Santiago, AA, Chile}, abstract = {Herein, we advocate for the integration of the pipelines for data science (e.g., extraction, cleaning, and exploration) and machine learning (e.g., training data collection, feature selection, model selection, and parameter tuning), toward responsible and trustworthy artificial intelligence. We argue that the metadata generated by the machine-learning pipeline, which includes model outputs and model accuracy scores, is best managed and analyzed using data-science tools, thereby obtaining actionable insights into model performance, interpretability, and bias. We illustrate via two examples from our recent work as proof of concept: data summarization for model performance diagnostics; and input and output exploration to understand retrieval-augmented language models.} }
@inproceedings{10.1145/3644815.3644976, title = {Component-based Approach to Software Engineering of Machine Learning-enabled Systems}, booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI}, pages = {250--252}, year = {2024}, isbn = {9798400705915}, doi = {10.1145/3644815.3644976}, url = {https://doi.org/10.1145/3644815.3644976}, author = {Indykov, Vladislav}, keywords = {machine learning, software architecture, software quality, location = Lisbon, Portugal}, abstract = {Machine Learning (ML) - enabled systems capture new frontiers of industrial use. The development of such systems is becoming a priority course for many vendors due to the unique capabilities of Artificial Intelligence (AI) techniques. The current trend today is to integrate ML functionality into complex systems as architectural components. There are a lot of relevant challenges associated with this strategy in terms of the overall system architecture and in the context of development workflow (MLOps). The probabilistic nature, crucial dependency on data, and work in an environment of high uncertainty do not allow software engineers to apply traditional software development methodologies. As a result, there is a community request to systematize the most relevant experience in building software architectures with ML components, to create new approaches to organizing the process of developing ML-enabled systems, and to build new models for assessing the system quality. Our research contributes to all mentioned directions and aims to create a methodology for the efficient implementation of ML-enabled software and AI components. The results of the research can be used in the design and development in industrial settings, as well as a basis for further studies in the research field, which is of both practical and scientific value.} }
@inproceedings{10.1145/3644713.3644838, title = {Machine Learning Algorithms to Detect Illicit Accounts on Ethereum Blockchain}, booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems}, pages = {747--752}, year = {2024}, isbn = {9798400709036}, doi = {10.1145/3644713.3644838}, url = {https://doi.org/10.1145/3644713.3644838}, author = {Obi-Okoli, Chibuzo and Jogunola, Olamide and Adebisi, Bamidele and Hammoudeh, Mohammad}, keywords = {Ethereum blockchain, anomaly detection, blockchain security, illicit activities, machine learning, location = Dubai, United Arab Emirates}, abstract = {The rapid growth and psudonomity inherent in blockchain technology such as in Bitcoin and Ethereum has marred its original intent to reduce dependant on centralised system, but created an avenue for illicit activities, including fraud, phishing, scams, etc. This undermines the reputation of blockchain network, giving rise to the need to identify these illicit activities within the blockchain network. This current work tackles this crucial problem by investigating and implementing six machine learning algorithms with a particular emphasis on striking a balance between accuracy, precision and recall. The novelty of the work lies in the utilising of the synthetic minority over-sampling technique to handle data imbalance. Thus, increasing the accuracy of the light gradient boosting machine classifier to 98.4\%. The outcome of this work holds great potential for enhancing the security and credibility of blockchain ecosystems paving the way for a more secure and dependable digital future in the age of decentralised and trustless systems.} }
@article{10.5555/3606402.3606413, title = {Practical Machine Learning for Liberal Arts Undergraduates}, journal = {J. Comput. Sci. Coll.}, volume = {38}, pages = {69--79}, year = {2023}, issn = {1937-4771}, author = {Sherman, Mark and Hogan, Alyssa and O'Sullivan, Jamison and Schumacher, Samantha}, abstract = {Liberal arts education provides students with many interdisciplinary problem-solving skills, but utilizing high-level computational tools like machine learning (ML) remains largely inaccessible without a significant background in computer science. We present a course to bridge that gap: a full-semester undergraduate course that teaches the concepts of ML and deep learning with emphasis on real-world application and ethical considerations. The course is low-code, exploring concepts and applications through a collection of visual tools. Early outcomes show that after this course students are equipped to learn code-based systems, feel empowered to understand and identify misunderstandings in popular AI discourse, and can design ML-based solutions for data-oriented problems in their fields.} }
@article{10.1145/3759242, title = {Load Balancing in the Internet of Vehicles: A Comprehensive Review of SDN and Machine Learning Approaches}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3759242}, url = {https://doi.org/10.1145/3759242}, author = {Marwein, Phibadeity S. and Kandar, Debdatta}, keywords = {WSN-LB, IoT-LB, UAV-LB, IoV-LB, SDN-based LB, ML-based LB, mmWave, THz}, abstract = {Efficient load balancing (LB) is crucial for optimizing network performance in Wireless Sensor Networks (WSN), the Internet of Things (IoT), and Unmanned Aerial Vehicles (UAV), as well as the emerging Internet of Vehicles (IoV). In this article, we study various LB techniques across these domains, including Software-Defined Networking (SDN) and Machine Learning (ML)-based approaches. SDN enables centralized control and real-time adaptability, while ML enhances decision-making through predictive analytics. Given the limited research on IoV, we leverage insights from WSN, IoT, and UAVs to propose an innovative technique that integrates SDN with ML for intelligent, adaptive LB in IoV. This approach promises to optimize network performance, reduce latency, and improve fault tolerance, offering a new research direction in vehicular networks.} }
@inproceedings{10.1145/3717664.3717687, title = {Exploring the Impact of Digital Transformation on Bank Credit Risk through Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {132--136}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717687}, url = {https://doi.org/10.1145/3717664.3717687}, author = {Lu, Xunuo and Tu, Binghao and Yu, Zengyi}, keywords = {Bank Digital Transformation, Credit Risk, Decision Tree, Machine Learning}, abstract = {In the current economic environment, digital transformation has become a new trend of innovation and development of commercial banks, while its impact on bank credit risk cannot be ignored. This paper takes 59043 bank data points in 2011-2021 as the research sample, preprocesses the data based on the K-means clustering method with pruning and 5-fold cross-validation which is used resulting in 96.2\% model accuracy, and finally examines the impact of banks' digital transformation on credit risk using decision tree analysis, . The study finds that (1) digital transformation has a more significant negative effect on bank credit risk. (2) In the decomposition of the degree of digital transformation into three indicators, namely, "strategic digitalization", "business digitalization" and "management digitalization", business digitalization has the most significant impact on it. (3) There is a heterogeneous effect of banks' digital transformation on credit risk, and the dampening effect on credit risk is more pronounced for urban commercial banks, local state-owned banks and listed banks than for rural commercial banks, non-local state-owned banks and non-listed banks. This paper not only provides the basis for related research, but also provides the basis for the study of the machine tool. This paper not only provides machine learning research methods for related research, but also has important significance for the government to guide the development of fintech, promote the digital transformation of banks, and improve the national financial risk management ability.} }
@inbook{10.1145/3746027.3756875, title = {PySimPace v2.0: An Easy-to-Use Simulation Tool with Machine Learning Pipelines for Realistic MRI Motion Artifact Generation}, booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia}, pages = {13656--13659}, year = {2025}, isbn = {9798400720352}, url = {https://doi.org/10.1145/3746027.3756875}, author = {Kumar, Snehil and Vaughan, Neil and Fu, Zeyu and Wilson, Heather}, abstract = {Motion artifacts in structural and functional magnetic resonance imaging (MRI) pose a significant challenge for both clinical use and machine learning (ML)-based image analysis. Existing ML approaches for artifact correction require paired clean and corrupted datasets, which are difficult to acquire. We present py-simpace, an open-source, pip-installable MRI motion artifact simulation toolkit with native ML integration. py-simpace supports structural MRI and functional MRI (fMRI) simulation, offering configurable k-space and image-space motion, ghosting, Gibbs ringing, and physiological noise. It provides an end-to-end pipeline with a ready-to-use PyTorch Dataset interface for ML training. We describe the design of py-simpace v2.0, compare it with existing tools, and demonstrate its utility for robust artifact correction model development.} }
@inproceedings{10.1145/3647444.3647842, title = {Transforming Healthcare through Machine Learning: A Revolution in Patient Care}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647842}, url = {https://doi.org/10.1145/3647444.3647842}, author = {Gupta, Divya and Kaur, Jaspreet and Kaur, Simarjeet}, keywords = {Additional Key Words and Phrases: Machine learning (ML), healthcare, Artificial Intelligence (AI), location = Jaipur, India}, abstract = {Machine learning (ML) ushered in a new era in healthcare by providing unprecedented opportunities to improve patient health, improve clinical decision making, and improve patient outcomes. This article provides an overview of the far-reaching implications of machine learning in healthcare, highlighting its different applications and the challenges associated with it. In this study, we explore how machine learning (ML) can be used to improve healthcare, including personalised treatment planning, disease prediction, medication development, and medical picture analysis. The ability of ML algorithms to spot subtle patterns, forecast illness outbreaks, and enable early intervention has been proved by studying a large diversity of healthcare data. This study also explores the difficulties and moral dilemmas associated with ML integration into healthcare systems, highlighting the significance of data protection, bias reduction, and legal compliance. We also look at the possible advantages and drawbacks of these technologies in environments with limited resources, highlighting the necessity for flexible and accessible solutions.} }
@inbook{10.1145/3757749.3757838, title = {Research on Landscape Architecture Space Optimization Based on Machine Learning: Taking Tourist Behavior Analysis as an Example}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {535--539}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757838}, author = {Wang, Yanrong}, abstract = {This study presents a data-driven framework integrating machine learning (ML) techniques to analyze tourist behavior and optimize spatial configurations in landscape architecture. By leveraging GPS trajectory data, IoT sensor networks, and GIS spatial analytics, we develop models to cluster visitor groups, predict activity preferences, and forecast crowd dynamics. Key methodologies include DBSCAN for activity zone identification, Random Forest for spatial preference modeling, and LSTM networks for congestion forecasting. A case study of Qingfeng Park in Shanghai demonstrates the framework's application, yielding a 32\% reduction in peak-hour congestion and a 28\% improvement in facility utilization. Mathematical formulations for spatial interaction modeling and model evaluation metrics are explicitly detailed, providing a replicable framework for evidence-based landscape design.} }
@inproceedings{10.1145/3640310.3674087, title = {Enhancing Automata Learning with Statistical Machine Learning: A Network Security Case Study}, booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems}, pages = {172--182}, year = {2024}, isbn = {9798400705045}, doi = {10.1145/3640310.3674087}, url = {https://doi.org/10.1145/3640310.3674087}, author = {Ayoughi, Negin and Nejati, Shiva and Sabetzadeh, Mehrdad and Saavedra, Patricio}, keywords = {Decision trees, Denial of Service (DoS) attacks, Intrusion detection, Model checking, Query checking, State-machine learning, location = Linz, Austria}, abstract = {Intrusion detection systems are crucial for network security. Verification of these systems is complicated by various factors, including the heterogeneity of network platforms and the continuously changing landscape of cyber threats. In this paper, we use automata learning to derive state machines from network-traffic data with the objective of supporting behavioural verification of intrusion detection systems. The most innovative aspect of our work is addressing the inability to directly apply existing automata learning techniques to network-traffic data due to the numeric nature of such data. Specifically, we use interpretable machine learning (ML) to partition numeric ranges into intervals that strongly correlate with a system's decisions regarding intrusion detection. These intervals are subsequently used to abstract numeric ranges before automata learning. We apply our ML-enhanced automata learning approach to a commercial network intrusion detection system developed by our industry partner, RabbitRun Technologies. Our approach results in an average 67.5\% reduction in the number of states and transitions of the learned state machines, while achieving an average 28\% improvement in accuracy compared to using expertise-based numeric data abstraction. Furthermore, the resulting state machines help practitioners in verifying system-level security requirements and exploring previously unknown system behaviours through model checking and temporal query checking. We make our implementation and experimental data available online.} }
@article{10.5555/3648699.3648715, title = {Globally-consistent rule-based summary-explanations for machine learning models: application to credit-risk evaluation}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Rudin, Cynthia and Shaposhnik, Yaron}, keywords = {explainable artificial intelligence (XAI), local explanations, interpretability, credit risk}, abstract = {We develop a method for understanding specific predictions made by (global) predictive models by constructing (local) models tailored to each specific observation (these are also called "explanations" in the literature). Unlike existing work that "explains" specific observations by approximating global models in the vicinity of these observations, we fit models that are globally-consistent with predictions made by the global model on past data. We focus on rule-based models (also known as association rules or conjunctions of predicates), which are interpretable and widely used in practice. We design multiple algorithms to extract such rules from discrete and continuous datasets, and study their theoretical properties. Finally, we apply these algorithms to multiple credit-risk models trained on the Explainable Machine Learning Challenge data from FICO and demonstrate that our approach effectively produces sparse summary-explanations of these models in seconds. Our approach is model-agnostic (that is, can be used to explain any predictive model), and solves a minimum set cover problem to construct its summaries.} }
@inproceedings{10.1145/3704323.3704367, title = {Comparison of Peruvian coffee varieties applying near infrared spectroscopy (NIRS) and machine learning}, booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition}, pages = {240--246}, year = {2025}, isbn = {9798400717482}, doi = {10.1145/3704323.3704367}, url = {https://doi.org/10.1145/3704323.3704367}, author = {Ovalle, Christian and Santillan Aching, Omar and Temoche Lopez, Alfredo and Bojorquez Segura, Jorge Alfredo}, keywords = {Coffee, MAE, Machine Learning, Near Infrared Spectroscopy, PLS, RMSE, Random Forests, SVM}, abstract = {In the coffee industry, the traditional cupping process, based on subjective judgments, has been superseded by implementing more objective techniques. Near-infrared spectroscopy (NIR) and Machine Learning (ML) algorithms were used to compare Peruvian coffee varieties in this context. Data preparation was critical, addressing outlier detection and management and partitioning into training and test sets. The evaluation focused on four models: PLS, SVM, and Random Forests. The results revealed that Random Forests stood out with exceptional performance, achieving an average accuracy of 96\%. This key metric was complemented by RMSE (Root Mean Square Error), MAE (Mean Absolute Error), and overall accuracy. The comparison of these metrics allowed us not only to evaluate the predictive ability of the models but also to understand the magnitude and nature of the prediction errors. Implementing these models provided a rapid and non-destructive evaluation of the quality of coffee varieties, overcoming the limitations of the traditional cupping approach. This study supports the feasibility of integrating NIR with ML as an effective tool for comparing Peruvian coffee varieties, as well as providing insight into the objective evaluation of coffee quality, opening new perspectives in the coffee industry.} }
@inproceedings{10.1145/3660853.3660929, title = {Secure Data Transmission in IoT Networks using Machine Learning-based Encryption Techniques}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {285--291}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660929}, url = {https://doi.org/10.1145/3660853.3660929}, author = {Thamer, Khudhair Abed and Ahmed, Saadaldeen Rashid and Almashhadany, Mohammed Thakir Mahmood and Abdulqader, Sarah G. and Abduladheem, Wameedh and Algburi, Sameer}, keywords = {Encryption Techniques, Internet of Things (IoT), Machine Learning, data transmission, security protocols, location = undefinedstanbul, Turkiye}, abstract = {The penetration of Internet of Things (IoT) appliances by day leads to the escalation of security protocols in data transmission as the topic of the highest priority level. While traditional admission procedures grow on the stream of innovative internet linked technology, they lack countering the fluctuation caused by the innovation. This study explicitly focuses on the area of ML encryption algorithms, believed to be renovators of the essential principles of IoT security. The ability of machine learning to make humans ready to constantly update the algorithms doing programming and harness the wisdom given by real-time data examination engenders a masterpiece (tapestry) of solutions that can reject the continuous evolution of cybersecurity menaces among IoT networks. An exploration experience is endured in deep, to the hearts, experiments, which then disclose the courses that networks in transit, encrypted ones, follow across the full collection of state-of-the art machine learning algorithms. Tender as the artificial neural network may be, it perseveres and finally overshadows the naysayers near the conclusion of the scene when it replaces the flawed analyzing method. These measurements of validation toll like a sacrament, quivering with the resounding acclaim that testifies the latent power that lurks at the center of those approaches. Like an angelic cry of the arena, this remark stimulates and uplifts IoT ecosystem security within sacred limits. The waves that arise from these findings expand and assault activities played by many, which serves as a reminder of the essential role played by blending the most modern machine learning technologies with tailored solutions unique to the demands of IoT devices. This traversal through the historical chapters of the study imputes onto the realm of IoT security a mantle with an increased drive for expansion that is intended at further creating a new story that will determine the future route for this vital component.CCS CONCEPTS • Security and privacy∼ Systems security∼ Network security∼Software and application security} }
@inproceedings{10.1145/3531146.3533233, title = {Evaluation Gaps in Machine Learning Practice}, booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages = {1859--1876}, year = {2022}, isbn = {9781450393522}, doi = {10.1145/3531146.3533233}, url = {https://doi.org/10.1145/3531146.3533233}, author = {Hutchinson, Ben and Rostamzadeh, Negar and Greer, Christina and Heller, Katherine and Prabhakaran, Vinodkumar}, keywords = {applications, evaluation, machine learning, location = Seoul, Republic of Korea}, abstract = {Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.} }
@inproceedings{10.1145/3606042.3616460, title = {AI/Machine Learning for Internet of Dependable and Controllable Things}, booktitle = {Proceedings of the 2023 Workshop on Advanced Multimedia Computing for Smart Manufacturing and Engineering}, pages = {1--2}, year = {2023}, isbn = {9798400702730}, doi = {10.1145/3606042.3616460}, url = {https://doi.org/10.1145/3606042.3616460}, author = {Song, Houbing Herbert}, keywords = {artificial intelligence, internet of things, machine learning, location = Ottawa ON, Canada}, abstract = {The Internet of Things (IoT) has the potential to enable a variety of applications and services. However, it also presents grand challenges in security, safety, and privacy. Therefore, there is a need for moving from IoT to Internet of Dependable Things, which is defined as Internet of Things which is designed, built, deployed and operated in a highly trustworthy manner, and Internet of Controllable Things, which is defined as Internet of Things which is operated in a highly controllable manner. A massive resurgence of artificial intelligence (AI) and machine learning (ML) presents tremendous opportunities for Internet of Dependable and Controllable Things and as well as significant challenges to Internet of Dependable and Controllable Things. In this lecture, I will present the state of the art by reviewing and classifying the existing literature, evaluate the opportunities and challenges, and identify trends by evaluating what needs to be done to enable AI/Machine Learning for Internet of Dependable and Controllable Things.} }
@article{10.14778/3749646.3749700, title = {Time-Series Clustering: A Comprehensive Study of Data Mining, Machine Learning, and Deep Learning Methods}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {4380--4395}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3749646.3749700}, url = {https://doi.org/10.14778/3749646.3749700}, author = {Paparrizos, John and Bogireddy, Sai Prasanna Teja Reddy}, abstract = {Time-series clustering is a key task in time series analysis, enabling unsupervised data exploration and often serving as a subroutine for other tasks. Despite decades of active cross-disciplinary research, benchmarking of time-series clustering methods has received limited attention. Existing studies have (i) excluded popular methods and entire method classes; (ii) used a narrow range of distance measures; (iii) evaluated only a few datasets; (iv) lacked statistical validation; (v) had poor reproducibility; or (vi) relied on questionable evaluation setups. The rise of deep learning—especially foundation models claiming broad generalization—further emphasizes the need for comprehensive evaluation, as their role in time-series clustering remains largely untested. To address these gaps, we evaluate 84 time-series clustering methods across 10 method classes from data mining, machine learning, and deep learning. Our analysis spans 128 time-series datasets and uses rigorous statistical methods. Within a fair comparison framework, we (i) identify the top-performing method in each class; (ii) highlight previously overlooked, high-performing classes; (iii) challenge assumptions about elastic distance measures; (iv) refute the claimed superiority of deep learning methods, including foundation models; (v) expose reproducibility issues; (vi) analyze performance variation across dataset properties; and (vii) assess scalability. Our findings reveal an illusion of progress: no method significantly outperforms the decade-old k-Shape method. Still, we highlight a deep learning-based approach with notable promise. Our results provide a strong benchmark for advancing time-series clustering, and we have open-sourced our work to support future research.} }
@inproceedings{10.1145/3745238.3745387, title = {Epidemic risk and corporate financing decisions: machine learning based prediction and ESG factor analysis}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {951--957}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745387}, url = {https://doi.org/10.1145/3745238.3745387}, author = {Feng, Huixuan and Chen, Song and Lyu, Juncheng and Wu, Jiarui}, keywords = {And Governance (ESG), Corporate Financing Decisions, Efficient Chicken Swarm tuned Extreme Gradient Boosting (ECS-XGBoost), Environmental, Epidemic Risk, Social}, abstract = {The recent worldwide health challenges, especially epidemics, have greatly influenced corporate funding choices, revealing weaknesses in conventional financial models. In this situation, companies are progressively incorporating Environmental, Social, and Governance (ESG) factors into their approaches to manage risk and bolster resilience. To handle risk and bolster resilience in this climate, businesses are integrating ESG considerations more and more into their strategies. To predict financial behavior in times of stress, this research employs machine learning (ML) techniques to explore the link between pandemic risks and financial decisions made by firms. This study introduces an ESG-Aware Investment Portfolio (ESG-AIP) model that utilizes Efficient Chicken Swarm tuned Extreme Gradient Boosting (ECS-XGBoost) for predicting stock returns and making corporate financing decisions in stressful circumstances. The research collects financial data and ESG metrics from companies during pandemic and post-pandemic eras, including stock values, debt amounts, and ESG ratings. Standard portfolio selection models assist in allocating assets to maximize returns while minimizing risk, duly considering constraints like ESG criteria. ECS aids in optimizing ESG elements, while XGBoost can be used for making more precise and robust stock return forecasts. The suggested ECS-XGBoost method has achieved an R2 of 0.8625, an RMSE of 1.03, a MAPE of 1.99, and a MAE of 0.54. This study highlights the critical role that ESG factors play in mitigating risk during crises and provides insights into how companies adjust their financing strategies in response to public health emergencies with an emphasis on long-term sustainability.} }
@inproceedings{10.1145/3706598.3713198, title = {A Critical Analysis of Machine Learning Eco-feedback Tools through the Lens of Sustainable HCI}, booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, year = {2025}, isbn = {9798400713941}, doi = {10.1145/3706598.3713198}, url = {https://doi.org/10.1145/3706598.3713198}, author = {G\"or\"uc\"u, Sinem and Morais, Luiz A. and Panagiotidou, Georgia}, keywords = {eco-feedback, sustainable HCI, green AI, carbon reporting, materiality of machine learning}, abstract = {In light of machine learning’s increasing computational needs, developers created energy and carbon-reporting tools to calculate and communicate their models’ environmental impact. These tools use modeling parameters as inputs and respond with expected or incurred energy requirements or carbon emissions. This work critically and systematically analyses them regarding their content, form, and design process. Besides their noble intentions, many of the shortcomings of early sustainable HCI eco-feedback tools are still being propagated in these tools. Moreover, their design and development have limited inclusion of potential stakeholders. We argue the need for a next generation of approaches to ML eco-feedback that (a) further support rematerialization, (b) use participatory approaches in their design and development to support collaborative team environments and go beyond individual persuasion, (c) consider complexities of ML models and processes, and more broadly, (d) re-center around sufficiency rather than only efficiency.} }
@inproceedings{10.1145/3728985.3728990, title = {Research Experience for Undergraduates to Explore Human-Robot Interaction and Machine Learning with Spot}, booktitle = {Proceedings of the 2024 10th International Conference on Robotics and Artificial Intelligence}, pages = {80--85}, year = {2025}, isbn = {9798400717451}, doi = {10.1145/3728985.3728990}, url = {https://doi.org/10.1145/3728985.3728990}, author = {Tang, Fang and Pham, Vinh and Napinas, Raphael and Siguenza, Alfredo}, keywords = {human-robot interaction, machine learning, undergraduate research experience}, abstract = {This paper discusses the efforts to provide research experience to undergraduate students to expose them to state-of-art technologies in robotics. Built on the platform of Boston Dynamics SPOT, we present two research projects that focus on human-robot interaction and machine learning. The first project is an implementation of the real-world tic-tac-toe game between SPOT and a human. The second project is playing “catch” with SPOT. These two projects were among six other undergraduate projects that have been designed and developed in the past two years. In the end, we reflect upon our project management and lessons learned and hope to bring a better research experience to our students in the future.} }
@inproceedings{10.1145/3526073.3527584, title = {Operationalizing machine learning models: a systematic literature review}, booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI}, pages = {1--8}, year = {2023}, isbn = {9781450393195}, doi = {10.1145/3526073.3527584}, url = {https://doi.org/10.1145/3526073.3527584}, author = {Kolltveit, Ask Berstad and Li, Jingyue}, keywords = {MLOps, deployment, machine learning, operationalization, systematic literature review, location = Pittsburgh, Pennsylvania}, abstract = {Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments.} }
@inproceedings{10.1145/3664646.3664769, title = {Leveraging Machine Learning for Optimal Object-Relational Database Mapping in Software Systems}, booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software}, pages = {94--102}, year = {2024}, isbn = {9798400706851}, doi = {10.1145/3664646.3664769}, url = {https://doi.org/10.1145/3664646.3664769}, author = {Azizian, Sasan and Rastegari, Elham and Bagheri, Hamid}, keywords = {Dynamic Analysis, Machine Learning, ORM Mapping, Relational logic, Specification-driven Synthesis, Static Analysis, Tradespace Analysis, location = Porto de Galinhas, Brazil}, abstract = {Modern software systems, developed using object-oriented programming languages (OOPL), often rely on relational databases (RDB) for persistent storage, leading to the object-relational impedance mismatch problem (IMP). Although Object-Relational Mapping (ORM) tools like Hibernate and Django provide a layer of indirection, designing efficient application-specific data mappings remains challenging and error-prone. The selection of mapping strategies significantly influences data storage and retrieval performance, necessitating a thorough understanding of paradigms and systematic tradeoff exploration. The state-of-the-art systematic design tradeoff space exploration faces scalability issues, especially in large systems. This paper introduces a novel methodology, dubbed Leant, for learning-based analysis of tradeoffs, leveraging machine learning to derive domain knowledge autonomously, thus aiding the effective mapping of object models to relational schemas. Our preliminary results indicate a reduction in time and cost overheads associated with developing (Pareto-) optimal object-relational database schemas, showcasing Leant's potential in addressing the challenges of object-relational impedance mismatch and advancing object-relational mapping optimization and database design.} }
@inproceedings{10.1145/3565287.3617639, title = {Mitigating Racial Biases for Machine Learning Based Skin Cancer Detection}, booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing}, pages = {556--561}, year = {2023}, isbn = {9781450399265}, doi = {10.1145/3565287.3617639}, url = {https://doi.org/10.1145/3565287.3617639}, author = {Abhari, Julian and Ashok, Ashwin}, keywords = {skin cancer, machine learning, domain adaptation, racial, skin tones, artificial intelligence, bias, computer vision, app, generative adversarial networks, location = Washington, DC, USA}, abstract = {Machine learning (ML) based skin cancer detection tools are an example of a transformative medical technology that could potentially democratize early detection for skin cancer cases for everyone. However, due to the dependency of datasets for training, ML based skin cancer detection always suffers from a systemic racial bias. Racial communities and ethnicity not well represented within the training datasets will not be able to use these tools, leading to health disparities being amplified. Based on empirical observations we posit that skin cancer training data is biased as it's dataset represents mostly communities of lighter skin tones, despite skin cancer being far more lethal for people of color. In this paper we use domain adaptation techniques by employing CycleGANs to mitigate racial biases existing within state of the art machine learning based skin cancer detection tools by adapting minority images to appear as the majority. Using our domain adaptation techniques to augment our minority datasets, we are able to improve the accuracy, precision, recall, and F1 score of typical image classification machine learning models for skin cancer classification from the biased 50\% accuracy rate to a 79\% accuracy rate when testing on minority skin tone images. We evaluate and demonstrate a proof-of-concept smartphone application.} }
@inproceedings{10.1145/3605098.3635983, title = {GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption}, booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing}, pages = {953--962}, year = {2024}, isbn = {9798400702433}, doi = {10.1145/3605098.3635983}, url = {https://doi.org/10.1145/3605098.3635983}, author = {Frimpong, Eugene and Nguyen, Khoa and Budzys, Mindaugas and Khan, Tanveer and Michalas, Antonis}, keywords = {hybrid homomorphic encryption, machine learning as a service, privacy-preserving machine learning, location = Avila, Spain}, abstract = {Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model. We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data. Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data. Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach. The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices.} }
@inproceedings{10.1145/3689236.3691500, title = {Multimodal Machine Learning Based Object Recognition Techniques for Engineering Applications}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {724--730}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3691500}, url = {https://doi.org/10.1145/3689236.3691500}, author = {Zeng, An and Qian, Ying and Zhou, Qing and Xie, Honghui}, keywords = {Corner Detection, Deep Learning, Graph Convolutional Neural Network, Multimodal}, abstract = {Object recognition technology has important applications in the industrial field, but there are limitations in relying on single modal information. To address this problem, the study proposes a multimodal object recognition system that combines tactile sensors and visual information, and processes tactile data through graph convolutional neural networks to improve recognition accuracy. The system utilizes multi-channel impulse map convolutional layer, attention layer and fully connected layer for tactile data analysis, and introduces Harris corner point detection algorithm, orientation gradient histogram and optical flow field orientation histogram to enhance visual feature extraction. The experimental results show that the recognition accuracy of the multimodal neural network system on different datasets reaches 95.85\%, which is better than the performance of traditional KNN and DTW algorithms. The research system demonstrates high accuracy and stability in error analysis and physical localization experiments, for example, the error is only 0.1 cm in distance measurement experiments, and the attitude angle error is within 3 degrees. By combining haptic and visual information, the system realizes a more comprehensive and precise recognition of objects, which provides strong support for industrial automation and intelligent manufacturing. In summary, the effectiveness and advantages of multimodal fusion technology in object recognition tasks are remarkable and have a wide range of application potential.} }
@inproceedings{10.1145/3725988.3725994, title = {Applications and Comparative Analysis of Machine Learning and Statistical Learning Signal Processing in Music Genre Classification}, booktitle = {Proceedings of the 2025 9th International Conference on Digital Signal Processing}, pages = {15--26}, year = {2025}, isbn = {9798400710469}, doi = {10.1145/3725988.3725994}, url = {https://doi.org/10.1145/3725988.3725994}, author = {Wu, Yue and Cheng, Zhihan and Zhou, Kaige and Chen, Baixuan and Tong, Guanchao}, keywords = {Audio Signal Processing, Deep Learning, MFCC, Machine Learning, STFT}, abstract = {Music genre classification is a crucial task in audio signal processing and machine learning, widely applied in music recommendation systems, streaming platforms, and digital libraries. This study presents a comprehensive framework integrating advanced preprocessing techniques, feature extraction, and a comparative analysis of traditional machine learning models and deep learning architectures. The preprocessing pipeline employs the Short-Time Fourier Transform (STFT) to extract time-frequency domain information, complemented by the computation of Mel-Frequency Cepstral Coefficients (MFCC) and Mel spectrograms as essential features. Data augmentation and normalization are utilized to enhance the robustness and generalization of the models. The framework evaluates traditional machine learning methods, including Support Vector Machines (SVM), Naive Bayes, Random Forest, and XGBoost, alongside deep learning architectures such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRU). The performance of these models is evaluated using the GTZAN Music Genre Dataset, a widely used benchmark in the field of music genre classification, which provides a rich and diverse set of music genres for testing.\&nbsp;Experimental results assessed using metrics like accuracy, F1 score, and confusion matrix, demonstrate that deep learning models excel at leveraging time-frequency features, while traditional models perform effectively with smaller datasets. By combining Fourier Transform-based preprocessing with robust modeling strategies, this research offers a systematic approach to improving the music genre classification performance and provides valuable insights for advancements in music signal processing.} }
@inproceedings{10.5555/3566055.3566095, title = {Workshop: Machine Learning in Software Quality}, booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering}, pages = {249--250}, year = {2022}, author = {Azim, Akramul and Smith, Kevin}, abstract = {The workshop focuses on talks related to advancing the software quality paradigm using machine learning (ML) and/or artificial intelligence (AI). Software testing is an essential part of develop-ment that can be further advanced with the help of automation and effectively leveraging historical information. Continuous inte-gration environments enable large-scale software testing and the large volume of data generated in the process promotes the use of data analytics and ML techniques. Moreover, the historical infor-mation is useful to improve the next version of the software from models to code, which are essential components of software engi-neering. The workshop aims to cover the topics on software testing using ML, software testing in continuous integration environments, test case prioritization using ML, software quality assurance and requirements engineering using AI/ML.} }
@inproceedings{10.1145/3745533.3745632, title = {Reliability Analysis of Seismic Foundation Bearing Capacity via Machine Learning and Monte Carlo Simulation}, booktitle = {Proceedings of the 2025 5th International Conference on Applied Mathematics, Modelling and Intelligent Computing}, pages = {614--620}, year = {2025}, isbn = {9798400713873}, doi = {10.1145/3745533.3745632}, url = {https://doi.org/10.1145/3745533.3745632}, author = {Wang, Naixin and Wang, Yujie and Sun, Ping}, keywords = {Finite Element Limit Analysis (FELA), Monte Carlo Simulation (MCS), Multi-Layer Perceptron (MLP), Reliability analysis, Seismic foundation bearing capacity}, abstract = {The assessment of seismic foundation bearing capacity is crucial for ensuring the stability and safety of buildings, hydraulic structures, and bridges. Traditional safety factor methods are limited in capturing parameter uncertainties and quantifying failure probabilities. To address this issue, This study develops an efficient reliability assessment framework by integrating FELA with a Multi-Layer Perceptron (MLP) model. The FELA method is employed to compute seismic foundation bearing capacity and generate a large-scale dataset encompassing diverse parameter combinations. This dataset is used to train the MLP model, which establishes a highly accurate mapping between input parameters and ultimate bearing capacity. Monte Carlo Simulation (MCS) is incorporated to quantify failure probabilities and reliability indices under static and seismic conditions, significantly improving computational efficiency. The analysis determines a reliability index threshold of β = 2.58 for seismic conditions, providing a probabilistic reference for foundation safety evaluation. An engineering case study validates the effectiveness and practical applicability of the proposed approach. The combination of FELA and machine learning offers a robust and accurate method for seismic foundation reliability assessment, advancing the optimization of seismic foundation design.} }
@inproceedings{10.1145/3638530.3664166, title = {Towards Evolutionary-based Automated Machine Learning for Small Molecule Pharmacokinetic Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1544--1553}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3664166}, url = {https://doi.org/10.1145/3638530.3664166}, author = {de S\'a, Alex G. C. and Ascher, David B.}, keywords = {AutoML, bio(chem)informatics, grammar-based genetic programming, small molecules, pharmacokinetics, location = Melbourne, VIC, Australia}, abstract = {Machine learning (ML) is revolutionising drug discovery by expediting the prediction of small molecule properties essential for developing new drugs. These properties - including absorption, distribution, metabolism and excretion (ADME) - are crucial in the early stages of drug development since they provide an understanding of the course of the drug in the organism, i.e., the drug's pharmacokinetics. However, existing methods lack personalisation and rely on manually crafted ML algorithms or pipelines, which can introduce inefficiencies and biases into the process. To address these challenges, we propose a novel evolutionary-based automated ML method (AutoML) specifically designed for predicting small molecule properties, with a particular focus on pharmacokinetics. Leveraging the advantages of grammar-based genetic programming, our AutoML method streamlines the process by automatically selecting algorithms and designing predictive pipelines tailored to the particular characteristics of input molecular data. Results demonstrate AutoML's effectiveness in selecting diverse ML algorithms, resulting in comparable or even improved predictive performances compared to conventional approaches. By offering personalised ML-driven pipelines, our method promises to enhance small molecule research in drug discovery, providing researchers with a valuable tool for accelerating the development of novel therapeutic drugs.} }
@inproceedings{10.5555/3712729.3712877, title = {Enhancing Machine Learning for Situation Aware Dispatching through Generative Adversarial Network Based Synthetic Data Generation}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {1785--1796}, year = {2025}, isbn = {9798331534202}, author = {Chan, Chew Wye and Gan, Boon Ping and Cai, Wentong}, abstract = {Adapting dispatch rules via machine learning in a complex manufacturing environment has shown overall factory performance in various studies. However, the performance of the machine learning model depends on the training data. Limited data could reduce the prediction accuracy of the machine learning model, thereby negatively influencing the overall factory performance. Addressing this, we generate synthetic data for the lot attributes, simulate it through a discrete event simulator, and use the resulting data to improve the prediction accuracy for the machine learning model. We evaluate three synthetic data generation approaches: Latin Hypercube, Synthetic Minority Oversampling Technique, and Generative Adversarial Networks (GAN), demonstrating GAN suitability for synthetic data generation. To validate our approach, we apply two evaluation processes: Train on Real, Test on Real, and Train on Synthetic, Test on Real, showing the improved predictive accuracy of the machine learning model when trained with synthetic data.} }
@inproceedings{10.1145/3659677.3659707, title = {Active Metadata and Machine Learning based Framework for Enhancing Big Data Quality}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659707}, url = {https://doi.org/10.1145/3659677.3659707}, author = {Elouataoui, Widad and El Mendili, Saida and Gahi, Youssef}, keywords = {Active metadata, Anomaly correction, Anomaly detection, Big data quality, Machine learning, location = Meknes, AA, Morocco}, abstract = {The advent of big data has ushered in a new era of opportunities across industries, facilitating transformative insights and operational enhancements. However, the inherent challenges of big data, including its voluminous nature, rapid generation pace, and heterogeneous sources, pose serious issues to data quality. Inaccuracies, incompleteness, and inconsistency undermine the integrity and reliability of analytics, necessitating robust solutions for quality assurance. Despite the recognition of these challenges, existing solutions often lack comprehensive and adaptable mechanisms to ensure data quality throughout its lifecycle. To address this gap, this paper proposes a novel framework using active metadata, enriched with machine learning capabilities, to enhance big data quality effectively and intelligently. The framework comprises five key steps, starting with metadata acquisition to provide foundational insights into data characteristics. Subsequent phases involve advanced preprocessing techniques, machine learning-based anomaly detection using acquired metadata, and correction of anomalies using predictive models within appropriate metadata neighborhoods. Based on active metadata and machine learning, the framework automatically identifies and rectifies discrepancies, thereby improving overall data reliability and usability. Experimental validation of the proposed framework using a large dataset demonstrates its efficacy in correcting quality anomalies.} }
@proceedings{10.1145/3697467, title = {IoTML '24: Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, year = {2024}, isbn = {9798400710353} }
@inproceedings{10.1145/3756580.3756620, title = {An Empirical Analysis of Factors Influencing the Effectiveness of Legal Education in Chinese Universities Using Machine Learning}, booktitle = {Proceedings of the 2025 6th International Conference on Education, Knowledge and Information Management}, pages = {245--249}, year = {2025}, isbn = {9798400715624}, doi = {10.1145/3756580.3756620}, url = {https://doi.org/10.1145/3756580.3756620}, author = {Yang, Danming and Lv, Tong and Wu, Miaoxian and Wu, Jinhe and Xie, Xinjiu and Wu, Ruiqing}, keywords = {Legal education in universities, SHAP interpretation, XGBoost, learning attitude, legal cognition, machine learning}, abstract = {Legal education is essential for improving students’ legal awareness, yet its effectiveness often falls short, and the key influencing factors remain understudied using quantitative methods. This study uses survey data from university students and combines machine learning with traditional statistical approaches to identify the drivers of legal education outcomes. A questionnaire was designed to assess legal cognition and learning attitudes. Using XGBoost and SHAP values, the model revealed that active participation, teacher-student interaction, and real-life content relevance were the strongest predictors of legal cognition. In contrast, background factors such as being a law major or taking legal courses had minimal impact. These findings were validated through PCA, multiple linear regression, logistic regression, and t-tests. Logistic models struggled with class imbalance, resulting in low AUC (∼0.6), which limits their predictive utility. The results underscore that student engagement and positive learning attitudes are more important than formal background in shaping legal understanding. This study provides empirical evidence to guide improvements in legal education and suggests future research directions, including behavioral data integration, class imbalance solutions, and deep learning applications.} }
@inproceedings{10.1145/3745812.3745845, title = {Enhancing Early Detection of Coronary Artery Disease with Machine Learning and Clinical Data Integration}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745845}, url = {https://doi.org/10.1145/3745812.3745845}, author = {Singh, Uday Pratap and Kumari, Bersha and Choudhary, Madhu and Tanveer, Adil}, keywords = {Cardiovascular Disease, Early Detection, Healthcare Informatics, Medical Data Analysis, Supervised Machine Learning}, abstract = {The foremost step in the precise diagnosis of cardiovascular disease is the early and quick identification of patients showing heart disease. Most conventional cardiac diagnostic methods are very tedious and subjective, using lots of lab investigations and human interpretation. The present study, therefore, attempts to apply machine learning (ML) as one of the approaches for fast-tracking the diagnoses. We constitute several classification algorithms such as K-Nearest Neighbors (KNN), Random Forest, Naive Bayes Classifier, Support Vector Machine (SVM), and Decision Tree for examining basic clinical indicators of tobacco usage, obesity, blood sugar levels, and blood pressure. The aim of this development is to produce a highly accurate cardiovascular disease predictive model that may reduce and/or eliminate invasive tests, ensure rapid diagnosis, and improve patient outcomes.} }
@inproceedings{10.1145/3756423.3756451, title = {Research on the Application of Machine Learning Algorithms in Marketing Data Analysis under the Background of Artificial Intelligence}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing}, pages = {179--184}, year = {2025}, isbn = {9798400714351}, doi = {10.1145/3756423.3756451}, url = {https://doi.org/10.1145/3756423.3756451}, author = {Li, Baohong}, keywords = {Customer, Machine learning, Marketing data}, abstract = {In the digital age, the marketing industry is undergoing profound changes. With the rapid development of Internet technology, consumers' behavioral patterns and shopping habits have undergone tremendous changes. They make decisions based on a vast amount of information, which makes it difficult for traditional marketing methods to precisely reach target customers. Machine learning algorithms, as an important branch in the field of artificial intelligence, possess powerful capabilities in data processing and analysis. It can automatically learn patterns and rules from a large amount of data, thereby achieving the prediction and classification of unknown data. In marketing data analysis, machine learning algorithms can help enterprises discover the potential demands of consumers, accurately identify target customer groups, optimize marketing strategies, and enhance marketing effectiveness and return on investment. This study aims to deeply analyze the application of machine learning algorithms in marketing data analysis and compare the performance and applicable scenarios of different algorithms. Through the analysis of actual cases and empirical research on data, the advantages and potential of machine learning algorithms in improving marketing effectiveness are revealed, providing theoretical support and practical guidance for enterprises to rationally select and apply machine learning algorithms in marketing decisions.} }
@inproceedings{10.1145/3685088.3685106, title = {Analysis of Smart Home Applications Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Smart City and Information System}, pages = {88--92}, year = {2024}, isbn = {9798400710155}, doi = {10.1145/3685088.3685106}, url = {https://doi.org/10.1145/3685088.3685106}, author = {Xu, Jianing and Jiang, Ying and Lou, Fei and Chen, Haonan}, abstract = {This article conducts research on smart homes and proposes a semantic connectivity model by analyzing the daily life behavior of users. This model can describe the association relationships between entities and devices in the user's living environment and is applied in behavior recognition and assistance algorithms, providing assistance services to users while maximizing the satisfaction of actual user habits. Finally, the proposed algorithm was simulated and evaluated using a publicly available database. The simulation results showed that the hybrid semantic model proposed in this paper can effectively improve the accuracy of life service assistance algorithms while reducing the algorithm's dependence on individual behavior models.} }
@inproceedings{10.1145/3461702.3462611, title = {Quantum Fair Machine Learning}, booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {843--853}, year = {2021}, isbn = {9781450384735}, doi = {10.1145/3461702.3462611}, url = {https://doi.org/10.1145/3461702.3462611}, author = {Perrier, Elija}, keywords = {fair, learning, machine, quantum, location = Virtual Event, USA}, abstract = {In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.} }
@inproceedings{10.1145/3534678.3542636, title = {Automated Machine Learning \&amp; Tuning with FLAML}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4828--4829}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542636}, url = {https://doi.org/10.1145/3534678.3542636}, author = {Wang, Chi and Wu, Qingyun and Liu, Xueqing and Quintanilla, Luis}, keywords = {automl, tutorial, location = Washington DC, USA}, abstract = {In this tutorial, we will provide an in-depth and hands-on tutorial on Automated Machine Learning \&amp; Tuning with a fast python library FLAML. We will start with an overview of the AutoML problem and the FLAML library. In the first half of the tutorial, we will then give a hands-on tutorial on how to use FLAML to automate typical machine learning tasks in an end-to-end manner with different customization options and how to perform general tuning tasks on user-defined functions. In the second half of the tutorial, we will introduce several advanced functionalities of the library. For example, zero-shot AutoML, fair AutoML, and online AutoML. We will close the tutorial with several open problems, and challenges learned from AutoML practice.} }
@inproceedings{10.1145/3703935.3703979, title = {Research on Printing Icon Defect Detection Based on Variation Model and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Artificial Intelligence and Pattern Recognition}, pages = {615--622}, year = {2025}, isbn = {9798400717178}, doi = {10.1145/3703935.3703979}, url = {https://doi.org/10.1145/3703935.3703979}, author = {Liu, Jiawei and Li, Yanjin and Qi, Wenjing and Li, Yule and Liu, Jihong}, keywords = {defect detection, machine vision, support vector machine, variation model}, abstract = {A printing icon defect detection method based on variation model and machine learning is proposed to address the shortcomings of defect detection methods for glass bottle printing on industrial production lines. On the basis of using the standard variation model for bottle icon detection, subjective measurement standards are introduced from two perspectives to construct a locally variable variation model, and support vector machine in machine learning is combined to address the false detection problem in the detection process. The experimental results show that compared with existing methods, the research method proposed in this paper can more accurately detect defective bottle icons, and is more subjective and efficient in practical applications.} }
