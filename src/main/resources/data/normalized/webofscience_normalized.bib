@article{WOS:000358218600040, title = {Machine learning: Trends, perspectives, and prospects}, journal = {SCIENCE}, volume = {349}, pages = {255-260}, year = {2015}, issn = {0036-8075}, doi = {10.1126/science.aaa8415}, author = {Jordan, M. I. and Mitchell, T. M.}, abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.} }
@article{WOS:000439850800044, title = {Machine learning for molecular and materials science}, journal = {NATURE}, volume = {559}, pages = {547-555}, year = {2018}, issn = {0028-0836}, doi = {10.1038/s41586-018-0337-2}, author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron}, abstract = {Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.} }
@article{WOS:001172157700001, title = {A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Human Action Recognition in Machine and Robotic Vision}, journal = {TECHNOLOGIES}, volume = {12}, year = {2024}, doi = {10.3390/technologies12020015}, author = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and Fragulis, George F.}, abstract = {Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications.} }
@article{WOS:001296591100001, title = {Pushing the frontiers in climate modelling and analysis with machine learning}, journal = {NATURE CLIMATE CHANGE}, year = {2024}, issn = {1758-678X}, doi = {10.1038/s41558-024-02095-y}, author = {Eyring, Veronika and Collins, William D. and Gentine, Pierre and Barnes, Elizabeth A. and Barreiro, Marcelo and Beucler, Tom and Bocquet, Marc and Bretherton, Christopher S. and Christensen, Hannah M. and Dagon, Katherine and Gagne, David John and Hall, David and Hammerling, Dorit and Hoyer, Stephan and Iglesias-Suarez, Fernando and Lopez-Gomez, Ignacio and Mcgraw, Marie C. and Meehl, Gerald A. and Molina, Maria J. and Monteleoni, Claire and Mueller, Juliane and Pritchard, Michael S. and Rolnick, David and Runge, Jakob and Stier, Philip and Watt-Meyer, Oliver and Weigel, Katja and Yu, Rose and Zanna, Laure}, abstract = {Climate modelling and analysis are facing new demands to enhance projections and climate information. Here we argue that now is the time to push the frontiers of machine learning beyond state-of-the-art approaches, not only by developing machine-learning-based Earth system models with greater fidelity, but also by providing new capabilities through emulators for extreme event projections with large ensembles, enhanced detection and attribution methods for extreme events, and advanced climate model analysis and benchmarking. Utilizing this potential requires key machine learning challenges to be addressed, in particular generalization, uncertainty quantification, explainable artificial intelligence and causality. This interdisciplinary effort requires bringing together machine learning and climate scientists, while also leveraging the private sector, to accelerate progress towards actionable climate science. Machine learning methods allow for advances in many aspects of climate research. In this Perspective, the authors give an overview of recent progress and remaining challenges to harvest the full potential of machine learning methods.} }
@article{WOS:001203939100001, title = {Machine learning for battery systems applications: Progress, challenges, and opportunities}, journal = {JOURNAL OF POWER SOURCES}, volume = {601}, year = {2024}, issn = {0378-7753}, doi = {10.1016/j.jpowsour.2024.234272}, author = {Nozarijouybari, Zahra and Fathy, Hosam K.}, abstract = {Machine learning has emerged as a transformative force throughout the entire engineering life cycle of electrochemical batteries. Its applications encompass a wide array of critical domains, including material discovery, model development, quality control during manufacturing, real-time monitoring, state estimation, optimization of charge cycles, fault detection, and life cycle management. Machine learning excels in its ability to identify and capture complex behavioral trends in batteries, which may be challenging to model using more traditional methods. The goal of this survey paper is to synthesize the rich existing literature on battery machine learning into a structured perspective on the successes, challenges, and prospects within this research domain. This critical examination highlights several key insights. Firstly, the selection of data sets, features, and algorithms significantly influences the success of machine learning applications, yet it remains an open research area with vast potential. Secondly, data set richness and size are both pivotal for the efficacy of machine learning algorithms, suggesting a potential for active machine learning techniques in the battery systems domain. Lastly, the field of machine learning in battery systems has extensive room for growth, moving beyond its current focus on specific applications like state of charge (SOC) and state of health (SOH) estimation, offering ample opportunities for innovation and expansion.} }
@article{WOS:001507295200001, title = {A Review of Machine Learning Algorithms for Biomedical Applications}, journal = {ANNALS OF BIOMEDICAL ENGINEERING}, volume = {52}, pages = {1159-1183}, year = {2024}, issn = {0090-6964}, doi = {10.1007/s10439-024-03459-3}, author = {Binson, V. A. and Thomas, Sania and Subramoniam, M. and Arun, J. and Naveen, S. and Madhu, S.}, abstract = {As the amount and complexity of biomedical data continue to increase, machine learning methods are becoming a popular tool in creating prediction models for the underlying biomedical processes. Although all machine learning methods aim to fit models to data, the methodologies used can vary greatly and may seem daunting at first. A comprehensive review of various machine learning algorithms per biomedical applications is presented. The key concepts of machine learning are supervised and unsupervised learning, feature selection, and evaluation metrics. Technical insights on the major machine learning methods such as decision trees, random forests, support vector machines, and k-nearest neighbors are analyzed. Next, the dimensionality reduction methods like principal component analysis and t-distributed stochastic neighbor embedding methods, and their applications in biomedical data analysis were reviewed. Moreover, in biomedical applications predominantly feedforward neural networks, convolutional neural networks, and recurrent neural networks are utilized. In addition, the identification of emerging directions in machine learning methodology will serve as a useful reference for individuals involved in biomedical research, clinical practice, and related professions who are interested in understanding and applying machine learning algorithms in their research or practice.} }
@article{WOS:001064661300018, title = {Machine learning applications in stroke medicine: advancements, challenges, and future prospectives}, journal = {NEURAL REGENERATION RESEARCH}, volume = {19}, pages = {769-773}, year = {2024}, issn = {1673-5374}, doi = {10.4103/1673-5374.382228}, author = {Daidone, Mario and Ferrantelli, Sergio and Tuttolomondo, Antonino}, abstract = {Stroke is a leading cause of disability and mortality worldwide, necessitating the development of advanced technologies to improve its diagnosis, treatment, and patient outcomes. In recent years, machine learning techniques have emerged as promising tools in stroke medicine, enabling efficient analysis of large-scale datasets and facilitating personalized and precision medicine approaches. This abstract provides a comprehensive overview of machine learning's applications, challenges, and future directions in stroke medicine. Recently introduced machine learning algorithms have been extensively employed in all the fields of stroke medicine. Machine learning models have demonstrated remarkable accuracy in imaging analysis, diagnosing stroke subtypes, risk stratifications, guiding medical treatment, and predicting patient prognosis. Despite the tremendous potential of machine learning in stroke medicine, several challenges must be addressed. These include the need for standardized and interoperable data collection, robust model validation and generalization, and the ethical considerations surrounding privacy and bias. In addition, integrating machine learning models into clinical workflows and establishing regulatory frameworks are critical for ensuring their widespread adoption and impact in routine stroke care. Machine learning promises to revolutionize stroke medicine by enabling precise diagnosis, tailored treatment selection, and improved prognostication. Continued research and collaboration among clinicians, researchers, and technologists are essential for overcoming challenges and realizing the full potential of machine learning in stroke care, ultimately leading to enhanced patient outcomes and quality of life. This review aims to summarize all the current implications of machine learning in stroke diagnosis, treatment, and prognostic evaluation. At the same time, another purpose of this paper is to explore all the future perspectives these techniques can provide in combating this disabling disease.} }
@article{WOS:001219213300001, title = {Machine learning in construction and demolition waste management: Progress, challenges, and future directions}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {162}, year = {2024}, issn = {0926-5805}, doi = {10.1016/j.autcon.2024.105380}, author = {Gao, Yu and Wang, Jiayuan and Xu, Xiaoxiao}, abstract = {The application of machine learning contributes to intelligent and efficient management of construction and demolition waste, leading to a reduction in waste generation and an increased emphasis on recycling. This research conducts a comprehensive analysis of 98 journals related to the application of machine learning in construction waste management from 2012 to 2023 to identify current hot topics and emerging trends. The results reveal that machine learning is applied in four main areas and 15 subfields, specifically focusing on construction and demolition waste generation, on-site handling, transportation, and disposal. Various models, such as artificial neural networks, deep learning, convolutional neural networks, and support vector machines, demonstrate their effectiveness in different processes of construction and demolition waste management. The findings of this research will aid researchers in gaining a comprehensive understanding of the current state and future directions of machine learning in construction waste management.} }
@article{WOS:001137399100001, title = {Machine Learning to Solve Vehicle Routing Problems: A Survey}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {25}, pages = {4754-4772}, year = {2024}, issn = {1524-9050}, doi = {10.1109/TITS.2023.3334976}, author = {Bogyrbayeva, Aigerim and Meraliyev, Meraryslan and Mustakhov, Taukekhan and Dauletbayev, Bissenbay}, abstract = {This paper provides a systematic overview of machine learning methods applied to solve NP-hard Vehicle Routing Problems (VRPs). Recently, there has been great interest from both the machine learning and operations research communities in solving VRPs either through pure learning methods or by combining them with traditional handcrafted heuristics. We present a taxonomy of studies on learning paradigms, solution structures, underlying models, and algorithms. Detailed results of state-of-the-art methods are presented, demonstrating their competitiveness with traditional approaches. The survey highlights the advantages of the machine learning-based models that aim to exploit the symmetry of VRP solutions. The paper outlines future research directions to incorporate learning-based solutions to address the challenges of modern transportation systems.} }
@article{WOS:001310661500002, title = {Cardiovascular disease diagnosis: a holistic approach using the integration of machine learning and deep learning models}, journal = {EUROPEAN JOURNAL OF MEDICAL RESEARCH}, volume = {29}, year = {2024}, issn = {0949-2321}, doi = {10.1186/s40001-024-02044-7}, author = {Sadr, Hossein and Salari, Arsalan and Ashoobi, Mohammad Taghi and Nazari, Mojdeh}, abstract = {BackgroundThe incidence and mortality rates of cardiovascular disease worldwide are a major concern in the healthcare industry. Precise prediction of cardiovascular disease is essential, and the use of machine learning and deep learning can aid in decision-making and enhance predictive abilities.ObjectiveThe goal of this paper is to introduce a model for precise cardiovascular disease prediction by combining machine learning and deep learning.MethodTwo public heart disease classification datasets with 70,000 and 1190 records besides a locally collected dataset with 600 records were used in our experiments. Then, a model which makes use of both machine learning and deep learning was proposed in this paper. The proposed model employed CNN and LSTM, as the representatives of deep learning models, besides KNN and XGB, as the representatives of machine learning models. As each classifier defined the output classes, majority voting was then used as an ensemble learner to predict the final output class.ResultThe proposed model obtained the highest classification performance based on all evaluation metrics on all datasets, demonstrating its suitability and reliability in forecasting the probability of cardiovascular disease.} }
@article{WOS:001263598700003, title = {Fuzzy Machine Learning: A Comprehensive Framework and Systematic Review}, journal = {IEEE TRANSACTIONS ON FUZZY SYSTEMS}, volume = {32}, pages = {3861-3878}, year = {2024}, issn = {1063-6706}, doi = {10.1109/TFUZZ.2024.3387429}, author = {Lu, Jie and Ma, Guangzhi and Zhang, Guangquan}, abstract = {Machine learning draws its power from various disciplines, including computer science, cognitive science, and statistics. Although machine learning has achieved great advancements in both theory and practice, its methods have some limitations when dealing with complex situations and highly uncertain environments. Insufficient data, imprecise observations, and ambiguous information/relationships can all confound traditional machine learning systems. To address these problems, researchers have integrated machine learning from different aspects and fuzzy techniques, including fuzzy sets, fuzzy systems, fuzzy logic, fuzzy measures, fuzzy relations, and so on. This article presents a systematic review of fuzzy machine learning, from theory, approach to application, with the overall objective of providing an overview of recent achievements in the field of fuzzy machine learning. To this end, the concepts and frameworks discussed are divided into five categories: 1) fuzzy classical machine learning; 2) fuzzy transfer learning; 3) fuzzy data stream learning; 4) fuzzy reinforcement learning; and 5) fuzzy recommender systems. The literature presented should provide researchers with a solid understanding of the current progress in fuzzy machine learning research and its applications.} }
@article{WOS:001166743400001, title = {Machine learning assisted biosensing technology: An emerging powerful tool for improving the intelligence of food safety detection}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {8}, year = {2024}, doi = {10.1016/j.crfs.2024.100679}, author = {Zhou, Zixuan and Tian, Daoming and Yang, Yingao and Cui, Han and Li, Yanchun and Ren, Shuyue and Han, Tie and Gao, Zhixian}, abstract = {Recently, the application of biosensors in food safety assessment has gained considerable research attention. Nevertheless, the evaluation of biosensors' sensitivity, accuracy, and efficiency is still ongoing. The advent of machine learning has enhanced the application of biosensors in food security assessment, yielding improved results. Machine learning has been preliminarily applied in combination with different biosensors in food safety assessment, with positive results. This review offers a comprehensive summary of the diverse machine learning methods employed in biosensors for food safety. Initially, the primary machine learning methods were outlined, and the integrated application of biosensors and machine learning in food safety was thoroughly examined. Lastly, the challenges and limitations of machine learning and biosensors in the realm of food safety were underscored, and potential solutions were explored. The review's findings demonstrated that algorithms grounded in machine learning can aid in the early detection of food safety issues. Furthermore, preliminary research suggests that biosensors could be optimized through machine learning for real-time, multifaceted analyses of food safety variables and their interactions. The potential of machine learning and biosensors in realtime monitoring of food quality has been discussed.} }
@article{WOS:001155473600001, title = {Expanding the Horizons of Machine Learning in Nanomaterials to Chiral Nanostructures}, journal = {ADVANCED MATERIALS}, volume = {36}, year = {2024}, issn = {0935-9648}, doi = {10.1002/adma.202308912}, author = {Kuznetsova, Vera and Coogan, Aine and Botov, Dmitry and Gromova, Yulia and Ushakova, Elena V. and Gun'ko, Yurii K.}, abstract = {Machine learning holds significant research potential in the field of nanotechnology, enabling nanomaterial structure and property predictions, facilitating materials design and discovery, and reducing the need for time-consuming and labor-intensive experiments and simulations. In contrast to their achiral counterparts, the application of machine learning for chiral nanomaterials is still in its infancy, with a limited number of publications to date. This is despite the great potential of machine learning to advance the development of new sustainable chiral materials with high values of optical activity, circularly polarized luminescence, and enantioselectivity, as well as for the analysis of structural chirality by electron microscopy. In this review, an analysis of machine learning methods used for studying achiral nanomaterials is provided, subsequently offering guidance on adapting and extending this work to chiral nanomaterials. An overview of chiral nanomaterials within the framework of synthesis-structure-property-application relationships is presented and insights on how to leverage machine learning for the study of these highly complex relationships are provided. Some key recent publications are reviewed and discussed on the application of machine learning for chiral nanomaterials. Finally, the review captures the key achievements, ongoing challenges, and the prospective outlook for this very important research field. This review analyzes machine learning methods for studying achiral nanomaterials and offers guidance for adapting and extending this work to chiral nanomaterials. An overview of chiral nanomaterials in the context of synthesis-structure-property-application relationships is presented, offering insights on leveraging machine learning for these complex relationships. Key achievements, challenges, and outlook for machine learning in chiral nanomaterials research are discussed. image} }
@article{WOS:001156411300001, title = {The role of hyperparameters in machine learning models and how to tune them}, journal = {POLITICAL SCIENCE RESEARCH AND METHODS}, volume = {12}, pages = {841-848}, year = {2024}, issn = {2049-8470}, doi = {10.1017/psrm.2023.61}, author = {Arnold, Christian and Biedebach, Luka and Kuepfer, Andreas and Neunhoeffer, Marcel}, abstract = {Hyperparameters critically influence how well machine learning models perform on unseen, out-of-sample data. Systematically comparing the performance of different hyperparameter settings will often go a long way in building confidence about a model's performance. However, analyzing 64 machine learning related manuscripts published in three leading political science journals (APSR, PA, and PSRM) between 2016 and 2021, we find that only 13 publications (20.31 percent) report the hyperparameters and also how they tuned them in either the paper or the appendix. We illustrate the dangers of cursory attention to model and tuning transparency in comparing machine learning models' capability to predict electoral violence from tweets. The tuning of hyperparameters and their documentation should become a standard component of robustness checks for machine learning models.} }
@article{WOS:001335131500001, title = {Avoiding common machine learning pitfalls}, journal = {PATTERNS}, volume = {5}, year = {2024}, issn = {2666-3899}, doi = {10.1016/j.patter.2024.101046}, author = {Lones, Michael A.}, abstract = {Mistakes in machine learning practice are commonplace and can result in loss of confidence in the findings and products of machine learning. This tutorial outlines common mistakes that occur when using machine learning and what can be done to avoid them. While it should be accessible to anyone with a basic understanding of machine learning techniques, it focuses on issues that are of particular concern within academic research, such as the need to make rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.} }
@article{WOS:001230185600011, title = {In-Network Machine Learning Using Programmable Network Devices: A Survey}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {26}, pages = {1171-1200}, year = {2024}, doi = {10.1109/COMST.2023.3344351}, author = {Zheng, Changgang and Hong, Xinpeng and Ding, Damu and Vargaftik, Shay and Ben-Itzhak, Yaniv and Zilberman, Noa}, abstract = {Machine learning is widely used to solve networking challenges, ranging from traffic classification and anomaly detection to network configuration. However, machine learning also requires significant processing and often increases the load on both networks and servers. The introduction of in-network computing, enabled by programmable network devices, has allowed to run applications within the network, providing higher throughput and lower latency. Soon after, in-network machine learning solutions started to emerge, enabling machine learning functionality within the network itself. This survey introduces the concept of in-network machine learning and provides a comprehensive taxonomy. The survey provides an introduction to the technology and explains the different types of machine learning solutions built upon programmable network devices. It explores the different types of machine learning models implemented within the network, and discusses related challenges and solutions. In-network machine learning can significantly benefit cloud computing and next-generation networks, and this survey concludes with a discussion of future trends.} }
@article{WOS:001359921300001, title = {A roadmap to fault diagnosis of industrial machines via machine learning: A brief review}, journal = {MEASUREMENT}, volume = {242}, year = {2025}, issn = {0263-2241}, doi = {10.1016/j.measurement.2024.116216}, author = {Vashishtha, Govind and Chauhan, Sumika and Sehri, Mert and Zimroz, Radoslaw and Dumond, Patrick and Kumar, Rajesh and Gupta, Munish Kumar}, abstract = {In fault diagnosis, machine learning theories are gaining popularity as they proved to be an efficient tool that not only reduces human effort but also identifies the health conditions of the machines automatically. In this work, an attempt has been made to systematically review the progress of machine learning theories in fault diagnosis from scratch to future perspectives. Initially, artificial intelligence came into the picture which started to weaken the human effort whose efficiency relies on feature extraction which depends on expert knowledge. The introduction of deep learning theories has reformed the fault diagnosis process by realising the artificial aid, encouraging end-to-end encryption in the diagnostic procedure. The deep learning theories have also filled the gap between the large amount of monitoring data and the health conditions of industrial machines. The future of deep learning theories i.e. transfer learning which uses the knowledge of one domain to another related domain during fault diagnosis has been reviewed. In last, the research trends of the machine learning theories have been briefly discussed along with their challenges in fault diagnostics.} }
@article{WOS:001334283900001, title = {A comprehensive review of quantum machine learning: from NISQ to fault tolerance}, journal = {REPORTS ON PROGRESS IN PHYSICS}, volume = {87}, year = {2024}, issn = {0034-4885}, doi = {10.1088/1361-6633/ad7f69}, author = {Wang, Yunfei and Liu, Junyu}, abstract = {Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.} }
@article{WOS:001166777200003, title = {Machine learning in cartography}, journal = {CARTOGRAPHY AND GEOGRAPHIC INFORMATION SCIENCE}, volume = {51}, pages = {1-19}, year = {2024}, issn = {1523-0406}, doi = {10.1080/15230406.2023.2295948}, author = {Harrie, Lars and Touya, Guillaume and Oucheikh, Rachid and Ai, Tinghua and Courtial, Azelle and Richter, Kai-Florian}, abstract = {Machine learning is increasingly used as a computing paradigm in cartographic research. In this extended editorial, we provide some background of the papers in the CaGIS special issue Machine Learning in Cartography with a special focus on pattern recognition in maps, cartographic generalization, style transfer, and map labeling. In addition, the paper includes a discussion about map encodings for machine learning applications and the possible need for explicit cartographic knowledge and procedural modeling in cartographic machine learning models.} }
@article{WOS:001331425700001, title = {Nanofluid heat transfer and machine learning: Insightful review of machine learning for nanofluid heat transfer enhancement in porous media and heat exchangers as sustainable and renewable energy solutions}, journal = {RESULTS IN ENGINEERING}, volume = {24}, year = {2024}, issn = {2590-1230}, doi = {10.1016/j.rineng.2024.103002}, author = {Riyadi, Tri W. B. and Herawan, Safarudin G. and Tirta, Andy and Ee, Yit Jing and Hananto, April Lia and Paristiawan, Permana A. and Yusuf, Abdulfatah Abdu and Venu, Harish and Irianto and Veza, Ibham}, abstract = {Nanofluid, coupled with machine learning, is at the forefront of cutting-edge research in sustainable and renewable energy sector. This review paper examines the latest developments in the intersection of nanofluid and machine learning for heat transfer enhancement. This hybrid nanofluid-machine learning review investigates nanofluid heat transfer enhancement leveraged by machine learning both in porous media as well as heat exchangers. Several studies in porous media nanofluid transport utilize advanced methodologies that integrate machine learning and computational techniques. Machine learning and computational methods are employed to tackle complex thermodynamics, transport processes, and heat transfer challenges in complex multiphysics systems. An interesting hybrid nanofluid-machine learning application involves applying a machine learning method such as Support Vector Machine (SVM) to forecast movement of hybrid nanofluid flows across porous surfaces. Such hybrid nanofluid-machine learning technique involves utilising training data obtained from computational fluid dynamics (CFD) to decrease computational time and expenses. Machine learning offers a more efficient and cost-effective modelling for nanofluid heat transfer enhancement. Techniques such as scanning electron microscopy (SEM) along with X-ray diffraction (XRD) are also often used for assessing the forms as well as nanocomposites configurations in heat exchangers while studying nanofluids. The importance of machine learning models, especially artificial neural networks (ANNs) and genetic algorithms, is evident in their ability to predict and optimize thermal performance of nanofluid application for nanofluid heat transfer enhancement. Furthermore, integrating nanofluids into various heat exchanger designs has demonstrated significant enhancements in efficiency, decreased energy usage, and total cost reduction. These achievements align with the research goal in sustainable and renewable energy, highlighting the critical role of nanofluid-enhanced heat exchange systems in tackling current difficulties related to energy efficiency and sustainability. Overall, combining nanofluids with machine learning shows promising advancements, providing a route toward creating more efficient and eco-friendly heat exchange systems.} }
@article{WOS:001225931500001, title = {Research trends in deep learning and machine learning for cloud computing security}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {57}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-10776-5}, author = {Alzoubi, Yehia Ibrahim and Mishra, Alok and Topcu, Ahmet Ercan}, abstract = {Deep learning and machine learning show effectiveness in identifying and addressing cloud security threats. Despite the large number of articles published in this field, there remains a dearth of comprehensive reviews that synthesize the techniques, trends, and challenges of using deep learning and machine learning for cloud computing security. Accordingly, this paper aims to provide the most updated statistics on the development and research in cloud computing security utilizing deep learning and machine learning. Up to the middle of December 2023, 4051 publications were identified after we searched the Scopus database. This paper highlights key trend solutions for cloud computing security utilizing machine learning and deep learning, such as anomaly detection, security automation, and emerging technology's role. However, challenges such as data privacy, scalability, and explainability, among others, are also identified as challenges of using machine learning and deep learning for cloud security. The findings of this paper reveal that deep learning and machine learning for cloud computing security are emerging research areas. Future research directions may include addressing these challenges when utilizing machine learning and deep learning for cloud security. Additionally, exploring the development of algorithms and techniques that comply with relevant laws and regulations is essential for effective implementation in this domain.} }
@article{WOS:001230996600003, title = {ddml: Double/debiased machine learning in Stata}, journal = {STATA JOURNAL}, volume = {24}, pages = {3-45}, year = {2024}, issn = {1536-867X}, doi = {10.1177/1536867X241233641}, author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E. and Wiemann, Thomas}, abstract = {In this article, we introduce a package, ddml, for double/debiased machine learning in Stata. Estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in Stata. We recommend using double/debiased machine learning in combination with stacking estimation, which combines multiple machine learners into a final predictor. We provide Monte Carlo evidence to support our recommendation.} }
@article{WOS:001167870100001, title = {DoubleML: An Object-Oriented Implementation of Double Machine Learning in R}, journal = {JOURNAL OF STATISTICAL SOFTWARE}, volume = {108}, year = {2024}, issn = {1548-7660}, doi = {10.18637/jss.v108.i03}, author = {Bach, Philipp and Kurz, Malte S. and Chernozhukov, Victor and Spindler, Martin and Klaassen, Sven}, abstract = {The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consists of three key ingredients: Neyman orthogonality, high -quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object -oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods.} }
@article{WOS:001309841200001, title = {Planter: Rapid Prototyping of In-Network Machine Learning Inference}, journal = {ACM SIGCOMM COMPUTER COMMUNICATION REVIEW}, volume = {54}, pages = {2-20}, year = {2024}, issn = {0146-4833}, author = {Zheng, Changgang and Zang, Mingyuan and Hong, Xinpeng and Perreault, Liam and Bensoussane, Riyad and Vargaftik, Shay and Ben-Itzhak, Yaniv and Zilberman, Noa}, abstract = {In-network machine learning inference provides high throughput and low latency. It is ideally located within the network, power efficient, and improves applications' performance. Despite its advantages, the bar to in-network machine learning research is high, requiring significant expertise in programmable data planes, in addition to knowledge of machine learning and the application area. Existing solutions are mostly one-time efforts, hard to reproduce, change, or port across platforms. In this paper, we present Planter: a modular and efficient open-source framework for rapid prototyping of in-network machine learning models across a range of platforms and pipeline architectures. By identifying general mapping methodologies for machine learning algorithms, Planter introduces new machine learning mappings and improves existing ones. It provides users with several example use cases and supports different datasets, and was already extended by users to new fields and applications. Our evaluation shows that Planter improves machine learning performance compared with previous model-tailored works, while significantly reducing resource consumption and co-existing with network functionality. Planter-supported algorithms run at line rate on unmodified commodity hardware, providing billions of inference decisions per second.} }
@article{WOS:001206759600001, title = {Active Machine Learning for Chemical Engineers: A Bright Future Lies Ahead!}, journal = {ENGINEERING}, volume = {27}, pages = {23-30}, year = {2023}, issn = {2095-8099}, doi = {10.1016/j.eng.2023.02.019}, author = {Ureel, Yannick and Dobbelaere, Maarten R. and Ouyang, Yi and De Ras, Kevin and Sabbe, Maarten K. and Marin, Guy B. and Van Geem, Kevin M.}, abstract = {By combining machine learning with the design of experiments, thereby achieving so-called active machine learning, more efficient and cheaper research can be conducted. Machine learning algorithms are more flexible and are better than traditional design of experiment algorithms at investigating processes spanning all length scales of chemical engineering. While active machine learning algorithms are maturing, their applications are falling behind. In this article, three types of challenges presented by active machine learning-namely, convincing the experimental researcher, the flexibility of data creation, and the robustness of active machine learning algorithms-are identified, and ways to overcome them are discussed. A bright future lies ahead for active machine learning in chemical engineering, thanks to increasing automation and more efficient algorithms that can drive novel discoveries. (c) 2023 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:001139747600005, title = {Advancements in machine learning for material design and process optimization in the field of additive manufacturing}, journal = {CHINA FOUNDRY}, volume = {21}, pages = {101-115}, year = {2024}, issn = {1672-6421}, doi = {10.1007/s41230-024-3145-3}, author = {Zhou, Hao-ran and Yang, Hao and Li, Huai-qian and Ma, Ying-chun and Yu, Sen and Shi, Jian and Cheng, Jing-chang and Gao, Peng and Yu, Bo and Miao, Zhi-quan and Wei, Yan-peng}, abstract = {Additive manufacturing technology is highly regarded due to its advantages, such as high precision and the ability to address complex geometric challenges. However, the development of additive manufacturing process is constrained by issues like unclear fundamental principles, complex experimental cycles, and high costs. Machine learning, as a novel artificial intelligence technology, has the potential to deeply engage in the development of additive manufacturing process, assisting engineers in learning and developing new techniques. This paper provides a comprehensive overview of the research and applications of machine learning in the field of additive manufacturing, particularly in model design and process development. Firstly, it introduces the background and significance of machine learning-assisted design in additive manufacturing process. It then further delves into the application of machine learning in additive manufacturing, focusing on model design and process guidance. Finally, it concludes by summarizing and forecasting the development trends of machine learning technology in the field of additive manufacturing.} }
@article{WOS:000611065800001, title = {Explainable AI: A Review of Machine Learning Interpretability Methods}, journal = {ENTROPY}, volume = {23}, year = {2021}, doi = {10.3390/e23010018}, author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris}, abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.} }
@article{WOS:000895445500045, title = {Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {35}, pages = {614-633}, year = {2023}, issn = {1041-4347}, doi = {10.1109/TKDE.2021.3079836}, author = {von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Michal and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis}, abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.} }
@article{WOS:000957059700001, title = {Small data machine learning in materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {9}, year = {2023}, doi = {10.1038/s41524-023-01000-z}, author = {Xu, Pengcheng and Ji, Xiaobo and Li, Minjie and Lu, Wencong}, abstract = {This review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by small data. Then, the workflow of materials machine learning has been introduced. Next, the methods of dealing with small data were introduced, including data extraction from publications, materials database construction, high-throughput computations and experiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level; active learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machine learning in materials science were proposed.} }
@article{WOS:001171070000005, title = {Machine Learning and Health Science Research: Tutorial}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {26}, year = {2024}, issn = {1438-8871}, doi = {10.2196/50890}, author = {Cho, Hunyong and She, Jane and De Marchi, Daniel and El-Zaatari, Helal and Barnes, Edward L. and Kahkoska, Anna R. and Kosorok, Michael R. and Virkud, Arti V.}, abstract = {Machine learning (ML) has seen impressive growth in health science research due to its capacity for handling complex data to perform a range of tasks, including unsupervised learning, supervised learning, and reinforcement learning. To aid health science researchers in understanding the strengths and limitations of ML and to facilitate its integration into their studies, we present here a guideline for integrating ML into an analysis through a structured framework, covering steps from framing a research question to study design and analysis techniques for specialized data types.} }
@article{WOS:000893245700006, title = {Challenges in Deploying Machine Learning: A Survey of Case Studies}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3533378}, author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.}, abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.} }
@article{WOS:001028153700001, title = {Interpretable machine learning for building energy management: A state-of-the-art review}, journal = {ADVANCES IN APPLIED ENERGY}, volume = {9}, year = {2023}, issn = {2666-7924}, doi = {10.1016/j.adapen.2023.100123}, author = {Chen, Zhe and Xiao, Fu and Guo, Fangzhou and Yan, Jinyue}, abstract = {Machine learning has been widely adopted for improving building energy efficiency and flexibility in the past decade owing to the ever-increasing availability of massive building operational data. However, it is challenging for end-users to understand and trust machine learning models because of their black-box nature. To this end, the interpretability of machine learning models has attracted increasing attention in recent studies because it helps users understand the decisions made by these models. This article reviews previous studies that adopted interpretable machine learning techniques for building energy management to analyze how model interpretability is improved. First, the studies are categorized according to the application stages of interpretable machine learning techniques: ante-hoc and post-hoc approaches. Then, the studies are analyzed in detail according to specific techniques with critical comparisons. Through the review, we find that the broad application of interpretable machine learning in building energy management faces the following significant challenges: (1) different terminologies are used to describe model interpretability which could cause confusion, (2) performance of interpretable ML in different tasks is difficult to compare, and (3) current prevalent techniques such as SHAP and LIME can only provide limited interpretability. Finally, we discuss the future R \\& D needs for improving the interpretability of black-box models that could be significant to accelerate the application of machine learning for building energy management.} }
@article{WOS:001194621500001, title = {Machine learning with a reject option: a survey}, journal = {MACHINE LEARNING}, volume = {113}, pages = {3073-3110}, year = {2024}, issn = {0885-6125}, doi = {10.1007/s10994-024-06534-x}, author = {Hendrickx, Kilian and Perini, Lorenzo and van der Plas, Dries and Meert, Wannes and Davis, Jesse}, abstract = {Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.} }
@article{WOS:000298103200003, title = {Scikit-learn: Machine Learning in Python}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {12}, pages = {2825-2830}, year = {2011}, issn = {1532-4435}, author = {Pedregosa, Fabian and Varoquaux, Gaeel and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard}, abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.} }
@article{WOS:001370097700001, title = {The rise of scientific machine learning: a perspective on combining mechanistic modelling with machine learning for systems biology}, journal = {FRONTIERS IN SYSTEMS BIOLOGY}, volume = {4}, year = {2024}, doi = {10.3389/fsysb.2024.1407994}, author = {Noordijk, Ben and Gomez, Monica L. Garcia and ten Tusscher, Kirsten H. W. J. and de Ridder, Dick and van Dijk, Aalt D. J. and Smith, Robert W.}, abstract = {Both machine learning and mechanistic modelling approaches have been used independently with great success in systems biology. Machine learning excels in deriving statistical relationships and quantitative prediction from data, while mechanistic modelling is a powerful approach to capture knowledge and infer causal mechanisms underpinning biological phenomena. Importantly, the strengths of one are the weaknesses of the other, which suggests that substantial gains can be made by combining machine learning with mechanistic modelling, a field referred to as Scientific Machine Learning (SciML). In this review we discuss recent advances in combining these two approaches for systems biology, and point out future avenues for its application in the biological sciences.} }
@article{WOS:000987798300001, title = {A Comprehensive Review on Machine Learning in Healthcare Industry: Classification, Restrictions, Opportunities and Challenges}, journal = {SENSORS}, volume = {23}, year = {2023}, doi = {10.3390/s23094178}, author = {An, Qi and Rahman, Saifur and Zhou, Jingwen and Kang, James Jin}, abstract = {Recently, various sophisticated methods, including machine learning and artificial intelligence, have been employed to examine health-related data. Medical professionals are acquiring enhanced diagnostic and treatment abilities by utilizing machine learning applications in the healthcare domain. Medical data have been used by many researchers to detect diseases and identify patterns. In the current literature, there are very few studies that address machine learning algorithms to improve healthcare data accuracy and efficiency. We examined the effectiveness of machine learning algorithms in improving time series healthcare metrics for heart rate data transmission (accuracy and efficiency). In this paper, we reviewed several machine learning algorithms in healthcare applications. After a comprehensive overview and investigation of supervised and unsupervised machine learning algorithms, we also demonstrated time series tasks based on past values (along with reviewing their feasibility for both small and large datasets).} }
@article{WOS:001086797900003, title = {Machine Learning-Assisted Low-Dimensional Electrocatalysts Design for Hydrogen Evolution Reaction}, journal = {NANO-MICRO LETTERS}, volume = {15}, year = {2023}, issn = {2311-6706}, doi = {10.1007/s40820-023-01192-5}, author = {Li, Jin and Wu, Naiteng and Zhang, Jian and Wu, Hong-Hui and Pan, Kunming and Wang, Yingxue and Liu, Guilong and Liu, Xianming and Yao, Zhenpeng and Zhang, Qiaobao}, abstract = {Efficient electrocatalysts are crucial for hydrogen generation from electrolyzing water. Nevertheless, the conventional ``trial and error'' method for producing advanced electrocatalysts is not only cost-ineffective but also time-consuming and labor-intensive. Fortunately, the advancement of machine learning brings new opportunities for electrocatalysts discovery and design. By analyzing experimental and theoretical data, machine learning can effectively predict their hydrogen evolution reaction (HER) performance. This review summarizes recent developments in machine learning for low-dimensional electrocatalysts, including zero-dimension nanoparticles and nanoclusters, one-dimensional nanotubes and nanowires, two-dimensional nanosheets, as well as other electrocatalysts. In particular, the effects of descriptors and algorithms on screening low-dimensional electrocatalysts and investigating their HER performance are highlighted. Finally, the future directions and perspectives for machine learning in electrocatalysis are discussed, emphasizing the potential for machine learning to accelerate electrocatalyst discovery, optimize their performance, and provide new insights into electrocatalytic mechanisms. Overall, this work offers an in-depth understanding of the current state of machine learning in electrocatalysis and its potential for future research. [GRAPHICS]} }
@article{WOS:001102175100001, title = {Machine learning for microbiologists}, journal = {NATURE REVIEWS MICROBIOLOGY}, volume = {22}, pages = {191-205}, year = {2024}, issn = {1740-1526}, doi = {10.1038/s41579-023-00984-1}, author = {Asnicar, Francesco and Thomas, Andrew Maltez and Passerini, Andrea and Waldron, Levi and Segata, Nicola}, abstract = {Machine learning is increasingly important in microbiology where it is used for tasks such as predicting antibiotic resistance and associating human microbiome features with complex host diseases. The applications in microbiology are quickly expanding and the machine learning tools frequently used in basic and clinical research range from classification and regression to clustering and dimensionality reduction. In this Review, we examine the main machine learning concepts, tasks and applications that are relevant for experimental and clinical microbiologists. We provide the minimal toolbox for a microbiologist to be able to understand, interpret and use machine learning in their experimental and translational activities. In this Review, Segata, Waldron and colleagues discuss important key concepts of machine learning that are relevant to microbiologists and provide them with a set of tools essential to apply machine learning in microbiology research.} }
@article{WOS:000906478900001, title = {A comprehensive review of machine learning-based methods in landslide susceptibility mapping}, journal = {GEOLOGICAL JOURNAL}, volume = {58}, pages = {2283-2301}, year = {2023}, issn = {0072-1050}, doi = {10.1002/gj.4666}, author = {Liu, Songlin and Wang, Luqi and Zhang, Wengang and He, Yuwei and Pijush, Samui}, abstract = {Landslide susceptibility mapping (LSM) has been widely used as an important reference for development and construction planning to mitigate the potential social-eco impact caused by landslides. Originally, most of those maps were generated by the judgements of experts, which is time-consuming and laborious, and whose accuracy is difficult to be quantified because of the subjective effects. With the development of machine learning algorithms and the methods of data collection, big data and artificial intelligence have now been popularized in this field, significantly improving mapping accuracy and efficiency. Various machine learning-based methods, mainly including conventional machine learning, deep learning, and transfer learning have been applied and compared in LSM in different areas by previous researchers. Nevertheless, none of them can be effective in all cases. Although deep learning-based methods were proven more accurate than conventional machine learning-based methods in most data-rich situations, the latter is sometimes more popularly used in LSM, as there is not that much data in this field to train a deep learning network perfectly. In a more rigorous situation when there is very limited data, transfer learning-based approaches are applied by several researchers, which have contributed to improve the workability and the accuracy of LSM in data-limited areas. Such technical explosion has promoted the application of landslide susceptibility maps, thus contributing to mitigating the social-eco impact associated with landslides. This paper comprehensively reviews the whole process of generating landslide susceptibility maps based on machine learning methods, introduces and compares the commonly used machine learning methods, and discusses the topics for future research.} }
@article{WOS:001438192600001, title = {Machine learning and remote sensing integration for leveraging urban sustainability: A review and framework}, journal = {SUSTAINABLE CITIES AND SOCIETY}, volume = {96}, year = {2023}, issn = {2210-6707}, doi = {10.1016/j.scs.2023.104653}, author = {Li, Fei and Yigitcanlar, Tan and Nepal, Madhav and Nguyen, Kien and Dur, Fatih}, abstract = {Climate change and rapid urbanisation exacerbated multiple urban issues threatening urban sustainability. Numerous studies integrated machine learning and remote sensing to monitor urban issues and develop mitigation strategies for sustainability. However, few studies comparatively analysed joint applications of machine learning and remote sensing for urban issues and sustainability. This paper presents a systematic review and formulates a framework integrating machine learning and remote sensing in urban studies. The literature analysis reveals: Most studies occurred in Asia, Europe, and North America, driven by technical and ethical factors, highlighting responsible approaches for data-scarce regions; Reviewed studies prioritised physical spatial aspects over socioeconomic factors, requiring multi-source data for comprehensive analysis; Conventional satellite, aerial images, and Lidar data are prevalent due to affordability, quality, and accessibility; Although supervised machine learning dominates, unsupervised methods and algorithm selection paradigms require exploration; Integration offers accurate results and thorough analysis in image processing and analytics, while image acquisition and decision-making necessitate human supervision. This paper provides a comprehensive review and an integrative framework for machine learning and remote sensing, enriching insights into their potential for urban studies and spatial analytics. The study informs urban planning and policymaking by promoting efficient management via enhanced machine learning and remote sensing integration, and bolstering data-driven decision-making.} }
@article{WOS:001165437500001, title = {Scope of machine learning in materials research-A review}, journal = {APPLIED SURFACE SCIENCE ADVANCES}, volume = {18}, year = {2023}, issn = {2666-5239}, doi = {10.1016/j.apsadv.2023.100523}, author = {Mobarak, Md Hosne and Mimona, Mariam Akter and Islam, Md. Aminul and Hossain, Nayem and Zohura, Fatema Tuz and Imtiaz, Ibnul and Rimon, Md Israfil Hossain}, abstract = {This comprehensive review investigates the multifaceted applications of machine learning in materials research across six key dimensions, redefining the field's boundaries. It explains various knowledge acquisition mechanisms starting with supervised, unsupervised, reinforcement, and deep learning techniques. These techniques are transformative tools for transforming unactionable data into insightful actions. Moving on to the materials synthesis, the review emphasizes the profound influence of machine learning, as demonstrated by predictive models that speed up material selection, structure-property relationships that reveal crucial connections, and data-driven discovery that fosters innovation. Machine learning reshapes our comprehension and manipulation of materials by accelerating discovery and enabling tailored design through property prediction models and structure-property relationships. Machine learning extends its influence to image processing, improving object detection, classification, and segmentation precision and enabling methods like image generation, revolutionizing the potential of image processing in materials research. The most recent developments show how machine learning can have a transformative impact at the atomic level by enabling precise property prediction and intricate data extraction, representing significant advancements in material understanding and innovation. The review highlights how machine learning has the potential to revolutionize materials research by accelerating discovery, improving performance, and stimulating innovation. It does so while acknowledging obstacles like poor data quality and complicated algorithms. Machine learning offers a wide range of exciting prospects for scientific investigation and technological advancement, positioning it as a powerful force for influencing the future of materials research.} }
@article{WOS:000638010100001, title = {Machine learning and deep learning}, journal = {ELECTRONIC MARKETS}, volume = {31}, pages = {685-695}, year = {2021}, issn = {1019-6781}, doi = {10.1007/s12525-021-00475-2}, author = {Janiesch, Christian and Zschech, Patrick and Heinrich, Kai}, abstract = {Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.} }
@article{WOS:000977155100001, title = {Advancements and Challenges in Machine Learning: A Comprehensive Review of Models, Libraries, Applications, and Algorithms}, journal = {ELECTRONICS}, volume = {12}, year = {2023}, issn = {2079-9292}, doi = {10.3390/electronics12081789}, author = {Tufail, Shahid and Riggs, Hugo and Tariq, Mohd and Sarwat, Arif I.}, abstract = {In the current world of the Internet of Things, cyberspace, mobile devices, businesses, social media platforms, healthcare systems, etc., there is a lot of data online today. Machine learning (ML) is something we need to understand to do smart analyses of these data and make smart, automated applications that use them. There are many different kinds of machine learning algorithms. The most well-known ones are supervised, unsupervised, semi-supervised, and reinforcement learning. This article goes over all the different kinds of machine-learning problems and the machine-learning algorithms that are used to solve them. The main thing this study adds is a better understanding of the theory behind many machine learning methods and how they can be used in the real world, such as in energy, healthcare, finance, autonomous driving, e-commerce, and many more fields. This article is meant to be a go-to resource for academic researchers, data scientists, and machine learning engineers when it comes to making decisions about a wide range of data and methods to start extracting information from the data and figuring out what kind of machine learning algorithm will work best for their problem and what results they can expect. Additionally, this article presents the major challenges in building machine learning models and explores the research gaps in this area. In this article, we also provided a brief overview of data protection laws and their provisions in different countries.} }
@article{WOS:000939178900001, title = {Quantum Machine Learning: A Review and Case Studies}, journal = {ENTROPY}, volume = {25}, year = {2023}, doi = {10.3390/e25020287}, author = {Zeguendry, Amine and Jarir, Zahi and Quafafou, Mohamed}, abstract = {Despite its undeniable success, classical machine learning remains a resource-intensive process. Practical computational efforts for training state-of-the-art models can now only be handled by high speed computer hardware. As this trend is expected to continue, it should come as no surprise that an increasing number of machine learning researchers are investigating the possible advantages of quantum computing. The scientific literature on Quantum Machine Learning is now enormous, and a review of its current state that can be comprehended without a physics background is necessary. The objective of this study is to present a review of Quantum Machine Learning from the perspective of conventional techniques. Departing from giving a research path from fundamental quantum theory through Quantum Machine Learning algorithms from a computer scientist's perspective, we discuss a set of basic algorithms for Quantum Machine Learning, which are the fundamental components for Quantum Machine Learning algorithms. We implement the Quanvolutional Neural Networks (QNNs) on a quantum computer to recognize handwritten digits, and compare its performance to that of its classical counterpart, the Convolutional Neural Networks (CNNs). Additionally, we implement the QSVM on the breast cancer dataset and compare it to the classical SVM. Finally, we implement the Variational Quantum Classifier (VQC) and many classical classifiers on the Iris dataset to compare their accuracies.} }
@article{WOS:000926890700009, title = {Open-world Machine Learning: Applications, Challenges, and Opportunities}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3561381}, author = {Parmar, Jitendra and Chouhan, Satyendra and Raychoudhury, Vaskar and Rathore, Santosh}, abstract = {Traditional machine learning, mainly supervised learning, follows the assumptions of closed-world learning, i.e., for each testing class, a training class is available. However, such machine learning models fail to identify the classes, which were not available during training time. These classes can be referred to as unseen classes. Open-world Machine Learning (OWML) is a novel technique, which deals with unseen classes. Although OWML is around for a few years and many significant research works have been carried out in this domain, there is no comprehensive survey of the characteristics, applications, and impact of OWML on the major research areas. In this article, we aimed to capture the different dimensions of OWML with respect to other traditional machine learning models. We have thoroughly analyzed the existing literature and provided a novel taxonomy of OWML considering its two major application domains: Computer Vision and Natural Language Processing. We listed the available software packages and open datasets in OWML for future researchers. Finally, the article concludes with a set of research gaps, open challenges, and future directions.} }
@article{WOS:001131493500001, title = {Machine Learning Applications in Agriculture: Current Trends, Challenges, and Future Perspectives}, journal = {AGRONOMY-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/agronomy13122976}, author = {Araujo, Sara Oleiro and Peres, Ricardo Silva and Ramalho, Jose Cochicho and Lidon, Fernando and Barata, Jose}, abstract = {Progress in agricultural productivity and sustainability hinges on strategic investments in technological research. Evolving technologies such as the Internet of Things, sensors, robotics, Artificial Intelligence, Machine Learning, Big Data, and Cloud Computing are propelling the agricultural sector towards the transformative Agriculture 4.0 paradigm. The present systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to explore the usage of Machine Learning in agriculture. The study investigates the foremost applications of Machine Learning, including crop, water, soil, and animal management, revealing its important role in revolutionising traditional agricultural practices. Furthermore, it assesses the substantial impacts and outcomes of Machine Learning adoption and highlights some challenges associated with its integration in agricultural systems. This review not only provides valuable insights into the current landscape of Machine Learning applications in agriculture, but it also outlines promising directions for future research and innovation in this rapidly evolving field.} }
@article{WOS:000991648200011, title = {Integrated Sensing and Communication for 6G: Ten Key Machine Learning Roles}, journal = {IEEE COMMUNICATIONS MAGAZINE}, volume = {61}, pages = {113-119}, year = {2023}, issn = {0163-6804}, doi = {10.1109/MCOM.006.2200480}, author = {Demirhan, Umut and Alkhateeb, Ahmed}, abstract = {Integrating sensing and communication is a defining theme for future wireless systems. This is motivated by the promising performance gains, especially as they assist each other, and by the better utilization of the wireless and hardware resources. Realizing these gains in practice, however, is subject to several challenges where leveraging machine learning can provide a potential solution. This article focuses on ten key machine learning roles for joint sensing and communication, sensing-aided communication, and communication-aided sensing systems, explains why and how machine learning can be utilized, and highlights important directions for future research. The article also presents real-world results for some of these machine learning roles based on the large-scale real-world dataset DeepSense 6G, which could be adopted in investigating a wide range of integrated sensing and communication problems.} }
@article{WOS:000972340500002, title = {Advances in machine learning- and artificial intelligence-assisted material design of steels}, journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS}, volume = {30}, pages = {1003-1024}, year = {2023}, issn = {1674-4799}, doi = {10.1007/s12613-022-2595-0}, author = {Pan, Guangfei and Wang, Feiyang and Shang, Chunlei and Wu, Honghui and Wu, Guilin and Gao, Junheng and Wang, Shuize and Gao, Zhijun and Zhou, Xiaoye and Mao, Xinping}, abstract = {With the rapid development of artificial intelligence technology and increasing material data, machine learning- and artificial intelligence-assisted design of high-performance steel materials is becoming a mainstream paradigm in materials science. Machine learning methods, based on an interdisciplinary discipline between computer science, statistics and material science, are good at discovering correlations between numerous data points. Compared with the traditional physical modeling method in material science, the main advantage of machine learning is that it overcomes the complex physical mechanisms of the material itself and provides a new perspective for the research and development of novel materials. This review starts with data preprocessing and the introduction of different machine learning models, including algorithm selection and model evaluation. Then, some successful cases of applying machine learning methods in the field of steel research are reviewed based on the main theme of optimizing composition, structure, processing, and performance. The application of machine learning methods to the performance-oriented inverse design of material composition and detection of steel defects is also reviewed. Finally, the applicability and limitations of machine learning in the material field are summarized, and future directions and prospects are discussed.} }
@article{WOS:000935967900001, title = {Quantum machine learning: from physics to software engineering}, journal = {ADVANCES IN PHYSICS-X}, volume = {8}, year = {2023}, issn = {2374-6149}, doi = {10.1080/23746149.2023.2165452}, author = {Melnikov, Alexey and Kordzanganeh, Mohammad and Alodjants, Alexander and Lee, Ray-Kuang}, abstract = {Quantum machine learning is a rapidly growing field at the intersection of quantum technology and artificial intelligence. This review provides a two-fold overview of several key approaches that can offer advancements in both the development of quantum technologies and the power of artificial intelligence. Among these approaches are quantum-enhanced algorithms, which apply quantum software engineering to classical information processing to improve keystone machine learning solutions. In this context, we explore the capability of hybrid quantum-classical neural networks to improve model generalization and increase accuracy while reducing computational resources. We also illustrate how machine learning can be used both to mitigate the effects of errors on presently available noisy intermediate-scale quantum devices, and to understand quantum advantage via an automatic study of quantum walk processes on graphs. In addition, we review how quantum hardware can be enhanced by applying machine learning to fundamental and applied physics problems as well as quantum tomography and photonics. We aim to demonstrate how concepts in physics can be translated into practical engineering of machine learning solutions using quantum software.} }
@article{WOS:001193522300001, title = {Design and analysis of quantum machine learning: a survey}, journal = {CONNECTION SCIENCE}, volume = {36}, year = {2024}, issn = {0954-0091}, doi = {10.1080/09540091.2024.2312121}, author = {Chen, Linshu and Li, Tao and Chen, Yuxiang and Chen, Xiaoyan and Wozniak, Marcin and Xiong, Neal and Liang, Wei}, abstract = {Machine learning has demonstrated tremendous potential in solving real-world problems. However, with the exponential growth of data amount and the increase of model complexity, the processing efficiency of machine learning declines rapidly. Meanwhile, the emergence of quantum computing has given rise to quantum machine learning, which relies on superposition and entanglement, exhibiting exponential optimisation compared to traditional machine learning. Therefore, in the paper, we survey the basic concepts, algorithms, applications and challenges of quantum machine learning. Concretely, we first review the basic concepts of quantum computing including qubit, quantum gates, quantum entanglement, etc.. Secondly, we in-depth discuss 5 quantum machine learning algorithms of quantum support vector machine, quantum neural network, quantum k-nearest neighbour, quantum principal component analysis and quantum k-Means algorithm. Thirdly, we conduct discussions on the applications of quantum machine learning in image recognition, drug efficacy prediction and cybersecurity. Finally, we summarise the challenges of quantum machine learning consisting of algorithm design, hardware limitations, data encoding, quantum landscapes, noise and decoherence.} }
@article{WOS:001359559300001, title = {Quantum machine learning: Classifications, challenges, and solutions}, journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION}, volume = {42}, year = {2024}, issn = {2467-964X}, doi = {10.1016/j.jii.2024.100736}, author = {Lu, Wei and Lu, Yang and Li, Jin and Sigov, Alexander and Ratkin, Leonid and Ivanov, Leonid A.}, abstract = {Recently, research at the intersection of quantum mechanics and machine learning has gained attention. This interdisciplinary field aims to tackle the computational efficiency of machine learning by leveraging quantum computing and to derive novel machine learning algorithms inspired by quantum principles. Despite substantial progress in quantum science research, several challenges persist, including the preservation of quantum coherence, mitigation of environmental constraints, advancing quantum computer development, and formulating comprehensive quantum machine learning algorithms. To date, a comprehensive theoretical framework for quantum machine learning is lacking, with much of the research still in the exploratory and experimental stages. This study conducts a thorough survey on quantum machine learning, with the aim of classifying quantum machine learning algorithms while addressing the existing challenges and potential solutions in this emerging field.} }
@article{WOS:001061188500001, title = {Application of Machine Learning in Material Synthesis and Property Prediction}, journal = {MATERIALS}, volume = {16}, year = {2023}, doi = {10.3390/ma16175977}, author = {Huang, Guannan and Guo, Yani and Chen, Ye and Nie, Zhengwei}, abstract = {Material innovation plays a very important role in technological progress and industrial development. Traditional experimental exploration and numerical simulation often require considerable time and resources. A new approach is urgently needed to accelerate the discovery and exploration of new materials. Machine learning can greatly reduce computational costs, shorten the development cycle, and improve computational accuracy. It has become one of the most promising research approaches in the process of novel material screening and material property prediction. In recent years, machine learning has been widely used in many fields of research, such as superconductivity, thermoelectrics, photovoltaics, catalysis, and high-entropy alloys. In this review, the basic principles of machine learning are briefly outlined. Several commonly used algorithms in machine learning models and their primary applications are then introduced. The research progress of machine learning in predicting material properties and guiding material synthesis is discussed. Finally, a future outlook on machine learning in the materials science field is presented.} }
@article{WOS:000978351900001, title = {Synthesis optimization and adsorption modeling of biochar for pollutant removal via machine learning}, journal = {BIOCHAR}, volume = {5}, year = {2023}, issn = {2524-7972}, doi = {10.1007/s42773-023-00225-x}, author = {Zhang, Wentao and Chen, Ronghua and Li, Jie and Huang, Tianyin and Wu, Bingdang and Ma, Jun and Wen, Qingqi and Tan, Jie and Huang, Wenguang}, abstract = {Due to large specific surface area, abundant functional groups and low cost, biochar is widely used for pollutant removal. The adsorption performance of biochar is related to biochar synthesis and adsorption parameters. But the influence factor is numerous, the traditional experimental enumeration is powerless. In recent years, machine learning has been gradually employed for biochar, but there is no comprehensive review on the whole process regulation of biochar adsorbents, covering synthesis optimization and adsorption modeling. This review article systematically summarized the application of machine learning in biochar adsorbents from the perspective of all-round regulation for the first time, including the synthesis optimization and adsorption modeling of biochar adsorbents. Firstly, the overview of machine learning was introduced. Then, the latest advances of machine learning in biochar synthesis for pollutant removal were summarized, including prediction of biochar yield and physicochemical properties, optimal synthetic conditions and economic cost. And the application of machine learning in pollutant adsorption by biochar was reviewed, covering prediction of adsorption efficiency, optimization of experimental conditions and revelation of adsorption mechanism. General guidelines for the application of machine learning in whole-process optimization of biochar from synthesis to adsorption were presented. Finally, the existing problems and future perspectives of machine learning for biochar adsorbents were put forward. We hope that this review can promote the integration of machine learning and biochar, and thus light up the industrialization of biochar.} }
@article{WOS:001073553500001, title = {Forecasting Stock Market Prices Using Machine Learning and Deep Learning Models: A Systematic Review, Performance Analysis and Discussion of Implications}, journal = {INTERNATIONAL JOURNAL OF FINANCIAL STUDIES}, volume = {11}, year = {2023}, issn = {2227-7072}, doi = {10.3390/ijfs11030094}, author = {Sonkavde, Gaurang and Dharrao, Deepak Sudhakar and Bongale, Anupkumar M. and Deokate, Sarika T. and Doreswamy, Deepak and Bhat, Subraya Krishna}, abstract = {The financial sector has greatly impacted the monetary well-being of consumers, traders, and financial institutions. In the current era, artificial intelligence is redefining the limits of the financial markets based on state-of-the-art machine learning and deep learning algorithms. There is extensive use of these techniques in financial instrument price prediction, market trend analysis, establishing investment opportunities, portfolio optimization, etc. Investors and traders are using machine learning and deep learning models for forecasting financial instrument movements. With the widespread adoption of AI in finance, it is imperative to summarize the recent machine learning and deep learning models, which motivated us to present this comprehensive review of the practical applications of machine learning in the financial industry. This article examines algorithms such as supervised and unsupervised machine learning algorithms, ensemble algorithms, time series analysis algorithms, and deep learning algorithms for stock price prediction and solving classification problems. The contributions of this review article are as follows: (a) it provides a description of machine learning and deep learning models used in the financial sector; (b) it provides a generic framework for stock price prediction and classification; and (c) it implements an ensemble model-''Random Forest + XG-Boost + LSTM''-for forecasting TAINIWALCHM and AGROPHOS stock prices and performs a comparative analysis with popular machine learning and deep learning models.} }
@article{WOS:001041998200001, title = {Machine learning in marine ecology: an overview of techniques and applications}, journal = {ICES JOURNAL OF MARINE SCIENCE}, volume = {80}, pages = {1829-1853}, year = {2023}, issn = {1054-3139}, doi = {10.1093/icesjms/fsad100}, author = {Rubbens, Peter and Brodie, Stephanie and Cordier, Tristan and Destro Barcellos, Diogo and Devos, Paul and Fernandes-Salvador, Jose A. and Fincham, I, Jennifer and Gomes, Alessandra and Handegard, Nils Olav and Howell, Kerry and Jamet, Cedric and Kartveit, Kyrre Heldal and Moustahfid, Hassan and Parcerisas, Clea and Politikos, Dimitris and Sauzede, Raphaelle and Sokolova, Maria and Uusitalo, Laura and Van den Bulcke, Laure and van Helmond, Aloysius T. M. and Watson, Jordan T. and Welch, Heather and Beltran-Perez, Oscar and Chaffron, Samuel and Greenberg, David S. and Kuehn, Bernhard and Kiko, Rainer and Lo, Madiop and Lopes, Rubens M. and Moeller, Klas Ove and Michaels, William and Pala, Ahmet and Romagnan, Jean-Baptiste and Schuchert, Pia and Seydi, Vahid and Villasante, Sebastian and Malde, Ketil and Irisson, Jean-Olivier}, abstract = {Machine learning covers a large set of algorithms that can be trained to identify patterns in data. Thanks to the increase in the amount of data and computing power available, it has become pervasive across scientific disciplines. We first highlight why machine learning is needed in marine ecology. Then we provide a quick primer on machine learning techniques and vocabulary. We built a database of \\& SIM;1000 publications that implement such techniques to analyse marine ecology data. For various data types (images, optical spectra, acoustics, omics, geolocations, biogeochemical profiles, and satellite imagery), we present a historical perspective on applications that proved influential, can serve as templates for new work, or represent the diversity of approaches. Then, we illustrate how machine learning can be used to better understand ecological systems, by combining various sources of marine data. Through this coverage of the literature, we demonstrate an increase in the proportion of marine ecology studies that use machine learning, the pervasiveness of images as a data source, the dominance of machine learning for classification-type problems, and a shift towards deep learning for all data types. This overview is meant to guide researchers who wish to apply machine learning methods to their marine datasets.} }
@article{WOS:000935501400001, title = {A survey of machine learning in additive manufacturing technologies}, journal = {INTERNATIONAL JOURNAL OF COMPUTER INTEGRATED MANUFACTURING}, volume = {36}, pages = {1258-1280}, year = {2023}, issn = {0951-192X}, doi = {10.1080/0951192X.2023.2177740}, author = {Jiang, Jingchao}, abstract = {Thirty years into its development, additive manufacturing has become a mainstream manufacturing process. Additive manufacturing fabricates products by adding materials layer-by-layer directly based on a 3D model. It is able to manufacture complex parts and allows more freedom of design optimization compared with traditional manufacturing techniques. Machine learning is now a hot technology that has been used in medical diagnosis, image processing, prediction, classification, learning association, regression, etc. Currently, focuses are increasingly given to using machine learning in the manufacturing industry, including additive manufacturing. Due to the rapid development of machine learning in additive manufacturing, a special issue `Machine Learning in Additive Manufacturing' in International Journal of Computer Integrated Manufacturing is organized. This paper gives a comprehensive understanding of the current status of machine learning enhanced additive manufacturing technologies for this special issue. Discussions and future perspectives are also provided.} }
@article{WOS:001020157200009, title = {Machine learning in agriculture: a review of crop management applications}, journal = {MULTIMEDIA TOOLS AND APPLICATIONS}, volume = {83}, pages = {12875-12915}, year = {2024}, issn = {1380-7501}, doi = {10.1007/s11042-023-16105-2}, author = {Attri, Ishana and Awasthi, Lalit Kumar and Sharma, Teek Parval}, abstract = {Machine learning has created new opportunities for data-intensive study in interdisciplinary domains as a result of the advancement of big data technologies and high-performance computers. Search engines, email spam filters, websites that offer personalized recommendations, banking software that alerts users to suspicious activity, and a plethora of smartphone apps that perform tasks like voice recognition, image recognition, and natural language processing are just a few examples of the online and offline services that have incorporated machine learning in recent years. One of the most crucial areas where machine learning applications still has to be investigated is agriculture, which directly affects people's well-being. In this article, a literature review on machine learning algorithms used in agriculture is presented. The proposed paper deal with various crop management applications which are categorised into five parts i.e., Weed and pest detection, Plant disease detection, Stress detection in plants, Smart farms or automation in farms and the last one is Crop yield estimation and prediction. The articles' filtering and categorization show how machine learning may improve agriculture. This article examines machine learning breakthroughs in agriculture. This paper's findings show that by using novel machine learning approaches, models may achieve improved accuracy and shorter inference time for real-world applications.} }
@article{WOS:001008445700001, title = {Machine learning in biohydrogen production: a review}, journal = {BIOFUEL RESEARCH JOURNAL-BRJ}, volume = {10}, pages = {1844-1858}, year = {2023}, issn = {2292-8782}, doi = {10.18331/BRJ2023.10.2.4}, author = {Alagumalai, Avinash and Devarajan, Balaji and Song, Hua and Wongwises, Somchai and Ledesma-Amaro, Rodrigo and Mahian, Omid and Sheremet, Mikhail and Lichtfouse, Eric}, abstract = {Biohydrogen is emerging as a promising carbon-neutral and sustainable energy carrier with high energy yield to replace conventional fossil fuels. However, biohydrogen commercial uptake is mainly hindered by the supply side. As a result, various operating parameters must be optimized to realize biohydrogen commercial uptake on a large-scale. Recently, machine learning algorithms have demonstrated the ability to handle large amounts of data while requiring less in-depth knowledge of the system and being capable of adapting to evolving circumstances. This review critically reviews the role of machine learning in categorizing and predicting data related to biohydrogen production. The accuracy and potential of different machine learning algorithms are reported. Also, the practical implications of machine learning models to realize biohydrogen uptake by the transportation sector are discussed. The review indicates that machine learning algorithms can successfully model non-linear and complex interactions between operational and performance parameters in biohydrogen production. Additionally, machine learning algorithms can help researchers identify the most efficient methods for producing biohydrogen, leading to a more sustainable and cost-effective energy source. (c) 2023 BRTeam. All rights reserved.} }
@article{WOS:000695380500001, title = {A guide to machine learning for biologists}, journal = {NATURE REVIEWS MOLECULAR CELL BIOLOGY}, volume = {23}, pages = {40-55}, year = {2022}, issn = {1471-0072}, doi = {10.1038/s41580-021-00407-0}, author = {Greener, Joe G. and Kandathil, Shaun M. and Moffat, Lewis and Jones, David T.}, abstract = {Machine learning is becoming a widely used tool for the analysis of biological data. However, for experimentalists, proper use of machine learning methods can be challenging. This Review provides an overview of machine learning techniques and provides guidance on their applications in biology. The expanding scale and inherent complexity of biological data have encouraged a growing use of machine learning in biology to build informative and predictive models of the underlying biological processes. All machine learning techniques fit models to data; however, the specific methods are quite varied and can at first glance seem bewildering. In this Review, we aim to provide readers with a gentle introduction to a few key machine learning techniques, including the most recently developed and widely used techniques involving deep neural networks. We describe how different techniques may be suited to specific types of biological data, and also discuss some best practices and points to consider when one is embarking on experiments involving machine learning. Some emerging directions in machine learning methodology are also discussed.} }
@article{WOS:001117835000001, title = {Review of machine learning and deep learning models for toxicity prediction}, journal = {EXPERIMENTAL BIOLOGY AND MEDICINE}, volume = {248}, pages = {1952-1973}, year = {2023}, issn = {1535-3702}, doi = {10.1177/15353702231209421}, author = {Guo, Wenjing and Liu, Jie and Dong, Fan and Song, Meng and Li, Zoe and Khan, Md Kamrul Hasan and Patterson, Tucker A. and Hong, Huixiao}, abstract = {The ever-increasing number of chemicals has raised public concerns due to their adverse effects on human health and the environment. To protect public health and the environment, it is critical to assess the toxicity of these chemicals. Traditional in vitro and in vivo toxicity assays are complicated, costly, and time-consuming and may face ethical issues. These constraints raise the need for alternative methods for assessing the toxicity of chemicals. Recently, due to the advancement of machine learning algorithms and the increase in computational power, many toxicity prediction models have been developed using various machine learning and deep learning algorithms such as support vector machine, random forest, k-nearest neighbors, ensemble learning, and deep neural network. This review summarizes the machine learning- and deep learning-based toxicity prediction models developed in recent years. Support vector machine and random forest are the most popular machine learning algorithms, and hepatotoxicity, cardiotoxicity, and carcinogenicity are the frequently modeled toxicity endpoints in predictive toxicology. It is known that datasets impact model performance. The quality of datasets used in the development of toxicity prediction models using machine learning and deep learning is vital to the performance of the developed models. The different toxicity assignments for the same chemicals among different datasets of the same type of toxicity have been observed, indicating benchmarking datasets is needed for developing reliable toxicity prediction models using machine learning and deep learning algorithms. This review provides insights into current machine learning models in predictive toxicology, which are expected to promote the development and application of toxicity prediction models in the future.} }
@article{WOS:000922087600001, title = {Quantum machine learning in medical image analysis: A survey}, journal = {NEUROCOMPUTING}, volume = {525}, pages = {42-53}, year = {2023}, issn = {0925-2312}, doi = {10.1016/j.neucom.2023.01.049}, author = {Wei, Lin and Liu, Haowen and Xu, Jing and Shi, Lei and Shan, Zheng and Zhao, Bo and Gao, Yufei}, abstract = {With the outstanding superposition and entanglement properties of quantum computing, quantum machine learning has attracted widespread attention in many fields, such as medical image analysis, password cracking, and pattern recognition. Although classical machine learning is widely used and has shown great potential in medical image analysis, the bottlenecks of insufficient labeled data and low processing efficiency still exist. To overcome these challenges, massive studies combined quantum computing with machine learning to explore more advanced algorithms, which have achieved distin-guished improvements in parameter optimization, execution efficiency, and the reduction of error rates. Quantum machine learning provides new insights for the intersectional research of quantum technology and medical image analysis and contributes to the future development of medical image analysis. This review delivers an overview of the definition and taxonomy of quantum machine learning, as well as summarizes various quantum machine learning methods and their applications in medical image analy-sis over the past decade.(c) 2023 Elsevier B.V. All rights reserved.} }
@article{WOS:000977859400001, title = {Weakly supervised machine learning}, journal = {CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY}, volume = {8}, pages = {549-580}, year = {2023}, issn = {2468-6557}, doi = {10.1049/cit2.12216}, author = {Ren, Zeyu and Wang, Shuihua and Zhang, Yudong}, abstract = {Supervised learning aims to build a function or model that seeks as many mappings as possible between the training data and outputs, where each training data will predict as a label to match its corresponding ground-truth value. Although supervised learning has achieved great success in many tasks, sufficient data supervision for labels is not accessible in many domains because accurate data labelling is costly and laborious, particularly in medical image analysis. The cost of the dataset with ground-truth labels is much higher than in other domains. Therefore, it is noteworthy to focus on weakly supervised learning for medical image analysis, as it is more applicable for practical applications. In this review, the authors give an overview of the latest process of weakly supervised learning in medical image analysis, including incomplete, inexact, and inaccurate supervision, and introduce the related works on different applications for medical image analysis. Related concepts are illustrated to help readers get an overview ranging from supervised to unsupervised learning within the scope of machine learning. Furthermore, the challenges and future works of weakly supervised learning in medical image analysis are discussed.} }
@article{WOS:001162814300001, title = {Robust machine learning models: linear and nonlinear}, journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS}, volume = {20}, pages = {1043-1050}, year = {2025}, issn = {2364-415X}, doi = {10.1007/s41060-024-00512-1}, author = {Giudici, Paolo and Raffinetti, Emanuela and Riani, Marco}, abstract = {Artificial Intelligence relies on the application of machine learning models which, while reaching high predictive accuracy, lack explainability and robustness. This is a problem in regulated industries, as authorities aimed at monitoring the risks arising from the application of Artificial Intelligence methods may not validate them. No measurement methodologies are yet available to jointly assess accuracy, explainability and robustness of machine learning models. We propose a methodology which fills the gap, extending the Forward Search approach, employed in robust statistical learning, to machine learning models. Doing so, we will be able to evaluate, by means of interpretable statistical tests, whether a specific Artificial Intelligence application is accurate, explainable and robust, through a unified methodology. We apply our proposal to the context of Bitcoin price prediction, comparing a linear regression model against a nonlinear neural network model.} }
@article{WOS:001191466700001, title = {Compact Data Learning for Machine Learning Classifications}, journal = {AXIOMS}, volume = {13}, year = {2024}, doi = {10.3390/axioms13030137}, author = {Kim, Song-Kyoo (Amang)}, abstract = {This paper targets the area of optimizing machine learning (ML) training data by constructing compact data. The methods of optimizing ML training have improved and become a part of artificial intelligence (AI) system development. Compact data learning (CDL) is an alternative practical framework to optimize a classification system by reducing the size of the training dataset. CDL originated from compact data design, which provides the best assets without handling complex big data. CDL is a dedicated framework for improving the speed of the machine learning training phase without affecting the accuracy of the system. The performance of an ML-based arrhythmia detection system and its variants with CDL maintained the same statistical accuracy. ML training with CDL could be maximized by applying an 85\\% reduced input dataset, which indicated that a trained ML system could have the same statistical accuracy by only using 15\\% of the original training dataset.} }
@article{WOS:001074642900001, title = {Beyond generalization: a theory of robustness in machine learning}, journal = {SYNTHESE}, volume = {202}, year = {2023}, issn = {0039-7857}, doi = {10.1007/s11229-023-04334-9}, author = {Freiesleben, Timo and Grote, Thomas}, abstract = {The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept.} }
@article{WOS:001015789300001, title = {Machine Learning Algorithms and Fundamentals as Emerging Safety Tools in Preservation of Fruits and Vegetables: A Review}, journal = {PROCESSES}, volume = {11}, year = {2023}, doi = {10.3390/pr11061720}, author = {Pandey, Vinay Kumar and Srivastava, Shivangi and Dash, Kshirod Kumar and Singh, Rahul and Mukarram, Shaikh Ayaz and Kovacs, Bela and Harsanyi, Endre}, abstract = {Machine learning assists with food process optimization techniques by developing a model to predict the optimal solution for given input data. Machine learning includes unsupervised and supervised learning, data pre-processing, feature engineering, model selection, assessment, and optimization methods. Various problems with food processing optimization could be resolved using these techniques. Machine learning is increasingly being used in the food industry to improve production efficiency, reduce waste, and create personalized customer experiences. Machine learning may be used to improve ingredient utilization and save costs, automate operations such as packing and labeling, and even forecast consumer preferences to develop personalized products. Machine learning is also being used to identify food safety hazards before they reach the consumer, such as contaminants or spoiled food. The usage of machine learning in the food sector is predicted to rise in the near future as more businesses understand the potential of this technology to enhance customer experience and boost productivity. Machine learning may be utilized to enhance nano-technological operations and fruit and vegetable preservation. Machine learning algorithms may find trends regarding various factors that impact the quality of the product being preserved by examining data from prior tests. Furthermore, machine learning may be utilized to determine optimal parameter combinations that result in maximal produce preservation. The review discusses the relevance of machine learning in ready-to-eat foods and its use as a safety tool for preservation were highlighted. The application of machine learning in agriculture, food packaging, food processing, and food safety is reviewed. The working principle and methodology, as well as the principles of machine learning, were discussed.} }
@article{WOS:000875839900013, title = {Explainable Heart Disease Prediction Using Ensemble-Quantum Machine Learning Approach}, journal = {INTELLIGENT AUTOMATION AND SOFT COMPUTING}, volume = {36}, pages = {761-779}, year = {2023}, issn = {1079-8587}, doi = {10.32604/iasc.2023.032262}, author = {Abdulsalam, Ghada and Meshoul, Souham and Shaiba, Hadil}, abstract = {Nowadays, quantum machine learning is attracting great interest in a wide range of fields due to its potential superior performance and capabilities. The massive increase in computational capacity and speed of quantum computers can lead to a quantum leap in the healthcare field. Heart disease seriously threa-tens human health since it is the leading cause of death worldwide. Quantum machine learning methods can propose effective solutions to predict heart disease and aid in early diagnosis. In this study, an ensemble machine learning model based on quantum machine learning classifiers is proposed to predict the risk of heart disease. The proposed model is a bagging ensemble learning model where a quantum support vector classifier was used as a base classifier. Further-more, in order to make the model's outcomes more explainable, the importance of every single feature in the prediction is computed and visualized using SHapley Additive exPlanations (SHAP) framework. In the experimental study, other stand-alone quantum classifiers, namely, Quantum Support Vector Classifier (QSVC), Quantum Neural Network (QNN), and Variational Quantum Classifier (VQC) are applied and compared with classical machine learning classifiers such as Sup-port Vector Machine (SVM), and Artificial Neural Network (ANN). The experi-mental results on the Cleveland dataset reveal the superiority of QSVC compared to the others, which explains its use in the proposed bagging model. The Bagging-QSVC model outperforms all aforementioned classifiers with an accuracy of 90.16\\% while showing great competitiveness compared to some state-of-the-art models using the same dataset. The results of the study indicate that quantum machine learning classifiers perform better than classical machine learning classi-fiers in predicting heart disease. In addition, the study reveals that the bagging ensemble learning technique is effective in improving the prediction accuracy of quantum classifiers.} }
@article{WOS:000579808700028, title = {On hyperparameter optimization of machine learning algorithms: Theory and practice}, journal = {NEUROCOMPUTING}, volume = {415}, pages = {295-316}, year = {2020}, issn = {0925-2312}, doi = {10.1016/j.neucom.2020.07.061}, author = {Yang, Li and Shami, Abdallah}, abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyperparameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-theart optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000517855800037, title = {Applications of machine learning to machine fault diagnosis: A review and roadmap}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {138}, year = {2020}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2019.106587}, author = {Lei, Yaguo and Yang, Bin and Jiang, Xinwei and Jia, Feng and Li, Naipeng and Nandi, Asoke K.}, abstract = {Intelligent fault diagnosis (IFD) refers to applications of machine learning theories to machine fault diagnosis. This is a promising way to release the contribution from human labor and automatically recognize the health states of machines, thus it has attracted much attention in the last two or three decades. Although IFD has achieved a considerable number of successes, a review still leaves a blank space to systematically cover the development of IFD from the cradle to the bloom, and rarely provides potential guidelines for the future development. To bridge the gap, this article presents a review and roadmap to systematically cover the development of IFD following the progress of machine learning theories and offer a future perspective. In the past, traditional machine learning theories began to weak the contribution of human labor and brought the era of artificial intelligence to machine fault diagnosis. Over the recent years, the advent of deep learning theories has reformed IFD in further releasing the artificial assistance since the 2010s, which encourages to construct an end-to-end diagnosis procedure. It means to directly bridge the relationship between the increasingly-grown monitoring data and the health states of machines. In the future, transfer learning theories attempt to use the diagnosis knowledge from one or multiple diagnosis tasks to other related ones, which prospectively overcomes the obstacles in applications of IFD to engineering scenarios. Finally, the roadmap of IFD is pictured to show potential research trends when combined with the challenges in this field. (C) 2019 Elsevier Ltd. All rights reserved.} }
@article{WOS:000952240100002, title = {A review of deep learning and machine learning techniques for hydrological inflow forecasting}, journal = {ENVIRONMENT DEVELOPMENT AND SUSTAINABILITY}, volume = {25}, pages = {12189-12216}, year = {2023}, issn = {1387-585X}, doi = {10.1007/s10668-023-03131-1}, author = {Latif, Sarmad Dashti and Ahmed, Ali Najah}, abstract = {Conventional machine learning models have been widely used for reservoir inflow and rainfall prediction. Nowadays, researchers focus on a new computing architecture in the area of AI, namely, deep learning for hydrological forecasting parameters. This review paper tends to broadcast more of the intriguing interest in reservoir inflow prediction utilizing deep learning and machine learning algorithms. The AI models utilized for different hydrology sectors, as well as the most prevalent machine learning techniques, will be explored in this thorough study, which divides AI techniques into two primary categories: deep learning and machine learning. In this study, we look at the long short-term memory deep learning method as well as three traditional machine learning algorithms: support vector machine, random forest, and boosted regression tree. Under each part, a summary of the findings is provided. For convenience of reference, some of the benefits and drawbacks discovered through literature reviews have been listed. Finally, future recommendations and overall conclusions based on research findings are given. This review focuses on papers from high-impact factor periodicals published over a 4 years period beginning in 2018 onwards.} }
@article{WOS:000917312100001, title = {Navigating with chemometrics and machine learning in chemistry}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {9089-9114}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-023-10391-w}, author = {Joshi, Payal B.}, abstract = {Chemometrics and machine learning are artificial intelligence-based methods stirring a transformative change in chemistry. Organic synthesis, drug discovery and analytical techniques are incorporating machine learning techniques at an accelerated pace. However, machine-assisted chemistry faces challenges while solving critical problems in chemistry due to complex relationships in data sets. Even with increasing publishing volumes on machine learning, its application in areas of chemistry is not a straightforward endeavour. A particular concern in applying machine learning in chemistry is data availability and reproducibility. The present review article discusses the various chemometric methods, expert systems, and machine learning techniques developed for solving problems of organic synthesis and drug discovery with selected examples. Further, a concise discussion on chemometrics and ML deployed in analytical techniques such as, spectroscopy, microscopy and chromatography are presented. Finally, the review reflects the challenges, opportunities and future perspectives on machine learning and automation in chemistry. The review concludes by pondering on some tough questions on applying machine learning and their possibility of navigation in the different terrains of chemistry.} }
@article{WOS:000978338900001, title = {A Review of Macroscopic Carbon Emission Prediction Model Based on Machine Learning}, journal = {SUSTAINABILITY}, volume = {15}, year = {2023}, doi = {10.3390/su15086876}, author = {Zhao, Yuhong and Liu, Ruirui and Liu, Zhansheng and Liu, Liang and Wang, Jingjing and Liu, Wenxiang}, abstract = {Under the background of global warming and the energy crisis, the Chinese government has set the goal of carbon peaking and carbon neutralization. With the rapid development of machine learning, some advanced machine learning algorithms have also been applied to the control and prediction of carbon emissions due to their high efficiency and accuracy. In this paper, the current situation of machine learning applied to carbon emission prediction is studied in detail by means of paper retrieval. It was found that machine learning has become a hot topic in the field of carbon emission prediction models, and the main carbon emission prediction models are mainly based on back propagation neural networks, support vector machines, long short-term memory neural networks, random forests and extreme learning machines. By describing the characteristics of these five types of carbon emission prediction models and conducting a comparative analysis, we determined the applicable characteristics of each model, and based on this, future research ideas for carbon emission prediction models based on machine learning are proposed.} }
@article{WOS:001089938100001, title = {Machine Learning: Models, Challenges, and Research Directions}, journal = {FUTURE INTERNET}, volume = {15}, year = {2023}, issn = {1999-5903}, doi = {10.3390/fi15100332}, author = {Khoei, Tala Talaei and Kaabouch, Naima}, abstract = {Machine learning techniques have emerged as a transformative force, revolutionizing various application domains, particularly cybersecurity. The development of optimal machine learning applications requires the integration of multiple processes, such as data pre-processing, model selection, and parameter optimization. While existing surveys have shed light on these techniques, they have mainly focused on specific application domains. A notable gap that exists in current studies is the lack of a comprehensive overview of machine learning architecture and its essential phases in the cybersecurity field. To address this gap, this survey provides a holistic review of current studies in machine learning, covering techniques applicable to any domain. Models are classified into four categories: supervised, semi-supervised, unsupervised, and reinforcement learning. Each of these categories and their models are described. In addition, the survey discusses the current progress related to data pre-processing and hyperparameter tuning techniques. Moreover, this survey identifies and reviews the research gaps and key challenges that the cybersecurity field faces. By analyzing these gaps, we propose some promising research directions for the future. Ultimately, this survey aims to serve as a valuable resource for researchers interested in learning about machine learning, providing them with insights to foster innovation and progress across diverse application domains.} }
@article{WOS:001136730400009, title = {Interpretability of Machine Learning: Recent Advances and Future Prospects}, journal = {IEEE MULTIMEDIA}, volume = {30}, pages = {105-118}, year = {2023}, issn = {1070-986X}, doi = {10.1109/MMUL.2023.3272513}, author = {Gao, Lei and Guan, Ling}, abstract = {The proliferation of machine learning (ML) has drawn unprecedented interest in the study of various multimedia contents such as text, image, audio, and video, among others. Consequently, understanding and learning ML-based representations have taken center stage in knowledge discovery in intelligent multimedia research and applications. Nevertheless, the black-box nature of contemporary ML, especially in deep neural networks, has posed a primary challenge for ML-based representation learning. To address this black-box problem, studies on the interpretability of ML have attracted tremendous interest in recent years. This article presents a survey on recent advances in and future prospects for the interpretability of ML, with several application examples pertinent to multimedia computing, including text-image cross-modal representation learning, face recognition, and the recognition of objects. It is evidently shown that the study of the interpretability of ML promises an important research direction, one that is worth further investment in.} }
@article{WOS:001043801800001, title = {Machine learning in architecture}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {154}, year = {2023}, issn = {0926-5805}, doi = {10.1016/j.autcon.2023.105012}, author = {Topuz, Beyza and Alp, Nese Cakici}, abstract = {This paper explores the utilisation of machine learning in architecture, focusing on the addressed problems and commonly employed programming languages, software, platforms, libraries, and algorithms. Eight major academic search and publishing platforms were systematically reviewed, covering the period from 2007 to 2022, resulting in the selection of 60 relevant articles from a pool of 175. The articles were categorised based on their thematic focus, primarily centring around Computer-Aided Design (CAD), Computer-Aided Engineering (CAE), and Computer-Aided Manufacturing (CAM). By evaluating the current state of machine learning in architecture, this study provides valuable insights into its usage and identifies potential areas for future research. This paper contributes to a comprehensive understanding of the evolving landscape of machine learning in the field by investigating subfields within architecture and the specific tools used to tackle architectural challenges.} }
@article{WOS:001061075500001, title = {Ground truth tracings (GTT): On the epistemic limits of machine learning}, journal = {BIG DATA \\& SOCIETY}, volume = {10}, year = {2023}, issn = {2053-9517}, doi = {10.1177/20539517221146122}, author = {Kang, Edward B.}, abstract = {There is a gap in existing critical scholarship that engages with the ways in which current ``machine listening'' or voice analytics/biometric systems intersect with the technical specificities of machine learning. This article examines the sociotechnical assemblage of machine learning techniques, practices, and cultures that underlie these technologies. After engaging with various practitioners working in companies that develop machine listening systems, ranging from CEOs, machine learning engineers, data scientists, and business analysts, among others, I bring attention to the centrality of ``learnability'' as a malleable conceptual framework that bends according to various ``ground-truthing'' practices in formalizing certain listening-based prediction tasks for machine learning. In response, I introduce a process I call Ground Truth Tracings to examine the various ontological translations that occur in training a machine to ``learn to listen.'' Ultimately, by further examining this notion of learnability through the aperture of power, I take insights acquired through my fieldwork in the machine listening industry and propose a strategically reductive heuristic through which the epistemological and ethical soundness of machine learning, writ large, can be contemplated.} }
@article{WOS:001026855700002, title = {Practical advantage of quantum machine learning in ghost imaging}, journal = {COMMUNICATIONS PHYSICS}, volume = {6}, year = {2023}, issn = {2399-3650}, doi = {10.1038/s42005-023-01290-1}, author = {Xiao, Tailong and Zhai, Xinliang and Wu, Xiaoyan and Fan, Jianping and Zeng, Guihua}, abstract = {Quantum computation can provide practical applications where quantum algorithms can outperform classical ones. The authors focus on experimental ghost imaging using a mixture of classical and quantum machine learning (simulated on a classical computer) to process the experimental data and reconstruct/classify the image, finding that quantum machine learning techniques allow for a better reconstruction compared to standard methods. Demonstrating the practical advantage of quantum computation remains a long-standing challenge whereas quantum machine learning becomes a promising application that can be resorted to. In this work, we investigate the practical advantage of quantum machine learning in ghost imaging by overcoming the limitations of classical methods in blind object identification and imaging. We propose two hybrid quantum-classical machine learning algorithms and a physical-inspired patch strategy to allow distributed quantum learning with parallel variational circuits. In light of the algorithm, we conduct experiments for imaging-free object identification and blind ghost imaging under different physical sampling rates. We further quantitatively analyze the advantage through the lens of information geometry and generalization capability. The numerical results showcase that quantum machine learning can restore high-quality images but classical machine learning fails. The advantage of identification rate are up to 10\\% via fair comparison with the classical machine learning methods. Our work explores a physics-related application capable of practical quantum advantage, which highlights the prospect of quantum computation in the machine learning field.} }
@article{WOS:000626358900001, title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods}, journal = {MACHINE LEARNING}, volume = {110}, pages = {457-506}, year = {2021}, issn = {0885-6125}, doi = {10.1007/s10994-021-05946-3}, author = {Huellermeier, Eyke and Waegeman, Willem}, abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.} }
@article{WOS:000842577300001, title = {Human-in-the-loop machine learning: a state of the art}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {3005-3054}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-022-10246-w}, author = {Mosqueira-Rey, Eduardo and Hernandez-Pereira, Elena and Alonso-Rios, David and Bobes-Bascaran, Jose and Fernandez-Leal, Angel}, abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.} }
@article{WOS:000742179000001, title = {Machine Learning Testing: Survey, Landscapes and Horizons}, journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING}, volume = {48}, pages = {1-36}, year = {2022}, issn = {0098-5589}, doi = {10.1109/TSE.2019.2962027}, author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang}, abstract = {This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.} }
@article{WOS:000743249300001, title = {Interpretable machine learning: Fundamental principles and 10 grand challenges}, journal = {STATISTICS SURVEYS}, volume = {16}, pages = {1-85}, year = {2022}, issn = {1935-7516}, doi = {10.1214/21-SS133}, author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi}, abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the ``Rashomon set'' of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.} }
@article{WOS:001099973100001, title = {Interpretable machine learning assessment}, journal = {NEUROCOMPUTING}, volume = {561}, year = {2023}, issn = {0925-2312}, doi = {10.1016/j.neucom.2023.126891}, author = {Han, Henry and Wu, Yi and Wang, Jiacun and Han, Ashley}, abstract = {With the surge of machine learning in AI and data science, there remains an urgent need to not only compare the performance of different methods across diverse datasets but also to analyze machine learning behaviors with sensitivity using an explainable approach. In this study, we introduce a uniquely designed diagnostic index: dindex to tackle this challenge. This tool integrates classification effectiveness from multiple dimensions, delivering a transparent and comprehensive assessment that transcends the limitations of traditional evaluation methods in classification. We propose two innovative concepts: breakeven states and imbalanced points in this study. Integrated with the d-index, these concepts afford a more profound understanding of the learning behaviors across different machine learning models compared to the existing classification metrics. Significantly, the d-index excels as a powerful tool, identifying learning singularity problems (LSPs) that remain elusive to most current machine learning models and imbalanced learning techniques. Furthermore, leveraging the d-index, we unravel the mechanisms behind imbalanced point generation in binary and multiclass classification. We also put forth a novel technique: identifying a priori informative kernels to optimize support vector machine learning, ensuring outstanding d-index values with the fewest necessary support vectors. Moreover, we address a seldomdiscussed state of overfitting in deep learning, where overfitting occurs despite the training and testing loss curves exhibiting favorable trends throughout the epochs. To the best of our knowledge, this work represents a pioneering stride in the realm of explainable machine learning assessments and will inspire further studies in this area.} }
@article{WOS:000952857700002, title = {Machine Learning Security in Industry: A Quantitative Survey}, journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}, volume = {18}, pages = {1749-1762}, year = {2023}, issn = {1556-6013}, doi = {10.1109/TIFS.2023.3251842}, author = {Grosse, Kathrin and Bieringer, Lukas and Besold, Tarek R. and Biggio, Battista and Krombholz, Katharina}, abstract = {Despite the large body of academic work on machine learning security, little is known about the occurrence of attacks on machine learning systems in the wild. In this paper, we report on a quantitative study with 139 industrial practitioners. We analyze attack occurrence and concern and evaluate statistical hypotheses on factors influencing threat perception and exposure. Our results shed light on real-world attacks on deployed machine learning. On the organizational level, while we find no predictors for threat exposure in our sample, the amount of implement defenses depends on exposure to threats or expected likelihood to become a target. We also provide a detailed analysis of practitioners' replies on the relevance of individual machine learning attacks, unveiling complex concerns like unreliable decision making, business information leakage, and bias introduction into models. Finally, we find that on the individual level, prior knowledge about machine learning security influences threat perception. Our work paves the way for more research about adversarial machine learning in practice, but yields also insights for regulation and auditing.} }
@article{WOS:001070983900001, title = {A survey on multimodal bidirectional machine learning translation of image and natural language processing}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {235}, year = {2024}, issn = {0957-4174}, doi = {10.1016/j.eswa.2023.121168}, author = {Nam, Wongyung and Jang, Beakcheol}, abstract = {Advances in multimodal machine learning help artificial intelligence to resemble human intellect more closely, which perceives the world from multiple modalities. We surveyed state-of-the-art research on the modalities of bidirectional machine learning translation of image and natural language processing (NLP), which address a considerable proportion of human life. Recently, with the advances in deep learning model architectures and learning methods in the fields of image and NLP, considerable progress has been made in multimodal machine learning translations that can be built by integrating image and NLP. Our goal is to explore and summarize state-of-the-art research on multimodal machine learning translation and present a taxonomy for the multimodal bidirectional machine learning translation of image and NLP. Furthermore, we reviewed the evaluation metrics and compared state-of-the-art approaches that influences this field. We believe that this survey will become a cornerstone of future research by discussing the challenges in multimodal machine learning translation and direction of future research based on understanding state-of-the-art research in the field.} }
@article{WOS:000888210300020, title = {Challenges and opportunities in quantum machine learning}, journal = {NATURE COMPUTATIONAL SCIENCE}, volume = {2}, pages = {567-576}, year = {2022}, doi = {10.1038/s43588-022-00311-3}, author = {Cerezo, M. and Verdon, Guillaume and Huang, Hsin-Yuan and Cincio, Lukasz and Coles, Patrick J.}, abstract = {At the intersection of machine learning and quantum computing, quantum machine learning has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry and high-energy physics. Nevertheless, challenges remain regarding the trainability of quantum machine learning models. Here we review current methods and applications for quantum machine learning. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with quantum machine learning.} }
@article{WOS:001339794500006, title = {A review of the application of machine learning in water quality evaluation}, journal = {ECO-ENVIRONMENT \\& HEALTH}, volume = {1}, pages = {107-116}, year = {2022}, doi = {10.1016/j.eehl.2022.06.001}, author = {Zhu, Mengyuan and Wang, Jiawei and Yang, Xiao and Zhang, Yu and Zhang, Linyu and Ren, Hongqiang and Wu, Bing and Ye, Lin}, abstract = {With the rapid increase in the volume of data on the aquatic environment, machine learning has become an important tool for data analysis, classification, and prediction. Unlike traditional models used in water-related research, data-driven models based on machine learning can efficiently solve more complex nonlinear problems. In water environment research, models and conclusions derived from machine learning have been applied to the construction, monitoring, simulation, evaluation, and optimization of various water treatment and management systems. Additionally, machine learning can provide solutions for water pollution control, water quality improvement, and watershed ecosystem security management. In this review, we describe the cases in which machine learning algorithms have been applied to evaluate the water quality in different water environments, such as surface water, groundwater, drinking water, sewage, and seawater. Furthermore, we propose possible future applications of machine learning approaches to water environments.} }
@article{WOS:000833418600004, title = {A survey of human-in-the-loop for machine learning}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {135}, pages = {364-381}, year = {2022}, issn = {0167-739X}, doi = {10.1016/j.future.2022.05.014}, author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang}, abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-the-loop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions. Keywords: Human-in-the-loop Machine learning Deep learning Data processing Computer vision Natural language processing (C) 2022 Elsevier B.V. All rights reserved.} }
@article{WOS:000849857300001, title = {Machine Learning for Electrocatalyst and Photocatalyst Design and Discovery}, journal = {CHEMICAL REVIEWS}, volume = {122}, pages = {13478-13515}, year = {2022}, issn = {0009-2665}, doi = {10.1021/acs.chemrev.2c00061}, author = {Mai, Haoxin and Le, Tu C. and Chen, Dehong and Winkler, David A. and Caruso, Rachel A.}, abstract = {Electrocatalysts and photocatalysts are key to a sustainable future, generating clean fuels, reducing the impact of global warming, and providing solutions to environmental pollution. Improved processes for catalyst design and a better understanding of electro/ photocatalytic processes are essential for improving catalyst effectiveness. Recent advances in data science and artificial intelligence have great potential to accelerate electrocatalysis and photocatalysis research, particularly the rapid exploration of large materials chemistry spaces through machine learning. Here a comprehensive introduction to, and critical review of, machine learning techniques used in electrocatalysis and photocatalysis research are provided. Sources of electro/photocatalyst data and current approaches to representing these materials by mathematical features are described, the most commonly used machine learning methods summarized, and the quality and utility of electro/photocatalyst models evaluated. Illustrations of how machine learning models are applied to novel electro/ photocatalyst discovery and used to elucidate electrocatalytic or photocatalytic reaction mechanisms are provided. The review offers a guide for materials scientists on the selection of machine learning methods for electrocatalysis and photocatalysis research. The application of machine learning to catalysis science represents a paradigm shift in the way advanced, next-generation catalysts will be designed and synthesized.} }
@article{WOS:000770194100002, title = {Interpretable machine learning for knowledge generation in heterogeneous catalysis}, journal = {NATURE CATALYSIS}, volume = {5}, pages = {175-184}, year = {2022}, issn = {2520-1158}, doi = {10.1038/s41929-022-00744-z}, author = {Esterhuizen, Jacques A. and Goldsmith, Bryan R. and Linic, Suljo}, abstract = {Most applications of machine learning in heterogeneous catalysis thus far have used black-box models to predict computable physical properties (descriptors), such as adsorption or formation energies, that can be related to catalytic performance (that is, activity or stability). Extracting meaningful physical insights from these black-box models has proved challenging, as the internal logic of these black-box models is not readily interpretable due to their high degree of complexity. Interpretable machine learning methods that merge the predictive capacity of black-box models with the physical interpretability of physics-based models offer an alternative to black-box models. In this Perspective, we discuss the various interpretable machine learning methods available to catalysis researchers, highlight the potential of interpretable machine learning to accelerate hypothesis formation and knowledge generation, and outline critical challenges and opportunities for interpretable machine learning in heterogeneous catalysis.} }
@article{WOS:000899553000001, title = {Predictive models for concrete properties using machine learning and deep learning approaches: A review}, journal = {JOURNAL OF BUILDING ENGINEERING}, volume = {63}, year = {2023}, doi = {10.1016/j.jobe.2022.105444}, author = {Moein, Mohammad Mohtasham and Saradar, Ashkan and Rahmati, Komeil and Mousavinejad, Seyed Hosein Ghasemzadeh and Bristow, James and Aramali, Vartenie and Karakouzian, Moses}, abstract = {Concrete is one of the most widely used materials in various civil engineering applications. Its global production rate is increasing to meet demand. Mechanical properties of concrete are among important parameters in designing and evaluating its performance. Over the past few decades, machine learning has been used to model real-world problems. Machine learning, as a branch of artificial intelligence, is gaining popularity in many scientific fields such as robotics, statistics, bioinformatics, computer science, and construction materials. Machine learning has many advantages over statistical and experimental models, such as optimal accuracy, highperformance speed, responsiveness in complex environments, and economic cost-effectiveness. Recently, more researchers are looking into deep learning, which is a group of machine learning algorithms, as a powerful method in matters of diagnosis and classification. Hence, this paper provides a review of successful ML and DL model applications to predict concrete mechanical properties. Several modeling algorithms were reviewed highlighting their applications, performance, current knowledge gaps, and suggestions for future research. This paper will assist construction material engineers and researchers in selecting suitable and accurate techniques that fit their applications.} }
@article{WOS:000410555900032, title = {Quantum machine learning}, journal = {NATURE}, volume = {549}, pages = {195-202}, year = {2017}, issn = {0028-0836}, doi = {10.1038/nature23474}, author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth}, abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.} }
@article{WOS:001150877600003, title = {An Overview of Machine Learning for Asset Management}, journal = {JOURNAL OF PORTFOLIO MANAGEMENT}, volume = {49}, pages = {31-63}, year = {2023}, issn = {0095-4918}, author = {Lee, Yongjae and Thompson, John R. J. and Kim, Jang Ho and Kim, Woo Chang and Fabozzi, Francesco A.}, abstract = {Machine learning has been widely used in the asset management industry to improve operations and make data-driven decisions. This article provides an overview of machine learning for asset management by presenting various machine learning models in the context of their applications, including general classification and regression, time-series forecasting, natural language processing, dimension reduction, reinforcement learning, data generation, recommendation, and clustering. Additionally, it highlights the challenges of implementing machine learning in asset management, such as data quality and quantity, interpretability, and fairness.} }
@article{WOS:000917611300001, title = {Precision Machine Learning}, journal = {ENTROPY}, volume = {25}, year = {2023}, doi = {10.3390/e25010175}, author = {Michaud, Eric J. J. and Liu, Ziming and Tegmark, Max}, abstract = {We explore unique considerations involved in fitting machine learning (ML) models to data with very high precision, as is often required for science applications. We empirically compare various function approximation methods and study how they scale with increasing parameters and data. We find that neural networks (NNs) can often outperform classical approximation methods on high-dimensional examples, by (we hypothesize) auto-discovering and exploiting modular structures therein. However, neural networks trained with common optimizers are less powerful for low-dimensional cases, which motivates us to study the unique properties of neural network loss landscapes and the corresponding optimization challenges that arise in the high precision regime. To address the optimization issue in low dimensions, we develop training tricks which enable us to train neural networks to extremely low loss, close to the limits allowed by numerical precision.} }
@article{WOS:000998914800001, title = {Applied machine learning in hematopathology}, journal = {INTERNATIONAL JOURNAL OF LABORATORY HEMATOLOGY}, volume = {45}, pages = {87-94}, year = {2023}, issn = {1751-5521}, doi = {10.1111/ijlh.14110}, author = {Dehkharghanian, Taher and Mu, Youqing and Tizhoosh, Hamid R. and Campbell, Clinton J. V.}, abstract = {An increasing number of machine learning applications are being developed and applied to digital pathology, including hematopathology. The goal of these modern computerized tools is often to support diagnostic workflows by extracting and summarizing information from multiple data sources, including digital images of human tissue. Hematopathology is inherently multimodal and can serve as an ideal case study for machine learning applications. However, hematopathology also poses unique challenges compared to other pathology subspecialities when applying machine learning approaches. By modeling the pathologist workflow and thinking process, machine learning algorithms may be designed to address practical and tangible problems in hematopathology. In this article, we discuss the current trends in machine learning in hematopathology. We review currently available machine learning enabled medical devices supporting hematopathology workflows. We then explore current machine learning research trends of the field with a focus on bone marrow cytology and histopathology, and how adoption of new machine learning tools may be enabled through the transition to digital pathology.} }
@article{WOS:001011556200001, title = {Machine Learning Classification Model Comparison}, journal = {SOCIO-ECONOMIC PLANNING SCIENCES}, volume = {87}, year = {2023}, issn = {0038-0121}, doi = {10.1016/j.seps.2023.101560}, author = {Giudici, Paolo and Gramegna, Alex and Raffinetti, Emanuela}, abstract = {Machine learning models are boosting Artificial Intelligence applications in many domains, such as automotive, finance and health care. This is mainly due to their advantage, in terms of predictive accuracy, with respect to classic statistical models. However, machine learning models are much less explainable: less transparent, less interpretable. This paper proposes to improve machine learning models, by proposing a model selection methodology, based on Lorenz Zonoids, which allows to compare them in terms of predictive accuracy significant gains, leading to a selected model which maintains accuracy while improving explainability. We illustrate our proposal by means of simulated datasets and of a real credit scoring problem. The analysis of the former shows that the proposal improves alternative methods, based on the AUROC. The analysis of the latter shows that the proposal leads to models made up of two/three relevant variables that measure the profitability and the financial leverage of the companies asking for credit.} }
@article{WOS:001036662400001, title = {Machine learning in cardiology: Clinical application and basic research}, journal = {JOURNAL OF CARDIOLOGY}, volume = {82}, pages = {128-133}, year = {2023}, issn = {0914-5087}, doi = {10.1016/j.jjcc.2023.04.020}, author = {Komuro, Jin and Kusumoto, Dai and Hashimoto, Hisayuki and Yuasa, Shinsuke}, abstract = {Machine learning is a subfield of artificial intelligence. The quality and versatility of machine learning have been rapidly improving and playing a critical role in many aspects of social life. This trend is also observed in the med-ical field. Generally, there are three main types of machine learning: supervised, unsupervised, and reinforcement learning. Each type of learning is adequately selected for the purpose and type of data. In the field of medicine, various types of information are collected and used, and research using machine learning is becoming increas-ingly relevant. Many clinical studies are conducted using electronic health and medical records, including in the cardiovascular area. Machine learning has also been applied in basic research. Machine learning has been widely used for several types of data analysis, such as clustering of microarray analysis and RNA sequence anal-ysis. Machine learning is essential for genome and multi-omics analyses. This review summarizes the recent ad-vancements in the use of machine learning in clinical applications and basic cardiovascular research.\\& COPY; 2023 Japanese College of Cardiology. Published by Elsevier Ltd. All rights reserved.} }
@article{WOS:001133095300012, title = {pystacked: Stacking generalization and machine learning in Stata}, journal = {STATA JOURNAL}, volume = {23}, pages = {909-931}, year = {2023}, issn = {1536-867X}, doi = {10.1177/1536867X231212426}, author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E.}, abstract = {The pystacked command implements stacked generalization (Wolpert, 1992, Neural Networks 5: 241-259) for regression and binary classification via Python's scikit-learn. Stacking combines multiple supervised machine learners-the ``base'' or ``level-0'' learners-into one learner. The currently supported base learners include regularized regression, random forest, gradient boosted trees, support vector machines, and feed-forward neural nets (multilayer perceptron). pystacked can also be used as a ``regular'' machine learning program to fit one base learner and thus provides an easy-to-use application programming interface for scikit-learn's machine learning algorithms.} }
@article{WOS:001001420400001, title = {An enhanced Runge Kutta boosted machine learning framework for medical diagnosis}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {160}, year = {2023}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2023.106949}, author = {Qiao, Zenglin and Li, Lynn and Zhao, Xinchao and Liu, Lei and Zhang, Qian and Hechmi, Shili and Atri, Mohamed and Li, Xiaohua}, abstract = {With the development and maturity of machine learning methods, medical diagnosis aided with machine learning methods has become a popular method to assist doctors in diagnosing and treating patients. However, machine learning methods are greatly affected by their hyperparameters, for instance, the kernel parameter in kernel extreme learning machine (KELM) and the learning rate in residual neural networks (ResNet). If the hyperparameters are appropriately set, the performance of the classifier can be significantly improved. To boost the performance of the machine learning methods, this paper proposes to improve the Runge Kutta optimizer (RUN) to adaptively adjust the hyperparameters of the machine learning methods for medical diagnosis pur -poses. Although RUN has a solid mathematical theoretical foundation, there are still some performance defects when dealing with complex optimization problems. To remedy these defects, this paper proposes a new enhanced RUN method with a grey wolf mechanism and an orthogonal learning mechanism called GORUN. The superior performance of the GORUN was validated against other well-established optimizers on IEEE CEC 2017 bench-mark functions. Then, the proposed GORUN is employed to optimize the machine learning models, including the KELM and ResNet, to construct robust models for medical diagnosis. The performance of the proposed machine learning framework was validated on several medical data sets, and the experimental results have demonstrated its superiority.} }
@article{WOS:000985911200001, title = {Ensemble machine learning methods in screening electronic health records: A scoping review}, journal = {DIGITAL HEALTH}, volume = {9}, year = {2023}, issn = {2055-2076}, doi = {10.1177/20552076231173225}, author = {Stevens, Christophe A. T. and Lyons, Alexander R. M. and Dharmayat, I, Kanika and Mahani, Alireza and Ray, Kausik K. and Vallejo-Vaz, Antonio J. and Sharabiani, Mansour T. A.}, abstract = {BackgroundElectronic health records provide the opportunity to identify undiagnosed individuals likely to have a given disease using machine learning techniques, and who could then benefit from more medical screening and case finding, reducing the number needed to screen with convenience and healthcare cost savings. Ensemble machine learning models combining multiple prediction estimates into one are often said to provide better predictive performances than non-ensemble models. Yet, to our knowledge, no literature review summarises the use and performances of different types of ensemble machine learning models in the context of medical pre-screening. MethodWe aimed to conduct a scoping review of the literature reporting the derivation of ensemble machine learning models for screening of electronic health records. We searched EMBASE and MEDLINE databases across all years applying a formal search strategy using terms related to medical screening, electronic health records and machine learning. Data were collected, analysed, and reported in accordance with the PRISMA scoping review guideline. ResultsA total of 3355 articles were retrieved, of which 145 articles met our inclusion criteria and were included in this study. Ensemble machine learning models were increasingly employed across several medical specialties and often outperformed non-ensemble approaches. Ensemble machine learning models with complex combination strategies and heterogeneous classifiers often outperformed other types of ensemble machine learning models but were also less used. Ensemble machine learning models methodologies, processing steps and data sources were often not clearly described. ConclusionsOur work highlights the importance of deriving and comparing the performances of different types of ensemble machine learning models when screening electronic health records and underscores the need for more comprehensive reporting of machine learning methodologies employed in clinical research.} }
@article{WOS:001124950300001, title = {Bilevel optimization for automated machine learning: a new perspective on framework and algorithm}, journal = {NATIONAL SCIENCE REVIEW}, volume = {11}, year = {2023}, issn = {2095-5138}, doi = {10.1093/nsr/nwad292}, author = {Liu, Risheng and Lin, Zhouchen}, abstract = {Formulating the methodology of machine learning by bilevel optimization techniques provides a new perspective to understand and solve automated machine learning problems.} }
@article{WOS:001015283800001, title = {Low-Code Machine Learning Platforms: A Fastlane to Digitalization}, journal = {INFORMATICS-BASEL}, volume = {10}, year = {2023}, doi = {10.3390/informatics10020050}, author = {Raghavendran, Krishna Raj and Elragal, Ahmed}, abstract = {In the context of developing machine learning models, until and unless we have the required data engineering and machine learning development competencies as well as the time to train and test different machine learning models and tune their hyperparameters, it is worth trying out the automatic machine learning features provided by several cloud-based and cloud-agnostic platforms. This paper explores the possibility of generating automatic machine learning models with low-code experience. We developed criteria to compare different machine learning platforms for generating automatic machine learning models and presenting their results. Thereafter, lessons learned by developing automatic machine learning models from a sample dataset across four different machine learning platforms were elucidated. We also interviewed machine learning experts to conceptualize their domain-specific problems that automatic machine learning platforms can address. Results showed that automatic machine learning platforms can provide a fast track for organizations seeking the digitalization of their businesses. Automatic machine learning platforms help produce results, especially for time-constrained projects where resources are lacking. The contribution of this paper is in the form of a lab experiment in which we demonstrate how low-code platforms can provide a viable option to many business cases and, henceforth, provide a lane that is faster than the usual hiring and training of already scarce data scientists and to analytics projects that suffer from overruns.} }
@article{WOS:001096009300001, title = {Machine Learning in Gamification and Gamification in Machine Learning: A Systematic Literature Mapping}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app132011427}, author = {Swacha, Jakub and Gracel, Michal}, abstract = {Albeit in different ways, both machine learning and gamification have transfigured the user experience of information systems. Although both are hot research topics, so far, little attention has been paid to how these two technologies converge with each other. This relation is not obvious as while it is feasible to enhance gamification with machine learning, it is also feasible to support machine learning with gamification; moreover, there are applications in which machine learning and gamification are combined yet not directly connected. In this study, we aim to shed light on the use of both machine learning in gamification and gamification in machine learning, as well as the related topics of using gamification in machine learning education and machine learning in gamification research. By performing a systematic literature mapping, we not only identify prior works addressing these respective themes, but also analyze how their popularity evolved in time, investigate the areas of application reported by prior works, used machine learning techniques and software tools, as well as the character of research contribution and the character of evaluation results for works that presented them.} }
@article{WOS:001001117700011, title = {Introducing Machine Learning in Auditing Courses}, journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING}, volume = {20}, pages = {195-211}, year = {2023}, issn = {1554-1908}, doi = {10.2308/JETA-2022-017}, author = {Huang, Feiqi and Wang, Yunsen}, abstract = {The advances in machine learning have gained close attention from audit practitioners and standard setters. However, fewer than half of accounting programs teach predictive analysis, including machine learning. To develop students' knowledge and skills of machine learning in auditing applications, this study introduces machine learning to the accounting curriculum and presents a novel hands-on approach for teaching machine learning in auditing courses. The objective is to provide students who have no statistics background and programming skills with the basic knowledge of machine learning and hands-on exercises for predicting auditing tasks. In addition to instruction manuals, this study demonstrates an implementation of machine learning exercises in a graduate-level course.} }
@article{WOS:000760318100006, title = {Stable learning establishes some common ground between causal inference and machine learning}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {4}, pages = {110-115}, year = {2022}, doi = {10.1038/s42256-022-00445-z}, author = {Cui, Peng and Athey, Susan}, abstract = {Causal inference has recently attracted substantial attention in the machine learning and artificial intelligence community. It is usually positioned as a distinct strand of research that can broaden the scope of machine learning from predictive modelling to intervention and decision-making. In this Perspective, however, we argue that ideas from causality can also be used to improve the stronghold of machine learning, predictive modelling, if predictive stability, explainability and fairness are important. With the aim of bridging the gap between the tradition of precise modelling in causal inference and black-box approaches from machine learning, stable learning is proposed and developed as a source of common ground. This Perspective clarifies a source of risk for machine learning models and discusses the benefits of bringing causality into learning. We identify the fundamental problems addressed by stable learning, as well as the latest progress from both causal inference and learning perspectives, and we discuss relationships with explainability and fairness problems. Machine learning performs well at predictive modelling based on statistical correlations, but for high-stakes applications, more robust, explainable and fair approaches are required. Cui and Athey discuss the benefits of bringing causal inference into machine learning, presenting a stable learning approach.} }
@article{WOS:000819703300001, title = {Machine Learning for Organic Photovoltaic Polymers: A Minireview}, journal = {CHINESE JOURNAL OF POLYMER SCIENCE}, volume = {40}, pages = {870-876}, year = {2022}, issn = {0256-7679}, doi = {10.1007/s10118-022-2782-5}, author = {Mahmood, Asif and Irfan, Ahmad and Wang, Jin-Liang}, abstract = {Machine learning is a powerful tool that can provide a way to revolutionize the material science. Its use for the designing and screening of materials for polymer solar cells is also increasing. Search of efficient polymeric materials for solar cells is really difficult task. Researchers have synthesized and fabricated so many materials. Sorting the results and get feedback for further research requires an innovative approach. In this minireview, we provides brief introduction of machine learning. The importance of machine learning is also mentioned, and the application of machine learning for polymeric material design is discussed. The key challenges that are hindering the wide spread use of machine are discussed. Suggestions are also given to improve the use of data science. The predictions using machine learning maybe not highly accurate but it definitely better than no prediction at all.} }
@article{WOS:000760354200006, title = {Machine learning for multi-omics data integration in cancer}, journal = {ISCIENCE}, volume = {25}, year = {2022}, doi = {10.1016/j.isci.2022.103798}, author = {Cai, Zhaoxiang and Poulos, Rebecca C. and Liu, Jia and Zhong, Qing}, abstract = {Multi-omics data analysis is an important aspect of cancer molecular biology studies and has led to ground-breaking discoveries. Many efforts have been made to develop machine learning methods that automatically integrate omics data. Here, we review machine learning tools categorized as either general-purpose or task-specific, covering both supervised and unsupervised learning for integrative analysis of multi-omics data. We benchmark the performance of five machine learning approaches using data from the Cancer Cell Line Encyclopedia, reporting accuracy on cancer type classification and mean absolute error on drug response prediction, and evaluating runtime efficiency. This review provides recommendations to researchers regarding suitable machine learning method selection for their specific applications. It should also promote the development of novel machine learning methodologies for data integration, which will be essential for drug discovery, clinical trial design, and personalized treatments.} }
@article{WOS:001059183900006, title = {Machine learning for combustion}, journal = {ENERGY AND AI}, volume = {7}, year = {2022}, issn = {2666-5468}, doi = {10.1016/j.egyai.2021.100128}, author = {Zhou, Lei and Song, Yuntong and Ji, Weiqi and Wei, Haiqiao}, abstract = {Combustion science is an interdisciplinary study that involves nonlinear physical and chemical phenomena in time and length scales, including complex chemical reactions and fluid flows. Combustion widely supplies energy for powering vehicles, heating houses, generating electricity, cooking food, etc. The key to study combustion is to improve the combustion efficiency with minimum emission of pollutants. Machine learning facilitates datadriven techniques for handling large amounts of combustion data, either obtained through experiments or simulations under multiple spatiotemporal scales, thereby finding the hidden patterns underlying these data and promoting combustion research. This work presents an overview of studies on the applications of machine learning in combustion science fields over the past several decades. We introduce the fundamentals of machine learning and its usage in aiding chemical reactions, combustion modeling, combustion measurement, engine performance prediction and optimization, and fuel design. The opportunities and limitations of using machine learning in combustion studies are also discussed. This paper aims to provide readers with a portrait of what and how machine learning can be used in combustion research and to inspire researchers in their ongoing studies. Machine learning techniques are rapidly advancing in this era of big data, and there is high potential for exploring the combination between machine learning and combustion research and achieving remarkable results.} }
@article{WOS:000804596500003, title = {A comparative study of different machine learning methods for reservoir landslide displacement prediction}, journal = {ENGINEERING GEOLOGY}, volume = {298}, year = {2022}, issn = {0013-7952}, doi = {10.1016/j.enggeo.2022.106544}, author = {Wang, Yankun and Tang, Huiming and Huang, Jinsong and Wen, Tao and Ma, Junwei and Zhang, Junrong}, abstract = {ABSTR A C T This paper compares the performance of five popular machine learning methods, namely, particle swarm opti-mization-extreme learning machine (PSO-ELM), particle swarm optimization-kernel extreme learning machine (PSO-KELM), particle swarm optimization-support vector machine (PSO-SVM), particle swarm opti-mization-least squares support vector machine (PSO-LSSVM), and long short-term memory neural network (LSTM), in the prediction of reservoir landslide displacement. The Baishuihe, Shuping, and Baijiabao landslides in the Three Gorges reservoir area of China were used for case studies. Cumulative displacement was decom-posed into trend displacement and periodic displacement by the Hodrick-Prescott filter. The double exponential smoothing method and the five machine learning methods were used to predict the trend and periodic displacement, respectively. The five machine learning methods are compared in three aspects: highest single prediction accuracy, mean prediction accuracy, and prediction stability. The results show that no method per -formed the best for all three aspects in the three landslide cases. LSTM and PSO-ELM achieved better single prediction accuracy, but worse mean prediction accuracy and stability. PSO-KELM, PSO-LSSVM, and PSO-SVM always yielded consistent predictions with slight variations. On the whole, PSO-KELM and PSO-LSSVM are recommended for their superior mean prediction accuracy and prediction stability.} }
@article{WOS:000812536000080, title = {Data Poisoning Attacks on Federated Machine Learning}, journal = {IEEE INTERNET OF THINGS JOURNAL}, volume = {9}, pages = {11365-11375}, year = {2022}, issn = {2327-4662}, doi = {10.1109/JIOT.2021.3128646}, author = {Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Lyu, Lingjuan and Liu, Ji}, abstract = {Federated machine learning which enables resource-constrained node devices (e.g., Internet of Things (IoT) devices and smartphones) to establish a knowledge-shared model while keeping the raw data local, could provide privacy preservation, and economic benefit by designing an effective communication protocol. However, this communication protocol can be adopted by attackers to launch data poisoning attacks for different nodes, which has been shown as a big threat to most machine learning models. Therefore, we in this article intend to study the model vulnerability of federated machine learning, and even on IoT systems. To be specific, we here attempt to attacking a popular federated multitask learning framework, which uses a general multitask learning framework to handle statistical challenges in the federated learning setting. The problem of calculating optimal poisoning attacks on federated multitask learning is formulated as a bilevel program, which is adaptive to the arbitrary selection of target nodes and source attacking nodes. We then propose a novel systems-aware optimization method, called as attack on federated learning (AT(2)FL), to efficiently derive the implicit gradients for poisoned data, and further attain optimal attack strategies in the federated machine learning. This is an earlier work, to our knowledge, that explores attacking federated machine learning via data poisoning. Finally, experiments on several real-world data sets demonstrate that when the attackers directly poison the target nodes or indirectly poison the related nodes via using the communication protocol, the federated multitask learning model is sensitive to both poisoning attacks.} }
@article{WOS:000851470400007, title = {SUBSTITUTING HUMAN DECISION-MAKING WITH MACHINE LEARNING: IMPLICATIONS FOR ORGANIZATIONAL LEARNING}, journal = {ACADEMY OF MANAGEMENT REVIEW}, volume = {47}, pages = {448-465}, year = {2022}, issn = {0363-7425}, doi = {10.5465/amr.2019.0470}, author = {Balasubramanian, Natarajan and Ye, Yang and Xu, Mingtao}, abstract = {The richness of organizational learning relies on the ability of humans to develop diverse patterns of action by actively engaging with their environments and applying substantive rationality. The substitution of human decision-making with machine learning has the potential to alter this richness of organizational learning. Though machine learning is significantly faster and seemingly unconstrained by human cognitive limitations and inflexibility, it is not true sentient learning and relies on formal statistical analysis for decision-making. We propose that the distinct differences between human learning and machine learning risk decreasing the within-organizational diversity in organizational routines and the extent of causal, contextual, and general knowledge associated with routines. We theorize that these changes may affect organizational learning by exacerbating the myopia of learning, and highlight some important contingencies that may mute or amplify the risk of such myopia.} }
@article{WOS:000863168300001, title = {Naive automated machine learning}, journal = {MACHINE LEARNING}, volume = {112}, pages = {1131-1170}, year = {2023}, issn = {0885-6125}, doi = {10.1007/s10994-022-06200-0}, author = {Mohr, Felix and Wever, Marcel}, abstract = {An essential task of automated machine learning (AutoML) is the problem of automatically finding the pipeline with the best generalization performance on a given dataset. This problem has been addressed with sophisticated black-box optimization techniques such as Bayesian optimization, grammar-based genetic algorithms, and tree search algorithms. Most of the current approaches are motivated by the assumption that optimizing the components of a pipeline in isolation may yield sub-optimal results. We present Naive AutoML, an approach that precisely realizes such an in-isolation optimization of the different components of a pre-defined pipeline scheme. The returned pipeline is obtained by just taking the best algorithm of each slot. The isolated optimization leads to substantially reduced search spaces, and, surprisingly, this approach yields comparable and sometimes even better performance than current state-of-the-art optimizers.} }
@article{WOS:000838252900001, title = {Open-environment machine learning}, journal = {NATIONAL SCIENCE REVIEW}, volume = {9}, year = {2022}, issn = {2095-5138}, doi = {10.1093/nsr/nwac123}, author = {Zhou, Zhi-Hua}, abstract = {Conventional machine learning studies generally assume close-environment scenarios where important factors of the learning process hold invariant. With the great success of machine learning, nowadays, more and more practical tasks, particularly those involving open-environment scenarios where important factors are subject to change, called open-environment machine learning in this article, are present to the community. Evidently, it is a grand challenge for machine learning turning from close environment to open environment. It becomes even more challenging since, in various big data tasks, data are usually accumulated with time, like streams, while it is hard to train the machine learning model after collecting all data as in conventional studies. This article briefly introduces some advances in this line of research, focusing on techniques concerning emerging new classes, decremental/incremental features, changing data distributions and varied learning objectives, and discusses some theoretical issues. This article briefly introduces Open Environment Machine Learning, where important factors of the machine learning process are subject to change, as occurring in many practical tasks.} }
@article{WOS:000852243600006, title = {Cognitive Workload Recognition Using EEG Signals and Machine Learning: A Review}, journal = {IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS}, volume = {14}, pages = {799-818}, year = {2022}, issn = {2379-8920}, doi = {10.1109/TCDS.2021.3090217}, author = {Zhou, Yueying and Huang, Shuo and Xu, Ziming and Wang, Pengpai and Wu, Xia and Zhang, Daoqiang}, abstract = {Machine learning and its subfield deep learning techniques provide opportunities for the development of operator mental state monitoring, especially for cognitive workload recognition using electroencephalogram (EEG) signals. Although a variety of machine learning methods have been proposed for recognizing cognitive workload via EEG recently, there does not yet exist a review that covers in-depth the application of machine learning methods. To alleviate this gap, in this article, we survey cognitive workload and machine learning literature to identify the approaches and highlight the primary advances. To be specific, we first introduce the concepts of cognitive workload and machine learning. Then, we discuss the steps of classical machine learning for cognitive workload recognition from the following aspects, i.e., EEG data preprocessing, feature extraction and selection, classification method, and evaluation methods. Further, we review the commonly used deep learning methods for this domain. Finally, we expound on the open problem and future outlooks.} }
@article{WOS:000816020800006, title = {Understanding from Machine Learning Models}, journal = {BRITISH JOURNAL FOR THE PHILOSOPHY OF SCIENCE}, volume = {73}, pages = {109-133}, year = {2022}, issn = {0007-0882}, doi = {10.1093/bjps/axz035}, author = {Sullivan, Emily}, abstract = {Simple idealized models seem to provide more understanding than opaque, complex, and hyper-realistic models. However, an increasing number of scientists are going in the opposite direction by utilizing opaque machine learning models to make predictions and draw inferences, suggesting that scientists are opting for models that have less potential for understanding. Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided? In this article, using the case of deep neural networks, I argue that it is not the complexity or black box nature of a model that limits how much understanding the model provides. Instead, it is a lack of scientific and empirical evidence supporting the link that connects a model to the target phenomenon that primarily prohibits understanding.} }
@article{WOS:000819852500009, title = {Machine learning models and over-fitting considerations}, journal = {WORLD JOURNAL OF GASTROENTEROLOGY}, volume = {28}, pages = {605-607}, year = {2022}, issn = {1007-9327}, doi = {10.3748/wjg.v28.i5.605}, author = {Charilaou, Paris and Battat, Robert}, abstract = {Machine learning models may outperform traditional statistical regression algorithms for predicting clinical outcomes. Proper validation of building such models and tuning their underlying algorithms is necessary to avoid over-fitting and poor generalizability, which smaller datasets can be more prone to. In an effort to educate readers interested in artificial intelligence and model-building based on machine-learning algorithms, we outline important details on cross-validation techniques that can enhance the performance and generalizability of such models.} }
@article{WOS:000602863200001, title = {Machine learning for combinatorial optimization: A methodological tour d'horizon}, journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH}, volume = {290}, pages = {405-421}, year = {2021}, issn = {0377-2217}, doi = {10.1016/j.ejor.2020.07.063}, author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine}, abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000868715700001, title = {A Review on Machine Learning Styles in Computer Vision-Techniques and Future Directions}, journal = {IEEE ACCESS}, volume = {10}, pages = {107293-107329}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3209825}, author = {Mahadevkar, V, Supriya and Khemani, Bharti and Patil, Shruti and Kotecha, Ketan and Vora, Deepali R. and Abraham, Ajith and Gabralla, Lubna Abdelkareim}, abstract = {Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.} }
@article{WOS:000794033200007, title = {Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods}, journal = {JOURNAL OF BUSINESS RESEARCH}, volume = {144}, pages = {93-106}, year = {2022}, issn = {0148-2963}, doi = {10.1016/j.jbusres.2022.01.076}, author = {van Giffen, Benjamin and Herhausen, Dennis and Fahse, Tobias}, abstract = {Over the last decade, the importance of machine learning increased dramatically in business and marketing. However, when machine learning is used for decision-making, bias rooted in unrepresentative datasets, inade-quate models, weak algorithm designs, or human stereotypes can lead to low performance and unfair decisions, resulting in financial, social, and reputational losses. This paper offers a systematic, interdisciplinary literature review of machine learning biases as well as methods to avoid and mitigate these biases. We identified eight distinct machine learning biases, summarized these biases in the cross-industry standard process for data mining to account for all phases of machine learning projects, and outline twenty-four mitigation methods. We further contextualize these biases in a real-world case study and illustrate adequate mitigation strategies. These insights synthesize the literature on machine learning biases in a concise manner and point to the importance of human judgment for machine learning algorithms.} }
@article{WOS:000913331400001, title = {Machine learning accelerates the materials discovery}, journal = {MATERIALS TODAY COMMUNICATIONS}, volume = {33}, year = {2022}, doi = {10.1016/j.mtcomm.2022.104900}, author = {Fang, Jiheng and Xie, Ming and He, Xingqun and Zhang, Jiming and Hu, Jieqiong and Chen, Yongtai and Yang, Youcai and Jin, Qinglin}, abstract = {As the big data generated by the development of modern experiments and computing technology becomes more and more accessible, the material design method based on machine learning (ML) has opened a new paradigm for materials science research. With its ability to automatically solve complex tasks, machine learning is being used as a new method to help discover the relevance of materials, understand materials' properties, and accelerate the discovery of materials. This paper first introduces the general process of machine learning in materials science. Secondly, the applications of machine learning in material properties prediction, classification and identification, auxiliary micro-scale characterization, phase transformation research and phase diagram construction, process optimization, service behavior evaluation, accelerating the development of computational simulation technology, multi-objective optimization and inverse design of materials are reviewed. Finally, we discuss the main challenges and possible solutions in machine learning, and predict the potential research directions.} }
@article{WOS:000737778100001, title = {Applying machine learning to study fluid mechanics}, journal = {ACTA MECHANICA SINICA}, volume = {37}, pages = {1718-1726}, year = {2021}, issn = {0567-7718}, doi = {10.1007/s10409-021-01143-6}, author = {Brunton, Steven L.}, abstract = {This paper provides a short overview of how to use machine learning to build data-driven models in fluid mechanics. The process of machine learning is broken down into five stages: (1) formulating a problem to model, (2) collecting and curating training data to inform the model, (3) choosing an architecture with which to represent the model, (4) designing a loss function to assess the performance of the model, and (5) selecting and implementing an optimization algorithm to train the model. At each stage, we discuss how prior physical knowledge may be embedding into the process, with specific examples from the field of fluid mechanics.} }
@article{WOS:000761920900004, title = {Machine learning in the quantum realm: The state-of-the-art, challenges, and future vision}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {194}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.116512}, author = {Houssein, Essam H. and Abohashima, Zainab and Elhoseny, Mohamed and Mohamed, Waleed M.}, abstract = {Machine learning has become a ubiquitous and effective technique for data processing and classification. Furthermore, due to the superiority and progress of quantum computing in many areas (e.g., cryptography, machine learning, healthcare), a combination of classical machine learning and quantum information processing has established a new field, called, quantum machine learning. One of the most frequently used applications of quantum computing is machine learning. This paper aims to present a comprehensive review of state-of-the-art advances in quantum machine learning. Besides, this paper outlines recent works on different architectures of quantum deep learning, and illustrates classification tasks in the quantum domain as well as encoding methods and quantum subroutines. Furthermore, this paper examines how the concept of quantum computing enhances classical machine learning. Two methods for improving the performance of classical machine learning are presented. Finally, this work provides a general review of challenges and the future vision of quantum machine learning.} }
@article{WOS:001156335200001, title = {Machine learning in energy storage materials}, journal = {INTERDISCIPLINARY MATERIALS}, volume = {1}, pages = {175-195}, year = {2022}, issn = {2767-4401}, doi = {10.1002/idm2.12020}, author = {Shen, Zhong-Hui and Liu, Han-Xing and Shen, Yang and Hu, Jia-Mian and Chen, Long-Qing and Nan, Ce-Wen}, abstract = {With its extremely strong capability of data analysis, machine learning has shown versatile potential in the revolution of the materials research paradigm. Here, taking dielectric capacitors and lithium-ion batteries as two representative examples, we review substantial advances of machine learning in the research and development of energy storage materials. First, a thorough discussion of the machine learning framework in materials science is presented. Then, we summarize the applications of machine learning from three aspects, including discovering and designing novel materials, enriching theoretical simulations, and assisting experimentation and characterization. Finally, a brief outlook is highlighted to spark more insights on the innovative implementation of machine learning in materials science.} }
@article{WOS:000789003800004, title = {A Survey on Large-Scale Machine Learning}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {34}, pages = {2574-2594}, year = {2022}, issn = {1041-4347}, doi = {10.1109/TKDE.2020.3015777}, author = {Wang, Meng and Fu, Weijie and He, Xiangnan and Hao, Shijie and Wu, Xindong}, abstract = {Machine learning can provide deep insights into data, allowing machines to make high-quality predictions and having been widely used in real-world applications, such as text mining, visual classification, and recommender systems. However, most sophisticated machine learning approaches suffer from huge time costs when operating on large-scale data. This issue calls for the need of Large-scale Machine Learning (LML), which aims to learn patterns from big data with comparable performance efficiently. In this paper, we offer a systematic survey on existing LML methods to provide a blueprint for the future developments of this area. We first divide these LML methods according to the ways of improving the scalability: 1) model simplification on computational complexities, 2) optimization approximation on computational efficiency, and 3) computation parallelism on computational capabilities. Then we categorize the methods in each perspective according to their targeted scenarios and introduce representative methods in line with intrinsic strategies. Lastly, we analyze their limitations and discuss potential directions as well as open issues that are promising to address in the future.} }
@article{WOS:000781177700001, title = {Scientific machine learning benchmarks}, journal = {NATURE REVIEWS PHYSICS}, volume = {4}, pages = {413-420}, year = {2022}, doi = {10.1038/s42254-022-00441-7}, author = {Thiyagalingam, Jeyan and Shankar, Mallikarjun and Fox, Geoffrey and Hey, Tony}, abstract = {Finding the most appropriate machine learning algorithm for the analysis of any given scientific dataset is currently challenging, but new machine learning benchmarks for science are being developed to help. Deep learning has transformed the use of machine learning technologies for the analysis of large experimental datasets. In science, such datasets are typically generated by large-scale experimental facilities, and machine learning focuses on the identification of patterns, trends and anomalies to extract meaningful scientific insights from the data. In upcoming experimental facilities, such as the Extreme Photonics Application Centre (EPAC) in the UK or the international Square Kilometre Array (SKA), the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis. However, at present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is a challenge due to the potential applicability of many different machine learning frameworks, computer architectures and machine learning models. Historically, for modelling and simulation on high-performance computing systems, these issues have been addressed through benchmarking computer applications, algorithms and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to open, curated scientific datasets is a new challenge for both scientists and computer scientists. Here, we introduce the concept of machine learning benchmarks for science and review existing approaches. As an example, we describe the SciMLBench suite of scientific machine learning benchmarks.} }
@article{WOS:000754182500001, title = {New Opportunity: Machine Learning for Polymer Materials Design and Discovery}, journal = {ADVANCED THEORY AND SIMULATIONS}, volume = {5}, year = {2022}, doi = {10.1002/adts.202100565}, author = {Xu, Pengcheng and Chen, Huimin and Li, Minjie and Lu, Wencong}, abstract = {Under the guidance of the material genome initiative (MGI), the use of data-driven methods to discover new materials has become an innovation of materials science. The polymer materials have been one of the most important parts in materials science for the excellent physical and chemical properties as well as corresponding complex structures. Machine learning, as the core of data-driven methods, has taken an important place in polymer materials design and discovery. In this review, the authors have introduced the applications of machine learning in the design and discovery of polymer materials. The development tendency of published papers about machine learning in polymer materials, the commonly used algorithms, the polymer descriptors, the workflow of machine learning in polymer materials, and recent progresses of machine learning in materials are summarized. Then, the detail of how to use machine learning to assist design and discovery of polymer materials is fully discussed combined with two cases. Finally, the opportunities and challenges on the future development prospects of machine learning in the field of polymer materials are proposed.} }
@article{WOS:000884152000002, title = {Unsupervised machine learning methods and emerging applications in healthcare}, journal = {KNEE SURGERY SPORTS TRAUMATOLOGY ARTHROSCOPY}, volume = {31}, pages = {376-381}, year = {2023}, issn = {0942-2056}, doi = {10.1007/s00167-022-07233-7}, author = {Eckhardt, Christina M. and Madjarova, Sophia J. and Williams, Riley J. and Ollivier, Mattheu and Karlsson, Jon and Pareek, Ayoosh and Nwachukwu, Benedict U.}, abstract = {Unsupervised machine learning methods are important analytical tools that can facilitate the analysis and interpretation of high-dimensional data. Unsupervised machine learning methods identify latent patterns and hidden structures in high-dimensional data and can help simplify complex datasets. This article provides an overview of key unsupervised machine learning techniques including K-means clustering, hierarchical clustering, principal component analysis, and factor analysis. With a deeper understanding of these analytical tools, unsupervised machine learning methods can be incorporated into health sciences research to identify novel risk factors, improve prevention strategies, and facilitate delivery of personalized therapies and targeted patient care.} }
@article{WOS:000479252200001, title = {Recent advances and applications of machine learning in solid-state materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, doi = {10.1038/s41524-019-0221-0}, author = {Schmidt, Jonathan and Marques, Mario R. G. and Botti, Silvana and Marques, Miguel A. L.}, abstract = {One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure-property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.} }
@article{WOS:000761186500001, title = {Machine Learning and Deep Learning Approaches for CyberSecurity: A Review}, journal = {IEEE ACCESS}, volume = {10}, pages = {19572-19585}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3151248}, author = {Halbouni, Asmaa and Gunawan, Teddy Surya and Habaebi, Mohamed Hadi and Halbouni, Murad and Kartiwi, Mira and Ahmad, Robiah}, abstract = {The rapid evolution and growth of the internet through the last decades led to more concern about cyber-attacks that are continuously increasing and changing. As a result, an effective intrusion detection system was required to protect data, and the discovery of artificial intelligence's sub-fields, machine learning, and deep learning, was one of the most successful ways to address this problem. This paper reviewed intrusion detection systems and discussed what types of learning algorithms machine learning and deep learning are using to protect data from malicious behavior. It discusses recent machine learning and deep learning work with various network implementations, applications, algorithms, learning approaches, and datasets to develop an operational intrusion detection system.} }
@article{WOS:000870821400025, title = {Technology readiness levels for machine learning systems}, journal = {NATURE COMMUNICATIONS}, volume = {13}, year = {2022}, doi = {10.1038/s41467-022-33128-9}, author = {Lavin, Alexander and Gilligan-Lee, Ciaran M. and Visnjic, Alessya and Ganju, Siddha and Newman, Dava and Ganguly, Sujoy and Lange, Danny and Baydin, Atilim Gunes and Sharma, Amit and Gibson, Adam and Zheng, Stephan and Xing, Eric P. and Mattmann, Chris and Parr, James and Gal, Yarin}, abstract = {The development of machine learning systems has to ensure their robustness and reliability. The authors introduce a framework that defines a principled process of machine learning system formation, from research to production, for various domains and data scenarios. The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. Lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, with mission critical measures and robustness throughout the process. Drawing on experience in both spacecraft engineering and machine learning (research through product across domain areas), we've developed a proven systems engineering approach for machine learning and artificial intelligence: the Machine Learning Technology Readiness Levels framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for machine learning workflows, including key distinctions from traditional software engineering, and a lingua franca for people across teams and organizations to work collaboratively on machine learning and artificial intelligence technologies. Here we describe the framework and elucidate with use-cases from physics research to computer vision apps to medical diagnostics.} }
@article{WOS:000765501200001, title = {Application of machine learning for advanced material prediction and design}, journal = {ECOMAT}, volume = {4}, year = {2022}, doi = {10.1002/eom2.12194}, author = {Chan, Cheuk Hei and Sun, Mingzi and Huang, Bolong}, abstract = {In material science, traditional experimental and computational approaches require investing enormous time and resources, and the experimental conditions limit the experiments. Sometimes, traditional approaches may not yield satisfactory results for the desired purpose. Therefore, it is essential to develop a new approach to accelerate experimental progress and avoid unnecessary wasting of time and resources. As a data-driven method, machine learning provides reliable and accurate performance to solve problems in material science. This review first outlines the fundamental information of machine learning. It continues with the research concerning the prediction of various properties of materials by machine learning. Then it discusses the methods for the discovery of new materials and the prediction of their structural information. Finally, we summarize other applications of machine learning in material science. This review will be beneficial for future application of machine learning in more material science research.} }
@article{WOS:000674857200001, title = {Principles and Practice of Explainable Machine Learning}, journal = {FRONTIERS IN BIG DATA}, volume = {4}, year = {2021}, doi = {10.3389/fdata.2021.688969}, author = {Belle, Vaishak and Papantonis, Ioannis}, abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods-machine learning (ML) and pattern recognition models in particular-so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.} }
@article{WOS:000599821400007, title = {Machine learning applications for building structural design and performance assessment: State-of-the-art review}, journal = {JOURNAL OF BUILDING ENGINEERING}, volume = {33}, year = {2021}, doi = {10.1016/j.jobe.2020.101816}, author = {Sun, Han and Burton, Henry V. and Huang, Honglan}, abstract = {Machine learning models have been shown to be useful for predicting and assessing structural performance, identifying structural condition and informing preemptive and recovery decisions by extracting patterns from data collected via various sources and media. This paper presents a review of the historical development and recent advances in the application of machine learning to the area of building structural design and performance assessment. To this end, an overview of machine learning theory and the most relevant algorithms is provided with the goal of identifying problems suitable for machine learning and the appropriate models to use. The machine learning applications in building structural design and performance assessment are then reviewed in four main categories: (1) predicting structural response and performance, (2) interpreting experimental data and formulating models to predict component-level structural properties, (3) information retrieval using images and written text and (4) recognizing patterns in structural health monitoring data. The challenges of bringing machine learning into structural engineering practice are identified, and future research opportunities are discussed.} }
@article{WOS:000649545300034, title = {A review of machine learning in building load prediction}, journal = {APPLIED ENERGY}, volume = {285}, year = {2021}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2021.116452}, author = {Zhang, Liang and Wen, Jin and Li, Yanfei and Chen, Jianli and Ye, Yunyang and Fu, Yangyang and Livingood, William}, abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.} }
@article{WOS:000606751200009, title = {Comparative analysis of image classification algorithms based on traditional machine learning and deep learning}, journal = {PATTERN RECOGNITION LETTERS}, volume = {141}, pages = {61-67}, year = {2021}, issn = {0167-8655}, doi = {10.1016/j.patrec.2020.07.042}, author = {Wang, Pin and Fan, En and Wang, Peng}, abstract = {Image classification is a hot research topic in today's society and an important direction in the field of image processing research. SVM is a very powerful classification model in machine learning. CNN is a type of feedforward neural network that includes convolution calculation and has a deep structure. It is one of the representative algorithms of deep learning. Taking SVM and CNN as examples, this paper compares and analyzes the traditional machine learning and deep learning image classification algorithms. This study found that when using a large sample mnist dataset, the accuracy of SVM is 0.88 and the accuracy of CNN is 0.98; when using a small sample COREL1000 dataset, the accuracy of SVM is 0.86 and the accuracy of CNN is 0.83. The experimental results in this paper show that traditional machine learning has a better solution effect on small sample data sets, and deep learning framework has higher recognition accuracy on large sample data sets. (C) 2020 Published by Elsevier B.V.} }
@article{WOS:000663421300008, title = {Using machine learning approaches for multi-omics data analysis: A review}, journal = {BIOTECHNOLOGY ADVANCES}, volume = {49}, year = {2021}, issn = {0734-9750}, doi = {10.1016/j.biotechadv.2021.107739}, author = {Reel, Parminder S. and Reel, Smarti and Pearson, Ewan and Trucco, Emanuele and Jefferson, Emily}, abstract = {With the development of modern high-throughput omic measurement platforms, it has become essential for biomedical studies to undertake an integrative (combined) approach to fully utilise these data to gain insights into biological systems. Data from various omics sources such as genetics, proteomics, and metabolomics can be integrated to unravel the intricate working of systems biology using machine learning-based predictive algorithms. Machine learning methods offer novel techniques to integrate and analyse the various omics data enabling the discovery of new biomarkers. These biomarkers have the potential to help in accurate disease prediction, patient stratification and delivery of precision medicine. This review paper explores different integrative machine learning methods which have been used to provide an in-depth understanding of biological systems during normal physiological functioning and in the presence of a disease. It provides insight and recommendations for interdisciplinary professionals who envisage employing machine learning skills in multi-omics studies.} }
@article{WOS:000675035000001, title = {Machine learning for alloys}, journal = {NATURE REVIEWS MATERIALS}, volume = {6}, pages = {730-755}, year = {2021}, issn = {2058-8437}, doi = {10.1038/s41578-021-00340-w}, author = {Hart, Gus L. W. and Mueller, Tim and Toher, Cormac and Curtarolo, Stefano}, abstract = {Alloy modelling has a history of machine-learning-like approaches, preceding the tide of data-science-inspired work. The dawn of computational databases has made the integration of analysis, prediction and discovery the key theme in accelerated alloy research. Advances in machine-learning methods and enhanced data generation have created a fertile ground for computational materials science. Pairing machine learning and alloys has proven to be particularly instrumental in pushing progress in a wide variety of materials, including metallic glasses, high-entropy alloys, shape-memory alloys, magnets, superalloys, catalysts and structural materials. This Review examines the present state of machine-learning-driven alloy research, discusses the approaches and applications in the field and summarizes theoretical predictions and experimental validations. We foresee that the partnership between machine learning and alloys will lead to the design of new and improved systems. Machine learning is enabling a metallurgical renaissance. This Review discusses recent progress in representations, descriptors and interatomic potentials, overviewing metallic glasses, high-entropy alloys, superalloys and shape-memory alloys, magnets and catalysts, and the prediction of mechanical and thermal properties.} }
@article{WOS:000626617900002, title = {A Survey on Data Collection for Machine Learning: A Big Data-AI Integration Perspective}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {33}, pages = {1328-1347}, year = {2021}, issn = {1041-4347}, doi = {10.1109/TKDE.2019.2946162}, author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong}, abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.} }
@article{WOS:000658723000005, title = {Power of data in quantum machine learning}, journal = {NATURE COMMUNICATIONS}, volume = {12}, year = {2021}, doi = {10.1038/s41467-021-22539-9}, author = {Huang, Hsin-Yuan and Broughton, Michael and Mohseni, Masoud and Babbush, Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.}, abstract = {The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits. Expectations for quantum machine learning are high, but there is currently a lack of rigorous results on which scenarios would actually exhibit a quantum advantage. Here, the authors show how to tell, for a given dataset, whether a quantum model would give any prediction advantage over a classical one.} }
@article{WOS:000644444900006, title = {When Machine Learning Meets Privacy: A Survey and Outlook}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3436755}, author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai}, abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.} }
@article{WOS:000656549000001, title = {Best practices in machine learning for chemistry comment}, journal = {NATURE CHEMISTRY}, volume = {13}, pages = {505-508}, year = {2021}, issn = {1755-4330}, doi = {10.1038/s41557-021-00716-z}, author = {Artrith, Nongnuch and Butler, Keith T. and Coudert, Francois-Xavier and Han, Seungwu and Isayev, Olexandr and Jain, Anubhav and Walsh, Aron}, abstract = {Statistical tools based on machine learning are becoming integrated into chemistry research workflows. We discuss the elements necessary to train reliable, repeatable and reproducible models, and recommend a set of guidelines for machine learning reports.} }
@article{WOS:000628819200010, title = {Application of supervised machine learning paradigms in the prediction of petroleum reservoir properties: Comparative analysis of ANN and SVM models}, journal = {JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING}, volume = {200}, year = {2021}, issn = {0920-4105}, doi = {10.1016/j.petrol.2020.108182}, author = {Otchere, Daniel Asante and Ganat, Tarek Omar Arbi and Gholami, Raoof and Ridha, Syahrir}, abstract = {The advent of Artificial Intelligence (AI) in the petroleum industry has seen an increase in its use in exploration, development, production, reservoir engineering and management planning to accelerate decision making, reduce cost and time. Supervised machine learning has gained much popularity in establishing a relationship between complex non-linear datasets. This type of machine learning algorithm has showcased its superiority over petroleum engineering regression techniques in terms of prediction errors for high dimensional data, computational power and memory. This review focuses on the most widely used machine learning algorithm employed in the petroleum industry, the Artificial Neural Network (ANN) with different shallow models used in reservoir characterisation. The Support Vector Machine (SVM) and Relevant Vector Machine (RVM) has over the years emerged as competitive algorithms where in most cases based on this review it outperformed the ANN. This makes it preferable than the ANN when there are limited data sets. Finally, hybridisation of multiple algorithms methodologies also showed improved performance over singularly applied algorithms offering a pathway in improving reservoir characterisation based on supervised machine learning as future scope of work.} }
@article{WOS:000611850000003, title = {Machine learning for high performance organic solar cells: current scenario and future prospects}, journal = {ENERGY \\& ENVIRONMENTAL SCIENCE}, volume = {14}, pages = {90-105}, year = {2021}, issn = {1754-5692}, doi = {10.1039/d0ee02838j}, author = {Mahmood, Asif and Wang, Jin-Liang}, abstract = {Machine learning (ML) is a field of computer science that uses algorithms and techniques for automating solutions to complex problems that are hard to program using conventional programming methods. Owing to the chemical versatility of organic building blocks, a large number of organic semi-conductors have been used for organic solar cells. Selecting a suitable organic semi-conductor is like searching for a needle in a haystack. Data-driven science, the fourth paradigm of science, has the potential to guide experimentalists to discover and develop new high-performance materials. The last decade has seen impressive progress in materials informatics and data science; however, data-driven molecular design of organic solar cell materials is still challenging. The data-analysis capability of machine learning methods is well known. This review is written about the use of machine learning methods for organic solar cell research. In this review, we have outlined the basics of machine learning and common procedures for applying machine learning. A brief introduction on different classes of machine learning algorithms as well as related software and tools is provided. Then, the current research status of machine learning in organic solar cells is reviewed. We have discussed the challenges in anticipating the data driven material design, such as the complexity metric of organic solar cells, diversity of chemical structures and necessary programming ability. We have also proposed some suggestions that can enhance the usefulness of machine learning for organic solar cell research enterprises.} }
@article{WOS:000799950300002, title = {Machine learning in subsurface geothermal energy: Two decades in review}, journal = {GEOTHERMICS}, volume = {102}, year = {2022}, issn = {0375-6505}, doi = {10.1016/j.geothermics.2022.102401}, author = {Okoroafor, Esuru Rita and Smith, Connor M. and Ochie, Karen Ifeoma and Nwosu, Chinedu Joseph and Gudmundsdottir, Halldora and Aljubran, Mohammad (Jabs)}, abstract = {This paper reviews the trends in applying machine learning to subsurface geothermal resource development. The review is focused on the machine learning applications over the past two decades (from 2002 to 2021) to determine which machine learning algorithms are being used. In addition, the review seeks to determine what types of problems are being addressed with machine learning and how machine learning is aiding decisionmaking and problem-solving for subsurface aspects of the geothermal industry. The study shows that there has been a steady increase in the application of machine learning in the geothermal industry over the past 20 years, with an exponential increase in machine learning applications from 2018 to 2021. Several research areas associated with geothermal resource development were reviewed, including exploration, drilling, reservoir characterization, seismicity, petrophysics, reservoir engineering, and production and injection engineering. The study reveals that the field of reservoir characterization had the most significant applications of machine learning in the geothermal industry. Though machine learning has been applied across all the geothermal research areas we investigated, this study shows that there are still opportunities to improve and expand the adoption of machine learning in exploration, drilling, and seismicity. The main challenges that would need to be addressed are ensuring researchers have access to data, curating the data to be suitable for machine learning, and training geothermal industry students and professionals on artificial intelligence related to the energy sector.} }
@article{WOS:000930523100002, title = {Shear strength prediction of reinforced concrete beams using machine learning}, journal = {STRUCTURES}, volume = {47}, pages = {1196-1211}, year = {2023}, issn = {2352-0124}, doi = {10.1016/j.istruc.2022.11.140}, author = {Sandeep, M. S. and Tiprak, Koravith and Kaewunruen, Sakdirat and Pheinsusom, Phoonsak and Pansuk, Withit}, abstract = {Recent years have witnessed a surge in the application of machine learning techniques for solving hard to solve structural engineering problems. The application of machine learning can replace the use of empirical and semiempirical prediction models currently used in practice with highly accurate models. This paper provides a detailed discussion on the basic terminologies and concepts of commonly used machine learning algorithms for solving structural engineering problems. To provide confidence to use this method and show the potential of machine learning in accurately predicting the results of complex civil engineering problems, a comprehensive literature review on the application of machine learning in shear strength prediction is also presented. The literature review covers the application of different machine learning algorithms in predicting the shear strength of conventional concrete beams, steel fibre reinforced concrete beams, beams reinforced with FRP bars as well as high strength concrete beams. Major observations, challenges and future scope in this field are also discussed in detail. This article will be a valuable resource for individuals who are unfamiliar with machine learning yet aspire to learn more about it.} }
@article{WOS:000880854200004, title = {Using machine learning in photovoltaics to create smarter and cleaner energy generation systems: A comprehensive review}, journal = {JOURNAL OF CLEANER PRODUCTION}, volume = {364}, year = {2022}, issn = {0959-6526}, doi = {10.1016/j.jclepro.2022.132701}, author = {Sohani, Ali and Sayyaadi, Hoseyn and Cornaro, Cristina and Shahverdian, Mohammad Hassan and Pierro, Marco and Moser, David and Karimi, Nader and Doranehgard, Mohammad Hossein and Li, Larry K. B.}, abstract = {Photovoltaic (PV) technologies are expected to play an increasingly important role in future energy production. In parallel, machine learning has gained prominence because of a combination of factors such as advances in computational hardware, data collection and storage, and data-driven algorithms. Against this backdrop, we provide a comprehensive review of machine learning techniques applied to PV systems. First, conventional methods for modeling PV systems are introduced from both electrical and thermal perspectives. Then, the application of machine learning to the analysis of PV systems is discussed. We focus on reviewing the use of machine learning algorithms to predict performance and detect faults, and on discussing how machine learning can help humanity to achieve a cleaner environment in the worldwide drive towards carbon neutrality. This review also discusses the challenges to and future directions of using machine learning to analyze PV systems. A key conclusion is that the use of machine learning to analyze PV systems is still in its infancy, with many small-scale PV technologies, such as building integrated photovoltaic thermal systems (BIPV/T), not yet benefiting fully in terms of system efficiency and economic viability. The wider application of machine learning to PV systems could therefore forge a shorter path towards sustainable energy production.} }
@article{WOS:000803594800001, title = {Machine Learning and Application in Terahertz Technology: A Review on Achievements and Future Challenges}, journal = {IEEE ACCESS}, volume = {10}, pages = {53761-53776}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3174595}, author = {Jiang, Yuying and Li, Guangming and Ge, Hongyi and Wang, Faye and Li, Li and Chen, Xinyu and Lu, Ming and Zhang, Yuan}, abstract = {Terahertz (THz) radiation (0.1 similar to 10 THz) shows great potential in agricultural products detection, biomedical, and security inspection in recent years. Machine learning methods are widely used to support the user demand of higher efficiency and high prediction accuracy. The technological and key challenges of machine learning methods are for THz spectroscopy and image data preprocessing, reconstruction algorithms, and qualitative and quantitative analysis. In this paper, an exhaustive review of recent related works of THz detection and imaging techniques and machine learning methods are presented. The application of machine learning methods combined with THz technology in quality inspection of agricultural products, biomedical, security inspection, and materials science are highlighted. Challenges of machine learning methods for these applications are addressed. The development trend and future perspectives of THz technology are also discussed.} }
@article{WOS:000797748400001, title = {Machine learning and density functional theory}, journal = {NATURE REVIEWS PHYSICS}, volume = {4}, pages = {357-358}, year = {2022}, doi = {10.1038/s42254-022-00470-2}, author = {Pederson, Ryan and Kalita, Bhupalee and Burke, Kieron}, abstract = {Over the past decade machine learning has made significant advances in approximating density functionals, but whether this signals the end of human-designed functionals remains to be seen. Ryan Pederson, Bhupalee Kalita and Kieron Burke discuss the rise of machine learning for functional design.} }
@article{WOS:001466774000001, title = {Cybersecurity Threats and Their Mitigation Approaches Using Machine Learning-A Review}, journal = {JOURNAL OF CYBERSECURITY AND PRIVACY}, volume = {2}, pages = {527-555}, year = {2022}, doi = {10.3390/jcp2030027}, author = {Ahsan, Mostofa and Nygard, Kendall E. and Gomes, Rahul and Chowdhury, Md Minhaz and Rifat, Nafiz and Connolly, Jayden F.}, abstract = {Machine learning is of rising importance in cybersecurity. The primary objective of applying machine learning in cybersecurity is to make the process of malware detection more actionable, scalable and effective than traditional approaches, which require human intervention. The cybersecurity domain involves machine learning challenges that require efficient methodical and theoretical handling. Several machine learning and statistical methods, such as deep learning, support vector machines and Bayesian classification, among others, have proven effective in mitigating cyber-attacks. The detection of hidden trends and insights from network data and building of a corresponding data-driven machine learning model to prevent these attacks is vital to design intelligent security systems. In this survey, the focus is on the machine learning techniques that have been implemented on cybersecurity data to make these systems secure. Existing cybersecurity threats and how machine learning techniques have been used to mitigate these threats have been discussed. The shortcomings of these state-of-the-art models and how attack patterns have evolved over the past decade have also been presented. Our goal is to assess how effective these machine learning techniques are against the ever-increasing threat of malware that plagues our online community.} }
@article{WOS:000819919700004, title = {Choice modelling in the age of machine learning Discussion paper}, journal = {JOURNAL OF CHOICE MODELLING}, volume = {42}, year = {2022}, issn = {1755-5345}, doi = {10.1016/j.jocm.2021.100340}, author = {van Cranenburgh, Sander and Wang, Shenhao and Vij, Akshay and Pereira, Francisco and Walker, Joan}, abstract = {Since its inception, the choice modelling field has been dominated by theory-driven modelling approaches. Machine learning offers an alternative data-driven approach for modelling choice behaviour and is increasingly drawing interest in our field. Cross-pollination of machine learning models, techniques and practices could help overcome problems and limitations encountered in the current theory-driven modelling paradigm, such as subjective labour-intensive search processes for model selection, and the inability to work with text and image data. However, despite the potential benefits of using the advances of machine learning to improve choice modelling practices, the choice modelling field has been hesitant to embrace machine learning. This discussion paper aims to consolidate knowledge on the use of machine learning models, techniques and practices for choice modelling, and discuss their potential. Thereby, we hope not only to make the case that further integration of machine learning in choice modelling is beneficial, but also to further facilitate it. To this end, we clarify the similarities and differences between the two modelling paradigms; we review the use of machine learning for choice modelling; and we explore areas of opportunities for embracing machine learning models and techniques to improve our practices. To conclude this discussion paper, we put forward a set of research questions which must be addressed to better understand if and how machine learning can benefit choice modelling.} }
@article{WOS:000652706400001, title = {MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21062222}, author = {Kang, Jaeyong and Ullah, Zahid and Gwak, Jeonghwan}, abstract = {Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.} }
@article{WOS:000731150400004, title = {Machine learning techniques for analysis of hyperspectral images to determine quality of food products: A review}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {4}, pages = {28-44}, year = {2021}, doi = {10.1016/j.crfs.2021.01.002}, author = {Saha, Dhritiman and Manickavasagan, Annamalai}, abstract = {Non-destructive testing techniques have gained importance in monitoring food quality over the years. Hyperspectral imaging is one of the important non-destructive quality testing techniques which provides both spatial and spectral information. Advancement in machine learning techniques for rapid analysis with higher classification accuracy have improved the potential of using this technique for food applications. This paper provides an overview of the application of different machine learning techniques in analysis of hyperspectral images for determination of food quality. It covers the principle underlying hyperspectral imaging, the advantages, and the limitations of each machine learning technique. The machine learning techniques exhibited rapid analysis of hyperspectral images of food products with high accuracy thereby enabling robust classification or regression models. The selection of effective wavelengths from the hyperspectral data is of paramount importance since it greatly reduces the computational load and time which enhances the scope for real time applications. Due to the feature learning nature of deep learning, it is one of the most promising and powerful techniques for real time applications. However, the field of deep learning is relatively new and need further research for its full utilization. Similarly, lifelong machine learning paves the way for real time HSI applications but needs further research to incorporate the seasonal variations in food quality. Further, the research gaps in machine learning techniques for hyperspectral image analysis, and the prospects are discussed.} }
@article{WOS:000835498400003, title = {Data poisoning attacks against machine learning algorithms}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {208}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.118101}, author = {Yerlikaya, Fahri Anil and Bahtiyar, Serif}, abstract = {For the past decade, machine learning technology has increasingly become popular and it has been contributing to many areas that have the potential to influence the society considerably. Generally, machine learning is used by various industries to enhance their performances. Moreover, machine learning algorithms are used to solve some hard problems of systems that may contain very critical information. This makes machine learning algorithms a target of adversaries, which is an important problem for systems that use such algorithms. Therefore, it is significant to determine the performance and the robustness of a machine learning algorithm against attacks. In this paper, we analyze empirically the robustness and performances of six machine learning algorithms against two types of adversarial attacks by using four different datasets and three metrics. In our experiments, we analyze the robustness of Support Vector Machine, Stochastic Gradient Descent, Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest Neighbor algorithms to create learning models. We observe their performances in spam, botnet, malware, and cancer detection datasets when we launch adversarial attacks against these environments. We use data poisoning for manipulating training data during adversarial attacks, which are random label flipping and distance-based label flipping attacks. We analyze the performance of each algorithm for a specific dataset by modifying the amount of poisoned data and analyzing behaviors of accuracy rate, f1-score, and AUC score. Analyses results show that machine learning algorithms have various performance results and robustness under different adversarial attacks. Moreover, machine learning algorithms are affected differently in each stage of an adversarial attacks. Furthermore, the behavior of a machine learning algorithm highly depends on the type of the dataset. On the other hand, some machine learning algorithms have better robustness and performance results against adversarial attacks for almost all datasets.} }
@article{WOS:000799943600006, title = {A review on machine learning and deep learning for various antenna design applications}, journal = {HELIYON}, volume = {8}, year = {2022}, doi = {10.1016/j.heliyon.2022.e09317}, author = {Khan, Mohammad Monirujjaman and Hossain, Sazzad and Mozumdar, Puezia and Akter, Shamima and Ashique, Ratil H.}, abstract = {The next generation of wireless communication networks will rely heavily on machine learning and deep learning. In comparison to traditional ground-based systems, the development of various communication-based applications is projected to increase coverage and spectrum efficiency. Machine learning and deep learning can be used to optimize solutions in a variety of applications, including antennas. The latter have grown popular for obtaining effective solutions due to high computational processing, clean data, and large data storage capability. In this research, machine learning and deep learning for various antenna design applications have been discussed in detail. The general concept of machine learning and deep learning is introduced. However, the main focus is on various antenna applications, such as millimeter wave, body-centric, terahertz, satellite, unmanned aerial vehicle, global positioning system, and textiles. The feasibility of antenna applications with respect to conventional methods, acceleration of the antenna design process, reduced number of simulations, and better computational feasibility features are highlighted. Overall, machine learning and deep learning provide satisfactory results for antenna design.} }
@article{WOS:000880551800002, title = {Artificial intelligence and machine learning}, journal = {ELECTRONIC MARKETS}, volume = {32}, pages = {2235-2244}, year = {2022}, issn = {1019-6781}, doi = {10.1007/s12525-022-00598-0}, author = {Kuehl, Niklas and Schemmer, Max and Goutier, Marc and Satzger, Gerhard}, abstract = {Within the last decade, the application of ``artificial intelligence'' and ``machine learning'' has become popular across multiple disciplines, especially in information systems. The two terms are still used inconsistently in academia and industry-sometimes as synonyms, sometimes with different meanings. With this work, we try to clarify the relationship between these concepts. We review the relevant literature and develop a conceptual framework to specify the role of machine learning in building (artificial) intelligent agents. Additionally, we propose a consistent typology for AI-based information systems. We contribute to a deeper understanding of the nature of both concepts and to more terminological clarity and guidance-as a starting point for interdisciplinary discussions and future research.} }
@article{WOS:000818226100001, title = {Review on Interpretable Machine Learning in Smart Grid}, journal = {ENERGIES}, volume = {15}, year = {2022}, doi = {10.3390/en15124427}, author = {Xu, Chongchong and Liao, Zhicheng and Li, Chaojie and Zhou, Xiaojun and Xie, Renyou}, abstract = {In recent years, machine learning, especially deep learning, has developed rapidly and has shown remarkable performance in many tasks of the smart grid field. The representation ability of machine learning algorithms is greatly improved, but with the increase of model complexity, the interpretability of machine learning algorithms is worse. The smart grid is a critical infrastructure area, so machine learning models involving it must be interpretable in order to increase user trust and improve system reliability. Unfortunately, the black-box nature of most machine learning models remains unresolved, and many decisions of intelligent systems still lack explanation. In this paper, we elaborate on the definition, motivations, properties, and classification of interpretability. In addition, we review the relevant literature addressing interpretability for smart grid applications. Finally, we discuss the future research directions of interpretable machine learning in the smart grid.} }
@article{WOS:000762430500001, title = {Artificial intelligence and machine learning in emergency medicine: a narrative review}, journal = {ACUTE MEDICINE \\& SURGERY}, volume = {9}, year = {2022}, issn = {2052-8817}, doi = {10.1002/ams2.740}, author = {Mueller, Brianna and Kinoshita, Takahiro and Peebles, Alexander and Graber, Mark A. and Lee, Sangil}, abstract = {Aim: The emergence and evolution of artificial intelligence (AI) has generated increasing interest in machine learning applications for health care. Specifically, researchers are grasping the potential of machine learning solutions to enhance the quality of care in emergency medicine. Methods: We undertook a narrative review of published works on machine learning applications in emergency medicine and provide a synopsis of recent developments. Results: This review describes fundamental concepts of machine learning and presents clinical applications for triage, risk stratification specific to disease, medical imaging, and emergency department operations. Additionally, we consider how machine learning models could contribute to the improvement of causal inference in medicine, and to conclude, we discuss barriers to safe implementation of AI. Conclusion: We intend that this review serves as an introduction to AI and machine learning in emergency medicine.} }
@article{WOS:000787825500001, title = {Integration of machine learning and first principles models}, journal = {AICHE JOURNAL}, volume = {68}, year = {2022}, issn = {0001-1541}, doi = {10.1002/aic.17715}, author = {Rajulapati, Lokesh and Chinta, Sivadurgaprasad and Shyamala, Bala and Rengaswamy, Raghunathan}, abstract = {Model building and parameter estimation are traditional concepts widely used in chemical, biological, metallurgical, and manufacturing industries. Early modeling methodologies focused on mathematically capturing the process knowledge and domain expertise of the modeler. The models thus developed are termed first principles models (or white-box models). Over time, computational power became cheaper, and massive amounts of data became available for modeling. This led to the development of cutting edge machine learning models (black-box models) and artificial intelligence (AI) techniques. Hybrid models (gray-box models) are a combination of first principles and machine learning models. The development of hybrid models has captured the attention of researchers as this combines the best of both modeling paradigms. Recent attention to this field stems from the interest in explainable AI (XAI), a critical requirement as AI systems become more pervasive. This work aims at identifying and categorizing various hybrid models available in the literature that integrate machine-learning models with different forms of domain knowledge. Benefits such as enhanced predictive power, extrapolation capabilities, and other advantages of combining the two approaches are summarized. The goal of this article is to consolidate the published corpus in the area of hybrid modeling and develop a comprehensive framework to understand the various techniques presented. This framework can further be used as the foundation to explore rational associations between several models.} }
@article{WOS:001072225000001, title = {Machine learning-guided property prediction of energetic materials: Recent advances, challenges, and perspectives}, journal = {ENERGETIC MATERIALS FRONTIERS}, volume = {3}, pages = {177-186}, year = {2022}, doi = {10.1016/j.enmf.2022.07.005}, author = {Tian, Xiao-lan and Song, Si-wei and Chen, Fang and Qi, Xiu-juan and Wang, Yi and Zhang, Qing-hua}, abstract = {Predicting chemical properties is one of the most important applications of machine learning. In recent years, the prediction of the properties of energetic materials using machine learning has been receiving more attention. This review summarized recent advances in predicting energetic compounds' properties (e.g., density, detonation velocity, enthalpy of formation, sensitivity, the heat of the explosion, and decomposition temperature) using machine learning. Moreover, it presented general steps for applying machine learning to the prediction of practical chemical properties from the aspects of data, molecular representation, algorithms, and general accu-racy. Additionally, it raised some controversies specific to machine learning in energetic materials and its possible development directions. Machine learning is expected to become a new power for driving the development of energetic materials soon.} }
@article{WOS:000908283400019, title = {Exploring teachers' preconceptions of teaching machine learning in high school: A preliminary insight from Africa}, journal = {COMPUTERS AND EDUCATION OPEN}, volume = {3}, year = {2022}, issn = {2666-5573}, doi = {10.1016/j.caeo.2021.100072}, author = {Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday and Omidiora, Joseph Olamide}, abstract = {The teaching of machine learning is now considered essential and relevant in schools globally. Despite the ongoing discourse and increased research in the emerging field, teachers' conceptions of machine learning remain under-researched. This study aims at filling the gap by describing the initial conceptions of teaching machine learning by 12 African in-service teachers. We detailed the result of a phenomenographic analysis of teachers' pre-conceptions on teaching machine learning in K-12 settings. Twelve high school (Grades 10-12) computer science teachers in some selected African countries were recruited for a semi-structured interview. Five categories emerged from the analysis of the semi-structured interviews as follows: supporting student technical knowledge, having knowledge of the concept, focusing on professional development practices, contextualizing teaching resources and tools, and sustainability for development goals. These involve the relevance of teaching machine learning, the pedagogical approaches, strategies, and sustainability relating to practical implementation in schools. The results suggest the need to train in-service teachers to use existing tools designed for introducing machine learning. The teachers should also be involved in the co-designing process of resources considering contextual factors and, significantly, the curriculum to integrate machine learning into mainstream education. Involving teachers in the development process would help contextualize machine learning, contributing to real impact and societal changes.} }
@article{WOS:000833855900006, title = {A review of ultrasonic sensing and machine learning methods to monitor industrial processes}, journal = {ULTRASONICS}, volume = {124}, year = {2022}, issn = {0041-624X}, doi = {10.1016/j.ultras.2022.106776}, author = {Bowler, Alexander L. and Pound, Michael P. and Watson, Nicholas J.}, abstract = {Supervised machine learning techniques are increasingly being combined with ultrasonic sensor measurements owing to their strong performance. These techniques also offer advantages over calibration procedures of more complex fitting, improved generalisation, reduced development time, ability for continuous retraining, and the correlation of sensor data to important process information. However, their implementation requires expertise to extract and select appropriate features from the sensor measurements as model inputs, select the type of machine learning algorithm to use, and find a suitable set of model hyperparameters. The aim of this article is to facilitate implementation of machine learning techniques in combination with ultrasonic measurements for in-line and online monitoring of industrial processes and other similar applications. The article first reviews the use of ultrasonic sensors for monitoring processes, before reviewing the combination of ultrasonic measurements and machine learning. We include literature from other sectors such as structural health monitoring. This review covers feature extraction, feature selection, algorithm choice, hyperparameter selection, data augmentation, domain adaptation, semi-supervised learning and machine learning interpretability. Finally, recommendations for applying machine learning to the reviewed processes are made.} }
@article{WOS:000895081000015, title = {A Systematic Review of Machine Learning Techniques for GNSS Use Cases}, journal = {IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS}, volume = {58}, pages = {5043-5077}, year = {2022}, issn = {0018-9251}, doi = {10.1109/TAES.2022.3219366}, author = {Siemuri, Akpojoto and Selvan, Kannan and Kuusniemi, Heidi and Valisuo, Petri and Elmusrati, Mohammed S.}, abstract = {In terms of the availability and accuracy of positioning, navigation, and timing (PNT), the traditional Global Navigation Satellite System (GNSS) algorithms and models perform well under good signal conditions. In order to improve their robustness and performance in less than optimal signal environments, many researchers have proposed machine learning (ML) based GNSS models (ML models) as early as the 1990s. However, no study has been done in a systematic way to analyze the extent of the research on the utilization of ML models in GNSS and their performance. In this study, we perform a systematic review of studies from 2000 to 2021 in the literature that utilizes machine learning techniques in GNSS use cases. We assess the performance of the machine learning techniques in the existing literature on their application to GNSS. Furthermore, the strengths and weaknesses of machine learning techniques are summarized. In this paper, we have identified 213 selected studies and ten categories of machine learning techniques. The results prove the acceptable performance of machine learning techniques in several GNSS use cases. In most cases, the models using the machine learning techniques in these GNSS use cases outperform the traditional GNSS models. ML models are promising in their utilization in GNSS. However, the application of ML models in the industry is still limited. More effort and incentives are needed to facilitate the utilization of ML models in the PNT context. Therefore, based on the findings of this review, we provide recommendations for researchers and guidelines for practitioners.} }
@article{WOS:000741323700002, title = {Selecting an appropriate supervised machine learning algorithm for predictive maintenance}, journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY}, volume = {119}, pages = {4277-4301}, year = {2022}, issn = {0268-3768}, doi = {10.1007/s00170-021-08551-9}, author = {Ouadah, Abdelfettah and Zemmouchi-Ghomari, Leila and Salhi, Nedjma}, abstract = {Predictive maintenance refers to predicting malfunctions using data from monitoring equipment and process performance measurements. Machine learning algorithms and techniques are often used to analyze equipment monitoring data. Machine learning is the process in which a computer can work more precisely by collecting and analyzing data. It is often the case that machine learning algorithms use supervised learning, in which labelled data is used to feed the algorithm. However, there are many supervised machine learning algorithms available. Therefore, choosing the best-supervised machine learning algorithm to resolve predictive maintenance issues is not trivial. This paper aims to increase the performance of predictive maintenance and achieve its goals by selecting the most suitable supervised machine learning algorithm. Based on the most commonly used criteria in research articles, we selected three supervised machine learning algorithms from a comparative study: Random forest, Decision tree and KNN. We then tested selected algorithms on data from real-world and simulation scenarios. Finally, we conducted the experiment based on vibration analysis and reliability evaluation. We noticed that Random forests and Decision trees obtained slightly the same performance. KNN is a better classification algorithm for extensive volumes of data; on the contrary, Random forest performs better in the case of small datasets.} }
@article{WOS:000768294100005, title = {Conceptual challenges for interpretable machine learning}, journal = {SYNTHESE}, volume = {200}, year = {2022}, issn = {0039-7857}, doi = {10.1007/s11229-022-03485-5}, author = {Watson, David S.}, abstract = {As machine learning has gradually entered into ever more sectors of public and private life, there has been a growing demand for algorithmic explainability. How can we make the predictions of complex statistical models more intelligible to end users? A subdiscipline of computer science known as interpretable machine learning (IML) has emerged to address this urgent question. Numerous influential methods have been proposed, from local linear approximations to rule lists and counterfactuals. In this article, I highlight three conceptual challenges that are largely overlooked by authors in this area. I argue that the vast majority of IML algorithms are plagued by (1) ambiguity with respect to their true target; (2) a disregard for error rates and severe testing; and (3) an emphasis on product over process. Each point is developed at length, drawing on relevant debates in epistemology and philosophy of science. Examples and counterexamples from IML are considered, demonstrating how failure to acknowledge these problems can result in counterintuitive and potentially misleading explanations. Without greater care for the conceptual foundations of IML, future work in this area is doomed to repeat the same mistakes.} }
@article{WOS:000848617400011, title = {A Machine Learning Tutorial for Operational Meteorology. Part I: Traditional Machine Learning}, journal = {WEATHER AND FORECASTING}, volume = {37}, pages = {1509-1529}, year = {2022}, issn = {0882-8156}, doi = {10.1175/WAF-D-22-0070.1}, author = {Chase, Randy J. and Harrison, David R. and Burke, Amanda and Lackmann, Gary M. and McGovern, Amy}, abstract = {Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are ``black boxes'' and thus end-users are hesitant to apply the machine learning methods in their everyday workflow. To reduce the opaqueness of machine learning methods and lower hesitancy toward machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression, logistic regression, decision trees, random forest, gradient boosted decision trees, naive Bayes, and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyze the use of machine learning in meteorology.} }
@article{WOS:000855096700002, title = {Application of tabular data synthesis using generative adversarial networks on machine learning-based multiaxial fatigue life prediction}, journal = {INTERNATIONAL JOURNAL OF PRESSURE VESSELS AND PIPING}, volume = {199}, year = {2022}, issn = {0308-0161}, doi = {10.1016/j.ijpvp.2022.104779}, author = {He, GaoYuan and Zhao, YongXiang and Yan, ChuLiang}, abstract = {Machine learning has gradually developed into a new and effective scheme for fatigue life prediction. The novelty of this work is the proposal and verification of using virtual synthetic multiaxial fatigue data as input of machine learning models. First, the data generated by tabular generative adversarial networks are applied to machine learning models for life prediction. Then based on equivalent stress (strain) amplitude-life relationship curve, a multiaxial fatigue data generation evaluation metric is proposed. Finally, the effect of the generated sample size on the predictions of machine learning models is investigated. The method is demonstrated on 5 multiaxial fatigue data sets. The results indicate the synthetic data help machine learning models arrive at good life prediction ability. Using this method will help expand the application of machine learning-based multiaxial fatigue life prediction.} }
@article{WOS:000838619300001, title = {Quantum Machine Learning Applications in the Biomedical Domain: A Systematic Review}, journal = {IEEE ACCESS}, volume = {10}, pages = {80463-80484}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3195044}, author = {Maheshwari, Danyal and Garcia-Zapirain, Begonya and Sierra-Sosa, Daniel}, abstract = {Quantum technologies have become powerful tools for a wide range of application disciplines, which tend to range from chemistry to agriculture, natural language processing, and healthcare due to exponentially growing computational power and advancement in machine learning algorithms. Furthermore, the processing of classical data and machine learning algorithms in the quantum domain has given rise to an emerging field like quantum machine learning. Recently, quantum machine learning has become quite a challenging field in the case of healthcare applications. As a result, quantum machine learning has become a common and effective technique for data processing and classification across a wide range of domains. Consequently, quantum machine learning is the most commonly used application of quantum computing. The main objective of this work is to present a brief overview of current state-of-the-art published articles between 2013 and 2021 to identify, analyze, and classify the different QML algorithms and applications in the biomedical field. Furthermore, the approach adheres to the requirements for conducting systematic literature review techniques such as research questions and quality metrics of the articles. Initially, we discovered 3149 articles, excluded the 2847 papers, and read the 121 full papers. Therefore, this research compiled 30 articles that comply with the quantum machine learning models and quantum circuits using biomedical data. Eventually, this article provides a broad overview of quantum machine learning limitations and future prospects.} }
@article{WOS:000856097600001, title = {Quantum-Inspired Machine Learning for 6G: Fundamentals, Security, Resource Allocations, Challenges, and Future Research Directions}, journal = {IEEE OPEN JOURNAL OF VEHICULAR TECHNOLOGY}, volume = {3}, pages = {375-387}, year = {2022}, doi = {10.1109/OJVT.2022.3202876}, author = {Duong, Trung Q. and Ansere, James Adu and Narottama, Bhaskara and Sharma, Vishal and Dobre, Octavia A. and Shin, Hyundong}, abstract = {Quantum computing is envisaged as an evolving paradigm for solving computationally complex optimization problems with a large-number factorization and exhaustive search. Recently, there has been a proliferating growth of the size of multi-dimensional datasets, the input-output space dimensionality, and data structures. Hence, the conventional machine learning approaches in data training and processing have exhibited their limited computing capabilities to support the sixth-generation (6G) networks with highly dynamic applications and services. In this regard, the fast developing quantum computing with machine learning for 6G networks is investigated. Quantum machine learning algorithm can significantly enhance the processing efficiency and exponentially computational speed-up for effective quantum data representation and superposition framework, highly capable of guaranteeing high data storage and secured communications. We present the state-of-the-art in quantum computing and provide a comprehensive overview of its potential, via machine learning approaches. Furthermore, we introduce quantum-inspired machine learning applications for 6G networks in terms of resource allocation and network security, considering their enabling technologies and potential challenges. Finally, some dominating research issues and future research directions for the quantum-inspired machine learning in 6G networks are elaborated.} }
@article{WOS:000913916400001, title = {Machine learning in fermentative biohydrogen production: Advantages, challenges, and applications}, journal = {BIORESOURCE TECHNOLOGY}, volume = {370}, year = {2023}, issn = {0960-8524}, doi = {10.1016/j.biortech.2022.128502}, author = {Pandey, Ashutosh Kumar and Park, Jungsu and Ko, Jeun and Joo, Hwan-Hong and Raj, Tirath and Singh, Lalit Kumar and Singh, Noopur and Kim, Sang-Hyoun}, abstract = {Hydrogen can be produced in an environmentally friendly manner through biological processes using a variety of organic waste and biomass as feedstock. However, the complexity of biological processes limits their predictability and reliability, which hinders the scale-up and dissemination. This article reviews contemporary research and perspectives on the application of machine learning in biohydrogen production technology. Several machine learning algorithems have recently been implemented for modeling the nonlinear and complex relationships among operational and performance parameters in biohydrogen production as well as predicting the process performance and microbial population dynamics. Reinforced machine learning methods exhibited precise state prediction and retrieved the underlying kinetics effectively. Machine-learning based prediction was also improved by using microbial sequencing data as input parameters. Further research on machine learning could be instrumental in designing a process control tool to maintain reliable hydrogen production performance and identify connection between the process performance and the microbial population.} }
@article{WOS:000862449600001, title = {An introduction to machine learning for classification and prediction}, journal = {FAMILY PRACTICE}, year = {2022}, issn = {0263-2136}, doi = {10.1093/fampra/cmac104}, author = {Black, Jason E. and Kueper, Jacqueline K. and Williamson, Tyler S.}, abstract = {Classification and prediction tasks are common in health research. With the increasing availability of vast health data repositories (e.g. electronic medical record databases) and advances in computing power, traditional statistical approaches are being augmented or replaced with machine learning (ML) approaches to classify and predict health outcomes. ML describes the automated process of identifying (''learning'') patterns in data to perform tasks. Developing an ML model includes selecting between many ML models (e.g. decision trees, support vector machines, neural networks); model specifications such as hyperparameter tuning; and evaluation of model performance. This process is conducted repeatedly to find the model and corresponding specifications that optimize some measure of model performance. ML models can make more accurate classifications and predictions than their statistical counterparts and confer greater flexibility when modelling unstructured data or interactions between covariates; however, many ML models require larger sample sizes to achieve good classification or predictive performance and have been criticized as ``black box'' for their poor transparency and interpretability. ML holds potential in family medicine for risk profiling of patients' disease risk and clinical decision support to present additional information at times of uncertainty or high demand. In the future, ML approaches are positioned to become commonplace in family medicine. As such, it is important to understand the objectives that can be addressed using ML approaches and the associated techniques and limitations. This article provides a brief introduction into the use of ML approaches for classification and prediction tasks in family medicine.} }
@article{WOS:000847846800001, title = {Machine learning for energy-resource allocation, workflow scheduling and live migration in cloud computing: State-of-the-art survey}, journal = {SUSTAINABLE COMPUTING-INFORMATICS \\& SYSTEMS}, volume = {36}, year = {2022}, issn = {2210-5379}, doi = {10.1016/j.suscom.2022.100780}, author = {Kumar, Yogesh and Kaul, Surabhi and Hu, Yu-Chen}, abstract = {Machine learning and artificial intelligence techniques have been proven helpful when pragmatic to a wide range of complex problems and areas such as energy optimization, workflow scheduling, video gaming, and cloud computing. When machine learning and cloud computing algorithms are combined, they help achieve better outcomes by providing the improved performance of cloud data centers compared to solutions currently employed by various researchers. It is also helpful for migrating the virtual machines based on the current traffic condition and fluctuation due to network congestion and bandwidth availability. The survey aims to present the improvement in dynamic load allocation, task scheduling, energy optimization, live migration, mobile cloud computing, and security on the cloud using machine learning classification. Machine learning algorithms are prevailing analytical approaches that allow machines to identify patterns and simplify the human learning process. The flow of the paper consists of an introduction part, motivation, and background study, including a framework for cloud-machine learning integration, best practices of introducing machine learning in cloud computing, and the objective of the work. The paper also highlights the machine learning-based cloud services and the role of artificial intelligence in different cloud computing platforms. This comprehensive study provides mindfulness and valuable facilities to the researchers by giving thorough studies about various machine learning algorithms and their applicability in cloud computing.} }
@article{WOS:000747379200007, title = {Machine learning, artificial intelligence and the prediction of dementia}, journal = {CURRENT OPINION IN PSYCHIATRY}, volume = {35}, pages = {123-129}, year = {2022}, issn = {0951-7367}, doi = {10.1097/YCO.0000000000000768}, author = {Merkin, Alexander and Krishnamurthi, Rita and Medvedev, Oleg N.}, abstract = {Purpose of review Artificial intelligence and its division machine learning are emerging technologies that are increasingly applied in medicine. Artificial intelligence facilitates automatization of analytical modelling and contributes to prediction, diagnostics and treatment of diseases. This article presents an overview of the application of artificial intelligence in dementia research. Recent findings Machine learning and its branch Deep Learning are widely used in research to support in diagnosis and prediction of dementia. Deep Learning models in certain tasks often result in better accuracy of detection and prediction of dementia than traditional machine learning methods, but they are more costly in terms of run times and hardware requirements. Both machine learning and Deep Learning models have their own strengths and limitations. Currently, there are few datasets with limited data available to train machine learning models. There are very few commercial applications of machine learning in medical practice to date, mostly represented by mobile applications, which include questionnaires and psychometric assessments with limited machine learning data processing. Application of machine learning technologies in detection and prediction of dementia may provide an advantage to psychiatry and neurology by promoting a better understanding of the nature of the disease and more accurate evidence-based processes that are reproducible and standardized.} }
@article{WOS:000795191700002, title = {Dropout prediction in Moocs using deep learning and machine learning}, journal = {EDUCATION AND INFORMATION TECHNOLOGIES}, volume = {27}, pages = {11499-11513}, year = {2022}, issn = {1360-2357}, doi = {10.1007/s10639-022-11068-7}, author = {Basnet, Ram B. and Johnson, Clayton and Doleck, Tenzin}, abstract = {The nature of teaching and learning has evolved over the years, especially as technology has evolved. Innovative application of educational analytics has gained momentum. Indeed, predictive analytics have become increasingly salient in education. Considering the prevalence of learner-system interaction data and the potential value of such data, it is not surprising that significant scholarly attention has been directed at understanding ways of drawing insights from educational data. Although prior literature on educational big data recognizes the utility of deep learning and machine learning methods, little research examines both deep learning and machine learning together, and the differences in predictive performance have been relatively understudied. This paper aims to present a comprehensive comparison of predictive performance using deep learning and machine learning. Specifically, we use educational big data in the context of predicting dropout in MOOCs. We find that machine learning classifiers can predict equally well as deep learning classifiers. This research advances our understanding of the use of deep learning and machine learning in optimizing dropout prediction performance models.} }
@article{WOS:000887333600001, title = {Machine Learning Models for Data-Driven Prediction of Diabetes by Lifestyle Type}, journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH}, volume = {19}, year = {2022}, doi = {10.3390/ijerph192215027}, author = {Qin, Yifan and Wu, Jinlong and Xiao, Wen and Wang, Kun and Huang, Anbing and Liu, Bowen and Yu, Jingxuan and Li, Chuhao and Yu, Fengyu and Ren, Zhanbing}, abstract = {The prevalence of diabetes has been increasing in recent years, and previous research has found that machine-learning models are good diabetes prediction tools. The purpose of this study was to compare the efficacy of five different machine-learning models for diabetes prediction using lifestyle data from the National Health and Nutrition Examination Survey (NHANES) database. The 1999-2020 NHANES database yielded data on 17,833 individuals data based on demographic characteristics and lifestyle-related variables. To screen training data for machine models, the Akaike Information Criterion (AIC) forward propagation algorithm was utilized. For predicting diabetes, five machine-learning models (CATBoost, XGBoost, Random Forest (RF), Logistic Regression (LR), and Support Vector Machine (SVM)) were developed. Model performance was evaluated using accuracy, sensitivity, specificity, precision, F1 score, and receiver operating characteristic (ROC) curve. Among the five machine-learning models, the dietary intake levels of energy, carbohydrate, and fat, contributed the most to the prediction of diabetes patients. In terms of model performance, CATBoost ranks higher than RF, LG, XGBoost, and SVM. The best-performing machine-learning model among the five is CATBoost, which achieves an accuracy of 82.1\\% and an AUC of 0.83. Machine-learning models based on NHANES data can assist medical institutions in identifying diabetes patients.} }
@article{WOS:000792068500001, title = {Potential benefits and limitations of machine learning in the field of eating disorders: current research and future directions}, journal = {JOURNAL OF EATING DISORDERS}, volume = {10}, year = {2022}, issn = {2050-2974}, doi = {10.1186/s40337-022-00581-2}, author = {Fardouly, Jasmine and Crosby, Ross D. and Sukunesan, Suku}, abstract = {Plain English Summary Machine learning models are computer algorithms that learn from data to reach an optimal solution for a problem. These algorithms provide exciting potential for the accurate, accessible, and cost-effective early identification, prevention, and treatment of eating disorders, but this potential is just beginning to be explored. Research to date has mainly used machine learning to predict women's eating disorder status with relatively high levels of accuracy from responses to validated surveys, social media posts, or neuroimaging data. These studies show potential for the use of machine learning in the field, but we are far from using these methods in practice. Useful avenues for future research include the use of machine learning to personalise prevention and treatment options, provide ecological momentary interventions via smartphones, and to aid clinicians with their treatment fidelity and effectiveness. More research is needed with large samples of diverse participants to ensure that machine learning models are accurate, unbiased, and generalisable to all people with eating disorders. There are limitations and ethical considerations with using these methods in practice. If accurate and generalisable machine learning models can be created in the field of eating disorders, it could improve the way we identify, prevent, and treat these debilitating disorders. Advances in machine learning and digital data provide vast potential for mental health predictions. However, research using machine learning in the field of eating disorders is just beginning to emerge. This paper provides a narrative review of existing research and explores potential benefits, limitations, and ethical considerations of using machine learning to aid in the detection, prevention, and treatment of eating disorders. Current research primarily uses machine learning to predict eating disorder status from females' responses to validated surveys, social media posts, or neuroimaging data often with relatively high levels of accuracy. This early work provides evidence for the potential of machine learning to improve current eating disorder screening methods. However, the ability of these algorithms to generalise to other samples or be used on a mass scale is only beginning to be explored. One key benefit of machine learning over traditional statistical methods is the ability of machine learning to simultaneously examine large numbers (100s to 1000s) of multimodal predictors and their complex non-linear interactions, but few studies have explored this potential in the field of eating disorders. Machine learning is also being used to develop chatbots to provide psychoeducation and coping skills training around body image and eating disorders, with implications for early intervention. The use of machine learning to personalise treatment options, provide ecological momentary interventions, and aid the work of clinicians is also discussed. Machine learning provides vast potential for the accurate, rapid, and cost-effective detection, prevention, and treatment of eating disorders. More research is needed with large samples of diverse participants to ensure that machine learning models are accurate, unbiased, and generalisable to all people with eating disorders. There are important limitations and ethical considerations with utilising machine learning methods in practice. Thus, rather than a magical solution, machine learning should be seen as an important tool to aid the work of researchers, and eventually clinicians, in the early identification, prevention, and treatment of eating disorders.} }
@article{WOS:000795188600001, title = {Data pricing in machine learning pipelines}, journal = {KNOWLEDGE AND INFORMATION SYSTEMS}, volume = {64}, pages = {1417-1455}, year = {2022}, issn = {0219-1377}, doi = {10.1007/s10115-022-01679-4}, author = {Cong, Zicun and Luo, Xuan and Pei, Jian and Zhu, Feida and Zhang, Yong}, abstract = {Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data are critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.} }
@article{WOS:000993711000001, title = {Past, present, and future of the application of machine learning in cryptocurrency research}, journal = {RESEARCH IN INTERNATIONAL BUSINESS AND FINANCE}, volume = {63}, year = {2022}, issn = {0275-5319}, doi = {10.1016/j.ribaf.2022.101799}, author = {Ren, Yi-Shuai and Ma, Chao-Qun and Kong, Xiao-Lin and Baltas, Konstantinos and Zureigat, Qasim}, abstract = {Cryptocurrency has captured the interest of financial scholars and become a major research topic in blockchain. In cryptocurrency research, the use of machine learning algorithms is enabled by the presence of many types of data and abundant resources. However, there is currently no comprehensive review on cryptocurrencies using machine learning. Therefore, we collect papers on cryptocurrency-related using machine learning in the web of science database, and summarize these papers according to the algorithm, and draw the following conclusions: (1) The application of machine learning for cryptocurrencies research is increasing year over year; (2) Predicting cryptocurrency price trends and income fluctuations is the most relevant research topic; (3) The machine learning algorithm utilized in cryptocurrency research is not unique, and the practise of combining multiple machine learning approaches has emerged; (4) Concerns such as overfitting and interpretability still persist with machine learning methods. Finally, we suggest future research directions.} }
@article{WOS:000839022000001, title = {Improving Results of Existing Groundwater Numerical Models Using Machine Learning Techniques: A Review}, journal = {WATER}, volume = {14}, year = {2022}, doi = {10.3390/w14152307}, author = {Di Salvo, Cristina}, abstract = {This paper presents a review of papers specifically focused on the use of both numerical and machine learning methods for groundwater level modelling. In the reviewed papers, machine learning models (also called data-driven models) are used to improve the prediction or speed process of existing numerical modelling. When long runtimes inhibit the use of numerical models, machine learning models can be a valid alternative, capable of reducing the time for model development and calibration without sacrificing accuracy of detail in groundwater level forecasting. The results of this review highlight that machine learning models do not offer a complete representation of the physical system, such as flux estimates or total water balance and, thus, cannot be used to substitute numerical models in large study areas; however, they are affordable tools to improve predictions at specific observation wells. Numerical and machine learning models can be successfully used as complementary to each other as a powerful groundwater management tool. The machine learning techniques can be used to improve calibration of numerical models, whereas results of numerical models allow us to understand the physical system and select proper input variables for machine learning models. Machine learning models can be integrated in decision-making processes when rapid and effective solutions for groundwater management need to be considered. Finally, machine learning models are computationally efficient tools to correct head error prediction of numerical models.} }
@article{WOS:000741129900001, title = {Machine learning in the analysis of biomolecular simulations}, journal = {ADVANCES IN PHYSICS-X}, volume = {7}, year = {2022}, issn = {2374-6149}, doi = {10.1080/23746149.2021.2006080}, author = {Kaptan, Shreyas and Vattulainen, Ilpo}, abstract = {Machine learning has rapidly become a key method for the analysis and organization of large-scale data in all scientific disciplines. In life sciences, the use of machine learning techniques is a particularly appealing idea since the enormous capacity of computational infrastructures generates terabytes of data through millisecond simulations of atomistic and molecular-scale biomolecular systems. Due to this explosion of data, the automation, reproducibility, and objectivity provided by machine learning methods are highly desirable features in the analysis of complex systems. In this review, we focus on the use of machine learning in biomolecular simulations. We discuss the main categories of machine learning tasks, such as dimensionality reduction, clustering, regression, and classification used in the analysis of simulation data. We then introduce the most popular classes of techniques involved in these tasks for the purpose of enhanced sampling, coordinate discovery, and structure prediction. Whenever possible, we explain the scope and limitations of machine learning approaches, and we discuss examples of applications of these techniques.} }
@article{WOS:000844738900003, title = {Machine Learning in Chemoinformatics and Medicinal Chemistry}, journal = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE}, volume = {5}, pages = {43-65}, year = {2022}, issn = {2574-3414}, doi = {10.1146/annurev-biodatasci-122120-124216}, author = {Rodriguez-Perez, Raquel and Miljkovic, Filip and Bajorath, Juergen}, abstract = {In chemoinformatics and medicinal chemistry, machine learning has evolved into an important approach. In recent years, increasing computational resources and new deep learning algorithms have put machine learning onto a new level, addressing previously unmet challenges in pharmaceutical research. In silico approaches for compound activity predictions, de novo design, and reaction modeling have been further advanced by new algorithmic developments and the emergence of big data in the field. Herein, novel applications of machine learning and deep learning in chemoinformatics and medicinal chemistry are reviewed. Opportunities and challenges for new methods and applications are discussed, placing emphasis on proper baseline comparisons, robust validation methodologies, and new applicability domains.} }
@article{WOS:000867398400004, title = {Adversarial machine learning in IoT from an insider point of view}, journal = {JOURNAL OF INFORMATION SECURITY AND APPLICATIONS}, volume = {70}, year = {2022}, issn = {2214-2126}, doi = {10.1016/j.jisa.2022.103341}, author = {Aloraini, Fatimah and Javed, Amir and Rana, Omer and Burnap, Pete}, abstract = {With the rapid progress and significant successes in various applications, machine learning has been considered a crucial component in the Internet of Things ecosystem. However, machine learning models have recently been vulnerable to carefully crafted perturbations, so-called adversarial attacks. A capable insider adversary can subvert the machine learning model at either the training or testing phase, causing them to behave differently. The vulnerability of machine learning to adversarial attacks becomes one of the significant risks. Therefore, there is a need to secure machine learning models enabling the safe adoption in malicious insider cases. This paper reviews and organizes the body of knowledge in adversarial attacks and defense presented in IoT literature from an insider adversary point of view. We proposed a taxonomy of adversarial methods against machine learning models that an insider can exploit. Under the taxonomy, we discuss how these methods can be applied in real-life IoT applications. Finally, we explore defensive methods against adversarial attacks. We believe this can draw a comprehensive overview of the scattered research works to raise awareness of the existing insider threats landscape and encourages others to safeguard machine learning models against insider threats in the IoT ecosystem.} }
@article{WOS:000765072200001, title = {Transfer Learning for Radio Frequency Machine Learning: A Taxonomy and Survey}, journal = {SENSORS}, volume = {22}, year = {2022}, doi = {10.3390/s22041416}, author = {Wong, Lauren J. and Michaels, Alan J.}, abstract = {Transfer learning is a pervasive technology in computer vision and natural language processing fields, yielding exponential performance improvements by leveraging prior knowledge gained from data with different distributions. However, while recent works seek to mature machine learning and deep learning techniques in applications related to wireless communications, a field loosely termed radio frequency machine learning, few have demonstrated the use of transfer learning techniques for yielding performance gains, improved generalization, or to address concerns of training data costs. With modifications to existing transfer learning taxonomies constructed to support transfer learning in other modalities, this paper presents a tailored taxonomy for radio frequency applications, yielding a consistent framework that can be used to compare and contrast existing and future works. This work offers such a taxonomy, discusses the small body of existing works in transfer learning for radio frequency machine learning, and outlines directions where future research is needed to mature the field.} }
@article{WOS:000827662800001, title = {Toward a sociology of machine learning explainability: Human-machine interaction in deep neural network-based automated trading}, journal = {BIG DATA \\& SOCIETY}, volume = {9}, year = {2022}, issn = {2053-9517}, doi = {10.1177/20539517221111361}, author = {Borch, Christian and Hee Min, Bo}, abstract = {Machine learning systems are making considerable inroads in society owing to their ability to recognize and predict patterns. However, the decision-making logic of some widely used machine learning models, such as deep neural networks, is characterized by opacity, thereby rendering them exceedingly difficult for humans to understand and explain and, as a result, potentially risky to use. Considering the importance of addressing this opacity, this paper calls for research that studies empirically and theoretically how machine learning experts and users seek to attain machine learning explainability. Focusing on automated trading, we take steps in this direction by analyzing a trading firm's quest for explaining its deep neural network system's actionable predictions. We demonstrate that this explainability effort involves a particular form of human-machine interaction that contains both anthropomorphic and technomorphic elements. We discuss this attempt to attain machine learning explainability in light of reflections on cross-species companionship and consider it an example of human-machine companionship.} }
@article{WOS:000597401000025, title = {Landslide identification using machine learning}, journal = {GEOSCIENCE FRONTIERS}, volume = {12}, pages = {351-364}, year = {2021}, issn = {1674-9871}, doi = {10.1016/j.gsf.2020.02.012}, author = {Wang, Haojie and Zhang, Limin and Yin, Kesheng and Luo, Hongyu and Li, Jinhui}, abstract = {Landslide identification is critical for risk assessment and mitigation. This paper proposes a novel machine-learning and deep-learning method to identify natural-terrain landslides using integrated geodatabases. First, landslide-related data are compiled, including topographic data, geological data and rainfall-related data. Then, three integrated geodatabases are established; namely, Recent Landslide Database (RecLD), Relict Landslide Database (RelLD) and Joint Landslide Database (JLD). After that, five machine learning and deep learning algorithms, including logistic regression (LR), support vector machine (SVM), random forest (RF), boosting methods and convolutional neural network (CNN), are utilized and evaluated on each database. A case study in Lantau, Hong Kong, is conducted to demonstrate the application of the proposed method. From the results of the case study, CNN achieves an identification accuracy of 92.5\\% on RecLD, and outperforms other algorithms due to its strengths in feature extraction and multi dimensional data processing. Boosting methods come second in terms of accuracy, followed by RF, LR and SVM. By using machine learning and deep learning techniques, the proposed landslide identification method shows outstanding robustness and great potential in tackling the landslide identification problem.} }
@article{WOS:000864033400001, title = {Leveraging Theory for Enhanced Machine Learning}, journal = {ACS MACRO LETTERS}, year = {2022}, doi = {10.1021/acsmacrolett.2c00369}, author = {Audus, Debra J. and McDannald, Austin and DeCost, Brian}, abstract = {The application of machine learning to the materials domain has traditionally struggled with two major challenges: a lack of large, curated data sets and the need to understand the physics behind the machine-learning prediction. The former problem is particularly acute in the polymers domain. Here we aim to simultaneously tackle these challenges through the incorporation of scientific knowledge, thus, providing improved predictions for smaller data sets, both under interpolation and extrapolation, and a degree of explainability. We focus on imperfect theories, as they are often readily available and easier to interpret. Using a system of a polymer in different solvent qualities, we explore numerous methods for incorporating theory into machine learning using different machine-learning models, including Gaussian process regression. Ultimately, we find that encoding the functional form of the theory performs best followed by an encoding of the numeric values of the theory.} }
@article{WOS:000771334100001, title = {Special Issue: Geostatistics and Machine Learning}, journal = {MATHEMATICAL GEOSCIENCES}, volume = {54}, pages = {459-465}, year = {2022}, issn = {1874-8961}, doi = {10.1007/s11004-022-09998-6}, author = {De Iaco, Sandra and Hristopulos, Dionissios T. and Lin, Guang}, abstract = {Recent years have seen a steady growth in the number of papers that apply machine learning methods to problems in the earth sciences. Although they have different origins, machine learning and geostatistics share concepts and methods. For example, the kriging formalism can be cast in the machine learning framework of Gaussian process regression. Machine learning, with its focus on algorithms and ability to seek, identify, and exploit hidden structures in big data sets, is providing new tools for exploration and prediction in the earth sciences. Geostatistics, on the other hand, offers interpretable models of spatial (and spatiotemporal) dependence. This special issue on Geostatistics and Machine Learning aims to investigate applications of machine learning methods as well as hybrid approaches combining machine learning and geostatistics which advance our understanding and predictive ability of spatial processes.} }
@article{WOS:000907756500001, title = {Applying machine learning technologies to explore students' learning features and performance prediction}, journal = {FRONTIERS IN NEUROSCIENCE}, volume = {16}, year = {2022}, doi = {10.3389/fnins.2022.1018005}, author = {Su, Yu-Sheng and Lin, Yu-Da and Liu, Tai-Quan}, abstract = {To understand students' learning behaviors, this study uses machine learning technologies to analyze the data of interactive learning environments, and then predicts students' learning outcomes. This study adopted a variety of machine learning classification methods, quizzes, and programming system logs, found that students' learning characteristics were correlated with their learning performance when they encountered similar programming practice. In this study, we used random forest (RF), support vector machine (SVM), logistic regression (LR), and neural network (NN) algorithms to predict whether students would submit on time for the course. Among them, the NN algorithm showed the best prediction results. Education-related data can be predicted by machine learning techniques, and different machine learning models with different hyperparameters can be used to obtain better results.} }
@article{WOS:000910903500001, title = {A systematic literature review on the use of machine learning in code clone research}, journal = {COMPUTER SCIENCE REVIEW}, volume = {47}, year = {2023}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2022.100528}, author = {Kaur, Manpreet and Rattan, Dhavleesh}, abstract = {Context: Research related to code clones includes detection of clones in software systems, analysis, visualization and management of clones. Detection of semantic clones and management of clones have attracted use of machine learning techniques in code clone related research.Objective: The aim of this study is to report the extent of machine learning usage in code clone related research areas.Method: The paper uses a systematic review method to report the use of machine learning in research related to code clones. The study considers a comprehensive set of 57 articles published in leading conferences, workshops and journals.Results: Code clone related research using machine learning techniques is classified into different categories. Machine learning and deep learning algorithms used in the code clone research are reported. The datasets, features used to train machine learning models and metrics used to evaluate machine learning algorithms are reported. The comparative results of various machine learning algorithms presented in primary studies are reported.Conclusion: The research will help to identify the status of using machine learning in different code clone related research areas. We identify the need of more empirical studies to assess the benefits of machine learning in code clone research and give recommendations for future research.(c) 2022 Elsevier Inc. All rights reserved.} }
@article{WOS:000912711800001, title = {Sufficiency of Ensemble Machine Learning Methods for Phishing Websites Detection}, journal = {IEEE ACCESS}, volume = {10}, pages = {124103-124113}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3224781}, author = {Wei, Yi and Sekiya, Yuji}, abstract = {Phishing is a kind of worldwide spread cybercrime that uses disguised websites to trick users into downloading malware or providing personally sensitive information to attackers. With the rapid development of artificial intelligence, more and more researchers in the cybersecurity field utilize machine learning and deep learning algorithms to classify phishing websites. In order to compare the performances of various machine learning and deep learning methods, several experiments are conducted in this study. According to the experimental results, ensemble machine learning algorithms stand out among other candidates in both detection accuracy and computational consumption. Furthermore, the ensemble architectures still provide impressive capability when the amount of features decreases sharply in the dataset. Subsequently, the paper discusses the factors why ensemble machine learning methods are more suitable for the binary phishing classification challenge in up-date training and real-time detecting environment, which reflects the sufficiency of ensemble machine learning methods in anti-phishing techniques.} }
@article{WOS:000901956500001, title = {Review of Machine Learning Applications to the Modeling and Design Optimization of Switched Reluctance Motors}, journal = {IEEE ACCESS}, volume = {10}, pages = {130444-130468}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3229043}, author = {Omar, Mohamed and Sayed, Ehab and Abdalmagid, Mohamed and Bilgin, Berker and Bakr, Mohamed H. and Emadi, Ali}, abstract = {This work presents a comprehensive review of the developments in using Machine Learning (ML)-based algorithms for the modeling and design optimization of switched reluctance motors (SRMs). We reviewed Machine Learning-based numerical and analytical approaches used in modeling SRMs. We showed the difference between the supervised, unsupervised and reinforcement learning algorithms. More focus is placed on supervised learning algorithms as they are the most used algorithms in this area. The supervised learning algorithms studied in this work include the feedforward neural networks, recurrent neural networks, support vector machines, extreme learning machines, and Bayesian networks. This work also discusses several essential aspects of the considered machine learning algorithms, such as core concept, structure, and computational time. It also surveys sample data acquisition methods and data size. Finally, comparisons between the different considered ML-based algorithms are conducted in terms of electric motor type, dataset inputs and outputs, and algorithm's structure and accuracy to provide a summary overview of the ML-based algorithms for SRMs modeling and design.} }
@article{WOS:000431395700001, title = {Optimization Methods for Large-Scale Machine Learning}, journal = {SIAM REVIEW}, volume = {60}, pages = {223-311}, year = {2018}, issn = {0036-1445}, doi = {10.1137/16M1080173}, author = {Bottou, Leon and Curtis, Frank E. and Nocedal, Jorge}, abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.} }
@article{WOS:000517757300010, title = {Introduction to Machine Learning, Neural Networks, and Deep Learning}, journal = {TRANSLATIONAL VISION SCIENCE \\& TECHNOLOGY}, volume = {9}, year = {2020}, issn = {2164-2591}, doi = {10.1167/tvst.9.2.14}, author = {Choi, Rene Y. and Coyner, Aaron S. and Kalpathy-Cramer, Jayashree and Chiang, Michael F. and Campbell, J. Peter}, abstract = {Purpose: To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods: A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results: A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions: Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance: The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.} }
@article{WOS:000556899700018, title = {Machine learning methods for landslide susceptibility studies: A comparative overview of algorithm performance}, journal = {EARTH-SCIENCE REVIEWS}, volume = {207}, year = {2020}, issn = {0012-8252}, doi = {10.1016/j.earscirev.2020.103225}, author = {Merghadi, Abdelaziz and Yunus, Ali P. and Dou, Jie and Whiteley, Jim and Binh ThaiPham and Dieu Tien Bui and Avtar, Ram and Abderrahmane, Boumezbeur}, abstract = {Landslides are one of the catastrophic natural hazards that occur in mountainous areas, leading to loss of life, damage to properties, and economic disruption. Landslide susceptibility models prepared in a Geographic Information System (GIS) integrated environment can be key for formulating disaster prevention measures and mitigating future risk. The accuracy and precision of susceptibility models is evolving rapidly from opinion-driven models and statistical learning toward increased use of machine learning techniques. Critical reviews on opinion-driven models and statistical learning in landslide susceptibility mapping have been published, but an overview of current machine learning models for landslide susceptibility studies, including background information on their operation, implementation, and performance is currently lacking. Here, we present an overview of the most popular machine learning techniques available for landslide susceptibility studies. We find that only a handful of researchers use machine learning techniques in landslide susceptibility mapping studies. Therefore, we present the architecture of various Machine Learning (ML) algorithms in plain language, so as to be understandable to a broad range of geoscientists. Furthermore, a comprehensive study comparing the performance of various ML algorithms is absent from the current literature, making an assessment of comparative performance and predictive capabilities difficult. We therefore undertake an extensive analysis and comparison between different ML techniques using a case study from Algeria. We summarize and discuss the algorithm's accuracies, advantages and limitations using a range of evaluation criteria. We note that tree-based ensemble algorithms achieve excellent results compared to other machine learning algorithms and that the Random Forest algorithm offers robust performance for accurate landslide susceptibility mapping with only a small number of adjustments required before training the model.} }
@article{WOS:000546615200005, title = {Failure mode and effects analysis of RC members based on machine-learning-based SHapley Additive exPlanations (SHAP) approach}, journal = {ENGINEERING STRUCTURES}, volume = {219}, year = {2020}, issn = {0141-0296}, doi = {10.1016/j.engstruct.2020.110927}, author = {Mangalathu, Sujith and Hwang, Seong-Hoon and Jeon, Jong-Su}, abstract = {Machine learning approaches can establish the complex and non-linear relationship among input and response variables for the seismic damage assessment of structures. However, lack of explainability of complex machine learning models prevents their use in such assessment. This paper uses extensive experimental databases to suggest random forest machine learning models for failure mode predictions of reinforced concrete columns and shear walls, employs the recently developed SHapley Additive exPlanations approach to rank input variables for identification of failure modes, and explains why the machine learning model predicts a specific failure mode for a given sample or experiment. A random forest model established provides an accuracy of 84\\% and 86\\% for unknown data of columns and shear walls, respectively. The geometric variables and reinforcement indices are critical parameters that influence failure modes. The study also reveals that existing strategies of failure mode identification based solely on geometric features are not enough to properly identify failure modes.} }
@article{WOS:000566732800001, title = {Supervised Machine Learning: A Brief Primer}, journal = {BEHAVIOR THERAPY}, volume = {51}, pages = {675-687}, year = {2020}, issn = {0005-7894}, author = {Jiang, Tammy and Gradus, Jaimie L. and Rosellini, Anthony J.}, abstract = {Machine learning is increasingly used in mental health research and has the potential to advance our understanding of how to characterize, predict, and treat mental disorders and associated adverse health outcomes (e.g., suicidal behavior). Machine learning offers new tools to overcome challenges for which traditional statistical methods are not well-suited. This paper provides an overview of machine learning with a specific focus on supervised learning (i.e., methods that are designed to predict or classify an outcome of interest). Several common supervised learning methods are described, along with applied examples from the published literature. We also provide an overview of supervised learning model building, validation, and performance evaluation. Finally, challenges in creating robust and generalizable machine learning algorithms are discussed.} }
@article{WOS:000548811800021, title = {A Survey of Optimization Methods From a Machine Learning Perspective}, journal = {IEEE TRANSACTIONS ON CYBERNETICS}, volume = {50}, pages = {3668-3681}, year = {2020}, issn = {2168-2267}, doi = {10.1109/TCYB.2019.2950779}, author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing}, abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.} }
@article{WOS:000525389000018, title = {Explainable Machine Learning for Scientific Insights and Discoveries}, journal = {IEEE ACCESS}, volume = {8}, pages = {42200-42216}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2976199}, author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen}, abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.} }
@article{WOS:000582585700009, title = {A Survey on Distributed Machine Learning}, journal = {ACM COMPUTING SURVEYS}, volume = {53}, year = {2020}, issn = {0360-0300}, doi = {10.1145/3377454}, author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.}, abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.} }
@article{WOS:000509755200009, title = {A survey on machine learning for data fusion}, journal = {INFORMATION FUSION}, volume = {57}, pages = {115-129}, year = {2020}, issn = {1566-2535}, doi = {10.1016/j.inffus.2019.12.001}, author = {Meng, Tong and Jing, Xuyang and Yan, Zheng and Pedrycz, Witold}, abstract = {Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.} }
@article{WOS:000537804900016, title = {Automated machine learning: Review of the state-of-the-art and opportunities for healthcare}, journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE}, volume = {104}, year = {2020}, issn = {0933-3657}, doi = {10.1016/j.artmed.2020.101822}, author = {Waring, Jonathan and Lindvall, Charlotta and Umeton, Renato}, abstract = {Objective: This work aims to provide a review of the existing literature in the field of automated machine learning (AutoML) to help healthcare professionals better utilize machine learning models ``off-the-shelf'' with limited data science expertise. We also identify the potential opportunities and barriers to using AutoML in healthcare, as well as existing applications of AutoML in healthcare. Methods: Published papers, accompanied with code, describing work in the field of AutoML from both a computer science perspective or a biomedical informatics perspective were reviewed. We also provide a short summary of a series of AutoML challenges hosted by ChaLearn. Results: A review of 101 papers in the field of AutoML revealed that these automated techniques can match or improve upon expert human performance in certain machine learning tasks, often in a shorter amount of time. The main limitation of AutoML at this point is the ability to get these systems to work efficiently on a large scale, i.e. beyond small- and medium-size retrospective datasets. Discussion: The utilization of machine learning techniques has the demonstrated potential to improve health outcomes, cut healthcare costs, and advance clinical research. However, most hospitals are not currently deploying machine learning solutions. One reason for this is that health care professionals often lack the machine learning expertise that is necessary to build a successful model, deploy it in production, and integrate it with the clinical workflow. In order to make machine learning techniques easier to apply and to reduce the demand for human experts, automated machine learning (AutoML) has emerged as a growing field that seeks to automatically select, compose, and parametrize machine learning models, so as to achieve optimal performance on a given task and/or dataset. Conclusion: While there have already been some use cases of AutoML in the healthcare field, more work needs to be done in order for there to be widespread adoption of AutoML in healthcare.} }
@article{WOS:000523319000015, title = {How Machine Learning Will Transform Biomedicine}, journal = {CELL}, volume = {181}, pages = {92-101}, year = {2020}, issn = {0092-8674}, doi = {10.1016/j.cell.2020.03.022}, author = {Goecks, Jeremy and Jalili, Vahid and Heiser, Laura M. and Gray, Joe W.}, abstract = {This Perspective explores the application of machine learning toward improved diagnosis and treatment. We outline a vision for how machine learning can transform three broad areas of biomedicine: clinical diagnostics, precision treatments, and health monitoring, where the goal is to maintain health through a range of diseases and the normal aging process. For each area, early instances of successful machine learning applications are discussed, as well as opportunities and challenges for machine learning. When these challenges are met, machine learning promises a future of rigorous, outcomes-based medicine with detection, diagnosis, and treatment strategies that are continuously adapted to individual and environmental differences.} }
@article{WOS:000520831800008, title = {Machine Learning and Artificial Intelligence: Definitions, Applications, and Future Directions}, journal = {CURRENT REVIEWS IN MUSCULOSKELETAL MEDICINE}, volume = {13}, pages = {69-76}, year = {2020}, issn = {1935-973X}, doi = {10.1007/s12178-020-09600-8}, author = {Helm, J. Matthew and Swiergosz, Andrew M. and Haeberle, Heather S. and Karnuta, Jaret M. and Schaffer, Jonathan L. and Krebs, Viktor E. and Spitzer, I, Andrew and Ramkumar, Prem N.}, abstract = {Purpose of Review With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care. Recent Findings Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care.} }
@article{WOS:000577150900001, title = {Machine learning assisted materials design and discovery for rechargeable batteries}, journal = {ENERGY STORAGE MATERIALS}, volume = {31}, pages = {434-450}, year = {2020}, issn = {2405-8297}, doi = {10.1016/j.ensm.2020.06.033}, author = {Liu, Yue and Guo, Biru and Zou, Xinxin and Li, Yajie and Shi, Siqi}, abstract = {Machine learning plays an important role in accelerating the discovery and design process for novel electrochemical energy storage materials. This review aims to provide the state-of-the-art and prospects of machine learning for the design of rechargeable battery materials. After illustrating the key concepts of machine learning and basic procedures for applying machine learning in rechargeable battery materials science, we focus on how to obtain the most important features from the specific physical, chemical and/or other properties of material by using wrapper feature selection method, embedded feature selection method, and the combination of these two methods. And then, the applications of machine learning in rechargeable battery materials design and discovery are reviewed, including the property prediction for liquid electrolytes, solid electrolytes, electrode materials, and the discovery of novel rechargeable battery materials through component prediction and structure prediction. More importantly, we discuss the key challenges related to machine learning in rechargeable battery materials science, including the contradiction between high dimension and small sample, the conflict between the complexity and accuracy of machine learning models, and the inconsistency between learning results and domain expert knowledge. In response to these challenges, we propose possible countermeasures and forecast potential directions of future research. This review is expected to shed light on machine learning in rechargeable battery materials design and property optimization.} }
@article{WOS:000892292800005, title = {Equivalence of machine learning models in modeling chaos}, journal = {CHAOS SOLITONS \\& FRACTALS}, volume = {165}, year = {2022}, issn = {0960-0779}, doi = {10.1016/j.chaos.2022.112831}, author = {Chen, Xiaolu and Weng, Tongfeng and Li, Chunzi and Yang, Huijie}, abstract = {Recent advances have demonstrated that machine learning models are effective methods for predicting chaotic systems. Although short-term chaos prediction can be successfully realized by seemingly different machine learning models, an intriguing question of their correlation is still unknown. Here, we focus on three commonly used machine learning models that are reservoir computing, long-short term memory networks, and deep belief networks, respectively. We find that these selected models present almost identical long-term statistical properties as that of a learned chaotic system. Specifically, we show that these machine learning models have the same correlation dimension and recurrence time. Furthermore, by sharing a common signal, we realize synchronization, cascading synchronization, and coupled synchronization among machine learning models. Our findings reveal the equivalence of machine learning models in characterizing and modeling chaotic systems.} }
@article{WOS:000895564800001, title = {Human-in-the-loop machine learning with applications for population health}, journal = {CCF TRANSACTIONS ON PERVASIVE COMPUTING AND INTERACTION}, volume = {5}, pages = {1-12}, year = {2023}, issn = {2524-521X}, doi = {10.1007/s42486-022-00115-4}, author = {Chen, Long and Wang, Jiangtao and Guo, Bin and Chen, Liming}, abstract = {Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Next, a case study of our recent application study in human-in-the-loop machine learning for population health is introduced. Finally, we analyze current research gaps and point out future research directions.} }
@article{WOS:000873821800023, title = {Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?}, journal = {COMPUTER}, volume = {55}, pages = {94-99}, year = {2022}, issn = {0018-9162}, doi = {10.1109/MC.2022.3190787}, author = {Oprea, Alina and Singhal, Anoop and Vassilev, Apostol}, abstract = {Many practical applications benefit from machine learning and artificial intelligence technologies, but their security needs to be studied in more depth. We discuss the risk of poisoning attacks against the training stage of machine learning and challenges of defending against them.} }
@article{WOS:000605460600001, title = {Optimization problems for machine learning: A survey}, journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH}, volume = {290}, pages = {807-828}, year = {2021}, issn = {0377-2217}, doi = {10.1016/j.ejor.2020.08.045}, author = {Gambella, Claudio and Ghaddar, Bissan and Naoum-Sawaya, Joe}, abstract = {This paper surveys the machine learning literature and presents in an optimization framework several commonly used machine learning approaches. Particularly, mathematical optimization models are presented for regression, classification, clustering, deep learning, and adversarial learning, as well as new emerging applications in machine teaching, empirical model learning, and Bayesian network structure learning. Such models can benefit from the advancement of numerical optimization techniques which have already played a distinctive role in several machine learning settings. The strengths and the shortcomings of these models are discussed and potential research directions and open problems are highlighted. (C) 2020 Elsevier B.V. Allrights reserved.} }
@article{WOS:000643029400001, title = {Privacy Preserving Machine Learning with Homomorphic Encryption and Federated Learning}, journal = {FUTURE INTERNET}, volume = {13}, year = {2021}, issn = {1999-5903}, doi = {10.3390/fi13040094}, author = {Fang, Haokun and Qian, Quan}, abstract = {Privacy protection has been an important concern with the great success of machine learning. In this paper, it proposes a multi-party privacy preserving machine learning framework, named PFMLP, based on partially homomorphic encryption and federated learning. The core idea is all learning parties just transmitting the encrypted gradients by homomorphic encryption. From experiments, the model trained by PFMLP has almost the same accuracy, and the deviation is less than 1\\%. Considering the computational overhead of homomorphic encryption, we use an improved Paillier algorithm which can speed up the training by 25-28\\%. Moreover, comparisons on encryption key length, the learning network structure, number of learning clients, etc. are also discussed in detail in the paper.} }
@article{WOS:000744050600002, title = {Benchmark and Survey of Automated Machine Learning Frameworks}, journal = {JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH}, volume = {70}, pages = {409-472}, year = {2021}, issn = {1076-9757}, author = {Zoeller, Marc-Andre and Huber, Marco F.}, abstract = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.} }
@article{WOS:000652961400002, title = {Coronavirus disease (COVID-19) cases analysis using machine-learning applications}, journal = {APPLIED NANOSCIENCE}, year = {2021}, issn = {2190-5509}, doi = {10.1007/s13204-021-01868-7}, author = {Kwekha-Rashid, Ameer Sardar and Abduljabbar, Heamn N. and Alhayani, Bilal}, abstract = {Today world thinks about coronavirus disease that which means all even this pandemic disease is not unique. The purpose of this study is to detect the role of machine-learning applications and algorithms in investigating and various purposes that deals with COVID-19. Review of the studies that had been published during 2020 and were related to this topic by seeking in Science Direct, Springer, Hindawi, and MDPI using COVID-19, machine learning, supervised learning, and unsupervised learning as keywords. The total articles obtained were 16,306 overall but after limitation; only 14 researches of these articles were included in this study. Our findings show that machine learning can produce an important role in COVID-19 investigations, prediction, and discrimination. In conclusion, machine learning can be involved in the health provider programs and plans to assess and triage the COVID-19 cases. Supervised learning showed better results than other Unsupervised learning algorithms by having 92.9\\% testing accuracy. In the future recurrent supervised learning can be utilized for superior accuracy.} }
@article{WOS:000719871700005, title = {Machine Learning in Chemical Engineering: Strengths, Weaknesses, Opportunities, and Threats}, journal = {ENGINEERING}, volume = {7}, pages = {1201-1211}, year = {2021}, issn = {2095-8099}, doi = {10.1016/j.eng.2021.03.019}, author = {Dobbelaere, Maarten R. and Plehiers, Pieter P. and van de Vijver, Ruben and V. Stevens, Christian and Van Geem, Kevin M.}, abstract = {Chemical engineers rely on models for design, research, and daily decision-making, often with potentially large financial and safety implications. Previous efforts a few decades ago to combine artificial intelligence and chemical engineering for modeling were unable to fulfill the expectations. In the last five years, the increasing availability of data and computational resources has led to a resurgence in machine learning-based research. Many recent efforts have facilitated the roll-out of machine learning techniques in the research field by developing large databases, benchmarks, and representations for chemical applications and new machine learning frameworks. Machine learning has significant advantages over traditional modeling techniques, including flexibility, accuracy, and execution speed. These strengths also come with weaknesses, such as the lack of interpretability of these black-box models. The greatest opportunities involve using machine learning in time-limited applications such as real-time optimization and planning that require high accuracy and that can build on models with a self-learning ability to recognize patterns, learn from data, and become more intelligent over time. The greatest threat in artificial intelligence research today is inappropriate use because most chemical engineers have had limited training in computer science and data analysis. Nevertheless, machine learning will definitely become a trustworthy element in the modeling toolbox of chemical engineers. (C) 2021 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company.} }
@article{WOS:000662774900003, title = {Machine learning in construction: From shallow to deep learning}, journal = {DEVELOPMENTS IN THE BUILT ENVIRONMENT}, volume = {6}, year = {2021}, doi = {10.1016/j.dibe.2021.100045}, author = {Xu, Yayin and Zhou, Ying and Sekula, Przemyslaw and Ding, Lieyun}, abstract = {The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction ``smart''. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.} }
@article{WOS:000632782800001, title = {Reproducibility in machine learning for health research: Still a ways to go}, journal = {SCIENCE TRANSLATIONAL MEDICINE}, volume = {13}, year = {2021}, issn = {1946-6234}, doi = {10.1126/scitranslmed.abb1655}, author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Foschini, Luca and Ghassemi, Marzyeh}, abstract = {Machine learning for health must be reproducible to ensure reliable clinical use. We evaluated 511 scientific papers across several machine learning subfields and found that machine learning for health compared poorly to other areas regarding reproducibility metrics, such as dataset and code accessibility. We propose recommendations to address this problem.} }
@incollection{WOS:000652490700019, title = {Machine Learning for Social Science: An Agnostic Approach}, booktitle = {ANNUAL REVIEW OF POLITICAL SCIENCE, VOL 24, 2021}, volume = {24}, pages = {395-419}, year = {2021}, issn = {1094-2939}, doi = {10.1146/annurev-polisci-053119-015921}, author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.}, abstract = {Social scientists are now in an era of data abundance, and machine learning tools are increasingly used to extract meaning from data sets both massive and small. We explain how the inclusion of machine learning in the social sciences requires us to rethink not only applications of machine learning methods but also best practices in the social sciences. In contrast to the traditional tasks for machine learning in computer science and statistics, when machine learning is applied to social scientific data, it is used to discover new concepts, measure the prevalence of those concepts, assess causal effects, and make predictions. The abundance of data and resources facilitates the move away from a deductive social science to a more sequential, interactive, and ultimately inductive approach to inference. We explain how an agnostic approach to machine learning methods focused on the social science tasks facilitates progress across a wide range of questions.} }
@article{WOS:000655346100001, title = {Machine learning for hydrologic sciences: An introductory overview}, journal = {WILEY INTERDISCIPLINARY REVIEWS-WATER}, volume = {8}, year = {2021}, issn = {2049-1948}, doi = {10.1002/wat2.1533}, author = {Xu, Tianfang and Liang, Feng}, abstract = {The hydrologic community has experienced a surge in interest in machine learning in recent years. This interest is primarily driven by rapidly growing hydrologic data repositories, as well as success of machine learning in various academic and commercial applications, now possible due to increasing accessibility to enabling hardware and software. This overview is intended for readers new to the field of machine learning. It provides a non-technical introduction, placed within a historical context, to commonly used machine learning algorithms and deep learning architectures. Applications in hydrologic sciences are summarized next, with a focus on recent studies. They include the detection of patterns and events such as land use change, approximation of hydrologic variables and processes such as rainfall-runoff modeling, and mining relationships among variables for identifying controlling factors. The use of machine learning is also discussed in the context of integrated with process-based modeling for parameterization, surrogate modeling, and bias correction. Finally, the article highlights challenges of extrapolating robustness, physical interpretability, and small sample size in hydrologic applications.} }
@article{WOS:000953243000001, title = {Mitigating bias in machine learning for medicine}, journal = {COMMUNICATIONS MEDICINE}, volume = {1}, year = {2021}, issn = {2730-664X}, doi = {10.1038/s43856-021-00028-w}, author = {Vokinger, Kerstin N. and Feuerriegel, Stefan and Kesselheim, Aaron S.}, abstract = {Several sources of bias can affect the performance of machine learning systems used in medicine and potentially impact clinical care. Here, we discuss solutions to mitigate bias across the different development steps of machine learning-based systems for medical applications.} }
@article{WOS:000677963600002, title = {DOME: recommendations for supervised machine learning validation in biology}, journal = {NATURE METHODS}, volume = {18}, pages = {1122-1127}, year = {2021}, issn = {1548-7091}, doi = {10.1038/s41592-021-01205-4}, author = {Walsh, Ian and Fishman, Dmytro and Garcia-Gasulla, Dario and Titma, Tiina and Pollastri, Gianluca and Harrow, Jennifer and Psomopoulos, Fotis E. and Tosatto, Silvio C. E. and ELIXIR Machine Learning Focus Grp}, abstract = {DOME is a set of community-wide recommendations for reporting supervised machine learning-based analyses applied to biological studies. Broad adoption of these recommendations will help improve machine learning assessment and reproducibility.} }
@article{WOS:000684854500006, title = {Machine learning applications in microbial ecology, human microbiome studies, and environmental monitoring}, journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL}, volume = {19}, pages = {1092-1107}, year = {2021}, issn = {2001-0370}, doi = {10.1016/j.csbj.2021.01.028}, author = {Ghannam, Ryan B. and Techtmann, Stephen M.}, abstract = {Advances in nucleic acid sequencing technology have enabled expansion of our ability to profile microbial diversity. These large datasets of taxonomic and functional diversity are key to better understanding microbial ecology. Machine learning has proven to be a useful approach for analyzing microbial commu-nity data and making predictions about outcomes including human and environmental health. Machine learning applied to microbial community profiles has been used to predict disease states in human health, environmental quality and presence of contamination in the environment, and as trace evidence in forensics. Machine learning has appeal as a powerful tool that can provide deep insights into microbial communities and identify patterns in microbial community data. However, often machine learning models can be used as black boxes to predict a specific outcome, with little understanding of how the models arrived at predictions. Complex machine learning algorithms often may value higher accuracy and per-formance at the sacrifice of interpretability. In order to leverage machine learning into more translational research related to the microbiome and strengthen our ability to extract meaningful biological informa-tion, it is important for models to be interpretable. Here we review current trends in machine learning applications in microbial ecology as well as some of the important challenges and opportunities for more broad application of machine learning to understanding microbial communities. (C) 2021 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.} }
@article{WOS:000620625100026, title = {Review of machine learning methods in soft robotics}, journal = {PLOS ONE}, volume = {16}, year = {2021}, doi = {10.1371/journal.pone.0246102}, author = {Kim, Daekyum and Kim, Sang-Hun and Kim, Taekyoung and Kang, Brian Byunghyun and Lee, Minhyuk and Park, Wookeun and Ku, Subyeong and Kim, DongWook and Kwon, Junghan and Lee, Hochang and Bae, Joonbum and Park, Yong-Lae and Cho, Kyu-Jin and Jo, Sungho}, abstract = {Soft robots have been extensively researched due to their flexible, deformable, and adaptive characteristics. However, compared to rigid robots, soft robots have issues in modeling, calibration, and control in that the innate characteristics of the soft materials can cause complex behaviors due to non-linearity and hysteresis. To overcome these limitations, recent studies have applied various approaches based on machine learning. This paper presents existing machine learning techniques in the soft robotic fields and categorizes the implementation of machine learning approaches in different soft robotic applications, which include soft sensors, soft actuators, and applications such as soft wearable robots. An analysis of the trends of different machine learning approaches with respect to different types of soft robot applications is presented; in addition to the current limitations in the research field, followed by a summary of the existing machine learning methods for soft robots.} }
@article{WOS:000649692900007, title = {Application of machine learning in intelligent fish aquaculture: A review}, journal = {AQUACULTURE}, volume = {540}, year = {2021}, issn = {0044-8486}, doi = {10.1016/j.aquaculture.2021.736724}, author = {Zhao, Shili and Zhang, Song and Liu, Jincun and Wang, He and Zhu, Jia and Li, Daoliang and Zhao, Ran}, abstract = {Among the background of developments in automation and intelligence, machine learning technology has been extensively applied in aquaculture in recent years, providing a new opportunity for the realization of digital fishery farming. In the present paper, the machine learning algorithms and techniques adopted in intelligent fish aquaculture in the past five years are expounded, and the application of machine learning in aquaculture is explored in detail, including the information evaluation of fish biomass, the identification and classification of fish, behavioral analysis and prediction of water quality parameters. Further, the application of machine learning algorithms in aquaculture is outlined, and the results are analyzed. Finally, several current problems in aquaculture are highlighted, and the development trend is considered.} }
@article{WOS:001343358300003, title = {Machine learning in agriculture domain: A state-of-art survey}, journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES}, volume = {1}, year = {2021}, doi = {10.1016/j.ailsci.2021.100010}, author = {Meshram, Vishal and Patil, Kailas and Meshram, Vidula and Hanchate, Dinesh and Ramkteke, S. D.}, abstract = {Food is considered as a basic need of human being which can be satisfied through farming. Agriculture not only fulfills humans' basic needs, but also considered as source of employment worldwide. Agriculture is considered as a backbone of economy and source of employment in the developing countries like India. Agriculture contributes 15.4\\% in the GDP of India. Agriculture activities are broadly categorized into three major areas: pre-harvesting, harvesting and post harvesting. Advancement in area of machine learning has helped improving gains in agriculture. Machine learning is the current technology which is benefiting farmers to minimize the losses in the farming by providing rich recommendations and insights about the crops. This paper presents an extensive survey of latest machine learning application in agriculture to alleviate the problems in the three areas of pre-harvesting, harvesting and post-harvesting. Application of machine learning in agriculture allows more efficient and precise farming with less human manpower with high quality production.} }
@article{WOS:000605576700001, title = {Tackling Photonic Inverse Design with Machine Learning}, journal = {ADVANCED SCIENCE}, volume = {8}, year = {2021}, doi = {10.1002/advs.202002923}, author = {Liu, Zhaocheng and Zhu, Dayu and Raju, Lakshmi and Cai, Wenshan}, abstract = {Machine learning, as a study of algorithms that automate prediction and decision-making based on complex data, has become one of the most effective tools in the study of artificial intelligence. In recent years, scientific communities have been gradually merging data-driven approaches with research, enabling dramatic progress in revealing underlying mechanisms, predicting essential properties, and discovering unconventional phenomena. It is becoming an indispensable tool in the fields of, for instance, quantum physics, organic chemistry, and medical imaging. Very recently, machine learning has been adopted in the research of photonics and optics as an alternative approach to address the inverse design problem. In this report, the fast advances of machine-learning-enabled photonic design strategies in the past few years are summarized. In particular, deep learning methods, a subset of machine learning algorithms, dealing with intractable high degrees-of-freedom structure design are focused upon.} }
@article{WOS:000493720200024, title = {Definitions, methods, and applications in interpretable machine learning}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {116}, pages = {22071-22080}, year = {2019}, issn = {0027-8424}, doi = {10.1073/pnas.1900654116}, author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin}, abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.} }
@article{WOS:000688449200019, title = {Comprehensive Survey on Machine Learning in Vehicular Network: Technology, Applications and Challenges}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {23}, pages = {2027-2057}, year = {2021}, doi = {10.1109/COMST.2021.3089688}, author = {Tang, Fengxiao and Mao, Bomin and Kato, Nei and Gui, Guan}, abstract = {Towards future intelligent vehicular network, the machine learning as the promising artificial intelligence tool is widely researched to intelligentize communication and networking functions. In this paper, we provide a comprehensive survey on various machine learning techniques applied to both communication and network parts in vehicular network. To benefit reading, we first give a preliminary on communication technologies and machine learning technologies in vehicular network. Then, we detailedly describe the challenges of conventional techniques in vehicular network and corresponding machine learning based solutions. Finally, we present several open issues and emphasize potential directions that are worthy of research for the future intelligent vehicular network.} }
@article{WOS:000702995500001, title = {Innovative Materials Science via Machine Learning}, journal = {ADVANCED FUNCTIONAL MATERIALS}, volume = {32}, year = {2022}, issn = {1616-301X}, doi = {10.1002/adfm.202108044}, author = {Gao, Chaochao and Min, Xin and Fang, Minghao and Tao, Tianyi and Zheng, Xiaohong and Liu, Yangai and Wu, Xiaowen and Huang, Zhaohui}, abstract = {Nowadays, the research on materials science is rapidly entering a phase of data-driven age. Machine learning, one of the most powerful data-driven methods, have been being applied to materials discovery and performances prediction with undoubtedly tremendous application foreground. Herein, the challenges and current progress of machine learning are summarized in materials science, the design strategies are classified and highlighted, and possible perspectives are proposed for the future development. It is hoped this review can provide important scientific guidance for innovating materials science and technology via machine learning in the future.} }
@article{WOS:000660500300002, title = {Enhancing gravitational-wave science with machine learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abb93a}, author = {Cuoco, Elena and Powell, Jade and Cavaglia, Marco and Ackley, Kendall and Bejger, Michal and Chatterjee, Chayan and Coughlin, Michael and Coughlin, Scott and Easter, Paul and Essick, Reed and Gabbard, Hunter and Gebhard, Timothy and Ghosh, Shaon and Haegel, Leila and Iess, Alberto and Keitel, David and Marka, Zsuzsa and Marka, Szabolcs and Morawski, Filip and Nguyen, Tri and Ormiston, Rich and Puerrer, Michael and Razzano, Massimiliano and Staats, Kai and Vajente, Gabriele and Williams, Daniel}, abstract = {Machine learning has emerged as a popular and powerful approach for solving problems in astrophysics. We review applications of machine learning techniques for the analysis of ground-based gravitational-wave (GW) detector data. Examples include techniques for improving the sensitivity of Advanced Laser Interferometer GW Observatory and Advanced Virgo GW searches, methods for fast measurements of the astrophysical parameters of GW sources, and algorithms for reduction and characterization of non-astrophysical detector noise. These applications demonstrate how machine learning techniques may be harnessed to enhance the science that is possible with current and future GW detectors.} }
@article{WOS:000517788300001, title = {An Introduction to Machine Learning}, journal = {CLINICAL PHARMACOLOGY \\& THERAPEUTICS}, volume = {107}, pages = {871-885}, year = {2020}, issn = {0009-9236}, doi = {10.1002/cpt.1796}, author = {Badillo, Solveig and Banfai, Balazs and Birzele, Fabian and Davydov, Iakov I. and Hutchinson, Lucy and Kam-Thong, Tony and Siebourg-Polster, Juliane and Steiert, Bernhard and Zhang, Jitao David}, abstract = {In the last few years, machine learning (ML) and artificial intelligence have seen a new wave of publicity fueled by the huge and ever-increasing amount of data and computational power as well as the discovery of improved learning algorithms. However, the idea of a computer learning some abstract concept from data and applying them to yet unseen situations is not new and has been around at least since the 1950s. Many of these basic principles are very familiar to the pharmacometrics and clinical pharmacology community. In this paper, we want to introduce the foundational ideas of ML to this community such that readers obtain the essential tools they need to understand publications on the topic. Although we will not go into the very details and theoretical background, we aim to point readers to relevant literature and put applications of ML in molecular biology as well as the fields of pharmacometrics and clinical pharmacology into perspective.} }
@article{WOS:000921883400006, title = {Machine Learning, Functions and Goals}, journal = {CROATIAN JOURNAL OF PHILOSOPHY}, volume = {22}, pages = {351-370}, year = {2022}, issn = {1333-1108}, doi = {10.52685/cjp.22.66.5}, author = {Butlin, Patrick}, abstract = {Machine learning researchers distinguish between reinforcement learning and supervised learning and refer to reinforcement learning systems as ``agents''. This paper vindicates the claim that systems trained by reinforcement learning are agents while those trained by supervised learning are not. Systems of both kinds satisfy Dretske's criteria for agency, because they both learn to produce outputs selectively in response to inputs. However, reinforcement learning is sensitive to the instrumental value of outputs, giving rise to systems which exploit the effects of outputs on subsequent inputs to achieve good performance over episodes of interaction with their environments. Supervised learning systems, in contrast, merely learn to produce better outputs in response to individual inputs.} }
@article{WOS:000663500200010, title = {Machine learning algorithms for social media analysis: A survey}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100395}, author = {Balaji, T. K. and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree}, abstract = {Social Media (SM) are the most widespread and rapid data generation applications on the Internet increase the study of these data. However, the efficient processing of such massive data is challenging, so we require a system that learns from these data, like machine learning. Machine learning methods make the systems to learn itself. Many papers are published on SM using machine learning approaches over the past few decades. In this paper, we provide a comprehensive survey of multiple applications of SM analysis using robust machine learning algorithms. Initially, we discuss a summary of machine learning algorithms, which are used in SM analysis. After that, we provide a detailed survey of machine learning approaches to SM analysis. Furthermore, we summarize the challenges and benefits of Machine Learning usages in SM analysis. Finally, we presented open issues and consequences in SM analysis for further research. (c) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000751704800103, title = {A Survey of Topological Machine Learning Methods}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {4}, year = {2021}, doi = {10.3389/frai.2021.681108}, author = {Hensel, Felix and Moor, Michael and Rieck, Bastian}, abstract = {The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as ``topological machine learning,'' i.e., the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.} }
@article{WOS:000719226000003, title = {Machine-learning interpretability techniques for seismic performance assessment of infrastructure systems}, journal = {ENGINEERING STRUCTURES}, volume = {250}, year = {2022}, issn = {0141-0296}, doi = {10.1016/j.engstruct.2021.112883}, author = {Mangalathu, Sujith and Karthikeyan, Karthika and Feng, De-Cheng and Jeon, Jong-Su}, abstract = {Machine-learning has recently gained considerable attention in the earthquake engineering community, as it can map the complex relationship between the expected damage and the input parameters. It is often necessary to understand the reasons for the behavior and predictions of the machine-learning model. This paper addresses this issue through interpretable machine-learning approaches such as partial dependence plots, accumulated local effects, and Shapely additive explanations. The evaluation of these approaches is carried out (1) at a component level by analyzing the shear strength predictions by a machine-learning model and (2) at a regional level through the machine-learning model for the regional damage assessment of bridges in California. The comparison helps to identify (1) the proper implementation of these approaches for the efficient use of machine-learning models and (2) key influential variables and thresholds that govern the prediction of the machine-learning models.} }
@article{WOS:000656789000001, title = {Role of machine learning in medical research: A survey}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100370}, author = {Garg, Arunim and Mago, Vijay}, abstract = {Machine learning is one of the essential and effective tools in analyzing highly complex medical data. With vast amounts of medical data being generated, there is an urgent need to effectively use this data to benefit the medical and health care sectors all across the world. This survey paper presents a systematic literature review for the investigation of various machine learning techniques used for numerous medical applications which are published in highly reputable venues in recent years. Considering only the recent work, we are able to survey the current machine learning and deep learning models that are being used for medical data. This literature review identifies a clear shift of artificial intelligence techniques used in the medical domain, with deep learning methods taking precedence over machine learning methods. (C) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000656859400017, title = {Machine learning for biochemical engineering: A review}, journal = {BIOCHEMICAL ENGINEERING JOURNAL}, volume = {172}, year = {2021}, issn = {1369-703X}, doi = {10.1016/j.bej.2021.108054}, author = {Mowbray, Max and Savage, Thomas and Wu, Chufan and Song, Ziqi and Cho, Bovinille Anye and Del Rio-Chanona, Ehecatl A. and Zhang, Dongda}, abstract = {The field of machine learning is comprised of techniques, which have proven powerful approaches to knowledge discovery and construction of `digital twins' in the highly dimensional, nonlinear and stochastic domains common to biochemical engineering. We review the use of machine learning within biochemical engineering over the last 20 years. The most prevalent machine learning methods are demystified, and their impact across individual biochemical engineering subfields is outlined. In doing so we provide insights into the true benefits of each technique, and obstacles for their wider deployment. Finally, core challenges into the application of machine learning in biochemical engineering are thoroughly discussed, and further insight into adoption of innovative hybrid modelling and transfer learning strategies for development of new digital biotechnologies is provided.} }
@article{WOS:000664988000001, title = {Machine Learning-Reinforced Noninvasive Biosensors for Healthcare}, journal = {ADVANCED HEALTHCARE MATERIALS}, volume = {10}, year = {2021}, issn = {2192-2640}, doi = {10.1002/adhm.202100734}, author = {Zhang, Kaiyi and Wang, Jianwu and Liu, Tianyi and Luo, Yifei and Loh, Xian Jun and Chen, Xiaodong}, abstract = {The emergence and development of noninvasive biosensors largely facilitate the collection of physiological signals and the processing of health-related data. The utilization of appropriate machine learning algorithms improves the accuracy and efficiency of biosensors. Machine learning-reinforced biosensors are started to use in clinical practice, health monitoring, and food safety, bringing a digital revolution in healthcare. Herein, the recent advances in machine learning-reinforced noninvasive biosensors applied in healthcare are summarized. First, different types of noninvasive biosensors and physiological signals collected are categorized and summarized. Then machine learning algorithms adopted in subsequent data processing are introduced and their practical applications in biosensors are reviewed. Finally, the challenges faced by machine learning-reinforced biosensors are raised, including data privacy and adaptive learning capability, and their prospects in real-time monitoring, out-of-clinic diagnosis, and onsite food safety detection are proposed.} }
@article{WOS:000624582400019, title = {A Review of Using Machine Learning Approaches for Precision Education}, journal = {EDUCATIONAL TECHNOLOGY \\& SOCIETY}, volume = {24}, pages = {250-266}, year = {2021}, issn = {1176-3647}, author = {Luan, Hui and Tsai, Chin-Chung}, abstract = {In recent years, in the field of education, there has been a clear progressive trend toward precision education. As a rapidly evolving AI technique, machine learning is viewed as an important means to realize it. In this paper, we systematically review 40 empirical studies regarding machine-learning-based precision education. The results showed that the majority of studies focused on the prediction of learning performance or dropouts, and were carried out in online or blended learning environments among university students majoring in computer science or STEM, whereas the data sources were divergent. The commonly used machine learning algorithms, evaluation methods, and validation approaches are presented. The emerging issues and future directions are discussed accordingly.} }
@article{WOS:000709474600010, title = {Machine learning and applications in microbiology}, journal = {FEMS MICROBIOLOGY REVIEWS}, volume = {45}, year = {2021}, issn = {0168-6445}, doi = {10.1093/femsre/fuab015}, author = {Goodswen, Stephen J. and Barratt, Joel L. N. and Kennedy, Paul J. and Kaufer, Alexa and Calarco, Larissa and Ellis, John T.}, abstract = {To understand the intricacies of microorganisms at the molecular level requires making sense of copious volumes of data such that it may now be humanly impossible to detect insightful data patterns without an artificial intelligence application called machine learning. Applying machine learning to address biological problems is expected to grow at an unprecedented rate, yet it is perceived by the uninitiated as a mysterious and daunting entity entrusted to the domain of mathematicians and computer scientists. The aim of this review is to identify key points required to start the journey of becoming an effective machine learning practitioner. These key points are further reinforced with an evaluation of how machine learning has been applied so far in a broad scope of real-life microbiology examples. This includes predicting drug targets or vaccine candidates, diagnosing microorganisms causing infectious diseases, classifying drug resistance against antimicrobial medicines, predicting disease outbreaks and exploring microbial interactions. Our hope is to inspire microbiologists and other related researchers to join the emerging machine learning revolution.} }
@article{WOS:000646865900001, title = {Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology}, journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION}, volume = {3}, pages = {392-413}, year = {2021}, doi = {10.3390/make3020020}, author = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and Mueller, Klaus-Robert}, abstract = {Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.} }
@article{WOS:000696667700006, title = {How does Machine Learning Change Software Development Practices?}, journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING}, volume = {47}, pages = {1857-1871}, year = {2021}, issn = {0098-5589}, doi = {10.1109/TSE.2019.2937083}, author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.}, abstract = {Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.} }
@article{WOS:000628641300001, title = {A Survey of Machine Learning Techniques for Indoor Localization and Navigation Systems}, journal = {JOURNAL OF INTELLIGENT \\& ROBOTIC SYSTEMS}, volume = {101}, year = {2021}, issn = {0921-0296}, doi = {10.1007/s10846-021-01327-z}, author = {Roy, Priya and Chowdhury, Chandreyee}, abstract = {In the recent past, we have witnessed the adoption of different machine learning techniques for indoor positioning applications using WiFi, Bluetooth and other technologies. The techniques range from heuristically derived hand-crafted feature-based traditional machine learning algorithms, feature selection algorithms to the hierarchically self-evolving feature-based Deep Learning algorithms. The transient and chaotic nature of the WiFi/Bluetooth fingerprint data along with different signal sensitivity of different device configurations presents numerous challenges that influence the performance of the indoor localization system in the wild. This article is intended to offer a comprehensive state-of-the-art survey on machine learning techniques that have recently been adopted for localization purposes. Hence, we review the applicability of machine learning techniques in this domain along with basic localization principles, applications, and the underlying problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art machine learning techniques to visualize the possible future directions in the research field of indoor localization.} }
@article{WOS:000701765000004, title = {Machine learning applied to the design and inspection of reinforced concrete bridges: Resilient methods and emerging applications}, journal = {STRUCTURES}, volume = {33}, pages = {3954-3963}, year = {2021}, issn = {2352-0124}, doi = {10.1016/j.istruc.2021.06.110}, author = {Fan, Weiying and Chen, Yao and Li, Jiaqiang and Sun, Yue and Feng, Jian and Hassanin, Hany and Sareh, Pooya}, abstract = {Machine learning is one of the key pillars of industry 4.0 that has enabled rapid technological advancement through establishing complex connections among heterogeneous and highly complex engineering data automatically. Once the machine learning model is trained appropriately, it becomes able to effectively predict and make decisions. The technology is rapidly evolving and has found numerous applications in various branches of engineering due to its preponderance. This study is focused on exploring the recent advances of machine learning and its applications in reinforced concrete bridges. It covers a range of different machine learning techniques exploited in structural design, construction quality management, bridge engineering, and the inspection of reinforced concrete bridges. This review demonstrated that machine learning algorithms have established new research directions in bridge engineering, in particular for applications such as the form-finding of innovative long-span structures, structural reinforcement, and structural optimization.} }
@article{WOS:000723910100001, title = {Machine Learning (ML) in Medicine: Review, Applications, and Challenges}, journal = {MATHEMATICS}, volume = {9}, year = {2021}, doi = {10.3390/math9222970}, author = {Rahmani, Amir Masoud and Yousefpoor, Efat and Yousefpoor, Mohammad Sadegh and Mehmood, Zahid and Haider, Amir and Hosseinzadeh, Mehdi and Ali Naqvi, Rizwan}, abstract = {Today, artificial intelligence (AI) and machine learning (ML) have dramatically advanced in various industries, especially medicine. AI describes computational programs that mimic and simulate human intelligence, for example, a person's behavior in solving problems or his ability for learning. Furthermore, ML is a subset of artificial intelligence. It extracts patterns from raw data automatically. The purpose of this paper is to help researchers gain a proper understanding of machine learning and its applications in healthcare. In this paper, we first present a classification of machine learning-based schemes in healthcare. According to our proposed taxonomy, machine learning-based schemes in healthcare are categorized based on data pre-processing methods (data cleaning methods, data reduction methods), learning methods (unsupervised learning, supervised learning, semi-supervised learning, and reinforcement learning), evaluation methods (simulation-based evaluation and practical implementation-based evaluation in real environment) and applications (diagnosis, treatment). According to our proposed classification, we review some studies presented in machine learning applications for healthcare. We believe that this review paper helps researchers to familiarize themselves with the newest research on ML applications in medicine, recognize their challenges and limitations in this area, and identify future research directions.} }
@article{WOS:000731472800001, title = {Machine Learning in Action: Stroke Diagnosis and Outcome Prediction}, journal = {FRONTIERS IN NEUROLOGY}, volume = {12}, year = {2021}, issn = {1664-2295}, doi = {10.3389/fneur.2021.734345}, author = {Mainali, Shraddha and Darsie, Marin E. and Smetana, Keaton S.}, abstract = {The application of machine learning has rapidly evolved in medicine over the past decade. In stroke, commercially available machine learning algorithms have already been incorporated into clinical application for rapid diagnosis. The creation and advancement of deep learning techniques have greatly improved clinical utilization of machine learning tools and new algorithms continue to emerge with improved accuracy in stroke diagnosis and outcome prediction. Although imaging-based feature recognition and segmentation have significantly facilitated rapid stroke diagnosis and triaging, stroke prognostication is dependent on a multitude of patient specific as well as clinical factors and hence accurate outcome prediction remains challenging. Despite its vital role in stroke diagnosis and prognostication, it is important to recognize that machine learning output is only as good as the input data and the appropriateness of algorithm applied to any specific data set. Additionally, many studies on machine learning tend to be limited by small sample size and hence concerted efforts to collate data could improve evaluation of future machine learning tools in stroke. In the present state, machine learning technology serves as a helpful and efficient tool for rapid clinical decision making while oversight from clinical experts is still required to address specific aspects not accounted for in an automated algorithm. This article provides an overview of machine learning technology and a tabulated review of pertinent machine learning studies related to stroke diagnosis and outcome prediction.} }
@article{WOS:000684547900025, title = {Machine learning and earthquake forecasting-next steps}, journal = {NATURE COMMUNICATIONS}, volume = {12}, year = {2021}, doi = {10.1038/s41467-021-24952-6}, author = {Beroza, Gregory C. and Segou, Margarita and Mostafa Mousavi, S.}, abstract = {A new generation of earthquake catalogs developed through supervised machine-learning illuminates earthquake activity with unprecedented detail. Application of unsupervised machine learning to analyze the more complete expression of seismicity in these catalogs may be the fastest route to improving earthquake forecasting.} }
@article{WOS:000709466800001, title = {Semantic similarity and machine learning with ontologies}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {22}, year = {2021}, issn = {1467-5463}, doi = {10.1093/bib/bbaa199}, author = {Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and Hoehndorf, Robert}, abstract = {Ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. Recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. The methods employed to combine ontologies and machine learning are still novel and actively being developed. We provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. The methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/bio-ontology-research-group/machine-learning-with-ont ologies.} }
@article{WOS:000659549200030, title = {Data Evaluation and Enhancement for Quality Improvement of Machine Learning}, journal = {IEEE TRANSACTIONS ON RELIABILITY}, volume = {70}, pages = {831-847}, year = {2021}, issn = {0018-9529}, doi = {10.1109/TR.2021.3070863}, author = {Chen, Haihua and Chen, Jiangping and Ding, Junhua}, abstract = {Poor data quality has a direct impact on the performance of the machine learning system that is built on the data. As a demonstrated effective approach for data quality improvement, transfer learning has been widely used to improve machine learning quality. However, the ``quality improvement'' brought by transfer learning was rarely rigorously validated, and some of the quality improvement results were misleading. This article first exposed the hidden quality problem in the datasets used to build a machine learning system for normalizing medical concepts in social media text. The system was claimed to have achieved the best performance compared to existing work on a machine learning task. However, the results of our experiments showed that the ``best performance'' was due to the poor quality of the datasets and the defective validation process. To address the data quality issue and build a high-performance medical concept normalization system, we developed a transfer-learning-based strategy for data quality enhancement and system performance improvement. The results of the experiments showed a strong correlation between the quality of the datasets and the performance of the machine learning system. The results also demonstrated that a rigorous evaluation of data quality is necessary for guiding the quality improvement of machine learning. Therefore, we propose a data quality evaluation framework that includes the quality criteria and their corresponding evaluation approaches. The data validation process, the performance improvement strategy, and the data quality evaluation framework discussed in this article can be used for machine learning researchers and practitioners to build high-performance machine learning systems. The code and datasets used in this research are available in GitHub (https://github.com/haihua0913/dataEvaluationML).} }
@article{WOS:000649132600001, title = {Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft AI}, journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES}, volume = {379}, year = {2021}, issn = {1364-503X}, doi = {10.1098/rsta.2020.0083}, author = {Chantry, Matthew and Christensen, Hannah and Dueben, Peter and Palmer, Tim}, abstract = {In September 2019, a workshop was held to highlight the growing area of applying machine learning techniques to improve weather and climate prediction. In this introductory piece, we outline the motivations, opportunities and challenges ahead in this exciting avenue of research. This article is part of the theme issue `Machine learning for weather and climate modelling'.} }
@article{WOS:000627674800001, title = {Beneficial and harmful explanatory machine learning}, journal = {MACHINE LEARNING}, volume = {110}, pages = {695-721}, year = {2021}, issn = {0885-6125}, doi = {10.1007/s10994-020-05941-0}, author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, Celine and Gromowski, Mark and Schmid, Ute}, abstract = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine's involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.} }
@article{WOS:000690882400007, title = {Comparison of physical and machine learning models for estimating solar irradiance and photovoltaic power}, journal = {RENEWABLE ENERGY}, volume = {178}, pages = {1006-1019}, year = {2021}, issn = {0960-1481}, doi = {10.1016/j.renene.2021.06.079}, author = {Ramadhan, Raden A. A. and Heatubun, Yosca R. J. and Tan, Sek F. and Lee, Hyun-Jin}, abstract = {Conventional models to estimate solar irradiance and photovoltaic power rely on physics and use empirical correlations to handle regional climate and complex physics. Recently, machine learning emerges as an advanced statistical tool to construct more accurate correlations between inputs and outputs. Although machine learning has been applied for modeling solar irradiance and power, no study has reported the accuracy improvement by machine learning compared to conventional physical models. Hence, this study aims to compare the accuracies of physical and machine learning models at each step of solar power modeling, i.e., modeling of global horizontal irradiance, direct normal irradiance, global tilted irradiance, and photovoltaic power. Comparison results demonstrated that machine learning models generally outperform physical models when input parameters are appropriately selected. Machine learning models more significantly reduced the mean bias difference (MBD) than the root mean square difference (RMSD). For global horizontal irradiance and photovoltaic power, machine learning models led to substantially unbiased estimations with 0.96\\% and 0.03\\% of MBD, respectively. Among machine learning algorithms, long short-term memory and gated recurrent unit were more recommendable. However, the physical model for solar power estimation was more efficient to reduce RMSD because of their ability to consider constant parameters as input. (C) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000660500300003, title = {Quantum machine learning in high energy physics}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abc17d}, author = {Guan, Wen and Perdue, Gabriel and Pesah, Arthur and Schuld, Maria and Terashi, Koji and Vallecorsa, Sofia and Vlimant, Jean-Roch}, abstract = {Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.} }
@article{WOS:000702918200001, title = {Machine learning for predicting thermal transport properties of solids}, journal = {MATERIALS SCIENCE \\& ENGINEERING R-REPORTS}, volume = {146}, year = {2021}, issn = {0927-796X}, doi = {10.1016/j.mser.2021.100642}, author = {Qian, Xin and Yang, Ronggui}, abstract = {Quantitative descriptions of the structure-thermal property correlation have always been a challenging bottleneck in designing functional materials with superb thermal properties. In the past decade, the first-principlesbased modeling of phonon properties using density functional theory and the Boltzmann transport equation has become a common practice for predicting the thermal conductivity of new materials. However, firstprinciples calculations of thermal properties are too costly for high-throughput material screening and multiscale structural design. First-principles calculations also face several fundamental challenges in modeling thermal transport properties, for example, of crystalline materials with defects, of amorphous materials, and for materials at high temperatures. In the past five years or so, machine learning started to play a role in solving the aforementioned challenges. This review provides a comprehensive summary and discussion on the state-of-theart, future opportunities, and the remaining challenges in implementing machine learning techniques for studying thermal conductivity. After a brief introduction to the working principles of machine learning algorithms and descriptors for characterizing material structures, recent research using machine learning to study nanoscale thermal transport is discussed. Three major applications of machine learning techniques for predicting thermal properties are discussed. First, machine learning is applied to solve the challenges in modeling phonon transport of crystals with defects, in amorphous materials, and at high temperatures. In particular, machine learning is used to build high-fidelity interatomic potentials to bridge the gap between first-principles calculations and empirical molecular dynamics simulations. Second, machine learning can be used to study the correlation between thermal conductivity and other relevant properties for the high-throughput screening of functional materials. Finally, machine learning is a powerful tool for structural design to achieve target thermal conductance or thermal conductivity. This review concludes with a summary and outlook for future directions for implementing machine learning in thermal sciences.} }
@article{WOS:000637712400007, title = {Practical issues in implementing machine-learning models for building energy efficiency: Moving beyond obstacles}, journal = {RENEWABLE \\& SUSTAINABLE ENERGY REVIEWS}, volume = {143}, year = {2021}, issn = {1364-0321}, doi = {10.1016/j.rser.2021.110929}, author = {Wang, Zeyu and Liu, Jian and Zhang, Yuanxin and Yuan, Hongping and Zhang, Ruixue and Srinivasan, Ravi S.}, abstract = {Implementing machine-learning models in real applications is crucial to achieving intelligent building control and high energy efficiency. Over the past few decades, numerous studies have attempted to explore the application of machine-learning models to building energy efficiency. However, these studies have focused on analyzing the technical feasibility and superiority of machine learning algorithms for fitting building energyrelated data and have not considered methods of implementing machine learning technology in building energy efficiency applications. Therefore, this review aims to summarize the current practical issues involved in applying machine-learning models to building energy efficiency by systematically analyzing existing research findings and limitations. The paper first reviews the application status of machine learning-based building energy efficiency research by analyzing the model implementation process and summarizing the main uses of the technology in the overall building energy management life cycle. The paper then elaborates on the causes of, influences on, and potential solutions for practical issues found in the implementation and promotion of machine learning-based building energy efficiency measures. Finally, this paper discusses valuable future machine learning-based building energy efficiency research directions with regard to technology opportunity discovery, data governance, feature engineering, generalizability test, technology diffusion, and knowledge sharing. This paper will provide building researchers and practitioners with a better understanding of the current application statuses of and potential research directions for machine learning models in building energy efficiency.} }
@article{WOS:000687473600006, title = {Bayesian networks for interpretable machine learning and optimization}, journal = {NEUROCOMPUTING}, volume = {456}, pages = {648-665}, year = {2021}, issn = {0925-2312}, doi = {10.1016/j.neucom.2021.01.138}, author = {Mihaljevic, Bojan and Bielza, Concha and Larranaga, Pedro}, abstract = {As artificial intelligence is being increasingly used for high-stakes applications, it is becoming more and more important that the models used be interpretable. Bayesian networks offer a paradigm for inter-pretable artificial intelligence that is based on probability theory. They provide a semantics that enables a compact, declarative representation of a joint probability distribution over the variables of a domain by leveraging the conditional independencies among them. The representation consists of a directed acyclic graph that encodes the conditional independencies among the variables and a set of parameters that encodes conditional distributions. This representation has provided a basis for the development of algo-rithms for probabilistic reasoning (inference) and for learning probability distributions from data. Bayesian networks are used for a wide range of tasks in machine learning, including clustering, super -vised classification, multi-dimensional supervised classification, anomaly detection, and temporal mod-eling. They also provide a basis for estimation of distribution algorithms, a class of evolutionary algorithms for heuristic optimization. We illustrate the use of Bayesian networks for interpretable machine learning and optimization by presenting applications in neuroscience, the industry, and bioin-formatics, covering a wide range of machine learning and optimization tasks. (c) 2021 Published by Elsevier B.V.} }
@article{WOS:000645724900001, title = {Incorporating Machine Learning into Established Bioinformatics Frameworks}, journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}, volume = {22}, year = {2021}, issn = {1661-6596}, doi = {10.3390/ijms22062903}, author = {Auslander, Noam and Gussow, Ayal B. and Koonin, Eugene V.}, abstract = {The exponential growth of biomedical data in recent years has urged the application of numerous machine learning techniques to address emerging problems in biology and clinical research. By enabling the automatic feature extraction, selection, and generation of predictive models, these methods can be used to efficiently study complex biological systems. Machine learning techniques are frequently integrated with bioinformatic methods, as well as curated databases and biological networks, to enhance training and validation, identify the best interpretable features, and enable feature and model investigation. Here, we review recently developed methods that incorporate machine learning within the same framework with techniques from molecular evolution, protein structure analysis, systems biology, and disease genomics. We outline the challenges posed for machine learning, and, in particular, deep learning in biomedicine, and suggest unique opportunities for machine learning techniques integrated with established bioinformatics approaches to overcome some of these challenges.} }
@article{WOS:000607931400002, title = {Eight ways machine learning is assisting medicine}, journal = {NATURE MEDICINE}, volume = {27}, pages = {2-3}, year = {2021}, issn = {1078-8956}, doi = {10.1038/s41591-020-01197-2}, author = {May, Mike}, abstract = {There has been a lot of hype around the applications of machine learning in medicine. But how is machine learning actually helping bench-to-bedside scientists and clinicians do their jobs?} }
@article{WOS:000623811400031, title = {Review and analysis of supervised machine learning algorithms for hazardous events in drilling operations}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {147}, pages = {367-384}, year = {2021}, issn = {0957-5820}, doi = {10.1016/j.psep.2020.09.038}, author = {Osarogiagbon, Augustine Uhunoma and Khan, Faisal and Venkatesan, Ramachandran and Gillard, Paul}, abstract = {Results of bibliometric analysis and a detailed review are reported on the use of supervised machine learning to study hazardous drilling events. The bibliometric analysis attempts to answer pertinent questions related to progress in the use of supervised machine learning for hazardous events due to drilling fluid density/mud weight. The analysis indicates artificial neural network as the most popular algorithm among researchers. Also, deep learning, random forest and support vector machine have gained momentum in recent use. A critical review of literature on hazardous events and supervised machine learning algorithms are reported. This review was done to observe how the algorithms were used, their relative successes, limitations, as well as input parameters which aided in detection or estimation by the machine learning algorithms. An introduction to deep learning and a review of literature on the use of deep learning with respect to operations involving drilling parameters is presented. The review on deep learning and drilling parameters covered the following operations: lithology identification, drilling rig state determination, generating logging/other drilling parameters and detecting abnormality in data. The study highlights need of publicly accessible large database with data from different oilfields for development of machine learning algorithms. These algorithms could be used globally for the enhancement of machine learning for new fields or fields with limited data. The availability of such large database would aid researchers in improving or customizing deep learning algorithms in line with the unique needs of drilling activities. (C) 2020 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000700671700001, title = {Machine Learning of Spatial Data}, journal = {ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION}, volume = {10}, year = {2021}, doi = {10.3390/ijgi10090600}, author = {Nikparvar, Behnam and Thill, Jean-Claude}, abstract = {Properties of spatially explicit data are often ignored or inadequately handled in machine learning for spatial domains of application. At the same time, resources that would identify these properties and investigate their influence and methods to handle them in machine learning applications are lagging behind. In this survey of the literature, we seek to identify and discuss spatial properties of data that influence the performance of machine learning. We review some of the best practices in handling such properties in spatial domains and discuss their advantages and disadvantages. We recognize two broad strands in this literature. In the first, the properties of spatial data are developed in the spatial observation matrix without amending the substance of the learning algorithm; in the other, spatial data properties are handled in the learning algorithm itself. While the latter have been far less explored, we argue that they offer the most promising prospects for the future of spatial machine learning.} }
@article{WOS:000425056700007, title = {Implementation of machine-learning classification in remote sensing: an applied review}, journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING}, volume = {39}, pages = {2784-2817}, year = {2018}, issn = {0143-1161}, doi = {10.1080/01431161.2018.1433343}, author = {Maxwell, Aaron E. and Warner, Timothy A. and Fang, Fang}, abstract = {Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.} }
@article{WOS:000432490900005, title = {Feature selection in machine learning: A new perspective}, journal = {NEUROCOMPUTING}, volume = {300}, pages = {70-79}, year = {2018}, issn = {0925-2312}, doi = {10.1016/j.neucom.2017.11.077}, author = {Cai, Jie and Luo, Jiawei and Wang, Shulin and Yang, Sheng}, abstract = {High-dimensional data analysis is a challenge for researchers and engineers in the fields of machine learning and data mining. Feature selection provides an effective way to solve this problem by removing irrelevant and redundant data, which can reduce computation time, improve learning accuracy, and facilitate a better understanding for the learning model or data. In this study, we discuss several frequentlyused evaluation measures for feature selection, and then survey supervised, unsupervised, and semisupervised feature selection methods, which are widely applied in machine learning problems, such as classification and clustering. Lastly, future challenges about feature selection are discussed.} }
@article{WOS:000643700200002, title = {A review of possible effects of cognitive biases on interpretation of rule-based machine learning models}, journal = {ARTIFICIAL INTELLIGENCE}, volume = {295}, year = {2021}, issn = {0004-3702}, doi = {10.1016/j.artint.2021.103458}, author = {Kliegr, Tomas and Bahnik, Stepan and Fuernkranz, Johannes}, abstract = {While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. The goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain. (C) 2021 The Authors. Published by Elsevier B.V.} }
@article{WOS:000624645700001, title = {Machine Learning Techniques for THz Imaging and Time-Domain Spectroscopy}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21041186}, author = {Park, Hochong and Son, Joo-Hiuk}, abstract = {Terahertz imaging and time-domain spectroscopy have been widely used to characterize the properties of test samples in various biomedical and engineering fields. Many of these tasks require the analysis of acquired terahertz signals to extract embedded information, which can be achieved using machine learning. Recently, machine learning techniques have developed rapidly, and many new learning models and learning algorithms have been investigated. Therefore, combined with state-of-the-art machine learning techniques, terahertz applications can be performed with high performance that cannot be achieved using modeling techniques that precede the machine learning era. In this review, we introduce the concept of machine learning and basic machine learning techniques and examine the methods for performance evaluation. We then summarize representative examples of terahertz imaging and time-domain spectroscopy that are conducted using machine learning.} }
@article{WOS:000663460400001, title = {A machine learning and deep learning based approach to predict the thermal performance of phase change material integrated building envelope}, journal = {BUILDING AND ENVIRONMENT}, volume = {199}, year = {2021}, issn = {0360-1323}, doi = {10.1016/j.buildenv.2021.107927}, author = {Bhamare, Dnyandip K. and Saikia, Pranaynil and Rathod, Manish K. and Rakshit, Dibakar and Banerjee, Jyotirmay}, abstract = {This study aims to develop a machine learning and deep learning-based model for thermal performance prediction of PCM integrated roof building. Performance prediction is carried out using the newly proposed MKR index. Five machine learning and one deep learning technique are explored in order to predict the thermal performance of PCM integrated roof considering variations in thermophysical properties of PCM. Total 500 data points are generated using numerical simulations considering variations in thermophysical properties of PCM. The five machine learning models used in this study are Random forest regression, Extra trees regression, Gradient boosting regression, Extreme Gradient boosting regression, and Catboost regression. The results indicate that Gradient boosting regression is the best-performing model compared to other machine learning models. An artificial neural network is used as a deep learning approach for predicting the MKR index. The ANN-based model performed best among all five machine learning models and proved its efficacy in training, testing, and sensitivity analysis with the independent dataset.} }
@article{WOS:000697377500048, title = {mlr3proba: an R package for machine learning in survival analysis}, journal = {BIOINFORMATICS}, volume = {37}, pages = {2789-2791}, year = {2021}, issn = {1367-4803}, doi = {10.1093/bioinformatics/btab039}, author = {Sonabend, Raphael and Kiraly, Franz J. and Bender, Andreas and Bischl, Bernd and Lang, Michel}, abstract = {As machine learning has become increasingly popular over the last few decades, so too has the number of machine-learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering and more. mlr3proba provides a comprehensive machine-learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modelling and evaluation.} }
@article{WOS:000626579600078, title = {A Hybrid Posture Detection Framework: Integrating Machine Learning and Deep Neural Networks}, journal = {IEEE SENSORS JOURNAL}, volume = {21}, pages = {9515-9522}, year = {2021}, issn = {1530-437X}, doi = {10.1109/JSEN.2021.3055898}, author = {Liaqat, Sidrah and Dashtipour, Kia and Arshad, Kamran and Assaleh, Khaled and Ramzan, Naeem}, abstract = {The posture detection received lots of attention in the fields of human sensing and artificial intelligence. Posture detection can be used for the monitoring health status of elderly remotely by identifying their postures such as standing, sitting and walking. Most of the current studies used traditional machine learning classifiers to identify the posture. However, these methods do not perform well to detect the postures accurately. Therefore, in this study, we proposed a novel hybrid approach based on machine learning classifiers (i. e., support vector machine (SVM), logistic regression (KNN), decision tree, Naive Bayes, random forest, Linear discrete analysis and Quadratic discrete analysis) and deep learning classifiers (i. e., 1D-convolutional neural network (1D-CNN), 2D-convolutional neural network (2D-CNN), LSTM and bidirectional LSTM) to identify posture detection. The proposed hybrid approach uses prediction of machine learning (ML) and deep learning (DL) to improve the performance of ML and DL algorithms. The experimental results on widely benchmark dataset are shown and results achieved an accuracy of more than 98\\%.} }
@article{WOS:000533911600040, title = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence}, journal = {INFORMATION}, volume = {11}, year = {2020}, doi = {10.3390/info11040193}, author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey}, abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.} }
@article{WOS:000528284900001, title = {Engineering problems in machine learning systems}, journal = {MACHINE LEARNING}, volume = {109}, pages = {1103-1126}, year = {2020}, issn = {0885-6125}, doi = {10.1007/s10994-020-05872-w}, author = {Kuwajima, Hiroshi and Yasuoka, Hirotoshi and Nakae, Toshihiro}, abstract = {Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems-that is, in terms of requirement, design, and verification of machine learning models and systems-as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.} }
@article{WOS:000632089400001, title = {Machine Learning for Design Optimization of Electromagnetic Devices: Recent Developments and Future Directions}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11041627}, author = {Li, Yanbin and Lei, Gang and Bramerdorfer, Gerd and Peng, Sheng and Sun, Xiaodong and Zhu, Jianguo}, abstract = {This paper reviews the recent developments of design optimization methods for electromagnetic devices, with a focus on machine learning methods. First, the recent advances in multi-objective, multidisciplinary, multilevel, topology, fuzzy, and robust design optimization of electromagnetic devices are overviewed. Second, a review is presented to the performance prediction and design optimization of electromagnetic devices based on the machine learning algorithms, including artificial neural network, support vector machine, extreme learning machine, random forest, and deep learning. Last, to meet modern requirements of high manufacturing/production quality and lifetime reliability, several promising topics, including the application of cloud services and digital twin, are discussed as future directions for design optimization of electromagnetic devices.} }
@article{WOS:000704508900007, title = {Machine learning in orthopaedic surgery}, journal = {WORLD JOURNAL OF ORTHOPEDICS}, volume = {12}, pages = {685-699}, year = {2021}, issn = {2218-5836}, doi = {10.5312/wjo.v12.i9.685}, author = {Lalehzarian, Simon P. and Gowd, Anirudh K. and Liu, Joseph N.}, abstract = {Artificial intelligence and machine learning in orthopaedic surgery has gained mass interest over the last decade or so. In prior studies, researchers have demonstrated that machine learning in orthopaedics can be used for different applications such as fracture detection, bone tumor diagnosis, detecting hip implant mechanical loosening, and grading osteoarthritis. As time goes on, the utility of artificial intelligence and machine learning algorithms, such as deep learning, continues to grow and expand in orthopaedic surgery. The purpose of this review is to provide an understanding of the concepts of machine learning and a background of current and future orthopaedic applications of machine learning in risk assessment, outcomes assessment, imaging, and basic science fields. In most cases, machine learning has proven to be just as effective, if not more effective, than prior methods such as logistic regression in assessment and prediction. With the help of deep learning algorithms, such as artificial neural networks and convolutional neural networks, artificial intelligence in orthopaedics has been able to improve diagnostic accuracy and speed, flag the most critical and urgent patients for immediate attention, reduce the amount of human error, reduce the strain on medical professionals, and improve care. Because machine learning has shown diagnostic and prognostic uses in orthopaedic surgery, physicians should continue to research these techniques and be trained to use these methods effectively in order to improve orthopaedic treatment.} }
@article{WOS:000718837900001, title = {An introduction to quantum machine learning: from quantum logic to quantum deep learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {3}, year = {2021}, issn = {2524-4906}, doi = {10.1007/s42484-021-00056-8}, author = {Alchieri, Leonardo and Badalotti, Davide and Bonardi, Pietro and Bianco, Simone}, abstract = {The aim of this work is to give an introduction for a non-practical reader to the growing field of quantum machine learning, which is a recent discipline that combines the research areas of machine learning and quantum computing. This work presents the most notable scientific literature about quantum machine learning, starting from the basics of quantum logic to some specific elements and algorithms of quantum computing (such as QRAM, Grover and HHL), in order to allow a better understanding of latest quantum machine learning techniques. The main aspects of quantum machine learning are then covered, with detailed descriptions of some notable algorithms, such as quantum natural gradient and quantum support vector machines, up to the most recent quantum deep learning techniques, such as quantum neural networks.} }
@article{WOS:000739121700001, title = {Machine learning for optical fiber communication systems: An introduction and overview}, journal = {APL PHOTONICS}, volume = {6}, year = {2021}, issn = {2378-0967}, doi = {10.1063/5.0070838}, author = {Nevin, Josh W. and Nallaperuma, Sam and Shevchenko, Nikita A. and Li, Xiang and Faruk, Md. Saifuddin and Savory, Seb J.}, abstract = {Optical networks generate a vast amount of diagnostic, control, and performance monitoring data. When information is extracted from these data, reconfigurable network elements and reconfigurable transceivers allow the network to adapt not only to changes in the physical infrastructure but also to changing traffic conditions. Machine learning is emerging as a disruptive technology for extracting useful information from these raw data to enable enhanced planning, monitoring, and dynamic control. We provide a survey of the recent literature and highlight numerous promising avenues for machine learning applied to optical networks, including explainable machine learning, digital twins, and approaches in which we embed our knowledge into machine learning such as physics-informed machine learning for the physical layer and graph-based machine learning for the networking layer.} }
@article{WOS:000703568200007, title = {Introduction to Artificial Intelligence and Machine Learning for Pathology}, journal = {ARCHIVES OF PATHOLOGY \\& LABORATORY MEDICINE}, volume = {145}, pages = {1228-1254}, year = {2021}, issn = {0003-9985}, doi = {10.5858/arpa.2020-0541-CP)}, author = {Harrison, James H. Jr Jr and Gilbertson, John R. and Hanna, Matthew G. and Olson, Niels H. and Seheult, Jansen N. and Sorace, James M. and Stram, Michelle N.}, abstract = {center dot Context.-Recent developments in machine learning have stimulated intense interest in software that may augment or replace human experts. Machine learning may impact pathology practice by offering new capabilities in analysis, interpretation, and outcomes prediction using images and other data. The principles of operation and management of machine learning systems are unfamiliar to pathologists, who anticipate a need for additional education to be effective as expert users and managers of the new tools. Objective.-To provide a background on machine learning for practicing pathologists, including an overview of algorithms, model development, and performance evaluation; to examine the current status of machine learning in pathology and consider possible roles and requirements for pathologists in local deployment and management of machine learning systems; and to highlight existing challenges and gaps in deployment methodology and regulation. Data Sources.-Sources include the biomedical and engineering literature, white papers from professional organizations, government reports, electronic resources, and authors' experience in machine learning. References were chosen when possible for accessibility to practicing pathologists without specialized training in mathematics, statistics, or software development. Conclusions.-Machine learning offers an array of techniques that in recent published results show substantial promise. Data suggest that human experts working with machine learning tools outperform humans or machines separately, but the optimal form for this combination in pathology has not been established. Significant questions related to the generalizability of machine learning systems, local site verification, and performance monitoring remain to be resolved before a consensus on best practices and a regulatory environment can be established. (Arch Pathol Lab Med. 2021;145:1228-1254 ; doi: 10.5858/arpa.2020-0541-CP)} }
@incollection{WOS:000713670600026, title = {Machine Learning for Sustainable Energy Systems}, booktitle = {ANNUAL REVIEW OF ENVIRONMENT AND RESOURCES, VOL 46, 2021}, volume = {46}, pages = {719-747}, year = {2021}, issn = {1543-5938}, isbn = {978-0-8243-2346-2}, doi = {10.1146/annurev-environ-020220-061831}, author = {Donti, Priya L. and Kolter, J. Zico}, abstract = {In recent years, machine learning has proven to be a powerful tool for deriving insights from data. In this review, we describe ways in which machine learning has been leveraged to facilitate the development and operation of sustainable energy systems. We first provide a taxonomy of machine learning paradigms and techniques, along with a discussion of their strengths and limitations. We then provide an overview of existing research using machine learning for sustainable energy production, delivery, and storage. Finally, we identify gaps in this literature, propose future research directions, and discuss important considerations for deployment.} }
@article{WOS:000701828000003, title = {Do machine learning platforms provide out-of-the-box reproducibility?}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {126}, pages = {34-47}, year = {2022}, issn = {0167-739X}, doi = {10.1016/j.future.2021.06.014}, author = {Gundersen, Odd Erik and Shamsaliei, Saeid and Isdahl, Richard Juul}, abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95\\% confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted. (C) 2021 The Author(s). Published by Elsevier B.V.} }
@article{WOS:000676750300001, title = {Machine Learning-A Review of Applications in Mineral Resource Estimation}, journal = {ENERGIES}, volume = {14}, year = {2021}, doi = {10.3390/en14144079}, author = {Dumakor-Dupey, Nelson K. and Arya, Sampurna}, abstract = {Mineral resource estimation involves the determination of the grade and tonnage of a mineral deposit based on its geological characteristics using various estimation methods. Conventional estimation methods, such as geometric and geostatistical techniques, remain the most widely used methods for resource estimation. However, recent advances in computer algorithms have allowed researchers to explore the potential of machine learning techniques in mineral resource estimation. This study presents a comprehensive review of papers that have employed machine learning to estimate mineral resources. The review covers popular machine learning techniques and their implementation and limitations. Papers that performed a comparative analysis of both conventional and machine learning techniques were also considered. The literature shows that the machine learning models can accommodate several geological parameters and effectively approximate complex nonlinear relationships among them, exhibiting superior performance over the conventional techniques.} }
@article{WOS:000611115200001, title = {An Empirical Review of Automated Machine Learning}, journal = {COMPUTERS}, volume = {10}, year = {2021}, issn = {2073-431X}, doi = {10.3390/computers10010011}, author = {Vaccaro, Lorenzo and Sansonetti, Giuseppe and Micarelli, Alessandro}, abstract = {In recent years, Automated Machine Learning (AutoML) has become increasingly important in Computer Science due to the valuable potential it offers. This is testified by the high number of works published in the academic field and the significant efforts made in the industrial sector. However, some problems still need to be resolved. In this paper, we review some Machine Learning (ML) models and methods proposed in the literature to analyze their strengths and weaknesses. Then, we propose their use-alone or in combination with other approaches-to provide possible valid AutoML solutions. We analyze those solutions from a theoretical point of view and evaluate them empirically on three Atari games from the Arcade Learning Environment. Our goal is to identify what, we believe, could be some promising ways to create truly effective AutoML frameworks, therefore able to replace the human expert as much as possible, thereby making easier the process of applying ML approaches to typical problems of specific domains. We hope that the findings of our study will provide useful insights for future research work in AutoML.} }
@article{WOS:000685591400001, title = {Applying Machine Learning Approaches to Suicide Prediction Using Healthcare Data: Overview and Future Directions}, journal = {FRONTIERS IN PSYCHIATRY}, volume = {12}, year = {2021}, issn = {1664-0640}, doi = {10.3389/fpsyt.2021.707916}, author = {Boudreaux, Edwin D. and Rundensteiner, Elke and Liu, Feifan and Wang, Bo and Larkin, Celine and Agu, Emmanuel and Ghosh, Samiran and Semeter, Joshua and Simon, Gregory and Davis-Martin, Rachel E.}, abstract = {Objective: Early identification of individuals who are at risk for suicide is crucial in supporting suicide prevention. Machine learning is emerging as a promising approach to support this objective. Machine learning is broadly defined as a set of mathematical models and computational algorithms designed to automatically learn complex patterns between predictors and outcomes from example data, without being explicitly programmed to do so. The model's performance continuously improves over time by learning from newly available data. Method: This concept paper explores how machine learning approaches applied to healthcare data obtained from electronic health records, including billing and claims data, can advance our ability to accurately predict future suicidal behavior. Results: We provide a general overview of machine learning concepts, summarize exemplar studies, describe continued challenges, and propose innovative research directions. Conclusion: Machine learning has potential for improving estimation of suicide risk, yet important challenges and opportunities remain. Further research can focus on incorporating evolving methods for addressing data imbalances, understanding factors that affect generalizability across samples and healthcare systems, expanding the richness of the data, leveraging newer machine learning approaches, and developing automatic learning systems.} }
@article{WOS:000611043000001, title = {Evidence of Inflated Prediction Performance: A Commentary on Machine Learning and Suicide Research}, journal = {CLINICAL PSYCHOLOGICAL SCIENCE}, volume = {9}, pages = {129-134}, year = {2021}, issn = {2167-7026}, doi = {10.1177/2167702620954216}, author = {Jacobucci, Ross and Littlefield, Andrew K. and Millner, Alexander J. and Kleiman, Evan M. and Steinley, Douglas}, abstract = {The use of machine learning is increasing in clinical psychology, yet it is unclear whether these approaches enhance the prediction of clinical outcomes. Several studies show that machine-learning algorithms outperform traditional linear models. However, many studies that have found such an advantage use the same algorithm, random forests with the optimism-corrected bootstrap, for internal validation. Through both a simulation and empirical example, we demonstrate that the pairing of nonlinear, flexible machine-learning approaches, such as random forests with the optimism-corrected bootstrap, provide highly inflated prediction estimates. We find no advantage for properly validated machine-learning models over linear models.} }
@article{WOS:000565731100014, title = {The myth of generalisability in clinical research and machine learning in health care}, journal = {LANCET DIGITAL HEALTH}, volume = {2}, pages = {E489-E492}, year = {2020}, author = {Futoma, Joseph and Simons, Morgan and Panch, Trishan and Doshi-Velez, Finale and Celi, Leo Anthony}, abstract = {An emphasis on overly broad notions of generalisability as it pertains to applications of machine learning in health care can overlook situations in which machine learning might provide clinical utility. We believe that this narrow focus on generalisability should be replaced with wider considerations for the ultimate goal of building machine learning systems that are useful at the bedside.} }
@article{WOS:000537106200097, title = {Edge Machine Learning for AI-Enabled IoT Devices: A Review}, journal = {SENSORS}, volume = {20}, year = {2020}, doi = {10.3390/s20092533}, author = {Merenda, Massimo and Porcaro, Carlo and Iero, Demetrio}, abstract = {In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors' data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning ``Hello World''.} }
@article{WOS:000542963900016, title = {Ten Challenges in Advancing Machine Learning Technologies toward 6G}, journal = {IEEE WIRELESS COMMUNICATIONS}, volume = {27}, pages = {96-103}, year = {2020}, issn = {1536-1284}, doi = {10.1109/MWC.001.1900476}, author = {Kato, Nei and Mao, Bomin and Tang, Fengxiao and Kawamoto, Yuichi and Liu, Jiajia}, abstract = {As the 5G standard is being completed, academia and industry have begun to consider a more developed cellular communication technique, 6G, which is expected to achieve high data rates up to 1 Tb/s and broad frequency bands of 100 GHz to 3 THz. Besides the significant upgrade of the key communication metrics, Artificial Intelligence (AI) has been envisioned by many researchers as the most important feature of 6G, since the state-of-the-art machine learning technique has been adopted as the top solution in many extremely complex scenarios. Network intelligentization will be the new trend to address the challenges of exponentially increasing number of connected heterogeneous devices. However, compared with the application of machine learning in other fields, such as computer games, current research on intelligent networking still has a long way to go to realize the automatically- configured cellular communication systems. Various problems in terms of communication system, machine learning architectures, and computation efficiency should be addressed for the full use of this technique in 6G. In this paper, we analyze machine learning techniques and introduce 10 most critical challenges in advancing the intelligent 6G system.} }
@article{WOS:000594889300001, title = {Machine Learning Methods in Drug Discovery}, journal = {MOLECULES}, volume = {25}, year = {2020}, doi = {10.3390/molecules25225277}, author = {Patel, Lauv and Shukla, Tripti and Huang, Xiuzhen and Ussery, David W. and Wang, Shanzhi}, abstract = {The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.} }
@article{WOS:000551576900008, title = {Machine Learning Applications for Mass Spectrometry-Based Metabolomics}, journal = {METABOLITES}, volume = {10}, year = {2020}, doi = {10.3390/metabo10060243}, author = {Liebal, Ulf W. and Phan, An N. T. and Sudhakar, Malvika and Raman, Karthik and Blank, Lars M.}, abstract = {The metabolome of an organism depends on environmental factors and intracellular regulation and provides information about the physiological conditions. Metabolomics helps to understand disease progression in clinical settings or estimate metabolite overproduction for metabolic engineering. The most popular analytical metabolomics platform is mass spectrometry (MS). However, MS metabolome data analysis is complicated, since metabolites interact nonlinearly, and the data structures themselves are complex. Machine learning methods have become immensely popular for statistical analysis due to the inherent nonlinear data representation and the ability to process large and heterogeneous data rapidly. In this review, we address recent developments in using machine learning for processing MS spectra and show how machine learning generates new biological insights. In particular, supervised machine learning has great potential in metabolomics research because of the ability to supply quantitative predictions. We review here commonly used tools, such as random forest, support vector machines, artificial neural networks, and genetic algorithms. During processing steps, the supervised machine learning methods help peak picking, normalization, and missing data imputation. For knowledge-driven analysis, machine learning contributes to biomarker detection, classification and regression, biochemical pathway identification, and carbon flux determination. Of important relevance is the combination of different omics data to identify the contributions of the various regulatory levels. Our overview of the recent publications also highlights that data quality determines analysis quality, but also adds to the challenge of choosing the right model for the data. Machine learning methods applied to MS-based metabolomics ease data analysis and can support clinical decisions, guide metabolic engineering, and stimulate fundamental biological discoveries.} }
@incollection{WOS:000590407100004, title = {Opportunities and Challenges for Machine Learning in Materials Science}, booktitle = {ANNUAL REVIEW OF MATERIALS RESEARCH, VOL 50, 2020}, volume = {50}, pages = {71-103}, year = {2020}, issn = {1531-7331}, isbn = {978-0-8243-1750-8}, doi = {10.1146/annurev-matsci-070218-010015}, author = {Morgan, Dane and Jacobs, Ryan}, abstract = {Advances in machine learning have impacted myriad areas of materials science, such as the discovery of novel materials and the improvement ofmolecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide amore detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.} }
@article{WOS:000596015500004, title = {Machine learning techniques for biomedical image segmentation: An overview of technical aspects and introduction to state-of-art applications}, journal = {MEDICAL PHYSICS}, volume = {47}, pages = {E148-E167}, year = {2020}, issn = {0094-2405}, doi = {10.1002/mp.13649}, author = {Seo, Hyunseok and Khuzani, Masoud Badiei and Vasudevan, Varun and Huang, Charles and Ren, Hongyi and Xiao, Ruoxiu and Jia, Xiao and Xing, Lei}, abstract = {In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep-learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep-learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past 3 yr. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.} }
@article{WOS:000546550100020, title = {Statistical and machine learning models in credit scoring: A systematic literature survey}, journal = {APPLIED SOFT COMPUTING}, volume = {91}, year = {2020}, issn = {1568-4946}, doi = {10.1016/j.asoc.2020.106263}, author = {Dastile, Xolani and Celik, Turgay and Potsane, Moshe}, abstract = {In practice, as a well-known statistical method, the logistic regression model is used to evaluate the credit-worthiness of borrowers due to its simplicity and transparency in predictions. However, in literature, sophisticated machine learning models can be found that can replace the logistic regression model. Despite the advances and applications of machine learning models in credit scoring, there are still two major issues: the incapability of some of the machine learning models to explain predictions; and the issue of imbalanced datasets. As such, there is a need for a thorough survey of recent literature in credit scoring. This article employs a systematic literature survey approach to systematically review statistical and machine learning models in credit scoring, to identify limitations in literature, to propose a guiding machine learning framework, and to point to emerging directions. This literature survey is based on 74 primary studies, such as journal and conference articles, that were published between 2010 and 2018. According to the meta-analysis of this literature survey, we found that in general, an ensemble of classifiers performs better than single classifiers. Although deep learning models have not been applied extensively in credit scoring literature, they show promising results. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000643857400011, title = {Interpretable machine learning with an ensemble of gradient boosting machines}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {222}, year = {2021}, issn = {0950-7051}, doi = {10.1016/j.knosys.2021.106993}, author = {Konstantinov, V, Andrei and Utkin, V, Lev}, abstract = {A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation. (C) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000503751600001, title = {Comparing different supervised machine learning algorithms for disease prediction}, journal = {BMC MEDICAL INFORMATICS AND DECISION MAKING}, volume = {19}, year = {2019}, doi = {10.1186/s12911-019-1004-8}, author = {Uddin, Shahadat and Khan, Arif and Hossain, Md Ekramul and Moni, Mohammad Ali}, abstract = {Background Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction. Methods In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction. Results We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naive Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53\\%. This was followed by SVM which topped in 41\\% of the studies it was considered. Conclusion This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.} }
@article{WOS:000466380000034, title = {Big data and machine learning algorithms for health-care delivery}, journal = {LANCET ONCOLOGY}, volume = {20}, pages = {E262-E273}, year = {2019}, issn = {1470-2045}, doi = {10.1016/S1470-2045(19)30149-4}, author = {Ngiam, Kee Yuan and Khor, Ing Wei}, abstract = {Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data preprocessing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care.} }
@article{WOS:000517852500004, title = {Machine learning for enterprises: Applications, algorithm selection, and challenges}, journal = {BUSINESS HORIZONS}, volume = {63}, pages = {157-170}, year = {2020}, issn = {0007-6813}, doi = {10.1016/j.bushor.2019.10.005}, author = {Lee, In and Shin, Yong Jae}, abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications. (C) 2019 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights reserved.} }
@article{WOS:000540393400001, title = {Machine learning for continuous innovation in battery technologies}, journal = {NATURE REVIEWS MATERIALS}, volume = {5}, pages = {725-727}, year = {2020}, issn = {2058-8437}, doi = {10.1038/s41578-020-0216-y}, author = {Aykol, Muratahan and Herring, Patrick and Anapolsky, Abraham}, abstract = {Batteries, as complex materials systems, pose unique challenges for the application of machine learning. Although a shift to data-driven, machine learning-based battery research has started, new initiatives in academia and industry are needed to fully exploit its potential.} }
@article{WOS:000506166100099, title = {explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning}, journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}, volume = {26}, pages = {1064-1074}, year = {2020}, issn = {1077-2626}, doi = {10.1109/TVCG.2019.2934629}, author = {Spinner, Thilo and Schlegel, Udo and Schaefer, Hanna and El-Assady, Mennatallah}, abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.} }
@article{WOS:000554567500001, title = {A Review of Android Malware Detection Approaches Based on Machine Learning}, journal = {IEEE ACCESS}, volume = {8}, pages = {124579-124607}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3006143}, author = {Liu, Kaijun and Xu, Shengwei and Xu, Guoai and Zhang, Miao and Sun, Dawei and Liu, Haifeng}, abstract = {Android applications are developing rapidly across the mobile ecosystem, but Android malware is also emerging in an endless stream. Many researchers have studied the problem of Android malware detection and have put forward theories and methods from different perspectives. Existing research suggests that machine learning is an effective and promising way to detect Android malware. Notwithstanding, there exist reviews that have surveyed different issues related to Android malware detection based on machine learning. We believe our work complements the previous reviews by surveying a wider range of aspects of the topic. This paper presents a comprehensive survey of Android malware detection approaches based on machine learning. We briefly introduce some background on Android applications, including the Android system architecture, security mechanisms, and classification of Android malware. Then, taking machine learning as the focus, we analyze and summarize the research status from key perspectives such as sample acquisition, data preprocessing, feature selection, machine learning models, algorithms, and the evaluation of detection effectiveness. Finally, we assess the future prospects for research into Android malware detection based on machine learning. This review will help academics gain a full picture of Android malware detection based on machine learning. It could then serve as a basis for subsequent researchers to start new work and help to guide research in the field more generally.} }
@article{WOS:001483708400001, title = {Typical applications and perspectives of machine learning for advanced precision machining: A comprehensive review}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {283}, year = {2025}, issn = {0957-4174}, doi = {10.1016/j.eswa.2025.127770}, author = {Liang, Yiji and Dai, Canwen and Wang, Jingwei and Zhang, Guoqing and To, Suet and Zhao, Zejia}, abstract = {Advanced precision machining technologies, such as micro/ultraprecision mechanical machining and atomic and close-to-atomic scale manufacturing, are critical to high-value industries like aerospace and defense. However, extreme precision requirements and nonlinear dynamics pose significant challenges for accurate modeling, as traditional methods often struggle to capture intricate interactions and inherent variability. Machine learning emerges as a transformative solution, enabling data-driven modeling with unprecedented accuracy. This paper provides a comprehensive overview of the significant advancements and typical applications of machine learning in advanced precision machining, focusing on model architectures and methodologies to guide industrial implementation. For instance, this paper presents various examples, such as the application of LSTM networks in predicting tool life by capturing temporal dependencies in force signals, which illustrates how machine learning models are tailored to address specific challenges in precision machining. However, industrial adoption of machine learning remains hindered by limited datasets and computational constraints. This paper offers forward-looking recommendations to address these issues, integrating machine learning into precision machining within the framework of Industry 5.0 and providing robust support for the further promotion and application of machine learning in actual production environments. Furthermore, this research establishes a robust framework for recognizing similarities in machine learning applications across diverse machining domains, facilitating transfer learning among various advanced precision machining processes. By bridging the gap between theoretical models and industrial scalability, this review highlights the transformative role of machine learning in advanced precision machining toward intelligent, sustainable production, ultimately supporting highperformance component manufacturing.} }
@article{WOS:000669776700006, title = {Machine Learning for Soft Robotic Sensing and Control}, journal = {ADVANCED INTELLIGENT SYSTEMS}, volume = {2}, year = {2020}, doi = {10.1002/aisy.201900171}, author = {Chin, Keene and Hellebrekers, Tess and Majidi, Carmel}, abstract = {Herein, the progress of machine learning methods in the field of soft robotics, specifically in the applications of sensing and control, is outlined. Data-driven methods such as machine learning are especially suited to systems with governing functions that are unknown, impractical or impossible to represent analytically, or computationally intractable to integrate into real-world solutions. Function approximation with careful formulation of the machine learning architecture enables the encoding of dynamic behavior and nonlinearities, with the added potential to address hysteresis and nonstationary behavior. Supervised learning and reinforcement learning in simulation and on a wide variety of physical robotic systems have shown promising results for the use of empirical data-driven methods as a solution to contemporary soft robotics problems.} }
@article{WOS:000670264800013, title = {Advances of Four Machine Learning Methods for Spatial Data Handling: a Review}, journal = {JOURNAL OF GEOVISUALIZATION AND SPATIAL ANALYSIS}, volume = {4}, year = {2020}, issn = {2509-8810}, doi = {10.1007/s41651-020-00048-5}, author = {Du, Peijun and Bai, Xuyu and Tan, Kun and Xue, Zhaohui and Samat, Alim and Xia, Junshi and Li, Erzhu and Su, Hongjun and Liu, Wei}, abstract = {Most machine learning tasks can be categorized into classification or regression problems. Regression and classification models are normally used to extract useful geographic information from observed or measured spatial data, such as land cover classification, spatial interpolation, and quantitative parameter retrieval. This paper reviews the progress of four advanced machine learning methods for spatial data handling, namely, support vector machine (SVM)-based kernel learning, semi-supervised and active learning, ensemble learning, and deep learning. These four machine learning modes are representative because they improve learning performances from different views, for example, feature space transform and decision function (SVM), optimized uses of samples (semi-supervised and active learning), and enhanced learning models and capabilities (ensemble learning and deep learning). For spatial data handling via machine learning that can be improved by the four machine learning models, three key elements are learning algorithms, training samples, and input features. To apply machine learning methods to spatial data handling successfully, a four-level strategy is suggested: experimenting and evaluating the applicability, extending the algorithms by embedding spatial properties, optimizing the parameters for better performance, and enhancing the algorithm by multiple means. Firstly, the advances of SVM are reviewed to demonstrate the merits of novel machine learning methods for spatial data, running the line from direct use and comparison with traditional classifiers, and then targeted improvements to address multiple class problems, to optimize parameters of SVM, and to use spatial and spectral features. To overcome the limits of small-size training samples, semi-supervised learning and active learning methods are then utilized to deal with insufficient labeled samples, showing the potential of learning from small-size training samples. Furthermore, considering the poor generalization capacity and instability of machine learning algorithms, ensemble learning is introduced to integrate the advantages of multiple learners and to enhance the generalization capacity. The typical research lines, including the combination of multiple classifiers, advanced ensemble classifiers, and spatial interpolation, are presented. Finally, deep learning, one of the most popular branches of machine learning, is reviewed with specific examples for scene classification and urban structural type recognition from high-resolution remote sensing images. By this review, it can be concluded that machine learning methods are very effective for spatial data handling and have wide application potential in the big data era.} }
@article{WOS:000512357000002, title = {A Perspective on Using Machine Learning in 3D Bioprinting}, journal = {INTERNATIONAL JOURNAL OF BIOPRINTING}, volume = {6}, year = {2020}, issn = {2424-7723}, doi = {10.18063/ijb.v6i1.253}, author = {Yu, Chunling and Jiang, Jingchao}, abstract = {Recently, three-dimensional (3D) printing technologies have been widely applied in industry and our daily lives. The term 3D bioprinting has been coined to describe 3D printing at the biomedical level. Machine learning is currently becoming increasingly active and has been used to improve 3D printing processes, such as process optimization, dimensional accuracy analysis, manufacturing defect detection, and material property prediction. However, few studies have been found to use machine learning in 3D bioprinting processes. In this paper, related machine learning methods used in 3D printing are briefly reviewed and a perspective on how machine learning can also benefit 3D bioprinting is discussed. We believe that machine learning can significantly affect the future development of 3D bioprinting and hope this paper can inspire some ideas on how machine learning can be used to improve 3D bioprinting.} }
@article{WOS:000591283600001, title = {Machine learning for landslides prevention: a survey}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {33}, pages = {10881-10907}, year = {2021}, issn = {0941-0643}, doi = {10.1007/s00521-020-05529-8}, author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco}, abstract = {Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.} }
@incollection{WOS:000524457700003, title = {Machine Learning in Epidemiology and Health Outcomes Research}, booktitle = {ANNUAL REVIEW OF PUBLIC HEALTH, VOL 41}, volume = {41}, pages = {21-36}, year = {2020}, issn = {0163-7525}, isbn = {978-0-8243-2741-5}, doi = {10.1146/annurev-publhealth-040119-094437}, author = {Wiemken, Timothy L. and Kelley, Robert R.}, abstract = {Machine learning approaches to modeling of epidemiologic data are becoming increasingly more prevalent in the literature. These methods have the potential to improve our understanding of health and opportunities for intervention, far beyond our past capabilities. This article provides a walkthrough for creating supervised machine learning models with current examples from the literature. From identifying an appropriate sample and selecting features through training, testing, and assessing performance, the end-to-end approach to machine learning can be a daunting task. We take the reader through each step in the process and discuss novel concepts in the area of machine learning, including identifying treatment effects and explaining the output from machine learning models.} }
@article{WOS:000573738100002, title = {Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism}, journal = {NATURE COMMUNICATIONS}, volume = {11}, year = {2020}, doi = {10.1038/s41467-020-17910-1}, author = {Zhang, Jie and Petersen, Soren D. and Radivojevic, Tijana and Ramirez, Andres and Perez-Manriquez, Andres and Abeliuk, Eduardo and Sanchez, Benjamin J. and Costello, Zak and Chen, Yu and Fero, Michael J. and Martin, Hector Garcia and Nielsen, Jens and Keasling, Jay D. and Jensen, Michael K.}, abstract = {Through advanced mechanistic modeling and the generation of large high-quality datasets, machine learning is becoming an integral part of understanding and engineering living systems. Here we show that mechanistic and machine learning models can be combined to enable accurate genotype-to-phenotype predictions. We use a genome-scale model to pinpoint engineering targets, efficient library construction of metabolic pathway designs, and high-throughput biosensor-enabled screening for training diverse machine learning algorithms. From a single data-generation cycle, this enables successful forward engineering of complex aromatic amino acid metabolism in yeast, with the best machine learning-guided design recommendations improving tryptophan titer and productivity by up to 74 and 43\\%, respectively, compared to the best designs used for algorithm training. Thus, this study highlights the power of combining mechanistic and machine learning models to effectively direct metabolic engineering efforts. In metabolic engineering, mechanistic models require prior metabolism knowledge of the chassis strain, whereas machine learning models need ample training data. Here, the authors combine the mechanistic and machine learning models to improve prediction performance of tryptophan metabolism in baker's yeast.} }
@article{WOS:000609037300001, title = {A review on speech processing using machine learning paradigm}, journal = {INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY}, volume = {24}, pages = {367-388}, year = {2021}, issn = {1381-2416}, doi = {10.1007/s10772-021-09808-0}, author = {Bhangale, Kishor Barasu and Mohanaprasad, K.}, abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human-machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.} }
@article{WOS:000668245900001, title = {Meta-learning and the new challenges of machine learning}, journal = {INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS}, volume = {36}, pages = {6240-6272}, year = {2021}, issn = {0884-8173}, doi = {10.1002/int.22549}, author = {Monteiro, Jose Pedro and Ramos, Diogo and Carneiro, Davide and Duarte, Francisco and Fernandes, Joao M. and Novais, Paulo}, abstract = {In the last years, organizations and companies in general have found the true potential value of collecting and using data for supporting decision-making. As a consequence, data are being collected at an unprecedented rate. This poses several challenges, including, for example, regarding the storage and processing of these data. Machine Learning (ML) is also not an exception, in the sense that algorithms must now deal with novel challenges, such as learn from streaming data or deal with concept drift. ML engineers also have a harder task when it comes to selecting the most appropriate model, given the wealth of algorithms and possible configurations that exist nowadays. At the same time, training time is a stronger restriction as the computational complexity of the training model increases. In this paper we propose a framework for dealing with these challenges, based on meta-learning. Specifically, we tackle two well-defined problems: automatic algorithm selection and continuous algorithm updates that do not require the retraining of the whole algorithm to adapt to new data. Results show that the proposed framework can contribute to ameliorate the identified issues.} }
@article{WOS:000594402900006, title = {Integrating Machine Learning with Human Knowledge}, journal = {ISCIENCE}, volume = {23}, year = {2020}, doi = {10.1016/j.isci.2020.101656}, author = {Deng, Changyu and Ji, Xunbi and Rainey, Colton and Zhang, Jianyu and Lu, Wei}, abstract = {Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.} }
@article{WOS:000649908600001, title = {Machine Learning Methods with Noisy, Incomplete or Small Datasets}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11094132}, author = {Caiafa, Cesar F. and Sun, Zhe and Tanaka, Toshihisa and Marti-Puig, Pere and Sole-Casals, Jordi}, abstract = {In this article, we present a collection of fifteen novel contributions on machine learning methods with low-quality or imperfect datasets, which were accepted for publication in the special issue ``Machine Learning Methods with Noisy, Incomplete or Small Datasets'', Applied Sciences (ISSN 2076-3417). These papers provide a variety of novel approaches to real-world machine learning problems where available datasets suffer from imperfections such as missing values, noise or artefacts. Contributions in applied sciences include medical applications, epidemic management tools, methodological work, and industrial applications, among others. We believe that this special issue will bring new ideas for solving this challenging problem, and will provide clear examples of application in real-world scenarios.} }
@incollection{WOS:000677831600018, title = {Probabilistic Machine Learning for Healthcare}, booktitle = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE, VOL 4}, volume = {4}, pages = {393-415}, year = {2021}, issn = {2574-3414}, doi = {10.1146/annurev-biodatasci-092820-033938}, author = {Chen, Irene Y. and Joshi, Shalmali and Ghassemi, Marzyeh and Ranganath, Rajesh}, abstract = {Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial, including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.} }
@article{WOS:000709064000001, title = {A Literature Review of Using Machine Learning in Software Development Life Cycle Stages}, journal = {IEEE ACCESS}, volume = {9}, pages = {140896-140920}, year = {2021}, issn = {2169-3536}, doi = {10.1109/ACCESS.2021.3119746}, author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander}, abstract = {The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.} }
@article{WOS:000704764100004, title = {Kernel extreme learning machine based hierarchical machine learning for multi-type and concurrent fault diagnosis}, journal = {MEASUREMENT}, volume = {184}, year = {2021}, issn = {0263-2241}, doi = {10.1016/j.measurement.2021.109923}, author = {Chen, Qiuan and Wei, Haipeng and Rashid, Muhammad and Cai, Zhiqiang}, abstract = {The detection and identification of faults in rotary machines are of great significance to the mechanical equipment reliability especially the gearbox. Traditional machine learning algorithms suffer from low diagnosis accuracy of faults that have multiple types and exist concurrently. A novel machine learning method called hierarchical machine learning (HML) was proposed in this study to improve the faults diagnosis accuracy. The proposed algorithm consists of two layers. The first layer comprises a traditional machine learning model to identify the faults with distinguishable features and filter out these faults with indistinguishable features. The second layer model recognizes the faults filtered out by the first layer. In order to verify the effectiveness of the proposed method, the gearbox simulation experiment is carried out in the study. The simulation results validate that the proposed method outperforms other algorithms under an identical measure.} }
@article{WOS:000742888800012, title = {Data Acquisition for Improving Machine Learning Models}, journal = {PROCEEDINGS OF THE VLDB ENDOWMENT}, volume = {14}, pages = {1832-1844}, year = {2021}, issn = {2150-8097}, doi = {10.14778/3467861.3467872}, author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick}, abstract = {The vast advances in Machine Learning (ML) over the last ten years have been powered by the availability of suitably prepared data for training purposes. The future of ML-enabled enterprise hinges on data. As such, there is already a vibrant market offering data annotation services to tailor sophisticated ML models. In this paper, inspired by the recent vision of online data markets and associated market designs, we present research on the practical problem of obtaining data in order to improve the accuracy of ML models. We consider an environment in which consumers query for data to enhance the accuracy of their models and data providers who possess data make them available for training purposes. We first formalize this interaction process laying out the suitable framework and associated parameters for data exchange. We then propose two data acquisition strategies that consider a trade-off between exploration during which we obtain data to learn about the distribution of a provider's data and exploitation during which we optimize our data inquiries utilizing the gained knowledge. In the first strategy, Estimation and Allocation (EA), we utilize queries to estimate the utilities of various predicates while learning about the distribution of the provider's data; then we proceed to the allocation stage in which we utilize those learned utility estimates to inform our data acquisition decisions. The second algorithmic proposal, named Sequential Predicate Selection (SPS), utilizes a sampling strategy to explore the distribution of the provider's data, adaptively investing more resources to parts of the data space that are statistically more promising to improve overall model accuracy. We present a detailed experimental evaluation of our proposals utilizing a variety of ML models and associated real data sets exploring all applicable parameters of interest. Our results demonstrate the relative benefits of the proposed algorithms. Depending on the models trained and the associated learning tasks we identify trade-offs and highlight the relative benefits of each algorithm to further optimize model accuracy.} }
@article{WOS:000732946500010, title = {The role of machine learning analytics and metrics in retailing research}, journal = {JOURNAL OF RETAILING}, volume = {97}, pages = {658-675}, year = {2021}, issn = {0022-4359}, doi = {10.1016/j.jretai.2020.12.001}, author = {Wang, Xin (Shane) and Ryoo, Jun Hyun (Joseph) and Bendle, Neil and Kopalle, Praveen K.}, abstract = {This research presents the use of machine learning analytics and metrics in the retailing context. We first discuss what is machine learning and explain the field's origins. We then demonstrate the strengths of machine learning methods using an online retailing dataset, noting key areas of divergence from the traditional explanatory approach to data analysis. We then provide a review of the current state of machine learning in top-level retailing and marketing research, integrating ideas for future research and showcasing potential applications for practitioners. We propose that the explanatory and machine learning approaches need not be mutually exclusive. Particularly, we discuss four key areas in the general scientific research process that can benefit from machine learning: data exploration/theory building, variable creation, estimation, and predicting an outcome metric. Due to the customer-facing nature of retailing, we anticipate several challenges researchers and practitioners might face in the adoption and implementation of machine learning, such as ethical prediction and customer privacy issues. Overall, our belief is that machine learning can enhance customer experience and, accordingly, we advance opportunities for future research. (c) 2020 New York University. Published by Elsevier Inc. All rights reserved.} }
@article{WOS:000554897200004, title = {Machine learning in materials science}, journal = {INFOMAT}, volume = {1}, pages = {338-358}, year = {2019}, doi = {10.1002/inf2.12028}, author = {Wei, Jing and Chu, Xuan and Sun, Xiang-Yu and Xu, Kun and Deng, Hui-Xiong and Chen, Jigen and Wei, Zhongming and Lei, Ming}, abstract = {Traditional methods of discovering new materials, such as the empirical trial and error method and the density functional theory (DFT)-based method, are unable to keep pace with the development of materials science today due to their long development cycles, low efficiency, and high costs. Accordingly, due to its low computational cost and short development cycle, machine learning is coupled with powerful data processing and high prediction performance and is being widely used in material detection, material analysis, and material design. In this article, we discuss the basic operational procedures in analyzing material properties via machine learning, summarize recent applications of machine learning algorithms to several mature fields in materials science, and discuss the improvements that are required for wide-ranging application.} }
@article{WOS:000477857700021, title = {Machine-learning-guided directed evolution for protein engineering}, journal = {NATURE METHODS}, volume = {16}, pages = {687-694}, year = {2019}, issn = {1548-7091}, doi = {10.1038/s41592-019-0496-6}, author = {Yang, Kevin K. and Wu, Zachary and Arnold, Frances H.}, abstract = {Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence-function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function.} }
@article{WOS:000645896700001, title = {Advances in Machine Learning and Deep Neural Networks}, journal = {PROCEEDINGS OF THE IEEE}, volume = {109}, pages = {607-611}, year = {2021}, issn = {0018-9219}, doi = {10.1109/JPROC.2021.3072172}, author = {Chellappa, Rama and Theodoridis, Sergios and van Schaik, Andre}, abstract = {We are currently experiencing the dawn of what is known as the fourth industrial revolution. At the center of this historical happening, as one of the key enabling technologies, lies a discipline that deals with data and whose goal is to extract information and related knowledge that is hidden in it, in order to make predictions and, subsequently, take decisions. Machine learning (ML) is the name that is used as an umbrella to cover a wide range of theories, methods, algorithms, and architectures that are used to this end.} }
@article{WOS:000625545300001, title = {An Introduction to Machine Learning for Panel Data}, journal = {INTERNATIONAL ADVANCES IN ECONOMIC RESEARCH}, volume = {27}, pages = {1-16}, year = {2021}, issn = {1083-0898}, doi = {10.1007/s11294-021-09815-6}, author = {Chen, James Ming}, abstract = {Machine learning has dramatically expanded the range of tools for evaluating economic panel data. This paper applies a variety of machine-learning methods to the Boston housing dataset, an iconic proving ground for machine learning. Though machine learning often lacks the overt interpretability of linear regression, methods based on decision trees score the relative importance of dataset features. In addition to addressing the theoretical tradeoff between bias and variance, this paper discusses practices rarely followed in traditional economics: the splitting of data into training, validation, and test sets; the scaling of data; and the preference for retaining all data. The choice between traditional and machine-learning methods hinges on practical rather than mathematical considerations. In settings emphasizing interpretative clarity through the scale and sign of regression coefficients, machine learning may best play an ancillary role. Wherever predictive accuracy is paramount, however, or where heteroskedasticity or high dimensionality might impair the clarity of linear methods, machine learning can deliver superior results.} }
@article{WOS:000538049100005, title = {Combining machine learning and process engineering physics towards enhanced accuracy and explainability of data-driven models}, journal = {COMPUTERS \\& CHEMICAL ENGINEERING}, volume = {138}, year = {2020}, issn = {0098-1354}, doi = {10.1016/j.compchemeng.2020.106834}, author = {Bikmukhametov, Timur and Jaschke, Johannes}, abstract = {Machine learning models are often considered as black-box solutions which is one of the main reasons why they are still not widely used in operation of process engineering systems. One approach to overcome this problem is to combine machine learning with first principles models of a process engineering system. In this work, we investigate different methods of combining machine learning with first principles and test them on a case study of multiphase flowrate estimation in a petroleum production system. However, the methods can be applied to any process engineering system. The results show that by adding physics-based models to machine learning, it is possible not only to improve the performance of the purely black-box machine learning models, but also to make them more transparent and interpretable. We also propose a step-by-step procedure for selecting a method for combining physics and machine learning depending on the process engineering system conditions. (C) 2020 The Authors. Published by Elsevier Ltd.} }
@article{WOS:000496269400199, title = {Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey}, journal = {APPLIED SCIENCES-BASEL}, volume = {9}, year = {2019}, doi = {10.3390/app9204396}, author = {Liu, Hongyu and Lang, Bo}, abstract = {Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.} }
@article{WOS:000459730200026, title = {A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {21}, pages = {686-728}, year = {2019}, doi = {10.1109/COMST.2018.2847722}, author = {Mishra, Preeti and Varadharajan, Vijay and Tupakula, Uday and Pilli, Emmanuel S.}, abstract = {Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.} }
@article{WOS:000474586200010, title = {Machine Learning for the Geosciences: Challenges and Opportunities}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {31}, pages = {1544-1554}, year = {2019}, issn = {1041-4347}, doi = {10.1109/TKDE.2018.2861006}, author = {Karpatne, Anuj and Ebert-Uphoff, Imme and Ravela, Sai and Babaie, Hassan Ali and Kumar, Vipin}, abstract = {Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)-that has been widely successful in commercial domains-offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.} }
@article{WOS:000530830800100, title = {Machine Learning Security: Threats, Countermeasures, and Evaluations}, journal = {IEEE ACCESS}, volume = {8}, pages = {74720-74742}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2987435}, author = {Xue, Mingfu and Yuan, Chengxiang and Wu, Heyi and Zhang, Yushu and Liu, Weiqiang}, abstract = {Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.} }
@article{WOS:000601374000001, title = {Applications of machine learning in spectroscopy}, journal = {APPLIED SPECTROSCOPY REVIEWS}, volume = {56}, pages = {733-763}, year = {2021}, issn = {0570-4928}, doi = {10.1080/05704928.2020.1859525}, author = {Meza Ramirez, Carlos A. and Greenop, Michael and Ashton, Lorna and Rehman, Ihtesham Ur}, abstract = {The way to analyze data in spectroscopy has changed substantially. At the same time, data science has evolved to the point where spectroscopy can find space to be housed, adapted and be functional. The integration of the two sciences has introduced a knowledge gap between data scientists who know about advanced machine learning techniques and spectroscopists who have a solid background in chemometrics. To reach a symbiosis, the knowledge gap requires bridging. This review article focuses on introducing data science subjects to non-specialist spectroscopists, or those unfamiliar with the subject. The article will explain concepts that are covered in machine learning, such as supervised learning, unsupervised learning, deep learning, and most importantly, the difference between machine learning and artificial intelligence. This article also includes examples of published spectroscopy research, in which some of the concepts explained here are applied. Machine learning together with spectroscopy can provide a useful, fast, and efficient tool to analyze samples of interest both for industrial and research purposes.} }
@article{WOS:000543560600001, title = {Review on Machine Learning Algorithm Based Fault Detection in Induction Motors}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {1929-1940}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09446-w}, author = {Kumar, Prashant and Hati, Ananda Shankar}, abstract = {Fault detection prior to their occurrence or complete shut-down in induction motor is essential for the industries. The fault detection based on condition monitoring techniques and application of machine learning have tremendous potential. The power of machine learning can be harnessed and optimally used for fault detection. The faults especially in induction motor needs to be addressed at a proper time for avoiding losses. Machine learning algorithm applications in the domain of fault detection provides a reliable and effective solution for preventive maintenance. This paper presents a review of the machine learning algorithm applications in fault detection in induction motors. This paper also presents the future prospects and challenges for an efficient machine learning based fault detection systems.} }
@article{WOS:000568991800003, title = {Machine learning in breast MRI}, journal = {JOURNAL OF MAGNETIC RESONANCE IMAGING}, volume = {52}, pages = {998-1018}, year = {2020}, issn = {1053-1807}, doi = {10.1002/jmri.26852}, author = {Reig, Beatriu and Heacock, Laura and Geras, Krzysztof J. and Moy, Linda}, abstract = {Machine-learning techniques have led to remarkable advances in data extraction and analysis of medical imaging. Applications of machine learning to breast MRI continue to expand rapidly as increasingly accurate 3D breast and lesion segmentation allows the combination of radiologist-level interpretation (eg, BI-RADS lexicon), data from advanced multiparametric imaging techniques, and patient-level data such as genetic risk markers. Advances in breast MRI feature extraction have led to rapid dataset analysis, which offers promise in large pooled multiinstitutional data analysis. The object of this review is to provide an overview of machine-learning and deep-learning techniques for breast MRI, including supervised and unsupervised methods, anatomic breast segmentation, and lesion segmentation. Finally, it explores the role of machine learning, current limitations, and future applications to texture analysis, radiomics, and radiogenomics. Technical Efficacy Stage:2 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:998-1018.} }
@article{WOS:000544436100001, title = {Prediction of significant wave height; comparison between nested grid numerical model, and machine learning models of artificial neural networks, extreme learning and support vector machines}, journal = {ENGINEERING APPLICATIONS OF COMPUTATIONAL FLUID MECHANICS}, volume = {14}, pages = {805-817}, year = {2020}, issn = {1994-2060}, doi = {10.1080/19942060.2020.1773932}, author = {Shamshirband, Shahaboddin and Mosavi, Amir and Rabczuk, Timon and Nabipour, Narjes and Chau, Kwok-wing}, abstract = {Estimation of wave height is essential for several coastal engineering applications. This study advances a nested grid numerical model and compare its efficiency with three machine learning (ML) methods of artificial neural networks (ANN), extreme learning machines (ELM) and support vector regression (SVR) for wave height modeling. The models are trained by surface wind data. The results demonstrate that all the models generally provide sound predictions. Due to the high level of variability in the bathymetry of the study area, implementation of the nested grid with different Whitecapping coefficient is a suitable approach to improve the efficiency of the numerical models. Performance on the ML models do not differ remarkably even though the ELM model slightly outperforms the other models.} }
@article{WOS:000638379600001, title = {A Survey of Machine Learning-Based System Performance Optimization Techniques}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11073235}, author = {Choi, Hyejeong and Park, Sejin}, abstract = {Recently, the machine learning research trend expands to the system performance optimization field, where it has still been proposed by researchers based on their intuitions and heuristics. Compared to conventional major machine learning research areas such as image or speech recognition, machine learning-based system performance optimization fields are at the beginning stage. However, recent papers show that this approach is promising and has significant potential. This paper reviews 11 machine learning-based system performance optimization approaches from nine recent papers based on well-known machine learning models such as perceptron, LSTM, and RNN. This survey provides a detailed design and summarizes model, input, output, and prediction method of each approach. This paper covers various system performance areas from the data structure to essential system components of a computer system such as index structure, branch predictor, sort, and cache management. The result shows that machine learning-based system performance optimization has an important potential for future research. We expect that this paper shows a wide range of applicability of machine learning technology and provides a new perspective for system performance optimization.} }
@article{WOS:000551254000005, title = {Applied machine learning and artificial intelligence in rheumatology}, journal = {RHEUMATOLOGY ADVANCES IN PRACTICE}, volume = {4}, year = {2020}, doi = {10.1093/rap/rkaa005}, author = {Hugle, Maria and Omoumi, Patrick and van Laar, Jacob M. and Boedecker, Joschka and Hugle, Thomas}, abstract = {Machine learning as a field of artificial intelligence is increasingly applied in medicine to assist patients and physicians. Growing datasets provide a sound basis with which to apply machine learning methods that learn from previous experiences. This review explains the basics of machine learning and its subfields of supervised learning, unsupervised learning, reinforcement learning and deep learning. We provide an overview of current machine learning applications in rheumatology, mainly supervised learning methods for e-diagnosis, disease detection and medical image analysis. In the future, machine learning will be likely to assist rheumatologists in predicting the course of the disease and identifying important disease factors. Even more interestingly, machine learning will probably be able to make treatment propositions and estimate their expected benefit (e.g. by reinforcement learning). Thus, in future, shared decision-making will not only include the patient's opinion and the rheumatologist's empirical and evidence-based experience, but it will also be influenced by machine-learned evidence.} }
@article{WOS:000600294400001, title = {Machine Learning: Quantum vs Classical}, journal = {IEEE ACCESS}, volume = {8}, pages = {219275-219294}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3041719}, author = {Khan, Tariq M. and Robles-Kelly, Antonio}, abstract = {Encouraged by growing computing power and algorithmic development, machine learning technologies have become powerful tools for a wide variety of application areas, spanning from agriculture to chemistry and natural language processing. The use of quantum systems to process classical data using machine learning algorithms has given rise to an emerging research area, i.e. quantum machine learning. Despite its origins in the processing of classical data, quantum machine learning also explores the use of quantum phenomena for learning systems, the use of quantum computers for learning on quantum data and how machine learning algorithms and software can be formulated and implemented on quantum computers. Quantum machine learning can have a transformational effect on computer science. It may speed up the processing of information well beyond the existing classical speeds. Recent work has seen the development of quantum algorithms that could serve as foundations for machine learning applications. Despite its great promise, there are still significant hardware and software challenges that need to be resolved before quantum machine learning becomes practical. In this paper, we present an overview of quantum machine learning in the light of classical approaches. Departing from foundational concepts of machine learning and quantum computing, we discuss various technical contributions, strengths and similarities of the research work in this domain. We also elaborate upon the recent progress of different quantum machine learning approaches, their complexity, and applications in various fields such as physics, chemistry and natural language processing.} }
@article{WOS:000601128500001, title = {Supervised machine learning tools: a tutorial for clinicians}, journal = {JOURNAL OF NEURAL ENGINEERING}, volume = {17}, year = {2020}, issn = {1741-2560}, doi = {10.1088/1741-2552/abbff2}, author = {Lo Vercio, Lucas and Amador, Kimberly and Bannister, Jordan J. and Crites, Sebastian and Gutierrez, Alejandro and MacDonald, M. Ethan and Moore, Jasmine and Mouches, Pauline and Rajashekar, Deepthi and Schimert, Serena and Subbanna, Nagesh and Tuladhar, Anup and Wang, Nanjia and Wilms, Matthias and Winder, Anthony and Forkert, Nils D.}, abstract = {In an increasingly data-driven world, artificial intelligence is expected to be a key tool for converting big data into tangible benefits and the healthcare domain is no exception to this. Machine learning aims to identify complex patterns in multi-dimensional data and use these uncovered patterns to classify new unseen cases or make data-driven predictions. In recent years, deep neural networks have shown to be capable of producing results that considerably exceed those of conventional machine learning methods for various classification and regression tasks. In this paper, we provide an accessible tutorial of the most important supervised machine learning concepts and methods, including deep learning, which are potentially the most relevant for the medical domain. We aim to take some of the mystery out of machine learning and depict how machine learning models can be useful for medical applications. Finally, this tutorial provides a few practical suggestions for how to properly design a machine learning model for a generic medical problem.} }
@article{WOS:000530832200187, title = {Explainability of a Machine Learning Granting Scoring Model in Peer-to-Peer Lending}, journal = {IEEE ACCESS}, volume = {8}, pages = {64873-64890}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2984412}, author = {Janny Ariza-Garzon, Miller and Arroyo, Javier and Caparrini, Antonio and Segovia-Vargas, Maria-Jesus}, abstract = {Peer-to-peer (P2P) lending demands effective and explainable credit risk models. Typical machine learning algorithms offer high prediction performance, but most of them lack explanatory power. However, this deficiency can be solved with the help of the explainability tools proposed in the last few years, such as the SHAP values. In this work, we assess the well-known logistic regression model and several machine learning algorithms for granting scoring in P2P lending. The comparison reveals that the machine learning alternative is superior in terms of not only classification performance but also explainability. More precisely, the SHAP values reveal that machine learning algorithms can reflect dispersion, nonlinearity and structural breaks in the relationships between each feature and the target variable. Our results demonstrate that is possible to have machine learning credit scoring models be both accurate and transparent. Such models provide the trust that the industry, regulators and end-users demand in P2P lending and may lead to a wider adoption of machine learning in this and other risk assessment applications where explainability is required.} }
@article{WOS:000626551700002, title = {R.ROSETTA: an interpretable machine learning framework}, journal = {BMC BIOINFORMATICS}, volume = {22}, year = {2021}, issn = {1471-2105}, doi = {10.1186/s12859-021-04049-z}, author = {Garbulowski, Mateusz and Diamanti, Klev and Smolinska, Karolina and Baltzer, Nicholas and Stoll, Patricia and Bornelov, Susanne and Ohrn, Aleksander and Feuk, Lars and Komorowski, Jan}, abstract = {BackgroundMachine learning involves strategies and algorithms that may assist bioinformatics analyses in terms of data mining and knowledge discovery. In several applications, viz. in Life Sciences, it is often more important to understand how a prediction was obtained rather than knowing what prediction was made. To this end so-called interpretable machine learning has been recently advocated. In this study, we implemented an interpretable machine learning package based on the rough set theory. An important aim of our work was provision of statistical properties of the models and their components.ResultsWe present the R.ROSETTA package, which is an R wrapper of ROSETTA framework. The original ROSETTA functions have been improved and adapted to the R programming environment. The package allows for building and analyzing non-linear interpretable machine learning models. R.ROSETTA gathers combinatorial statistics via rule-based modelling for accessible and transparent results, well-suited for adoption within the greater scientific community. The package also provides statistics and visualization tools that facilitate minimization of analysis bias and noise. The R.ROSETTA package is freely available at https://github.com/komorowskilab/R.ROSETTA. To illustrate the usage of the package, we applied it to a transcriptome dataset from an autism case-control study. Our tool provided hypotheses for potential co-predictive mechanisms among features that discerned phenotype classes. These co-predictors represented neurodevelopmental and autism-related genes.ConclusionsR.ROSETTA provides new insights for interpretable machine learning analyses and knowledge-based systems. We demonstrated that our package facilitated detection of dependencies for autism-related genes. Although the sample application of R.ROSETTA illustrates transcriptome data analysis, the package can be used to analyze any data organized in decision tables.} }
@article{WOS:000546625200003, title = {Machine Learning and Natural Language Processing in Psychotherapy Research: Alliance as Example Use Case}, journal = {JOURNAL OF COUNSELING PSYCHOLOGY}, volume = {67}, pages = {438-448}, year = {2020}, issn = {0022-0167}, doi = {10.1037/cou0000382}, author = {Goldberg, Simon B. and Flemotomos, Nikolaos and Martinez, Victor R. and Tanana, Michael J. and Kuo, Patty B. and Pace, Brian T. and Villatte, Jennifer L. and Georgiou, Panayiotis G. and Van Epps, Jake and Imel, Zac E. and Narayanan, Shrikanth S. and Atkins, David C.}, abstract = {Artificial intelligence generally and machine learning specifically have become deeply woven into the lives and technologies of modern life. Machine learning is dramatically changing scientific research and industry and may also hold promise for addressing limitations encountered in mental health care and psychotherapy. The current paper introduces machine learning and natural language processing as related methodologies that may prove valuable for automating the assessment of meaningful aspects of treatment. Prediction of therapeutic alliance from session recordings is used as a case in point. Recordings from 1,235 sessions of 386 clients seen by 40 therapists at a university counseling center were processed using automatic speech recognition software. Machine learning algorithms learned associations between client ratings of therapeutic alliance exclusively from session linguistic content. Using a portion of the data to train the model, machine learning algorithms modestly predicted alliance ratings from session content in an independent test set (Spearman's rho =15, p <.001). These results highlight the potential to harness natural language processing and machine learning to predict a key psychotherapy process variable that is relatively distal from linguistic content. Six practical suggestions for conducting psychotherapy research using machine learning are presented along with several directions for future research. Questions of dissemination and implementation may be particularly important to explore as machine learning improves in its ability to automate assessment of psychotherapy process and outcome.} }
@article{WOS:000545164100001, title = {Machine learning and artificial intelligence in haematology}, journal = {BRITISH JOURNAL OF HAEMATOLOGY}, volume = {192}, pages = {239-250}, year = {2021}, issn = {0007-1048}, doi = {10.1111/bjh.16915}, author = {Shouval, Roni and Fein, Joshua A. and Savani, Bipin and Mohty, Mohamad and Nagler, Arnon}, abstract = {Digitalization of the medical record and integration of genomic methods into clinical practice have resulted in an unprecedented wealth of data. Machine learning is a subdomain of artificial intelligence that attempts to computationally extract meaningful insights from complex data structures. Applications of machine learning in haematological scenarios are steadily increasing. However, basic concepts are often unfamiliar to clinicians and investigators. The purpose of this review is to provide readers with tools to interpret and critically appraise machine learning literature. We begin with the elucidation of standard terminology and then review examples in haematology. Guidelines for designing and evaluating machine-learning studies are provided. Finally, we discuss limitations of the machine-learning approach.} }
@article{WOS:000595502700002, title = {A primer for understanding radiology articles about machine learning and deep learning}, journal = {DIAGNOSTIC AND INTERVENTIONAL IMAGING}, volume = {101}, pages = {765-770}, year = {2020}, issn = {2211-5684}, doi = {10.1016/j.diii.2020.10.001}, author = {Nakaura, Takeshi and Higaki, Toru and Awai, Kazuo and Ikeda, Osamu and Yamashita, Yasuyuki}, abstract = {The application of machine learning and deep learning in the field of imaging is rapidly growing. Although the principles of machine and deep learning are unfamiliar to the majority of clinicians, the basics are not so complicated. One of the major issues is that commentaries written by experts are difficult to understand, and are not primarily written for clinicians. The purpose of this article was to describe the different concepts behind machine learning, radiomics, and deep learning to make clinicians more familiar with these techniques. (C) 2020 Societe francaise de radiologie. Published by Elsevier Masson SAS. All rights reserved.} }
@article{WOS:000544758900009, title = {Machine learning in haematological malignancies}, journal = {LANCET HAEMATOLOGY}, volume = {7}, pages = {E541-E550}, year = {2020}, issn = {2352-3026}, author = {Radakovich, Nathan and Nagy, Matthew and Nazha, Aziz}, abstract = {Machine learning is a branch of computer science and statistics that generates predictive or descriptive models by learning from training data rather than by being rigidly programmed. It has attracted substantial attention for its many applications in medicine, both as a catalyst for research and as a means of improving clinical care across the cycle of diagnosis, prognosis, and treatment of disease. These applications include the management of haematological malignancy, in which machine learning has created inroads in pathology, radiology, genomics, and the analysis of electronic health record data. As computational power becomes cheaper and the tools for implementing machine learning become increasingly democratised, it is likely to become increasingly integrated into the research and practice landscape of haematology. As such, machine learning merits understanding and attention from researchers and clinicians alike. This narrative Review describes important concepts in machine learning for unfamiliar readers, details machine learning?s current applications in haematological malignancy, and summarises important concepts for clinicians to be aware of when appraising research that uses machine learning.} }
@article{WOS:000518473500027, title = {Expert-augmented machine learning}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {117}, pages = {4571-4577}, year = {2020}, issn = {0027-8424}, doi = {10.1073/pnas.1906831117}, author = {Gennatas, Efstathios D. and Friedman, Jerome H. and Ungar, Lyle H. and Pirracchio, Romain and Eaton, Eric and Reichmann, Lara G. and Interian, Yannet and Luna, Jose Marcio and Simone, II, Charles B. and Auerbach, Andrew and Delgado, Elier and van der Laan, Mark J. and Solberg, Timothy D. and Valdes, Gilmer}, abstract = {Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.} }
@article{WOS:000568280500001, title = {The Role of Machine Learning in Spine Surgery: The Future Is Now}, journal = {FRONTIERS IN SURGERY}, volume = {7}, year = {2020}, issn = {2296-875X}, doi = {10.3389/fsurg.2020.00054}, author = {Chang, Michael and Canseco, Jose A. and Nicholson, Kristen J. and Patel, Neil and Vaccaro, Alexander R.}, abstract = {The recent influx of machine learning centered investigations in the spine surgery literature has led to increased enthusiasm as to the prospect of using artificial intelligence to create clinical decision support tools, optimize postoperative outcomes, and improve technologies used in the operating room. However, the methodology underlying machine learning in spine research is often overlooked as the subject matter is quite novel and may be foreign to practicing spine surgeons. Improper application of machine learning is a significant bioethics challenge, given the potential consequences of over- or underestimating the results of such studies for clinical decision-making processes. Proper peer review of these publications requires a baseline familiarity of the language associated with machine learning, and how it differs from classical statistical analyses. This narrative review first introduces the overall field of machine learning and its role in artificial intelligence, and defines basic terminology. In addition, common modalities for applying machine learning, including classification and regression decision trees, support vector machines, and artificial neural networks are examined in the context of examples gathered from the spine literature. Lastly, the ethical challenges associated with adapting machine learning for research related to patient care, as well as future perspectives on the potential use of machine learning in spine surgery, are discussed specifically.} }
@article{WOS:000888566100001, title = {Moving towards reproducible machine learning}, journal = {NATURE COMPUTATIONAL SCIENCE}, volume = {1}, pages = {629-630}, year = {2021}, doi = {10.1038/s43588-021-00152-6}, author = {[Anonymous]}, abstract = {We provide some recommendations on how to report machine learning-based research in order to improve transparency and reproducibility.} }
@article{WOS:000762389600001, title = {Machine learning in tutorials - Universal applicability, underinformed application, and other misconceptions}, journal = {BIG DATA \\& SOCIETY}, volume = {8}, year = {2021}, issn = {2053-9517}, doi = {10.1177/20539517211017593}, author = {Heuer, Hendrik and Jarke, Juliane and Breiter, Andreas}, abstract = {Machine learning has become a key component of contemporary information systems. Unlike prior information systems explicitly programmed in formal languages, ML systems infer rules from data. This paper shows what this difference means for the critical analysis of socio-technical systems based on machine learning. To provide a foundation for future critical analysis of machine learning-based systems, we engage with how the term is framed and constructed in self-education resources. For this, we analyze machine learning tutorials, an important information source for self-learners and a key tool for the formation of the practices of the machine learning community. Our analysis identifies canonical examples of machine learning as well as important misconceptions and problematic framings. Our results show that machine learning is presented as being universally applicable and that the application of machine learning without special expertise is actively encouraged. Explanations of machine learning algorithms are missing or strongly limited. Meanwhile, the importance of data is vastly understated. This has implications for the manifestation of (new) social inequalities through machine learning-based systems.} }
@article{WOS:000726375800010, title = {Application of Machine Learning in Animal Disease Analysis and Prediction}, journal = {CURRENT BIOINFORMATICS}, volume = {16}, pages = {972-982}, year = {2021}, issn = {1574-8936}, doi = {10.2174/1574893615999200728195613}, author = {Zhang, Shuwen and Su, Qiang and Chen, Qin}, abstract = {Major animal diseases pose a great threat to animal husbandry and human beings. With the deepening of globalization and the abundance of data resources, the prediction and analysis of animal diseases by using big data are becoming more and more important. The focus of machine learning is to make computers how to learn from data and use the learned experience to analyze and predict. Firstly, this paper introduces the animal epidemic situation and machine learning. Then it briefly introduces the application of machine learning in animal disease analysis and prediction. Machine learning is mainly divided into supervised learning and unsupervised learning. Supervised learning includes support vector machines, naive bayes, decision trees, random forests, logistic regression, artificial neural networks, deep learning, and AdaBoost. Unsupervised learning has maximum expectation algorithm, principal component analysis hierarchical clustering algorithm and maxent. Through the discussion of this paper, people have a clearer concept of machine learning and an understanding of its application prospect in animal diseases.} }
@article{WOS:000503916400004, title = {Machine Learning Principles for Radiology Investigators}, journal = {ACADEMIC RADIOLOGY}, volume = {27}, pages = {13-25}, year = {2020}, issn = {1076-6332}, doi = {10.1016/j.acra.2019.07.030}, author = {Borstelmann, Stephen M.}, abstract = {Artificial intelligence and deep learning are areas of high interest for radiology investigators at present. However, the field of machine learning encompasses multiple statistics-based techniques useful for investigators, which may be complementary to deep learning approaches. After a refresher in basic statistical concepts, relevant considerations for machine learning practitioners are reviewed: regression, classification, decision boundaries, and bias-variance tradeoff. Regularization, ground truth, and populations are discussed along with compute and data management principles. Advanced statistical machine learning techniques including bootstrapping, bagging, boosting, decision trees, random forest, XGboost, and support vector machines are reviewed along with relevant examples from the radiology literature.} }
@article{WOS:000590917300001, title = {Machine learning for human learners: opportunities, issues, tensions and threats}, journal = {ETR\\&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT}, volume = {69}, pages = {2109-2130}, year = {2021}, issn = {1042-1629}, doi = {10.1007/s11423-020-09858-2}, author = {Webb, Mary E. and Fluck, Andrew and Magenheim, Johannes and Malyn-Smith, Joyce and Waters, Juliet and Deschenes, Michelle and Zagami, Jason}, abstract = {Machine learning systems are infiltrating our lives and are beginning to become important in our education systems. This article, developed from a synthesis and analysis of previous research, examines the implications of recent developments in machine learning for human learners and learning. In this article we first compare deep learning in computers and humans to examine their similarities and differences. Deep learning is identified as a sub-set of machine learning, which is itself a component of artificial intelligence. Deep learning often depends on backwards propagation in weighted neural networks, so is non-deterministic-the system adapts and changes through practical experience or training. This adaptive behaviour predicates the need for explainability and accountability in such systems. Accountability is the reverse of explainability. Explainability flows through the system from inputs to output (decision) whereas accountability flows backwards, from a decision to the person taking responsibility for it. Both explainability and accountability should be incorporated in machine learning system design from the outset to meet social, ethical and legislative requirements. For students to be able to understand the nature of the systems that may be supporting their own learning as well as to act as responsible citizens in contemplating the ethical issues that machine learning raises, they need to understand key aspects of machine learning systems and have opportunities to adapt and create such systems. Therefore, some changes are needed to school curricula. The article concludes with recommendations about machine learning for teachers, students, policymakers, developers and researchers.} }
@article{WOS:000519572500022, title = {The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research}, journal = {COMPUTATIONAL MATERIALS SCIENCE}, volume = {176}, year = {2020}, issn = {0927-0256}, doi = {10.1016/j.commatsci.2020.109544}, author = {Jacobs, Ryan and Mayeshiba, Tam and Afflerbach, Ben and Miles, Luke and Williams, Max and Turner, Matthew and Finkel, Raphael and Morgan, Dane}, abstract = {As data science and machine learning methods are taking on an increasingly important role in the materials research community, there is a need for the development of machine learning software tools that are easy to use (even for nonexperts with no programming ability), provide flexible access to the most important algorithms, and codify best practices of machine learning model development and evaluation. Here, we introduce the Materials Simulation Toolkit for Machine Learning (MAST-ML), an open source Python-based software package designed to broaden and accelerate the use of machine learning in materials science research. MAST-ML provides predefined routines for many input setup, model fitting, and post-analysis tasks, as well as a simple structure for executing a multi-step machine learning model workflow. In this paper, we describe how MAST-ML is used to streamline and accelerate the execution of machine learning problems. We walk through how to acquire and run MAST-ML, demonstrate how to execute different components of a supervised machine learning workflow via a customized input file, and showcase a number of features and analyses conducted automatically during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by showcasing examples of recent materials informatics studies which used MAST-ML to formulate and evaluate various machine learning models for an array of materials applications. Finally, we lay out a vision of how MAST-ML, together with complementary software packages and emerging cyberinfrastructure, can advance the rapidly growing field of materials informatics, with a focus on producing machine learning models easily, reproducibly, and in a manner that facilitates model evolution and improvement in the future.} }
@article{WOS:000592624200002, title = {Machine Learning and Computational Mathematics}, journal = {COMMUNICATIONS IN COMPUTATIONAL PHYSICS}, volume = {28}, pages = {1639-1670}, year = {2020}, issn = {1815-2406}, doi = {10.4208/cicp.OA-2020-0185}, author = {Weinan, E.}, abstract = {Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ``black box'' type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, can impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.} }
@article{WOS:000572456900001, title = {Learning from Machine Learning in Accounting and Assurance}, journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING}, volume = {17}, pages = {1-10}, year = {2020}, issn = {1554-1908}, doi = {10.2308/jeta-10718}, author = {Cho, Soohyun and Vasarhelyi, Miklos A. and Sun, Ting (Sophia) and Zhang, Chanyuan (Abigail)}, abstract = {Machine learning is a subset of artificial intelligence, and it is a computational method that learns patterns from large and complex data. The learning processes enable us to make predictions for future events. In the accounting and assurance profession, machine learning is gradually being applied to various tasks like reviewing source documents, analyzing business transactions or activities, and assessing risks. In academic research, machine learning has been used to make predictions of fraud, bankruptcy, material misstatements, and accounting estimates. More importantly, machine learning is generating awareness about the inductive reasoning methodology, which has long been undervalued in the mainstream of academic research in accounting and auditing. The use of machine learning in accounting/auditing research and practice is also raising concerns about its potential bias and ethical implications. Therefore, this editorial aims to call the readers' attention to these issues and encourage scholars to perform research in this domain.} }
@article{WOS:000522553100014, title = {Research on radar signal recognition based on automatic machine learning}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {32}, pages = {1959-1969}, year = {2020}, issn = {0941-0643}, doi = {10.1007/s00521-019-04494-1}, author = {Li, Peng}, abstract = {With the advancement of machine learning and radar technology, machine learning is becoming more and more widely used in the field of radar. Radar scanning, signal acquisition and processing, one-dimensional range image, radar SAR, ISAR image recognition, radar tracking and guidance are all integrated into machine learning technology, but machine learning technology relies heavily on human machine learning experts for radar signal recognition. In order to realize the automation of radar signal recognition by machine learning, this paper proposes an automatic machine learning AUTO-SKLEARN system and applies it to radar radiation source signals. Identification: Firstly, this paper briefly introduces the classification of traditional machine learning algorithms and the types of algorithms specifically included in each type of algorithm. On this basis, the machine learning Bayesian algorithm is introduced. Secondly, the automatic machine learning AUTO based on Bayesian algorithm is proposed. -SKLEARN system, elaborates the process of AUTO-SKLEARN system in solving automatic selection algorithm and hyperparameter optimization, including meta-learning and its program implementation and automatic model integration construction. Finally, this paper introduces the process of automatic machine learning applied to radar emitter signal recognition. Through data simulation and experiment, the effect of traditional machine learning k-means algorithm and automatic machine learning AUTO-SKLEARN system in radar signal recognition is compared, which shows that automatic machine learning is feasible for radar signal recognition. The automatic machine learning AUTO-SKLEARN system can significantly improve the accuracy of the radar emitter signal recognition process, and the scheme is more reliable in signal recognition stability.} }
@article{WOS:000579583300001, title = {Robustness Evaluations of Sustainable Machine Learning Models against Data Poisoning Attacks in the Internet of Things}, journal = {SUSTAINABILITY}, volume = {12}, year = {2020}, doi = {10.3390/su12166434}, author = {Dunn, Corey and Moustafa, Nour and Turnbull, Benjamin}, abstract = {With the increasing popularity of the Internet of Things (IoT) platforms, the cyber security of these platforms is a highly active area of research. One key technology underpinning smart IoT systems is machine learning, which classifies and predicts events from large-scale data in IoT networks. Machine learning is susceptible to cyber attacks, particularly data poisoning attacks that inject false data when training machine learning models. Data poisoning attacks degrade the performances of machine learning models. It is an ongoing research challenge to develop trustworthy machine learning models resilient and sustainable against data poisoning attacks in IoT networks. We studied the effects of data poisoning attacks on machine learning models, including the gradient boosting machine, random forest, naive Bayes, and feed-forward deep learning, to determine the levels to which the models should be trusted and said to be reliable in real-world IoT settings. In the training phase, a label modification function is developed to manipulate legitimate input classes. The function is employed at data poisoning rates of 5\\%, 10\\%, 20\\%, and 30\\% that allow the comparison of the poisoned models and display their performance degradations. The machine learning models have been evaluated using the ToN\_IoT and UNSW NB-15 datasets, as they include a wide variety of recent legitimate and attack vectors. The experimental results revealed that the models' performances will be degraded, in terms of accuracy and detection rates, if the number of the trained normal observations is not significantly larger than the poisoned data. At the rate of data poisoning of 30\\% or greater on input data, machine learning performances are significantly degraded.} }
@article{WOS:000722548300001, title = {Machine Learning and Small Data}, journal = {EDUCATIONAL MEASUREMENT-ISSUES AND PRACTICE}, volume = {40}, pages = {8-12}, year = {2021}, issn = {0731-1745}, doi = {10.1111/emip.12472}, author = {Cui, Zhongmin}, abstract = {Commonly used machine learning applications seem to relate to big data. This article provides a gentle review of machine learning and shows why machine learning can be applied to small data too. An example of applying machine learning to screen irregularity reports is presented. In the example, the support vector machine and multinomial naive Bayes methods were used and compared. The performance of machine learning was compared to human experts in terms of flagging records to be excluded from equating. The application of machine learning seemed to be successful, although the data only consisted of a couple of thousand records. Recommendations in using machine learning are provided.} }
@article{WOS:000518547500020, title = {What is Machine Learning? A Primer for the Epidemiologist}, journal = {AMERICAN JOURNAL OF EPIDEMIOLOGY}, volume = {188}, pages = {2222-2239}, year = {2019}, issn = {0002-9262}, doi = {10.1093/aje/kwz189}, author = {Bi, Qifang and Goodman, Katherine E. and Kaminsky, Joshua and Lessler, Justin}, abstract = {Machine learning is a branch of computer science that has the potential to transform epidemiologic sciences. Amid a growing focus on ``Big Data,'' it offers epidemiologists new tools to tackle problems for which classical methods are not well-suited. In order to critically evaluate the value of integrating machine learning algorithms and existing methods, however, it is essential to address language and technical barriers between the two fields that can make it difficult for epidemiologists to read and assess machine learning studies. Here, we provide an overview of the concepts and terminology used in machine learning literature, which encompasses a diverse set of tools with goals ranging from prediction to classification to clustering. We provide a brief introduction to 5 common machine learning algorithms and 4 ensemble-based approaches. We then summarize epidemiologic applications of machine learning techniques in the published literature. We recommend approaches to incorporate machine learning in epidemiologic research and discuss opportunities and challenges for integrating machine learning and existing epidemiologic research methods.} }
@article{WOS:000660871100001, title = {Toward a theory of machine learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abe6d7}, author = {Vanchurin, Vitaly}, abstract = {We define a neural network as a septuple consisting of (1) a state vector, (2) an input projection, (3) an output projection, (4) a weight matrix, (5) a bias vector, (6) an activation map and (7) a loss function. We argue that the loss function can be imposed either on the boundary (i.e. input and/or output neurons) or in the bulk (i.e. hidden neurons) for both supervised and unsupervised systems. We apply the principle of maximum entropy to derive a canonical ensemble of the state vectors subject to a constraint imposed on the bulk loss function by a Lagrange multiplier (or an inverse temperature parameter). We show that in an equilibrium the canonical partition function must be a product of two factors: a function of the temperature, and a function of the bias vector and weight matrix. Consequently, the total Shannon entropy consists of two terms which represent, respectively, a thermodynamic entropy and a complexity of the neural network. We derive the first and second laws of learning: during learning the total entropy must decrease until the system reaches an equilibrium (i.e. the second law), and the increment in the loss function must be proportional to the increment in the thermodynamic entropy plus the increment in the complexity (i.e. the first law). We calculate the entropy destruction to show that the efficiency of learning is given by the Laplacian of the total free energy, which is to be maximized in an optimal neural architecture, and explain why the optimization condition is better satisfied in a deep network with a large number of hidden layers. The key properties of the model are verified numerically by training a supervised feedforward neural network using the stochastic gradient descent method. We also discuss a possibility that the entire Universe at its most fundamental level is a neural network.} }
@article{WOS:000563451300001, title = {A systematic review of fuzzing based on machine learning techniques}, journal = {PLOS ONE}, volume = {15}, year = {2020}, issn = {1932-6203}, doi = {10.1371/journal.pone.0237749}, author = {Wang, Yan and Jia, Peng and Liu, Luping and Huang, Cheng and Liu, Zhonglin}, abstract = {Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzz testing faces many challenges, such as how to mutate input seed files, how to increase code coverage, and how to bypass the format verification effectively. Therefore machine learning techniques have been introduced as a new method into fuzz testing to alleviate these challenges. This paper reviews the research progress of using machine learning techniques for fuzz testing in recent years, analyzes how machine learning improves the fuzzing process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies five different stages in which machine learning has been used. Then this paper systematically studies machine learning-based fuzzing models from five dimensions of selection of machine learning algorithms, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Secondly, this paper assesses the performance of the machine learning techniques in existing research for fuzz testing. The results of the evaluation prove that machine learning techniques have an acceptable capability of prediction for fuzzing. Finally, the capability of discovering vulnerabilities both traditional fuzzers and machine learning-based fuzzers is analyzed. The results depict that the introduction of machine learning techniques can improve the performance of fuzzing. We hope to provide researchers with a systematic and more in-depth understanding of fuzzing based on machine learning techniques and provide some references for this field through analysis and summarization of multiple dimensions.} }
@article{WOS:000600895900004, title = {Machine learning and its applications in plant molecular studies}, journal = {BRIEFINGS IN FUNCTIONAL GENOMICS}, volume = {19}, pages = {40-48}, year = {2020}, issn = {2041-2649}, doi = {10.1093/bfgp/elz036}, author = {Sun, Shanwen and Wang, Chunyu and Ding, Hui and Zou, Quan}, abstract = {The advent of high-throughput genomic technologies has resulted in the accumulation of massive amounts of genomic information. However, biologists are challenged with how to effectively analyze these data. Machine learning can provide tools for better and more efficient data analysis. Unfortunately, because many plant biologists are unfamiliar with machine learning, its application in plant molecular studies has been restricted to a few species and a limited set of algorithms. Thus, in this study, we provide the basic steps for developing machine learning frameworks and present a comprehensive overview of machine learning algorithms and various evaluation metrics. Furthermore, we introduce sources of important curated plant genomic data and R packages to enable plant biologists to easily and quickly apply appropriate machine learning algorithms in their research. Finally, we discuss current applications of machine learning algorithms for identifying various genes related to resistance to biotic and abiotic stress. Broad application of machine learning and the accumulation of plant sequencing data will advance plant molecular studies.} }
@article{WOS:000577397300001, title = {The use of machine learning and deep learning algorithms in functional magnetic resonance imaging-A systematic review}, journal = {EXPERT SYSTEMS}, volume = {37}, year = {2020}, issn = {0266-4720}, doi = {10.1111/exsy.12644}, author = {Rashid, Mamoon and Singh, Harjeet and Goyal, Vishal}, abstract = {Functional Magnetic Resonance Imaging (fMRI) is presently one of the most popular techniques for analysing the dynamic states in brain images using various kinds of algorithms. From the last decade, there is an exponential rise in the use of the machine and deep learning algorithms of artificial intelligence for analysing fMRI data. However, it is a big challenge for every researcher to choose a suitable machine or deep learning algorithm for analysing fMRI data due to the availability of a large number of algorithms in the literature. It takes much time for each researcher to know about the various approaches and algorithms which are in use for fMRI data. This paper provides a review in a systematic manner for the present literature of fMRI data that makes use of the machine and deep learning algorithms. The major goals of this review paper are to (a) identify machine learning and deep learning research trends for the implementation of fMRI; (b) identify usage of Machine Learning Algorithms and deep learning in fMRI, and (c) help new researchers based on fMRI to put their new findings appropriately in existing domain of fMRI research. The results of this systematic review identified various fMRI studies and classified them based on fMRI types, mental diseases, use of machine learning and deep learning algorithms. The authors have provided the studies with the best performance of machine learning and deep learning algorithms used in fMRI. The authors believe that this systematic review will help incoming researchers on fMRI in their future works.} }
@article{WOS:000437004000009, title = {Next-Generation Machine Learning for Biological Networks}, journal = {CELL}, volume = {173}, pages = {1581-1592}, year = {2018}, issn = {0092-8674}, doi = {10.1016/j.cell.2018.05.015}, author = {Camacho, Diogo M. and Collins, Katherine M. and Powers, Rani K. and Costello, James C. and Collins, James J.}, abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology.} }
@article{WOS:000641958000001, title = {Applications for Machine Learning}, journal = {IEEE SYSTEMS MAN AND CYBERNETICS MAGAZINE}, volume = {7}, pages = {3}, year = {2021}, issn = {2380-1298}, doi = {10.1109/MSMC.2021.3058718}, author = {Nahavandi, Saeid}, abstract = {In this issue of IEEE Systems, Man, and Cybernetics Magazine, four articles are presented that relate to the fascinating topic of machine learning and its application for real-world systems.} }
@article{WOS:000691881100022, title = {Training analysis of optimization models in machine learning}, journal = {INTERNATIONAL JOURNAL OF NONLINEAR ANALYSIS AND APPLICATIONS}, volume = {12}, pages = {1453-1461}, year = {2021}, issn = {2008-6822}, doi = {10.22075/ijnaa.2021.5261}, author = {Alridha, Ahmed and Wahbi, Fadhil Abdalhasan and Kadhim, Mazin Kareem}, abstract = {Machine learning is fast evolving, with numerous theoretical advances and applications in a variety of domains. In reality, most machine learning algorithms are based on optimization issues. This interaction is also explored in the special topic on machine learning and large-scale optimization. Furthermore, machine learning optimization issues have several unique characteristics that are rarely seen in other optimization contexts. Aside from that, the notions of classical optimization vs machine learning will be discussed. Finally, this study will give an outline of these peculiar aspects of machine learning optimization.} }
@article{WOS:000453280300005, title = {Machine Learning Methods for Histopathological Image Analysis}, journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL}, volume = {16}, pages = {34-42}, year = {2018}, issn = {2001-0370}, doi = {10.1016/j.csbj.2018.01.001}, author = {Komura, Daisuke and Ishikawa, Shumpei}, abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to suchanalysis, and propose possible solutions. (C) 2018 Komura, Ishikawa. Published by Elsevier B.V. on behalf of the Research Network of Computational and Structural Biotechnology.} }
@article{WOS:000559782300004, title = {Julia language in machine learning: Algorithms, applications, and open issues}, journal = {COMPUTER SCIENCE REVIEW}, volume = {37}, year = {2020}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2020.100254}, author = {Gao, Kaifeng and Mei, Gang and Piccialli, Francesco and Cuomo, Salvatore and Tu, Jingzhi and Huo, Zenan}, abstract = {Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the applications of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning. (c) 2020 The Authors. Published by Elsevier Inc.} }
@article{WOS:000489358200007, title = {Estimation of energy consumption in machine learning}, journal = {JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING}, volume = {134}, pages = {75-88}, year = {2019}, issn = {0743-7315}, doi = {10.1016/j.jpdc.2019.07.007}, author = {Garcia-Martin, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Hakan}, abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning. (C) 2019 The Authors. Published by Elsevier Inc.} }
@article{WOS:000595025900001, title = {Quantum Driven Machine Learning}, journal = {INTERNATIONAL JOURNAL OF THEORETICAL PHYSICS}, volume = {59}, pages = {4013-4024}, year = {2020}, issn = {0020-7748}, doi = {10.1007/s10773-020-04656-1}, author = {Saini, Shivani and Khosla, P. K. and Kaur, Manjit and Singh, Gurmohan}, abstract = {Quantum computing is proving to be very beneficial for solving complex machine learning problems. Quantum computers are inherently excellent in handling and manipulating vectors and matrix operations. The ever increasing size of data has started creating bottlenecks for classical machine learning systems. Quantum computers are emerging as potential solutions to tackle big data related problems. This paper presents a quantum machine learning model based on quantum support vector machine (QSVM) algorithm to solve a classification problem. The quantum machine learning model is practically implemented on quantum simulators and real-time superconducting quantum processors. The performance of quantum machine learning model is computed in terms of processing speed and accuracy and compared against its classical counterpart. The breast cancer dataset is used for the classification problem. The results are indicative that quantum computers offer quantum speed-up.} }
@article{WOS:000848787000001, title = {Learning dynamics from large biological data sets: Machine learning meets systems biology}, journal = {CURRENT OPINION IN SYSTEMS BIOLOGY}, volume = {22}, pages = {1-7}, year = {2020}, issn = {2452-3100}, doi = {10.1016/j.coisb.2020.07.009}, author = {Gilpin, William and Huang, Yitong and Forger, Daniel B.}, abstract = {In the past few decades, mathematical models based on dynamical systems theory have provided new insight into diverse biological systems. In this review, we ask whether the recent success of machine learning techniques for large-scale biological data analysis provides a complementary or competing approach to more traditional modeling approaches. Recent applications of machine learning to the problem of learning biological dynamics in diverse systems range from neuroscience to animal behavior. We compare the underlying mechanisms and limitations of traditional dynamical models with those of machine learning models. We highlight the unique role that traditional modeling has played in providing predictive insights into biological systems, and we propose several avenues for bridging traditional dynamical systems theory with large-scale analysis enabled by machine learning.} }
@article{WOS:000468930900005, title = {Machine Learning Made Easy: A Review of Scikit-learn Package in Python Programming Language}, journal = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS}, volume = {44}, pages = {348-361}, year = {2019}, issn = {1076-9986}, doi = {10.3102/1076998619832248}, author = {Hao, Jiangang and Ho, Tin Kam}, abstract = {Machine learning is a popular topic in data analysis and modeling. Many different machine learning algorithms have been developed and implemented in a variety of programming languages over the past 20 years. In this article, we first provide an overview of machine learning and clarify its difference from statistical inference. Then, we review Scikit-learn, a machine learning package in the Python programming language that is widely used in data science. The Scikit-learn package includes implementations of a comprehensive list of machine learning methods under unified data and modeling procedure conventions, making it a convenient toolkit for educational and behavior statisticians.} }
@article{WOS:000543431000001, title = {Benchmark AFLOW Data Sets for Machine Learning}, journal = {INTEGRATING MATERIALS AND MANUFACTURING INNOVATION}, volume = {9}, pages = {153-156}, year = {2020}, issn = {2193-9764}, doi = {10.1007/s40192-020-00174-4}, author = {Clement, Conrad L. and Kauwe, Steven K. and Sparks, Taylor D.}, abstract = {Materials informatics is increasingly finding ways to exploit machine learning algorithms. Techniques such as decision trees, ensemble methods, support vector machines, and a variety of neural network architectures are used to predict likely material characteristics and property values. Supplemented with laboratory synthesis, applications of machine learning to compound discovery and characterization represent one of the most promising research directions in materials informatics. A shortcoming of this trend, in its current form, is a lack of standardized materials data sets on which to train, validate, and test model effectiveness. Applied machine learning research depends on benchmark data to make sense of its results. Fixed, predetermined data sets allow for rigorous model assessment and comparison. Machine learning publications that do not refer to benchmarks are often hard to contextualize and reproduce. In this data descriptor article, we present a collection of data sets of different material properties taken from the AFLOW database. We describe them, the procedures that generated them, and their use as potential benchmarks. We provide a compressed ZIP file containing the data sets and a GitHub repository of associated Python code. Finally, we discuss opportunities for future work incorporating the data sets and creating similar benchmark collections.} }
@article{WOS:000591678300009, title = {Macroeconomic forecasting using factor models and machine learning: an application to Japan}, journal = {JOURNAL OF THE JAPANESE AND INTERNATIONAL ECONOMIES}, volume = {58}, year = {2020}, issn = {0889-1583}, doi = {10.1016/j.jjie.2020.101104}, author = {Maehashi, Kohei and Shintani, Mototsugu}, abstract = {We perform a thorough comparative analysis of factor models and machine learning to forecast Japanese macroeconomic time series. Our main results can be summarized as follows. First, in many instances, factor models and machine learning perform better than the conventional AR model. Second, predictions made by machine learning methods perform particularly well for medium to long forecast horizons. Third, the success of machine learning mainly comes from the nonlinearity and interaction of variables, which suggests the importance of nonlinear structure in predicting the Japanese macroeconomic series. Fourth, the composite forecast of factor models and machine learning performs better than factor models or machine learning alone; and machine learning methods applied to common factors are found to be useful in the composite forecast.} }
@article{WOS:000895964000001, title = {Survey on Lie Group Machine Learning}, journal = {BIG DATA MINING AND ANALYTICS}, volume = {3}, pages = {235-258}, year = {2020}, doi = {10.26599/BDMA.2020.9020011}, author = {Lu, Mei and Li, Fanzhang}, abstract = {Lie group machine learning is recognized as the theoretical basis of brain intelligence, brain learning, higher machine learning, and higher artificial intelligence. Sample sets of Lie group matrices are widely available in practical applications. Lie group learning is a vibrant field of increasing importance and extraordinary potential and thus needs to be developed further. This study aims to provide a comprehensive survey on recent advances in Lie group machine learning. We introduce Lie group machine learning techniques in three major categories: supervised Lie group machine learning, semisupervised Lie group machine learning, and unsupervised Lie group machine learning. In addition, we introduce the special application of Lie group machine learning in image processing. This work covers the following techniques: Lie group machine learning model, Lie group subspace orbit generation learning, symplectic group learning, quantum group learning, Lie group fiber bundle learning, Lie group cover learning, Lie group deep structure learning, Lie group semisupervised learning, Lie group kernel learning, tensor learning, frame bundle connection learning, spectral estimation learning, Finsler geometric learning, homology boundary learning, category representation learning, and neuromorphic synergy learning. Overall, this survey aims to provide an insightful overview of state-of-the-art development in the field of Lie group machine learning. It will enable researchers to comprehensively understand the state of the field, identify the most appropriate tools for particular applications, and identify directions for future research.} }
@article{WOS:000456754100058, title = {Current and future applications of statistical machine learning algorithms for agricultural machine vision systems}, journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE}, volume = {156}, pages = {585-605}, year = {2019}, issn = {0168-1699}, doi = {10.1016/j.compag.2018.12.006}, author = {Rehman, Tanzeel U. and Mahmud, Md Sultan and Chang, Young K. and Jin, Jian and Shin, Jaemyung}, abstract = {With being rapid increasing population in worldwide, the need for satisfactory level of crop production with decreased amount of agricultural lands. Machine vision would ensure the increase of crop production by using an automated, non-destructive and cost-effective technique. In last few years, remarkable results have been achieved in different sectors of agriculture. These achievements are integrated with machine learning techniques on machine vision approach that cope with colour, shape, texture and spectral analysis from the image of objects. Despite having many applications of different machine learning techniques, this review only described the statistical machine learning technologies with machine vision systems in agriculture due to broad area of machine learning applications. Two types of statistical machine learning techniques such as supervised and unsupervised learning have been utilized for agriculture. This paper comprehensively surveyed current application of statistical machine learning techniques in machine vision systems, analyses each technique potential for specific application and represents an overview of instructive examples in different agricultural areas. Suggestions of specific statistical machine learning technique for specific purpose and limitations of each technique are also given. Future trends of statistical machine learning technology applications are discussed.} }
@article{WOS:000491213400062, title = {Wind power forecasting based on daily wind speed data using machine learning algorithms}, journal = {ENERGY CONVERSION AND MANAGEMENT}, volume = {198}, year = {2019}, issn = {0196-8904}, doi = {10.1016/j.enconman.2019.111823}, author = {Demolli, Halil and Dokuz, Ahmet Sakir and Ecemis, Alper and Gokcek, Murat}, abstract = {Wind energy is a significant and eligible source that has the potential for producing energy in a continuous and sustainable manner among renewable energy sources. However, wind energy has several challenges, such as initial investment costs, the stationary property of wind plants, and the difficulty in finding wind-efficient energy areas. In this study, long-term wind power forecasting was performed based on daily wind speed data using five machine learning algorithms. We proposed a method based on machine learning algorithms to forecast wind power values efficiently. We conducted several case studies to reveal performances of machine learning algorithms. The results showed that machine learning algorithms could be used for forecasting long-term wind power values with respect to historical wind speed data. Furthermore, the results showed that machine learning-based models could be applied to a location different from model-trained locations. This study demonstrated that machine learning algorithms could be successfully used before the establishment of wind plants in an unknown geographical location whether it is logical by using the model of a base location.} }
@article{WOS:000464886100002, title = {Machine learning and complex biological data}, journal = {GENOME BIOLOGY}, volume = {20}, year = {2019}, issn = {1474-760X}, doi = {10.1186/s13059-019-1689-0}, author = {Xu, Chunming and Jackson, Scott A.}, abstract = {Machine learning has demonstrated potential in analyzing large, complex biological data. In practice, however, biological information is required in addition to machine learning for successful application.} }
@article{WOS:000484866500001, title = {Machine learning applications in epilepsy}, journal = {EPILEPSIA}, volume = {60}, pages = {2037-2047}, year = {2019}, issn = {0013-9580}, doi = {10.1111/epi.16333}, author = {Abbasi, Bardia and Goldenholz, Daniel M.}, abstract = {Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre-surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.} }
@article{WOS:000398133500010, title = {Machine Learning for Medical Imaging1}, journal = {RADIOGRAPHICS}, volume = {37}, pages = {505-515}, year = {2017}, issn = {0271-5333}, doi = {10.1148/rg.2017160130}, author = {Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy L.}, abstract = {Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.} }
@article{WOS:000473818300119, title = {Machine learning for email spam filtering: review, approaches and open research problems}, journal = {HELIYON}, volume = {5}, year = {2019}, doi = {10.1016/j.heliyon.2019.e01802}, author = {Dada, Emmanuel Gbenga and Bassi, Joseph Stephen and Chiroma, Haruna and Abdulhamid, Shafi'i Muhammad and Adetunmbi, Adebayo Olusola and Ajibuwa, Opeyemi Emmanuel}, abstract = {The upsurge in the volume of unwanted emails called spam has created an intense need for the development of more dependable and robust antispam fillers. Machine learning methods of recent are being used to successfully detect and filter spam emails. We present a systematic review of some of the popular machine learning based email spam filtering approaches. Our review covers survey of the important concepts, attempts, efficiency, and the research trend in spam filtering. The preliminary discussion in the study background examines the applications of machine learning techniques to the email spam filtering process of the leading internet service providers (ISPs) like Gmail, Yahoo and Outlook emails spam fillers. Discussion on general email spam filtering process, and the various efforts by different researchers in combating spam through the use machine learning techniques was done. Our review compares the strengths and drawbacks of existing machine learning approaches and the open research problems in spam filtering. We recommended deep leaning and deep adversarial learning as the future techniques that can effectively handle the menace of spam emails.} }
@article{WOS:000542536700007, title = {Machine Learning Systems and Intelligent Applications}, journal = {IEEE SOFTWARE}, volume = {37}, pages = {43-49}, year = {2020}, issn = {0740-7459}, doi = {10.1109/MS.2020.2985224}, author = {Benton, William C.}, abstract = {Machine learning techniques are useful in a wide range of contexts, but techniques alone are insufficient to solve real business problems. We introduce the intelligent applications concept, which characterizes the structure and responsibilities of contemporary machine learning systems.} }
@article{WOS:000485253300001, title = {Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods}, journal = {ACADEMIC PATHOLOGY}, volume = {6}, year = {2019}, issn = {2374-2895}, doi = {10.1177/2374289519873088}, author = {Rashidi, Hooman H. and Tran, Nam K. and Betts, Elham Vali and Howell, Lydia P. and Green, Ralph}, abstract = {Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).} }
@article{WOS:000470026000001, title = {Diversity in Machine Learning}, journal = {IEEE ACCESS}, volume = {7}, pages = {64323-64350}, year = {2019}, issn = {2169-3536}, doi = {10.1109/ACCESS.2019.2917620}, author = {Gong, Zhiqiang and Zhong, Ping and Hu, Weidong}, abstract = {Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.} }
@article{WOS:000497715600006, title = {A taxonomy and survey of attacks against machine learning}, journal = {COMPUTER SCIENCE REVIEW}, volume = {34}, year = {2019}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2019.100199}, author = {Pitropakis, Nikolaos and Panaousis, Emmanouil and Giannetsos, Thanassis and Anastasiadis, Eleftherios and Loukas, George}, abstract = {The majority of machine learning methodologies operate with the assumption that their environment is benign. However, this assumption does not always hold, as it is often advantageous to adversaries to maliciously modify the training (poisoning attacks) or test data (evasion attacks). Such attacks can be catastrophic given the growth and the penetration of machine learning applications in society. Therefore, there is a need to secure machine learning enabling the safe adoption of it in adversarial cases, such as spam filtering, malware detection, and biometric recognition. This paper presents a taxonomy and survey of attacks against systems that use machine learning. It organizes the body of knowledge in adversarial machine learning so as to identify the aspects where researchers from different fields can contribute to. The taxonomy identifies attacks which share key characteristics and as such can potentially be addressed by the same defence approaches. Thus, the proposed taxonomy makes it easier to understand the existing attack landscape towards developing defence mechanisms, which are not investigated in this survey. The taxonomy is also leveraged to identify open problems that can lead to new research areas within the field of adversarial machine learning. (C) 2019 Elsevier Inc. All rights reserved.} }
@article{WOS:000465199900003, title = {How to develop machine learning models for healthcare}, journal = {NATURE MATERIALS}, volume = {18}, pages = {410-414}, year = {2019}, issn = {1476-1122}, doi = {10.1038/s41563-019-0345-0}, author = {Chen, Po-Hsuan Cameron and Liu, Yun and Peng, Lily}, abstract = {Rapid progress in machine learning is enabling opportunities for improved clinical decision support. Importantly, however, developing, validating and implementing machine learning models for healthcare entail some particular considerations to increase the chances of eventually improving patient care.} }
@article{WOS:000447085500015, title = {Deep learning and its applications to machine health monitoring}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {115}, pages = {213-237}, year = {2019}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2018.05.050}, author = {Zhao, Rui and Yan, Ruqiang and Chen, Zhenghua and Mao, Kezhi and Wang, Peng and Gao, Robert X.}, abstract = {Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In addition, an experimental study on the performances of these approaches has been conducted, in which the data and code have been online. Finally, some new trends of DL-based machine health monitoring methods are discussed. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:000472453300009, title = {A survey on evolutionary machine learning}, journal = {JOURNAL OF THE ROYAL SOCIETY OF NEW ZEALAND}, volume = {49}, pages = {205-228}, year = {2019}, issn = {0303-6758}, doi = {10.1080/03036758.2019.1609052}, author = {Al-Sahaf, Harith and Bi, Ying and Chen, Qi and Lensen, Andrew and Mei, Yi and Sun, Yanan and Tran, Binh and Xue, Bing and Zhang, Mengjie}, abstract = {Artificial intelligence (AI) emphasises the creation of intelligent machines/systems that function like humans. AI has been applied to many real-world applications. Machine learning is a branch of AI based on the idea that systems can learn from data, identify hidden patterns, and make decisions with little/minimal human intervention. Evolutionary computation is an umbrella of population-based intelligent/learning algorithms inspired by nature, where New Zealand has a good international reputation. This paper provides a review on evolutionary machine learning, i.e. evolutionary computation techniques for major machine learning tasks such as classification, regression and clustering, and emerging topics including combinatorial optimisation, computer vision, deep learning, transfer learning, and ensemble learning. The paper also provides a brief review of evolutionary learning applications, such as supply chain and manufacturing for milk/dairy, wine and seafood industries, which are important to New Zealand. Finally, the paper presents current issues with future perspectives in evolutionary machine learning.} }
@article{WOS:000463615200004, title = {Using Machine Learning to Advance Personality Assessment and Theory}, journal = {PERSONALITY AND SOCIAL PSYCHOLOGY REVIEW}, volume = {23}, pages = {190-203}, year = {2019}, issn = {1088-8683}, doi = {10.1177/1088868318772990}, author = {Bleidorn, Wiebke and Hopwood, Christopher James}, abstract = {Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.} }
@article{WOS:000473429200010, title = {Machine Learning for Health Services Researchers}, journal = {VALUE IN HEALTH}, volume = {22}, pages = {808-815}, year = {2019}, issn = {1098-3015}, doi = {10.1016/j.jval.2019.02.012}, author = {Doupe, Patrick and Faghmous, James and Basu, Sanjay}, abstract = {Background: Machine learning is increasingly used to predict healthcare outcomes, including cost, utilization, and quality. Objective: We provide a high-level overview of machine learning for healthcare outcomes researchers and decision makers. Methods: We introduce key concepts for understanding the application of machine learning methods to healthcare outcomes research. We first describe current standards to rigorously learn an estimator, which is an algorithm developed through machine learning to predict a particular outcome. We include steps for data preparation, estimator family selection, parameter learning, regularization, and evaluation. We then compare 3 of the most common machine learning methods: (1) decision tree methods that can be useful for identifying how different subpopulations experience different risks for an outcome; (2) deep learning methods that can identify complex nonlinear patterns or interactions between variables predictive of an outcome; and (3) ensemble methods that can improve predictive performance by combining multiple machine learning methods. Results: We demonstrate the application of common machine methods to a simulated insurance claims dataset. We specifically include statistical code in R and Python for the development and evaluation of estimators for predicting which patients are at heightened risk for hospitalization from ambulatory care-sensitive conditions. Conclusions: Outcomes researchers should be aware of key standards for rigorously evaluating an estimator developed through machine learning approaches. Although multiple methods use machine learning concepts, different approaches are best suited for different research problems.} }
@article{WOS:000502191300012, title = {Machine learning in resting-state fMRI analysis}, journal = {MAGNETIC RESONANCE IMAGING}, volume = {64}, pages = {101-121}, year = {2019}, issn = {0730-725X}, doi = {10.1016/j.mri.2019.05.031}, author = {Khosla, Meenakshi and Jamison, Keith and Ngo, Gia H. and Kuceyeski, Amy and Sabuncu, Mert R.}, abstract = {Machine learning techniques have gained prominence for the analysis of resting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we present an overview of various unsupervised and supervised machine learning applications to rs-fMRI. We offer a methodical taxonomy of machine learning methods in restingstate fMRI. We identify three major divisions of unsupervised learning methods with regard to their applications to rs-fMRI, based on whether they discover principal modes of variation across space, time or population. Next, we survey the algorithms and rs-fMRI feature representations that have driven the success of supervised subjectlevel predictions. The goal is to provide a high-level overview of the burgeoning field of rs-fMRI from the perspective of machine learning applications.} }
@article{WOS:000355286600032, title = {Probabilistic machine learning and artificial intelligence}, journal = {NATURE}, volume = {521}, pages = {452-459}, year = {2015}, issn = {0028-0836}, doi = {10.1038/nature14541}, author = {Ghahramani, Zoubin}, abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.} }
@article{WOS:000501336600003, title = {Application and comparison of several machine learning algorithms and their integration models in regression problems}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {32}, pages = {5461-5469}, year = {2020}, issn = {0941-0643}, doi = {10.1007/s00521-019-04644-5}, author = {Huang, Jui-Chan and Ko, Kuo-Min and Shu, Ming-Hung and Hsu, Bi-Min}, abstract = {With the rapid development of machine learning technology, as a regression problem that helps people to find the law from the massive data to achieve the prediction effect, more and more people pay attention. Data prediction has become an important part of people's daily life. Currently, the technology is widely used in many fields such as weather forecasting, medical diagnosis and financial forecasting. Therefore, the research of machine learning algorithms in regression problems is a research hotspot in the field of machine learning in recent years. However, real-world regression problems often have very complex internal and external factors, and various machine learning algorithms have different effects on scalability and predictive performance. In order to better study the application effect of machine learning algorithm in regression problem, this paper mainly adopts three common machine learning algorithms: BP neural network, extreme learning machine and support vector machine. Then, by comparing the effects of the single model and integrated model of these machine learning algorithms in the application of regression problems, the advantages and disadvantages of each machine learning algorithm are studied. Finally, the performance of each machine learning algorithm in regression prediction is verified by simulation experiments on four different data sets. The results show that the research on several machine learning algorithms and their integration models has certain feasibility and rationality.} }
@article{WOS:000502169900112, title = {Machine Learning for Accelerated Discovery of Solar Photocatalysts}, journal = {ACS CATALYSIS}, volume = {9}, pages = {11774-11787}, year = {2019}, issn = {2155-5435}, doi = {10.1021/acscatal.9b02531}, author = {Masood, Hassan and Toe, Cui Ying and Teoh, Wey Yang and Sethu, Vidhyasaharan and Amal, Rose}, abstract = {Robust screening of materials on the basis of structure-property-activity relationships to discover active photocatalysts is a highly sought out aspect of photocatalysis research. Recent advancements in machine learning offer considerable opportunities to evolve photocatalysts discovery practices. Machine learning has largely facilitated various areas of science and engineering, including heterogeneous catalysis, but adaptation of it in photocatalysis research is still at an elementary stage. The scarcity of consistent training data is a major bottleneck, and we foresee the integration of photo-catalysis domain knowledge in mainstream machine learning protocols as a viable solution. Here, we present a holistic framework incorporating machine learning and domain knowledge to set directions toward accelerated discovery of solar photocatalysts. This Perspective begins with a discussion on domain knowledge available in photocatalysis which could potentially be leveraged to liaise with machine learning methods. Subsequently, we present prevalent machine learning practices in heterogeneous catalysis tailored to assist discovery of photocatalysts in a purely data-driven fashion. Lastly, we conceptualize various strategies for complementing data-driven machine learning with photocatalysis domain knowledge. The strategies involve the following: (i) integration of theoretical and prior empirical knowledge during the training of machine learning models; (ii) embedding the knowledge in feature space; and (iii) utilizing existing material databases to constrain machine learning predictions. The aforementioned human-in-loop framework (leveraging both human and machine intelligence) could possibly mitigate the lack of interpretability and reliability associated with data-driven machine learning and reinforce complex model architectures irrespective of data scarcity. The concept could also offer substantial benefits to photocatalysis informatics by promoting a paradigm shift away from the Edisonian approach.} }
@article{WOS:000471357900035, title = {Radiological images and machine learning: Trends, perspectives, and prospects}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {108}, pages = {354-370}, year = {2019}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2019.02.017}, author = {Zhang, Zhenwei and Sejdic, Ervin}, abstract = {The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.} }
@article{WOS:000503827500061, title = {Machine learning in the Internet of Things: Designed techniques for smart cities}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {100}, pages = {826-843}, year = {2019}, issn = {0167-739X}, doi = {10.1016/j.future.2019.04.017}, author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J. P. C. and Hassan, Suhaidi and Korotaev, Valery V.}, abstract = {Machine learning is one of the emerging technologies that has grabbed the attention of academicians and industrialists, and is expected to evolve in the near future. Machine learning techniques are anticipated to provide pervasive connections for wireless nodes. In fact, machine learning paves the way for the Internet of Things (IoT)-a network that supports communications among various devices without human interactions. Machine learning techniques are being utilized in several fields such as healthcare, smart grids, vehicular communications, and so on. In this paper, we study different IoT-based machine learning mechanisms that are used in the mentioned fields among others. In addition, the lessons learned are reported and the assessments are explored viewing the basic aim machine learning techniques are expected to play in IoT networks. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000478780200033, title = {Quantifying performance of machine learning methods for neuroimaging data}, journal = {NEUROIMAGE}, volume = {199}, pages = {351-365}, year = {2019}, issn = {1053-8119}, doi = {10.1016/j.neuroimage.2019.05.082}, author = {Jollans, Lee and Boyle, Rory and Artiges, Eric and Banaschewski, Tobias and Desrivieres, Sylvane and Grigis, Antoine and Martinot, Jean-Luc and Paus, Tomas and Smolka, Michael N. and Walter, Henrik and Schumann, Gunter and Garavan, Hugh and Whelan, Robert}, abstract = {Machine learning is increasingly being applied to neuroimaging data. However, most machine learning algorithms have not been designed to accommodate neuroimaging data, which typically has many more data points than subjects, in addition to multicollinearity and low signal-to-noise. Consequently, the relative efficacy of different machine learning regression algorithms for different types of neuroimaging data are not known. Here, we sought to quantify the performance of a variety of machine learning algorithms for use with neuroimaging data with various sample sizes, feature set sizes, and predictor effect sizes. The contribution of additional machine learning techniques - embedded feature selection and bootstrap aggregation (bagging) - to model performance was also quantified. Five machine learning regression methods - Gaussian Process Regression, Multiple Kernel Learning, Kernel Ridge Regression, the Elastic Net and Random Forest, were examined with both real and simulated MRI data, and in comparison to standard multiple regression. The different machine learning regression algorithms produced varying results, which depended on sample size, feature set size, and predictor effect size. When the effect size was large, the Elastic Net, Kernel Ridge Regression and Gaussian Process Regression performed well at most sample sizes and feature set sizes. However, when the effect size was small, only the Elastic Net made accurate predictions, but this was limited to analyses with sample sizes greater than 400. Random Forest also produced a moderate performance for small effect sizes, but could do so across all sample sizes. Machine learning techniques also improved prediction accuracy for multiple regression. These data provide empirical evidence for the differential performance of various machines on neuroimaging data, which are dependent on number of sample size, features and effect size.} }
@article{WOS:000512985500004, title = {A primer on machine learning}, journal = {RADIOLOGE}, volume = {60}, pages = {24-31}, year = {2020}, issn = {0033-832X}, doi = {10.1007/s00117-019-00616-x}, author = {Kleesiek, Jens and Murray, Jacob M. and Strack, Christian and Kaissis, Georgios and Braren, Rickmer}, abstract = {Background The methods of machine learning and artificial intelligence are slowly but surely being introduced in everyday medical practice. In the future, they will support us in diagnosis and therapy and thus improve treatment for the benefit of the individual patient. It is therefore important to deal with this topic and to develop a basic understanding of it. Objectives This article gives an overview of the exciting and dynamic field of machine learning and serves as an introduction to some methods primarily from the realm of supervised learning. In addition to definitions and simple examples, limitations are discussed. Conclusions The basic principles behind the methods are simple. Nevertheless, due to their high dimensional nature, the factors influencing the results are often difficult or impossible to understand by humans. In order to build confidence in the new technologies and to guarantee their safe application, we need explainable algorithms and prospective effectiveness studies.} }
@article{WOS:000425074100016, title = {The use of machine learning algorithms in recommender systems: A systematic review}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {97}, pages = {205-227}, year = {2018}, issn = {0957-4174}, doi = {10.1016/j.eswa.2017.12.020}, author = {Portugal, Ivens and Alencar, Paulo and Cowan, Donald}, abstract = {Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000471293100006, title = {Machine learning in cybersecurity: A review}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {9}, year = {2019}, issn = {1942-4787}, doi = {10.1002/widm.1306}, author = {Handa, Anand and Sharma, Ashu and Shukla, Sandeep K.}, abstract = {Machine learning technology has become mainstream in a large number of domains, and cybersecurity applications of machine learning techniques are plenty. Examples include malware analysis, especially for zero-day malware detection, threat analysis, anomaly based intrusion detection of prevalent attacks on critical infrastructures, and many others. Due to the ineffectiveness of signature-based methods in detecting zero day attacks or even slight variants of known attacks, machine learning-based detection is being used by researchers in many cybersecurity products. In this review, we discuss several areas of cybersecurity where machine learning is used as a tool. We also provide a few glimpses of adversarial attacks on machine learning algorithms to manipulate training and test data of classifiers, to render such tools ineffective. This article is categorized under: Application Areas > Science and Technology Technologies > Machine Learning Technologies > Classification Application Areas > Data Mining Software Tools} }
@article{WOS:000488315100001, title = {Machine learning approaches for pathologic diagnosis}, journal = {VIRCHOWS ARCHIV}, volume = {475}, pages = {131-138}, year = {2019}, issn = {0945-6317}, doi = {10.1007/s00428-019-02594-w}, author = {Komura, Daisuke and Ishikawa, Shumpei}, abstract = {Machine learning techniques, especially deep learning techniques such as convolutional neural networks, have been successfully applied to general image recognitions since their overwhelming performance at the 2012 ImageNet Large Scale Visual Recognition Challenge. Recently, such techniques have also been applied to various medical, including histopathological, images to assist the process of medical diagnosis. In some cases, deep learning-based algorithms have already outperformed experienced pathologists for recognition of histopathological images. However, pathological images differ from general images in some aspects, and thus, machine learning of histopathological images requires specialized learning methods. Moreover, many pathologists are skeptical about the ability of deep learning technology to accurately recognize histopathological images because what the learned neural network recognizes is often indecipherable to humans. In this review, we first introduce various applications incorporating machine learning developed to assist the process of pathologic diagnosis, and then describe machine learning problems related to histopathological image analysis, and review potential ways to solve these problems.} }
@article{WOS:000439847600006, title = {Machine learning in cardiovascular medicine: are we there yet?}, journal = {HEART}, volume = {104}, pages = {1156-1164}, year = {2018}, issn = {1355-6037}, doi = {10.1136/heartjnl-2017-311198}, author = {Shameer, Khader and Johnson, Kipp W. and Glicksberg, Benjamin S. and Dudley, Joel T. and Sengupta, Partho P.}, abstract = {Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.} }
@article{WOS:000481489500005, title = {Machine Learning for Stock Selection}, journal = {FINANCIAL ANALYSTS JOURNAL}, volume = {75}, pages = {70-88}, year = {2019}, issn = {0015-198X}, doi = {10.1080/0015198X.2019.1596678}, author = {Rasekhschaffe, Keywan Christian and Jones, Robert C.}, abstract = {Machine learning is an increasingly important and controversial topic in quantitative finance. A lively debate persists as to whether machine learning techniques can be practical investment tools. Although machine learning algorithms can uncover subtle, contextual. and nonlinear relationships, overfitting poses a major challenge when one is trying to extract signals from noisy historical data. We describe some of the basic concepts of machine learning and provide a simple example of how investors can use machine learning techniques to forecast the cross-section of stock returns while limiting the risk of overfitting.} }
@article{WOS:000484486600010, title = {Machine learning in predicting graft failure following kidney transplantation: A systematic review of published predictive models}, journal = {INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS}, volume = {130}, year = {2019}, issn = {1386-5056}, doi = {10.1016/j.ijmedinf.2019.103957}, author = {Senanayake, Sameera and White, Nicole and Graves, Nicholas and Healy, Helen and Baboolal, Keshwar and Kularatna, Sanjeewa}, abstract = {Introduction: Machine learning has been increasingly used to develop predictive models to diagnose different disease conditions. The heterogeneity of the kidney transplant population makes predicting graft outcomes extremely challenging. Several kidney graft outcome prediction models have been developed using machine learning, and are available in the literature. However, a systematic review of machine learning based prediction methods applied to kidney transplant has not been done to date. The main aim of our study was to perform an in-depth systematic analysis of different machine learning methods used to predict graft outcomes among kidney transplant patients, and assess their usefulness as an aid to decision-making. Methods: A systemic review of machine learning methods used to predict graft outcomes among kidney transplant patients was carried out using a search of the Medline, the Cumulative Index to Nursing and Allied Health Literature, EMBASE, PsycINFO and Cochrane databases. Results: A total of 295 articles were identified and extracted. Of these, 18 ma the inclusion criteria. Most of the studies were published in the United States after 2010. The population size used to develop the models varied from 80 to 92,844, and the number of features in the models ranged from 6 to 71. The most common machine learning methods used were artificial neural networks, decision trees and Bayesian belief networks. Most of the machine learning based predictive models predicted graft failure with high sensitivity and specificity. Only one machine learning based prediction model had modelled time-to-event (survival) information. Seven studies compared the predictive performance of machine learning models with traditional regression methods and the performance of machine learning methods was found to be mixed, when compared with traditional regression methods. Conclusion: There was a wide variation in the size of the study population and the input variables used. However, the prediction accuracy provided mixed results when machine learning and traditional predictive methods are compared. Based on reported gains in predictive performance, machine learning has the potential to improve kidney transplant outcome prediction and aid medical decision making} }
@article{WOS:000493335100001, title = {Identification of advanced spin-driven thermoelectric materials via interpretable machine learning}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, doi = {10.1038/s41524-019-0241-9}, author = {Iwasaki, Yuma and Sawada, Ryohto and Stanev, Valentin and Ishida, Masahiko and Kirihara, Akihiro and Omori, Yasutomo and Someya, Hiroko and Takeuchi, Ichiro and Saitoh, Eiji and Yorozu, Shinichi}, abstract = {Machine learning is becoming a valuable tool for scientific discovery. Particularly attractive is the application of machine learning methods to the field of materials development, which enables innovations by discovering new and better functional materials. To apply machine learning to actual materials development, close collaboration between scientists and machine learning tools is necessary. However, such collaboration has been so far impeded by the black box nature of many machine learning algorithms. It is often difficult for scientists to interpret the data-driven models from the viewpoint of material science and physics. Here, we demonstrate the development of spin-driven thermoelectric materials with anomalous Nernst effect by using an interpretable machine learning method called factorized asymptotic Bayesian inference hierarchical mixture of experts (FAB/HMEs). Based on prior knowledge of material science and physics, we were able to extract from the interpretable machine learning some surprising correlations and new knowledge about spin-driven thermoelectric materials. Guided by this, we carried out an actual material synthesis that led to the identification of a novel spin-driven thermoelectric material. This material shows the largest thermopower to date.} }
@article{WOS:000417528000001, title = {Ten quick tips for machine learning in computational biology}, journal = {BIODATA MINING}, volume = {10}, year = {2017}, issn = {1756-0381}, doi = {10.1186/s13040-017-0155-3}, author = {Chicco, Davide}, abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.} }
@article{WOS:000462868200006, title = {Machine Learning in Nuclear Medicine: Part 1-Introduction}, journal = {JOURNAL OF NUCLEAR MEDICINE}, volume = {60}, pages = {451-458}, year = {2019}, issn = {0161-5505}, doi = {10.2967/jnumed.118.223495}, author = {Uribe, Carlos F. and Mathotaarachchi, Sulantha and Gaudet, Vincent and Smith, Kenneth C. and Rosa-Neto, Pedro and Benard, Francois and Black, Sandra E. and Zukotynski, Katherine}, abstract = {Learning Objectives: On successful completion of this activity, participants should be able to (1) provide an introduction to machine learning, neural networks, and deep learning; (2) discuss common machine learning algorithms with illustrative examples and figures; and (3) compare machine learning algorithms and provide guidance on selection for a given application.} }
@article{WOS:000514113100007, title = {Sofware engneering challenges for machine learning applications: A literature review}, journal = {INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS}, volume = {13}, pages = {463-476}, year = {2019}, issn = {1872-4981}, doi = {10.3233/IDT-190160}, author = {Kumeno, Fumihiro}, abstract = {Machine learning techniques, especially deep learning, have achieved remarkable breakthroughs over the past decade. At present, machine learning applications are deployed in many fields. However, the outcomes of software engineering researches are not always easily utilized in the development and deployment of machine learning applications. The main reason for this difficulty is the many differences between machine learning applications and traditional information systems. Machine learning techniques are evolving rapidly, but face inherent technical and non-technical challenges that complicate their lifecycle activities. This review paper attempts to clarify the software engineering challenges for machine learning applications that either exist or potentially exist by conducting a systematic literature collection and by mapping the identified challenge topics to knowledge areas defined by the Software Engineering Body of Knowledge (Swebok).} }
@article{WOS:000454602800004, title = {Machine learning research that matters for music creation: A case study}, journal = {JOURNAL OF NEW MUSIC RESEARCH}, volume = {48}, pages = {36-55}, year = {2019}, issn = {0929-8215}, doi = {10.1080/09298215.2018.1515233}, author = {Sturm, Bob L. and Ben-Tal, Oded and Monaghan, Una and Collins, Nick and Herremans, Dorien and Chew, Elaine and Hadjeres, Gaetan and Deruty, Emmanuel and Pachet, Francois}, abstract = {Research applying machine learning to music modelling and generation typically proposes model architectures, training methods and datasets, and gauges system performance using quantitative measures like sequence likelihoods and/or qualitative listening tests. Rarely does such work explicitly question and analyse its usefulness for and impact on real-world practitioners, and then build on those outcomes to inform the development and application of machine learning. This article attempts to do these things for machine learning applied to music creation. Together with practitioners, we develop and use several applications of machine learning for music creation, and present a public concert of the results. We reflect on the entire experience to arrive at several ways of advancing these and similar applications of machine learning to music creation.} }
@article{WOS:000489702000038, title = {Autonomic machine learning platform}, journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT}, volume = {49}, pages = {491-501}, year = {2019}, issn = {0268-4012}, doi = {10.1016/j.ijinfomgt.2019.07.003}, author = {Lee, Keon Myung and Yoo, Jaesoo and Kim, Sang-Wook and Lee, Jee-Hyong and Hong, Jiman}, abstract = {Acquiring information properly through machine learning requires familiarity with the available algorithms and understanding how they work and how to address the given problem in the best possible way. However, even for machine-learning experts in specific industrial fields, in order to predict and acquire information properly in different industrial fields, it is necessary to attempt several instances of trial and error to succeed with the application of machine learning. For non-experts, it is much more difficult to make accurate predictions through machine learning. In this paper, we propose an autonomic machine learning platform which provides the decision factors to be made during the developing of machine learning applications. In the proposed autonomic machine learning platform, machine learning processes are automated based on the specification of autonomic levels. This autonomic machine learning platform can be used to derive a high-quality learning result by minimizing experts' interventions and reducing the number of design selections that require expert knowledge and intuition. We also demonstrate that the proposed autonomic machine learning platform is suitable for smart cities which typically require considerable amounts of security sensitive information.} }
@article{WOS:000504871100008, title = {Predictive Modeling of Outcomes After Traumatic and Nontraumatic Spinal Cord Injury Using Machine Learning: Review of Current Progress and Future Directions}, journal = {NEUROSPINE}, volume = {16}, pages = {678-685}, year = {2019}, issn = {2586-6583}, doi = {10.14245/ns.1938390.195}, author = {Khan, Omar and Badhiwala, Jetan H. and Wilson, Jamie R. F. and Jiang, Fan and Martin, Allan R. and Fehlings, Michael G.}, abstract = {Machine learning represents a promising frontier in epidemiological research on spine surgery. It consists of a series of algorithms that determines relationships between data. Machine learning maintains numerous advantages over conventional regression techniques, such as a reduced requirement for a priori knowledge on predictors and better ability to manage large datasets. Current studies have made extensive strides in employing machine learning to a greater capacity in spinal cord injury (SCI). Analyses using machine learning algorithms have been done on both traumatic SCI and nontraumatic SCI, the latter of which typically represents degenerative spine disease resulting in spinal cord compression, such as degenerative cervical myelopathy. This article is a literature review of current studies published in traumatic and nontraumatic SCI that employ machine learning for the prediction of a host of outcomes. The studies described utilize machine learning in a variety of capacities, including imaging analysis and prediction in large epidemiological data sets. We discuss the performance of these machine learning-based clinical prognostic models relative to conventional statistical prediction models. Finally, we detail the future steps needed for machine learning to become a more common modality for statistical analysis in SCI.} }
@article{WOS:000448233900010, title = {Smart Machining Process Using Machine Learning: A Review and Perspective on Machining Industry}, journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN TECHNOLOGY}, volume = {5}, pages = {555-568}, year = {2018}, issn = {2288-6206}, doi = {10.1007/s40684-018-0057-y}, author = {Kim, Dong-Hyeon and Kim, Thomas J. Y. and Wang, Xinlin and Kim, Mincheol and Quan, Ying-Jun and Oh, Jin Woo and Min, Soo-Hong and Kim, Hyungjung and Bhandari, Binayak and Yang, Insoon and Ahn, Sung-Hoon}, abstract = {The Fourth Industrial Revolution incorporates the digital revolution into the physical world, creating a new direction in a number of fields, including artificial intelligence, quantum computing, nanotechnology, biotechnology, robotics, 3D printing, autonomous vehicles, and the Internet of Things. The artificial intelligence field has encountered a turning point mainly due to advancements in machine learning, which allows machines to learn, improve, and perform a specific task through data without being explicitly programmed. Machine learning can be utilized with machining processes to improve product quality levels and productivity rates, to monitor the health of systems, and to optimize design and process parameters. This is known as smart machining, referring to a new machining paradigm in which machine tools are fully connected through a cyber-physical system. This paper reviews and summarizes machining processes using machine learning algorithms and suggests a perspective on the machining industry.} }
@article{WOS:000424709000002, title = {Machine learning, social learning and the governance of self-driving cars}, journal = {SOCIAL STUDIES OF SCIENCE}, volume = {48}, pages = {25-56}, year = {2018}, issn = {0306-3127}, doi = {10.1177/0306312717741687}, author = {Stilgoe, Jack}, abstract = {Self-driving cars, a quintessentially smart' technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking Who is learning, what are they learning and how are they learning?' Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. Self-driving' or autonomous' cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.} }
@article{WOS:000499323200025, title = {Simulation-assisted machine learning}, journal = {BIOINFORMATICS}, volume = {35}, pages = {4072-4080}, year = {2019}, issn = {1367-4803}, doi = {10.1093/bioinformatics/btz199}, author = {Deist, Timo M. and Patti, Andrew and Wang, Zhaoqi and Krane, David and Sorenson, Taylor and Craft, David}, abstract = {Motivation: In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these two extremes: some details of the system mechanics are known but not enough for creating simulations that can be used to make high quality predictions. In this context we propose using approximate simulations to build a kernel for use in kernelized machine learning methods, such as support vector machines. The results of multiple simulations (under various uncertainty scenarios) are used to compute similarity measures between every pair of samples: sample pairs are given a high similarity score if they behave similarly under a wide range of simulation parameters. These similarity values, rather than the original high dimensional feature data, are used to build the kernel. Results: We demonstrate and explore the simulation-based kernel (SimKern) concept using four synthetic complex systems-three biologically inspired models and one network flow optimization model. We show that, when the number of training samples is small compared to the number of features, the SimKern approach dominates over no-prior-knowledge methods. This approach should be applicable in all disciplines where predictive models are sought and informative yet approximate simulations are available.} }
@article{WOS:000710554200001, title = {Kernel methods in Quantum Machine Learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {1}, pages = {65-71}, year = {2019}, issn = {2524-4906}, doi = {10.1007/s42484-019-00007-4}, author = {Mengoni, Riccardo and Di Pierro, Alessandra}, abstract = {Quantum Machine Learning has established itself as one of the most promising applications of quantum computers and Noisy Intermediate Scale Quantum (NISQ) devices. In this paper, we review the latest developments regarding the usage of quantum computing for a particular class of machine learning algorithms known as kernel methods.} }
@article{WOS:000483104800010, title = {Machine Learning-Based Sentiment Analysis for Twitter Accounts}, journal = {MATHEMATICAL AND COMPUTATIONAL APPLICATIONS}, volume = {23}, year = {2018}, issn = {1300-686X}, doi = {10.3390/mca23010011}, author = {Hasan, Ali and Moin, Sana and Karim, Ahmad and Shamshirband, Shahaboddin}, abstract = {Growth in the area of opinion mining and sentiment analysis has been rapid and aims to explore the opinions or text present on different platforms of social media through machine-learning techniques with sentiment, subjectivity analysis or polarity calculations. Despite the use of various machine-learning techniques and tools for sentiment analysis during elections, there is a dire need for a state-of-the-art approach. To deal with these challenges, the contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naive Bayes and support vector machines (SVM).} }
@article{WOS:000391480800001, title = {MLlib: Machine Learning in Apache Spark}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {17}, year = {2016}, issn = {1532-4435}, author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, D. B. and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet}, abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides effcient functionality fo wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.} }
@article{WOS:000391065600020, title = {Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {18}, year = {2016}, issn = {1438-8871}, doi = {10.2196/jmir.5870}, author = {Luo, Wei and Phung, Dinh and Tran, Truyen and Gupta, Sunil and Rana, Santu and Karmakar, Chandan and Shilton, Alistair and Yearwood, John and Dimitrova, Nevenka and Ho, Tu Bao and Venkatesh, Svetha and Berk, Michael}, abstract = {Background: As more and more researchers are turning to big data for new opportunities of biomedical discoveries, machine learning models, as the backbone of big data analysis, are mentioned more often in biomedical journals. However, owing to the inherent complexity of machine learning methods, they are prone to misuse. Because of the flexibility in specifying machine learning models, the results are often insufficiently reported in research articles, hindering reliable assessment of model validity and consistent interpretation of model outputs. Objective: To attain a set of guidelines on the use of machine learning predictive models within clinical settings to make sure the models are correctly applied and sufficiently reported so that true discoveries can be distinguished from random coincidence. Methods: A multidisciplinary panel of machine learning experts, clinicians, and traditional statisticians were interviewed, using an iterative process in accordance with the Delphi method. Results: The process produced a set of guidelines that consists of (1) a list of reporting items to be included in a research article and (2) a set of practical sequential steps for developing predictive models. Conclusions: A set of guidelines was generated to enable correct application of machine learning models and consistent reporting of model specifications and results in biomedical research. We believe that such guidelines will accelerate the adoption of big data analysis, particularly with machine learning methods, in the biomedical research community.} }
@article{WOS:000433269000002, title = {Machine learning-based thermal response time ahead energy demand prediction for building heating systems}, journal = {APPLIED ENERGY}, volume = {221}, pages = {16-27}, year = {2018}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2018.03.125}, author = {Guo, Yabin and Wang, Jiangyu and Chen, Huanxin and Li, Guannan and Liu, Jiangyan and Xu, Chengliang and Huang, Ronggeng and Huang, Yao}, abstract = {Energy demand prediction of building heating is conducive to optimal control, fault detection and diagnosis and building intelligentization. In this study, energy demand prediction models are developed through machine learning methods, including extreme learning machine, multiple linear regression, support vector regression and backpropagation neural network. Seven different meteorological parameters, operating parameters, time and indoor temperature parameters are used as feature variables of the model. Correlation analysis method is utilized to optimize the feature sets. Moreover, this paper proposes a strategy for obtaining the thermal response time of building, which is used as the time ahead of prediction models. The prediction performances of extreme learning machine models with various hidden layer nodes are analyzed and contrasted. Actual data of building heating using a ground source heat pump system are collected and used to test the performances of the models. Results show that the thermal response time of the building is approximately 40 min. Four feature sets are obtained, and the performances of the models with feature set 4 are better. For different machine learning methods, the performances of extreme learning machine models are better than others. In addition, the optimal number of hidden layer nodes is 11 for the extreme learning machine model with feature set 4.} }
@article{WOS:000425839800026, title = {Machine Learning in Radiology: Applications Beyond Image Interpretation}, journal = {JOURNAL OF THE AMERICAN COLLEGE OF RADIOLOGY}, volume = {15}, pages = {350-359}, year = {2018}, issn = {1546-1440}, doi = {10.1016/j.jacr.2017.09.044}, author = {Lakhani, Paras and Prater, Adam B. and Hutson, R. Kent and Andriole, Kathy P. and Dreyer, Keith J. and Morey, Jose and Prevedello, Luciano M. and Clark, Toshi J. and Geis, J. Raymond and Itri, Jason N. and Hawkins, C. Matthew}, abstract = {Much attention has been given to machine learning and its perceived impact in radiology, particularly in light of recent success with image classification in international competitions. However, machine learning is likely to impact radiology outside of image interpretation long before a fully functional ``machine radiologist'' is implemented in practice. Here, we describe an overview of machine learning, its application to radiology and other domains, and many cases of use that do not involve image interpretation. We hope that better understanding of these potential applications will help radiology practices prepare for the future and realize performance improvement and efficiency gains.} }
@article{WOS:000416496500008, title = {From machine learning to deep learning: progress in machine intelligence for rational drug discovery}, journal = {DRUG DISCOVERY TODAY}, volume = {22}, pages = {1680-1685}, year = {2017}, issn = {1359-6446}, doi = {10.1016/j.drudis.2017.08.010}, author = {Zhang, Lu and Tan, Jianjun and Han, Dan and Zhu, Hao}, abstract = {Machine intelligence, which is normally presented as artificial intelligence, refers to the intelligence exhibited by computers. In the history of rational drug discovery, various machine intelligence approaches have been applied to guide traditional experiments, which are expensive and time-consuming. Over the past several decades, machine-learning tools, such as quantitative structure activity relationship (QSAR) modeling, were developed that can identify potential biological active molecules from millions of candidate compounds quickly and cheaply. However, when drug discovery moved into the era of `big' data, machine learning approaches evolved into deep learning approaches, which are a more powerful and efficient way to deal with the massive amounts of data generated from modern drug discovery approaches. Here, we summarize the history of machine learning and provide insight into recently developed deep learning approaches and their applications in rational drug discovery. We suggest that this evolution of machine intelligence now provides a guide for early-stage drug design and discovery in the current big data era.} }
@article{WOS:001190989000001, title = {From distributed machine to distributed deep learning: a comprehensive survey}, journal = {JOURNAL OF BIG DATA}, volume = {10}, year = {2023}, doi = {10.1186/s40537-023-00829-x}, author = {Dehghani, Mohammad and Yazdanparast, Zahra}, abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.} }
@article{WOS:000442625700004, title = {Machine Learning Methods for Analysis of Metabolic Data and Metabolic Pathway Modeling}, journal = {METABOLITES}, volume = {8}, year = {2018}, doi = {10.3390/metabo8010004}, author = {Cuperlovic-Culf, Miroslava}, abstract = {Machine learning uses experimental data to optimize clustering or classification of samples or features, or to develop, augment or verify models that can be used to predict behavior or properties of systems. It is expected that machine learning will help provide actionable knowledge from a variety of big data including metabolomics data, as well as results of metabolism models. A variety of machine learning methods has been applied in bioinformatics and metabolism analyses including self-organizing maps, support vector machines, the kernel machine, Bayesian networks or fuzzy logic. To a lesser extent, machine learning has also been utilized to take advantage of the increasing availability of genomics and metabolomics data for the optimization of metabolic network models and their analysis. In this context, machine learning has aided the development of metabolic networks, the calculation of parameters for stoichiometric and kinetic models, as well as the analysis of major features in the model for the optimal application of bioreactors. Examples of this very interesting, albeit highly complex, application of machine learning for metabolism modeling will be the primary focus of this review presenting several different types of applications for model optimization, parameter determination or system analysis using models, as well as the utilization of several different types of machine learning technologies.} }
@article{WOS:000376943000001, title = {A survey of machine learning for big data processing}, journal = {EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING}, year = {2016}, issn = {1687-6180}, doi = {10.1186/s13634-016-0355-x}, author = {Qiu, Junfei and Wu, Qihui and Ding, Guoru and Xu, Yuhua and Feng, Shuo}, abstract = {There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends.} }
@article{WOS:000479003300133, title = {Physician-Friendly Machine Learning: A Case Study with Cardiovascular Disease Risk Prediction}, journal = {JOURNAL OF CLINICAL MEDICINE}, volume = {8}, year = {2019}, doi = {10.3390/jcm8071050}, author = {Padmanabhan, Meghana and Yuan, Pengyu and Chada, Govind and Hien Van Nguyen}, abstract = {Machine learning is often perceived as a sophisticated technology accessible only by highly trained experts. This prevents many physicians and biologists from using this tool in their research. The goal of this paper is to eliminate this out-dated perception. We argue that the recent development of auto machine learning techniques enables biomedical researchers to quickly build competitive machine learning classifiers without requiring in-depth knowledge about the underlying algorithms. We study the case of predicting the risk of cardiovascular diseases. To support our claim, we compare auto machine learning techniques against a graduate student using several important metrics, including the total amounts of time required for building machine learning models and the final classification accuracies on unseen test datasets. In particular, the graduate student manually builds multiple machine learning classifiers and tunes their parameters for one month using scikit-learn library, which is a popular machine learning library to obtain ones that perform best on two given, publicly available datasets. We run an auto machine learning library called auto-sklearn on the same datasets. Our experiments find that automatic machine learning takes 1 h to produce classifiers that perform better than the ones built by the graduate student in one month. More importantly, building this classifier only requires a few lines of standard code. Our findings are expected to change the way physicians see machine learning and encourage wide adoption of Artificial Intelligence (AI) techniques in clinical domains.} }
@article{WOS:000430565600012, title = {Machine learning in heart failure: ready for prime time}, journal = {CURRENT OPINION IN CARDIOLOGY}, volume = {33}, pages = {190-195}, year = {2018}, issn = {0268-4705}, doi = {10.1097/HCO.0000000000000491}, author = {Awan, Saqib Ejaz and Sohel, Ferdous and Sanfilippo, Frank Mario and Bennamoun, Mohammed and Dwivedi, Girish}, abstract = {Purpose of review The aim of this review is to present an up-to-date overview of the application of machine learning methods in heart failure including diagnosis, classification, readmissions and medication adherence. Recent findings Recent studies have shown that the application of machine learning techniques may have the potential to improve heart failure outcomes and management, including cost savings by improving existing diagnostic and treatment support systems. Recently developed deep learning methods are expected to yield even better performance than traditional machine learning techniques in performing complex tasks by learning the intricate patterns hidden in big medical data. Summary The review summarizes the recent developments in the application of machine and deep learning methods in heart failure management.} }
@article{WOS:000427315000007, title = {What is the machine learning?}, journal = {PHYSICAL REVIEW D}, volume = {97}, year = {2018}, issn = {2470-0010}, doi = {10.1103/PhysRevD.97.056009}, author = {Chang, Spencer and Cohen, Timothy and Ostdiek, Bryan}, abstract = {Applications of machine learning tools to problems of physical interest are often criticized for producing sensitivity at the expense of transparency. To address this concern, we explore a data planing procedure for identifying combinations of variables-aided by physical intuition-that can discriminate signal from background. Weights are introduced to smooth away the features in a given variable(s). New networks are then trained on this modified data. Observed decreases in sensitivity diagnose the variable's discriminating power. Planing also allows the investigation of the linear versus nonlinear nature of the boundaries between signal and background. We demonstrate the efficacy of this approach using a toy example, followed by an application to an idealized heavy resonance scenario at the Large Hadron Collider. By unpacking the information being utilized by these algorithms, this method puts in context what it means for a machine to learn.} }
@article{WOS:000435911300004, title = {Machine learning for architectural design: Practices and infrastructure}, journal = {INTERNATIONAL JOURNAL OF ARCHITECTURAL COMPUTING}, volume = {16}, pages = {123-143}, year = {2018}, issn = {1478-0771}, doi = {10.1177/1478077118778580}, author = {Tamke, Martin and Nicholas, Paul and Zwierzycki, Mateusz}, abstract = {In this article, we propose that new architectural design practices might be based on machine learning approaches to better leverage data-rich environments and workflows. Through reference to recent architectural research, we describe how the application of machine learning can occur throughout the design and fabrication process, to develop varied relations between design, performance and learning. The impact of machine learning on architectural practices with performance-based design and fabrication is assessed in two cases by the authors. We then summarise what we perceive as current limits to a more widespread application and conclude by providing an outlook and direction for future research for machine learning in architectural design practice.} }
@article{WOS:000478765300004, title = {Machine learning in empirical asset pricing}, journal = {FINANCIAL MARKETS AND PORTFOLIO MANAGEMENT}, volume = {33}, pages = {93-104}, year = {2019}, issn = {1934-4554}, doi = {10.1007/s11408-019-00326-3}, author = {Weigand, Alois}, abstract = {The tremendous speedup in computing in recent years, the low data storage costs of today, the availability of ``big data'' as well as the broad range of free open-source software, have created a renaissance in the application of machine learning techniques in science. However, this new wave of research is not limited to computer science or software engineering anymore. Among others, machine learning tools are now used in financial problem settings as well. Therefore, this paper mentions a specific definition of machine learning in an asset pricing context and elaborates on the usefulness of machine learning in this context. Most importantly, the literature review gives the reader a theoretical overview of the most recent academic studies in empirical asset pricing that employ machine learning techniques. Overall, the paper concludes that machine learning can offer benefits for future research. However, researchers should be critical about these methodologies as machine learning has its pitfalls and is relatively new to asset pricing.} }
@article{WOS:000510726600002, title = {MODEL THEORY AND MACHINE LEARNING}, journal = {BULLETIN OF SYMBOLIC LOGIC}, volume = {25}, pages = {319-332}, year = {2019}, issn = {1079-8986}, doi = {10.1017/bsl.2018.71}, author = {Chase, Hunter and Freitag, James}, abstract = {About 25 years ago, it came to light that a single combinatorial property determines both an important dividing line in model theory (NIP) and machine learning (PAC-learnability). The following years saw a fruitful exchange of ideas between PAC-learning and the model theory of NIP structures. In this article, we point out a new and similar connection between model theory and machine learning, this time developing a correspondence between stability and learnability in various settings of online learning. In particular, this gives many new examples of mathematically interesting classes which are learnable in the online setting.} }
@article{WOS:000430730700007, title = {Machine Learning for Hardware Security: Opportunities and Risks}, journal = {JOURNAL OF ELECTRONIC TESTING-THEORY AND APPLICATIONS}, volume = {34}, pages = {183-201}, year = {2018}, issn = {0923-8174}, doi = {10.1007/s10836-018-5726-9}, author = {Elnaggar, Rana and Chakrabarty, Krishnendu}, abstract = {Recently, machine learning algorithms have been utilized by system defenders and attackers to secure and attack hardware, respectively. In this work, we investigate the impact of machine learning on hardware security. We explore the defense and attack mechanisms for hardware that are based on machine learning. Moreover, we identify suitable machine learning algorithms for each category of hardware security problems. Finally, we highlight some important aspects related to the application of machine learning to hardware security problems and show how the practice of applying machine learning to hardware security problems has changed over the past decade.} }
@article{WOS:000472240500010, title = {The Promise of Machine Learning: When Will it be Delivered?}, journal = {JOURNAL OF CARDIAC FAILURE}, volume = {25}, pages = {484-485}, year = {2019}, issn = {1071-9164}, doi = {10.1016/j.cardfail.2019.04.006}, author = {Akbilgic, Oguz and Davis, Robert L.}, abstract = {Background: The real-life applications of machine learning clinical decision making is currently lagging behind its promise. One of the critics on machine learning is that it doesn't outperform more traditional statistical approaches in every problem. Methods and Results: Authors of ``Predictive Abilities of Machine Learning Techniques May Be Limited by Dataset Characteristics: Insights From the UNOS Database'' presented in the current issue of the Journal of Cardiac Failure that machine learning approaches do not provide significantly higher performance when compared to more traditional statistical approaches in predicting mortality following heart transplant. In this brief report, we provide an insight on the possible reasons for why machine learning methods do not outperform more traditional approaches for every problem and every dataset. Conclusions: Most of the performance-focused critics on machine learning are because the bar is set unfairly too high for machine learning. In most cases, machine learning methods provides at least as good results as traditional statistical methods do. It is normal for machine learning models to provide similar performance with linear models if the actual underlying input-outcome relationship is linear. Moreover, machine learning methods outperforms linear statistical models when the underlying input-output relationship is not linear and if the dataset is large enough and include predictors capturing that nonlinear relationship.} }
@article{WOS:000895924400002, title = {Survey on Encoding Schemes for Genomic Data Representation and Feature Learning-From Signal Processing to Machine Learning}, journal = {BIG DATA MINING AND ANALYTICS}, volume = {1}, pages = {191-210}, year = {2018}, doi = {10.26599/BDMA.2018.9020018}, author = {Yu, Ning and Li, Zhihua and Yu, Zeng}, abstract = {Data-driven machine learning, especially deep learning technology, is becoming an important tool for handling big data issues in bioinformatics. In machine learning, DNA sequences are often converted to numerical values for data representation and feature learning in various applications. Similar conversion occurs in Genomic Signal Processing (GSP), where genome sequences are transformed into numerical sequences for signal extraction and recognition. This kind of conversion is also called encoding scheme. The diverse encoding schemes can greatly affect the performance of GSP applications and machine learning models. This paper aims to collect, analyze, discuss, and summarize the existing encoding schemes of genome sequence particularly in GSP as well as other genome analysis applications to provide a comprehensive reference for the genomic data representation and feature learning in machine learning.} }
@article{WOS:000405098300001, title = {A Proposal on Machine Learning via Dynamical Systems}, journal = {COMMUNICATIONS IN MATHEMATICS AND STATISTICS}, volume = {5}, pages = {1-11}, year = {2017}, issn = {2194-6701}, doi = {10.1007/s40304-017-0103-z}, author = {Weinan, E.}, abstract = {We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.} }
@article{WOS:000454710400002, title = {Deep learning-Using machine learning to study biological vision}, journal = {JOURNAL OF VISION}, volume = {18}, year = {2018}, issn = {1534-7362}, doi = {10.1167/18.13.2}, author = {Majaj, Najib J. and Pelli, Denis G.}, abstract = {Many vision science studies employ machine learning, especially the version called ``deep learning.'' Neuroscientists use machine learning to decode neural responses. Perception scientists try to understand how living organisms recognize objects. To them, deep neural networks offer benchmark accuracies for recognition of learned stimuli. Originally machine learning was inspired by the brain. Today, machine learning is used as a statistical tool to decode brain activity. Tomorrow, deep neural networks might become our best model of brain function. This brief overview of the use of machine learning in biological vision touches on its strengths, weaknesses, milestones, controversies, and current directions. Here, we hope to help vision scientists assess what role machine learning should play in their research.} }
@article{WOS:000348105800001, title = {Machine learning for neuroirnaging with scikit-learn}, journal = {FRONTIERS IN NEUROINFORMATICS}, volume = {8}, year = {2014}, doi = {10.3389/fninf.2014.00014}, author = {Abraham, Alexandre and Pedregosa, Fabian and Eickenberg, Michael and Gervais, Philippe and Mueller, Andreas and Kossaifi, Jean and Gramfort, Alexandre and Thirion, Bertrand and Varoquaux, Gael}, abstract = {Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.} }
@article{WOS:000731150400024, title = {Deep learning and machine vision for food processing: A survey}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {4}, pages = {233-249}, year = {2021}, doi = {10.1016/j.crfs.2021.03.009}, author = {Zhu, Lili and Spachos, Petros and Pensini, Erica and Plataniotis, Konstantinos N.}, abstract = {The quality and safety of food is an important issue to the whole society, since it is at the basis of human health, social development and stability. Ensuring food quality and safety is a complex process, and all stages of food processing must be considered, from cultivating, harvesting and storage to preparation and consumption. However, these processes are often labour-intensive. Nowadays, the development of machine vision can greatly assist researchers and industries in improving the efficiency of food processing. As a result, machine vision has been widely used in all aspects of food processing. At the same time, image processing is an important component of machine vision. Image processing can take advantage of machine learning and deep learning models to effectively identify the type and quality of food. Subsequently, follow-up design in the machine vision system can address tasks such as food grading, detecting locations of defective spots or foreign objects, and removing impurities. In this paper, we provide an overview on the traditional machine learning and deep learning methods, as well as the machine vision techniques that can be applied to the field of food processing. We present the current approaches and challenges, and the future trends.} }
@article{WOS:000397585800014, title = {Implementing Machine Learning in Radiology Practice and Research}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {208}, pages = {754-760}, year = {2017}, issn = {0361-803X}, doi = {10.2214/AJR.16.17224}, author = {Kohli, Marc and Prevedello, Luciano M. and Filice, Ross W. and Geis, J. Raymond}, abstract = {OBJECTIVE. The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk. CONCLUSION. Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement.} }
@incollection{WOS:000514284700008, title = {Machine Learning in Neural Networks}, booktitle = {FRONTIERS IN PSYCHIATRY: ARTIFCIAL INTELLIGENCE, PRECISION MEDICINE, AND OTHER PARADIGM SHIFTS}, volume = {1192}, pages = {127-137}, year = {2019}, issn = {0065-2598}, isbn = {978-981-32-9721-0; 978-981-32-9720-3}, doi = {10.1007/978-981-32-9721-0\_7}, author = {Lin, Eugene and Tsai, Shih-Jen}, abstract = {Evidence now suggests that precision psychiatry is becoming a cornerstone of medical practices by providing the patient of psychiatric disorders with the right medication at the right dose at the right time. In light of recent advances in neuroimaging and multi-omics, more and more biomarkers associated with psychiatric diseases and treatment responses are being discovered in precision psychiatry applications by leveraging machine learning and neural network approaches. In this article, we focus on the most recent developments for research in precision psychiatry using machine learning, deep learning, and neural network algorithms, together with neuroimaging and multi-omics data. First, we describe different machine learning approaches that are employed to assess prediction for diagnosis, prognosis, and treatment in various precision psychiatry studies. We also survey probable biomarkers that have been identified to be involved in psychiatric diseases and treatment responses. Furthermore, we summarize the limitations with respect to the mentioned precision psychiatry studies. Finally, we address a discussion of future directions and challenges.} }
@article{WOS:000439628500001, title = {Introduction to the Special Issue on Human-Centered Machine Learning}, journal = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS}, volume = {8}, year = {2018}, issn = {2160-6455}, doi = {10.1145/3205942}, author = {Fiebrink, Rebecca and Gillies, Marco}, abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.} }
@article{WOS:000793273600005, title = {Crack fault diagnosis of rotating machine in nuclear power plant based on ensemble learning}, journal = {ANNALS OF NUCLEAR ENERGY}, volume = {168}, year = {2022}, issn = {0306-4549}, doi = {10.1016/j.anucene.2021.108909}, author = {Zhong, Xianping and Ban, Heng}, abstract = {Crack faults in rotating machines can cause machine shutdown or scrapping, endangering the normal operation and safety of nuclear power plants. Intelligent diagnostic techniques based on machine learn-ing have the potential to diagnose crack faults. However, problems such as scarcity of field fault data and high noise of plant measurements pose challenges to the application of machine learning. This study pro -poses an ensemble learning approach to mitigate the negative impacts of the problems. Ensemble learn-ing is a strategy for combining multiple machine learning models into a composite model. The basic idea of ensemble learning is that even if one model makes a mistake, other models can correct it. Case studies based on bearing and gear system fault experiments show that the proposed ensemble learning models have better diagnostic results than the single model in the presence of noise and small data. (c) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000383849400001, title = {Quantum-Enhanced Machine Learning}, journal = {PHYSICAL REVIEW LETTERS}, volume = {117}, year = {2016}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.117.130501}, author = {Dunjko, Vedran and Taylor, Jacob M. and Briegel, Hans J.}, abstract = {The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.} }
@article{WOS:000353722300005, title = {An introduction to quantum machine learning}, journal = {CONTEMPORARY PHYSICS}, volume = {56}, pages = {172-185}, year = {2015}, issn = {0010-7514}, doi = {10.1080/00107514.2014.964942}, author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco}, abstract = {Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.} }
@article{WOS:000447430700018, title = {A review of automatic selection methods for machine learning algorithms and hyper-parameter values}, journal = {NETWORK MODELING AND ANALYSIS IN HEALTH INFORMATICS AND BIOINFORMATICS}, volume = {5}, year = {2016}, issn = {2192-6662}, doi = {10.1007/s13721-016-0125-6}, author = {Luo, Gang}, abstract = {Machine learning studies automatic algorithms that improve themselves through experience. It is widely used for analyzing and extracting value from large biomedical data sets, or ``big biomedical data,'' advancing biomedical research, and improving healthcare. Before a machine learning model is trained, the user of a machine learning software tool typically must manually select a machine learning algorithm and set one or more model parameters termed hyper-parameters. The algorithm and hyper-parameter values used can greatly impact the resulting model's performance, but their selection requires special expertise as well as many labor-intensive manual iterations. To make machine learning accessible to layman users with limited computing expertise, computer science researchers have proposed various automatic selection methods for algorithms and/or hyper-parameter values for a given supervised machine learning problem. This paper reviews these methods, identifies several of their limitations in the big biomedical data environment, and provides preliminary thoughts on how to address these limitations. These findings establish a foundation for future research on automatically selecting algorithms and hyper-parameter values for analyzing big biomedical data.} }
@article{WOS:000413244600057, title = {Voltage Stability Prediction Using Active Machine Learning}, journal = {IEEE TRANSACTIONS ON SMART GRID}, volume = {8}, pages = {3117-3124}, year = {2017}, issn = {1949-3053}, doi = {10.1109/TSG.2017.2693394}, author = {Malbasa, Vuk and Zheng, Ce and Chen, Po-Chen and Popovic, Tomo and Kezunovic, Mladen}, abstract = {An active machine learning technique for monitoring the voltage stability in transmission systems is presented. It has been shown that machine learning algorithms may be used to supplement the traditional simulation approach, but they suffer from the difficulties of online machine learning model update and offline training data preparation. We propose an active learning solution to enhance existing machine learning applications by actively interacting with the online prediction and offline training process. The technique identifies operating points where machine learning predictions based on power system measurements contradict with actual system conditions. By creating the training set around the identified operating points, it is possible to improve the capability of machine learning tools to predict future power system states. The technique also accelerates the offline training process by reducing the amount of simulations on a detailed power system model around operating points where correct predictions are made. Experiments show a significant advantage in relation to the training time, prediction time, and number of measurements that need to be queried to achieve high prediction accuracy.} }
@article{WOS:000382418300017, title = {Machine learning approaches in medical image analysis: From detection to diagnosis}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {33}, pages = {94-97}, year = {2016}, issn = {1361-8415}, doi = {10.1016/j.media.2016.06.032}, author = {de Bruijne, Marleen}, abstract = {Machine learning approaches are increasingly successful in image-based diagnosis, disease prognosis, and risk assessment. This paper highlights new research directions and discusses three main challenges related to machine learning in medical imaging: coping with variation in imaging protocols, learning from weak labels, and interpretation and evaluation of results. (C) 2016 Elsevier B.V. All rights reserved.} }
@article{WOS:000213214100001, title = {Supervised Machine Learning: A Review of Classification Techniques}, journal = {INFORMATICA-JOURNAL OF COMPUTING AND INFORMATICS}, volume = {31}, pages = {249-268}, year = {2007}, author = {Kotsiantis, S. B.}, abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.} }
@article{WOS:000388629300030, title = {Machine Learning-Based Antenna Selection in Wireless Communications}, journal = {IEEE COMMUNICATIONS LETTERS}, volume = {20}, pages = {2241-2244}, year = {2016}, issn = {1089-7798}, doi = {10.1109/LCOMM.2016.2594776}, author = {Joung, Jingon}, abstract = {This letter is the first attempt to conflate a machine learning technique with wireless communications. Through interpreting the antenna selection (AS) in wireless communications (i.e., an optimization-driven decision) to multiclass-classification learning (i.e., data-driven prediction), and through comparing the learning-based AS using k-nearest neighbors and support vector machine algorithms with conventional optimization-driven AS methods in terms of communications performance, computational complexity, and feedback overhead, we provide insight into the potential of fusion of machine learning and wireless communications.} }
@article{WOS:000412192800056, title = {The ALAMO approach to machine learning}, journal = {COMPUTERS \\& CHEMICAL ENGINEERING}, volume = {106}, pages = {785-795}, year = {2017}, issn = {0098-1354}, doi = {10.1016/j.compchemeng.2017.02.010}, author = {Wilson, Zachary T. and Sahinidis, Nikolaos V.}, abstract = {ALAMO is a computational methodology for learning algebraic functions from data. Given a data set, the approach begins by building a low-complexity, linear model composed of explicit non-linear transformations of the independent variables. Linear combinations of these non-linear transformations allow a linear model to better approximate complex behavior observed in real processes. The model is refined, as additional data are obtained in an adaptive fashion through error maximization sampling using derivative-free optimization. Models built using ALAMO can enforce constraints on the response variables to incorporate first-principles knowledge. The ability of ALAMO to generate simple and accurate models for a number of reaction problems is demonstrated. The error maximization sampling is compared with Latin hypercube designs to demonstrate its sampling efficiency. ALAMO's constrained regression methodology is used to further refine concentration models, resulting in models that perform better on validation data and satisfy upper and lower bounds placed on model outputs. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000412881200013, title = {Comparison of Machine Learning Techniques for Fetal Heart Rate Classification}, journal = {ACTA PHYSICA POLONICA A}, volume = {132}, pages = {451-454}, year = {2017}, issn = {0587-4246}, doi = {10.12693/APhysPolA.132.451}, author = {Comert, Z. and Kocamaz, A. F.}, abstract = {Cardiotocography is a monitoring technique providing important and vital information on fetal status during antepartum and intrapartum periods. The advances in modern obstetric practice allowed many robust and reliable machine learning techniques to be utilized in classifying fetal heart rate signals. The role of machine learning approaches in diagnosing diseases is becoming increasingly essential and intertwined. The main aim of the present study is to determine the most efficient machine learning technique to classify fetal heart rate signals. Therefore, the research has been focused on the widely used and practical machine learning techniques, such as artificial neural network, support vector machine, extreme learning machine, radial basis function network, and random forest. In a comparative way, fetal heart rate signals were classified as normal or hypoxic using the aforementioned machine learning techniques. The performance metrics derived from confusion matrix were used to measure classifiers' success. According to experimental results, although all machine learning techniques produced satisfactory results, artificial neural network yielded the rather well results with the sensitivity of 99.73\\% and specificity of 97.94\\%. The study results show that the artificial neural network was superior to other algorithms.} }
@article{WOS:000489304600003, title = {Fast and simple dataset selection for machine learning}, journal = {AT-AUTOMATISIERUNGSTECHNIK}, volume = {67}, pages = {833-842}, year = {2019}, issn = {0178-2312}, doi = {10.1515/auto-2019-0010}, author = {Peter, Timm J. and Nelles, Oliver}, abstract = {The task of data reduction is discussed and a novel selection approach which allows to control the optimal point distribution of the selected data subset is proposed. The proposed approach utilizes the estimation of probability density functions (pdfs). Due to its structure, the new method is capable of selecting a subset either by approximating the pdf of the original dataset or by approximating an arbitrary, desired target pdf. The new strategy evaluates the estimated pdfs solely on the selected data points, resulting in a simple and efficient algorithm with low computational and memory demand. The performance of the new approach is investigated for two different scenarios. For representative subset selection of a dataset, the new approach is compared to a recently proposed, more complex method and shows comparable results. For the demonstration of the capability of matching a target pdf, a uniform distribution is chosen as an example. Here the new method is compared to strategies for space-filling design of experiments and shows convincing results.} }
@article{WOS:000383095900019, title = {Machine Learning}, journal = {IEEE SOFTWARE}, volume = {33}, pages = {110-115}, year = {2016}, issn = {0740-7459}, doi = {10.1109/MS.2016.114}, author = {Louridas, Panos and Ebert, Christof}, abstract = {Machine learning is the major success factor in the ongoing digital transformation across industries. Startups and behemoths alike announce new products that will learn to perform tasks that previously only humans could do, and perform those tasks better, faster, and more intelligently. But how do they do it? What does it mean for IT developers and software engineers? Here, Panos Louridas and I present a brief overview of machine-learning technologies, with a concrete case study from code analysis. I look forward to hearing from both readers and prospective column authors.} }
@article{WOS:000371639700001, title = {Prototype-based models in machine learning}, journal = {WILEY INTERDISCIPLINARY REVIEWS-COGNITIVE SCIENCE}, volume = {7}, pages = {92-111}, year = {2016}, issn = {1939-5078}, doi = {10.1002/wcs.1378}, author = {Biehl, Michael and Hammer, Barbara and Villmann, Thomas}, abstract = {An overview is given of prototype-based models in machine learning. In this framework, observations, i.e., data, are stored in terms of typical representatives. Together with a suitable measure of similarity, the systems can be employed in the context of unsupervised and supervised analysis of potentially high-dimensional, complex datasets. We discuss basic schemes of competitive vector quantization as well as the so-called neural gas approach and Kohonen's topology-preserving self-organizing map. Supervised learning in prototype systems is exemplified in terms of learning vector quantization. Most frequently, the familiar Euclidean distance serves as a dissimilarity measure. We present extensions of the framework to nonstandard measures and give an introduction to the use of adaptive distances in relevance learning. (C) 2016 Wiley Periodicals, Inc.} }
@article{WOS:000393223300013, title = {A Review of Deep Machine Learning}, journal = {INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH IN AFRICA}, volume = {24}, pages = {124-136}, year = {2016}, issn = {1663-3571}, doi = {10.4028/www.scientific.net/JERA.24.124}, author = {Benuwa, Ben-Bright and Zhan, Yongzhao and Ghansah, Benjamin and Wornyo, Dickson Keddy and Kataka, Frank Banaseka}, abstract = {The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. Recently, Deep Learning (a surrogate of Machine Learning) have won several contests in pattern recognition and machine learning. This review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. This paper also discusses the motivations and principles regarding learning algorithms for deep architectures.} }
@article{WOS:000637534200002, title = {A Survey on Learning-Based Approaches for Modeling and Classification of Human-Machine Dialog Systems}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS}, volume = {32}, pages = {1418-1432}, year = {2021}, issn = {2162-237X}, doi = {10.1109/TNNLS.2020.2985588}, author = {Cui, Fuwei and Cui, Qian and Song, Yongduan}, abstract = {With the rapid development from traditional machine learning (ML) to deep learning (DL) and reinforcement learning (RL), dialog system equipped with learning mechanism has become the most effective solution to address human-machine interaction problems. The purpose of this article is to provide a comprehensive survey on learning-based human-machine dialog systems with a focus on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and also compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and evaluation metrics of the dialog model. Furthermore, the evaluation metrics of these dialog models are analyzed in detail. Finally, we briefly analyze the existing issues and point out the potential future direction on the human-machine dialog systems.} }
@article{WOS:000556008200001, title = {Machine Learning in Big Data}, journal = {INTERNATIONAL JOURNAL OF MATHEMATICAL ENGINEERING AND MANAGEMENT SCIENCES}, volume = {1}, pages = {52-61}, year = {2016}, issn = {2455-7749}, doi = {10.33889/IJMEMS.2016.1.2-006}, author = {Wang, Lidong and Alexander, Cheryl Ann}, abstract = {Machine learning is an artificial intelligence method of discovering knowledge for making intelligent decisions. Big Data has great impacts on scientific discoveries and value creation. This paper introduces methods in machine learning, main technologies in Big Data, and some applications of machine learning in Big Data. Challenges of machine learning applications in Big Data are discussed. Some new methods and technology progress of machine learning in Big Data are also presented.} }
@article{WOS:001136314900001, title = {Machine-learned wearable sensors for real-time hand-motion recognition: toward practical applications}, journal = {NATIONAL SCIENCE REVIEW}, volume = {11}, year = {2024}, issn = {2095-5138}, doi = {10.1093/nsr/nwad298}, author = {Pyun, Kyung Rok and Kwon, Kangkyu and Yoo, Myung Jin and Kim, Kyun Kyu and Gong, Dohyeon and Yeo, Woon-Hong and Han, Seungyong and Ko, Seung Hwan}, abstract = {Soft electromechanical sensors have led to a new paradigm of electronic devices for novel motion-based wearable applications in our daily lives. However, the vast amount of random and unidentified signals generated by complex body motions has hindered the precise recognition and practical application of this technology. Recent advancements in artificial-intelligence technology have enabled significant strides in extracting features from massive and intricate data sets, thereby presenting a breakthrough in utilizing wearable sensors for practical applications. Beyond traditional machine-learning techniques for classifying simple gestures, advanced machine-learning algorithms have been developed to handle more complex and nuanced motion-based tasks with restricted training data sets. Machine-learning techniques have improved the ability to perceive, and thus machine-learned wearable soft sensors have enabled accurate and rapid human-gesture recognition, providing real-time feedback to users. This forms a crucial component of future wearable electronics, contributing to a robust human-machine interface. In this review, we provide a comprehensive summary covering materials, structures and machine-learning algorithms for hand-gesture recognition and possible practical applications through machine-learned wearable electromechanical sensors. This review provides a thorough overview of the current research in machine-learned wearable sensors for real-time hand motion recognition, highlighting current challenges and future directions toward practical applications in reality.} }
@article{WOS:000392585400001, title = {Future Directions in Machine Learning}, journal = {FRONTIERS IN ROBOTICS AND AI}, volume = {3}, year = {2017}, issn = {2296-9144}, doi = {10.3389/frobt.2016.00079}, author = {Greenwald, Hal S. and Oertel, Carsten K.}, abstract = {Current machine learning (ML) algorithms identify statistical regularities in complex data sets and are regularly used across a range of application domains, but they lack the robustness and generalizability associated with human learning. If ML techniques could enable computers to learn from fewer examples, transfer knowledge between tasks, and adapt to changing contexts and environments, the results would have very broad scientific and societal impacts. Increased processing and memory resources have enabled larger, more capable learning models, but there is growing recognition that even greater computing resources would not be sufficient to yield algorithms capable of learning from a few examples and generalizing beyond initial training sets. This paper presents perspectives on feature selection, representation schemes and interpretability, transfer learning, continuous learning, and learning and adaptation in time-varying contexts and environments, five key areas that are essential for advancing ML capabilities. Appropriate learning tasks that require these capabilities can demonstrate the strengths of novel ML approaches that could address these challenges.} }
@article{WOS:001217367400004, title = {A survey on imbalanced learning: latest research, applications and future directions}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {57}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-10759-6}, author = {Chen, Wuxing and Yang, Kaixiang and Yu, Zhiwen and Shi, Yifan and Chen, C. L. Philip}, abstract = {Imbalanced learning constitutes one of the most formidable challenges within data mining and machine learning. Despite continuous research advancement over the past decades, learning from data with an imbalanced class distribution remains a compelling research area. Imbalanced class distributions commonly constrain the practical utility of machine learning and even deep learning models in tangible applications. Numerous recent studies have made substantial progress in the field of imbalanced learning, deepening our understanding of its nature while concurrently unearthing new challenges. Given the field's rapid evolution, this paper aims to encapsulate the recent breakthroughs in imbalanced learning by providing an in-depth review of extant strategies to confront this issue. Unlike most surveys that primarily address classification tasks in machine learning, we also delve into techniques addressing regression tasks and facets of deep long-tail learning. Furthermore, we explore real-world applications of imbalanced learning, devising a broad spectrum of research applications from management science to engineering, and lastly, discuss newly-emerging issues and challenges necessitating further exploration in the realm of imbalanced learning.} }
@article{WOS:000346856600043, title = {A systematic review of machine learning techniques for software fault prediction}, journal = {APPLIED SOFT COMPUTING}, volume = {27}, pages = {504-518}, year = {2015}, issn = {1568-4946}, doi = {10.1016/j.asoc.2014.11.023}, author = {Malhotra, Ruchika}, abstract = {Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. (C) 2014 Elsevier B.V. All rights reserved.} }
@article{WOS:000369369100004, title = {Toward a generic representation of random variables for machine learning}, journal = {PATTERN RECOGNITION LETTERS}, volume = {70}, pages = {24-31}, year = {2016}, issn = {0167-8655}, doi = {10.1016/j.patrec.2015.11.004}, author = {Donnat, Philippe and Marti, Gautier and Very, Philippe}, abstract = {This paper presents a pre-processing and a distance which improve the performance of machine learning algorithms working on independent and identically distributed stochastic processes. We introduce a novel non parametric approach to represent random variables which splits apart dependency and distribution without losing any information. We also propound an associated metric leveraging this representation and its statistical estimate. Besides experiments on synthetic datasets, the benefits of our contribution is illustrated through the example of clustering financial time series, for instance prices from the credit default swaps market. Results are available on the website http://www.datagrapple.com and an IPython Notebook tutorial is available at http://www.datagrapple.com/Tech for reproducible research. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000697475900018, title = {Exploration of machine algorithms based on deep learning model and feature extraction}, journal = {MATHEMATICAL BIOSCIENCES AND ENGINEERING}, volume = {18}, pages = {7602-7618}, year = {2021}, issn = {1547-1063}, doi = {10.3934/mbe.2021376}, author = {Qian, Yufeng}, abstract = {The study expects to solve the problems of insufficient labeling, high input dimension, and inconsistent task input distribution in traditional lifelong machine learning. A new deep learning model is proposed by combining feature representation with a deep learning algorithm. First, based on the theoretical basis of the deep learning model and feature extraction. The study analyzes several representative machine learning algorithms, and compares the performance of the optimized deep learning model with other algorithms in a practical application. By explaining the machine learning system, the study introduces two typical algorithms in machine learning, namely ELLA (Efficient lifelong learning algorithm) and HLLA (Hierarchical lifelong learning algorithm). Second, the flow of the genetic algorithm is described, and combined with mutual information feature extraction in a machine algorithm, to form a composite algorithm HLLA (Hierarchical lifelong learning algorithm). Finally, the deep learning model is optimized and a deep learning model based on the HLLA algorithm is constructed. When K = 1200, the classification error rate reaches 0.63\\%, which reflects the excellent performance of the unsupervised database algorithm based on this model. Adding the feature model to the updating iteration process of lifelong learning deepens the knowledge base ability of lifelong machine learning, which is of great value to reduce the number of labels required for subsequent model learning and improve the efficiency of lifelong learning.} }
@article{WOS:000351430600002, title = {Entanglement-Based Machine Learning on a Quantum Computer}, journal = {PHYSICAL REVIEW LETTERS}, volume = {114}, year = {2015}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.114.110504}, author = {Cai, X. -D. and Wu, D. and Su, Z. -E. and Chen, M. -C. and Wang, X. -L. and Li, Li and Liu, N. -L. and Lu, C. -Y. and Pan, J. -W.}, abstract = {Machine learning, a branch of artificial intelligence, learns from previous experience to optimize performance, which is ubiquitous in various fields such as computer sciences, financial analysis, robotics, and bioinformatics. A challenge is that machine learning with the rapidly growing ``big data'' could become intractable for classical computers. Recently, quantum machine learning algorithms [Lloyd, Mohseni, and Rebentrost, arXiv. 1307.0411] were proposed which could offer an exponential speedup over classical algorithms. Here, we report the first experimental entanglement-based classification of two-, four-, and eight-dimensional vectors to different clusters using a small-scale photonic quantum computer, which are then used to implement supervised and unsupervised machine learning. The results demonstrate the working principle of using quantum computers to manipulate and classify high-dimensional vectors, the core mathematical routine in machine learning. The method can, in principle, be scaled to larger numbers of qubits, and may provide a new route to accelerate machine learning.} }
@article{WOS:000256504400007, title = {Kernel methods in machine learning}, journal = {ANNALS OF STATISTICS}, volume = {36}, pages = {1171-1220}, year = {2008}, issn = {0090-5364}, doi = {10.1214/009053607000000677}, author = {Hofmann, Thomas and Schoelkopf, Bernhard and Smola, Alexander J.}, abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.} }
@article{WOS:001361255100001, title = {Self-Supervised Learning for Near-Wild Cognitive Workload Estimation}, journal = {JOURNAL OF MEDICAL SYSTEMS}, volume = {48}, year = {2024}, issn = {0148-5598}, doi = {10.1007/s10916-024-02122-7}, author = {Rafiei, Mohammad H. and Gauthier, Lynne V. and Adeli, Hojjat and Takabi, Daniel}, abstract = {Feedback on cognitive workload may reduce decision-making mistakes. Machine learning-based models can produce feedback from physiological data such as electroencephalography (EEG) and electrocardiography (ECG). Supervised machine learning requires large training data sets that are (1) relevant and decontaminated and (2) carefully labeled for accurate approximation, a costly and tedious procedure. Commercial over-the-counter devices are low-cost resolutions for the real-time collection of physiological modalities. However, they produce significant artifacts when employed outside of laboratory settings, compromising machine learning accuracies. Additionally, the physiological modalities that most successfully machine-approximate cognitive workload in everyday settings are unknown. To address these challenges, a first-ever hybrid implementation of feature selection and self-supervised machine learning techniques is introduced. This model is employed on data collected outside controlled laboratory settings to (1) identify relevant physiological modalities to machine approximate six levels of cognitive-physical workloads from a seven-modality repository and (2) postulate limited labeling experiments and machine approximate mental-physical workloads using self-supervised learning techniques.} }
@article{WOS:000965299600001, title = {A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {35}, pages = {3347-3366}, year = {2023}, issn = {1041-4347}, doi = {10.1109/TKDE.2021.3124599}, author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng}, abstract = {As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.} }
@article{WOS:001180702200015, title = {Learn to Unlearn: Insights Into Machine Unlearning}, journal = {COMPUTER}, volume = {57}, pages = {79-90}, year = {2024}, issn = {0018-9162}, doi = {10.1109/MC.2023.3333319}, author = {Qu, Youyang and Yuan, Xin and Ding, Ming and Ni, Wei and Rakotoarivelo, Thierry and Smith, David}, abstract = {This article presents a comprehensive review of recent machine unlearning techniques, verification mechanisms, and potential attacks. We highlight emerging challenges and prospective research directions, aiming to provide valuable resources for integrating privacy, equity, and resilience into machine learning systems and help them ``learn to unlearn.''} }
@article{WOS:000394061800034, title = {Generalized extreme learning machine autoencoder and a new deep neural network}, journal = {NEUROCOMPUTING}, volume = {230}, pages = {374-381}, year = {2017}, issn = {0925-2312}, doi = {10.1016/j.neucom.2016.12.027}, author = {Sun, Kai and Zhang, Jiangshe and Zhang, Chunxia and Hu, Junying}, abstract = {Extreme learning machine (ELM) is an efficient learning algorithm of training single layer feed-forward neural networks (SLFNs). With the development of unsupervised learning in recent years, integrating ELM with autoencoder has become a new perspective for extracting feature using unlabeled data. In this paper, we propose a new variant of extreme learning machine autoencoder (ELM-AE) called generalized extreme learning machine autoencoder (GELM-AE) which adds the manifold regularization to the objective of ELM-AE. Some experiments carried out on real-world data sets show that GELM-AE outperforms some state-of-the-art unsupervised learning algorithms, including k-means, laplacian embedding (LE), spectral clustering (SC) and ELM-AE. Furthermore, we also propose a new deep neural network called multilayer generalized extreme learning machine autoencoder (ML-GELM) by stacking several GELM-AE to detect more abstract representations. The experiments results show that ML-GELM outperforms ELM and many other deep models, such as multilayer ELM autoencoder (ML-ELM), deep belief network (DBN) and stacked autoencoder (SAE). Due to the utilization of ELM, ML-GELM is also faster than DBN and SAE.} }
@article{WOS:000398821300014, title = {Unsupervised extreme learning machine with representational features}, journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS}, volume = {8}, pages = {587-595}, year = {2017}, issn = {1868-8071}, doi = {10.1007/s13042-015-0351-8}, author = {Ding, Shifei and Zhang, Nan and Zhang, Jian and Xu, Xinzheng and Shi, Zhongzhi}, abstract = {Extreme learning machine (ELM) is not only an effective classifier but also a useful cluster. Unsupervised extreme learning machine (US-ELM) gives favorable performance compared to state-of-the-art clustering algorithms. Extreme learning machine as an auto encoder (ELM-AE) can obtain principal components which represent original samples. The proposed unsupervised extreme learning machine based on embedded features of ELM-AE (US-EF-ELM) algorithm applies ELM-AE to US-ELM. US-EF-ELM regards embedded features of ELM-AE as the outputs of US-ELM hidden layer, and uses US-ELM to obtain the embedded matrix of US-ELM. US-EF-ELM can handle the multi-cluster clustering. The learning capability and computational efficiency of US-EF-ELM are as same as US-ELM. By experiments on UCI data sets, we compared US-EF-ELM k-means algorithm with k-means algorithm, spectral clustering algorithm, and US-ELM k-means algorithm in accuracy and efficiency.} }
@article{WOS:000425723300001, title = {Fault diagnosis method based on wavelet packet-energy entropy and fuzzy kernel extreme learning machine}, journal = {ADVANCES IN MECHANICAL ENGINEERING}, volume = {10}, year = {2018}, issn = {1687-8140}, doi = {10.1177/1687814017751446}, author = {Ma, Jun and Wu, Jiande and Wang, Xiaodong}, abstract = {Aiming at connatural limitations of extreme learning machine in practice, a new fault diagnosis method based on wavelet packet-energy entropy and fuzzy kernel extreme learning machine is proposed. On one hand, the presented method can extract the more efficient features using the wavelet packet-energy entropy method, and on the other hand, the sample fuzzy membership degree matrix U, weight matrix W which is used to describe the sample imbalance, and the kernel function are introduced to construct the fuzzy kernel extreme learning machine model with high accuracy and reliability. The experimental results of rolling bearing and check valve are obtained and analyzed in MATLAB 2010b. The results show that the proposed fuzzy kernel extreme learning machine method can obtain fairly or slightly better classification performance than the traditional extreme learning machine, kernel extreme learning machine, back propagation, support vector machine, and fuzzy support vector machine.} }
@article{WOS:000480422500020, title = {Towards Achieving Machine Comprehension Using Deep Learning on Non-GPU Machines}, journal = {ENGINEERING TECHNOLOGY \\& APPLIED SCIENCE RESEARCH}, volume = {9}, pages = {4423-4427}, year = {2019}, issn = {2241-4487}, author = {Khan, Uzair and Khan, Khalid and Hasssan, Fadzil and Siddiqui, Anam and Afaq, Muhammad}, abstract = {Long efforts have been made to enable machines to understand human language. Nowadays such activities fall under the broad umbrella of machine comprehension. The results are optimistic due to the recent advancements in the field of machine learning. Deep learning promises to bring even better results but requires expensive and resource hungry hardware. In this paper, we demonstrate the use of deep learning in the context of machine comprehension by using non-GPU machines. Our results suggest that the good algorithm insight and detailed understanding of the dataset can help in getting meaningful results through deep learning even on non-GPU machines.} }
@article{WOS:000861328200001, title = {A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects}, journal = {IEEE ACCESS}, volume = {10}, pages = {99129-99149}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3207287}, author = {Mienye, Ibomoiye Domor and Sun, Yanxia}, abstract = {Ensemble learning techniques have achieved state-of-the-art performance in diverse machine learning applications by combining the predictions from two or more base models. This paper presents a concise overview of ensemble learning, covering the three main ensemble methods: bagging, boosting, and stacking, their early development to the recent state-of-the-art algorithms. The study focuses on the widely used ensemble algorithms, including random forest, adaptive boosting (AdaBoost), gradient boosting, extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), and categorical boosting (CatBoost). An attempt is made to concisely cover their mathematical and algorithmic representations, which is lacking in the existing literature and would be beneficial to machine learning researchers and practitioners.} }
@article{WOS:000312959700016, title = {Model-based machine learning}, journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES}, volume = {371}, year = {2013}, issn = {1364-503X}, doi = {10.1098/rsta.2012.0222}, author = {Bishop, Christopher M.}, abstract = {Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.} }
@article{WOS:000401624200005, title = {Unsupervised and semi-supervised extreme learning machine with wavelet kernel for high dimensional data}, journal = {MEMETIC COMPUTING}, volume = {9}, pages = {129-139}, year = {2017}, issn = {1865-9284}, doi = {10.1007/s12293-016-0198-x}, author = {Zhang, Nan and Ding, Shifei}, abstract = {Extreme learning machine (ELM) not only is an effective classifier in supervised learning, but also can be applied on unsupervised learning and semi-supervised learning. The model structure of unsupervised extreme learning machine (US-ELM) and semi-supervised extreme learning machine (SS-ELM) are same as ELM, the difference between them is the cost function. We introduce kernel function to US-ELM and propose unsupervised extreme learning machine with kernel (US-KELM). And SS-KELM has been proposed. Wavelet analysis has the characteristics of multivariate interpolation and sparse change, and Wavelet kernel functions have been widely used in support vector machine. Therefore, to realize a combination of the wavelet kernel function, US-ELM, and SS-ELM, unsupervised extreme learning machine with wavelet kernel function (US-WKELM) and semi-supervised extreme learning machine with wavelet kernel function (SS-WKELM) are proposed in this paper. The experimental results show the feasibility and validity of US-WKELM and SS-WKELM in clustering and classification.} }
@article{WOS:000620462600001, title = {A survey on federated learning}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {216}, year = {2021}, issn = {0950-7051}, doi = {10.1016/j.knosys.2021.106775}, author = {Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan}, abstract = {Federated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning. (C) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000435287000002, title = {Ensemble learning: A survey}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {8}, year = {2018}, issn = {1942-4787}, doi = {10.1002/widm.1249}, author = {Sagi, Omer and Rokach, Lior}, abstract = {Ensemble methods are considered the state-of-the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state-of-the-art ensemble methods and discusses current challenges and trends in the field. This article is categorized under: Algorithmic Development > Model Combining Technologies > Machine Learning Technologies > Classification} }
@article{WOS:000327394500015, title = {Matched-Pair Machine Learning}, journal = {TECHNOMETRICS}, volume = {55}, pages = {536-547}, year = {2013}, issn = {0040-1706}, doi = {10.1080/00401706.2013.838191}, author = {Theiler, James}, abstract = {Following an analogous distinction in statistical hypothesis testing and motivated by chemical plume detection in hyperspectral imagery, we investigate machine-learning algorithms where the training set is comprised of matched pairs. We find that even conventional classifiers exhibit improved performance when the input data have a matched-pair structure, and we develop an example of a ``dipole'' algorithm to directly exploit this structured input. In some scenarios, matched pairs can be generated from independent samples, with the effect of not only doubling the nominal size of the training set, but of providing the matched-pair structure that leads to better learning. The creation of matched pairs from a dataset of interest also permits a kind of transductive learning, which is found for the plume detection problem to exhibit improved performance. Supplementary materials for this article are available online.} }
@article{WOS:000209646300007, title = {PARADIGMS FOR REALIZING MACHINE LEARNING ALGORITHMS}, journal = {BIG DATA}, volume = {1}, pages = {BD207-BD214}, year = {2013}, issn = {2167-6461}, doi = {10.1089/big.2013.0006}, author = {Agneeswaran, Vijay Srinivas and Tonpay, Pranay and Tiwary, Jayati}, abstract = {The article explains the three generations of machine learning algorithms-with all three trying to operate on big data. The first generation tools are SAS, SPSS, etc., while second generation realizations include Mahout and RapidMiner (that work over Hadoop), and the third generation paradigms include Spark and GraphLab, among others. The essence of the article is that for a number of machine learning algorithms, it is important to look beyond the Hadoop's Map- Reduce paradigm in order to make them work on big data. A number of promising contenders have emerged in the third generation that can be exploited to realize deep analytics on big data.} }
@article{WOS:001068816800051, title = {Recent Advances for Quantum Neural Networks in Generative Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {45}, pages = {12321-12340}, year = {2023}, issn = {0162-8828}, doi = {10.1109/TPAMI.2023.3272029}, author = {Tian, Jinkai and Sun, Xiaoyu and Du, Yuxuan and Zhao, Shanshan and Liu, Qing and Zhang, Kaining and Yi, Wei and Huang, Wanrong and Wang, Chaoyue and Wu, Xingyao and Hsieh, Min-Hsiu and Liu, Tongliang and Yang, Wenjing and Tao, Dacheng}, abstract = {Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum variational autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relations and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.} }
@article{WOS:000645896700002, title = {Toward Causal Representation Learning}, journal = {PROCEEDINGS OF THE IEEE}, volume = {109}, pages = {612-634}, year = {2021}, issn = {0018-9219}, doi = {10.1109/JPROC.2021.3058954}, author = {Schoelkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua}, abstract = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.} }
@article{WOS:000660868300001, title = {The MLIP package: moment tensor potentials with MPI and active learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abc9fe}, author = {Novikov, Ivan S. and Gubaev, Konstantin and Podryabinkin, V, Evgeny and Shapeev, V, Alexander}, abstract = {The subject of this paper is the technology (the `how') of constructing machine-learning interatomic potentials, rather than science (the `what' and `why') of atomistic simulations using machine-learning potentials. Namely, we illustrate how to construct moment tensor potentials using active learning as implemented in the MLIP package, focusing on the efficient ways to automatically sample configurations for the training set, how expanding the training set changes the error of predictions, how to set up ab initio calculations in a cost-effective manner, etc. The MLIP package (short for Machine-Learning Interatomic Potentials) is available at https://mlip.skoltech.ru/download/.} }
@article{WOS:000314529000003, title = {DARWIN: A Framework for Machine Learning and Computer Vision Research and Development}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {13}, pages = {3533-3537}, year = {2012}, issn = {1532-4435}, author = {Gould, Stephen}, abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.} }
@article{WOS:000518368000010, title = {Deep learning and artificial intelligence methods for Raman and surface-enhanced Raman scattering}, journal = {TRAC-TRENDS IN ANALYTICAL CHEMISTRY}, volume = {124}, year = {2020}, issn = {0165-9936}, doi = {10.1016/j.trac.2019.115796}, author = {Lussier, Felix and Thibault, Vincent and Charron, Benjamin and Wallace, Gregory Q. and Masson, Jean-Francois}, abstract = {Machine learning is shaping up our lives in many ways. In analytical sciences, machine learning provides an unprecedented opportunity to extract information from complex or big datasets in chromatography, mass spectrometry, NMR, and spectroscopy, among others. This is especially the case in Raman and surface-enhanced Raman scattering (SERS) techniques where vibrational spectra of complex chemical mixtures are acquired as large datasets for the analysis or imaging of chemical systems. The classical linear methods of processing the information no longer suffice and thus machine learning methods for extracting the chemical information from Raman and SERS experiments have been implemented recently. In this review, we will provide a brief overview of the most common machine learning techniques employed in Raman, a guideline for new users to implement machine learning in their data analysis process, and an overview of modern applications of machine learning in Raman and SERS. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000302331900002, title = {Challenges and Opportunities in Applied Machine Learning}, journal = {AI MAGAZINE}, volume = {33}, pages = {11-24}, year = {2012}, issn = {0738-4602}, doi = {10.1609/aimag.v33i1.2367}, author = {Brodley, Carla E. and Rebbapragada, Umaa and Small, Kevin and Wallace, Byron C.}, abstract = {Machine-learning research is often conducted in vitro, divorced from motivating practical applications. A researcher might develop a new method for the general task of classification, then assess its utility by comparing its performance (for example, accuracy or AUC) to that of existing classification models on publicly available data sets. In terms of advancing machine learning as an academic discipline, this approach has thus far proven quite fruitful. However, it is our view that the most interesting open problems in machine learning are those that arise during its application to real-world problems. We illustrate this point by reviewing two of our interdisciplinary collaborations, both of which have posed unique machine-learning problems, providing fertile ground for novel research.} }
@article{WOS:000647800900004, title = {A deep learning based life prediction method for components under creep, fatigue and creep-fatigue conditions}, journal = {INTERNATIONAL JOURNAL OF FATIGUE}, volume = {148}, year = {2021}, issn = {0142-1123}, doi = {10.1016/j.ijfatigue.2021.106236}, author = {Zhang, Xiao-Cheng and Gong, Jian-Guo and Xuan, Fu-Zhen}, abstract = {Deep learning is a particular kind of machine learning, which achieves great power and flexibility by a nested hierarchy of concepts. A general life prediction method for components under creep, fatigue and creep-fatigue conditions is proposed. Fatigue, creep and creep-fatigue data of a typical austenitic stainless steel (i.e., 316) are integrated. Conventional machine learning models (e.g., support vector machine, random forest, Gaussian process regression, shallow neural network) and deep learning model (e.g., deep neural network) are applied for life predictions. Results show that deep learning model exhibits better prediction accuracy and generalization ability than conventional machine learning model.} }
@article{WOS:000252222600001, title = {Machine learning: a review of classification and combining techniques}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {26}, pages = {159-190}, year = {2006}, issn = {0269-2821}, doi = {10.1007/s10462-007-9052-3}, author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.}, abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy-ensembles of classifiers.} }
@article{WOS:000283941100019, title = {Learning Machine Learning: A Case Study}, journal = {IEEE TRANSACTIONS ON EDUCATION}, volume = {53}, pages = {672-676}, year = {2010}, issn = {0018-9359}, doi = {10.1109/TE.2009.2038992}, author = {Lavesson, Niklas}, abstract = {This correspondence reports on a case study conducted in the Master's-level Machine Learning (ML) course at Blekinge Institute of Technology, Sweden. The students participated in a self-assessment test and a diagnostic test of prerequisite subjects, and their results on these tests are correlated with their achievement of the course's learning objectives.} }
@article{WOS:000474499500001, title = {Deep learning and its application in geochemical mapping}, journal = {EARTH-SCIENCE REVIEWS}, volume = {192}, pages = {1-14}, year = {2019}, issn = {0012-8252}, doi = {10.1016/j.earscirev.2019.02.023}, author = {Zuo, Renguang and Xiong, Yihui and Wang, Jian and Carranza, Emmanuel John M.}, abstract = {Machine learning algorithms have been applied widely in the fields of natural science, social science and engineering. It can be expected that machine learning approaches especially deep learning algorithms will help geoscientists to discover mineral deposits through processing of various geoscience datasets. This study reviews the state-of-the-art application of deep learning algorithms for processing geochemical exploration data and mining the geochemical patterns. Deep learning algorithms can deal with complex and nonlinear problems and, therefore, can enhance the identification of geochemical anomalies and the recognition of hidden patterns. Applied geochemistry needs more applications of machine learning and/or deep learning algorithms.} }
@article{WOS:000267732400003, title = {Interacting meaningfully with machine learning systems: Three experiments}, journal = {INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES}, volume = {67}, pages = {639-662}, year = {2009}, issn = {1071-5819}, doi = {10.1016/j.ijhcs.2009.03.004}, author = {Stumpf, Simone and Rajaram, Vidya and Li, Lida and Wong, Weng-Keen and Burnett, Margaret and Dietterich, Thomas and Sullivan, Erin and Herlocker, Jonathan}, abstract = {Although machine learning is becoming commonly used in today's software, there has been little research into how end users might interact with machine learning systems, beyond communicating simple ``right/wrong'' judgments. If the users themselves could work hand-in-hand with machine learning systems, the users' understanding and trust of the system could improve and the accuracy of learning systems could be improved as well. We conducted three experiments to understand the potential for rich interactions between users and machine learning systems. The first experiment was a think-aloud study that investigated users' willingness to interact with machine learning reasoning, and what kinds of feedback users might give to machine learning systems. We then investigated the viability of introducing such feedback into machine learning systems, specifically, how to incorporate some of these types of user feedback into machine learning systems, and what their impact was on the accuracy of the system. Taken together, the results of our experiments show that supporting rich interactions between users and machine learning systems is feasible for both user and machine. This shows the potential of rich human-computer collaboration via on-the-spot interactions as a promising direction for machine learning systems and users to collaboratively share intelligence. Published by Elsevier Ltd.} }
@article{WOS:000258760000002, title = {Structured machine learning: the next ten years}, journal = {MACHINE LEARNING}, volume = {73}, pages = {3-23}, year = {2008}, issn = {0885-6125}, doi = {10.1007/s10994-008-5079-1}, author = {Dietterich, Thomas G. and Domingos, Pedro and Getoor, Lise and Muggleton, Stephen and Tadepalli, Prasad}, abstract = {The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.} }
@incollection{WOS:000448519100007, title = {Deep Learning and Its Application to LHC Physics}, booktitle = {ANNUAL REVIEW OF NUCLEAR AND PARTICLE SCIENCE, VOL 68}, volume = {68}, pages = {161-181}, year = {2018}, issn = {0163-8998}, isbn = {978-0-8243-1568-9}, doi = {10.1146/annurev-nucl-101917-021019}, author = {Guest, Dan and Cranmer, Kyle and Whiteson, Daniel}, abstract = {Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis arc explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LIIC data, and discussion of future prospects and concerns.} }
@article{WOS:000241792300002, title = {Recent advances in predictive (machine) learning}, journal = {JOURNAL OF CLASSIFICATION}, volume = {23}, pages = {175-197}, year = {2006}, issn = {0176-4268}, doi = {10.1007/s00357-006-0012-4}, author = {Friedman, Jerome H.}, abstract = {Prediction involves estimating the unknown value of an attribute of a system under study given the values of other measured attributes. In prediction (machine) learning the prediction rule is derived from data consisting of previously solved cases. Most methods for predictive learning were originated many years ago at the dawn of the computer age. Recently two new techniques have emerged that have revitalized the field. These are support vector machines and boosted decision trees. This paper provides an introduction to these two new methods tracing their respective ancestral roots to standard kernel methods and ordinary decision trees.} }
@article{WOS:000245388800005, title = {The interplay of optimization and machine learning research}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {7}, pages = {1265-1281}, year = {2006}, issn = {1532-4435}, author = {Bennett, Kristin P. and Parrado-Hernandez, Emilio}, abstract = {The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.} }
@article{WOS:000231025700002, title = {Machine-learning techniques and their applications in manufacturing}, journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF ENGINEERING MANUFACTURE}, volume = {219}, pages = {395-412}, year = {2005}, issn = {0954-4054}, doi = {10.1243/095440505X32274}, author = {Pham, D. T. and Afify, A. A.}, abstract = {Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.} }
@article{WOS:000629093200003, title = {Predictive Modeling for Machining Power Based on Multi-source Transfer Learning in Metal Cutting}, journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN TECHNOLOGY}, volume = {9}, pages = {107-125}, year = {2022}, issn = {2288-6206}, doi = {10.1007/s40684-021-00327-6}, author = {Kim, Young-Min and Shin, Seung-Jun and Cho, Hae-Won}, abstract = {Energy efficiency has become crucial in the metal cutting industry. Machining power has therefore become an important metric because it directly affects the energy consumed during the operation of a machine tool. Attempts to predict machining power using machine learning have relied on the training datasets processed from actual machining data to derive the numerical relationship between process parameters and machining power. However, real fields hardly provide training datasets because of the difficulties in data collection; consequently, traditional learning approaches are ineffective in such data-scarce or -absent environment. This paper proposes a transfer learning approach for the predictive modeling of machining power. The proposed approach creates machining power prediction models by transferring the knowledge acquired from prior machining to the target machining context where machining power data are absent. The proposed approach performs domain adaptation by adding workpiece material properties to the original feature space for accommodating different machining power patterns dependent on the types of workpiece materials. A case study demonstrates that the training datasets obtained from the fabrication of steel and aluminum materials can be successfully used to create the power-predictive models that anticipate machining power for titanium material.} }
@article{WOS:000341593600009, title = {An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels}, journal = {COGNITIVE COMPUTATION}, volume = {6}, pages = {376-390}, year = {2014}, issn = {1866-9956}, doi = {10.1007/s12559-014-9255-2}, author = {Huang, Guang-Bin}, abstract = {Extreme learning machines (ELMs) basically give answers to two fundamental learning problems: (1) Can fundamentals of learning (i.e., feature learning, clustering, regression and classification) be made without tuning hidden neurons (including biological neurons) even when the output shapes and function modeling of these neurons are unknown? (2) Does there exist unified framework for feedforward neural networks and feature space methods? ELMs that have built some tangible links between machine learning techniques and biological learning mechanisms have recently attracted increasing attention of researchers in widespread research areas. This paper provides an insight into ELMs in three aspects, viz: random neurons, random features and kernels. This paper also shows that in theory ELMs (with the same kernels) tend to outperform support vector machine and its variants in both regression and classification applications with much easier implementation.} }
@article{WOS:000697350100008, title = {Privacy attacks against deep learning models and their countermeasures}, journal = {JOURNAL OF SYSTEMS ARCHITECTURE}, volume = {114}, year = {2021}, issn = {1383-7621}, doi = {10.1016/j.sysarc.2020.101940}, author = {Shafee, Ahmed and Awaad, Tasneem A.}, abstract = {Keywords: Adversarial machine learning Convolutional neural network Deep neural network Machine learning} }
@article{WOS:000623001500001, title = {Quantum Reinforcement Learning with Quantum Photonics}, journal = {PHOTONICS}, volume = {8}, year = {2021}, doi = {10.3390/photonics8020033}, author = {Lamata, Lucas}, abstract = {Quantum machine learning has emerged as a promising paradigm that could accelerate machine learning calculations. Inside this field, quantum reinforcement learning aims at designing and building quantum agents that may exchange information with their environment and adapt to it, with the aim of achieving some goal. Different quantum platforms have been considered for quantum machine learning and specifically for quantum reinforcement learning. Here, we review the field of quantum reinforcement learning and its implementation with quantum photonics. This quantum technology may enhance quantum computation and communication, as well as machine learning, via the fruitful marriage between these previously unrelated fields.} }
@article{WOS:000544215800001, title = {Deep kernel learning in extreme learning machines}, journal = {PATTERN ANALYSIS AND APPLICATIONS}, volume = {24}, pages = {11-19}, year = {2021}, issn = {1433-7541}, doi = {10.1007/s10044-020-00891-8}, author = {Afzal, A. L. and Nair, Nikhitha K. and Asharaf, S.}, abstract = {Emergence of extreme learning machine as a breakneck learning algorithm has marked its prominence in solitary hidden layer feed-forward networks. Kernel-based extreme learning machine (KELM) reflected its efficiency in diverse applications where feature mapping functions of hidden nodes are concealed from users. The conventional KELM algorithms involve only solitary layer of kernels, thereby emulating shallow learning architectures for its feature transformation. Trend in migrating shallow-based learning models into deep learning architectures opens up a new outlook for machine learning domains. This paper attempts to bestow deep kernel learning approach in a conventional shallow architecture. The emerging arc-cosine kernels possess the potential to mimic the prevailing deep layered frameworks to a greater extent. Unlike other kernels such as linear, polynomial and Gaussian, arc-cosine kernels have a recursive nature by itself and have the potential to express multilayer computation in learning models. This paper explores the possibility of building a new deep kernel machine with extreme learning machine and multilayer arc-cosine kernels. This framework outperforms conventional KELM and deep support vector machine in terms of training time and accuracy.} }
@article{WOS:000494800100001, title = {Learning Moore machines from input-output traces}, journal = {INTERNATIONAL JOURNAL ON SOFTWARE TOOLS FOR TECHNOLOGY TRANSFER}, volume = {23}, pages = {1-29}, year = {2021}, issn = {1433-2779}, doi = {10.1007/s10009-019-00544-0}, author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos}, abstract = {The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper, we study this problem for finite-state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. We also carry out a performance comparison against two existing tools (LearnLib and flexfringe). Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.} }
@article{WOS:000424176900011, title = {Deep multiple multilayer kernel learning in core vector machines}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {96}, pages = {149-156}, year = {2018}, issn = {0957-4174}, doi = {10.1016/j.eswa.2017.11.058}, author = {Afzal, A. L. and Asharaf, S.}, abstract = {Over the last few years, we have been witnessing a dramatic progress of deep learning in many real world applications. Deep learning concepts have been originated in the area of neural network and show a quantum leap in effective feature learning techniques such as auto-encoders, convolutional neural networks, recurrent neural networks etc. In the case of kernel machines, there are several attempts to model learning machines that mimic deep neural networks. In this direction, Multilayer Kernel Machines (MKMs) was an attempt to build a kernel machine architecture with multiple layers of feature extraction. It composed of many layers of kernel PCA based feature extraction units with support vector machine having arc-cosine kernel as the final classifier. The other approaches like Multiple Kernel Learning (MKL) and deep core vector machines solve the fixed kernel computation problem and scalability aspects of MKMs respectively. In addition to this, there are lot of avenues where the use of unsupervised MKL with both single and multilayer kernels in the multilayer feature extraction framework have to be evaluated. In this context, this paper attempts to build a scalable deep kernel machines with multiple layers of feature extraction. Each kernel PCA based feature extraction layer in this framework is modeled by the combination of both single and multilayer kernels in an unsupervised manner. Core vector machine with arc-cosine kernel is used as the final layer classifier which ensure the scalability in this model. The major contribution of this paper is a novel effort to build a deep structured kernel machine architecture similar to deep neural network architecture for classification. It opens up an extendable research avenue for researchers in deep learning based intelligent system leveraging the principles of kernel theory. Experiments show that the proposed method consistently improves the generalization performances of existing deep core vector machine. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000927333400001, title = {Smart farming using artificial intelligence: A review}, journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}, volume = {120}, year = {2023}, issn = {0952-1976}, doi = {10.1016/j.engappai.2023.105899}, author = {Akkem, Yaganteeswarudu and Biswas, Saroj Kumar and Varanasi, Aruna}, abstract = {Smart farming with artificial intelligence provides an efficient solution to today's agricultural sustainability challenges. Machine learning, Deep learning, and time series analysis are essential in smart farming. Crop selection, crop yield prediction, soil compatibility classification, water management, and many other processes are involved in agriculture. Machine learning algorithms are used for crop selection and management, Deep learning techniques are used for crop selection and forecasting crop production, and time series analysis is used for demand forecasting of crops, commodity price prediction, and crop yield production forecasting. Crops are chosen using machine learning algorithms and deep learning algorithms based on soil, soil compatibility classification, and other factors. In the agriculture industry, this article offers a thorough review of machine learning and deep learning techniques. Crop data sets can be used to classify soil fertility, crop selection, and many other aspects using machine learning algorithms. Deep learning algorithms can be applied to farming data to do time series analysis and crop selection. Because there is more need for food due to the growing population, crop production forecasting is one of the crucial tasks. Therefore, future crop production must be predicted in order to overcome food insufficiency. In this article, several time series algorithms were reviewed. Suggesting appropriate crop recommendations using machine and deep learning by estimating crop yield by using time series analysis will reduce food insufficiency in the future.} }
@article{WOS:001242376600007, title = {Recent Developments and Future Directions of Wearable Skin Biosignal Sensors}, journal = {ADVANCED SENSOR RESEARCH}, volume = {3}, year = {2024}, issn = {2751-1219}, doi = {10.1002/adsr.202300118}, author = {Kim, Dohyung and Min, JinKi and Ko, Seung Hwan}, abstract = {This review article explores the transformative advancements in wearable biosignal sensors powered by machine learning, focusing on four notable biosignals: electrocardiogram (ECG), electromyogram (EMG), electroencephalogram (EEG), and photoplethysmogram (PPG). The integration of machine learning with these biosignals has led to remarkable breakthroughs in various medical monitoring and human-machine interface applications. For ECG, machine learning enables automated heartbeat classification and accurate disease detection, improving cardiac healthcare with early diagnosis and personalized interventions. EMG technology, combined with machine learning, facilitates real-time prediction and classification of human motions, revolutionizing applications in sports medicine, rehabilitation, prosthetics, and virtual reality interfaces. EEG analysis powered by machine learning goes beyond traditional clinical applications, enabling brain activity understanding in psychology, neurology, and human-computer interaction, and holds promise in brain-computer interfaces. PPG, augmented with machine learning, has shown exceptional progress in diagnosing and monitoring cardiovascular and respiratory disorders, offering non-invasive and accurate healthcare solutions. These integrated technologies, powered by machine learning, open new avenues for medical monitoring and human-machine interaction, shaping the future of healthcare. The convergence of wearable biosignal sensors and machine learning paves the way for significant advancements in healthcare, enabling early medical diagnosis and personalized health monitoring. This review article provides an overview of recent transformative advancements in wearable biosignal sensors powered by machine learning, focusing on four notable biosignals: electrocardiogram, electromyogram, electroencephalogram, and photoplethysmogram. image} }
@article{WOS:000705073600019, title = {AutoML to Date and Beyond: Challenges and Opportunities}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3470918}, author = {Karmaker (Santu), Shubhra Kanti and Hassan, Md Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan}, abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML's main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually-generally by a data scientist-and explain how this limits domain experts' access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.} }
@article{WOS:000751250600001, title = {AutoML: state of the art with a focus on anomaly detection, challenges, and research directions}, journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS}, volume = {14}, pages = {113-126}, year = {2022}, issn = {2364-415X}, doi = {10.1007/s41060-022-00309-0}, author = {Bahri, Maroua and Salutari, Flavia and Putina, Andrian and Sozio, Mauro}, abstract = {The last decade has witnessed the explosion of machine learning research studies with the inception of several algorithms proposed and successfully adopted in different application domains. However, the performance of multiple machine learning algorithms is very sensitive to multiple ingredients (e.g., hyper-parameters tuning and data cleaning) where a significant human effort is required to achieve good results. Thus, building well-performing machine learning algorithms requires domain knowledge and highly specialized data scientists. Automated machine learning (autoML) aims to make easier and more accessible the use of machine learning algorithms for researchers with varying levels of expertise. Besides, research effort to date has mainly been devoted to autoML for supervised learning, and only a few research proposals have been provided for the unsupervised learning. In this paper, we present an overview of the autoML field with a particular emphasis on the automated methods and strategies that have been proposed for unsupervised anomaly detection.} }
@article{WOS:000703968500001, title = {Algorithmic reparation}, journal = {BIG DATA \\& SOCIETY}, volume = {8}, year = {2021}, issn = {2053-9517}, doi = {10.1177/20539517211044808}, author = {Davis, Jenny L. and Williams, Apryl and Yang, Michael W.}, abstract = {Machine learning algorithms pervade contemporary society. They are integral to social institutions, inform processes of governance, and animate the mundane technologies of daily life. Consistently, the outcomes of machine learning reflect, reproduce, and amplify structural inequalities. The field of fair machine learning has emerged in response, developing mathematical techniques that increase fairness based on anti-classification, classification parity, and calibration standards. In practice, these computational correctives invariably fall short, operating from an algorithmic idealism that does not, and cannot, address systemic, Intersectional stratifications. Taking present fair machine learning methods as our point of departure, we suggest instead the notion and practice of algorithmic reparation. Rooted in theories of Intersectionality, reparative algorithms name, unmask, and undo allocative and representational harms as they materialize (American English sp) in sociotechnical form. We propose algorithmic reparation as a foundation for building, evaluating, adjusting, and when necessary, omitting and eradicating machine learning systems.} }
@article{WOS:000452640000033, title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules}, journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}, volume = {25}, pages = {342-352}, year = {2019}, issn = {1077-2626}, doi = {10.1109/TVCG.2018.2864812}, author = {Ming, Yao and Qu, Huamin and Bertini, Enrico}, abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.} }
@article{WOS:000498849000042, title = {Preventing undesirable behavior of intelligent machines}, journal = {SCIENCE}, volume = {366}, pages = {999+}, year = {2019}, issn = {0036-8075}, doi = {10.1126/science.aag3311}, author = {Thomas, Philip S. and da Silva, Bruno Castro and Barto, Andrew G. and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma}, abstract = {Intelligent machines using machine learning algorithms are ubiquitous, ranging from simple data analysis and pattern recognition tools to complex systems that achieve superhuman performance on various tasks. Ensuring that they do not exhibit undesirable behavior-that they do not, for example, cause harm to humans-is therefore a pressing problem. We propose a general and flexible framework for designing machine learning algorithms. This framework simplifies the problem of specifying and regulating undesirable behavior. To show the viability of this framework, we used it to create machine learning algorithms that precluded the dangerous behavior caused by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning.} }
@article{WOS:000572786300004, title = {Deep support vector neural networks}, journal = {INTEGRATED COMPUTER-AIDED ENGINEERING}, volume = {27}, pages = {389-402}, year = {2020}, issn = {1069-2509}, doi = {10.3233/ICA-200635}, author = {Diaz-Vico, David and Prada, Jesus and Omari, Adil and Dorronsoro, Jose}, abstract = {Kernel based Support Vector Machines, SVM, one of the most popular machine learning models, usually achieve top performances in two-class classification and regression problems. However, their training cost is at least quadratic on sample size, making them thus unsuitable for large sample problems. However, Deep Neural Networks (DNNs), with a cost linear on sample size, are able to solve big data problems relatively easily. In this work we propose to combine the advanced representations that DNNs can achieve in their last hidden layers with the hinge and epsilon insensitive losses that are used in two-class SVM classification and regression. We can thus have much better scalability while achieving performances comparable to those of SVMs. Moreover, we will also show that the resulting Deep SVM models are competitive with standard DNNs in two-class classification problems but have an edge in regression ones.} }
@article{WOS:000569592200001, title = {Handwritten Digit Recognition: Hyperparameters-Based Analysis}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175988}, author = {Albahli, Saleh and Alhassan, Fatimah and Albattah, Waleed and Khan, Rehan Ullah}, abstract = {Neural networks have several useful applications in machine learning. However, benefiting from the neural-network architecture can be tricky in some instances due to the large number of parameters that can influence performance. In general, given a particular dataset, a data scientist cannot do much to improve the efficiency of the model. However, by tuning certain hyperparameters, the model's accuracy and time of execution can be improved. Hence, it is of utmost importance to select the optimal values of hyperparameters. Choosing the optimal values of hyperparameters requires experience and mastery of the machine learning paradigm. In this paper, neural network-based architectures are tested based on altering the values of hyperparameters for handwritten-based digit recognition. Various neural network-based models are used to analyze different aspects of the same, primarily accuracy based on hyperparameter values. The extensive experimentation setup in this article should, therefore, provide the most accurate and time-efficient solution models. Such an evaluation will help in selecting the optimized values of hyperparameters for similar tasks.} }
@article{WOS:000345135700005, title = {Multi-view Laplacian twin support vector machines}, journal = {APPLIED INTELLIGENCE}, volume = {41}, pages = {1059-1068}, year = {2014}, issn = {0924-669X}, doi = {10.1007/s10489-014-0563-8}, author = {Xie, Xijiong and Sun, Shiliang}, abstract = {Twin support vector machines are a recently proposed learning method for pattern classification. They learn two hyperplanes rather than one as in usual support vector machines and often bring performance improvements. Semi-supervised learning has attracted great attention in machine learning in the last decade. Laplacian support vector machines and Laplacian twin support vector machines have been proposed in the semi-supervised learning framework. In this paper, inspired by the recent success of multi-view learning we propose multi-view Laplacian twin support vector machines, whose dual optimization problems are quadratic programming problems. We further extend them to kernel multi-view Laplacian twin support vector machines. Experimental results demonstrate that our proposed methods are effective.} }
@article{WOS:000481413400025, title = {If machines can learn, who needs scientists?}, journal = {JOURNAL OF MAGNETIC RESONANCE}, volume = {306}, pages = {162-166}, year = {2019}, issn = {1090-7807}, doi = {10.1016/j.jmr.2019.07.044}, author = {Hoch, Jeffrey C.}, abstract = {Machine learning has been used in NMR in for decades, but recent developments signal explosive growth is on the horizon. An obstacle to the application of machine learning in NMR is the relative paucity of available training data, despite the existence of numerous public NMR data repositories. Other challenges include the problem of interpreting the results of a machine learning algorithm, and incorporating machine learning into hypothesis-driven research. This perspective imagines the potential of machine learning in NMR and speculates on possible approaches to the hurdles. (C) 2019 Published by Elsevier Inc.} }
@article{WOS:000461166500028, title = {Deep embedding kernel}, journal = {NEUROCOMPUTING}, volume = {339}, pages = {292-302}, year = {2019}, issn = {0925-2312}, doi = {10.1016/j.neucom.2019.02.037}, author = {Linh Le and Xie, Ying}, abstract = {In this paper, we propose a novel supervised learning method that is called Deep Embedding Kernel (DEK). DEK combines the advantages of deep learning and kernel methods in a unified framework. More specifically, DEK is a learnable kernel represented by a newly designed deep architecture. Compared with predefined kernels, this kernel can be explicitly trained to map data to an optimized high-level feature space where data may have favorable features toward the application. Compared with typical deep learning using SoftMax or logistic regression as the top layer, DEK is expected to be more generalizable to new data. Experimental results show that DEK has superior performance than typical machine learning methods in identity detection and classification, and transfer learning, on different types of data including images, sequences, and regularly structured data. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000467010400067, title = {Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS}, volume = {66}, pages = {7316-7325}, year = {2019}, issn = {0278-0046}, doi = {10.1109/TIE.2018.2877090}, author = {Guo, Liang and Lei, Yaguo and Xing, Saibo and Yan, Tao and Li, Naipeng}, abstract = {The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.} }
@article{WOS:000720251600001, title = {A Zeroth-Order Adaptive Learning Rate Method to Reduce Cost of Hyperparameter Tuning for Deep Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app112110184}, author = {Li, Yanan and Ren, Xuebin and Zhao, Fangyuan and Yang, Shusen}, abstract = {Due to powerful data representation ability, deep learning has dramatically improved the state-of-the-art in many practical applications. However, the utility highly depends on fine-tuning of hyper-parameters, including learning rate, batch size, and network initialization. Although many first-order adaptive methods (e.g., Adam, Adagrad) have been proposed to adjust learning rate based on gradients, they are susceptible to the initial learning rate and network architecture. Therefore, the main challenge of using deep learning in practice is how to reduce the cost of tuning hyper-parameters. To address this, we propose a heuristic zeroth-order learning rate method, Adacomp, which adaptively adjusts the learning rate based only on values of the loss function. The main idea is that Adacomp penalizes large learning rates to ensure the convergence and compensates small learning rates to accelerate the training process. Therefore, Adacomp is robust to the initial learning rate. Extensive experiments, including comparison to six typically adaptive methods (Momentum, Adagrad, RMSprop, Adadelta, Adam, and Adamax) on several benchmark datasets for image classification tasks (MNIST, KMNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100), were conducted. Experimental results show that Adacomp is not only robust to the initial learning rate but also to the network architecture, network initialization, and batch size.} }
@article{WOS:000697551500015, title = {Deep tree-ensembles for multi-output prediction}, journal = {PATTERN RECOGNITION}, volume = {121}, year = {2022}, issn = {0031-3203}, doi = {10.1016/j.patcog.2021.108211}, author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine}, abstract = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suf-fer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply em-ploy label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demon-strated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the origi-nal feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks. (c) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000874639800001, title = {Designing Catalyst Descriptors for Machine Learning in Oxidative Coupling of Methane}, journal = {ACS CATALYSIS}, volume = {12}, pages = {11541-11546}, year = {2022}, issn = {2155-5435}, doi = {10.1021/acscatal.2c03142}, author = {Ishioka, Sora and Fujiwara, Aya and Nakanowatari, Sunao and Takahashi, Lauren and Taniike, Toshiaki and Takahashi, Keisuke}, abstract = {Catalysts descriptors for representing catalytic activities have been challenging in regard to machine learning. Machine learning and catalyst big data generated from high-throughput experiments are combined to explore the catalyst descriptors. Catalyst descriptors are designed using the physical quantities from the periodic table in the oxidative coupling of methane (OCM) reaction. Machine learning unveils the five key physical quantities representing ethylene/ethane selectivity (C2s) in the OCM reaction, where machine learning predicted three catalysts to have high C2s values. Experiments confirm that the proposed three catalysts have high C2s values in the OCM reaction. Hence, the physical quantities can be used as alternative descriptors for designing heterogeneous catalysts.} }
@article{WOS:001118122400001, title = {Application of Machine Learning Based on Structured Medical Data in Gastroenterology}, journal = {BIOMIMETICS}, volume = {8}, year = {2023}, doi = {10.3390/biomimetics8070512}, author = {Kim, Hye-Jin and Gong, Eun-Jeong and Bang, Chang-Seok}, abstract = {The era of big data has led to the necessity of artificial intelligence models to effectively handle the vast amount of clinical data available. These data have become indispensable resources for machine learning. Among the artificial intelligence models, deep learning has gained prominence and is widely used for analyzing unstructured data. Despite the recent advancement in deep learning, traditional machine learning models still hold significant potential for enhancing healthcare efficiency, especially for structured data. In the field of medicine, machine learning models have been applied to predict diagnoses and prognoses for various diseases. However, the adoption of machine learning models in gastroenterology has been relatively limited compared to traditional statistical models or deep learning approaches. This narrative review provides an overview of the current status of machine learning adoption in gastroenterology and discusses future directions. Additionally, it briefly summarizes recent advances in large language models.} }
@article{WOS:000429536600008, title = {Quantum Machine Learning in Chemical Compound Space}, journal = {ANGEWANDTE CHEMIE-INTERNATIONAL EDITION}, volume = {57}, pages = {4164-4169}, year = {2018}, issn = {1433-7851}, doi = {10.1002/anie.201709686}, author = {von Lilienfeld, O. Anatole}, abstract = {Rather than numerically solving the computationally demanding equations of quantum or statistical mechanics, machine learning methods can infer approximate solutions by interpolating previously acquired property data sets of molecules and materials. The case is made for quantum machine learning: An inductive molecular modeling approach which can be applied to quantum chemistry problems.} }
@article{WOS:000458997900018, title = {Survey on SDN based network intrusion detection system using machine learning approaches}, journal = {PEER-TO-PEER NETWORKING AND APPLICATIONS}, volume = {12}, pages = {493-501}, year = {2019}, issn = {1936-6442}, doi = {10.1007/s12083-017-0630-0}, author = {Sultana, Nasrin and Chilamkurti, Naveen and Peng, Wei and Alhadad, Rabei}, abstract = {Software Defined Networking Technology (SDN) provides a prospect to effectively detect and monitor network security problems ascribing to the emergence of the programmable features. Recently, Machine Learning (ML) approaches have been implemented in the SDN-based Network Intrusion Detection Systems (NIDS) to protect computer networks and to overcome network security issues. A stream of advanced machine learning approaches - the deep learning technology (DL) commences to emerge in the SDN context. In this survey, we reviewed various recent works on machine learning (ML) methods that leverage SDN to implement NIDS. More specifically, we evaluated the techniques of deep learning in developing SDN-based NIDS. In the meantime, in this survey, we covered tools that can be used to develop NIDS models in SDN environment. This survey is concluded with a discussion of ongoing challenges in implementing NIDS using ML/DL and future works.} }
@article{WOS:000602672200005, title = {Significant Applications of Machine Learning for COVID-19 Pandemic}, journal = {JOURNAL OF INDUSTRIAL INTEGRATION AND MANAGEMENT-INNOVATION AND ENTREPRENEURSHIP}, volume = {5}, pages = {453-479}, year = {2020}, issn = {2424-8622}, doi = {10.1142/S2424862220500268}, author = {Kushwaha, Shashi and Bahl, Shashi and Bagha, Ashok Kumar and Parmar, Kulwinder Singh and Javaid, Mohd and Haleem, Abid and Singh, Ravi Pratap}, abstract = {Machine learning is an innovative approach that has extensive applications in prediction. This technique needs to be applied for the COVID-19 pandemic to identify patients at high risk, their death rate, and other abnormalities. It can be used to understand the nature of this virus and further predict the upcoming issues. This literature-based review is done by searching the relevant papers on machine learning for COVID-19 from the databases of SCOPUS, Academia, Google Scholar, PubMed, and ResearchGate. This research attempts to discuss the significance of machine learning in resolving the COVID-19 pandemic crisis. This paper studied how machine learning algorithms and methods can be employed to fight the COVID-19 virus and the pandemic. It further discusses the primary machine learning methods that are helpful during the COVID-19 pandemic. We further identified and discussed algorithms used in machine learning and their significant applications. Machine learning is a useful technique, and this can be witnessed in various areas to identify the existing drugs, which also seems advantageous for the treatment of COVID-19 patients. This learning algorithm creates interferences out of unlabeled input datasets, which can be applied to analyze the unlabeled data as an input resource for COVID-19. It provides accurate and useful features rather than a traditional explicitly calculation-based method. Further, this technique is beneficial to predict the risk in healthcare during this COVID-19 crisis. Machine learning also analyses the risk factors as per age, social habits, location, and climate.} }
@article{WOS:000504871100004, title = {Applications of Machine Learning Using Electronic Medical Records in Spine Surgery}, journal = {NEUROSPINE}, volume = {16}, pages = {643-653}, year = {2019}, issn = {2586-6583}, doi = {10.14245/ns.1938386.193}, author = {Schwartz, John T. and Gao, Michael and Geng, Eric A. and Mody, Kush S. and Mikhail, Christopher M. and Cho, Samuel K.}, abstract = {Developments in machine learning in recent years have precipitated a surge in research on the applications of artificial intelligence within medicine. Machine learning algorithms are beginning to impact medicine broadly, and the field of spine surgery is no exception. Electronic medical records are a key source of medical data that can be leveraged for the creation of clinically valuable machine learning algorithms. This review examines the current state of machine learning using electronic medical records as it applies to spine surgery. Studies across the electronic medical record data domains of imaging, text, and structured data are reviewed. Discussed applications include clinical prognostication, preoperative planning, diagnostics, and dynamic clinical assistance, among others. The limitations and future challenges for machine learning research using electronic medical records are also discussed.} }
@article{WOS:000778251600007, title = {Event Detection for Distributed Acoustic Sensing: Combining Knowledge-Based, Classical Machine Learning, and Deep Learning Approaches}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21227527}, author = {Bublin, Mugdim}, abstract = {Distributed Acoustic Sensing (DAS) is a promising new technology for pipeline monitoring and protection. However, a big challenge is distinguishing between relevant events, like intrusion by an excavator near the pipeline, and interference, like land machines. This paper investigates whether it is possible to achieve adequate detection accuracy with classic machine learning algorithms using simulations and real system implementation. Then, we compare classical machine learning with a deep learning approach and analyze the advantages and disadvantages of both approaches. Although acceptable performance can be achieved with both approaches, preliminary results show that deep learning is the more promising approach, eliminating the need for laborious feature extraction and offering a six times lower event detection delay and twelve times lower execution time. However, we achieved the best results by combining deep learning with the knowledge-based and classical machine learning approaches. At the end of this manuscript, we propose general guidelines for efficient system design combining knowledge-based, classical machine learning, and deep learning approaches.} }
@article{WOS:000429088600004, title = {POINTS OF SIGNIFICANCE Statistics versus machine learning}, journal = {NATURE METHODS}, volume = {15}, pages = {232-233}, year = {2018}, issn = {1548-7091}, doi = {10.1038/nmeth.4642}, author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin}, abstract = {Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.} }
@article{WOS:000453925000013, title = {State of the Art: Machine Learning Applications in Glioma Imaging}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {212}, pages = {26-37}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.18.20218}, author = {Lotan, Eyal and Lotan, Eyal and Jain, Rajan and Razavian, Narges and Fatterpekar, Girish M. and Lui, Yvonne W.}, abstract = {OBJECTIVE. Machine learning has recently gained considerable attention because of promising results for a wide range of radiology applications. Here we review recent work using machine learning in brain tumor imaging, specifically segmentation and MRI radiomics of gliomas. CONCLUSION. We discuss available resources, state`of`the`art segmentation methods, and machine learning radiomics for glioma. We highlight the challenges of these techniques as well as the future potential in clinical diagnostics, prognostics, and decision making.} }
@article{WOS:000619114800012, title = {A Contemporary Review of Machine Learning in Otolaryngology-Head and Neck Surgery}, journal = {LARYNGOSCOPE}, volume = {130}, pages = {45-51}, year = {2020}, issn = {0023-852X}, doi = {10.1002/lary.27850}, author = {Crowson, Matthew G. and Ranisau, Jonathan and Eskander, Antoine and Babier, Aaron and Xu, Bin and Kahmke, Russel R. and Chen, Joseph M. and Chan, Timothy C. Y.}, abstract = {One of the key challenges with big data is leveraging the complex network of information to yield useful clinical insights. The confluence of massive amounts of health data and a desire to make inferences and insights on these data has produced a substantial amount of interest in machine-learning analytic methods. There has been a drastic increase in the otolaryngology literature volume describing novel applications of machine learning within the past 5 years. In this timely contemporary review, we provide an overview of popular machine-learning techniques, and review recent machine-learning applications in otolaryngology-head and neck surgery including neurotology, head and neck oncology, laryngology, and rhinology. Investigators have realized significant success in validated models with model sensitivities and specificities approaching 100\\%. Challenges remain in the implementation of machine-learning algorithms. This may be in part the unfamiliarity of these techniques to clinician leaders on the front lines of patient care. Spreading awareness and confidence in machine learning will follow with further validation and proof-of-value analyses that demonstrate model performance superiority over established methods. We are poised to see a greater influx of machine-learning applications to clinical problems in otolaryngology-head and neck surgery, and it is prudent for providers to understand the potential benefits and limitations of these technologies.} }
@article{WOS:000560159100002, title = {Learning local discriminative representations via extreme learning machine for machine fault diagnosis}, journal = {NEUROCOMPUTING}, volume = {409}, pages = {275-285}, year = {2020}, issn = {0925-2312}, doi = {10.1016/j.neucom.2020.05.021}, author = {Li, Yue and Zeng, Yijie and Qing, Yuanyuan and Huang, Guang-Bin}, abstract = {Recently, learning data representations have been investigated to reduce the dependences of human intervention and improve the performance of machine fault diagnosis. However, most of the representation learning methods are computationally intensive due to complex training procedures. Extreme learning machine is well-known for its fast training speed and strong generalization ability. It also has been applied to learn data representations for clustering and classification tasks. In this paper, a local discriminant preserving extreme learning machine autoencoder (LDELM-AE) is proposed to learn data representations with the local geometry and local discriminant exploited from the input data. Specifically, LDELM-AE utilizes two graphs to enhance the within-class compactness and between-class separability, respectively. Furthermore, the hierarchical representations can be obtained by stacking several LDELM-AEs. On several benchmark datasets, the proposed method demonstrates better classification accuracies than the state-of-the-art methods. Moreover, the proposed method has been used to diagnostic the rotary machine faults and achieves the diagnostic accuracy of 99.96\\%, which proves the proposed method is an efficient tool to diagnose machine faults. (C) 2020 Elsevier B.V. All rights reserved.} }
@incollection{WOS:000433057100004, title = {Machine Learning Approaches for Clinical Psychology and Psychiatry}, booktitle = {ANNUAL REVIEW OF CLINICAL PSYCHOLOGY, VOL 14}, volume = {14}, pages = {91-118}, year = {2018}, issn = {1548-5943}, doi = {10.1146/annurev-clinpsy-032816045037}, author = {Dwyer, Dominic B. and Falkai, Peter and Koutsouleris, Nikolaos}, abstract = {Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.} }
@article{WOS:000354655800008, title = {Machine learning applications in genetics and genomics}, journal = {NATURE REVIEWS GENETICS}, volume = {16}, pages = {321-332}, year = {2015}, issn = {1471-0056}, doi = {10.1038/nrg3920}, author = {Libbrecht, Maxwell W. and Noble, William Stafford}, abstract = {The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.} }
@article{WOS:001006490400001, title = {Machine learning facilitating the rational design of nanozymes}, journal = {JOURNAL OF MATERIALS CHEMISTRY B}, volume = {11}, pages = {6466-6477}, year = {2023}, issn = {2050-750X}, doi = {10.1039/d3tb00842h}, author = {Li, Yucong and Zhang, Ruofei and Yan, Xiyun and Fan, Kelong}, abstract = {As a component substitute for natural enzymes, nanozymes have the advantages of easy synthesis, convenient modification, low cost, and high stability, and are widely used in many fields. However, their application is seriously restricted by the difficulty of rapidly creating high-performance nanozymes. The use of machine learning techniques to guide the rational design of nanozymes holds great promise to overcome this difficulty. In this review, we introduce the recent progress of machine learning in assisting the design of nanozymes. Particular attention is given to the successful strategies of machine learning in predicting the activity, selectivity, catalytic mechanisms, optimal structures and other features of nanozymes. The typical procedures and approaches for conducting machine learning in the study of nanozymes are also highlighted. Moreover, we discuss in detail the difficulties of machine learning methods in dealing with the redundant and chaotic nanozyme data and provide an outlook on the future application of machine learning in the nanozyme field. We hope that this review will serve as a useful handbook for researchers in related fields and promote the utilization of machine learning in nanozyme rational design and related topics.} }
@article{WOS:000269069200037, title = {Dropout prediction in e-learning courses through the combination of machine learning techniques}, journal = {COMPUTERS \\& EDUCATION}, volume = {53}, pages = {950-965}, year = {2009}, issn = {0360-1315}, doi = {10.1016/j.compedu.2009.05.010}, author = {Lykourentzou, Ioanna and Giannoukos, Ioannis and Nikolopoulos, Vassilis and Mpardis, George and Loumos, Vassili}, abstract = {In this paper, a dropout prediction method for e-learning courses, based on three popular machine learning techniques and detailed student data, is proposed. The machine learning techniques used are feed-forward neural networks, support vector machines and probabilistic ensemble simplified fuzzy ARTMAP. Since a single technique may fail to accurately classify some e-learning students, whereas another may succeed, three decision schemes, which combine in different ways the results of the three machine learning techniques, were also tested. The method was examined in terms of overall accuracy, sensitivity and precision and its results were found to be significantly better than those reported in relevant literature. (C) 2009 Elsevier Ltd. All rights reserved.} }
@article{WOS:000736977400012, title = {Machine-learning methods for ligand-protein molecular docking}, journal = {DRUG DISCOVERY TODAY}, volume = {27}, pages = {151-164}, year = {2022}, issn = {1359-6446}, doi = {10.1016/j.drudis.2021.09.007}, author = {Crampon, Kevin and Giorkallos, Alexis and Deldossi, Myrtille and Baud, Stephanie and Steffenel, Luiz Angelo}, abstract = {Artificial intelligence (AI) is often presented as a new Industrial Revolution. Many domains use AI, including molecular simulation for drug discovery. In this review, we provide an overview of ligand-protein molecular docking and how machine learning (ML), especially deep learning (DL), a subset of ML, is transforming the field by tackling the associated challenges.} }
@article{WOS:000768730600017, title = {Adaptive machine learning for protein engineering}, journal = {CURRENT OPINION IN STRUCTURAL BIOLOGY}, volume = {72}, pages = {145-152}, year = {2022}, issn = {0959-440X}, doi = {10.1016/j.sbi.2021.11.002}, author = {Hie, Brian L. and Yang, Kevin K.}, abstract = {Machine-learning models that learn from data to predict how protein sequence encodes function are emerging as a useful protein engineering tool. However, when using these models to suggest new protein designs, one must deal with the vast combinatorial complexity of protein sequences. Here, we review how to use a sequence-to-function machine-learning surrogate model to select sequences for experimental measurement. First, we discuss how to select sequences through a single round of machine-learning optimization. Then, we discuss sequential optimization, where the goal is to discover optimized sequences and improve the model across multiple rounds of training, optimization, and experimental measurement.} }
@article{WOS:000512216200010, title = {Applying Machine Learning Techniques in Nomogram Prediction and Analysis for SMILE Treatment}, journal = {AMERICAN JOURNAL OF OPHTHALMOLOGY}, volume = {210}, pages = {71-77}, year = {2020}, issn = {0002-9394}, doi = {10.1016/j.ajo.2019.10.015}, author = {Cui, Tong and Wang, Yan and Ji, Shufan and Li, Yan and Hao, Weiting and Zou, Haohan and Jhanji, Vishal}, abstract = {PURPOSE: To analyze the outcome of machine learning technique for prediction of small incision lenticule extraction (SMILE) nomogram. DESIGN: Prospective, comparative clinical study. METHODS: A comparative study was conducted on the outcomes of SMILE surgery between surgeon group (nomogram set by surgeon) and machine learning group (nomogram predicted by machine learning model). The machine learning model was trained by 865 ideal cases (spherical equivalent [SE] within +/- 0.5 diopter [D] 3 months postoperatively) from an experienced surgeon. The visual outcomes of both groups were compared for safety, efficacy, predictability, and SE correction. RESULTS: There was no statistically significant difference between the baseline data in both groups. The efficacy index in the machine learning group (1.48 +/- 1.08) was significantly higher than in the surgeon group (1.3 +/- 0.27) (t = -2.17, P < .05). Eighty-three percent of eyes in the surgeon group and 93\\% of eyes in the machine learning group were within +/- 0.50 D, while 98\\% of eyes in the surgeon group and 96\\% of eyes in the machine learning group were within 1.00 D. The error of SE correction was -0.09 +/- 0.024 and -0.23 +/- 0.021 for machine learning and surgeon groups, respectively. CONCLUSIONS: The machine learning technique performed as well as surgeon in safety, but significantly better than surgeon in efficacy. As for predictability, the machine learning technique was comparable to surgeon, although less predictable for high myopia and astigmatism. ((C) 2019 Published by Elsevier Inc.)} }
@article{WOS:000468859100001, title = {Prospects of deep learning for medical imaging}, journal = {PRECISION AND FUTURE MEDICINE}, volume = {2}, pages = {37-52}, year = {2018}, issn = {2508-7940}, doi = {10.23838/pfm.2018.00030}, author = {Kim, Jonghoon and Hong, Jisu and Park, Hyunjin}, abstract = {Machine learning techniques are essential components of medical imaging research. Recently, a highly flexible machine learning approach known as deep learning has emerged as a disruptive technology to enhance the performance of existing machine learning techniques and to solve previously intractable problems. Medical imaging has been identified as one of the key research fields where deep learning can contribute significantly. This review article aims to survey deep learning literature in medical imaging and describe its potential for future medical imaging research. First, an overview of how traditional machine learning evolved to deep learning is provided. Second, a survey of the application of deep learning in medical imaging research is given. Third, well-known software tools for deep learning are reviewed. Finally, conclusions with limitations and future directions of deep learning in medical imaging are provided.} }
@article{WOS:000603902400001, title = {Comparative Study of the Dynamic Back-Analysis Methods of Concrete Gravity Dams Based on Multivariate Machine Learning Models}, journal = {JOURNAL OF EARTHQUAKE ENGINEERING}, volume = {25}, pages = {1-22}, year = {2021}, issn = {1363-2469}, doi = {10.1080/13632469.2018.1452802}, author = {Cheng, Lin and Tong, Fei and Li, Yanlong and Yang, Jie and Zheng, Dongjian}, abstract = {Two different back-analysis frameworks based on multivariate machine learning models used to determine the material dynamic parameters of concrete gravity dams are proposed. For the framework I, the back-analysis is performed by solving an optimization problem and a multivariate machine learning model is trained to replace the FEM calculation during the optimization process. While the framework II uses a multivariate machine learning model directly and the material dynamic parameters are predicted using the machine learning mode. By using a numerical example and an experimental investigation, the robustness, accuracy, computation efficiency of these proposed back-analysis methods is verified.} }
@article{WOS:000544969800031, title = {Utilizing crowdsourcing and machine learning in education: Literature review}, journal = {EDUCATION AND INFORMATION TECHNOLOGIES}, volume = {25}, pages = {2971-2986}, year = {2020}, issn = {1360-2357}, doi = {10.1007/s10639-020-10102-w}, author = {Alenezi, Hadeel S. and Faisal, Maha H.}, abstract = {For many years, learning continues to be a vital developing field since it is the key measure of the world's civilization and evolution with its enormous effect on both individuals and societies. Enhancing existing learning activities in general will have a significant impact on literacy rates around the world. One of the crucial activities in education is the assessment method because it is the primary way used to evaluate the student during their studies. The main purpose of this review is to examine the existing learning and e-learning approaches that use either crowdsourcing, machine learning, or both crowdsourcing and machine learning in their proposed solutions. This review will also investigate the addressed applications to identify the existing researches related to the assessment. Identifying all existing applications will assist in finding the unexplored gaps and limitations. This study presents a systematic literature review investigating 30 papers from the following databases: IEEE and ACM Digital Library. After performing the analysis, we found that crowdsourcing is utilized in 47.8\\% of the investigated learning activities, while each of the machine learning and the hybrid solutions are utilized in 26\\% of the investigated learning activities. Furthermore, all the existing approaches regarding the exam assessment problem that are using machine learning or crowdsourcing were identified. Some of the existing assessment systems are using the crowdsourcing approach and other systems are using the machine learning, however, none of the approaches provide a hybrid assessment system that uses both crowdsourcing and machine learning. Finally, it is found that using either crowdsourcing or machine learning in the online courses will enhance the interactions between the students. It is concluded that the current learning activities need to be enhanced since it is directly affecting the student's performance. Moreover, merging both the machine learning to the crowd wisdom will increase the accuracy and the efficiency of education.} }
@article{WOS:001132712600001, title = {<sc>FairCaipi</sc>: A Combination of Explanatory Interactive and Fair Machine Learning for Human and Machine Bias Reduction}, journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION}, volume = {5}, pages = {1519-1538}, year = {2023}, doi = {10.3390/make5040076}, author = {Heidrich, Louisa and Slany, Emanuel and Scheele, Stephan and Schmid, Ute and Cabitza, Federico and Chen, Fang and Zhou, Jianlong and Holzinger, Andreas}, abstract = {The rise of machine-learning applications in domains with critical end-user impact has led to a growing concern about the fairness of learned models, with the goal of avoiding biases that negatively impact specific demographic groups. Most existing bias-mitigation strategies adapt the importance of data instances during pre-processing. Since fairness is a contextual concept, we advocate for an interactive machine-learning approach that enables users to provide iterative feedback for model adaptation. Specifically, we propose to adapt the explanatory interactive machine-learning approach Caipi for fair machine learning. FairCaipi incorporates human feedback in the loop on predictions and explanations to improve the fairness of the model. Experimental results demonstrate that FairCaipi outperforms a state-of-the-art pre-processing bias mitigation strategy in terms of the fairness and the predictive performance of the resulting machine-learning model. We show that FairCaipi can both uncover and reduce bias in machine-learning models and allows us to detect human bias.} }
@article{WOS:001225632900001, title = {Enhancing flow stress predictions in CoCrFeNiV high entropy alloy with conventional and machine learning techniques}, journal = {JOURNAL OF MATERIALS RESEARCH AND TECHNOLOGY-JMR\\&T}, volume = {30}, pages = {2377-2387}, year = {2024}, issn = {2238-7854}, doi = {10.1016/j.jmrt.2024.03.164}, author = {Dewangan, Sheetal Kumar and Jain, Reliance and Bhattacharjee, Soumyabrata and Jain, Sandeep and Paswan, Manikant and Samal, Sumanta and Ahn, Byungmin}, abstract = {A machine learning technique leveraging artificial intelligence (AI) has emerged as a promising tool for expediting the exploration and design of novel high entropy alloys (HEAs) while predicting their mechanical properties at both room and elevated temperatures. In this paper, we predict the flow stress of hot-compressed CoCrFeNiV HEAs using conventional (qualitative and quantitative models) and advanced machine learning approaches across various temperature and strain rate conditions. Conventional modeling methods, including the modified Johnson -Cook (JC), modified Zerilli - Armstrong (ZA), and Arrhenius -type constitutive equations, are employed. Simultaneously, machine learning models are utilized to forecast flow stress under different hot working conditions. The performance of both conventional and machine learning models is evaluated using metrics such as coefficient of determination (R 2 ), mean abosolute error (MAE), and root mean squared error (RMSE). The analysis reveals that the gradient boosting machine learning model shows superior prediction accuracy (with value R 2 = 0.994, MAE = 7.77\\%, and RMSE = 9.7\\%) compared to conventional models and other machine learning approaches.} }
@article{WOS:001035352200001, title = {A survey on dataset quality in machine learning}, journal = {INFORMATION AND SOFTWARE TECHNOLOGY}, volume = {162}, year = {2023}, issn = {0950-5849}, doi = {10.1016/j.infsof.2023.107268}, author = {Gong, Youdi and Liu, Guangzhen and Xue, Yunzhi and Li, Rui and Meng, Lingzhong}, abstract = {With the rise of big data, the quality of datasets has become a crucial factor affecting the performance of machine learning models. High-quality datasets are essential for the realization of data value. This survey article summarizes the research direction of dataset quality in machine learning, including the definition of related concepts, analysis of quality issues and risks, and a review of dataset quality dimensions and metrics throughout the dataset lifecycle and a review of dataset quality metrics analyzed from a dataset lifecycle perspective and summarized in literatures. Furthermore, this article introduces a comprehensive quality evaluation process, which includes a framework for dataset quality evaluation with dimensions and metrics, computation methods for quality metrics, and assessment models. These studies provide valuable guidance for evaluating dataset quality in the field of machine learning, which can help improve the accuracy, efficiency, and generalization ability of machine learning models, and promote the development and application of artificial intelligence technology.} }
@article{WOS:001061249700006, title = {Perspective: Predicting and optimizing thermal transport properties with machine learning methods}, journal = {ENERGY AND AI}, volume = {8}, year = {2022}, issn = {2666-5468}, doi = {10.1016/j.egyai.2022.100153}, author = {Wei, Han and Bao, Hua and Ruan, Xiulin}, abstract = {In recent years, (big) data science has emerged as the ``fourth paradigm'' in physical science research. Data-driven techniques, e.g. machine learning, are advantageous in dealing with problems of high-dimensional features and complex mappings between quantities, which are otherwise of great difficulty or huge cost with other scientific paradigms. In the past five years or so, there has been a rapid growth of machine learning-assisted research on thermal transport. In this perspective, we review the recent progress in the intersection between machine learning and thermal transport, where machine learning methods generally serve as surrogate models for pre-dicting the thermal transport properties, or as tools for designing structures for the desired thermal properties and exploring thermal transport mechanisms. We provide perspectives about the advantages of machine learning methods in comparison to the physics-based methods for studying thermal transport properties. We also discuss how to improve the accuracy of predictive analytics and efficiency of structural optimization, to provide guid-ance for better utilizing machine learning-based methods to advance thermal transport research. Finally, we identify several outstanding challenges in this active area as well as opportunities for future developments, including developing machine learning methods suitable for small datasets, discovering effective physics-based descriptors, generating dataset from experiments and validating machine learning results with experiments, and making breakthroughs via discovering new physics.} }
@article{WOS:000653304100001, title = {Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm}, journal = {AGRICULTURE-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/agriculture11050387}, author = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu, Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and Rahman, Sk Mostafizur}, abstract = {This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96\\% using RF, 94\\% using SVM and 63\\% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.} }
@article{WOS:001062545900001, title = {Forecasting realized volatility with machine learning: Panel data perspective}, journal = {JOURNAL OF EMPIRICAL FINANCE}, volume = {73}, pages = {251-271}, year = {2023}, issn = {0927-5398}, doi = {10.1016/j.jempfin.2023.07.003}, author = {Zhu, Haibin and Bai, Lu and He, Lidan and Liu, Zhi}, abstract = {Machine learning approaches have become very popular in many fields in this big data age. This paper considers the problem of forecasting realized volatility with machine learning using high-frequency data. Instead of treating the realized volatility as a univariate time series studied by many existing works in literature, we employ panel data analysis to improve forecasting accuracy in the short term. We use six effective machine-learning methods for the realized volatility panel data. We compare our results with the traditional linear-type models under the same panel data framework and with the single time series forecasting via the same machine learning methods. The results show that the panel-data-based machine learning method (PDML) outperforms the other methods.} }
@article{WOS:000489302600009, title = {Machine Learning and Sampling Scheme: An Empirical Study of Money Laundering Detection}, journal = {COMPUTATIONAL ECONOMICS}, volume = {54}, pages = {1043-1063}, year = {2019}, issn = {0927-7099}, doi = {10.1007/s10614-018-9864-z}, author = {Zhang, Yan and Trubey, Peter}, abstract = {This paper studies the interplay of machine learning and sampling scheme in an empirical analysis of money laundering detection algorithms. Using actual transaction data provided by a U.S. financial institution, we study five major machine learning algorithms including Bayes logistic regression, decision tree, random forest, support vector machine, and artificial neural network. As the incidence of money laundering events is rare, we apply and compare two sampling techniques that increase the relative presence of the events. Our analysis reveals potential advantages of machine learning algorithms in modeling money laundering events. This paper provides insights into the use of machine learning and sampling schemes in money laundering detection specifically, and classification of rare events in general.} }
@article{WOS:001218673600001, title = {Machine learning assisted Raman spectroscopy: A viable approach for the detection of microplastics}, journal = {JOURNAL OF WATER PROCESS ENGINEERING}, volume = {60}, year = {2024}, issn = {2214-7144}, doi = {10.1016/j.jwpe.2024.105150}, author = {Sunil, Megha and Pallikkavaliyaveetil, Nazreen and Mithun, N. . and Gopinath, Anu and Chidangil, Santhosh and Kumar, Satheesh and Lukose, Jijo}, abstract = {The accumulation of microplastics (MPs) resulting from disposal of plastic waste into water sources, poses a significant threat to aquatic organisms. These are readily ingested by organisms, leading to the accumulation of harmful substances, disrupting their biological processes. Current methods for identifying microplastics have notable drawbacks, including low resolution, extended imaging time, and restricted particle size analysis. Integrating Raman spectroscopy with machine learning (ML) proves to be an effective approach for identifying and classifying MPs, especially in scenarios where they are found in environmental media or mixed with various types. Machine learning (ML) can be vital tool in assisting Raman analysis, owing to its robust feature extraction capabilities. This comprehensive review outlined the utilization of various machine learning techniques in conjunction with Raman spectral features for diverse investigations related to microplastics. The methodologies discussed encompass Principal Component Analysis, K-Nearest Neighbour, Random Forest, Support Vector Machine, and various deep learning algorithms.} }
@article{WOS:000456150600012, title = {Multimodal Machine Learning: A Survey and Taxonomy}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {41}, pages = {423-443}, year = {2019}, issn = {0162-8828}, doi = {10.1109/TPAMI.2018.2798607}, author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe}, abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.} }
@article{WOS:000823410900001, title = {Quantum machine learning for support vector machine classification}, journal = {EVOLUTIONARY INTELLIGENCE}, volume = {17}, pages = {819-828}, year = {2024}, issn = {1864-5909}, doi = {10.1007/s12065-022-00756-5}, author = {Kavitha, S. S. and Kaulgud, Narasimha}, abstract = {Quantum machine learning aims to execute machine learning algorithms in quantum computers by utilizing powerful laws like superposition and entanglement for solving problems more efficiently. Support vector machine (SVM) is proved to be one of the most efficient classification machine learning algorithms in today's world. Since in classical systems, as datasets become complex or mixed up, the SVM kernel approach tends to slow and might fail. Hence our research is focused to examine the execution speed and accuracy of quantum support vector machines classification compared to classical SVM classification by proper quantum feature mapping selection. As the size of the dataset becomes complex, a proper feature map has to be selected to outperform or equally perform the classification. Hence the paper focuses on the selection of the best feature map for some benchmark datasets. Additionally experimental results show that the processing time of the algorithm is considerably reduced concerning classical machine learning. For evaluation of quantum computation over the classical computer, Quantum labs from the IBMQ quantum computer cloud have been used.} }
@article{WOS:000219166600025, title = {A Comparison of the Effects of K-Anonymity on Machine Learning Algorithms}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {5}, pages = {155-160}, year = {2014}, issn = {2158-107X}, author = {Wimmer, Hayden and Powell, Loreen}, abstract = {While research has been conducted in machine learning algorithms and in privacy preserving in data mining (PPDM), a gap in the literature exists which combines the aforementioned areas to determine how PPDM affects common machine learning algorithms. The aim of this research is to narrow this literature gap by investigating how a common PPDM algorithm, K-Anonymity, affects common machine learning and data mining algorithms, namely neural networks, logistic regression, decision trees, and Bayesian classifiers. This applied research reveals practical implications for applying PPDM to data mining and machine learning and serves as a critical first step learning how to apply PPDM to machine learning algorithms and the effects of PPDM on machine learning. Results indicate that certain machine learning algorithms are more suited for use with PPDM techniques.} }
@article{WOS:000959408400001, title = {Easy and fast prediction of green solvents for small molecule donor-based organic solar cells through machine learning}, journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS}, volume = {25}, pages = {10417-10426}, year = {2023}, issn = {1463-9076}, doi = {10.1039/d3cp00177f}, author = {Mahmood, Asif and Sandali, Yahya and Wang, Jin-Liang}, abstract = {Solubility plays a critical role in many aspects of research (drugs to materials). Solubility parameters are very useful for selecting appropriate solvents/non-solvents for various applications. In the present study, Hansen solubility parameters are predicted using machine learning. More than 40 machine models are tried in the search for the best model. Molecular descriptors and fingerprints are used as inputs to get a comparative view. Machine learning models trained using molecular descriptors have shown higher prediction ability than the model trained using molecular fingerprints. Machine learning models trained using molecular descriptors have shown their potential to be easy and fast models compared to the density functional theory (DFT)/thermodynamic approach. Machine learning creates a ``black box'' connection to the properties. Therefore, minimal computational cost is required. With the help of the best-trained machine learning model, green solvents are selected for small molecule donors that are used in organic solar cells. Our introduced framework can help to select solvents for organic solar cells in an easy and fast way.} }
@article{WOS:000657036100005, title = {Galaxy-ML: An accessible, reproducible, and scalable machine learning toolkit for biomedicine}, journal = {PLOS COMPUTATIONAL BIOLOGY}, volume = {17}, year = {2021}, issn = {1553-734X}, doi = {10.1371/journal.pcbi.1009014}, author = {Gu, Qiang and Kumar, Anup and Bray, Simon and Creason, Allison and Khanteymoori, Alireza and Jalili, Vahid and Gruening, Bjoern and Goecks, Jeremy}, abstract = {Supervised machine learning is an essential but difficult to use approach in biomedical data analysis. The Galaxy-ML toolkit (https://galaxyproject.org/community/machine-learning/) makes supervised machine learning more accessible to biomedical scientists by enabling them to perform end-to-end reproducible machine learning analyses at large scale using only a web browser. Galaxy-ML extends Galaxy (https://galaxyproject.org), a biomedical computational workbench used by tens of thousands of scientists across the world, with a suite of tools for all aspects of supervised machine learning.} }
@article{WOS:000449215200023, title = {Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data}, journal = {JAMA INTERNAL MEDICINE}, volume = {178}, pages = {1544-1547}, year = {2018}, issn = {2168-6106}, doi = {10.1001/jamainternmed.2018.3763}, author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela}, abstract = {A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.} }
@article{WOS:001129216300001, title = {Machine learning and deep learning for brain tumor MRI image segmentation}, journal = {EXPERIMENTAL BIOLOGY AND MEDICINE}, volume = {248}, pages = {1974-1992}, year = {2023}, issn = {1535-3702}, doi = {10.1177/15353702231214259}, author = {Khan, Md Kamrul Hasan and Guo, Wenjing and Liu, Jie and Dong, Fan and Li, Zoe and Patterson, Tucker A. and Hong, Huixiao}, abstract = {Brain tumors are often fatal. Therefore, accurate brain tumor image segmentation is critical for the diagnosis, treatment, and monitoring of patients with these tumors. Magnetic resonance imaging (MRI) is a commonly used imaging technique for capturing brain images. Both machine learning and deep learning techniques are popular in analyzing MRI images. This article reviews some commonly used machine learning and deep learning techniques for brain tumor MRI image segmentation. The limitations and advantages of the reviewed machine learning and deep learning methods are discussed. Even though each of these methods has a well-established status in their individual domains, the combination of two or more techniques is currently an emerging trend.} }
@article{WOS:000788104800007, title = {Advances in Machine Learning Approaches to Heart Failure with Preserved Ejection Fraction}, journal = {HEART FAILURE CLINICS}, volume = {18}, pages = {287-300}, year = {2022}, issn = {1551-7136}, doi = {10.1016/j.hfc.2021.12.002}, author = {Ahmad, Faraz S. and Luo, Yuan and Wehbe, Ramsey M. and Thomas, James D. and Shah, Sanjiv J.}, abstract = {center dot Machine learning has the potential to guide precision medicine approaches for heart failure with preserved ejection fraction, such as identification of rare causes, subphenotyping, and increasing the efficiency of clinical trial enrollment. center dot Understanding the strengths, limitations, and pitfalls of machine learning approaches is critical to realizing the potential of machine learning to impact the health of the patient with heart failure with preserved ejection fraction.} }
@article{WOS:000306390300001, title = {Machine learning and radiology}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {16}, pages = {933-951}, year = {2012}, issn = {1361-8415}, doi = {10.1016/j.media.2012.02.005}, author = {Wang, Shijun and Summers, Ronald M.}, abstract = {In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers. (c) 2012 Published by Elsevier B.V.} }
@article{WOS:000838308500001, title = {Machine learning for microalgae detection and utilization}, journal = {FRONTIERS IN MARINE SCIENCE}, volume = {9}, year = {2022}, doi = {10.3389/fmars.2022.947394}, author = {Ning, Hongwei and Li, Rui and Zhou, Teng}, abstract = {Microalgae are essential parts of marine ecology, and they play a key role in species balance. Microalgae also have significant economic value. However, microalgae are too tiny, and there are many different kinds of microalgae in a single drop of seawater. It is challenging to identify microalgae species and monitor microalgae changes. Machine learning techniques have achieved massive success in object recognition and classification, and have attracted a wide range of attention. Many researchers have introduced machine learning algorithms into microalgae applications, and similarly significant effects are gained. The paper summarizes recent advances based on various machine learning algorithms in microalgae applications, such as microalgae classification, bioenergy generation from microalgae, environment purification with microalgae, and microalgae growth monitor. Finally, we prospect development of machine learning algorithms in microalgae treatment in the future.} }
@article{WOS:000559371800001, title = {Legal requirements on explainability in machine learning}, journal = {ARTIFICIAL INTELLIGENCE AND LAW}, volume = {29}, pages = {149-169}, year = {2021}, issn = {0924-8463}, doi = {10.1007/s10506-020-09270-4}, author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Frenay, Benoit}, abstract = {Deep learning and other black-box models are becoming more and more popular today. Despite their high performance, they may not be accepted ethically or legally because of their lack of explainability. This paper presents the increasing number of legal requirements on machine learning model interpretability and explainability in the context of private and public decision making. It then explains how those legal requirements can be implemented into machine-learning models and concludes with a call for more inter-disciplinary research on explainability.} }
@article{WOS:000587801200006, title = {Machine learning in the optimization of robotics in the operative field}, journal = {CURRENT OPINION IN UROLOGY}, volume = {30}, pages = {808-816}, year = {2020}, issn = {0963-0643}, doi = {10.1097/MOU.0000000000000816}, author = {Ma, Runzhuo and Vanstrum, Erik B. and Lee, Ryan and Chen, Jian and Hung, Andrew J.}, abstract = {Purpose of review The increasing use of robotics in urologic surgery facilitates collection of `big data'. Machine learning enables computers to infer patterns from large datasets. This review aims to highlight recent findings and applications of machine learning in robotic-assisted urologic surgery. Recent findings Machine learning has been used in surgical performance assessment and skill training, surgical candidate selection, and autonomous surgery. Autonomous segmentation and classification of surgical data have been explored, which serves as the stepping-stone for providing real-time surgical assessment and ultimately, improve surgical safety and quality. Predictive machine learning models have been created to guide appropriate surgical candidate selection, whereas intraoperative machine learning algorithms have been designed to provide 3-D augmented reality and real-time surgical margin checks. Reinforcement-learning strategies have been utilized in autonomous robotic surgery, and the combination of expert demonstrations and trial-and-error learning by the robot itself is a promising approach towards autonomy. Robot-assisted urologic surgery coupled with machine learning is a burgeoning area of study that demonstrates exciting potential. However, further validation and clinical trials are required to ensure the safety and efficacy of incorporating machine learning into surgical practice.} }
@article{WOS:000571725400014, title = {Robust Coreset Construction for Distributed Machine Learning}, journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS}, volume = {38}, pages = {2400-2417}, year = {2020}, issn = {0733-8716}, doi = {10.1109/JSAC.2020.3000373}, author = {Lu, Hanlin and Li, Ming-Ju and He, Ting and Wang, Shiqiang and Narayanan, Vijaykrishnan and Chan, Kevin S.}, abstract = {Coreset, which is a summary of the original dataset in the form of a small weighted set in the same sample space, provides a promising approach to enable machine learning over distributed data. Although viewed as a proxy of the original dataset, each coreset is only designed to approximate the cost function of a specific machine learning problem, and thus different coresets are often required to solve different machine learning problems, increasing the communication overhead. We resolve this dilemma by developing robust coreset construction algorithms that can support a variety of machine learning problems. Motivated by empirical evidence that suitably-weighted k-clustering centers provide a robust coreset, we harden the observation by establishing theoretical conditions under which the coreset provides a guaranteed approximation for a broad range of machine learning problems, and developing both centralized and distributed algorithms to generate coresets satisfying the conditions. The robustness of the proposed algorithms is verified through extensive experiments on diverse datasets with respect to both supervised and unsupervised learning problems.} }
@article{WOS:000505697300001, title = {Machine learning and the physical sciences}, journal = {REVIEWS OF MODERN PHYSICS}, volume = {91}, year = {2019}, issn = {0034-6861}, doi = {10.1103/RevModPhys.91.045002}, author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborova, Lenka}, abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.} }
@article{WOS:001319107600001, title = {Applications of machine learning to behavioral sciences: focus on categorical data}, journal = {DISCOVER PSYCHOLOGY}, volume = {2}, year = {2022}, doi = {10.1007/s44202-022-00027-5}, author = {Dehghan, Pegah and Alashwal, Hany and Moustafa, Ahmed A.}, abstract = {In the last two decades, advancements in artificial intelligence and data science have attracted researchers' attention to machine learning. Growing interests in applying machine learning algorithms can be observed in different scientific areas, including behavioral sciences. However, most of the research conducted in this area applied machine learning algorithms to imagining and physiological data such as EEG and fMRI and there are relatively limited non-imaging and non-physiological behavioral studies which have used machine learning to analyze their data. Therefore, in this perspective article, we aim to (1) provide a general understanding of models built for inference, models built for prediction (i.e., machine learning), methods used in these models, and their strengths and limitations; (2) investigate the applications of machine learning to categorical data in behavioral sciences; and (3) highlight the usefulness of applying machine learning algorithms to non-imaging and non-physiological data (e.g., clinical and categorical) data and provide evidence to encourage researchers to conduct further machine learning studies in behavioral and clinical sciences.} }
@article{WOS:000684698800020, title = {Performance Analysis on Machine Learning-Based Channel Estimation}, journal = {IEEE TRANSACTIONS ON COMMUNICATIONS}, volume = {69}, pages = {5183-5193}, year = {2021}, issn = {0090-6778}, doi = {10.1109/TCOMM.2021.3083597}, author = {Mei, Kai and Liu, Jun and Zhang, Xiaochen and Rajatheva, Nandana and Wei, Jibo}, abstract = {Recently, machine learning-based channel estimation has attracted much attention. The performance of machine learning-based estimation has been validated by simulation experiments. However, little attention has been paid to the theoretical performance analysis. In this paper, we investigate the mean square error (MSE) performance of machine learning-based estimation. Hypothesis testing is employed to analyze its MSE upper bound. Furthermore, we build a statistical model for hypothesis testing, which holds when the linear learning module with a low input dimension is used in machine learning-based channel estimation, and derive a clear analytical relation between the size of the training data and performance. Then, we simulate the machine learning-based channel estimation in orthogonal frequency division multiplexing (OFDM) systems to verify our analysis results. Finally, the design considerations for the situation where only limited training data is available are discussed. In this situation, our analysis results can be applied to assess the performance and support the design of machine learning-based channel estimation.} }
@article{WOS:000822914300001, title = {Estimating the thermal conductivity of soils using six machine learning algorithms}, journal = {INTERNATIONAL COMMUNICATIONS IN HEAT AND MASS TRANSFER}, volume = {136}, year = {2022}, issn = {0735-1933}, doi = {10.1016/j.icheatmasstransfer.2022.106139}, author = {Li, Kai-Qi and Liu, Yong and Kang, Qing}, abstract = {Many machine learning algorithms have been applied to determine soil properties in recent years. However, the prediction performances of thermal conductivity of soils via machine learning algorithms are still unclear due to the lack of sufficient databases. In this work, a large database containing 2197 data points from various literature was compiled. Six machine learning algorithms, namely multivariance linear regression (MLR), Gaussian process regression (GPR), support vector machine (SVM), decision tree (DT), random forest (RF) and adaptive boosting methods (AdaBoost) were implemented to predict the thermal conductivity of soils based on the compiled database. In addition, Spearman correlation analysis and grey relational analysis were adopted to compute the strength of influencing factors in thermal conductivity. For obtaining the best prediction model, the performances of six machine learning algorithms were assessed via eight evaluation indicators and compared with three typical empirical models. Results show that the AdaBoost can yield good predicted values of the thermal conductivity of soils with minimum errors (i.e., RMSE = 0.099). This study constructs a database of thermal conductivity and provides a reference for evaluating the thermal conductivity of soils via machine learning algorithms.} }
@article{WOS:001153683600001, title = {An overview of clinical machine learning applications in neurology}, journal = {JOURNAL OF THE NEUROLOGICAL SCIENCES}, volume = {455}, year = {2023}, issn = {0022-510X}, doi = {10.1016/j.jns.2023.122799}, author = {Smith, Colin M. and Weathers, Allison L. and Lewis, Steven L.}, abstract = {Machine learning techniques for clinical applications are evolving, and the potential impact this will have on clinical neurology is important to recognize. By providing a broad overview on this growing paradigm of clinical tools, this article aims to help healthcare professionals in neurology prepare to navigate both the opportunities and challenges brought on through continued advancements in machine learning. This narrative review first elaborates on how machine learning models are organized and implemented. Machine learning tools are then classified by clinical application, with examples of uses within neurology described in more detail. Finally, this article addresses limitations and considerations regarding clinical machine learning applications in neurology.} }
@article{WOS:000562067400001, title = {Prediction of Breast Cancer, Comparative Review of Machine Learning Techniques, and Their Analysis}, journal = {IEEE ACCESS}, volume = {8}, pages = {150360-150376}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3016715}, author = {Fatima, Noreen and Liu, Li and Hong, Sha and Ahmed, Haroon}, abstract = {Breast cancer is type of tumor that occurs in the tissues of the breast. It is most common type of cancer found in women around the world and it is among the leading causes of deaths in women. This article presents the comparative analysis of machine learning, deep learning and data mining techniques being used for the prediction of breast cancer. Many researchers have put their efforts on breast cancer diagnoses and prognoses, every technique has different accuracy rate and it varies for different situations, tools and datasets being used. Our main focus is to comparatively analyze different existing Machine Learning and Data Mining techniques in order to find out the most appropriate method that will support the large dataset with good accuracy of prediction. The main purpose of this review is to highlight all the previous studies of machine learning algorithms that are being used for breast cancer prediction and this article provides the all necessary information to the beginners who want to analyze the machine learning algorithms to gain the base of deep learning.} }
@article{WOS:000802148900003, title = {Machine-Learning-Based Lightpath QoT Estimation and Forecasting}, journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY}, volume = {40}, pages = {3115-3127}, year = {2022}, issn = {0733-8724}, doi = {10.1109/JLT.2022.3160379}, author = {Allogba, Stephanie and Aladin, Sandra and Tremblay, Christine}, abstract = {Machine learning (ML) is more and more used to address the challenges of managing the physical layer of increasingly heterogeneous and complex optical networks. In this tutorial, we illustrate how simple and more sophisticated machine learning methods can be used in lightpath quality of transmission (QoT) estimation and forecast tasks. We also discuss data processing strategies with the aim to determine relevant features to feed the ML classifiers and predictors. We then introduce a preliminary study on the application of transfer learning to try to overcome the scarcity of field data.} }
@article{WOS:000943845300001, title = {Machine learning on protein-protein interaction prediction: models, challenges and trends}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {24}, year = {2023}, issn = {1467-5463}, doi = {10.1093/bib/bbad076}, author = {Tang, Tao and Zhang, Xiaocai and Liu, Yuansheng and Peng, Hui and Zheng, Binshuang and Yin, Yanlin and Zeng, Xiangxiang}, abstract = {Protein-protein interactions (PPIs) carry out the cellular processes of all living organisms. Experimental methods for PPI detection suffer from high cost and false-positive rate, hence efficient computational methods are highly desirable for facilitating PPI detection. In recent years, benefiting from the enormous amount of protein data produced by advanced high-throughput technologies, machine learning models have been well developed in the field of PPI prediction. In this paper, we present a comprehensive survey of the recently proposed machine learning-based prediction methods. The machine learning models applied in these methods and details of protein data representation are also outlined. To understand the potential improvements in PPI prediction, we discuss the trend in the development of machine learning-based methods. Finally, we highlight potential directions in PPI prediction, such as the use of computationally predicted protein structures to extend the data source for machine learning models. This review is supposed to serve as a companion for further improvements in this field.} }
@article{WOS:001013826800001, title = {Enhancing Social Media Platforms with Machine Learning Algorithms and Neural Networks}, journal = {ALGORITHMS}, volume = {16}, year = {2023}, doi = {10.3390/a16060271}, author = {Taherdoost, Hamed}, abstract = {Network analysis aids management in reducing overall expenditures and maintenance workload. Social media platforms frequently use neural networks to suggest material that corresponds with user preferences. Machine learning is one of many methods for social network analysis. Machine learning algorithms operate on a collection of observable features that are taken from user data. Machine learning and neural network-based systems represent a topic of study that spans several fields. Computers can now recognize the emotions behind particular content uploaded by users to social media networks thanks to machine learning. This study examines research on machine learning and neural networks, with an emphasis on social analysis in the context of the current literature.} }
@article{WOS:000326889800022, title = {A survey of multi-view machine learning}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {23}, pages = {2031-2038}, year = {2013}, issn = {0941-0643}, doi = {10.1007/s00521-013-1362-6}, author = {Sun, Shiliang}, abstract = {Multi-view learning or learning with multiple distinct feature sets is a rapidly growing direction in machine learning with well theoretical underpinnings and great practical success. This paper reviews theories developed to understand the properties and behaviors of multi-view learning and gives a taxonomy of approaches according to the machine learning mechanisms involved and the fashions in which multiple views are exploited. This survey aims to provide an insightful organization of current developments in the field of multi-view learning, identify their limitations, and give suggestions for further research. One feature of this survey is that we attempt to point out specific open problems which can hopefully be useful to promote the research of multi-view machine learning.} }
@article{WOS:000704376900007, title = {Federated learning on non-IID data: A survey}, journal = {NEUROCOMPUTING}, volume = {465}, pages = {371-390}, year = {2021}, issn = {0925-2312}, doi = {10.1016/j.neucom.2021.07.098}, author = {Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu}, abstract = {Federated learning is an emerging distributed machine learning framework for privacy preservation. However, models trained in federated learning usually have worse performance than those trained in the standard centralized learning mode, especially when the training data are not independent and identically distributed (Non-IID) on the local devices. In this survey, we provide a detailed analysis of the influence of Non-IID data on both parametric and non-parametric machine learning models in both horizontal and vertical federated learning. In addition, current research work on handling challenges of NonIID data in federated learning are reviewed, and both advantages and disadvantages of these approaches are discussed. Finally, we suggest several future research directions before concluding the paper. (c) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000899349300014, title = {Asset Management in Machine Learning: State-of-research and State-of-practice}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3543847}, author = {Idowu, Samuel and Struber, Daniel and Berger, Thorsten}, abstract = {Machine learning components are essential for today's software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets. This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning assetmanagement. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces (CLIs). We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.} }
@article{WOS:000488199200019, title = {Online sequential class-specific extreme learning machine for binary imbalanced learning}, journal = {NEURAL NETWORKS}, volume = {119}, pages = {235-248}, year = {2019}, issn = {0893-6080}, doi = {10.1016/j.neunet.2019.08.018}, author = {Shukla, Sanyam and Raghuwanshi, Bhagat Singh}, abstract = {Many real-world applications suffer from the class imbalance problem, in which some classes have significantly fewer examples compared to the other classes. In this paper, we focus on online sequential learning methods, which are considerably more preferable to tackle the large size imbalanced classification problems effectively. For example, weighted online sequential extreme learning machine (WOS-ELM), voting based weighted online sequential extreme learning machine (VWOS-ELM) and weighted online sequential extreme learning machine with kernels (WOS-ELMK), etc. handle the imbalanced learning effectively. One of our recent works class-specific extreme learning machine (CS-ELM) uses class-specific regularization and has been shown to perform better for imbalanced learning. This work proposes a novel online sequential class-specific extreme learning machine (OSCSELM), which is a variant of CS-ELM. OSCSELM supports online learning technique in both chunkby-chunk and one-by-one learning mode. It targets to handle the class imbalance problem for both small and larger datasets. The proposed work has less computational complexity in contrast with WOS-ELM for imbalanced learning. The proposed method is assessed by utilizing benchmark real-world imbalanced datasets. Experimental results illustrate the effectiveness of the proposed approach as it outperforms the other methods for imbalanced learning. (C) 2019 Elsevier Ltd. All rights reserved.} }
@article{WOS:001333821800001, title = {Yoked learning in molecular data science}, journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES}, volume = {5}, year = {2024}, doi = {10.1016/j.ailsci.2023.100089}, author = {Li, Zhixiong and Xiang, Yan and Wen, Yujing and Reker, Daniel}, abstract = {Active machine learning is an established and increasingly popular experimental design technique where the machine learning model can request additional data to improve the model's predictive performance. It is generally assumed that this data is optimal for the machine learning model since it relies on the model's predictions or model architecture and therefore cannot be transferred to other models. Inspired by research in pedagogy, we here introduce the concept of yoked machine learning where a second machine learning model learns from the data selected by another model. We found that in 48\\% of the benchmarked combinations, yoked learning performed similar or better than active learning. We analyze distinct cases in which yoked learning can improve active learning performance. In particular, we prototype yoked deep learning (YoDeL) where a classic machine learning model provides data to a deep neural network, thereby mitigating challenges of active deep learning such as slow refitting time per learning iteration and poor performance on small datasets. In summary, we expect the new concept of yoked (deep) learning to provide a competitive option to boost the performance of active learning and benefit from distinct capabilities of multiple machine learning models during data acquisition, training, and deployment.} }
@article{WOS:000688449200012, title = {Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {23}, pages = {1759-1799}, year = {2021}, doi = {10.1109/COMST.2021.3090430}, author = {Khan, Latif U. and Saad, Walid and Han, Zhu and Hossain, Ekram and Hong, Choong Seon}, abstract = {The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithm for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Finally, we present several open research challenges with their possible solutions.} }
@article{WOS:000804965300003, title = {A systematic review on machine learning models for online learning and examination systems}, journal = {PEERJ COMPUTER SCIENCE}, volume = {8}, year = {2022}, doi = {10.7717/peerj-cs.986}, author = {Kaddoura, Sanaa and Popescu, Daniela Elena and Hemanth, Jude D.}, abstract = {Examinations or assessments play a vital role in every student's life; they determine their future and career paths. The COVID pandemic has left adverse impacts in all areas, including the academic field. The regularized classroom learning and face-to-face real-time examinations were not feasible to avoid widespread infection and ensure safety. During these desperate times, technological advancements stepped in to aid students in continuing their education without any academic breaks. Machine learning is a key to this digital transformation of schools or colleges from real-time to online mode. Online learning and examination during lockdown were made possible by Machine learning methods. In this article, a systematic review of the role of Machine learning in Lockdown Exam Management Systems was conducted by evaluating 135 studies over the last five years. The significance of Machine learning in the entire exam cycle from pre-exam preparation, conduction of examination, and evaluation were studied and discussed. The unsupervised or supervised Machine learning algorithms were identified and categorized in each process. The primary aspects of examinations, such as authentication, scheduling, proctoring, and cheat or fraud detection, are investigated in detail with Machine learning perspectives. The main attributes, such as prediction of at-risk students, adaptive learning, and monitoring of students, are integrated for more understanding of the role of machine learning in exam preparation, followed by its management of the post-examination process. Finally, this review concludes with issues and challenges that machine learning imposes on the examination system, and these issues are discussed with solutions.} }
@article{WOS:000460685600001, title = {Machine (Deep) Learning Methods for Image Processing and Radiomics}, journal = {IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES}, volume = {3}, pages = {104-108}, year = {2019}, issn = {2469-7311}, doi = {10.1109/TRPMS.2019.2899538}, author = {Hatt, Mathieu and Parmar, Chintan and Qi, Jinyi and El Naqa, Issam}, abstract = {Methods from the field of machine (deep) learning have been successful in tackling a number of tasks in medical imaging, from image reconstruction or processing to predictive modeling, clinical planning and decision-aid systems. The ever growing availability of data and the improving ability of algorithms to learn from them has led to the rise of methods based on neural networks to address most of these tasks with higher efficiency and often superior performance than previous, ``shallow'' machine learning methods. The present editorial aims at contextualizing within this framework the recent developments of these techniques, including these described in the papers published in the present special issue on machine (deep) learning for image processing and radiomics in radiation-based medical sciences.} }
@article{WOS:001210794000001, title = {Multi-agent modelling and analysis of the knowledge learning of a human-machine hybrid intelligent organization with human-machine trust}, journal = {SYSTEMS SCIENCE \\& CONTROL ENGINEERING}, volume = {12}, year = {2024}, doi = {10.1080/21642583.2024.2343301}, author = {Xue, Chaogai and Zhang, Haoxiang and Cao, Haiwang}, abstract = {Machine learning (ML) technologies have changed the paradigm of knowledge discovery in organizations and transformed traditional organizational learning to human-machine hybrid intelligent organizational learning. However, the general distrust among humans towards knowledge derived from machine learning has hindered effective knowledge exchange between humans and machines, thereby compromising the efficiency of human-machine hybrid intelligent organizational learning. To explore this issue, we used multi-agent simulation to construct a knowledge learning model of a human-machine hybrid intelligent organization with human-machine trust. The simulation showed that whether human-machine trust has a positive effect on knowledge level depends on the initial input and the magnitude of the effect depends on the human learning propensity (exploration and exploitation). When humans reconfigure machine learning excessively, whether human-machine trust has a positive effect on the knowledge level depends on human learning propensity (exploration and exploitation). Maintaining appropriate human-machine trust in turbulent environments assists humans in integrating diverse knowledge to meet changing knowledge needs. Our study extends the human-machine hybrid intelligence organizational learning model by modeling human-machine trust. It will assist managers in effectively designing the most economical level of human-machine trust, thereby enhancing the efficiency of human-machine collaboration in human-machine hybrid intelligent organization.} }
@article{WOS:000680962300001, title = {Machine-Learning-Assisted Intelligent Imaging Flow Cytometry: A Review}, journal = {ADVANCED INTELLIGENT SYSTEMS}, volume = {3}, year = {2021}, doi = {10.1002/aisy.202100073}, author = {Luo, Shaobo and Shi, Yuzhi and Chin, Lip Ket and Hutchinson, Paul Edward and Zhang, Yi and Chierchia, Giovanni and Talbot, Hugues and Jiang, Xudong and Bourouina, Tarik and Liu, Ai-Qun}, abstract = {Imaging flow cytometry has been widely adopted in numerous applications such as optical sensing, environmental monitoring, clinical diagnostics, and precision agriculture. The system, with the assistance of machine learning, shows unprecedented advantages in automated image analysis, thus enabling high-throughput measurement, identification, and sorting of biological entities. Recently, with the burgeoning developments of machine learning algorithms, deep learning has taken over most of data analysis and promised tremendous performance in intelligent imaging flow cytometry. Herein, an overview of the basic knowledge of intelligent imaging flow cytometry, the evolution of machine learning and the typical applications, and how machine learning can be applied to assist intelligent imaging flow cytometry is provided. Perspectives of emerging machine learning algorithms in implementing future intelligent imaging flow cytometry are also discussed.} }
@article{WOS:000801156200002, title = {Machine Learning for Cataract Classification/Grading on Ophthalmic Imaging Modalities: A Survey}, journal = {MACHINE INTELLIGENCE RESEARCH}, volume = {19}, pages = {184-208}, year = {2022}, issn = {2731-538X}, doi = {10.1007/s11633-022-1329-0}, author = {Zhang, Xiao-Qing and Hu, Yan and Xiao, Zun-Jie and Fang, Jian-Sheng and Higashita, Risa and Liu, Jiang}, abstract = {Cataracts are the leading cause of visual impairment and blindness globally. Over the years, researchers have achieved significant progress in developing state-of-the-art machine learning techniques for automatic cataract classification and grading, aiming to prevent cataracts early and improve clinicians' diagnosis efficiency. This survey provides a comprehensive survey of recent advances in machine learning techniques for cataract classification/grading based on ophthalmic images. We summarize existing literature from two research directions: conventional machine learning methods and deep learning methods. This survey also provides insights into existing works of both merits and limitations. In addition, we discuss several challenges of automatic cataract classification/grading based on machine learning techniques and present possible solutions to these challenges for future research.} }
@article{WOS:000725106400001, title = {Applying machine learning algorithms to predict default probability in the online credit market: Evidence from China}, journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS}, volume = {79}, year = {2022}, issn = {1057-5219}, doi = {10.1016/j.irfa.2021.101971}, author = {Liu, Yi and Yang, Menglong and Wang, Yudong and Li, Yongshan and Xiong, Tiancheng and Li, Anzhe}, abstract = {Using data from Renrendai and three machine learning algorithms, namely, k-nearest neighbor, support vector machine, and random forest, we predicted the default probability of online loan borrowers and compared their prediction performance with that of a logistic model. The results show that, first, based on the AUC (area under the ROC curve) value, accuracy rate and Brier score, the machine learning models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the machine learning algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that the machine learning algorithms can provide more benefits to investors.} }
@article{WOS:000559380900005, title = {Machine learning-driven new material discovery}, journal = {NANOSCALE ADVANCES}, volume = {2}, pages = {3115-3130}, year = {2020}, issn = {2516-0230}, doi = {10.1039/d0na00388c}, author = {Cai, Jiazhen and Chu, Xuan and Xu, Kun and Li, Hongbo and Wei, Jing}, abstract = {New materials can bring about tremendous progress in technology and applications. However, the commonly used trial-and-error method cannot meet the current need for new materials. Now, a newly proposed idea of using machine learning to explore new materials is becoming popular. In this paper, we review this research paradigm of applying machine learning in material discovery, including data preprocessing, feature engineering, machine learning algorithms and cross-validation procedures. Furthermore, we propose to assist traditional DFT calculations with machine learning for material discovery. Many experiments and literature reports have shown the great effects and prospects of this idea. It is currently showing its potential and advantages in property prediction, material discovery, inverse design, corrosion detection and many other aspects of life.} }
@article{WOS:001016803900001, title = {Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions}, journal = {IEEE ACCESS}, volume = {11}, pages = {60153-60170}, year = {2023}, issn = {2169-3536}, doi = {10.1109/ACCESS.2023.3286344}, author = {Mandalapu, Varun and Elluri, Lavanya and Vyas, Piyush and Roy, Nirmalya}, abstract = {Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime prediction techniques, law enforcement agencies can develop strategies to prevent and respond to criminal activities more effectively.} }
@article{WOS:000431709400001, title = {Automatic Differentiation in Machine Learning: a Survey}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {18}, year = {2018}, issn = {1532-4435}, author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark}, abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ``autodiff'', is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ``dynamic computational graphs'' and ``differentiable programming''. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ``autodiff'', ``automatic differentiation'', and ``symbolic differentiation'' as these are encountered more and more in machine learning settings.} }
@article{WOS:001395340500013, title = {Recent Advances in Optimal Transport for Machine Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {47}, pages = {1161-1180}, year = {2025}, issn = {0162-8828}, doi = {10.1109/TPAMI.2024.3489030}, author = {Montesuma, Eduardo Fernandes and Mboula, Fred Maurice Ngole and Souloumiac, Antoine}, abstract = {Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 - 2023, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport and its extensions, such as partial, unbalanced, Gromov and Neural Optimal Transport, and its interplay with Machine Learning practice.} }
@article{WOS:000297611600005, title = {Voting based extreme learning machine}, journal = {INFORMATION SCIENCES}, volume = {185}, pages = {66-77}, year = {2012}, issn = {0020-0255}, doi = {10.1016/j.ins.2011.09.015}, author = {Cao, Jiuwen and Lin, Zhiping and Huang, Guang-Bin and Liu, Nan}, abstract = {This paper proposes an improved learning algorithm for classification which is referred to as voting based extreme learning machine. The proposed method incorporates the voting method into the popular extreme learning machine (ELM) in classification applications. Simulations on many real world classification datasets have demonstrated that this algorithm generally outperforms the original ELM algorithm as well as several recent classification algorithms. (C) 2011 Elsevier Inc. All rights reserved.} }
@article{WOS:000617870300006, title = {Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment}, journal = {WIRELESS PERSONAL COMMUNICATIONS}, volume = {117}, pages = {3505-3525}, year = {2021}, issn = {0929-6212}, doi = {10.1007/s11277-021-08284-8}, author = {Chen Hongsong and Zhang Yongpeng and Cao Yongrui and Bhargava, Bharat}, abstract = {Under big data environment, machine learning has been rapidly developed and widely used. It has been successfully applied in computer vision, natural language processing, computer security and other application fields. However, there are many security problems in machine learning under big data environment. For example, attackers can add ``poisoned'' sample to the data source, and big data process system will process these ``poisoned'' sample and use machine learning methods to train model, which will directly lead to wrong prediction results. In this paper, machine learning system and machine learning pipeline are proposed. The security problems that maybe occur in each stage of machine learning system under big data processing pipeline are analyzed comprehensively. We use four different attack methods to compare the attack experimental results.The security problems are classified comprehensively, and the defense approaches to each security problem are analyzed. Drone-deploy MapEngine is selected as a case study, we analyze the security threats and defense approaches in the Drone-Cloud machine learning application envirolment. At last,the future development drections of security issues and challenages in the machine learning system are proposed.} }
@article{WOS:000838497700001, title = {Machine Learning in Precision Agriculture: A Survey on Trends, Applications and Evaluations Over Two Decades}, journal = {IEEE ACCESS}, volume = {10}, pages = {73786-73803}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3188649}, author = {Condran, Sarah and Bewong, Michael and Islam, Md Zahidul and Maphosa, Lancelot and Zheng, Lihong}, abstract = {Precision agriculture represents the new age of conventional agriculture. This is made possible by the advancement of various modern technologies such as the internet of things. The unparalleled potential for data collection and analytics has resulted in an increase in multi-disciplinary research within machine learning and agriculture. However, the application of machine learning techniques to agriculture seems to be out of step with core machine learning research. This gap is further exacerbated by the inherent challenges associated with agricultural data. In this work, we conduct a systematic review of a large body of academic literature published between 2000 and 2022, on the application of machine learning techniques to agriculture. We identify and discuss some of the key data issues such as class imbalance, data sparsity and high dimensionality. Further, we study the impact of these data issues on various machine learning approaches within the context of agriculture. Finally, we identify some of the common pitfalls in the machine learning and agriculture research including the misapplication of machine learning evaluation techniques. To this end, this survey presents a holistic view on the state of affairs in the cross-domain of machine learning and agriculture and proposes some suitable mitigation strategies to address these challenges.} }
@article{WOS:000556625500002, title = {Digital Pharmaceutical Sciences}, journal = {AAPS PHARMSCITECH}, volume = {21}, year = {2020}, issn = {1530-9932}, doi = {10.1208/s12249-020-01747-4}, author = {Damiati, Safa A.}, abstract = {Artificial intelligence (AI) and machine learning, in particular, have gained significant interest in many fields, including pharmaceutical sciences. The enormous growth of data from several sources, the recent advances in various analytical tools, and the continuous developments in machine learning algorithms have resulted in a rapid increase in new machine learning applications in different areas of pharmaceutical sciences. This review summarizes the past, present, and potential future impacts of machine learning technologies on different areas of pharmaceutical sciences, including drug design and discovery, preformulation, and formulation. The machine learning methods commonly used in pharmaceutical sciences are discussed, with a specific emphasis on artificial neural networks due to their capability to model the nonlinear relationships that are commonly encountered in pharmaceutical research. AI and machine learning technologies in common day-to-day pharma needs as well as industrial and regulatory insights are reviewed. Beyond traditional potentials of implementing digital technologies using machine learning in the development of more efficient, fast, and economical solutions in pharmaceutical sciences are also discussed.} }
@article{WOS:000678520000001, title = {The absorption and multiplication of uncertainty in machine-learning-driven finance}, journal = {BRITISH JOURNAL OF SOCIOLOGY}, volume = {72}, pages = {1015-1029}, year = {2021}, issn = {0007-1315}, doi = {10.1111/1468-4446.12880}, author = {Hansen, Kristian Bondo and Borch, Christian}, abstract = {Uncertainty about market developments and their implications characterize financial markets. Increasingly, machine learning is deployed as a tool to absorb this uncertainty and transform it into manageable risk. This article analyses machine-learning-based uncertainty absorption in financial markets by drawing on 182 interviews in the finance industry, including 45 interviews with informants who were actively applying machine-learning techniques to investment management, trading, or risk management problems. We argue that while machine-learning models are deployed to absorb financial uncertainty, they also introduce a new and more profound type of uncertainty, which we call critical model uncertainty. Critical model uncertainty refers to the inability to explain how and why the machine-learning models (particularly neural networks) arrive at their predictions and decisions-their uncertainty-absorbing accomplishments. We suggest that the dialectical relation between machine-learning models' uncertainty absorption and multiplication calls for further research in the field of finance and beyond.} }
@article{WOS:000502568900010, title = {Using machine learning to explain the heterogeneity of schizophrenia. Realizing the promise and avoiding the hype}, journal = {SCHIZOPHRENIA RESEARCH}, volume = {214}, pages = {70-75}, year = {2019}, issn = {0920-9964}, doi = {10.1016/j.schres.2019.08.032}, author = {Tandon, Neeraj and Tandon, Rajiv}, abstract = {Despite extensive research and prodigious advances in neuroscience, our comprehension of the nature of schizophrenia remains rudimentary. Our failure to make progress is attributed to the extreme heterogeneity of this condition, enormous complexity of the human brain, limitations of extant research paradigms, and inadequacy of traditional statistical methods to integrate or interpret increasingly large amounts of multidimensional information relevant to unravelling brain function. Fortunately, the rapidly developing science of machine learning appears to provide tools capable of addressing each of these impediments. Enthusiasm about the potential of machine learning methods to break the current impasse is reflected in the steep increase in the number of scientific publication about the application of machine learning to the study of schizophrenia. Machine learning approaches are, however, poorly understood by schizophrenia researchers and clinicians alike. In this paper, we provide a simple description of the nature and techniques of machine learning and their application to the study of schizophrenia. We then summarize its potential and constraints with illustrations from six studies of machine learning in schizophrenia and address some common misconceptions about machine learning. We suggest some guidelines for researchers, readers, science editors and reviewers of the burgeoning machine learning literature in schizophrenia. In order to realize its enormous promise, we suggest the need for the disciplined application of machine learning methods to the study of schizophrenia with a dear recognition of its capability and challenges accompanied by a concurrent effort to improve machine learning literacy among neuroscientists and mental health professionals. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000524750000009, title = {Analysis of Dimensionality Reduction Techniques on Big Data}, journal = {IEEE ACCESS}, volume = {8}, pages = {54776-54788}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2980942}, author = {Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and Baker, Thar}, abstract = {Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.} }
@article{WOS:000550878000001, title = {How can machine learning aid behavioral marketing research?}, journal = {MARKETING LETTERS}, volume = {31}, pages = {361-370}, year = {2020}, issn = {0923-0645}, doi = {10.1007/s11002-020-09535-7}, author = {Hagen, Linda and Uetake, Kosuke and Yang, Nathan and Bollinger, Bryan and Chaney, Allison J. B. and Dzyabura, Daria and Etkin, Jordan and Goldfarb, Avi and Liu, Liu and Sudhir, K. and Wang, Yanwen and Wright, James R. and Zhu, Ying}, abstract = {Behavioral science and machine learning have rapidly progressed in recent years. As there is growing interest among behavioral scholars to leverage machine learning, we present strategies for how these methods that can be of value to behavioral scientists using examples centered on behavioral research.} }
@article{WOS:000444659200024, title = {Wild patterns: Ten years after the rise of adversarial machine learning}, journal = {PATTERN RECOGNITION}, volume = {84}, pages = {317-331}, year = {2018}, issn = {0031-3203}, doi = {10.1016/j.patcog.2018.07.023}, author = {Biggio, Battista and Roli, Fabio}, abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:001091638200001, title = {Machine learning in marketing: Recent progress and future research directions}, journal = {JOURNAL OF BUSINESS RESEARCH}, volume = {170}, year = {2024}, issn = {0148-2963}, doi = {10.1016/j.jbusres.2023.114254}, author = {Herhausen, Dennis and Bernritter, Stefan F. and Ngai, Eric W. T. and Kumar, Ajay and Delen, Dursun}, abstract = {Decision-making in marketing has changed dramatically in the past decade. Companies increasingly use algorithms to generate predictions for marketing decisions, such as which consumers to target with which offers. Such algorithmic decision-making promises to make marketing more intelligent, efficient, consumer-friendly, and, ultimately, more effective. Not surprisingly, machine learning is a trending topic for marketing researchers and practitioners. However, machine learning also introduces important challenges to the marketing landscape. We discuss this development by outlining recent progress and future research directions of machine learning in marketing. Specifically, we provide an overview of typical machine learning applications in marketing and present a guiding framework. We position the articles in the Journal of Business Research's Special Issue on ``Machine Learning in Marketing'' within this framework and conclude by putting forward a research agenda to further guide future research in this area.} }
@article{WOS:000703023000002, title = {Machine Learning Approaches to Retrieve High-Quality, Clinically Relevant Evidence From the Biomedical Literature: Systematic Review}, journal = {JMIR MEDICAL INFORMATICS}, volume = {9}, year = {2021}, doi = {10.2196/30401}, author = {Abdelkader, Wael and Navarro, Tamara and Parrish, Rick and Cotoi, Chris and Germini, Federico and Iorio, Alfonso and Haynes, R. Brian and Lokker, Cynthia}, abstract = {Background: The rapid growth of the biomedical literature makes identifying strong evidence a time-consuming task. Applying machine learning to the process could be a viable solution that limits effort while maintaining accuracy. Objective: The goal of the research was to summarize the nature and comparative performance of machine learning approaches that have been applied to retrieve high-quality evidence for clinical consideration from the biomedical literature. Methods: We conducted a systematic review of studies that applied machine learning techniques to identify high-quality clinical articles in the biomedical literature. Multiple databases were searched to July 2020. Extracted data focused on the applied machine learning model, steps in the development of the models, and model performance. Results: From 3918 retrieved studies, 10 met our inclusion criteria. All followed a supervised machine learning approach and applied, from a limited range of options, a high-quality standard for the training of their model. The results show that machine learning can achieve a sensitivity of 95\\% while maintaining a high precision of 86\\%. Conclusions: Machine learning approaches perform well in retrieving high-quality clinical studies. Performance may improve by applying more sophisticated approaches such as active learning and unsupervised machine learning approaches.} }
@article{WOS:001181952800001, title = {Machine learning enabled Industrial IoT Security: Challenges, Trends and Solutions}, journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION}, volume = {38}, year = {2024}, issn = {2467-964X}, doi = {10.1016/j.jii.2023.100549}, author = {Ni, Chunchun and Li, Shan Cang}, abstract = {Introduction: The increasingly integrated Industrial IoT (IIoT) with industrial systems brings benefits such as intelligent analytics, predictive maintenance, and remote monitoring. However, it also exposes industry systems to malware, cyber attacks, and other security risks. Objectives: Machine learning techniques shows promising performance in cyber security, including threats detection, vulnerability analysis, risk assessment, etc. This work aims to investigate how machine learning based techniques can be used in enhancing IIoT security and preventing cyber security events against cyber attack, safety threats, and process disruption. Methods: A comprehensive survey was conducted to show how machine learning techniques learn detection malicious activities automatically. Conclusion: This work reviewed current literature related to machine learning based methods currently being used in IIoT cyber security, and key methods have been presented and compared in terms of their capability and performance against cyber attacks.} }
@article{WOS:000509098300009, title = {Recent progress in augmenting turbulence models with physics-informed machine learning}, journal = {JOURNAL OF HYDRODYNAMICS}, volume = {31}, pages = {1153-1158}, year = {2019}, issn = {1001-6058}, doi = {10.1007/s42241-019-0089-y}, author = {Zhang, Xinlei and Wu, Jinlong and Coutier-Delgosha, Olivier and Xiao, Heng}, abstract = {In view of the long stagnation in traditional turbulence modeling, researchers have attempted using machine learning to augment turbulence models. This paper presents some of the recent progresses in our group on augmenting turbulence models with physics-informed machine learning. We also discuss our works on ensemble-based field inversion to provide training data for constructing machine learning models. Future and on-going research efforts are introduced.} }
@article{WOS:000582339000004, title = {Machine learning and AI in marketing - Connecting computing power to human insights}, journal = {INTERNATIONAL JOURNAL OF RESEARCH IN MARKETING}, volume = {37}, pages = {481-504}, year = {2020}, issn = {0167-8116}, doi = {10.1016/j.ijresmar.2020.04.005}, author = {Ma, Liye and Sun, Baohong}, abstract = {Artificial intelligence (AI) agents driven by machine learning algorithms are rapidly transforming the business world, generating heightened interest from researchers. In this paper, we review and call for marketing research to leverage machine learning methods. We provide an overview of common machine learning tasks and methods, and compare them with statistical and econometric methods that marketing researchers traditionally use. We argue that machine learning methods can process large-scale and unstructured data, and have flexible model structures that yield strong predictive performance. Meanwhile, such methods may lack model transparency and interpretability. We discuss salient AI-driven industry trends and practices, and review the still nascent academic marketing literature which uses machine learning methods. More importantly, we present a unified conceptual framework and a multi-faceted research agenda. From five key aspects of empirical marketing research: method, data, usage, issue, and theory, we propose a number of research priorities, including extending machine learning methods and using them as core components in marketing research, using the methods to extract insights from large-scale unstructured, tracking, and network data, using them in transparent fashions for descriptive, causal, and prescriptive analyses, using them to map out customer purchase journeys and develop decision-support capabilities, and connecting the methods to human insights and marketing theories. Opportunities abound for machine learning methods in marketing, and we hope our multi-faceted research agenda will inspire more work in this exciting area. (c) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000936903600001, title = {Leakage detection in water distribution networks using machine-learning strategies}, journal = {WATER SUPPLY}, volume = {23}, pages = {1115-1126}, year = {2023}, issn = {1606-9749}, doi = {10.2166/ws.2023.054}, author = {Sousa, Diego and Du, Rong and da Silva Jr, Jose Mairton Barros and Cavalcante, Charles Casimiro and Fischione, Carlo}, abstract = {This work proposes a reliable leakage detection methodology for water distribution networks (WDNs) using machine-learning strategies. Our solution aims at detecting leakage in WDNs using efficient machine-learning strategies. We analyze pressure measurements from pumps in district metered areas (DMAs) in Stockholm, Sweden, where we consider a residential DMA of the water distribution network. Our proposed methodology uses learning strategies from unsupervised learning (K-means and cluster validation techniques), and supervised learning (learning vector quantization algorithms). The learning strategies we propose have low complexity, and the numerical experiments show the potential of using machine-learning strategies in leakage detection for monitored WDNs. Specifically, our experiments show that the proposed learning strategies are able to obtain correct classification rates up to 93.98\\%.} }
@article{WOS:001087877900001, title = {Machine learning for leaf disease classification: data, techniques and applications}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {S3571-S3616}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-023-10610-4}, author = {Yao, Jianping and Tran, Son N. and Sawyer, Samantha and Garg, Saurabh}, abstract = {The growing demand for sustainable development brings a series of information technologies to help agriculture production. Especially, the emergence of machine learning applications, a branch of artificial intelligence, has shown multiple breakthroughs which can enhance and revolutionize plant pathology approaches. In recent years, machine learning has been adopted for leaf disease classification in both academic research and industrial applications. Therefore, it is enormously beneficial for researchers, engineers, managers, and entrepreneurs to have a comprehensive view about the recent development of machine learning technologies and applications for leaf disease detection. This study will provide a survey in different aspects of the topic including data, techniques, and applications. The paper will start with publicly available datasets. After that, we summarize common machine learning techniques, including traditional (shallow) learning, deep learning, and augmented learning. Finally, we discuss related applications. This paper would provide useful resources for future study and application of machine learning for smart agriculture in general and leaf disease classification in particular.} }
@article{WOS:000764828500048, title = {Machine-Learning-Driven Digital Twin for Lifecycle Management of Complex Equipment}, journal = {IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING}, volume = {10}, pages = {9-22}, year = {2022}, issn = {2168-6750}, doi = {10.1109/TETC.2022.3143346}, author = {Ren, Zijie and Wan, Jiafu and Deng, Pan}, abstract = {The full life cycle management of complex equipment is considered fundamental to the intelligent transformation and upgrading of the modern manufacturing industry. Digital twin technology and machine learning have been emerging technologies in recent years. The application of these two technologies in the full life cycle management of complex equipment can make each stage of the life cycle more responsive, predictable, and adaptable. This paper first proposes a technical system that embeds machine learning modules into digital twins. Next, on this basis, a full life cycle digital twin for complex equipment is constructed, and joint application of sub-models and machine learning is explored. Then, the application of a combination of the digital twin in maintenance with machine learning in predictive maintenance of diesel locomotives is presented. The effectiveness of the proposed management method is verified by experiments. The abnormal axle temperature can be alarmed about one week in advance. Lastly. possible application advantages of the combination of digital twin and machine learning in addressing future research direction in this field are introduced.} }
@article{WOS:001158492200001, title = {Machine learning models in phononic metamaterials}, journal = {CURRENT OPINION IN SOLID STATE \\& MATERIALS SCIENCE}, volume = {28}, year = {2024}, issn = {1359-0286}, doi = {10.1016/j.cossms.2023.101133}, author = {Liu, Chen-Xu and Yu, Gui-Lan and Liu, Zhanli}, abstract = {Machine learning opens up a new avenue for advancing the development of phononic crystals and elastic metamaterials. Numerous learning models have been employed and developed to address various challenges in the field of phononic metamaterials. Here, we provide an overview of mainstream machine learning models applied to phononic metamaterials, discuss their capabilities as well as limitations, and explore potential directions for future research.} }
@article{WOS:001306820800003, title = {Time efficient variants of Twin Extreme Learning Machine}, journal = {INTELLIGENT SYSTEMS WITH APPLICATIONS}, volume = {17}, year = {2023}, doi = {10.1016/j.iswa.2022.200169}, author = {Anand, Pritam and Bharti, Amisha and Rastogi, Reshma}, abstract = {Twin Extreme Learning Machine models can obtain better generalization ability than the standard Extreme Learning Machine model. But, they require to solve a pair of quadratic programming problems for this. It makes them more complex and computationally expensive than the standard Extreme Learning Machine model. In this paper, we propose two novel time-efficient formulations of the Twin Extreme Learning Machine, which only require the solution of systems of linear equations for obtaining the final classifier. In this sense, they can combine the benefits of the Twin Support Vector Machine and standard Extreme Learning Machine in the true sense. We term our first formulation as `Least Squared Twin Extreme Learning Machine'. It minimizes the L 2-norm of error variables in its optimization problem. Our second formulation `Weighted Linear loss Twin Extreme Learning Machine' uses the weighted linear loss function for calculating the empirical error, which makes it insensitive towards outliers. Numerical results obtained with multiple benchmark datasets show that proposed formulations are time efficient with better generalization ability. Further, we have used the proposed formulations in the detection of phishing websites and shown that they are much more effective in the detection of phishing websites than other Extreme Learning Machine models.} }
@article{WOS:000339429200011, title = {Investigation on Cancer Classification Using Machine Learning Approaches}, journal = {JOURNAL OF BIOMATERIALS AND TISSUE ENGINEERING}, volume = {4}, pages = {492-500}, year = {2014}, issn = {2157-9083}, doi = {10.1166/jbt.2014.1186}, author = {Bharathi, A. and Anandakumar, K.}, abstract = {The objective of this paper is to develop an effective machine learning approaches for cancer classification, which could provide reliable cancer classification with better accuracy. The work comprises of two steps. In the first step, using Analysis of Variance (ANOVA) ranking scheme to choose the important genes. The second step involves the classification task using an efficient classifier. In this paper we use the three efficient machine learning classifiers such as Fast Support Vector Machine Learning (FSVML), Fast Extreme Learning Machine Learning (FELML) and Relevance Vector Machine Learning (RVMMLML). The investigational values are computed using three datasets namely Lymphoma, Leukemia and SRBCT. The results are interpreted in terms of Testing Accuracy and Training Time. From the investigational results, it is observed that the proposed RVMMLML machine learning approach gives better testing accuracy results for all the datasets considered.} }
@article{WOS:000327552100003, title = {A tour of machine learning: An AI perspective}, journal = {AI COMMUNICATIONS}, volume = {27}, pages = {11-23}, year = {2014}, issn = {0921-7126}, doi = {10.3233/AIC-130580}, author = {Sebag, Michele}, abstract = {Machine Learning has been at the core of Artificial Intelligence since its inception. Many promises have been held, if one is to consider that Google is a living demonstration of AI. This paper presents a historical perspective on Machine Learning, describing how the emphasis was gradually shifted from logical to statistical induction, from induction to optimization, from the search of hypotheses to the search of representations. The paper concludes with a discussion about the new frontier of Machine Learning.} }
@article{WOS:000412100700016, title = {Source localization in an ocean waveguide using supervised machine learning}, journal = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA}, volume = {142}, pages = {1176-1188}, year = {2017}, issn = {0001-4966}, doi = {10.1121/1.5000165}, author = {Niu, Haiqiang and Reeves, Emma and Gerstoft, Peter}, abstract = {Source localization in ocean acoustics is posed as a machine learning problem in which data-driven methods learn source ranges directly from observed acoustic data. The pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix and used as the input for three machine learning methods: feed-forward neural networks (FNN), support vector machines (SVM), and random forests (RF). The range estimation problem is solved both as a classification problem and as a regression problem by these three machine learning algorithms. The results of range estimation for the Noise09 experiment are compared for FNN, SVM, RF, and conventional matched-field processing and demonstrate the potential of machine learning for underwater source localization. (C) 2017 Acoustical Society of America.} }
@article{WOS:000631777400001, title = {Material analysis and big data monitoring of sports training equipment based on machine learning algorithm}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {2749-2763}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-021-05852-8}, author = {Zhang, Lei and Li, Ning}, abstract = {Different machine learning algorithms predict the application effect of perovskite materials in sports training equipment. The sensitivity to material data is different on different ranges of data sets. Therefore, the algorithm needs to be selected according to specific material data samples. This study compares the prediction performance of neural network prediction algorithm (NN), genetic algorithm, and support vector machine-based machine learning algorithm (SVM) and uses statistical analysis to perform data analysis and draw corresponding curves. Moreover, this study uses a single perovskite material to verify the algorithm performance. In addition, based on the real data, the three machine learning algorithms of this study are applied to the related performance prediction, and the comparative analysis method is used to analyze the prediction performance of the machine learning algorithm. Through data analysis and chart analysis, we can see that machine learning algorithms have a certain effect in the application prediction of perovskite materials in sports training equipment. Among the three machine learning algorithms selected in this study, the performance of the machine learning algorithm based on support vector machine in all aspects is more excellent.} }
@article{WOS:000922362000001, title = {A narrative review of the application of machine learning in venous thromboembolism}, journal = {VASCULAR}, volume = {32}, pages = {698-704}, year = {2024}, issn = {1708-5381}, doi = {10.1177/17085381231153216}, author = {Zou, Shirong and Wu, Zhoupeng}, abstract = {Objective To summarize the current research progress of machine learning and venous thromboembolism. Methods The literature on risk factors, diagnosis, prevention and prognosis of machine learning and venous thromboembolism in recent years was reviewed. Results Machine learning is the future of biomedical research, personalized medicine, and computer-aided diagnosis, and will significantly promote the development of biomedical research and healthcare. However, many medical professionals are not familiar with it. In this review, we will introduce several commonly used machine learning algorithms in medicine, discuss the application of machine learning in venous thromboembolism, and reveal the challenges and opportunities of machine learning in medicine. Conclusion The incidence of venous thromboembolism is high, the diagnostic measures are diverse, and it is necessary to classify and treat machine learning, and machine learning as a research tool, it is more necessary to strengthen the special research of venous thromboembolism and machine learning.} }
@article{WOS:000922762600001, title = {Emerging Trends in Machine Learning: A Polymer Perspective}, journal = {ACS POLYMERS AU}, volume = {3}, pages = {239-258}, year = {2023}, doi = {10.1021/acspolymersau.2c00053}, author = {Martin, Tyler B. and Audus, Debra J.}, abstract = {In the last five years, there has been tremendous growth in machine learning and artificial intelligence as applied to polymer science. Here, we highlight the unique challenges presented by polymers and how the field is addressing them. We focus on emerging trends with an emphasis on topics that have received less attention in the review literature. Finally, we provide an outlook for the field, outline important growth areas in machine learning and artificial intelligence for polymer science and discuss important advances from the greater material science community.} }
@article{WOS:000447385100002, title = {Laplacian twin extreme learning machine for semi-supervised classification}, journal = {NEUROCOMPUTING}, volume = {321}, pages = {17-27}, year = {2018}, issn = {0925-2312}, doi = {10.1016/j.neucom.2018.08.028}, author = {Li, Shuang and Song, Shiji and Wan, Yihe}, abstract = {Twin extreme learning machine (TELM) is an efficient and effective method for pattern classification, based on widely known extreme learning machine (ELM). However, TELM is mainly used to deal with supervised learning problems. In this paper, we extend TELM to handle semi-supervised learning problems and propose a novel Laplacian twin extreme learning machine (LapTELM), which simultaneously trains two related and paired semi-supervised ELMs with two nonparallel separating planes for the final classification. The proposed method exploits the geometry structure property of the unlabeled samples and incorporates it as a manifold regularization term. This allows LapTELM to reap the benefits of fully exploring the plentiful unlabeled samples while retaining the learning ability and efficiency of TELM. Moreover, the paper shows that semi-supervised and supervised TELM can form an unified learning framework. Compared with several mainstream semi-supervised learning methods, the experimental results on the synthetic and several real-world data sets verify the effectiveness and efficiency of LapTELM. (c) 2018 Published by Elsevier B.V.} }
@article{WOS:001273496200001, title = {A comprehensive survey on weed and crop classi fi cation using machine learning and deep learning}, journal = {ARTIFICIAL INTELLIGENCE IN AGRICULTURE}, volume = {13}, pages = {45-63}, year = {2024}, doi = {10.1016/j.aiia.2024.06.005}, author = {Adhinata, Faisal Dharma and Wahyono and Sumiharto, Raden}, abstract = {Machine learning and deep learning are subsets of Artificial Intelligence that have revolutionized object detection and classification in images or videos. This technology plays a crucial role in facilitating the transition from conventional to precision agriculture, particularly in the context of weed control. Precision agriculture, which previously relied on manual efforts, has now embraced the use of smart devices for more efficient weed detection. However, several challenges are associated with weed detection, including the visual similarity between weed and crop, occlusion and lighting effects, as well as the need for early-stage weed control. Therefore, this study aimed to provide a comprehensive review of the application of both traditional machine learning and deep learning, as well as the combination of the two methods, for weed detection across different crop fields. The results of this review show the advantages and disadvantages of using machine learning and deep learning. Generally, deep learning produced superior accuracy compared to machine learning under various conditions. Machine learning required the selection of the right combination of features to achieve high accuracy in classifying weed and crop, particularly under conditions consisting of lighting and early growth effects. Moreover, a precise segmentation stage would be required in cases of occlusion. Machine learning had the advantage of achieving real-time processing by producing smaller models than deep learning, thereby eliminating the need for additional GPUs. However, the development of GPU technology is currently rapid, so researchers are more often using deep learning for more accurate weed identification. (c) 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:000708445300016, title = {A Review on Machine Learning for EEG Signal Processing in Bioengineering}, journal = {IEEE REVIEWS IN BIOMEDICAL ENGINEERING}, volume = {14}, pages = {204-218}, year = {2021}, issn = {1937-3333}, doi = {10.1109/RBME.2020.2969915}, author = {Hosseini, Mohammad-Parsa and Hosseini, Amin and Ahi, Kiarash}, abstract = {Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. Due to the many different types of classifiers available to use, the analysis methods are also equally numerous. In this review, we will be examining specifically machine learning methods that have been developed for EEG analysis with bioengineering applications. We reviewed literature from 1988 to 2018 to capture previous and current classification methods for EEG in multiple applications. From this information, we are able to determine the overall effectiveness of each machine learning method as well as the key characteristics. We have found that all the primary methods used in machine learning have been applied in some form in EEG classification. This ranges from Naive-Bayes to Decision Tree/Random Forest, to Support Vector Machine (SVM). Supervised learning methods are on average of higher accuracy than their unsupervised counterparts. This includes SVM and KNN. While each of the methods individually is limited in their accuracy in their respective applications, there is hope that the combination of methods when implemented properly has a higher overall classification accuracy. This paper provides a comprehensive overview of Machine Learning applications used in EEG analysis. It also gives an overview of each of the methods and general applications that each is best suited to.} }
@article{WOS:001116894500001, title = {Quantum machine learning for natural language processing application}, journal = {PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS}, volume = {627}, year = {2023}, issn = {0378-4371}, doi = {10.1016/j.physa.2023.129123}, author = {Pandey, Shyambabu and Basisth, Nihar Jyoti and Sachan, Tushar and Kumari, Neha and Pakray, Partha}, abstract = {Quantum computing is a speedily emerging area that applies quantum mechanics properties to solve complex problems that are difficult for classical computing. Machine learning is a sub-field of artificial intelligence which makes computers learn patterns from experiences. Due to the exponential growth of data, machine learning algorithms may be insufficient for big data, whereas on other side quantum computing can do fast computing. A combination of quantum computing and machine learning gave rise to a new field known as quantum machine learning. Quantum machine learning algorithms take advantage of the fast processing of quantum computing and show speedup compared to their classical counterpart. Natural language processing is another area of artificial intelligence that enables the computer to understand human languages. Now, researchers are trying to take advantage of quantum machine learning speedup in natural language processing applications. In this paper, first, we discuss the path from quantum computing to quantum machine learning. Then we review the state of the art of quantum machine learning for natural language processing applications. We also provide classical and quantum-based long short-term memory for parts of speech tagging on social media code mixed language. Our experiment shows that quantum-based long short-term memory performance is better than classical long short-term memory for parts of speech tagging of code-mixed datasets.(c) 2023 Elsevier B.V. All rights reserved.} }
@article{WOS:001181945300001, title = {A review of traditional Chinese medicine diagnosis using machine learning: Inspection, auscultation-olfaction, inquiry, and palpation}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {170}, year = {2024}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2024.108074}, author = {Tian, Dingcheng and Chen, Weihao and Xu, Dechao and Xu, Lisheng and Xu, Gang and Guo, Yaochen and Yao, Yudong}, abstract = {Traditional Chinese medicine (TCM) is an essential part of the Chinese medical system and is recognized by the World Health Organization as an important alternative medicine. As an important part of TCM, TCM diagnosis is a method to understand a patient's illness, analyze its state, and identify syndromes. In the longterm clinical diagnosis practice of TCM, four fundamental and effective diagnostic methods of inspection, auscultation-olfaction, inquiry, and palpation (IAOIP) have been formed. However, the diagnostic information in TCM is diverse, and the diagnostic process depends on doctors' experience, which is subject to a highlevel subjectivity. At present, the research on the automated diagnosis of TCM based on machine learning is booming. Machine learning, which includes deep learning, is an essential part of artificial intelligence (AI), which provides new ideas for the objective and AI-related research of TCM. This paper aims to review and summarize the current research status of machine learning in TCM diagnosis. First, we review some key factors for the application of machine learning in TCM diagnosis, including data, data preprocessing, machine learning models, and evaluation metrics. Second, we review and summarize the research and applications of machine learning methods in TCM IAOIP and the synthesis of the four diagnostic methods. Finally, we discuss the challenges and research directions of using machine learning methods for TCM diagnosis.} }
@article{WOS:000557459300020, title = {Real-World Integration of a Sepsis Deep Learning Technology Into Routine Clinical Care: Implementation Study}, journal = {JMIR MEDICAL INFORMATICS}, volume = {8}, year = {2020}, doi = {10.2196/15182}, author = {Sendak, Mark P. and Ratliff, William and Sarro, Dina and Alderton, Elizabeth and Futoma, Joseph and Gao, Michael and Nichols, Marshall and Revoir, Mike and Yashar, Faraz and Miller, Corinne and Kester, Kelly and Sandhu, Sahil and Corey, Kristin and Brajer, Nathan and Tan, Christelle and Lin, Anthony and Brown, Tres and Engelbosch, Susan and Anstrom, Kevin and Elish, Madeleine Clare and Heller, Katherine and Donohoe, Rebecca and Theiling, Jason and Poon, Eric and Balu, Suresh and Bedoya, Armando and O'Brien, Cara}, abstract = {Background: Successful integrations of machine learning into routine clinical care are exceedingly rare, and barriers to its adoption are poorly characterized in the literature. Objective: This study aims to report a quality improvement effort to integrate a deep learning sepsis detection and management platform, Sepsis Watch, into routine clinical care. Methods: In 2016, a multidisciplinary team consisting of statisticians, data scientists, data engineers, and clinicians was assembled by the leadership of an academic health system to radically improve the detection and treatment of sepsis. This report of the quality improvement effort follows the learning health system framework to describe the problem assessment, design, development, implementation, and evaluation plan of Sepsis Watch. Results: Sepsis Watch was successfully integrated into routine clinical care and reshaped how local machine learning projects are executed. Frontline clinical staff were highly engaged in the design and development of the workflow, machine learning model, and application. Novel machine learning methods were developed to detect sepsis early, and implementation of the model required robust infrastructure. Significant investment was required to align stakeholders, develop trusting relationships, define roles and responsibilities, and to train frontline staff, leading to the establishment of 3 partnerships with internal and external research groups to evaluate Sepsis Watch. Conclusions: Machine learning models are commonly developed to enhance clinical decision making, but successful integrations of machine learning into routine clinical care are rare. Although there is no playbook for integrating deep learning into clinical care, learnings from the Sepsis Watch integration can inform efforts to develop machine learning technologies at other health care delivery systems.} }
@article{WOS:000656708300001, title = {Organizing workers and machine learning tools for a less oppressive workplace}, journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT}, volume = {59}, year = {2021}, issn = {0268-4012}, doi = {10.1016/j.ijinfomgt.2021.102353}, author = {Young, Amber Grace and Majchrzak, Ann and Kane, Gerald C.}, abstract = {Machine learning tools are increasingly infiltrating everyday work life with implications for workers. By looking at machine learning tools as part of a sociotechnical system, we explore how machine learning tools enforce oppression of workers. We theorize, normatively, that with reorganizing processes in place, oppressive characteristics could be converted to emancipatory characteristics. Drawing on Paulo Freire's critical theory of emancipatory pedagogy, we outline similarities between the characteristics Freire saw in oppressive societies and the characteristics of currently designed partnerships between humans and machine learning tools. Freire's theory offers a way forward in reorganizing humans and machine learning tools in the workplace. Rather than advocating human control or the decoupling of workers and machines, we follow Freire's theory in proposing four processes for emancipatory organizing of human and machine learning partnership: 1) awakening of a critical consciousness, 2) enabling role freedom, 3) instituting incentives and sanctions for accountability, and 4) identifying alternative emancipatory futures. Theoretical and practical implications of this emancipatory organizing theory are drawn.} }
@article{WOS:001139837400001, title = {Preserving data privacy in machine learning systems}, journal = {COMPUTERS \\& SECURITY}, volume = {137}, year = {2024}, issn = {0167-4048}, doi = {10.1016/j.cose.2023.103605}, author = {El Mestari, Soumia Zohra and Lenzini, Gabriele and Demirci, Huseyin}, abstract = {The wide adoption of Machine Learning to solve a large set of real-life problems came with the need to collect and process large volumes of data, some of which are considered personal and sensitive, raising serious concerns about data protection. Privacy-enhancing technologies (PETs) are often indicated as a solution to protect personal data and to achieve a general trustworthiness as required by current EU regulations on data protection and AI. However, an off-the-shelf application of PETs is insufficient to ensure a high-quality of data protection, which one needs to understand. This work systematically discusses the risks against data protection in modern Machine Learning systems taking the original perspective of the data owners, who are those who hold the various data sets, data models, or both, throughout the machine learning life cycle and considering the different Machine Learning architectures. It argues that the origin of the threats, the risks against the data, and the level of protection offered by PETs depend on the data processing phase, the role of the parties involved, and the architecture where the machine learning systems are deployed. By offering a framework in which to discuss privacy and confidentiality risks for data owners and by identifying and assessing privacy-preserving countermeasures for machine learning, this work could facilitate the discussion about compliance with EU regulations and directives.We discuss current challenges and research questions that are still unsolved in the field. In this respect, this paper provides researchers and developers working on machine learning with a comprehensive body of knowledge to let them advance in the science of data protection in machine learning field as well as in closely related fields such as Artificial Intelligence.} }
@article{WOS:001013949600001, title = {A Taxonomic Survey of Physics-Informed Machine Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app13126892}, author = {Pateras, Joseph and Rana, Pratip and Ghosh, Preetam}, abstract = {Physics-informed machine learning (PIML) refers to the emerging area of extracting physically relevant solutions to complex multiscale modeling problems lacking sufficient quantity and veracity of data with learning models informed by physically relevant prior information. This work discusses the recent critical advancements in the PIML domain. Novel methods and applications of domain decomposition in physics-informed neural networks (PINNs) in particular are highlighted. Additionally, we explore recent works toward utilizing neural operator learning to intuit relationships in physics systems traditionally modeled by sets of complex governing equations and solved with expensive differentiation techniques. Finally, expansive applications of traditional physics-informed machine learning and potential limitations are discussed. In addition to summarizing recent work, we propose a novel taxonomic structure to catalog physics-informed machine learning based on how the physics-information is derived and injected into the machine learning process. The taxonomy assumes the explicit objectives of facilitating interdisciplinary collaboration in methodology, thereby promoting a wider characterization of what types of physics problems are served by the physics-informed learning machines and assisting in identifying suitable targets for future work. To summarize, the major twofold goal of this work is to summarize recent advancements and introduce a taxonomic catalog for applications of physics-informed machine learning.} }
@article{WOS:001155835100001, title = {Machine learning for power transformer SFRA based fault detection}, journal = {INTERNATIONAL JOURNAL OF ELECTRICAL POWER \\& ENERGY SYSTEMS}, volume = {156}, year = {2024}, issn = {0142-0615}, doi = {10.1016/j.ijepes.2023.109779}, author = {Bjelic, Milos and Brkovic, Bogdan and Zarkovic, Mileta and Miljkovic, Tatjana}, abstract = {This paper presents machine learning methods for health assessment of power transformer based on sweep frequency response analysis. The paper presents an overview of monitoring and diagnostics based on statistical Sweep Frequency Response Analysis (SFRA) based indicators that are used to evaluate the state of the power transformer. Experimental data obtained from power transformers with internal short-circuit faults is used as a database for applying machine learning. Machine learning is implemented to achieve more precise asset management and condition-based maintenance. Unsupervised machine learning was applied through the k-means cluster method for classifying and dividing the examined power transformer state into groups with similar state and probability of failure. Artificial neural network (ANN) and Adaptive Neuro Fuzzy Inference System (ANFIS) as part of supervised machine learning are created in order to detect fault severity in tested power transformers of different lifetime. The presented machine learning methods can be used to improve health assessment of power transformers.} }
@article{WOS:000884211300001, title = {Relative Valuation with Machine Learning}, journal = {JOURNAL OF ACCOUNTING RESEARCH}, volume = {61}, pages = {329-376}, year = {2023}, issn = {0021-8456}, doi = {10.1111/1475-679X.12464}, author = {Geertsema, Paul and Lu, Helen}, abstract = {We use machine learning for relative valuation and peer firm selection. In out-of-sample tests, our machine learning models substantially outperform traditional models in valuation accuracy. This outperformance persists over time and holds across different types of firms. The valuations produced by machine learning models behave like fundamental values. Overvalued stocks decrease in price and undervalued stocks increase in price in the following month. Determinants of valuation multiples identified by machine learning models are consistent with theoretical predictions derived from a discounted cash flow approach. Profitability ratios, growth measures, and efficiency ratios are the most important value drivers throughout our sample period. We derive a novel method to express valuation multiples predicted by our machine learning models as weighted averages of peer firm multiples. These weights are a measure of peer-firm comparability and can be used for selecting peer-groups.} }
@article{WOS:001415042200001, title = {Applications of machine learning methods for design and characterization of high-performance fiber-reinforced cementitious composite (HPFRCC): a review}, journal = {JOURNAL OF SUSTAINABLE CEMENT-BASED MATERIALS}, volume = {14}, pages = {1726-1749}, year = {2025}, issn = {2165-0373}, doi = {10.1080/21650373.2025.2462183}, author = {Guo, Pengwei and Moghaddas, Seyed A. and Liu, Yiming and Meng, Weina and Li, Victor C. and Bao, Yi}, abstract = {High-Performance Fiber-Reinforced Cementitious Composite (HPFRCC) represents a family of advanced composite materials with remarkable mechanical properties and durability, but their design and characterization tasks involve unique challenges. Recently, advancements in machine learning techniques have offered new opportunities. This paper reviews the application of machine learning techniques in the design and characterization of HPFRCC. The application of machine learning to the design of HPFRCC is reviewed based on a prediction-optimization framework, and the steps for property prediction and material design considering fresh properties, cracks, and microstructures are elaborated. The latest development of knowledge-guided machine learning approach is discussed. The application of machine learning to the characterization of HPFRCC is reviewed, and the computer vision and deep learning techniques for characterizing HPFRCC are elaborated. The challenges and opportunities for the applications of machine learning methods are discussed, aiming to facilitate applications of machine learning techniques for HPFRCC.} }
@article{WOS:000697563900010, title = {When machine learning meets congestion control: A survey and comparison}, journal = {COMPUTER NETWORKS}, volume = {192}, year = {2021}, issn = {1389-1286}, doi = {10.1016/j.comnet.2021.108033}, author = {Jiang, Huiling and Li, Qing and Jiang, Yong and Shen, GengBiao and Sinnott, Richard and Tian, Chen and Xu, Mingwei}, abstract = {Machine learning has seen a significant surge and uptake across many diverse applications. The high flexibility, adaptability, and computing capabilities it provides extend traditional approaches used in multiple fields including network operation and management. Numerous surveys have explored machine learning algorithms in the context of networking, such as traffic engineering, performance optimization, and network security. Many machine learning approaches focus on clustering, classification, regression, and reinforcement learning. The innovation of this research, and the contribution of this paper lies in the detailed summary and comparison of learning-based congestion control approaches. Compared with traditional congestion control algorithms which are typically rule-based, capabilities to learn from historical experience are highly desirable. From the literature, it is observed that reinforcement learning is a crucial trend among learning-based congestion control algorithms. In this paper, we explore the performance of reinforcement learning-based congestion control algorithms and present current problems with reinforcement learning-based congestion control algorithms. Moreover, we outline challenges and trends related to learning-based congestion control algorithms.} }
@article{WOS:000397748100040, title = {Machine Learning Techniques for Optical Performance Monitoring From Directly Detected PDM-QAM Signals}, journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY}, volume = {35}, pages = {868-875}, year = {2017}, issn = {0733-8724}, doi = {10.1109/JLT.2016.2590989}, author = {Thrane, Jakob and Wass, Jesper and Piels, Molly and Diniz, Julio C. M. and Jones, Rasmus and Zibar, Darko}, abstract = {Linear signal processing algorithms areeffective in dealing with linear transmission channel and linear signal detection, whereas the nonlinear signal processing algorithms, from the machine learning community, are effective in dealing with nonlinear transmission channel and nonlinear signal detection. In this paper, a brief overview of the various machine learning methods and their application in optical communication is presented and discussed. Moreover, supervised machine learning methods, such as neural networks and support vector machine, are experimentally demonstrated for in-band optical signal to noise ratio estimation and modulation format classification, respectively. The proposed methods accurately evaluate optical signals employing up to 64 quadrature amplitude modulation, at 32 Gbd, using only directly detected data.} }
@article{WOS:000595291200006, title = {Machine learning for halide perovskite materials}, journal = {NANO ENERGY}, volume = {78}, year = {2020}, issn = {2211-2855}, doi = {10.1016/j.nanoen.2020.105380}, author = {Zhang, Lei and He, Mu and Shao, Shaofeng}, abstract = {Halide perovskite materials serve as excellent candidates for solar cell and optoelectronic devices. Recently, the design of the halide perovskite materials is greatly facilitated by machine learning techniques, which effectively identify suitable halide perovskite candidates and unveil hidden relationships by algorithms that mimic the human cognitive functions. In this manuscript, we review recent progresses on the machine learning studies of the halide perovskite materials, including the prediction and understanding of lead-free and stable halide perovskite materials. The structural descriptors to describe the property and performance of the halide perovskite materials are discussed. In addition, the design strategy of the additive species for the halide perovskite materials via the machine learning technique is provided. Suggestions to further develop the halide perovskite-based systems via the machine learning methods in the future are provided.} }
@article{WOS:000356700900001, title = {Deep Extreme Learning Machine and Its Application in EEG Classification}, journal = {MATHEMATICAL PROBLEMS IN ENGINEERING}, volume = {2015}, year = {2015}, issn = {1024-123X}, doi = {10.1155/2015/129021}, author = {Ding, Shifei and Zhang, Nan and Xu, Xinzheng and Guo, Lili and Zhang, Jian}, abstract = {Recently, deep learning has aroused wide interest in machine learning fields. Deep learning is a multilayer perceptron artificial neural network algorithm. Deep learning has the advantage of approximating the complicated function and alleviating the optimization difficulty associated with deep models. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network which takes advantages of deep learning and extreme learning machine. Not only does MLELM approximate the complicated function but it also does not need to iterate during the training process. We combining with MLELM and extreme learning machine with kernel (KELM) put forward deep extreme learning machine (DELM) and apply it to EEG classification in this paper. This paper focuses on the application of DELM in the classification of the visual feedback experiment, using MATLAB and the second brain-computer interface (BCI) competition datasets. By simulating and analyzing the results of the experiments, effectiveness of the application of DELM in EEG classification is confirmed.} }
@article{WOS:001045762300001, title = {Multiple stakeholders drive diverse interpretability requirements for machine learning in healthcare}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {5}, pages = {824-829}, year = {2023}, doi = {10.1038/s42256-023-00698-2}, author = {Imrie, Fergus and Davis, Robert and van der Schaar, Mihaela}, abstract = {Limited interpretability and understanding of machine learning methods in healthcare hinder their clinical impact. Imrie et al. discuss five types of machine learning interpretability. They examine medical stakeholders, highlight how interpretability meets their needs and emphasize the role of tailored interpretability in linking machine learning advancements to clinical impact. Applications of machine learning are becoming increasingly common in medicine and healthcare, enabling more accurate predictive models. However, this often comes at the cost of interpretability, limiting the clinical impact of machine learning methods. To realize the potential of machine learning in healthcare, it is critical to understand such models from the perspective of multiple stakeholders and various angles, necessitating different types of explanation. In this Perspective, we explore five fundamentally different types of post-hoc machine learning interpretability. We highlight the different types of information that they provide, and describe when each can be useful. We examine the various stakeholders in healthcare, delving into their specific objectives, requirements and goals. We discuss how current notions of interpretability can help meet these and what is required for each stakeholder to make machine learning models clinically impactful. Finally, to facilitate adoption, we release an open-source interpretability library containing implementations of the different types of interpretability, including tools for visualizing and exploring the explanations.} }
@article{WOS:000505074700001, title = {Artificial intelligence, machine learning and the pediatric airway}, journal = {PEDIATRIC ANESTHESIA}, volume = {30}, pages = {264-268}, year = {2020}, issn = {1155-5645}, doi = {10.1111/pan.13792}, author = {Matava, Clyde and Pankiv, Evelina and Ahumada, Luis and Weingarten, Benjamin and Simpao, Allan}, abstract = {Artificial intelligence and machine learning are rapidly expanding fields with increasing relevance in anesthesia and, in particular, airway management. The ability of artificial intelligence and machine learning algorithms to recognize patterns from large volumes of complex data makes them attractive for use in pediatric anesthesia airway management. The purpose of this review is to introduce artificial intelligence, machine learning, and deep learning to the pediatric anesthesiologist. Current evidence and developments in artificial intelligence, machine learning, and deep learning relevant to pediatric airway management are presented. We critically assess the current evidence on the use of artificial intelligence and machine learning in the assessment, diagnosis, monitoring, procedure assistance, and predicting outcomes during pediatric airway management. Further, we discuss the limitations of these technologies and offer areas for focused research that may bring pediatric airway management anesthesiology into the era of artificial intelligence and machine learning.} }
@article{WOS:000678606100001, title = {Optimizing Predictive Maintenance With Machine Learning for Reliability Improvement}, journal = {ASCE-ASME JOURNAL OF RISK AND UNCERTAINTY IN ENGINEERING SYSTEMS PART B-MECHANICAL ENGINEERING}, volume = {7}, year = {2021}, issn = {2332-9017}, doi = {10.1115/1.4049525}, author = {Ren, Yali}, abstract = {Predictive maintenance, as a form of pro-active maintenance, has increasing usage and shows significant superiority over the corrective and preventive maintenance. However, conventional methods of predictive maintenance have noteworthy limitations in maintenance optimization and reliability improvement. In the last two decades, machine learning has flourished and overcome many inherent flaws of conventional maintenance prediction methods. Meanwhile, machine learning displays unprecedented predictive power in maintenance prediction and optimization. This paper compares the features of corrective, preventive, and predictive maintenance, examines the conventional approaches to predictive maintenance, and analyzes their drawbacks. Subsequently, this paper explores the driving forces, and advantages of machine learning over conventional solutions in predictive maintenance. Specifically, this paper reviews popular supervised learning and reinforcement learning algorithms and the associated typical applications in predictive maintenance. Furthermore, this paper summarizes the four critical steps of machine learning applications in maintenance prediction. Finally, the author proposes the future researches concerning how to utilize machine learning to optimize maintenance prediction and planning, improve equipment reliability, and achieve the best possible benefit.} }
@article{WOS:000532109400001, title = {Interactive machine teaching: a human-centered approach to building machine-learned models}, journal = {HUMAN-COMPUTER INTERACTION}, volume = {35}, pages = {413-451}, year = {2020}, issn = {0737-0024}, doi = {10.1080/07370024.2020.1734931}, author = {Ramos, Gonzalo and Meek, Christopher and Simard, Patrice and Suh, Jina and Ghorashi, Soroush}, abstract = {Modern systems can augment people's capabilities by using machine-learned models to surface intelligent behaviors. Unfortunately, building these models remains challenging and beyond the reach of non-machine learning experts. We describe interactive machine teaching (IMT) and its potential to simplify the creation of machine-learned models. One of the key characteristics of IMT is its iterative process in which the human-in-the-loop takes the role of a teacher teaching a machine how to perform a task. We explore alternative learning theories as potential theoretical foundations for IMT, the intrinsic human capabilities related to teaching, and how IMT systems might leverage them. We argue that IMT processes that enable people to leverage these capabilities have a variety of benefits, including making machine learning methods accessible to subject-matter experts and the creation of semantic and debuggable machine learning (ML) models. We present an integrated teaching environment (ITE) that embodies principles from IMT, and use it as a design probe to observe how non-ML experts do IMT and as the basis of a system that helps us study how to guide teachers. We explore and highlight the benefits and challenges of IMT systems. We conclude by outlining six research challenges to advance the field of IMT.} }
@article{WOS:000691220800003, title = {Reproducibility standards for machine learning in the life sciences}, journal = {NATURE METHODS}, volume = {18}, pages = {1132-1135}, year = {2021}, issn = {1548-7091}, doi = {10.1038/s41592-021-01256-7}, author = {Heil, Benjamin J. and Hoffman, Michael M. and Markowetz, Florian and Lee, Su-In and Greene, Casey S. and Hicks, Stephanie C.}, abstract = {To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.} }
@article{WOS:000563833500002, title = {Loops, ladders and links: the recursivity of social and machine learning}, journal = {THEORY AND SOCIETY}, volume = {49}, pages = {803-832}, year = {2020}, issn = {0304-2421}, doi = {10.1007/s11186-020-09409-x}, author = {Fourcade, Marion and Johns, Fleur}, abstract = {Machine learning algorithms reshape how people communicate, exchange, and associate; how institutions sort them and slot them into social positions; and how they experience life, down to the most ordinary and intimate aspects. In this article, we draw on examples from the field of social media to review the commonalities, interactions, and contradictions between the dispositions of people and those of machines as they learn from and make sense of each other.} }
@article{WOS:000663500200003, title = {A survey on deep learning and its applications}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100379}, author = {Dong, Shi and Wang, Ping and Abbas, Khushnood}, abstract = {Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal-artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning. (C) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000413244600013, title = {Non-Intrusive Load Monitoring Using Semi-Supervised Machine Learning and Wavelet Design}, journal = {IEEE TRANSACTIONS ON SMART GRID}, volume = {8}, pages = {2648-2655}, year = {2017}, issn = {1949-3053}, doi = {10.1109/TSG.2016.2532885}, author = {Gillis, Jessie M. and Morsi, Walid G.}, abstract = {This paper presents a new approach based on semi-supervised machine learning and wavelet design applied to non-intrusive load monitoring. Co-training of two machine learning classifiers is used to automate the process of learning the load pattern after designing new wavelets. The numerical results demonstrating the effectiveness of the proposed approach are discussed and conclusions are drawn.} }
@article{WOS:000837752300005, title = {The statistical analysis in the era of big data}, journal = {INTERNATIONAL JOURNAL OF MODELLING IDENTIFICATION AND CONTROL}, volume = {40}, pages = {151-157}, year = {2022}, issn = {1746-6172}, doi = {10.1504/IJMIC.2022.124718}, author = {Wang, Zelin and Liu, Xinke and Zhang, Weiye and Zhi, Yingying and Cheng, Shi}, abstract = {In the big data environment, the traditional machine learning algorithm for data processing is somewhat inadequate. Therefore, machine learning algorithms adapted to big data environment have become a research hotspot. At the time of the marriage of big data and machine learning, it is necessary to predict the related challenges and opportunities. This paper mainly analyses and summarises the current research status of machine learning algorithms for processing big data, and discusses the new opportunities and challenges that machine learning paradigm will face in the era of big data. It also explores the new technology breakthrough that machine learning will produce in the era of big data.} }
@article{WOS:000576604100013, title = {Recent advances on constraint-based models by integrating machine learning}, journal = {CURRENT OPINION IN BIOTECHNOLOGY}, volume = {64}, pages = {85-91}, year = {2020}, issn = {0958-1669}, doi = {10.1016/j.copbio.2019.11.007}, author = {Rana, Pratip and Berry, Carter and Ghosh, Preetam and Fong, Stephen S.}, abstract = {Research that meaningfully integrates constraint-based modeling with machine learning is at its infancy but holds much promise. Here, we consider where machine learning has been implemented within the constraint-based modeling reconstruction framework and highlight the need to develop approaches that can identify meaningful features from large-scale data and connect them to biological mechanisms to establish causality to connect genotype to phenotype. We motivate the construction of iterative integrative schemes where machine learning can fine-tune the input constraints in a constraint-based model or contrarily, constraint-based model simulation results are analyzed by machine learning and reconciled with experimental data. This can iteratively refine a constraint-based model until there is consistency between experimental data, machine learning results, and constraint-based model simulations.} }
@article{WOS:000518683500002, title = {The rise of machine learning for detection and classification of malware: Research developments, trends and challenges}, journal = {JOURNAL OF NETWORK AND COMPUTER APPLICATIONS}, volume = {153}, year = {2020}, issn = {1084-8045}, doi = {10.1016/j.jnca.2019.102526}, author = {Gibert, Daniel and Mateu, Carles and Planes, Jordi}, abstract = {The struggle between security analysts and malware developers is a never-ending battle with the complexity of malware changing as quickly as innovation grows. Current state-of-the-art research focus on the development and application of machine learning techniques for malware detection due to its ability to keep pace with malware evolution. This survey aims at providing a systematic and detailed overview of machine learning techniques for malware detection and in particular, deep learning techniques. The main contributions of the paper are: (1) it provides a complete description of the methods and features in a traditional machine learning workflow for malware detection and classification, (2) it explores the challenges and limitations of traditional machine learning and (3) it analyzes recent trends and developments in the field with special emphasis on deep learning approaches. Furthermore, (4) it presents the research issues and unsolved challenges of the state-of-the-art techniques and (5) it discusses the new directions of research. The survey helps researchers to have an understanding of the malware detection field and of the new developments and directions of research explored by the scientific community to tackle the problem.} }
@article{WOS:000656417100004, title = {A review on deep learning in machining and tool monitoring: methods, opportunities, and challenges}, journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY}, volume = {115}, pages = {2683-2709}, year = {2021}, issn = {0268-3768}, doi = {10.1007/s00170-021-07325-7}, author = {Nasir, Vahid and Sassani, Farrokh}, abstract = {Data-driven methods provided smart manufacturing with unprecedented opportunities to facilitate the transition toward Industry 4.0-based production. Machine learning and deep learning play a critical role in developing intelligent systems for descriptive, diagnostic, and predictive analytics for machine tools and process health monitoring. This paper reviews the opportunities and challenges of deep learning (DL) for intelligent machining and tool monitoring. The components of an intelligent monitoring framework are introduced. The main advantages and disadvantages of machine learning (ML) models are presented and compared with those of deep models. The main DL models, including autoencoders, deep belief networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs), were discussed, and their applications in intelligent machining and tool condition monitoring were reviewed. The opportunities of data-driven smart manufacturing approach applied to intelligent machining were discussed to be (1) automated feature engineering, (2) handling big data, (3) handling high-dimensional data, (4) avoiding sensor redundancy, (5) optimal sensor fusion, and (6) offering hybrid intelligent models. Finally, the data-driven challenges in smart manufacturing, including the challenges associated with the data size, data nature, model selection, and process uncertainty, were discussed, and the research gaps were outlined.} }
@article{WOS:000484532300003, title = {Spatial extreme learning machines: An application on prediction of disease counts}, journal = {STATISTICAL METHODS IN MEDICAL RESEARCH}, volume = {28}, pages = {2583-2594}, year = {2019}, issn = {0962-2802}, doi = {10.1177/0962280218767985}, author = {Prates, Marcos O.}, abstract = {Extreme learning machines have gained a lot of attention by the machine learning community because of its interesting properties and computational advantages. With the increase in collection of information nowadays, many sources of data have missing information making statistical analysis harder or unfeasible. In this paper, we present a new model, coined spatial extreme learning machine, that combine spatial modeling with extreme learning machines keeping the nice properties of both methodologies and making it very flexible and robust. As explained throughout the text, the spatial extreme learning machines have many advantages in comparison with the traditional extreme learning machines. By a simulation study and a real data analysis we present how the spatial extreme learning machine can be used to improve imputation of missing data and uncertainty prediction estimation.} }
@article{WOS:001372855700001, title = {Applications of and issues with machine learning in medicine: Bridging the gap with explainable AI}, journal = {BIOSCIENCE TRENDS}, volume = {18}, pages = {497-504}, year = {2024}, issn = {1881-7815}, doi = {10.5582/bst.2024.01342}, author = {Karako, Kenji and Tang, Wei}, abstract = {In recent years, machine learning, and particularly deep learning, has shown remarkable potential in various fields, including medicine. Advanced techniques like convolutional neural networks and transformers have enabled high-performance predictions for complex problems, making machine learning a valuable tool in medical decision-making. From predicting postoperative complications to assessing disease risk, machine learning has been actively used to analyze patient data and assist healthcare professionals. However, the ``black box'' problem, wherein the internal workings of machine learning models are opaque and difficult to interpret, poses a significant challenge in medical applications. The lack of transparency may hinder trust and acceptance by clinicians and patients, making the development of explainable AI (XAI) techniques essential. XAI aims to provide both global and local explanations for machine learning models, offering insights into how predictions are made and which factors influence these outcomes. In this article, we explore various applications of machine learning in medicine, describe commonly used algorithms, and discuss explainable AI as a promising solution to enhance the interpretability of these models. By integrating explainability into machine learning, we aim to ensure its ethical and practical application in healthcare, ultimately improving patient outcomes and supporting personalized treatment strategies.} }
@article{WOS:000598812600004, title = {Machine learning for geographically differentiated climate change mitigation in urban areas}, journal = {SUSTAINABLE CITIES AND SOCIETY}, volume = {64}, year = {2021}, issn = {2210-6707}, doi = {10.1016/j.scs.2020.102526}, author = {Milojevic-Dupont, Nikola and Creutzig, Felix}, abstract = {Artificial intelligence and machine learning are transforming scientific disciplines, but their full potential for climate change mitigation remains elusive. Here, we conduct a systematic review of applied machine learning studies that are of relevance for climate change mitigation, focusing specifically on the fields of remote sensing, urban transportation, and buildings. The relevant body of literature spans twenty years and is growing exponentially. We show that the emergence of big data and machine learning methods enables climate solution research to overcome generic recommendations and provide policy solutions at urban, street, building and household scale, adapted to specific contexts, but scalable to global mitigation potentials. We suggest a meta-algorithmic architecture and framework for using machine learning to optimize urban planning for accelerating, improving and transforming urban infrastructure provision.} }
@article{WOS:000466837400054, title = {Machine learning as a supportive tool to recognize cardiac arrest in emergency calls}, journal = {RESUSCITATION}, volume = {138}, pages = {322-329}, year = {2019}, issn = {0300-9572}, doi = {10.1016/j.resuscitation.2019.01.015}, author = {Blomberg, Stig Nikolaj and Folke, Fredrik and Ersboll, Annette Kjaer and Christensen, Helle Collatz and Torp-Pedersen, Christian and Sayre, Michael R. and Counts, Catherine R. and Lippert, Freddy K.}, abstract = {Background: Emergency medical dispatchers fail to identify approximately 25\\% of cases of out of hospital cardiac arrest, thus lose the opportunity to provide the caller instructions in cardiopulmonary resuscitation. We examined whether a machine learning framework could recognize out-of-hospital cardiac arrest from audio files of calls to the emergency medical dispatch center. Methods: For all incidents responded to by Emergency Medical Dispatch Center Copenhagen in 2014, the associated call was retrieved. A machine learning framework was trained to recognize cardiac arrest from the recorded calls. Sensitivity, specificity, and positive predictive value for recognizing out-of-hospital cardiac arrest were calculated. The performance of the machine learning framework was compared to the actual recognition and time-to-recognition of cardiac arrest by medical dispatchers. Results: We examined 108,607 emergency calls, of which 918 (0.8\\%) were out-of-hospital cardiac arrest calls eligible for analysis. Compared with medical dispatchers, the machine learning framework had a significantly higher sensitivity (72.5\\% vs. 84.1\\%, p < 0.001) with lower specificity (98.8\\% vs. 97.3\\%, p < 0.001). The machine learning framework had a lower positive predictive value than dispatchers (20.9\\% vs. 33.0\\%, p < 0.001). Time-to-recognition was significantly shorter for the machine learning framework compared to the dispatchers (median 44 seconds vs. 54 s, p < 0.001). Conclusions: A machine learning framework performed better than emergency medical dispatchers for identifying out-of-hospital cardiac arrest in emergency phone calls. Machine learning may play an important role as a decision support tool for emergency medical dispatchers.} }
@article{WOS:000540241000001, title = {Building machine learning models without sharing patient data: A simulation-based analysis of distributed learning by ensembling}, journal = {JOURNAL OF BIOMEDICAL INFORMATICS}, volume = {106}, year = {2020}, issn = {1532-0464}, doi = {10.1016/j.jbi.2020.103424}, author = {Tuladhar, Anup and Gill, Sascha and Ismail, Zahinoor and Forkert, Nils D. and Alzheimers Dis Neuroimaging Initia}, abstract = {The development of machine learning solutions in medicine is often hindered by difficulties associated with sharing patient data. Distributed learning aims to train machine learning models locally without requiring data sharing. However, the utility of distributed learning for rare diseases, with only a few training examples at each contributing local center, has not been investigated. The aim of this work was to simulate distributed learning models by ensembling with artificial neural networks (ANN), support vector machines (SVM), and random forests (RF) and evaluate them using four medical datasets. Distributed learning by ensembling locally trained agents improved performance compared to models trained using the data from a single institution, even in cases where only a very few training examples are available per local center. Distributed learning improved when more locally trained models were added to the ensemble. Local class imbalance reduced distributed SVM performance but did not impact distributed RF and ANN classification. Our results suggest that distributed learning by ensembling can be used to train machine learning models without sharing patient data and is suitable to use with small datasets.} }
@article{WOS:000729048700005, title = {Machine Learning in Healthcare}, journal = {CURRENT GENOMICS}, volume = {22}, pages = {291-300}, year = {2021}, issn = {1389-2029}, doi = {10.2174/1389202922666210705124359}, author = {Habehh, Hafsa and Gohel, Suril}, abstract = {Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technol-ogy have brought on substantial strides in predicting and identifying health emergencies, disease populations, and disease state and immune response, amongst a few. Although, skepticism remains regarding the practical application and interpretation of results from ML-based approaches in healthcare settings, the inclusion of these approaches is increasing at a rapid pace. Here we provide a brief overview of machine learning-based approaches and learning algorithms including super -vised, unsupervised, and reinforcement learning along with examples. Second, we discuss the appli-cation of ML in several healthcare fields, including radiology, genetics, electronic health records, and neuroimaging. We also briefly discuss the risks and challenges of ML application to healthcare such as system privacy and ethical concerns and provide suggestions for future applications.} }
@article{WOS:000689068400001, title = {Deep Learning Aided Data-Driven Fault Diagnosis of Rotatory Machine: A Comprehensive Review}, journal = {ENERGIES}, volume = {14}, year = {2021}, doi = {10.3390/en14165150}, author = {Mushtaq, Shiza and Islam, M. M. Manjurul and Sohaib, Muhammad}, abstract = {This paper presents a comprehensive review of the developments made in rotating bearing fault diagnosis, a crucial component of a rotatory machine, during the past decade. A data-driven fault diagnosis framework consists of data acquisition, feature extraction/feature learning, and decision making based on shallow/deep learning algorithms. In this review paper, various signal processing techniques, classical machine learning approaches, and deep learning algorithms used for bearing fault diagnosis have been discussed. Moreover, highlights of the available public datasets that have been widely used in bearing fault diagnosis experiments, such as Case Western Reserve University (CWRU), Paderborn University Bearing, PRONOSTIA, and Intelligent Maintenance Systems (IMS), are discussed in this paper. A comparison of machine learning techniques, such as support vector machines, k-nearest neighbors, artificial neural networks, etc., deep learning algorithms such as a deep convolutional network (CNN), auto-encoder-based deep neural network (AE-DNN), deep belief network (DBN), deep recurrent neural network (RNN), and other deep learning methods that have been utilized for the diagnosis of rotary machines bearing fault, is presented.} }
@article{WOS:000999504400002, title = {Machine Learning Methods in Solving the Boolean Satisfiability Problem}, journal = {MACHINE INTELLIGENCE RESEARCH}, volume = {20}, pages = {640-655}, year = {2023}, issn = {2731-538X}, doi = {10.1007/s11633-022-1396-2}, author = {Guo, Wenxuan and Zhen, Hui-Ling and Li, Xijun and Luo, Wanqian and Yuan, Mingxuan and Jin, Yaohui and Yan, Junchi}, abstract = {This paper reviews the recent literature on solving the Boolean satisfiability problem (SAT), an archetypal NP-complete problem, with the aid of machine learning (ML) techniques. Over the last decade, the machine learning society advances rapidly and surpasses human performance on several tasks. This trend also inspires a number of works that apply machine learning methods for SAT solving. In this survey, we examine the evolving ML SAT solvers from naive classifiers with handcrafted features to emerging end-to-end SAT solvers, as well as recent progress on combinations of existing conflict-driven clause learning (CDCL) and local search solvers with machine learning methods. Overall, solving SAT with machine learning is a promising yet challenging research topic. We conclude the limitations of current works and suggest possible future directions. The collected paper list is available at https://github.com/ThinklabSJTU/awesome-ml4co.} }
@article{WOS:000363458900023, title = {Does machine learning need fuzzy logic?}, journal = {FUZZY SETS AND SYSTEMS}, volume = {281}, pages = {292-299}, year = {2015}, issn = {0165-0114}, doi = {10.1016/j.fss.2015.09.001}, author = {Huellermeier, Eyke}, abstract = {This article is a short position paper in which the author outlines his (necessarily subjective) perception of current research in fuzzy machine learning, that is, the use of formal concepts and mathematical tools from fuzzy sets and fuzzy logic in the field of machine learning. The paper starts with a critical appraisal of previous contributions to fuzzy machine learning and ends with a suggestion of some directions for future work. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000979995400004, title = {Uncovering expression signatures of synergistic drug responses via ensembles of explainable machine-learning models}, journal = {NATURE BIOMEDICAL ENGINEERING}, volume = {7}, pages = {811+}, year = {2023}, issn = {2157-846X}, doi = {10.1038/s41551-023-01034-0}, author = {Janizek, Joseph D. and Dincer, Ayse B. and Celik, Safiye and Chen, Hugh and Chen, William and Naxerova, Kamila and Lee, Su-In}, abstract = {Ensembles of explainable machine-learning models increase the quality of explanations for the molecular basis of synergetic drug combinations, as shown for the treatment of acute myeloid leukaemia. Machine learning may aid the choice of optimal combinations of anticancer drugs by explaining the molecular basis of their synergy. By combining accurate models with interpretable insights, explainable machine learning promises to accelerate data-driven cancer pharmacology. However, owing to the highly correlated and high-dimensional nature of transcriptomic data, naively applying current explainable machine-learning strategies to large transcriptomic datasets leads to suboptimal outcomes. Here by using feature attribution methods, we show that the quality of the explanations can be increased by leveraging ensembles of explainable machine-learning models. We applied the approach to a dataset of 133 combinations of 46 anticancer drugs tested in ex vivo tumour samples from 285 patients with acute myeloid leukaemia and uncovered a haematopoietic-differentiation signature underlying drug combinations with therapeutic synergy. Ensembles of machine-learning models trained to predict drug combination synergies on the basis of gene-expression data may improve the feature attribution quality of complex machine-learning models.} }
@article{WOS:000569734100001, title = {A Systematic Review of Defensive and Offensive Cybersecurity with Machine Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175811}, author = {Aiyanyo, Imatitikua D. and Samuel, Hamman and Lim, Heuiseok}, abstract = {This is a systematic review of over one hundred research papers about machine learning methods applied to defensive and offensive cybersecurity. In contrast to previous reviews, which focused on several fragments of research topics in this area, this paper systematically and comprehensively combines domain knowledge into a single review. Ultimately, this paper seeks to provide a base for researchers that wish to delve into the field of machine learning for cybersecurity. Our findings identify the frequently used machine learning methods within supervised, unsupervised, and semi-supervised machine learning, the most useful data sets for evaluating intrusion detection methods within supervised learning, and methods from machine learning that have shown promise in tackling various threats in defensive and offensive cybersecurity.} }
@article{WOS:000427574900001, title = {Deep Learning Applications in Medical Image Analysis}, journal = {IEEE ACCESS}, volume = {6}, pages = {9375-9389}, year = {2018}, issn = {2169-3536}, doi = {10.1109/ACCESS.2017.2788044}, author = {Ker, Justin and Wang, Lipo and Rao, Jai and Lim, Tchoyoson}, abstract = {The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.} }
@article{WOS:000573452600007, title = {Machine learning for microbial identification and antimicrobial susceptibility testing on MALDI-TOF mass spectra: a systematic review}, journal = {CLINICAL MICROBIOLOGY AND INFECTION}, volume = {26}, pages = {1310-1317}, year = {2020}, issn = {1198-743X}, doi = {10.1016/j.cmi.2020.03.014}, author = {Weis, V, C. and Jutzeler, C. R. and Borgwardt, K.}, abstract = {Background: The matrix assisted laser desorption/ionization and time-of-flight mass spectrometry (MALDI-TOF MS) technology has revolutionized the field of microbiology by facilitating precise and rapid species identification. Recently, machine learning techniques have been leveraged to maximally exploit the information contained in MALDI-TOF MS, with the ultimate goal to refine species identification and streamline antimicrobial resistance determination. Objectives: The aim was to systematically review and evaluate studies employing machine learning for the analysis of MALDI-TOF mass spectra. Data sources: Using PubMed/Medline, Scopus and Web of Science, we searched the existing literature for machine learning-supported applications of MALDI-TOF mass spectra for microbial species and antimicrobial susceptibility identification. Study eligibility criteria: Original research studies using machine learning to exploit MALDI-TOF mass spectra for microbial specie and antimicrobial susceptibility identification were included. Studies focusing on single proteins and peptides, case studies and review articles were excluded. Methods: A systematic review according to the PRISMA guidelines was performed and a quality assessment of the machine learning models conducted. Results: From the 36 studies that met our inclusion criteria, 27 employed machine learning for species identification and nine for antimicrobial susceptibility testing. Support Vector Machines, Genetic Algorithms, Artificial Neural Networks and Quick Classifiers were the most frequently used machine learning algorithms. The quality of the studies ranged between poor and very good. The majority of the studies reported how to interpret the predictors (88.89\\%) and suggested possible clinical applications of the developed algorithm (100\\%), but only four studies (11.11\\%) validated machine learning algorithms on external datasets. Conclusions: A growing number of studies utilize machine learning to optimize the analysis of MALDITOF mass spectra. This review, however, demonstrates that there are certain shortcomings of current machine learning-supported approaches that have to be addressed to make them widely available and incorporated them in the clinical routine. (C) 2020 The Authors. Published by Elsevier Ltd on behalf of European Society of Clinical Microbiology and Infectious Diseases.} }
@article{WOS:000530237100003, title = {Semantic Adversarial Deep Learning}, journal = {IEEE DESIGN \\& TEST}, volume = {37}, pages = {8-18}, year = {2020}, issn = {2168-2356}, doi = {10.1109/MDAT.2020.2968274}, author = {Seshia, Sanjit A. and Jha, Somesh and Dreossi, Tommaso}, abstract = {Adversarial examples have emerged as a key threat for machine-learning-based systems, especially the ones that employ deep neural networks. Unlike a large body of research in this area, this Keynote article accounts for the semantic, context, and specifications of the complete system with machine learning components in resource-constrained environments. -Muhammad Shafique, Technische Universitat Wien} }
@article{WOS:000988744600001, title = {A survey on machine learning based analysis of heterogeneous data in industrial automation}, journal = {COMPUTERS IN INDUSTRY}, volume = {149}, year = {2023}, issn = {0166-3615}, doi = {10.1016/j.compind.2023.103930}, author = {Kamm, Simon and Veekati, Sushma Sri and Mueller, Timo and Jazdi, Nasser and Weyrich, Michael}, abstract = {In many application domains data from different sources are increasingly available to thoroughly monitor and describe a system or device. Especially within the industrial automation domain, heterogeneous data and its analysis gain a lot of attention from research and industry, since it has the potential to improve or enable tasks like diagnostics, predictive maintenance, and condition monitoring. For data analysis, machine learning based approaches are mostly used in recent literature, as these algorithms allow us to learn complex correlations within the data. To analyze even heterogeneous data and gain benefits from it in an application, data from different sources need to be integrated, stored, and managed to apply machine learning algorithms. In a setting with heterogeneous data sources, the analysis algorithms should also be able to handle data source failures or newly added data sources. In addition, existing knowledge should be used to improve the machine learning based analysis or its training process. To find existing approaches for the machine learning based analysis of heterogeneous data in the industrial automation domain, this paper presents the result of a systematic literature review. The publications were reviewed, evaluated, and discussed concerning five requirements that are derived in this paper. We identified promising solutions and approaches and outlined open research challenges, which are not yet covered sufficiently in the literature.} }
@article{WOS:000595568500001, title = {Use of Machine Learning Approaches in Clinical Epidemiological Research of Diabetes}, journal = {CURRENT DIABETES REPORTS}, volume = {20}, year = {2020}, issn = {1534-4827}, doi = {10.1007/s11892-020-01353-5}, author = {Basu, Sanjay and Johnson, Karl T. and Berkowitz, Seth A.}, abstract = {Purpose of Review Machine learning approaches-which seek to predict outcomes or classify patient features by recognizing patterns in large datasets-are increasingly applied to clinical epidemiology research on diabetes. Given its novelty and emergence in fields outside of biomedical research, machine learning terminology, techniques, and research findings may be unfamiliar to diabetes researchers. Our aim was to present the use of machine learning approaches in an approachable way, drawing from clinical epidemiological research in diabetes published from 1 Jan 2017 to 1 June 2020. Recent Findings Machine learning approaches using tree-based learners-which produce decision trees to help guide clinical interventions-frequently have higher sensitivity and specificity than traditional regression models for risk prediction. Machine learning approaches using neural networking and ``deep learning'' can be applied to medical image data, particularly for the identification and staging of diabetic retinopathy and skin ulcers. Among the machine learning approaches reviewed, researchers identified new strategies to develop standard datasets for rigorous comparisons across older and newer approaches, methods to illustrate how a machine learner was treating underlying data, and approaches to improve the transparency of the machine learning process. Machine learning approaches have the potential to improve risk stratification and outcome prediction for clinical epidemiology applications. Achieving this potential would be facilitated by use of universal open-source datasets for fair comparisons. More work remains in the application of strategies to communicate how the machine learners are generating their predictions.} }
@article{WOS:000879006700002, title = {Machine learning for drilling applications: A review}, journal = {JOURNAL OF NATURAL GAS SCIENCE AND ENGINEERING}, volume = {108}, year = {2022}, issn = {1875-5100}, doi = {10.1016/j.jngse.2022.104807}, author = {Zhong, Ruizhi and Salehi, Cyrus and Johnson, Ray}, abstract = {In the past several decades, machine learning has gained increasing interest in the oil and gas industry. This paper presents a comprehensive review of machine learning studies for drilling applications in the following categories: (1) drilling fluids; (2) drilling hydraulics; (3) drilling dynamics; (4) drilling problems; and (5) miscellaneous drilling applications. In each study, the machine learning algorithm(s), sample size, inputs and output(s), and performance are extracted. In addition, similarities of studies in each category are summarized and recommendations are made for future development.} }
@article{WOS:000976730100001, title = {Recent Progresses in Machine Learning Assisted Raman Spectroscopy}, journal = {ADVANCED OPTICAL MATERIALS}, volume = {11}, year = {2023}, issn = {2195-1071}, doi = {10.1002/adom.202203104}, author = {Qi, Yaping and Hu, Dan and Jiang, Yucheng and Wu, Zhenping and Zheng, Ming and Chen, Esther Xinyi and Liang, Yong and Sadi, Mohammad A. A. and Zhang, Kang and Chen, Yong P. P.}, abstract = {With the development of Raman spectroscopy and the expansion of its application domains, conventional methods for spectral data analysis have manifested many limitations. Exploring new approaches to facilitate Raman spectroscopy and analysis has become an area of intensifying focus for research. It has been demonstrated that machine learning techniques can more efficiently extract valuable information from spectral data, creating unprecedented opportunities for analytical science. This paper outlines traditional and more recently developed statistical methods that are commonly used in machine learning (ML) and ML-algorithms for different Raman spectroscopy-based classification and recognition applications. The methods include Principal Component Analysis, K-Nearest Neighbor, Random Forest, and Support Vector Machine, as well as neural network-based deep learning algorithms such as Artificial Neural Networks, Convolutional Neural Networks, etc. The bulk of the review is dedicated to the research advances in machine learning applied to Raman spectroscopy from several fields, including material science, biomedical applications, food science, and others, which reached impressive levels of analytical accuracy. The combination of Raman spectroscopy and machine learning offers unprecedented opportunities to achieve high throughput and fast identification in many of these application fields. The limitations of current studies are also discussed and perspectives on future research are provided.} }
@article{WOS:001076506500001, title = {Bridging the Worlds of Pharmacometrics and Machine Learning}, journal = {CLINICAL PHARMACOKINETICS}, volume = {62}, pages = {1551-1565}, year = {2023}, issn = {0312-5963}, doi = {10.1007/s40262-023-01310-x}, author = {Stankeviciute, Kamile and Woillard, Jean-Baptiste and Peck, Richard W. and Marquet, Pierre and van der Schaar, Mihaela}, abstract = {Precision medicine requires individualized modeling of disease and drug dynamics, with machine learning-based computational techniques gaining increasing popularity. The complexity of either field, however, makes current pharmacological problems opaque to machine learning practitioners, and state-of-the-art machine learning methods inaccessible to pharmacometricians. To help bridge the two worlds, we provide an introduction to current problems and techniques in pharmacometrics that ranges from pharmacokinetic and pharmacodynamic modeling to pharmacometric simulations, model-informed precision dosing, and systems pharmacology, and review some of the machine learning approaches to address them. We hope this would facilitate collaboration between experts, with complementary strengths of principled pharmacometric modeling and flexibility of machine learning leading to synergistic effects in pharmacological applications.} }
@article{WOS:000605079100003, title = {CAN MACHINES ``LEARN'' FINANCE?}, journal = {JOURNAL OF INVESTMENT MANAGEMENT}, volume = {18}, pages = {23-36}, year = {2020}, issn = {1545-9144}, author = {Israel, Ronen and Kelly, Bryan and Moskowitz, Tobias}, abstract = {Machine learning for asset management faces a unique set of challenges that differ markedly from other domains where machine learning has excelled. Understanding these differences is critical for developing impactful approaches and realistic expectations for machine learning in asset management. We discuss a variety of beneficial use cases and potential pitfalls, and emphasize the importance of economic theory and human expertise for achieving success through financial machine learning.} }
@article{WOS:000537076300001, title = {The virtue of simplicity: On machine learning models in algorithmic trading}, journal = {BIG DATA \\& SOCIETY}, volume = {7}, year = {2020}, issn = {2053-9517}, doi = {10.1177/2053951720926558}, author = {Hansen, Kristian Bondo}, abstract = {Machine learning models are becoming increasingly prevalent in algorithmic trading and investment management. The spread of machine learning in finance challenges existing practices of modelling and model use and creates a demand for practical solutions for how to manage the complexity pertaining to these techniques. Drawing on interviews with quants applying machine learning techniques to financial problems, the article examines how these people manage model complexity in the process of devising machine learning-powered trading algorithms. The analysis shows that machine learning quants use Ockham's razor - things should not be multiplied without necessity - as a heuristic tool to prevent excess model complexity and secure a certain level of human control and interpretability in the modelling process. I argue that understanding the way quants handle the complexity of learning models is a key to grasping the transformation of the human's role in contemporary data and model-driven finance. The study contributes to social studies of finance research on the human-model interplay by exploring it in the context of machine learning model use.} }
@article{WOS:001114383400001, title = {From pixels to insights: Machine learning and deep learning for bioimage analysis}, journal = {BIOESSAYS}, volume = {46}, year = {2024}, issn = {0265-9247}, doi = {10.1002/bies.202300114}, author = {Jan, Mahta and Spangaro, Allie and Lenartowicz, Michelle and Usaj, Mojca Mattiazzi}, abstract = {Bioimage analysis plays a critical role in extracting information from biological images, enabling deeper insights into cellular structures and processes. The integration of machine learning and deep learning techniques has revolutionized the field, enabling the automated, reproducible, and accurate analysis of biological images. Here, we provide an overview of the history and principles of machine learning and deep learning in the context of bioimage analysis. We discuss the essential steps of the bioimage analysis workflow, emphasizing how machine learning and deep learning have improved preprocessing, segmentation, feature extraction, object tracking, and classification. We provide examples that showcase the application of machine learning and deep learning in bioimage analysis. We examine user-friendly software and tools that enable biologists to leverage these techniques without extensive computational expertise. This review is a resource for researchers seeking to incorporate machine learning and deep learning in their bioimage analysis workflows and enhance their research in this rapidly evolving field. Machine learning and deep learning have revolutionized bioimage analysis, automating and enhancing tasks like image preprocessing, object segmentation and tracking, feature extraction, and classification. This review showcases the pivotal role these approaches have played in the field, and highlights user-friendly bioimage analysis tools for biologists without extensive computational expertise. image} }
@article{WOS:000880955300001, title = {A Method for Analyzing the Performance Impact of Imbalanced Binary Data on Machine Learning Models}, journal = {AXIOMS}, volume = {11}, year = {2022}, doi = {10.3390/axioms11110607}, author = {Zheng, Ming and Wang, Fei and Hu, Xiaowen and Miao, Yuhao and Cao, Huo and Tang, Mingjing}, abstract = {Machine learning models may not be able to effectively learn and predict from imbalanced data in the fields of machine learning and data mining. This study proposed a method for analyzing the performance impact of imbalanced binary data on machine learning models. It systematically analyzes 1. the relationship between varying performance in machine learning models and imbalance rate (IR); 2. the performance stability of machine learning models on imbalanced binary data. In the proposed method, the imbalanced data augmentation algorithms are first designed to obtain the imbalanced dataset with gradually varying IR. Then, in order to obtain more objective classification results, the evaluation metric AFG, arithmetic mean of area under the receiver operating characteristic curve (AUC), F-measure and G-mean are used to evaluate the classification performance of machine learning models. Finally, based on AFG and coefficient of variation (CV), the performance stability evaluation method of machine learning models is proposed. Experiments of eight widely used machine learning models on 48 different imbalanced datasets demonstrate that the classification performance of machine learning models decreases with the increase of IR on the same imbalanced data. Meanwhile, the classification performances of LR, DT and SVC are unstable, while GNB, BNB, KNN, RF and GBDT are relatively stable and not susceptible to imbalanced data. In particular, the BNB has the most stable classification performance. The Friedman and Nemenyi post hoc statistical tests also confirmed this result. The SMOTE method is used in oversampling-based imbalanced data augmentation, and determining whether other oversampling methods can obtain consistent results needs further research. In the future, an imbalanced data augmentation algorithm based on undersampling and hybrid sampling should be used to analyze the performance impact of imbalanced binary data on machine learning models.} }
@article{WOS:001279022900001, title = {A survey on fault diagnosis of rotating machinery based on machine learning}, journal = {MEASUREMENT SCIENCE AND TECHNOLOGY}, volume = {35}, year = {2024}, issn = {0957-0233}, doi = {10.1088/1361-6501/ad6203}, author = {Wang, Qi and Huang, Rui and Xiong, Jianbin and Yang, Jianxiang and Dong, Xiangjun and Wu, Yipeng and Wu, Yinbo and Lu, Tiantian}, abstract = {With the booming development of modern industrial technology, rotating machinery fault diagnosis is of great significance to improve the safety, efficiency and sustainable development of industrial production. Machine learning as an effective solution for fault identification, has advantages over traditional fault diagnosis solutions in processing complex data, achieving automation and intelligence, adapting to different fault types, and continuously optimizing. It has high application value and broad development prospects in the field of fault diagnosis of rotating machinery. Therefore, this article reviews machine learning and its applications in intelligent fault diagnosis technology and covers advanced topics in emerging deep learning techniques and optimization methods. Firstly, this article briefly introduces the theories of several main machine learning methods, including Extreme Learning Machines (ELM), Support Vector Machines (SVM), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs) and related emerging deep learning technologies such as Transformer, adversarial neural network (GAN) and graph neural network (GNN) in recent years. The optimization techniques for diagnosing faults in rotating machinery are subsequently investigated. Then, a brief introduction is given to the papers on the application of these machine learning methods in the field of rotating machinery fault diagnosis, and the application characteristics of various methods are summarized. Finally, this survey discusses the problems to be solved by machine learning in fault diagnosis of rotating machinery and proposes an outlook.} }
@article{WOS:000728330600001, title = {Machine learning techniques to predict daily rainfall amount}, journal = {JOURNAL OF BIG DATA}, volume = {8}, year = {2021}, doi = {10.1186/s40537-021-00545-4}, author = {Liyew, Chalachew Muluken and Melese, Haileyesus Amsaya}, abstract = {Predicting the amount of daily rainfall improves agricultural productivity and secures food and water supply to keep citizens healthy. To predict rainfall, several types of research have been conducted using data mining and machine learning techniques of different countries' environmental datasets. An erratic rainfall distribution in the country affects the agriculture on which the economy of the country depends on. Wise use of rainfall water should be planned and practiced in the country to minimize the problem of the drought and flood occurred in the country. The main objective of this study is to identify the relevant atmospheric features that cause rainfall and predict the intensity of daily rainfall using machine learning techniques. The Pearson correlation technique was used to select relevant environmental variables which were used as an input for the machine learning model. The dataset was collected from the local meteorological office at Bahir Dar City, Ethiopia to measure the performance of three machine learning techniques (Multivariate Linear Regression, Random Forest, and Extreme Gradient Boost). Root mean squared error and Mean absolute Error methods were used to measure the performance of the machine learning model. The result of the study revealed that the Extreme Gradient Boosting machine learning algorithm performed better than others.} }
@article{WOS:000754809300001, title = {Using Machine Learning to Predict Complications in Pregnancy: A Systematic Review}, journal = {FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY}, volume = {9}, year = {2022}, issn = {2296-4185}, doi = {10.3389/fbioe.2021.780389}, author = {Bertini, Ayleen and Salas, Rodrigo and Chabert, Steren and Sobrevia, Luis and Pardo, Fabian}, abstract = {Introduction: Artificial intelligence is widely used in medical field, and machine learning has been increasingly used in health care, prediction, and diagnosis and as a method of determining priority. Machine learning methods have been features of several tools in the fields of obstetrics and childcare. This present review aims to summarize the machine learning techniques to predict perinatal complications.Objective: To identify the applicability and performance of machine learning methods used to identify pregnancy complications.Methods: A total of 98 articles were obtained with the keywords ``machine learning,'' ``deep learning,'' ``artificial intelligence,'' and accordingly as they related to perinatal complications (''complications in pregnancy,'' ``pregnancy complications'') from three scientific databases: PubMed, Scopus, and Web of Science. These were managed on the Mendeley platform and classified using the PRISMA method.Results: A total of 31 articles were selected after elimination according to inclusion and exclusion criteria. The features used to predict perinatal complications were primarily electronic medical records (48\\%), medical images (29\\%), and biological markers (19\\%), while 4\\% were based on other types of features, such as sensors and fetal heart rate. The main perinatal complications considered in the application of machine learning thus far are pre-eclampsia and prematurity. In the 31 studies, a total of sixteen complications were predicted. The main precision metric used is the AUC. The machine learning methods with the best results were the prediction of prematurity from medical images using the support vector machine technique, with an accuracy of 95.7\\%, and the prediction of neonatal mortality with the XGBoost technique, with 99.7\\% accuracy.Conclusion: It is important to continue promoting this area of research and promote solutions with multicenter clinical applicability through machine learning to reduce perinatal complications. This systematic review contributes significantly to the specialized literature on artificial intelligence and women's health.} }
@article{WOS:000620348900005, title = {Machine Learning in Predictive Toxicology: Recent Applications and Future Directions for Classification Models}, journal = {CHEMICAL RESEARCH IN TOXICOLOGY}, volume = {34}, pages = {217-239}, year = {2021}, issn = {0893-228X}, doi = {10.1021/acs.chemrestox.0c00316}, author = {Wang, Marcus W. H. and Goodman, Jonathan M. and Allen, Timothy E. H.}, abstract = {In recent times, machine learning has become increasingly prominent in predictive toxicology as it has shifted from in vivo studies toward in silico studies. Currently, in vitro methods together with other computational methods such as quantitative structure-activity relationship modeling and absorption, distribution, metabolism, and excretion calculations are being used. An overview of machine learning and its applications in predictive toxicology is presented here, including support vector machines (SVMs), random forest (RF) and decision trees (DTs), neural networks, regression models, naive Bayes, k-nearest neighbors, and ensemble learning. The recent successes of these machine learning methods in predictive toxicology are summarized, and a comparison of some models used in predictive toxicology is presented. In predictive toxicology, SVMs, RF, and DTs are the dominant machine learning methods due to the characteristics of the data available. Lastly, this review describes the current challenges facing the use of machine learning in predictive toxicology and offers insights into the possible areas of improvement in the field.} }
@article{WOS:001239040700001, title = {Applications of machine learning in phylogenetics}, journal = {MOLECULAR PHYLOGENETICS AND EVOLUTION}, volume = {196}, year = {2024}, issn = {1055-7903}, doi = {10.1016/j.ympev.2024.108066}, author = {Mo, Yu K. and Hahn, Matthew W. and Smith, Megan L.}, abstract = {Machine learning has increasingly been applied to a wide range of questions in phylogenetic inference. Supervised machine learning approaches that rely on simulated training data have been used to infer tree topologies and branch lengths, to select substitution models, and to perform downstream inferences of introgression and diversification. Here, we review how researchers have used several promising machine learning approaches to make phylogenetic inferences. Despite the promise of these methods, several barriers prevent supervised machine learning from reaching its full potential in phylogenetics. We discuss these barriers and potential paths forward. In the future, we expect that the application of careful network designs and data encodings will allow supervised machine learning to accommodate the complex processes that continue to confound traditional phylogenetic methods.} }
@article{WOS:000671787900017, title = {Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3453158}, author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior}, abstract = {In recent years, machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This article comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker' s goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. To the best of our knowledge, this work is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.} }
@article{WOS:000708780800001, title = {Reinforcement learning applications to machine scheduling problems: a comprehensive literature review}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {34}, pages = {905-929}, year = {2023}, issn = {0956-5515}, doi = {10.1007/s10845-021-01847-3}, author = {Kayhan, Behice Meltem and Yildiz, Gokalp}, abstract = {Reinforcement learning (RL) is one of the most remarkable branches of machine learning and attracts the attention of researchers from numerous fields. Especially in recent years, the RL methods have been applied to machine scheduling problems and are among the top five most encouraging methods for scheduling literature. Therefore, in this study, a comprehensive literature review about RL methods applications to machine scheduling problems was conducted. In this regard, Scopus and Web of Science databases were searched very inclusively using the proper keywords. As a result of the comprehensive research, 80 papers were found, published between 1995 and 2020. These papers were analyzed considering different aspects of the problem such as applied algorithms, machine environments, job and machine characteristics, objectives, benchmark methods, and a detailed classification scheme was constructed. Job shop scheduling, unrelated parallel machine scheduling, and single machine scheduling problems were found as the most studied problem type. The main contributions of the study are to examine essential aspects of reinforcement learning in machine scheduling problems, identify the most frequently investigated problem types, objectives, and constraints, and reveal the deficiencies and promising areas in the related literature. This study can help researchers who wish to study in this field through the comprehensive analysis of the related literature.} }
@article{WOS:000880508300001, title = {Stability modeling for chatter avoidance in self-aware machining: an application of physics-guided machine learning}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {34}, pages = {387-413}, year = {2023}, issn = {0956-5515}, doi = {10.1007/s10845-022-01999-w}, author = {Greis, Noel P. and Nogueira, Monica L. and Bhattacharya, Sambit and Spooner, Catherine and Schmitz, Tony}, abstract = {Physics-guided machine learning (PGML) offers a new approach to stability modeling during machining that leverages experimental data generated during the machining process while incorporating decades of theoretical process modeling efforts. This approach addresses specific limitations of machine learning models and physics-based models individually. Data-driven machine learning models are typically black box models that do not provide deep insight into the underlying physics and do not reflect physical constraints for the modeled system, sometimes yielding solutions that violate physical laws or operational constraints. In addition, acquiring the large amounts of manufacturing data needed for machine learning modeling can be costly. On the other hand, many physical processes are not completely understood by domain experts and have a high degree of uncertainty. Physics-based models must make simplifying assumptions that can compromise prediction accuracy. This research explores whether data generated by an uncertain physics-based milling stability model that is used to train a physics-guided machine learning stability model, and then updated with measured data, domain knowledge, and theory-based knowledge provides a useful approximation to the unknown true stability model for a specific set of factory operating conditions. Four novel strategies for updating the machine learning model with experimental data are explored. These updating strategies differ in their assumptions about and implementation of the type of physics-based knowledge included in the PGML model. Using a simulation experiment, these strategies achieve useful approximations of the underlying true stability model while reducing the number of experimental measurements required for model update.} }
@article{WOS:000787328500007, title = {Machine learning in modelling land-use and land cover-change (LULCC): Current status, challenges and prospects}, journal = {SCIENCE OF THE TOTAL ENVIRONMENT}, volume = {822}, year = {2022}, issn = {0048-9697}, doi = {10.1016/j.scitotenv.2022.153559}, author = {Wang, Junye and Bretz, Michael and Dewan, M. Ali Akber and Delavar, Mojtaba Aghajani}, abstract = {Land-use and land-cover change (LULCC) are of importance in natural resource management, environmental modelling and assessment, and agricultural production management. However, LULCC detection and modelling is a complex, data-driven process in the remote sensing field due to the processing of massive historical and current data, real-time interaction of scenario data, and spatial environmental data. In this paper, we review principles and methods of LULCC modelling, using machine learning and beyond, such as traditional cellular automata (CA). Then, we examine the characteristics, capabilities, limitations, and perspectives of machine learning. Machine learning has not yet been dramatic in modelling LULCC, such as urbanization prediction and crop yield prediction because competition and transition between land cover types are dynamic at a local scale under varying natural drivers and human activities. Upcoming challenges of machine learning in modelling LULCC remain in the detection and prediction of LULC evolutionary processes if considering their applicability and feasibility, such as the spatio-temporal transition mechanisms to describe occurrence, transition, spreading, and spatial patterns of changes, availability of training data of all the change drivers, particularly sequence data, and identification and inclusion of local ecological, hydrological, and social-economic drivers in addressing the spectral feature change. This review points out the need for multidisciplinary research beyond image processing and pattern recognition of machine learning in accelerating and advancing studies of LULCC modelling. Despite this, we believe that machine learning has strong potentials to incorporate new exploratory variables in modelling LULCC through expanding remote sensing big data and advancing transient algorithms.} }
@article{WOS:000438855100013, title = {A survey on application of machine learning for Internet of Things}, journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS}, volume = {9}, pages = {1399-1417}, year = {2018}, issn = {1868-8071}, doi = {10.1007/s13042-018-0834-5}, author = {Cui, Laizhong and Yang, Shu and Chen, Fei and Ming, Zhong and Lu, Nan and Qin, Jing}, abstract = {Internet of Things (IoT) has become an important network paradigm and there are lots of smart devices connected by IoT. IoT systems are producing massive data and thus more and more IoT applications and services are emerging. Machine learning, as an another important area, has obtained a great success in several research fields such as computer vision, computer graphics, natural language processing, speech recognition, decision-making, and intelligent control. It has also been introduced in networking research. Many researches study how to utilize machine learning to solve networking problems, including routing, traffic engineering, resource allocation, and security. Recently, there has been a rising trend of employing machine learning to improve IoT applications and provide IoT services such as traffic engineering, network management, security, Internet traffic classification, and quality of service optimization. This survey paper focuses on providing an overview of the application of machine learning in the domain of IoT. We provide a comprehensive survey highlighting the recent progresses in machine learning techniques for IoT and describe various IoT applications. The application of machine learning for IoT enables users to obtain deep analytics and develop efficient intelligent IoT applications. This paper is different from the previously published survey papers in terms of focus, scope, and breadth; specifically, we have written this paper to emphasize the application of machine learning for IoT and the coverage of most recent advances. This paper has made an attempt to cover the major applications of machine learning for IoT and the relevant techniques, including traffic profiling, IoT device identification, security, edge computing infrastructure, network management and typical IoT applications. We also make a discussion on research challenges and open issues.} }
@article{WOS:000969652500001, title = {Primer on Machine Learning in Electrophysiology}, journal = {ARRHYTHMIA \\& ELECTROPHYSIOLOGY REVIEW}, volume = {12}, year = {2023}, issn = {2050-3369}, doi = {10.15420/aer.2022.43}, author = {Loeffler, Shane E. and Trayanova, Natalia}, abstract = {Artificial intelligence has become ubiquitous. Machine learning, a branch of artificial intelligence, leads the current technological revolution through its remarkable ability to learn and perform on data sets of varying types. Machine learning applications are expected to change contemporary medicine as they are brought into mainstream clinical practice. In the field of cardiac arrhythmia and electrophysiology, machine learning applications have enjoyed rapid growth and popularity. To facilitate clinical acceptance of these methodologies, it is important to promote general knowledge of machine learning in the wider community and continue to highlight the areas of successful application. The authors present a primer to provide an overview of common supervised (least squares, support vector machine, neural networks and random forest) and unsupervised (k-means and principal component analysis) machine learning models. The authors also provide explanations as to how and why the specific machine learning models have been used in arrhythmia and electrophysiology studies.} }
@article{WOS:000472029100018, title = {Predicting the Young's Modulus of Silicate Glasses using High-Throughput Molecular Dynamics Simulations and Machine Learning}, journal = {SCIENTIFIC REPORTS}, volume = {9}, year = {2019}, issn = {2045-2322}, doi = {10.1038/s41598-019-45344-3}, author = {Yang, Kai and Xu, Xinyi and Yang, Benjamin and Cook, Brian and Ramos, Herbert and Krishnan, N. M. Anoop and Smedskjaer, Morten M. and Hoover, Christian and Bauchy, Mathieu}, abstract = {The application of machine learning to predict materials' properties usually requires a large number of consistent data for training. However, experimental datasets of high quality are not always available or self-consistent. Here, as an alternative route, we combine machine learning with high-throughput molecular dynamics simulations to predict theYoung's modulus of silicate glasses. We demonstrate that this combined approach offers good and reliable predictions over the entire compositional domain. By comparing the performances of select machine learning algorithms, we discuss the nature of the balance between accuracy, simplicity, and interpretability in machine learning.} }
@article{WOS:000418675900003, title = {Applications of Support Vector Machine (SVM) Learning in Cancer Genomics}, journal = {CANCER GENOMICS \\& PROTEOMICS}, volume = {15}, pages = {41-51}, year = {2018}, issn = {1109-6535}, doi = {10.21873/cgp.20063}, author = {Huang, Shujun and Cai, Nianguang and Pacheco, Pedro Penzuti and Narandes, Shavira and Wang, Yang and Xu, Wayne}, abstract = {Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.} }
@article{WOS:000462604300001, title = {Machine Learning SNP Based Prediction for Precision Medicine}, journal = {FRONTIERS IN GENETICS}, volume = {10}, year = {2019}, doi = {10.3389/fgene.2019.00267}, author = {Ho, Daniel Sik Wai and Schierding, William and Wake, Melissa and Saffery, Richard and O'Sullivan, Justin}, abstract = {In the past decade, precision genomics based medicine has emerged to provide tailored and effective healthcare for patients depending upon their genetic features. Genome Wide Association Studies have also identified population based risk genetic variants for common and complex diseases. In order to meet the full promise of precision medicine, research is attempting to leverage our increasing genomic understanding and further develop personalized medical healthcare through ever more accurate disease risk prediction models. Polygenic risk scoring and machine learning are two primary approaches for disease risk prediction. Despite recent improvements, the results of polygenic risk scoring remain limited due to the approaches that are currently used. By contrast, machine learning algorithms have increased predictive abilities for complex disease risk. This increase in predictive abilities results from the ability of machine learning algorithms to handle multi-dimensional data. Here, we provide an overview of polygenic risk scoring and machine learning in complex disease risk prediction. We highlight recent machine learning application developments and describe how machine learning approaches can lead to improved complex disease prediction, which will help to incorporate genetic features into future personalized healthcare. Finally, we discuss how the future application of machine learning prediction models might help manage complex disease by providing tissue-specific targets for customized, preventive interventions.} }
@article{WOS:001085771300006, title = {Predictive modelling and analytics for diabetes using a machine learning approach}, journal = {APPLIED COMPUTING AND INFORMATICS}, volume = {18}, pages = {90-100}, year = {2022}, issn = {2634-1964}, doi = {10.1016/j.aci.2018.12.004}, author = {Kaur, Harleen and Kumari, Vinita}, abstract = {Diabetes is a major metabolic disorder which can affect entire body system adversely. Undiagnosed diabetes can increase the risk of cardiac stroke, diabetic nephropathy and other disorders. All over the world millions of people are affected by this disease. Early detection of diabetes is very important to maintain a healthy life. This disease is a reason of global concern as the cases of diabetes are rising rapidly. Machine learning (ML) is a computational method for automatic learning from experience and improves the performance to make more accurate predictions. In the current research we have utilized machine learning technique in Pima Indian diabetes dataset to develop trends and detect patterns with risk factors using R data manipulation tool. To classify the patients into diabetic and non-diabetic we have developed and analyzed five different predictive models using R data manipulation tool. For this purpose we used supervised machine learning algorithms namely linear kernel support vector machine (SVM-linear), radial basis function (RBF) kernel support vector machine, k-nearest neighbour (k-NN), artificial neural network (ANN) and multifactor dimensionality reduction (MDR).} }
@article{WOS:000908510300001, title = {Interpretable Machine Learning Techniques in ECG-Based Heart Disease Classification: A Systematic Review}, journal = {DIAGNOSTICS}, volume = {13}, year = {2023}, doi = {10.3390/diagnostics13010111}, author = {Ayano, Yehualashet Megersa and Schwenker, Friedhelm and Dufera, Bisrat Derebssa and Debelee, Taye Girma}, abstract = {Heart disease is one of the leading causes of mortality throughout the world. Among the different heart diagnosis techniques, an electrocardiogram (ECG) is the least expensive non-invasive procedure. However, the following are challenges: the scarcity of medical experts, the complexity of ECG interpretations, the manifestation similarities of heart disease in ECG signals, and heart disease comorbidity. Machine learning algorithms are viable alternatives to the traditional diagnoses of heart disease from ECG signals. However, the black box nature of complex machine learning algorithms and the difficulty in explaining a model's outcomes are obstacles for medical practitioners in having confidence in machine learning models. This observation paves the way for interpretable machine learning (IML) models as diagnostic tools that can build a physician's trust and provide evidence-based diagnoses. Therefore, in this systematic literature review, we studied and analyzed the research landscape in interpretable machine learning techniques by focusing on heart disease diagnosis from an ECG signal. In this regard, the contribution of our work is manifold; first, we present an elaborate discussion on interpretable machine learning techniques. In addition, we identify and characterize ECG signal recording datasets that are readily available for machine learning-based tasks. Furthermore, we identify the progress that has been achieved in ECG signal interpretation using IML techniques. Finally, we discuss the limitations and challenges of IML techniques in interpreting ECG signals.} }
@article{WOS:000492778000001, title = {Rethinking Drug Repositioning and Development with Artificial Intelligence, Machine Learning, and Omics}, journal = {OMICS-A JOURNAL OF INTEGRATIVE BIOLOGY}, volume = {23}, pages = {539-548}, year = {2019}, issn = {1536-2310}, doi = {10.1089/omi.2019.0151}, author = {Koromina, Maria and Pandi, Maria-Theodora and Patrinos, George P.}, abstract = {Pharmaceutical industry and the art and science of drug development are sorely in need of novel transformative technologies in the current age of digital health and artificial intelligence (AI). Often described as game-changing technologies, AI and machine learning algorithms have slowly but surely begun to revolutionize pharmaceutical industry and drug development over the past 5 years. In this expert review, we describe the most frequently used machine learning algorithms in drug development pipelines and the -omics databases well poised to support machine learning and drug discovery. Subsequently, we analyze the emerging new computational approaches to drug discovery and the in silico pipelines for drug repositioning and the synergies among -omics system sciences, AI and machine learning. As with system sciences, AI and machine learning embody a system scale and Big Data driven vision for drug discovery and development. We conclude with a future outlook on the ways in which machine learning approaches can be implemented to buttress and expedite drug discovery and precision medicine. As AI and machine learning are rapidly entering pharmaceutical industry and the art and science of drug development, we need to critically examine the attendant prospects and challenges to benefit patients and public health.} }
@article{WOS:000505643500038, title = {Equivalent circuit model recognition of electrochemical impedance spectroscopy via machine learning}, journal = {JOURNAL OF ELECTROANALYTICAL CHEMISTRY}, volume = {855}, year = {2019}, issn = {1572-6657}, doi = {10.1016/j.jelechem.2019.113627}, author = {Zhu, Shan and Sun, Xinyang and Gao, Xiaoyang and Wang, Jianrong and Zhao, Naiqin and Sha, Junwei}, abstract = {Electrochemical impedance spectroscopy (EIS) is an effective method for studying electrochemical systems. The interpretation of EIS is the biggest challenge in this technology, which requires reasonable modeling. To overcome the subjectivity of human analysis, this work uses machine learning to carry out EIS model recognition. Raw EIS data and their equivalent circuit models are collected from the literature, and the support vector machine (SVM) is used to analyze these data. Comparing with other machine learning algorithms, SVM achieves the best comprehensive performance in this database. As a result, the optimized SVM model can efficiently figure out the most suitable equivalent circuit model of the given EIS spectrum. This study demonstrates the great potential of machine learning in electrochemical researches.} }
@article{WOS:000484832800014, title = {Do no harm: a roadmap for responsible machine learning for health care}, journal = {NATURE MEDICINE}, volume = {25}, pages = {1337-1340}, year = {2019}, issn = {1078-8956}, doi = {10.1038/s41591-019-0548-6}, author = {Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and Liu, Vincent X. and Doshi-Velez, Finale and Jung, Kenneth and Heller, Katherine and Kale, David and Saeed, Mohammed and Ossorio, Pilar N. and Thadaney-Israni, Sonoo and Goldenberg, Anna}, abstract = {Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).} }
@article{WOS:000466934400012, title = {Machine Learning for Semi Linear PDEs}, journal = {JOURNAL OF SCIENTIFIC COMPUTING}, volume = {79}, pages = {1667-1712}, year = {2019}, issn = {0885-7474}, doi = {10.1007/s10915-019-00908-3}, author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier}, abstract = {Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.} }
@article{WOS:000457022100009, title = {Improved online sequential extreme learning machine for identifying crack behavior in concrete dam}, journal = {ADVANCES IN STRUCTURAL ENGINEERING}, volume = {22}, pages = {402-412}, year = {2019}, issn = {1369-4332}, doi = {10.1177/1369433218788635}, author = {Dai, Bo and Gu, Chongshi and Zhao, Erfeng and Zhu, Kai and Cao, Wenhan and Qin, Xiangnan}, abstract = {Prediction models are essential in dam crack behavior identification. Prototype monitoring data arrive sequentially in dam safety monitoring. Given such characteristic, sequential learning algorithms are preferred over batch learning algorithms as they do not require retraining whenever new data are received. A new methodology using the genetic optimized online sequential extreme learning machine and bootstrap confidence intervals is proposed as a practical tool for identifying concrete dam crack behavior. First, online sequential extreme learning machine is adopted to build an online prediction model of crack behavior. The characteristic vector of crack behavior, which is taken as the online sequential extreme learning machine input, is extracted by the statistical model. A genetic algorithm is introduced to optimize the input weights and biases of online sequential extreme learning machine. Second, the BC(a )method is proposed to produce confidence intervals based on the improved online sequential extreme learning machine prediction. The improved online sequential extreme learning machine for identifying crack behavior is then built. Third, the crack behavior of an actual concrete dam is taken as an example. The capability of the built model for predicting dam crack opening is evaluated. The comparative results demonstrate that the improved online sequential extreme learning machine can provide highly accurate forecasts and reasonably identify crack behavior.} }
@article{WOS:000818095000001, title = {Interpretable and Explainable Machine Learning for Materials Science and Chemistry}, journal = {ACCOUNTS OF MATERIALS RESEARCH}, volume = {3}, pages = {597-607}, year = {2022}, doi = {10.1021/accountsmr.1c00244}, author = {Oviedo, Felipe and Ferres, Juan Lavista and Buonassisi, Tonio and Butler, Keith T.}, abstract = {CONSPECTUS: Machine learning has become a common and powerful tool in materials research. As more data become available, with the use of high-performance computing and high-throughput experimentation, machine learning has proven potential to accelerate scientific research and technology development. Though the uptake of data-driven approaches for materials science is at an exciting, early stage, to realize the true potential of machine learning models for successful scientific discovery, they must have qualities beyond purely predictive power. The predictions and inner workings of models should provide a certain degree of explainability by human experts, permitting the identification of potential model issues or limitations, building trust in model predictions, and unveiling unexpected correlations that may lead to scientific insights. In this work, we summarize applications of interpretability and explainability techniques for materials science and chemistry and discuss how these techniques can improve the outcome of scientific studies. We start by defining the fundamental concepts of interpretability and explainability in machine learning and making them less abstract by providing examples in the field. We show how interpretability in scientific machine learning has additional constraints compared to general applications. Building upon formal definitions in machine learning, we formulate the basic trade-offs among the explainability, completeness, and scientific validity of model explanations in scientific problems. In the context of these trade-offs, we discuss how interpretable models can be constructed, what insights they provide, and what drawbacks they have. We present numerous examples of the application of interpretable machine learning in a variety of experimental and simulation studies, encompassing first-principles calculations, physicochemical characterization, materials development, and integration into complex systems. We discuss the varied impacts and uses of interpretabiltiy in these cases according to the nature and constraints of the scientific study of interest. We discuss various challenges for interpretable machine learning in materials science and, more broadly, in scientific settings. In particular, we emphasize the risks of inferring causation or reaching generalization by purely interpreting machine learning models and the need for uncertainty estimates for model explanations. Finally, we showcase a number of exciting developments in other fields that could benefit interpretability in material science problems. Adding interpretability to a machine learning model often requires no more technical know-how than building the model itself. By providing concrete examples of studies (many with associated open source code and data), we hope that this Account will encourage all practitioners of machine learning in materials science to look deeper into their models.} }
@article{WOS:000649679300001, title = {Applications of artificial intelligence and machine learning approaches in echocardiography}, journal = {ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ULTRASOUND AND ALLIED TECHNIQUES}, volume = {38}, pages = {982-992}, year = {2021}, issn = {0742-2822}, doi = {10.1111/echo.15048}, author = {Nabi, Wafa and Bansal, Agam and Xu, Bo}, abstract = {Artificial intelligence and machine learning approaches have become increasingly applied in the field of echocardiography to streamline diagnostic and prognostic assessments, and to support treatment decisions. Artificial intelligence and machine learning have been applied to aid image acquisition and automation. They have also been applied to the integration of clinical and imaging data. Applications of artificial intelligence and machine learning approaches in echocardiography in conjunction with health information databases may be promising in improving the classification and treatment of many cardiac conditions. This review article provides an overview of the applications of artificial intelligence and machine learning approaches in echocardiography.} }
@article{WOS:000471070400002, title = {Can Machine Learning Revolutionize Directed Evolution of Selective Enzymes?}, journal = {ADVANCED SYNTHESIS \\& CATALYSIS}, volume = {361}, pages = {2377-2386}, year = {2019}, issn = {1615-4150}, doi = {10.1002/adsc.201900149}, author = {Li, Guangyue and Dong, Yijie and Reetz, Manfred T.}, abstract = {Machine learning as a form of artificial intelligence consists of algorithms and statistical models for improving computer performance for different tasks. Training data are utilized for making decisions and predictions. Since directed evolution of enzymes produces huge amounts of potential training data, machine learning seems to be ideally suited to support this protein engineering technique. Machine learning has been used in protein science for a long time with different purposes. This mini-review focuses on the utility of machine learning as an aid in the directed evolution of selective enzymes. Recent studies have shown that the algorithms ASRA and Innov'SAR are well suited as guides when performing saturation mutagenesis at sites lining the binding pocket for enhancing stereoselectivity and activity.} }
@article{WOS:000855316400001, title = {Research Progress and Trend of the Machine Learning based on Fusion}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {13}, pages = {1-7}, year = {2022}, issn = {2158-107X}, author = {Yu, Chen Xiao and Ying, Song and Min, Zhang Xiao and Feng, Gao}, abstract = {Machine learning is widely used in the data processing including data classification, data regression, data mining and so on, and based on a single type of machine learning technology, it is often difficult to meet the requirements of data processing; in recent years, the machine learning based on fusion has become an important approach to improve data processing effect, and at the same time, corresponding summary study is relatively limited. In this study, we summarize and compare different types of fusion machine learning such as ensemble learning, federated learning and transfer learning from the perspectives of classification, principle and characteristics, and try to explore the research development trend, in order to provide effective reference for subsequent related research and application; furthermore, as an application of fusion machine learning,we also conduct a study on the modeling optimization for car service complaint text classification.} }
@article{WOS:000507900400001, title = {Detecting Accounting Fraud in Publicly Traded US Firms Using a Machine Learning Approach}, journal = {JOURNAL OF ACCOUNTING RESEARCH}, volume = {58}, pages = {199-235}, year = {2020}, issn = {0021-8456}, doi = {10.1111/1475-679X.12292}, author = {Bao, Yang and Ke, Bin and Li, Bin and Yu, Y. Julia and Zhang, Jie}, abstract = {We develop a state-of-the-art fraud prediction model using a machine learning approach. We demonstrate the value of combining domain knowledge and machine learning methods in model building. We select our model input based on existing accounting theories, but we differ from prior accounting research by using raw accounting numbers rather than financial ratios. We employ one of the most powerful machine learning methods, ensemble learning, rather than the commonly used method of logistic regression. To assess the performance of fraud prediction models, we introduce a new performance evaluation metric commonly used in ranking problems that is more appropriate for the fraud prediction task. Starting with an identical set of theory-motivated raw accounting numbers, we show that our new fraud prediction model outperforms two benchmark models by a large margin: the Dechow et al. logistic regression model based on financial ratios, and the Cecchini et al. support-vector-machine model with a financial kernel that maps raw accounting numbers into a broader set of ratios.} }
@article{WOS:000874966900001, title = {Colloquium: Machine learning in nuclear physics}, journal = {REVIEWS OF MODERN PHYSICS}, volume = {94}, year = {2022}, issn = {0034-6861}, doi = {10.1103/RevModPhys.94.031003}, author = {Boehnlein, Amber and Diefenthaler, Markus and Sato, Nobuo and Schram, Malachi and Ziegler, Veronique and Fanelli, Cristiano and Hjorth-Jensen, Morten and Horn, Tanja and Kuchera, Michelle P. and Lee, Dean and Nazarewicz, Witold and Ostroumov, Peter and Orginos, Kostas and Poon, Alan and Wang, Xin-Nian and Scheinker, Alexander and Smith, Michael S. and Pang, Long-Gang}, abstract = {Advances in machine learning methods provide tools that have broad applicability in scientific research. These techniques are being applied across the diversity of nuclear physics research topics, leading to advances that will facilitate scientific discoveries and societal applications. This Colloquium provides a snapshot of nuclear physics research, which has been transformed by machine learning techniques.} }
@article{WOS:000882795100003, title = {Machine learning and deep learning in phononic crystals and metamaterials-A review}, journal = {MATERIALS TODAY COMMUNICATIONS}, volume = {33}, year = {2022}, doi = {10.1016/j.mtcomm.2022.104606}, author = {Muhammad and Kennedy, John and Lim, C. W.}, abstract = {Machine learning (ML), as a component of artificial intelligence, encourages structural design exploration which leads to new technological advancements. By developing and generating data-driven methodologies that supplement conventional physics and formula-based approaches, deep learning (DL), a subset of machine learning offers an efficient way to understand and harness artificial materials and structures. Recently, acoustic and mechanics communities have observed a surge of research interest in implementing machine learning and deep learning methods in the design and optimization of artificial materials. In this review we evaluate the recent developments and present a state-of-the-art literature survey in machine learning and deep learning based phononic crystals and metamaterial designs by giving historical context, discussing network architectures and working principles. We also explain the application of these network architectures adopted for design and optimization of artificial structures. Since this multidisciplinary research field is evolving, a summary of the future prospects is also covered. This review article serves to update the acoustics, mechanics, physics, material science and deep learning communities about the recent developments in this newly emerging research direction} }
@article{WOS:000635680800006, title = {What Role Does Hydrological Science Play in the Age of Machine Learning?}, journal = {WATER RESOURCES RESEARCH}, volume = {57}, year = {2021}, issn = {0043-1397}, doi = {10.1029/2020WR028091}, author = {Nearing, Grey S. and Kratzert, Frederik and Sampson, Alden Keefe and Pelissier, Craig S. and Klotz, Daniel and Frame, Jonathan M. and Prieto, Cristina and Gupta, Hoshin V.}, abstract = {Y This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall-runoff simulation indicate that there is significantly more information in large-scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence-based preferences for models based on a certain type of ``process understanding'' that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.} }
@article{WOS:000599992800002, title = {Return on Investment in Machine Learning: Crossing the Chasm between Academia and Business}, journal = {FOUNDATIONS OF COMPUTING AND DECISION SCIENCES}, volume = {45}, pages = {281-304}, year = {2020}, issn = {0867-6356}, doi = {10.2478/fcds-2020-0015}, author = {Mizgajski, Jan and Szymczak, Adrian and Morzy, Mikolaj and Augustyniak, Lukasz and Szymanski, Piotr and Zelasko, Piotr}, abstract = {Academia remains the central place of machine learning education. While academic culture is the predominant factor influencing the way we teach machine learning to students, many practitioners question this culture, claiming the lack of alignment between academic and business environments. Drawing on professional experiences from both sides of the chasm, we describe the main points of contention, in the hope that it will help better align academic syllabi with the expectations towards future machine learning practitioners. We also provide recommendations for teaching of the applied aspects of machine learning.} }
@article{WOS:000773410500001, title = {Machine-Learning Analysis of Small-Molecule Donors for Fullerene Based Organic Solar Cells}, journal = {ENERGY TECHNOLOGY}, volume = {10}, year = {2022}, issn = {2194-4288}, doi = {10.1002/ente.202200019}, author = {Janjua, Muhammad Ramzan Saeed Ashraf and Irfan, Ahmad and Hussien, Mohamed and Ali, Muhammad and Saqib, Muhammad and Sulaman, Muhammad}, abstract = {In recent years, development in organic solar cells speeds up and performance continuously increases. From the last few years, machine learning gains fame among scientists who are researching on organic solar cells. Herein, machine learning is used to screen the small-molecule donors for organic solar cells. Molecular descriptors are used as input to train machine models. A variety of machine-learning models are tested to find the suitable one. Random forest model shows best predictive capability (Pearson's coefficient = 0.93). New small-molecule donors are also designed from easily synthesizable building units. Their power conversion efficiencies (PCEs) are predicted. Potential candidates with PCE > 11\\% are selected. The approach presented herein helps to select the efficient materials in short time with ease.} }
@article{WOS:001091207400001, title = {A novel deep-learning technique for forecasting oil price volatility using historical prices of five precious metals in context of green financing - A comparison of deep learning, machine learning, and statistical models}, journal = {RESOURCES POLICY}, volume = {86}, year = {2023}, issn = {0301-4207}, doi = {10.1016/j.resourpol.2023.104216}, author = {Mohsin, Muhammad and Jamaani, Fouad}, abstract = {This study proposes a novel deep-learning convolution neural network (CNN) to forecast crude oil prices based on historical prices of five precious metals (Gold, Silver, Platinum, Palladium, and Rhodium) in context of green financing. The proposed deep learning CNN has three components: a convolution block called a group block, a novel convolutional neural network architecture called GroupNet, and a regression layer. The proposed model is tested against seven machine learning models and three traditional statistical models for predicting oil price volatility using the same independent variables (5 precious metals). A comparison of the deep learning model (our proposed model) with machine learning/deep learning models and statistical methods indicates that the proposed deep learning model has the highest prediction accuracy. A feature selection technique is also applied using the WEKA ML tool to improve the accuracy of the proposed model and existing machine learning and traditional statistical models. The findings indicate a non-linear correlation between oil price volatility and prices of precious metals. Moreover, statistical analysis indicates that deep learning can be used to predict oil price volatility with greater accuracy than machine learning and statistical methods while using precious metals as predictors. The results also indicate that machine learning models (Decision Tables and M5rules) can be used to predict oil price volatility with considerable accuracy. Moreover, the study proves that traditional statistical models can perform better than a few machine learning models (Lazy LWL and GPR).} }
@article{WOS:000535945200001, title = {Personality Research and Assessment in the Era of Machine Learning}, journal = {EUROPEAN JOURNAL OF PERSONALITY}, volume = {34}, pages = {613-631}, year = {2020}, issn = {0890-2070}, doi = {10.1002/per.2257}, author = {Stachl, Clemens and Pargent, Florian and Hilbert, Sven and Harari, Gabriella M. and Schoedel, Ramona and Vaid, Sumer and Gosling, Samuel D. and Buehner, Markus}, abstract = {The increasing availability of high-dimensional, fine-grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.} }
@article{WOS:000614760600005, title = {Chemist versus Machine: Traditional Knowledge versus Machine Learning Techniques}, journal = {TRENDS IN CHEMISTRY}, volume = {3}, pages = {86-95}, year = {2021}, doi = {10.1016/j.trechm.2020.10.007}, author = {George, Janine and Hautier, Geoffroy}, abstract = {Chemical heuristics have been fundamental to the advancement of chemistry and materials science. These heuristics are typically established by scientists using knowledge and creativity to extract patterns from limited datasets. Machine learning offers opportunities to perfect this approach using computers and larger datasets. Here, we discuss the relationships between traditional heuristics and machine learning approaches. We show how traditional rules can be challenged by large-scale statistical assessment and how traditional concepts commonly used as features are feeding the machine learning techniques. We stress the waste involved in relearning chemical rules and the challenges in terms of data size requirements for purely data-driven approaches. Our view is that heuristic and machine learning approaches are at their best when they work together.} }
@article{WOS:000757584200001, title = {A Concise Review on Recent Developments of Machine Learning for the Prediction of Vibrational Spectra}, journal = {JOURNAL OF PHYSICAL CHEMISTRY A}, volume = {126}, pages = {801-812}, year = {2022}, issn = {1089-5639}, doi = {10.1021/acs.jpca.1c10417}, author = {Han, Ruocheng and Ketkaew, Rangsiman and Luber, Sandra}, abstract = {Machine learning has become more and more popular in computational chemistry, as well as in the important field of spectroscopy. In this concise review, we walk the reader through a short summary of machine learning algorithms and a comprehensive discussion on the connection between machine learning methods and vibrational spectroscopy, particularly for the case of infrared and Raman spectroscopy. We also briefly discuss state-of-the-art molecular representations which serve as meaningful inputs for machine learning to predict vibrational spectra. In addition, this review provides an overview of the transferability and best practices of machine learning in the prediction of vibrational spectra as well as possible future research directions.} }
@article{WOS:000680450500001, title = {Machine learning and algorithmic fairness in public and population health}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {3}, pages = {659-666}, year = {2021}, doi = {10.1038/s42256-021-00373-4}, author = {Mhasawade, Vishwali and Zhao, Yuan and Chunara, Rumi}, abstract = {Until now, much of the work on machine learning and health has focused on processes inside the hospital or clinic. However, this represents only a narrow set of tasks and challenges related to health; there is greater potential for impact by leveraging machine learning in health tasks more broadly. In this Perspective we aim to highlight potential opportunities and challenges for machine learning within a holistic view of health and its influences. To do so, we build on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities. We present a brief introduction to research in these fields, data sources and types of tasks, and use these to identify settings where machine learning is relevant and can contribute to new knowledge. Given the key foci of health equity and disparities within public and population health, we juxtapose these topics with the machine learning subfield of algorithmic fairness to highlight specific opportunities where machine learning, public and population health may synergize to achieve health equity. Algorithmic solutions to improve treatment are starting to transform health care. Mhasawade and colleagues discuss in this Perspective how machine learning applications in population and public health can extend beyond clinical practice. While working with general health data comes with its own challenges, most notably ensuring algorithmic fairness in the face of existing health disparities, the area provides new kinds of data and questions for the machine learning community.} }
@article{WOS:000928683100001, title = {Learning machine learning with young children: exploring informal settings in an African context}, journal = {COMPUTER SCIENCE EDUCATION}, volume = {34}, pages = {161-192}, year = {2024}, issn = {0899-3408}, doi = {10.1080/08993408.2023.2175559}, author = {Sanusi, Ismaila Temitayo and Sunday, Kissinger and Oyelere, Solomon Sunday and Suhonen, Jarkko and Vartiainen, Henriikka and Tukiainen, Markku}, abstract = {Background and contextResearchers have been investigating ways to demystify machine learning for students from kindergarten to twelfth grade (K-12) levels. As little evidence can be found in the literature, there is a need for additional research to understand and facilitate the learning experience of children while also considering the African context.ObjectiveThe purpose of this study was to explore how young children teach and develop their understanding of machine learning based technologies in playful and informal settings.MethodUsing a qualitative methodological approach through fine-grained analysis of video recordings and interviews, we analysed how 18 children aged 3-13 years constructed their interactions with a machine-based technology (Google's Teachable Machine).FindingsThis study provides empirical support for the claim that Google's Teachable Machine contributes to the development of data literacy and conceptual understanding across K-12 irrespective of the learners' backgrounds. The results also confirmed children's ability to infer the relationship between their own expressions and the output of the machine learning-based tool, thus, identifying the input-output relationships in machine learning. In addition, this study opens a discussion around differentials in emerging technology use across different contexts through participatory learning.ImplicationsThe results provide a baseline for future research on the topic and preliminary evidence to discern how children learn about machine learning in the African K-12 context.} }
@article{WOS:000590441600001, title = {Machine Learning for Financial Risk Management: A Survey}, journal = {IEEE ACCESS}, volume = {8}, pages = {203203-203223}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3036322}, author = {Mashrur, Akib and Luo, Wei and Zaidi, Nayyar A. and Robles-Kelly, Antonio}, abstract = {Financial risk management avoids losses and maximizes profits, and hence is vital to most businesses. As the task relies heavily on information-driven decision making, machine learning is a promising source for new methods and technologies. In recent years, we have seen increasing adoption of machine learning methods for various risk management tasks. Machine-learning researchers, however, often struggle to navigate the vast and complex domain knowledge and the fast-evolving literature. This paper fills this gap, by providing a systematic survey of the rapidly growing literature of machine learning research for financial risk management. The contributions of the paper are four-folds: First, we present a taxonomy of financial-risk-management tasks and connect them with relevant machine learning methods. Secondly, we highlight significant publications in the past decade. Thirdly, we identify major challenges being faced by researchers in this area. And finally, we point out emerging trends and promising research directions.} }
@article{WOS:000828467800001, title = {Disclosure Sentiment: Machine Learning vs. Dictionary Methods}, journal = {MANAGEMENT SCIENCE}, volume = {68}, pages = {5514-5532}, year = {2022}, issn = {0025-1909}, doi = {10.1287/mnsc.2021.4156}, author = {Frankel, Richard and Jennings, Jared and Lee, Joshua}, abstract = {We compare the ability of dictionary-based and machine-learning methods to capture disclosure sentiment at 10-K filing and conference-call dates. Like Loughran and McDonald [Loughran T, McDonald B (2011) When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. J. Finance 66(1):35-65.], we use returns to assess sentiment. We find that measures based on machine learning offer a significant improvement in explanatory power over dictionary-based measures. Specifically, machine-learning measures explain returns at 10-K filing dates, whereas measures based on the Loughran and McDonald dictionary only explain returns at 10-K filing dates during the time period of their study. Moreover, at conference-call dates, machine-learning methods offer an improvement over the Loughran and McDonald dictionary method of a greater magnitude than the improvement of the Loughran and McDonald dictionary over the Harvard Psychosociological Dictionary. We further find that the random-forest-regression-tree method better captures disclosure sentiment than alternative algorithms, simplifying the application of the machine-learning approach. Overall, our results suggest that machine-learning methods offer an easily implementable, more powerful, and reliable measure of disclosure sentiment than dictionary-based methods.} }
@article{WOS:000403753100004, title = {Machine Learning: An Applied Econometric Approach}, journal = {JOURNAL OF ECONOMIC PERSPECTIVES}, volume = {31}, pages = {87-106}, year = {2017}, issn = {0895-3309}, doi = {10.1257/jep.31.2.87}, author = {Mullainathan, Sendhil and Spiess, Jann}, abstract = {Machines are increasingly doing ``intelligent'' things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble-and thus where they can be most usefully applied.} }
@article{WOS:000576604100002, title = {Machine learning applications in systems metabolic engineering}, journal = {CURRENT OPINION IN BIOTECHNOLOGY}, volume = {64}, pages = {1-9}, year = {2020}, issn = {0958-1669}, doi = {10.1016/j.copbio.2019.08.010}, author = {Kim, Gi Bae and Kim, Won Jun and Kim, Hyun Uk and Lee, Sang Yup}, abstract = {Systems metabolic engineering allows efficient development of high performing microbial strains for the sustainable production of chemicals and materials. In recent years, increasing availability of bio big data, for example, omics data, has led to active application of machine learning techniques across various stages of systems metabolic engineering, including host strain selection, metabolic pathway reconstruction, metabolic flux optimization, and fermentation. In this paper, recent contributions of machine learning approaches to each major step of systems metabolic engineering are discussed. As the use of machine learning in systems metabolic engineering will become more widespread in accordance with the ever-increasing volume of bio big data, future prospects are also provided for the successful applications of machine learning.} }
@article{WOS:001000655300009, title = {A Survey on Machine Learning in Hardware Security}, journal = {ACM JOURNAL ON EMERGING TECHNOLOGIES IN COMPUTING SYSTEMS}, volume = {19}, year = {2023}, issn = {1550-4832}, doi = {10.1145/3589506}, author = {Koylu, Troya Cagil and Reinbrecht, Cezar Rodolfo Wedig and Gebregiorgis, Anteneh and Hamdioui, Said and Taouil, Mottaqiallah}, abstract = {Hardware security is currently a very influential domain, where each year countless works are published concerning attacks against hardware and countermeasures. A significant number of them use machine learning, which is proven to be very effective in other domains. This survey, as one of the early attempts, presents the usage of machine learning in hardware security in a full and organized manner. Our contributions include classification and introduction to the relevant fields of machine learning, a comprehensive and critical overview of machine learning usage in hardware security, and an investigation of the hardware attacks against machine learning (neural network) implementations.} }
@article{WOS:000503335100001, title = {Surveying the reach and maturity of machine learning and artificial intelligence in astronomy}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {10}, year = {2020}, issn = {1942-4787}, doi = {10.1002/widm.1349}, author = {Fluke, Christopher J. and Jacobs, Colin}, abstract = {Machine learning (automated processes that learn by example in order to classify, predict, discover, or generate new data) and artificial intelligence (methods by which a computer makes decisions or discoveries that would usually require human intelligence) are now firmly established in astronomy. Every week, new applications of machine learning and artificial intelligence are added to a growing corpus of work. Random forests, support vector machines, and neural networks are now having a genuine impact for applications as diverse as discovering extrasolar planets, transient objects, quasars, and gravitationally lensed systems, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. This review surveys contemporary, published literature on machine learning and artificial intelligence in astronomy and astrophysics. Applications span seven main categories of activity: classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights. These categories form the basis of a hierarchy of maturity, as the use of machine learning and artificial intelligence emerges, progresses, or becomes established. This article is categorized under: Application Areas > Science and Technology Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining Technologies > Machine Learning} }
@article{WOS:000510903200065, title = {Correlated Differential Privacy: Feature Selection in Machine Learning}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {16}, pages = {2115-2124}, year = {2020}, issn = {1551-3203}, doi = {10.1109/TII.2019.2936825}, author = {Zhang, Tao and Zhu, Tianqing and Xiong, Ping and Huo, Huan and Tari, Zahir and Zhou, Wanlei}, abstract = {Privacy preserving in machine learning is a crucial issue in industry informatics since data used for training in industries usually contain sensitive information. Existing differentially private machine learning algorithms have not considered the impact of data correlation, which may lead to more privacy leakage than expected in industrial applications. For example, data collected for traffic monitoring may contain some correlated records due to temporal correlation or user correlation. To fill this gap, in this article, we propose a correlation reduction scheme with differentially private feature selection considering the issue of privacy loss when data have correlation in machine learning tasks. The proposed scheme involves five steps with the goal of managing the extent of data correlation, preserving the privacy, and supporting accuracy in the prediction results. In this way, the impact of data correlation is relieved with the proposed feature selection scheme, and moreover the privacy issue of data correlation in learning is guaranteed. The proposed method can be widely used in machine learning algorithms, which provide services in industrial areas. Experiments show that the proposed scheme can produce better prediction results with machine learning tasks and fewer mean square errors for data queries compared to existing schemes.} }
@article{WOS:000582822600018, title = {Machine learning in GI endoscopy: practical guidance in how to interpret a novel field}, journal = {GUT}, volume = {69}, pages = {2035-2045}, year = {2020}, issn = {0017-5749}, doi = {10.1136/gutjnl-2019-320466}, author = {van der Sommen, Fons and de Groof, Jeroen and Struyvenberg, Maarten and van der Putten, Joost and Boers, Tim and Fockens, Kiki and Schoon, Erik J. and Curvers, Wouter and de With, Peter and Mori, Yuichi and Byrne, Michael and Bergman, Jacques J. G. H. M.}, abstract = {There has been a vast increase in GI literature focused on the use of machine learning in endoscopy. The relative novelty of this field poses a challenge for reviewers and readers of GI journals. To appreciate scientific quality and novelty of machine learning studies, understanding of the technical basis and commonly used techniques is required. Clinicians often lack this technical background, while machine learning experts may be unfamiliar with clinical relevance and implications for daily practice. Therefore, there is an increasing need for a multidisciplinary, international evaluation on how to perform high-quality machine learning research in endoscopy. This review aims to provide guidance for readers and reviewers of peer-reviewed GI journals to allow critical appraisal of the most relevant quality requirements of machine learning studies. The paper provides an overview of common trends and their potential pitfalls and proposes comprehensive quality requirements in six overarching themes: terminology, data, algorithm description, experimental setup, interpretation of results and machine learning in clinical practice.} }
@article{WOS:000911287700001, title = {Review of interpretable machine learning for process industries}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {170}, pages = {647-659}, year = {2023}, issn = {0957-5820}, doi = {10.1016/j.psep.2022.12.018}, author = {Carter, A. and Imtiaz, S. and Naterer, G. F.}, abstract = {This review article examines recent advances in the use of machine learning for process industries. The article presents common process industry tasks that researchers are solving with machine learning techniques. It then identifies a lack of consensus among past studies when selecting an appropriate model given a prescribed application. Furthermore, the article identifies that relatively few past studies have considered model inter-pretability - a ``black-box'' challenge holding back machine learning's implementation in more high-risk in-dustrial applications. This interdisciplinary field of engineering and computer science is still reasonably young. Additional research is recommended to standardize methods and establish a strategic framework to manage risk during adoption of machine learning models.} }
@article{WOS:001527924800002, title = {Domain knowledge in artificial intelligence: Using conceptual modeling to increase machine learning accuracy and explainability}, journal = {DATA \\& KNOWLEDGE ENGINEERING}, volume = {160}, year = {2025}, issn = {0169-023X}, doi = {10.1016/j.datak.2025.102482}, author = {Storey, Veda C. and Parsons, Jeffrey and Bueso, Arturo Castellanos and Tremblay, Monica Chiarini and Lukyanenko, Roman and Castillo, Alfred and Maass, Wolfgang}, abstract = {Machine learning enables the extraction of useful information from large, diverse datasets. However, despite many successful applications, machine learning continues to suffer from performance and transparency issues. These challenges can be partially attributed to the limited use of domain knowledge by machine learning models. This research proposes using the domain knowledge represented in conceptual models to improve the preparation of the data used to train machine learning models. We develop and demonstrate a method, called the Conceptual Modeling for Machine Learning (CMML), which is comprised of guidelines for data preparation in machine learning and based on conceptual modeling constructs and principles. To assess the impact of CMML on machine learning outcomes, we first applied it to two real-world problems to evaluate its impact on model performance. We then solicited an assessment by data scientists on the applicability of the method. These results demonstrate the value of CMML for improving machine learning outcomes.} }
@article{WOS:000811299400001, title = {Machine Learning Advances in Microbiology: A Review of Methods and Applications}, journal = {FRONTIERS IN MICROBIOLOGY}, volume = {13}, year = {2022}, doi = {10.3389/fmicb.2022.925454}, author = {Jiang, Yiru and Luo, Jing and Huang, Danqing and Liu, Ya and Li, Dan-dan}, abstract = {Microorganisms play an important role in natural material and elemental cycles. Many common and general biology research techniques rely on microorganisms. Machine learning has been gradually integrated with multiple fields of study. Machine learning, including deep learning, aims to use mathematical insights to optimize variational functions to aid microbiology using various types of available data to help humans organize and apply collective knowledge of various research objects in a systematic and scaled manner. Classification and prediction have become the main achievements in the development of microbial community research in the direction of computational biology. This review summarizes the application and development of machine learning and deep learning in the field of microbiology and shows and compares the advantages and disadvantages of different algorithm tools in four fields: microbiome and taxonomy, microbial ecology, pathogen and epidemiology, and drug discovery.} }
@article{WOS:000939150800001, title = {Adversarial Machine Learning Attacks against Intrusion Detection Systems: A Survey on Strategies and Defense}, journal = {FUTURE INTERNET}, volume = {15}, year = {2023}, issn = {1999-5903}, doi = {10.3390/fi15020062}, author = {Alotaibi, Afnan and Rassam, Murad A.}, abstract = {Concerns about cybersecurity and attack methods have risen in the information age. Many techniques are used to detect or deter attacks, such as intrusion detection systems (IDSs), that help achieve security goals, such as detecting malicious attacks before they enter the system and classifying them as malicious activities. However, the IDS approaches have shortcomings in misclassifying novel attacks or adapting to emerging environments, affecting their accuracy and increasing false alarms. To solve this problem, researchers have recommended using machine learning approaches as engines for IDSs to increase their efficacy. Machine-learning techniques are supposed to automatically detect the main distinctions between normal and malicious data, even novel attacks, with high accuracy. However, carefully designed adversarial input perturbations during the training or testing phases can significantly affect their predictions and classifications. Adversarial machine learning (AML) poses many cybersecurity threats in numerous sectors that use machine-learning-based classification systems, such as deceiving IDS to misclassify network packets. Thus, this paper presents a survey of adversarial machine-learning strategies and defenses. It starts by highlighting various types of adversarial attacks that can affect the IDS and then presents the defense strategies to decrease or eliminate the influence of these attacks. Finally, the gaps in the existing literature and future research directions are presented.} }
@article{WOS:000774530000001, title = {Application of Soft Computing Techniques to Predict the Strength of Geopolymer Composites}, journal = {POLYMERS}, volume = {14}, year = {2022}, doi = {10.3390/polym14061074}, author = {Wang, Qichen and Ahmad, Waqas and Ahmad, Ayaz and Aslam, Fahid and Mohamed, Abdullah and Vatin, Nikolai Ivanovich}, abstract = {Geopolymers may be the best alternative to ordinary Portland cement because they are manufactured using waste materials enriched in aluminosilicate. Research on geopolymer composites is accelerating. However, considerable work, expense, and time are needed to cast, cure, and test specimens. The application of computational methods to the stated objective is critical for speedy and cost-effective research. In this study, supervised machine learning approaches were employed to predict the compressive strength of geopolymer composites. One individual machine learning approach, decision tree, and two ensembled machine learning approaches, AdaBoost and random forest, were used. The coefficient correlation (R-2), statistical tests, and k-fold analysis were used to determine the validity and comparison of all models. It was discovered that ensembled machine learning techniques outperformed individual machine learning techniques in forecasting the compressive strength of geopolymer composites. However, the outcomes of the individual machine learning model were also within the acceptable limit. R-2 values of 0.90, 0.90, and 0.83 were obtained for AdaBoost, random forest, and decision models, respectively. The models' decreased error values, such as mean absolute error, mean absolute percentage error, and root-mean-square errors, further confirmed the ensembled machine learning techniques' increased precision. Machine learning approaches will aid the building industry by providing quick and cost-effective methods for evaluating material properties.} }
@article{WOS:000520045900001, title = {Multiscale Modeling Meets Machine Learning: What Can We Learn?}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {1017-1037}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09405-5}, author = {Peng, Grace C. Y. and Alber, Mark and Tepole, Adrian Buganza and Cannon, William and De, Suvranu and Dura-Bernal, Salvador and Garikipati, Krishna and Karniadakis, George and Lytton, William W. and Perdikaris, Paris and Petzold, Linda and Kuhl, Ellen}, abstract = {Machine learning is increasingly recognized as a promising technology in the biological, biomedical, and behavioral sciences. There can be no argument that this technique is incredibly successful in image recognition with immediate applications in diagnostics including electrophysiology, radiology, or pathology, where we have access to massive amounts of annotated data. However, machine learning often performs poorly in prognosis, especially when dealing with sparse data. This is a field where classical physics-based simulation seems to remain irreplaceable. In this review, we identify areas in the biomedical sciences where machine learning and multiscale modeling can mutually benefit from one another: Machine learning can integrate physics-based knowledge in the form of governing equations, boundary conditions, or constraints to manage ill-posted problems and robustly handle sparse and noisy data; multiscale modeling can integrate machine learning to create surrogate models, identify system dynamics and parameters, analyze sensitivities, and quantify uncertainty to bridge the scales and understand the emergence of function. With a view towards applications in the life sciences, we discuss the state of the art of combining machine learning and multiscale modeling, identify applications and opportunities, raise open questions, and address potential challenges and limitations. We anticipate that it will stimulate discussion within the community of computational mechanics and reach out to other disciplines including mathematics, statistics, computer science, artificial intelligence, biomedicine, systems biology, and precision medicine to join forces towards creating robust and efficient models for biological systems.} }
@article{WOS:000940478300001, title = {A Survey of Underwater Acoustic Target Recognition Methods Based on Machine Learning}, journal = {JOURNAL OF MARINE SCIENCE AND ENGINEERING}, volume = {11}, year = {2023}, doi = {10.3390/jmse11020384}, author = {Luo, Xinwei and Chen, Lu and Zhou, Hanlu and Cao, Hongli}, abstract = {Underwater acoustic target recognition (UATR) technology has been implemented widely in the fields of marine biodiversity detection, marine search and rescue, and seabed mapping, providing an essential basis for human marine economic and military activities. With the rapid development of machine-learning-based technology in the acoustics field, these methods receive wide attention and display a potential impact on UATR problems. This paper reviews current UATR methods based on machine learning. We focus mostly, but not solely, on the recognition of target-radiated noise from passive sonar. First, we provide an overview of the underwater acoustic acquisition and recognition process and briefly introduce the classical acoustic signal feature extraction methods. In this paper, recognition methods for UATR are classified based on the machine learning algorithms used as UATR technologies using statistical learning methods, UATR methods based on deep learning models, and transfer learning and data augmentation technologies for UATR. Finally, the challenges of UATR based on the machine learning method are summarized and directions for UATR development in the future are put forward.} }
@article{WOS:001387703100001, title = {Machine Learning Advances in High-Entropy Alloys: A Mini-Review}, journal = {ENTROPY}, volume = {26}, year = {2024}, doi = {10.3390/e26121119}, author = {Sun, Yibo and Ni, Jun}, abstract = {The efficacy of machine learning has increased exponentially over the past decade. The utilization of machine learning to predict and design materials has become a pivotal tool for accelerating materials development. High-entropy alloys are particularly intriguing candidates for exemplifying the potency of machine learning due to their superior mechanical properties, vast compositional space, and intricate chemical interactions. This review examines the general process of developing machine learning models. The advances and new algorithms of machine learning in the field of high-entropy alloys are presented in each part of the process. These advances are based on both improvements in computer algorithms and physical representations that focus on the unique ordering properties of high-entropy alloys. We also show the results of generative models, data augmentation, and transfer learning in high-entropy alloys and conclude with a summary of the challenges still faced in machine learning high-entropy alloys today.} }
@article{WOS:001102417300001, title = {Artificial Intelligence and Machine Learning in Metallurgy. Part 1. Methods and Algorithms}, journal = {METALLURGIST}, volume = {67}, pages = {886-894}, year = {2023}, issn = {0026-0894}, doi = {10.1007/s11015-023-01576-3}, author = {Muntin, A. V. and Zhikharev, P. Yu. and Ziniagin, A. G. and Brayko, D. A.}, abstract = {The article contains information about machine learning methods used in modern metallurgy. The description of machine learning methods and their role in the processing of ``big data'' formed at metallurgical enterprises are given. The topic relevance has to do with the effectiveness of solving problems aimed at improving production processes using artificial intelligence and machine learning in various metallurgical processing stages.} }
@article{WOS:001137233600005, title = {Techniques and applications of Machine Learning and Artificial Intelligence in education: a systematic review}, journal = {RIED-REVISTA IBEROAMERICANA DE EDUCACION A DISTANCIA}, volume = {27}, year = {2024}, issn = {1138-2783}, doi = {10.5944/ried.27.1.37491}, author = {Forero-Corba, Wiston and Bennasar, Francisca Negre}, abstract = {Machine learning is a field of artificial intelligence that is impacting lately in all areas of knowledge. The areas of social sciences, especially education, are no stranger to it, so, a systematic review of the literature on the techniques and applications of machine learning and artificial intelligence in Education is performed. The lack of knowledge and skills of educators in machine learning and artificial intelligence limits the optimal implementation of these technologies in education. The objective of this research is to identify opportunities for improving teaching-learning processes and educational management at all levels of the educational context through the application of machine learning and artificial intelligence. The databases used for the bibliographic search were Web of Science and Scopus and the methodology applied is based on the PRISMA statement for obtaining and analyzing 55 articles published in high impact journals between the years 2021-2023. The results showed that the studies addressed a total of 33 machine learning and artificial intelligence techniques and multiple applications that were implemented in educational contexts at primary, secondary and higher education levels in 38 countries. The conclusions showed the strong impact of the use of machine learning and artificial intelligence. This impact is reflected in the use of different intelligent techniques in educational contexts and the increase of research in secondary schools on artificial intelligence.} }
@article{WOS:001126204200001, title = {Incorporating soil knowledge into machine-learning prediction of soil properties from soil spectra}, journal = {EUROPEAN JOURNAL OF SOIL SCIENCE}, volume = {74}, year = {2023}, issn = {1351-0754}, doi = {10.1111/ejss.13438}, author = {Ma, Yuxin and Minasny, Budiman and Dematte, Jose A. M. and McBratney, Alex B.}, abstract = {Various machine-learning models have been extensively applied to predict soil properties using infrared spectroscopy. Beyond the interpretability and transparency of these models, there is an ongoing discussion on the reliability of the prediction of soil properties generated from soil spectra. In this review, we contribute to this discussion by advocating for the integration of soil knowledge into machine-learning models. By doing so, researchers can delve deeper into the underlying soil constituents, ultimately enhancing prediction accuracy. Our review explores the soil information present in spectral data, the fallacy of model interpretability, methods to incorporate soil knowledge into machine-learning techniques, and the ways in which machine learning and soil spectroscopy can assist soil science. The combination of machine learning and domain knowledge is recommended to develop more meaningful models for predicting soil properties within the field of soil science.} }
@article{WOS:000730393300003, title = {Prospects of Artificial Intelligence and Machine Learning Application in Banking Risk Management}, journal = {JOURNAL OF CENTRAL BANKING THEORY AND PRACTICE}, volume = {10}, pages = {41-57}, year = {2021}, issn = {1800-9581}, doi = {10.2478/jcbtp-2021-0023}, author = {Milojevic, Nenad and Redzepagic, Srdjan}, abstract = {Artificial intelligence and machine learning have increasing influence on the financial sector, but also on economy as a whole. The impact of artificial intelligence and machine learning on banking risk management has become particularly interesting after the global financial crisis. The research focus is on artificial intelligence and machine learning potential for further banking risk management improvement. The paper seeks to explore the possibility for successful implementation yet taking into account challenges and problems which might occur as well as potential solutions. Artificial intelligence and machine learning have potential to support the mitigation measures for the contemporary global economic and financial challenges, including those caused by the COVID-19 crisis. The main focus in this paper is on credit risk management, but also on analysing artificial intelligence and machine learning application in other risk management areas. It is concluded that a measured and well-prepared further application of artificial intelligence, machine learning, deep learning and big data analytics can have further positive impact, especially on the following risk management areas: credit, market, liquidity, operational risk, and other related areas.} }
@article{WOS:000557470100001, title = {Machine learning as ecology}, journal = {JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL}, volume = {53}, year = {2020}, issn = {1751-8113}, doi = {10.1088/1751-8121/ab956e}, author = {Howell, Owen and Cui Wenping and Marsland, III, Robert and Mehta, Pankaj}, abstract = {Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms-including support vector machines (SVMs)-have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.} }
@article{WOS:000576782300009, title = {Corporate default forecasting with machine learning}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {161}, year = {2020}, issn = {0957-4174}, doi = {10.1016/j.eswa.2020.113567}, author = {Moscatelli, Mirko and Parlapiano, Fabio and Narizzano, Simone and Viggiano, Gianluca}, abstract = {We analyze the performance of a set of machine learning models in predicting default risk, using standard statistical models, such as the logistic regression, as a benchmark. When only a limited information set is available, for example in the case of an external assessment of credit risk, we find that machine learning models provide substantial gains in discriminatory power and precision, relative to statistical models. This advantage diminishes when confidential information, such as credit behavioral indicators, is also available, and it becomes negligible when the dataset is small. Moreover, we evaluate the consequences of using a credit allocation rule based on machine learning ratings on the overall supply of credit and the number of borrowers gaining access to credit. Machine learning models concentrate a greater extent of credit towards safer and larger borrowers, which would result in lower credit losses for their lenders. (c) 2020 Elsevier Ltd. All rights reserved.} }
@article{WOS:001100527300001, title = {Air Quality Index prediction using machine learning for Ahmedabad city}, journal = {DIGITAL CHEMICAL ENGINEERING}, volume = {7}, year = {2023}, issn = {2772-5081}, doi = {10.1016/j.dche.2023.100093}, author = {Maltare, Nilesh N. and Vahora, Safvan}, abstract = {Prediction of air pollution index may help in traffic routing and identifying serious pollutants. Modeling of the complex relationships between these variables by sophisticated methods in machine learning is a promising field. The objective of this work is to compare the various machine learning methods such as SARIMA, SVM and LSTM for the prediction of air quality index for Ahmedabad city of Gujarat, India. In this research, different preprocessing methods are used to manage the data before providing to the machine learning models. This study is carried out based on the data provided by the Central Pollution Control Board of India and it focuses on the support vector machine algorithm with RBF kernel model. So, that the results availed are comparatively better as compared to other kernels of the support vector machine models as well as SARIMA and LSTM models for Ahmedabad city.} }
@article{WOS:000603445400001, title = {Novel Method of Classification in Knee Osteoarthritis: Machine Learning Application Versus Logistic Regression Model}, journal = {ANNALS OF REHABILITATION MEDICINE-ARM}, volume = {44}, pages = {415-427}, year = {2020}, issn = {2234-0645}, doi = {10.5535/arm.20071}, author = {Yang, Jung Ho and Park, Jae Hyeon and Jang, Seong-Ho and Cho, Jaesung}, abstract = {Objective To present new classification methods of knee osteoarthritis (KOA) using machine learning and compare its performance with conventional statistical methods as classification techniques using machine learning have recently been developed. Methods A total of 84 KOA patients and 97 normal participants were recruited. KOA patients were clustered into three groups according to the Kellgren-Lawrence (K-L) grading system. All subjects completed gait trials under the same experimental conditions. Machine learning- based classification using the support vector machine (SVM) classifier was performed to classify KOA patients and the severity of KOA. Logistic regression analysis was also performed to compare the results in classifying KOA patients with machine learning method. Results In the classification between KOA patients and normal subjects, the accuracy of classification was higher in machine learning method than in logistic regression analysis. In the classification of KOA severity, accuracy was enhanced through the feature selection process in the machine learning method. The most significant gait feature for classification was flexion and extension of the knee in the swing phase in the machine learning method. Conclusion The machine learning method is thought to be a new approach to complement conventional logistic regression analysis in the classification of KOA patients. It can be clinically used for diagnosis and gait correction of KOA patients.} }
@article{WOS:000865315000004, title = {Application of Deep Learning on the Prognosis of Cutaneous Melanoma Based on Full Scan Pathology Images}, journal = {BIOMED RESEARCH INTERNATIONAL}, volume = {2022}, year = {2022}, issn = {2314-6133}, doi = {10.1155/2022/4864485}, author = {Li, Anhai and Li, Xiaoyuan and Li, Wenwen and Yu, Xiaoqian and Qi, Mengmeng and Li, Ding}, abstract = {Introduction. The purpose of this study is to use deep learning and machine learning to learn and classify patients with cutaneous melanoma with different prognoses and to explore the application value of deep learning in the prognosis of cutaneous melanoma patients. Methods. In deep learning, VGG-19 is selected as the network architecture and learning model for learning and classification. In machine learning, deep features are extracted through the VGG-19 network architecture, and the support vector machine (SVM) model is selected for learning and classification. Compare and explore the application value of deep learning and machine learning in predicting the prognosis of patients with cutaneous melanoma. Result. According to receiver operating characteristic (ROC) curves and area under the curve (AUC), the average accuracy of deep learning is higher than that of machine learning, and even the lowest accuracy is better than that of machine learning. Conclusion. As the number of learning increases, the accuracy of machine learning and deep learning will increase, but in the same number of cutaneous melanoma patient pathology maps, the accuracy of deep learning will be higher. This study provides new ideas and theories for computational pathology in predicting the prognosis of patients with cutaneous melanoma.} }
@article{WOS:001222668700001, title = {Making the most of AI and machine learning in organizations and strategy research: Supervised machine learning, causal inference, and matching models}, journal = {STRATEGIC MANAGEMENT JOURNAL}, volume = {45}, pages = {1926-1953}, year = {2024}, issn = {0143-2095}, doi = {10.1002/smj.3604}, author = {Rathje, Jason and Katila, Riitta and Reineke, Philipp}, abstract = {We spotlight the use of machine learning in two-stage matching models to deal with sample selection bias. Recent advances in machine learning have unlocked new empirical possibilities for inductive theorizing. In contrast, the opportunities to use machine learning in regression studies involving large-scale data with many covariates and a causal claim are still less well understood. Our core contribution is to guide researchers in the use of machine learning approaches to choosing matching variables for enhanced causal inference in propensity score matching models. We use an analysis of real-world technology invention data of public-private relationships to demonstrate the method and find that machine learning can provide an alternative approach to ad hoc matching. However, as with any method, it is also important to understand its limitations. This article explores the use of machine learning to enhance decision-making, particularly in addressing sample selection bias in large-scale datasets. The rapid development of AI and machine learning offers new, powerful tools especially for digital ecosystems where complex data and causal relationships are complex to analyze. We offer managers and stakeholders insight into the effective integration of machine learning for selecting critical variables in propensity score matching models. Through a detailed examination of real-world data on technology inventions within public-private relationships, we demonstrate the effectiveness of machine learning as a robust alternative to traditional matching methods.} }
@article{WOS:000526339000051, title = {Quantum Chemistry in the Age of Machine Learning}, journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS}, volume = {11}, pages = {2336-2347}, year = {2020}, issn = {1948-7185}, doi = {10.1021/acs.jpclett.9b03664}, author = {Dral, Pavlo O.}, abstract = {As the quantum chemistry (QC) community embraces machine learning (ML), the number of new methods and applications based on the combination of QC and ML is surging. In this Perspective, a view of the current state of affairs in this new and exciting research field is offered, challenges of using machine learning in quantum chemistry applications are described, and potential future developments are outlined. Specifically, examples of how machine learning is used to improve the accuracy and accelerate quantum chemical research are shown. Generalization and classification of existing techniques are provided to ease the navigation in the sea of literature and to guide researchers entering the field. The emphasis of this Perspective is on supervised machine learning.} }
@article{WOS:000519447700007, title = {Machine Learning in Chemical Dynamics}, journal = {RESONANCE-JOURNAL OF SCIENCE EDUCATION}, volume = {25}, pages = {59-75}, year = {2020}, issn = {0971-8044}, doi = {10.1007/s12045-019-0922-1}, author = {Biswas, Rupayan and Rashmi, Richa and Lourderaj, Upakarasamy}, abstract = {Machine learning has been applied to various fields and is envisaged as the technology of the future. We discuss here, the applications of machine learning methods to represent potential energy surfaces - an important aspect of chemical dynamics. We illustrate the process of machine learning using simple examples, and demonstrate how it can be extended to complicated problems.} }
@article{WOS:000468604900004, title = {Survey of Machine Learning Techniques in Drug Discovery}, journal = {CURRENT DRUG METABOLISM}, volume = {20}, pages = {185-193}, year = {2019}, issn = {1389-2002}, doi = {10.2174/1389200219666180820112457}, author = {Stephenson, Natalie and Shane, Emily and Chase, Jessica and Rowland, Jason and Ries, David and Justice, Nicola and Zhang, Jie and Chan, Leong and Cao, Renzhi}, abstract = {Background: Drug discovery, which is the process of discovering new candidate medications, is very important for pharmaceutical industries. At its current stage, discovering new drugs is still a very expensive and time-consuming process, requiring Phases I, II and III for clinical trials. Recently, machine learning techniques in Artificial Intelligence (AI), especially the deep learning techniques which allow a computational model to generate multiple layers, have been widely applied and achieved state-of-the-art performance in different fields, such as speech recognition, image classification, bioinformatics, etc. One very important application of these AI techniques is in the field of drug discovery. Methods: We did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and start-up companies to understand current status of machine learning techniques in drug discovery. Results: Our experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year. Conclusion: The main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey.} }
@article{WOS:001198662400026, title = {Mechanism for feature learning in neural networks and backpropagation-free machine learning models}, journal = {SCIENCE}, volume = {383}, pages = {1461-1467}, year = {2024}, issn = {0036-8075}, doi = {10.1126/science.adi5639}, author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail}, abstract = {Understanding how neural networks learn features, or relevant patterns in data, for prediction is necessary for their reliable use in technological and scientific applications. In this work, we presented a unifying mathematical mechanism, known as average gradient outer product (AGOP), that characterized feature learning in neural networks. We provided empirical evidence that AGOP captured features learned by various neural network architectures, including transformer-based language models, convolutional networks, multilayer perceptrons, and recurrent neural networks. Moreover, we demonstrated that AGOP, which is backpropagation-free, enabled feature learning in machine learning models, such as kernel machines, that a priori could not identify task-specific features. Overall, we established a fundamental mechanism that captured feature learning in neural networks and enabled feature learning in general machine learning models.} }
@article{WOS:000704764200003, title = {On-line chatter detection in milling with hybrid machine learning and physics-based model}, journal = {CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY}, volume = {35}, pages = {25-40}, year = {2021}, issn = {1755-5817}, doi = {10.1016/j.cirpj.2021.05.006}, author = {Rahimi, M. Hossein and Huynh, Hoai Nam and Altintas, Yusuf}, abstract = {Unstable vibrations, chatter, in machining lead to poor surface finish and damage to the tool and machine. It is desired to detect and avoid chatter on-line without false alarms for improved productivity. This paper presents the application of a combined machine learning network and physics-based model to detect chatter in milling. The vibration data collected during machining is converted into moving short-time frequency spectrums, whose features are mapped to five machining states as air cut, entry into and exit from the workpiece, stable cut, and chatter conditions. The machine learning network was trained and its architecture was reduced to a computationally optimal network with 3 convolution blocks followed by a neural network with one hidden layer. A parallel algorithm, which Kalman filters the stable forced vibrations to isolate chatter signals in raw data, is used to detect the chatter and its frequency. The combination of the machine learning and physics-based model led to a 98.90\\% success rate in chatter detection while allowing to further train the network during production with the help of the physics-based, deterministic model. (c) 2021 CIRP.} }
@article{WOS:000456929100002, title = {Machine learning, deep learning and Python language in field of geology}, journal = {ACTA PETROLOGICA SINICA}, volume = {34}, pages = {3173-3178}, year = {2018}, issn = {1000-0569}, author = {Zhou YongZhang and Wang Jun and Zuo RenGuang and Xiao Fan and Shen WenJie and Wang ShuGong}, abstract = {Geological big data is exponentially expanding. It is the only way to catch up with its extraordinary growing to develop intelligent data processing. As the core of artificial intelligence, machine learning is a fundamental way to endow computer with intelligence. Machine learning has been becoming the front hotspot of geological big data mining. It will attach wings to geological big data mining, and thereby bring revolution to geological research. Machine learning is a data adaptive training process and model, resulting in giving a good performance decision. As a subclass of machine learning, deep learning develops machine learning model with various hidden layers, and makes iterative evolution of the model through massive data training, and finally extracts essential features to help more exactly classing and predicting. The convolution neural network is one of the most frequently used deep learning algorithms. It is widely used in image recognition and speech analysis. Python language is playing an increasingly important role in science research. The Python Scikit-Learn is a machine learning-oriented library to provide with data preprocessing, classification, regression, clustering, prediction, model analysis and other modules. The Keras is a Python deep learning library based on Theano and Tensorflow, and can be used to construct concise artificial neural network.} }
@article{WOS:000696993100003, title = {Advancements within Modern Machine Learning Methodology: Impacts and Prospects in Biomarker Discovery}, journal = {CURRENT MEDICINAL CHEMISTRY}, volume = {28}, pages = {6512-6531}, year = {2021}, issn = {0929-8673}, doi = {10.2174/0929867328666210208111821}, author = {Ledesma, Dakila and Symes, Steven and Richards, Sean}, abstract = {Background: The adoption of biomarkers as part of high-throughput, complex microarray or sequencing data has necessitated the discovery and validation of these data through machine learning. Machine learning has remained a fundamental and indispensable tool due to its efficacy and efficiency in both feature extraction of relevant biomarkers as well as the classification of samples as validation of the discovered biomarkers. Objectives: This review aims to present the impact and ability of various machine learning methodologies and models to process high-throughput, high-dimensionality data found within mass spectrometry, microarray, and DNA/RNA-sequence data; data that precluded biomarker discovery prior to the use of machine learning. Methods: A vast array of literature highlighting machine learning for biomarker discovery was reviewed, resulting in the eligibility of 21 machine learning algorithms/networks and 3 combinatory architectures, spanning 17 fields of study. This literature was screened to investigate the usage and development of machine learning within the framework of biomarker discovery. Results: Out of the 93 papers collected, a total of 62 biomarker studies were further reviewed across different subfields-49 of which employed machine learning algorithms, and 13 of which employed neural network-based models. Through the application, innovation, and creation of tools in biomarker-related machine learning methodologies, its use allowed for the discovery, accumulation, validation, and interpretation of biomarkers within varied data formats, sources, as well as fields of study. Conclusion: The use of machine learning methodologies for biomarker discovery is critical to the analysis of various types of data used for biomarker discovery, such as mass spectrometry, nucleotide and protein sequencing, and image (e.g. CT-scan) data. Further studies containing more standardized techniques for evaluation, and the use of cutting-edge machine learning architectures may lead to more accurate and specific results.} }
@article{WOS:000993072400001, title = {Machine learning to enhance the management of highway pavements and bridges}, journal = {INFRASTRUCTURE ASSET MANAGEMENT}, volume = {11}, pages = {119-127}, year = {2023}, issn = {2053-0242}, doi = {10.1680/jinam.22.00031}, author = {Bashar, Mohammad Z. and Torres-Machi, Cristina}, abstract = {The adoption of machine learning in transportation asset management is hindered by the perception of being a black box, the natural resistance to change, and the challenges of integration with existing management systems. This paper aims to enhance the understanding of machine learning and provide guidance for the development and implementation of machine learning to support decision-making in the management of highway pavements and bridges. The paper identifies successful research efforts using machine learning, identifies opportunities and challenges in adopting machine learning, and derives recommendations on when and how to apply different machine learning algorithms to support asset management decisions. Four main challenges were identified: the trade-off between accuracy and interpretability, the shortage of machine learning engineers, data quality, and the limitations of machine learning algorithms. Although the complexities associated with training machine learning algorithms challenge the short-term implementation, machine learning offer a wide range of opportunities when compared to traditional approaches. The development of hybrid systems combining machine learning algorithms with expert opinions and traditional approaches seems a reasonable step forward to support agencies asset management decisions.} }
@article{WOS:000856997600004, title = {Explainable machine learning in materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {8}, year = {2022}, doi = {10.1038/s41524-022-00884-7}, author = {Zhong, Xiaoting and Gallagher, Brian and Liu, Shusen and Kailkhura, Bhavya and Hiszpanski, Anna and Han, T. Yong-Jin}, abstract = {Machine learning models are increasingly used in materials studies because of their exceptional accuracy. However, the most accurate machine learning models are usually difficult to explain. Remedies to this problem lie in explainable artificial intelligence (XAI), an emerging research field that addresses the explainability of complicated machine learning models like deep neural networks (DNNs). This article attempts to provide an entry point to XAI for materials scientists. Concepts are defined to clarify what explain means in the context of materials science. Example works are reviewed to show how XAI helps materials science research. Challenges and opportunities are also discussed.} }
@article{WOS:000532822600001, title = {Machine Learning and Psychological Research: The Unexplored Effect of Measurement}, journal = {PERSPECTIVES ON PSYCHOLOGICAL SCIENCE}, volume = {15}, pages = {809-816}, year = {2020}, issn = {1745-6916}, doi = {10.1177/1745691620902467}, author = {Jacobucci, Ross and Grimm, Kevin J.}, abstract = {Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science.} }
@article{WOS:001248184000003, title = {A systematic review of machine learning methods in software testing}, journal = {APPLIED SOFT COMPUTING}, volume = {162}, year = {2024}, issn = {1568-4946}, doi = {10.1016/j.asoc.2024.111805}, author = {Ajorloo, Sedighe and Jamarani, Amirhossein and Kashfi, Mehdi and Kashani, Mostafa Haghi and Najafizadeh, Abbas}, abstract = {Background: The quest for higher software quality remains a paramount concern in software testing, prompting a shift towards leveraging machine learning techniques for enhanced testing efficacy. Objective: The objective of this paper is to identify, categorize, and systematically compare the present studies on software testing utilizing machine learning methods. Method: This study conducts a systematic literature review (SLR) of 40 pertinent studies spanning from 2018 to March 2024 to comprehensively analyze and classify machine learning methods in software testing. The review encompasses supervised learning, unsupervised learning, reinforcement learning, and hybrid learning approaches. Results: The strengths and weaknesses of each reviewed paper are dissected in this study. This paper also provides an in-depth analysis of the merits of machine learning methods in the context of software testing and addresses current unresolved issues. Potential areas for future research have been discussed, and statistics of each review paper have been collected. Conclusion: By addressing these aspects, this study contributes to advancing the discourse on machine learning's role in software testing and paves the way for substantial improvements in testing efficacy and software quality.} }
@article{WOS:000529286600001, title = {Industry-scale application and evaluation of deep learning for drug target prediction}, journal = {JOURNAL OF CHEMINFORMATICS}, volume = {12}, year = {2020}, issn = {1758-2946}, doi = {10.1186/s13321-020-00428-5}, author = {Sturm, Noe and Mayr, Andreas and Thanh Le Van and Chupakhin, Vladimir and Ceulemans, Hugo and Wegner, Joerg and Golib-Dzib, Jose-Felipe and Jeliazkova, Nina and Vandriessche, Yves and Bohm, Stanislav and Cima, Vojtech and Martinovic, Jan and Greene, Nigel and Vander Aa, Tom and Ashby, Thomas J. and Hochreiter, Sepp and Engkvist, Ola and Klambauer, Guenter and Chen, Hongming}, abstract = {Artificial intelligence (AI) is undergoing a revolution thanks to the breakthroughs of machine learning algorithms in computer vision, speech recognition, natural language processing and generative modelling. Recent works on publicly available pharmaceutical data showed that AI methods are highly promising for Drug Target prediction. However, the quality of public data might be different than that of industry data due to different labs reporting measurements, different measurement techniques, fewer samples and less diverse and specialized assays. As part of a European funded project (ExCAPE), that brought together expertise from pharmaceutical industry, machine learning, and high-performance computing, we investigated how well machine learning models obtained from public data can be transferred to internal pharmaceutical industry data. Our results show that machine learning models trained on public data can indeed maintain their predictive power to a large degree when applied to industry data. Moreover, we observed that deep learning derived machine learning models outperformed comparable models, which were trained by other machine learning algorithms, when applied to internal pharmaceutical company datasets. To our knowledge, this is the first large-scale study evaluating the potential of machine learning and especially deep learning directly at the level of industry-scale settings and moreover investigating the transferability of publicly learned target prediction models towards industrial bioactivity prediction pipelines.} }
@article{WOS:000462654200002, title = {What can Android mobile app developers do about the energy consumption of machine learning?}, journal = {EMPIRICAL SOFTWARE ENGINEERING}, volume = {24}, pages = {562-601}, year = {2019}, issn = {1382-3256}, doi = {10.1007/s10664-018-9629-2}, author = {McIntosh, Andrea and Hassan, Safwat and Hindle, Abram}, abstract = {Machine learning is a popular method of learning functions from data to represent and to classify sensor inputs, multimedia, emails, and calendar events. Smartphone applications have been integrating more and more intelligence in the form of machine learning. Machine learning functionality now appears on most smartphones as voice recognition, spell checking, word disambiguation, face recognition, translation, spatial reasoning, and even natural language summarization. Excited app developers who want to use machine learning on mobile devices face one serious constraint that they did not face on desktop computers or cloud virtual machines: the end-user's mobile device has limited battery life, thus computationally intensive tasks can harm end users' phone availability by draining batteries of their stored energy. Currently, there are few guidelines for developers who want to employ machine learning on mobile devices yet are concerned about software energy consumption of their applications. In this paper, we combine empirical measurements of different machine learning algorithm implementations with complexity theory to provide concrete and theoretically grounded recommendations to developers who want to employ machine learning on smartphones. We conclude that some implementations of algorithms, such as J48, MLP, and SMO, do generally perform better than others in terms of energy consumption and accuracy, and that energy consumption is well-correlated to algorithmic complexity. However, to achieve optimal results a developer must consider their specific application as many factors dataset size, number of data attributes, whether the model will require updating, etc. affect which machine learning algorithm and implementation will provide the best results.} }
@article{WOS:000989319200011, title = {Artificial Intelligence and Machine Learning in Clinical Medicine, 2023}, journal = {NEW ENGLAND JOURNAL OF MEDICINE}, volume = {388}, pages = {1201-1208}, year = {2023}, issn = {0028-4793}, doi = {10.1056/NEJMra2302038}, author = {Haug, Charlotte J. J. and Drazen, Jeffrey M. M.}, abstract = {AI and Machine Learning in Clinical Medicine, 2023This first article in a series describes the history of artificial intelligence in medicine; the use of AI in image analysis, identification of disease outbreaks, and diagnosis; and the use of chatbots.} }
@article{WOS:000469544400001, title = {Machine learning approach for pavement performance prediction}, journal = {INTERNATIONAL JOURNAL OF PAVEMENT ENGINEERING}, volume = {22}, pages = {341-354}, year = {2021}, issn = {1029-8436}, doi = {10.1080/10298436.2019.1609673}, author = {Marcelino, Pedro and Antunes, Maria de Lurdes and Fortunato, Eduardo and Gomes, Marta Castilho}, abstract = {In recent years, there has been an increasing interest in the application of machine learning for the prediction of pavement performance. Prediction models are used to predict the future pavement condition, helping to optimally allocate maintenance and rehabilitation funds. However, few studies have proposed a systematic approach to the development of machine learning models for pavement performance prediction. Most of the studies focus on artificial neural networks models that are trained for high accuracy, disregarding other suitable machine learning algorithms and neglecting the importance of models' generalisation capability for Pavement Engineering applications. This paper proposes a general machine learning approach for the development of pavement performance prediction models in pavement management systems (PMS). The proposed approach supports different machine learning algorithms and emphasizes generalisation performance. A case study for prediction of International Roughness Index (IRI) for 5 and 10-years, using the Long-Term Pavement Performance, is presented. The proposed models were based on a random forest algorithm, using datasets comprising previous IRI measurements, structural, climatic, and traffic data.} }
@article{WOS:000449644300007, title = {Rapid estimation of activation energy in heterogeneous catalytic reactions via machine learning}, journal = {JOURNAL OF COMPUTATIONAL CHEMISTRY}, volume = {39}, pages = {2405-2408}, year = {2018}, issn = {0192-8651}, doi = {10.1002/jcc.25567}, author = {Takahashi, Keisuke and Miyazato, Itsuki}, abstract = {Estimation of activation energies within heterogeneous catalytic reactions is performed using machine learning and catalysts dataset. In particular, descriptors for determining activation energy are revealed within the 788 activation energy dataset. With the implementation of machine learning and chosen descriptors, activation energy can be instantly predicted with over 90\\% accuracy during cross-validation. Thus, rapid estimation of activation energies within heterogeneous catalytic reactions can be made achievable via machine learning, leading toward the acceleration of catalysts design and characterization. (c) 2018 Wiley Periodicals, Inc.} }
@article{WOS:001032105900001, title = {A review of machine learning applications in soccer with an emphasis on injury risk}, journal = {BIOLOGY OF SPORT}, volume = {40}, pages = {233-239}, year = {2023}, issn = {0860-021X}, author = {Nassis, George P. and Verhagen, Evert and Brito, Joao and Figueiredo, Pedro and Krustrup, Peter}, abstract = {This narrative review paper aimed to discuss the literature on machine learning applications in soccer with an emphasis on injury risk assessment. A secondary aim was to provide practical tips for the health and performance staff in soccer clubs on how machine learning can provide a competitive advantage. Performance analysis is the area with the majority of research so far. Other domains of soccer science and medicine with machine learning use are injury risk assessment, players' workload and wellness monitoring, movement analysis, players' career trajectory, club performance, and match attendance. Regarding injuries, which is a hot topic, machine learning does not seem to have a high predictive ability at the moment (models specificity ranged from 74.2\\%-97.7\\%. sensitivity from 15.2\\%-55.6\\% with area under the curve of 0.66-0.83). It seems, though, that machine learning can help to identify the early signs of elevated risk for a musculoskeletal injury. Future research should account for musculoskeletal injuries' dynamic nature for machine learning to provide more meaningful results for practitioners in soccer.} }
@article{WOS:001262652200001, title = {A Review on Machine Learning for Channel Coding}, journal = {IEEE ACCESS}, volume = {12}, pages = {89002-89025}, year = {2024}, issn = {2169-3536}, doi = {10.1109/ACCESS.2024.3412192}, author = {Lim Meng Kee, Heimrih and Ahmad, Norulhusna and Azri Mohd Izhar, Mohd and Anwar, Khoirul and Ng, Soon Xin}, abstract = {The usage of artificial intelligence and machine learning in wireless communications is the stepping stone towards a technological breakthrough in the current limitations of wireless communication systems. The trend of future coding schemes towards 6G appears to be based on rateless schemes and machine learning. Channel coding is important when transmitting data or information reliably as it provides error-correcting purposes. However, there is still a demand for more research regarding machine learning for channel coding. There is also a lack of a specific term or classification for existing machine learning applications for channel coding. This paper explores and compiles current trending machine learning techniques for channel coding. We are also introducing and proposing a new type of machine learning classification for channel coding purposes, as well as surveying some of the papers that fall under the respective class. This paper also discusses current challenges and future machine learning trends for channel coding, which are expected to impact future wireless communications development, especially in channel coding advancements.} }
@article{WOS:000435287000005, title = {Recent trends in machine learning for human activity recognition-A survey}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {8}, year = {2018}, issn = {1942-4787}, doi = {10.1002/widm.1254}, author = {Ramasamy Ramamurthy, Sreenivasan and Roy, Nirmalya}, abstract = {There has been an upsurge recently in investigating machine learning techniques for activity recognition (AR) problems as they have been very effective in extracting and learning knowledge from the activity datasets. The technique ranges from heuristically derived hand-crafted feature-based traditional machine learning algorithms to the recently developed hierarchically self-evolving feature-based deep learning algorithms. AR continues to remain a challenging problem in uncontrolled smart environments despite the amount of work contributed by the researcher in this field. The complex, volatile, and chaotic nature of the activity data presents numerous challenges that influence the performance of the AR systems in the wild. In this article, we present a comprehensive overview of recent machine learning and data mining techniques generally employed for AR and the underpinning problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art techniques in this domain in an attempt to identify the possible directions for future AR research. This article is categorized under: Application Areas > Science and Technology Algorithmic Development > Spatial and Temporal Data Mining Technologies > Machine Learning Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining} }
@article{WOS:000399838700001, title = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {18}, year = {2017}, issn = {1532-4435}, author = {Lemaitre, Guillaume and Nogueira, Fernando and Aridas, Christos K.}, abstract = {imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn.} }
@article{WOS:000712048500001, title = {Machine Learning and Deep Learning for the Pharmacogenomics of Antidepressant Treatments}, journal = {CLINICAL PSYCHOPHARMACOLOGY AND NEUROSCIENCE}, volume = {19}, pages = {577-588}, year = {2021}, issn = {1738-1088}, doi = {10.9758/cpn.2021.19.4.577}, author = {Lin, Eugene and Lin, Chieh-Hsin and Lane, Hsien-Yuan}, abstract = {A growing body of evidence now proposes that machine learning and deep learning techniques can serve as a vital foundation for the pharmacogenomics of antidepressant treatments in patients with major depressive disorder (MDD). In this review, we focus on the latest developments for pharmacogenomics research using machine learning and deep learning approaches together with neuroimaging and multi-omics data. First, we review relevant pharmacogenomics studies that leverage numerous machine learning and deep learning techniques to determine treatment prediction and potential biomarkers for antidepressant treatments in MDD. In addition, we depict some neuroimaging pharmacogenomics studies that utilize various machine learning approaches to predict antidepressant treatment outcomes in MDD based on the integration of research on pharmacogenomics and neuroimaging. Moreover, we summarize the limitations in regard to the past pharmacogenomics studies of antidepressant treatments in MDD. Finally, we outline a discussion of challenges and directions for future research. In light of latest advancements in neuroimaging and multi-omics, various genomic variants and biomarkers associated with antidepressant treatments in MDD are being identified in pharmacogenomics research by employing machine learning and deep learning algorithms.} }
@article{WOS:000685204500060, title = {AL: Autogenerating Supervised Learning Programs}, journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL}, volume = {3}, year = {2019}, doi = {10.1145/3360601}, author = {Cambronero, Jose P. and Rinard, Martin C.}, abstract = {We present AL, a novel automated machine learning system that learns to generate new supervised learning pipelines from an existing corpus of supervised learning programs. In contrast to existing automated machine learning tools, which typically implement a search over manually selected machine learning functions and classes, AL learns to identify the relevant classes in an API by analyzing dynamic program traces that use the target machine learning library. AL constructs a conditional probability model from these traces to estimate the likelihood of the generated supervised learning pipelines and uses this model to guide the search to generate pipelines for new datasets. Our evaluation shows that AL can produce successful pipelines for datasets that previous systems fail to process and produces pipelines with comparable predictive performance for datasets that previous systems process successfully.} }
@article{WOS:000741981200001, title = {Towards Multimodal Machine Learning Prediction of Individual Cognitive Evolution in Multiple Sclerosis}, journal = {JOURNAL OF PERSONALIZED MEDICINE}, volume = {11}, year = {2021}, doi = {10.3390/jpm11121349}, author = {Denissen, Stijn and Chen, Oliver Y. and De Mey, Johan and De Vos, Maarten and Van Schependom, Jeroen and Sima, Diana Maria and Nagels, Guy}, abstract = {Multiple sclerosis (MS) manifests heterogeneously among persons suffering from it, making its disease course highly challenging to predict. At present, prognosis mostly relies on biomarkers that are unable to predict disease course on an individual level. Machine learning is a promising technique, both in terms of its ability to combine multimodal data and through the capability of making personalized predictions. However, most investigations on machine learning for prognosis in MS were geared towards predicting physical deterioration, while cognitive deterioration, although prevalent and burdensome, remained largely overlooked. This review aims to boost the field of machine learning for cognitive prognosis in MS by means of an introduction to machine learning and its pitfalls, an overview of important elements for study design, and an overview of the current literature on cognitive prognosis in MS using machine learning. Furthermore, the review discusses new trends in the field of machine learning that might be adopted for future studies in the field.} }
@article{WOS:000814423300006, title = {FTAP: Feature transferring autonomous machine learning pipeline}, journal = {INFORMATION SCIENCES}, volume = {593}, pages = {385-397}, year = {2022}, issn = {0020-0255}, doi = {10.1016/j.ins.2022.02.006}, author = {Wu, Xing and Chen, Cheng and Li, Pan and Zhong, Mingyu and Wang, Jianjia and Qian, Quan and Ding, Peng and Yao, Junfeng and Guo, Yike}, abstract = {An effective method in machine learning often involves considerable experience with algorithms and domain expertise. Many existing machine learning methods highly rely on feature selection which are always domain-specific. However, the intervention by data scientists is time-consuming and labor-intensive. To meet this challenge, we propose a Feature Transferring Autonomous machine learning Pipeline (FTAP) to improve efficiency and performance. The proposed FTAP has been extensively evaluated on different modalities of data covering audios, images, and texts. Experimental results demonstrate that the proposed FTAP not only outperforms state-of-the-art methods on ESC-50 dataset with multi-class audio classification but also has good performance in distant domain transfer learning. Furthermore, FTAP outperforms TPOT, a state-of-the-art autonomous machine learning tool, on learning tasks. The quantitative and qualitative analysis proves the feasibility and robustness of the proposed FTAP. (C) 2022 Elsevier Inc. All rights reserved.} }
@article{WOS:000441903100001, title = {Rise of the machines? Machine learning approaches and mental health: opportunities and challenges}, journal = {BRITISH JOURNAL OF PSYCHIATRY}, volume = {213}, pages = {509-510}, year = {2018}, issn = {0007-1250}, doi = {10.1192/bjp.2018.105}, author = {Tiffin, Paul A. and Paton, Lewis W.}, abstract = {Machine learning methods are being increasingly applied to physical healthcare. In this article we describe some of the potential benefits, challenges and limitations of this approach in a mental health context. We provide a number of examples where machine learning could add value beyond conventional statistical modelling.Declaration of interestNone.} }
@article{WOS:000797778000010, title = {Applications of machine learning methods in port operations-A systematic literature review}, journal = {TRANSPORTATION RESEARCH PART E-LOGISTICS AND TRANSPORTATION REVIEW}, volume = {161}, year = {2022}, issn = {1366-5545}, doi = {10.1016/j.tre.2022.102722}, author = {Filom, Siyavash and Amiri, Amir M. and Razavi, Saiedeh}, abstract = {Ports are pivotal nodes in supply chain and transportation networks, in which most of the existing data remain underutilized. Machine learning methods are versatile tools to utilize and harness the hidden power of the data. Considering ever-growing adoption of machine learning as a data driven decision-making tool, the port industry is far behind other modes of transportation in this transition. To fill the gap, we aimed to provide a comprehensive systematic literature review on this topic to analyze the previous research from different perspectives such as area of the application, type of application, machine learning method, data, and location of the study. Results showed that the number of articles in the field has been increasing annually, and the most prevalent use case of machine learning methods is to predict different port characteristics. However, there are emerging prescriptive and autonomous use cases of machine learning methods in the literature. Furthermore, research gaps and challenges are identified, and future research directions have been discussed from method-centric and application-centric points of view.} }
@article{WOS:000610764000001, title = {Machine Learning in P\\&C Insurance: A Review for Pricing and Reserving}, journal = {RISKS}, volume = {9}, year = {2021}, doi = {10.3390/risks9010004}, author = {Blier-Wong, Christopher and Cossette, Helene and Lamontagne, Luc and Marceau, Etienne}, abstract = {In the past 25 years, computer scientists and statisticians developed machine learning algorithms capable of modeling highly nonlinear transformations and interactions of input features. While actuaries use GLMs frequently in practice, only in the past few years have they begun studying these newer algorithms to tackle insurance-related tasks. In this work, we aim to review the applications of machine learning to the actuarial science field and present the current state of the art in ratemaking and reserving. We first give an overview of neural networks, then briefly outline applications of machine learning algorithms in actuarial science tasks. Finally, we summarize the future trends of machine learning for the insurance industry.} }
@article{WOS:000569375400002, title = {On-the-Fly Active Learning of Interatomic Potentials for Large-Scale Atomistic Simulations}, journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS}, volume = {11}, pages = {6946-6955}, year = {2020}, issn = {1948-7185}, doi = {10.1021/acs.jpclett.0c01061}, author = {Jinnouchi, Ryosuke and Miwa, Kazutoshi and Karsai, Ferenc and Kresse, Georg and Asahi, Ryoji}, abstract = {The on-the-fly generation of machine-learning force fields by active-learning schemes attracts a great deal of attention in the community of atomistic simulations. The algorithms allow the machine to self-learn an interatomic potential and construct machine-learned models on the fly during simulations. State-of-the-art query strategies allow the machine to judge whether new structures are out of the training data set or not. Only when the machine judges the necessity of updating the data set with the new structures are first-principles calculations carried out. Otherwise, the yet available machine-learned model is used to update the atomic positions. In this manner, most of the first-principles calculations are bypassed during training, and overall, simulations are accelerated by several orders of magnitude while retaining almost first-principles accuracy. In this Perspective, after describing essential components of the active-learning algorithms, we demonstrate the power of the schemes by presenting recent applications.} }
@article{WOS:000571944500012, title = {Crop yield prediction using machine learning: A systematic literature review}, journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE}, volume = {177}, year = {2020}, issn = {0168-1699}, doi = {10.1016/j.compag.2020.105709}, author = {van Klompenburg, Thomas and Kassahun, Ayalew and Catal, Cagatay}, abstract = {Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops. Several machine learning algorithms have been applied to support crop yield prediction research. In this study, we performed a Systematic Literature Review (SLR) to extract and synthesize the algorithms and features that have been used in crop yield prediction studies. Based on our search criteria, we retrieved 567 relevant studies from six electronic databases, of which we have selected 50 studies for further analysis using inclusion and exclusion criteria. We investigated these selected studies carefully, analyzed the methods and features used, and provided suggestions for further research. According to our analysis, the most used features are temperature, rainfall, and soil type, and the most applied algorithm is Artificial Neural Networks in these models. After this observation based on the analysis of machine learning-based 50 papers, we performed an additional search in electronic databases to identify deep learning-based studies, reached 30 deep learning-based papers, and extracted the applied deep learning algorithms. According to this additional analysis, Convolutional Neural Networks (CNN) is the most widely used deep learning algorithm in these studies, and the other widely used deep learning algorithms are Long-Short Term Memory (LSTM) and Deep Neural Networks (DNN).} }
@article{WOS:000414424200001, title = {Machine learning: novel bioinformatics approaches for combating antimicrobial resistance}, journal = {CURRENT OPINION IN INFECTIOUS DISEASES}, volume = {30}, pages = {511-517}, year = {2017}, issn = {0951-7375}, doi = {10.1097/QCO.0000000000000406}, author = {Macesic, Nenad and Polubriaginof, Fernanda and Tatonetti, Nicholas P.}, abstract = {Purpose of review Antimicrobial resistance (AMR) is a threat to global health and new approaches to combating AMR are needed. Use of machine learning in addressing AMR is in its infancy but has made promising steps. We reviewed the current literature on the use of machine learning for studying bacterial AMR. Recent findings The advent of large-scale data sets provided by next-generation sequencing and electronic health records make applying machine learning to the study and treatment of AMR possible. To date, it has been used for antimicrobial susceptibility genotype/phenotype prediction, development of AMR clinical decision rules, novel antimicrobial agent discovery and antimicrobial therapy optimization. Summary Application of machine learning to studying AMR is feasible but remains limited. Implementation of machine learning in clinical settings faces barriers to uptake with concerns regarding model interpretability and data quality. Future applications of machine learning to AMR are likely to be laboratory-based, such as antimicrobial susceptibility phenotype prediction.} }
@article{WOS:000814485300003, title = {Lessons from infant learning for unsupervised machine learning}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {4}, pages = {510-520}, year = {2022}, doi = {10.1038/s42256-022-00488-2}, author = {Zaadnoordijk, Lorijn and Besold, Tarek R. and Cusack, Rhodri}, abstract = {Unsupervised machine learning algorithms reduce the dependence on curated, labeled datasets that are characteristic of supervised machine learning. The authors argue that the developmental science of infant cognition could inform the design of unsupervised machine learning approaches. The desire to reduce the dependence on curated, labeled datasets and to leverage the vast quantities of unlabeled data has triggered renewed interest in unsupervised (or self-supervised) learning algorithms. Despite improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning and clustering optimizations, unsupervised machine learning still falls short of its hypothesized potential as a breakthrough paradigm enabling generally intelligent systems. Inspiration from cognitive (neuro)science has been based mostly on adult learners with access to labels and a vast amount of prior knowledge. To push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. We identify three crucial factors enabling infants' quality and speed of learning: (1) babies' information processing is guided and constrained; (2) babies are learning from diverse, multimodal inputs; and (3) babies' input is shaped by development and active learning. We assess the extent to which these insights from infant learning have already been exploited in machine learning, examine how closely these implementations resemble the core insights, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.} }
@article{WOS:000778886900003, title = {Use of supervised machine learning to detect abuse of COVID-19 related domain names}, journal = {COMPUTERS \\& ELECTRICAL ENGINEERING}, volume = {100}, year = {2022}, issn = {0045-7906}, doi = {10.1016/j.compeleceng.2022.107864}, author = {Wang, Zheng}, abstract = {A comprehensive evaluation of supervised machine learning models for COVID-19 related domain name detection is presented. One representative conventional machine learning implementation and nineteen state-of-the-art deep learning implementations are evaluated. The deep learning implementation architectures evaluated include the recurrent, convolutional, and hybrid models. The detection rate metrics and the computing time metrics are considered in the evaluation. The result reveals that advanced deep learning models outperform conventional machine learning models in terms of detection rate. The results also show evidence of a tradeoff between detection rate and computing speed for the selection of machine learning models/architectures. High-frequency lexical analysis is provided for a better understanding of the COVID-19 related domain names. The limitations, implications, and considerations of the use of supervised machine learning to detect abuse of COVID-19 related domain names are discussed.} }
@article{WOS:000534813100012, title = {Performance evaluation of Botnet DDoS attack detection using machine learning}, journal = {EVOLUTIONARY INTELLIGENCE}, volume = {13}, pages = {283-294}, year = {2020}, issn = {1864-5909}, doi = {10.1007/s12065-019-00310-w}, author = {Tuan, Tong Anh and Long, Hoang Viet and Son, Le Hoang and Kumar, Raghvendra and Priyadarshini, Ishaani and Son, Nguyen Thi Kim}, abstract = {Botnet is regarded as one of the most sophisticated vulnerability threats nowadays. A large portion of network traffic is dominated by Botnets. Botnets are conglomeration of trade PCs (Bots) which are remotely controlled by their originator (BotMaster) under a Command and-Control (C\\&C) foundation. They are the keys to several Internet assaults like spams, Distributed Denial of Service Attacks (DDoS), rebate distortions, malwares and phishing. To over the problem of DDoS attack, various machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) (K-means, X-means etc.) were proposed. With the increasing popularity of Machine Learning in the field of Computer Security, it will be a remarkable accomplishment to carry out performance assessment of the machine learning methods given a common platform. This could assist developers in choosing a suitable method for their case studies and assist them in further research. This paper performed an experimental analysis of the machine learning methods for Botnet DDoS attack detection. The evaluation is done on the UNBS-NB 15 and KDD99 which are well-known publicity datasets for Botnet DDoS attack detection. Machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) are investigated for Accuracy, False Alarm Rate (FAR), Sensitivity, Specificity, False positive rate (FPR), AUC, and Matthews correlation coefficient (MCC) of datasets. Performance of KDD99 dataset has been experimentally shown to be better as compared to the UNBS-NB 15 dataset. This validation is significant in computer security and other related fields.} }
@article{WOS:000821577800005, title = {Hyperspectral Image Classification: Potentials, Challenges, and Future Directions}, journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE}, volume = {2022}, year = {2022}, issn = {1687-5265}, doi = {10.1155/2022/3854635}, author = {Datta, Debaleena and Mallick, Pradeep Kumar and Bhoi, Akash Kumar and Ijaz, Muhammad Fazal and Shafi, Jana and Choi, Jaeyoung}, abstract = {Recent imaging science and technology discoveries have considered hyperspectral imagery and remote sensing. The current intelligent technologies, such as support vector machines, sparse representations, active learning, extreme learning machines, transfer learning, and deep learning, are typically based on the learning of the machines. These techniques enrich the processing of such three-dimensional, multiple bands, and high-resolution images with their precision and fidelity. This article presents an extensive survey depicting machine-dependent technologies' contributions and deep learning on landcover classification based on hyperspectral images. The objective of this study is three-fold. First, after reading a large pool of Web of Science (WoS), Scopus, SCI, and SCIE-indexed and SCIE-related articles, we provide a novel approach for review work that is entirely systematic and aids in the inspiration of finding research gaps and developing embedded questions. Second, we emphasize contemporary advances in machine learning (ML) methods for identifying hyperspectral images, with a brief, organized overview and a thorough assessment of the literature involved. Finally, we draw the conclusions to assist researchers in expanding their understanding of the relationship between machine learning and hyperspectral images for future research.} }
@article{WOS:000755564500001, title = {Machine learning political orders}, journal = {REVIEW OF INTERNATIONAL STUDIES}, volume = {49}, pages = {20-36}, year = {2023}, issn = {0260-2105}, doi = {10.1017/S0260210522000031}, author = {Amoore, Louise}, abstract = {A significant set of epistemic and political transformations are taking place as states and societies begin to understand themselves and their problems through the paradigm of deep neural network algorithms. A machine learning political order does not merely change the political technologies of governance, but is itself a reordering of politics, of what the political can be. When algorithmic systems reduce the pluridimensionality of politics to the output of a model, they simultaneously foreclose the potential for other political claims to be made and alternative political projects to be built. More than this foreclosure, a machine learning political order actively profits and learns from the fracturing of communities and the destabilising of democratic rights. The transformation from rules-based algorithms to deep learning models has paralleled the undoing of rules-based social and international orders - from the use of machine learning in the campaigns of the UK EU referendum, to the trialling of algorithmic immigration and welfare systems, and the use of deep learning in the COVID-19 pandemic - with political problems becoming reconfigured as machine learning problems. Machine learning political orders decouple their attributes, features and clusters from underlying social values, no longer tethered to notions of good governance or a good society, but searching instead for the optimal function of abstract representations of data.} }
@article{WOS:000692200100001, title = {Universal Approximation Property of Quantum Machine Learning Models in Quantum-Enhanced Feature Spaces}, journal = {PHYSICAL REVIEW LETTERS}, volume = {127}, year = {2021}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.127.090506}, author = {Goto, Takahiro and Tran, Quoc Hoan and Nakajima, Kohei}, abstract = {Encoding classical data into quantum states is considered a quantum feature map to map classical data into a quantum Hilbert space. This feature map provides opportunities to incorporate quantum advantages into machine learning algorithms to be performed on near-term intermediate-scale quantum computers. The crucial idea is using the quantum Hilbert space as a quantum-enhanced feature space in machine learning models. Although the quantum feature map has demonstrated its capability when combined with linear classification models in some specific applications, its expressive power from the theoretical perspective remains unknown. We prove that the machine learning models induced from the quantum-enhanced feature space are universal approximators of continuous functions under typical quantum feature maps. We also study the capability of quantum feature maps in the classification of disjoint regions. Our work enables an important theoretical analysis to ensure that machine learning algorithms based on quantum feature maps can handle a broad class of machine learning tasks. In light of this, one can design a quantum machine learning model with more powerful expressivity.} }
@article{WOS:000760291800021, title = {Translating promise into practice: a review of machine learning in suicide research and prevention}, journal = {LANCET PSYCHIATRY}, volume = {9}, pages = {243-252}, year = {2022}, issn = {2215-0366}, doi = {10.1016/s2215-0366(21)00254-6}, author = {Kirtley, Olivia J. and van Mens, Kasper and Hoogendoorn, Mark and Kapur, Navneet and de Beurs, Derek}, abstract = {In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need-ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.} }
@article{WOS:001209420900001, title = {Integrating Machine Learning in Metabolomics: A Path to Enhanced Diagnostics and Data Interpretation}, journal = {SMALL METHODS}, volume = {8}, year = {2024}, issn = {2366-9608}, doi = {10.1002/smtd.202400305}, author = {Xu, Yudian and Cao, Linlin and Chen, Yifan and Zhang, Ziyue and Liu, Wanshan and Li, He and Ding, Chenhuan and Pu, Jun and Qian, Kun and Xu, Wei}, abstract = {Metabolomics, leveraging techniques like NMR and MS, is crucial for understanding biochemical processes in pathophysiological states. This field, however, faces challenges in metabolite sensitivity, data complexity, and omics data integration. Recent machine learning advancements have enhanced data analysis and disease classification in metabolomics. This study explores machine learning integration with metabolomics to improve metabolite identification, data efficiency, and diagnostic methods. Using deep learning and traditional machine learning, it presents advancements in metabolic data analysis, including novel algorithms for accurate peak identification, robust disease classification from metabolic profiles, and improved metabolite annotation. It also highlights multiomics integration, demonstrating machine learning's potential in elucidating biological phenomena and advancing disease diagnostics. This work contributes significantly to metabolomics by merging it with machine learning, offering innovative solutions to analytical challenges and setting new standards for omics data analysis.} }
@article{WOS:000428582200001, title = {A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View}, journal = {IEEE ACCESS}, volume = {6}, pages = {12103-12117}, year = {2018}, issn = {2169-3536}, doi = {10.1109/ACCESS.2018.2805680}, author = {Liu, Qiang and Li, Pan and Zhao, Wentao and Cai, Wei and Yu, Shui and Leung, Victor C. M.}, abstract = {Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.} }
@article{WOS:000963897500001, title = {Data Sensitivity and Domain Specificity in Reuse of Machine Learning Applications}, journal = {INFORMATION SYSTEMS FRONTIERS}, volume = {26}, pages = {633-640}, year = {2024}, issn = {1387-3326}, doi = {10.1007/s10796-023-10388-4}, author = {Rutschi, Corinna and Berente, Nicholas and Nwanganga, Frederick}, abstract = {Data sensitivity and domain specificity challenges arise in reuse of machine learning applications. We identify four types of machine learning applications based on different reuse strategies: generic, distinctive, selective, and exclusive. We conclude with lessons for developing and deploying machine learning applications.} }
@article{WOS:000592355900013, title = {Machine learning for suicidology: A practical review of exploratory and hypothesis-driven approaches}, journal = {CLINICAL PSYCHOLOGY REVIEW}, volume = {82}, year = {2020}, issn = {0272-7358}, doi = {10.1016/j.cpr.2020.101940}, author = {Cox, Christopher R. and Moscardini, Emma H. and Cohen, Alex S. and Tucker, Raymond P.}, abstract = {Machine learning is being used to discover models to predict the progression from suicidal ideation to action in clinical populations. While quantifiable improvements in prediction accuracy have been achieved over theory -driven efforts, models discovered through machine learning continue to fall short of clinical relevance. Thus, the value of machine learning for reaching this objective is hotly contested. We agree that machine learning, treated as a ``black box'' approach antithetical to theory-building, will not discover clinically relevant models of suicide. However, such models may be developed through deliberate synthesis of dataand theory-driven approaches. By providing an accessible overview of essential concepts and common methods, we highlight how generalizable models and scientific insight may be obtained by incorporating prior knowledge and expectations to machine learning research, drawing examples from suicidology. We then discuss challenges investigators will face when using machine learning to discover models of low prevalence outcomes, such as suicide.} }
@article{WOS:000684684400001, title = {Teaching Machine Learning in K-12 Classroom: Pedagogical and Technological Trajectories for Artificial Intelligence Education}, journal = {IEEE ACCESS}, volume = {9}, pages = {110558-110572}, year = {2021}, issn = {2169-3536}, doi = {10.1109/ACCESS.2021.3097962}, author = {Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen, Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold}, abstract = {Over the past decades, numerous practical applications of machine learning techniques have shown the potential of AI-driven and data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning and AI in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based ``traditional'' programming is a central aspect and building block in developing next generation computational thinking.} }
@article{WOS:000403140800087, title = {Machine Learning With Big Data: Challenges and Approaches}, journal = {IEEE ACCESS}, volume = {5}, pages = {7776-7797}, year = {2017}, issn = {2169-3536}, doi = {10.1109/ACCESS.2017.2696365}, author = {L'Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and Capretz, Miriam A. M.}, abstract = {The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.} }
@article{WOS:000833393900003, title = {Physics-based machine learning method and the application to energy consumption prediction in tunneling construction}, journal = {ADVANCED ENGINEERING INFORMATICS}, volume = {53}, year = {2022}, issn = {1474-0346}, doi = {10.1016/j.aei.2022.101642}, author = {Zhou, Siyang and Liu, Shanglin and Kang, Yilan and Cai, Jie and Xie, Haimei and Zhang, Qian}, abstract = {Representing causality in machine learning to predict control parameters is state-of-the-art research in intelligent control. This study presents a physics-based machine learning method providing a prediction model that guarantees enhanced interpretability conforming to physical laws. The proposed approach encodes physical knowledge as mapping relationships between variables in engineering dataset into the learning procedure through dimensional analysis. This derives causal relationships between the control parameter and its influencing factors. The proposed machine learning method's objective function is further improved by the penalty term in the regularization strategy. Verifications on the energy consumption prediction of tunnel boring machine prove that, the established model accords with basic principles in this field. Moreover, the proposed approach traces the impact of three major factors (structure, operation, and geology) along the construction section, offering each component's contribution rates to energy consumption. Compared with several commonly used machine learning algorithms, the proposed method reduces the need for large amounts of training data and demonstrates higher accuracy. The results indicate that the revealed causality and enhanced prediction performance of the proposed method advance the applicability of machine learning methods to intelligent control during construction.} }
@article{WOS:000617753000001, title = {Machine learning approach for the prediction and optimization of thermal transport properties}, journal = {FRONTIERS OF PHYSICS}, volume = {16}, year = {2021}, issn = {2095-0462}, doi = {10.1007/s11467-020-1041-x}, author = {Ouyang, Yulou and Yu, Cuiqian and Yan, Gang and Chen, Jie}, abstract = {Traditional simulation methods have made prominent progress in aiding experiments for understanding thermal transport properties of materials, and in predicting thermal conductivity of novel materials. However, huge challenges are also encountered when exploring complex material systems, such as formidable computational costs. As a rising computational method, machine learning has a lot to offer in this regard, not only in speeding up the searching and optimization process, but also in providing novel perspectives. In this work, we review the state-of-the-art studies on material's thermal properties based on machine learning technique. First, the basic principles of machine learning method are introduced. We then review applications of machine learning technique in the prediction and optimization of material's thermal properties, including thermal conductivity and interfacial thermal resistance. Finally, an outlook is provided for the future studies.} }
@article{WOS:000834797800001, title = {Distributing epistemic functions and tasks-A framework for augmenting human analytic power with machine learning in science education research}, journal = {JOURNAL OF RESEARCH IN SCIENCE TEACHING}, volume = {60}, pages = {423-447}, year = {2023}, issn = {0022-4308}, doi = {10.1002/tea.21803}, author = {Kubsch, Marcus and Krist, Christina and Rosenberg, Joshua M.}, abstract = {Machine learning (ML) has become commonplace in educational research and science education research, especially to support assessment efforts. Such applications of machine learning have shown their promise in replicating and scaling human-driven codes of students' work. Despite this promise, we and other scholars argue that machine learning has not yet achieved its transformational potential. We argue that this is because our field is currently lacking frameworks for supporting creative, principled, and critical endeavors to use machine learning in science education research. To offer considerations for science education researchers' use of ML, we present a framework, Distributing Epistemic Functions and Tasks (DEFT), that highlights the functions and tasks that pertain to generating knowledge that can be carried out by either trained researchers or machine learning algorithms. Such considerations are critical decisions that should occur alongside those about, for instance, the type of data or algorithm used. We apply this framework to two cases, one that exemplifies the cutting-edge use of machine learning in science education research and another that offers a wholly different means of using machine learning and human-driven inquiry together. We conclude with strategies for researchers to adopt machine learning and call for the field to rethink how we prepare science education researchers in an era of great advances in computational power and access to machine learning methods.} }
@article{WOS:000705849400001, title = {Quantum semi-supervised kernel learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {3}, year = {2021}, issn = {2524-4906}, doi = {10.1007/s42484-021-00053-x}, author = {Saeedi, Seyran and Panahi, Aliakbar and Arodz, Tom}, abstract = {Quantum machine learning methods have the potential to facilitate learning using extremely large datasets. While the availability of data for training machine learning models is steadily increasing, oftentimes it is much easier to collect feature vectors to obtain the corresponding labels. One of the approaches for addressing this issue is to use semi-supervised learning, which leverages not only the labeled samples, but also unlabeled feature vectors. Here, we present a quantum machine learning algorithm for training semi-supervised kernel support vector machines. The algorithm uses recent advances in quantum sample-based Hamiltonian simulation to extend the existing quantum LS-SVM algorithm to handle the semi-supervised term in the loss. Through a theoretical study of the algorithm's computational complexity, we show that it maintains the same speedup as the fully-supervised quantum LS-SVM.} }
@article{WOS:001138185200001, title = {State of the art in applications of machine learning in steelmaking process modeling}, journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS}, volume = {30}, pages = {2055-2075}, year = {2023}, issn = {1674-4799}, doi = {10.1007/s12613-023-2646-1}, author = {Zhang, Runhao and Yang, Jian}, abstract = {With the development of automation and informatization in the steelmaking industry, the human brain gradually fails to cope with an increasing amount of data generated during the steelmaking process. Machine learning technology provides a new method other than production experience and metallurgical principles in dealing with large amounts of data. The application of machine learning in the steelmaking process has become a research hotspot in recent years. This paper provides an overview of the applications of machine learning in the steelmaking process modeling involving hot metal pretreatment, primary steelmaking, secondary refining, and some other aspects. The three most frequently used machine learning algorithms in steelmaking process modeling are the artificial neural network, support vector machine, and case-based reasoning, demonstrating proportions of 56\\%, 14\\%, and 10\\%, respectively. Collected data in the steelmaking plants are frequently faulty. Thus, data processing, especially data cleaning, is crucially important to the performance of machine learning models. The detection of variable importance can be used to optimize the process parameters and guide production. Machine learning is used in hot metal pretreatment modeling mainly for endpoint S content prediction. The predictions of the endpoints of element compositions and the process parameters are widely investigated in primary steelmaking. Machine learning is used in secondary refining modeling mainly for ladle furnaces, Ruhrstahl-Heraeus, vacuum degassing, argon oxygen decarburization, and vacuum oxygen decarburization processes. Further development of machine learning in the steelmaking process modeling can be realized through additional efforts in the construction of the data platform, the industrial transformation of the research achievements to the practical steelmaking process, and the improvement of the universality of the machine learning models.} }
@article{WOS:000614082300001, title = {Machine Learning Methods for Diagnosing Autism Spectrum Disorder and Attention- Deficit/Hyperactivity Disorder Using Functional and Structural MRI: A Survey}, journal = {FRONTIERS IN NEUROINFORMATICS}, volume = {14}, year = {2021}, doi = {10.3389/fninf.2020.575999}, author = {Eslami, Taban and Almuqhim, Fahad and Raiker, Joseph S. and Saeed, Fahad}, abstract = {Here we summarize recent progress in machine learning model for diagnosis of Autism Spectrum Disorder (ASD) and Attention-deficit/Hyperactivity Disorder (ADHD). We outline and describe the machine-learning, especially deep-learning, techniques that are suitable for addressing research questions in this domain, pitfalls of the available methods, as well as future directions for the field. We envision a future where the diagnosis of ASD, ADHD, and other mental disorders is accomplished, and quantified using imaging techniques, such as MRI, and machine-learning models.} }
@article{WOS:000681124300016, title = {Predicting Machine Learning Pipeline Runtimes in the Context of Automated Machine Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {43}, pages = {3055-3066}, year = {2021}, issn = {0162-8828}, doi = {10.1109/TPAMI.2021.3056950}, author = {Mohr, Felix and Wever, Marcel and Tornede, Alexander and Huellermeier, Eyke}, abstract = {Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.} }
@article{WOS:000464121000002, title = {Machine Learning in Banking Risk Management: A Literature Review}, journal = {RISKS}, volume = {7}, year = {2019}, issn = {2227-9091}, doi = {10.3390/risks7010029}, author = {Leo, Martin and Sharma, Suneel and Maddulety, K.}, abstract = {There is an increasing influence of machine learning in business applications, with many solutions already implemented and many more being explored. Since the global financial crisis, risk management in banks has gained more prominence, and there has been a constant focus around how risks are being detected, measured, reported and managed. Considerable research in academia and industry has focused on the developments in banking and risk management and the current and emerging challenges. This paper, through a review of the available literature seeks to analyse and evaluate machine-learning techniques that have been researched in the context of banking risk management, and to identify areas or problems in risk management that have been inadequately explored and are potential areas for further research. The review has shown that the application of machine learning in the management of banking risks such as credit risk, market risk, operational risk and liquidity risk has been explored; however, it doesn't appear commensurate with the current industry level of focus on both risk management and machine learning. A large number of areas remain in bank risk management that could significantly benefit from the study of how machine learning can be applied to address specific problems.} }
@article{WOS:000253272100001, title = {Preliminary studyon Wilcoxon learning machines}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS}, volume = {19}, pages = {201-211}, year = {2008}, issn = {1045-9227}, doi = {10.1109/TNN.2007.904035}, author = {Hsieh, Jer-Guang and Lin, Yih-Lon and Jeng, Jyh-Horng}, abstract = {As is well known in statistics, the resulting linear regressors by using the rank-based Wilcoxon approach to linear regression problems are usually robust against (or insensitive to) outliers. This motivates us to introduce in this paper the Wilcoxon approach to the area, of machine learning. Specifically, we investigate four new learning machines, namely Wilcoxon neural network (WNN), Wilcoxon generalized radial basis function network (WGRBFN), Wilcoxon fuzzy neural network (WFNN), and kernel-based Wilcoxon regressor (KWR). These provide alternative learning machines when faced with general nonlinear learning problems. Simple weights updating rules based on gradient descent will be derived. Some numerical examples will be provided to compare the robustness against outliers for various learning machines. Simulation results show that the Wilcoxon learning machines proposed in this paper have good robustness against outliers. We firmly believe that the Wilcoxon approach will provide a promising methodology for many machine learning problems.} }
@article{WOS:000419350700030, title = {MoleculeNet: a benchmark for molecular machine learning}, journal = {CHEMICAL SCIENCE}, volume = {9}, pages = {513-530}, year = {2018}, issn = {2041-6520}, doi = {10.1039/c7sc02664a}, author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay}, abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.} }
@article{WOS:000808086800005, title = {Knowledge-Driven Machine Learning and Applications in Wireless Communications}, journal = {IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING}, volume = {8}, pages = {454-467}, year = {2022}, issn = {2332-7731}, doi = {10.1109/TCCN.2021.3128597}, author = {Li, Daofeng and Xu, Yamei and Zhao, Ming and Zhu, Jinkang and Zhang, Sihai}, abstract = {The power of big data and machine learning has been drastically demonstrated in many fields during the past twenty years which somehow leads to the vague even false understanding that the huge amount of precious human knowledge accumulated to date seems to no longer matter. In this paper, we are pioneering to propose the knowledge-driven machine learning (KDML) model to exhibit that knowledge can play an important role in machine learning tasks. Compared with conventional machine learning, KDML contains a unique knowledge module based on specific domain knowledge, which is able to simplify the machine learning network structures, reduce the training overhead and improve interpretability. Channel estimation problem of wireless communication is taken as a case verification because such machine learning-based solutions face huge challenges in terms of accuracy, complexity, and reliability. We integrate the classical wireless channel estimation algorithms into different machine learning neural networks and propose KDML-based channel estimators in Orthogonal Frequency Division Multiplexing (OFDM) and Massive Multiple Input Multiple Output (MIMO) system. The experimental results in both communication systems validate the effectiveness of the proposed KDML-based channel estimators.} }
@article{WOS:000831186100001, title = {PASSer2.0: Accurate Prediction of Protein Allosteric Sites Through Automated Machine Learning}, journal = {FRONTIERS IN MOLECULAR BIOSCIENCES}, volume = {9}, year = {2022}, doi = {10.3389/fmolb.2022.879251}, author = {Xiao, Sian and Tian, Hao and Tao, Peng}, abstract = {Allostery is a fundamental process in regulating protein activities. The discovery, design, and development of allosteric drugs demand better identification of allosteric sites. Several computational methods have been developed previously to predict allosteric sites using static pocket features and protein dynamics. Here, we define a baseline model for allosteric site prediction and present a computational model using automated machine learning. Our model, PASSer2.0, advanced the previous results and performed well across multiple indicators with 82.7\\% of allosteric pockets appearing among the top three positions. The trained machine learning model has been integrated with the to facilitate allosteric drug discovery.} }
@article{WOS:000612766700011, title = {Machine Learning and the Future of Cardiovascular Care JACC State-of-the-Art Review}, journal = {JACC-JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY}, volume = {77}, pages = {300-313}, year = {2021}, issn = {0735-1097}, doi = {10.1016/j.jacc.2020.11.030}, author = {Quer, Giorgio and Arnaout, Ramy and Henne, Michael and Arnaout, Rima}, abstract = {The role of physicians has always been to synthesize the data available to them to identify diagnostic patterns that guide treatment and follow response. Today, increasingly sophisticated machine learning algorithms may grow to support clinical experts in some of these tasks. Machine learning has the potential to benefit patients and cardiologists, but only if clinicians take an active role in bringing these new algorithms into practice. The aim of this review is to introduce clinicians who are not data science experts to key concepts in machine learning that will allow them to better understand the field and evaluate new literature and developments. The current published data in machine learning for cardiovascular disease is then summarized, using both a bibliometric survey, with code publicly available to enable similar analysis for any research topic of interest, and select case studies. Finally, several ways that clinicians can and must be involved in this emerging field are presented. (C) 2021 The Authors. Published by Elsevier on behalf of the American College of Cardiology Foundation.} }
@article{WOS:001292266400001, title = {Advances in Machine Learning for Wearable Sensors}, journal = {ACS NANO}, volume = {18}, pages = {22734-22751}, year = {2024}, issn = {1936-0851}, doi = {10.1021/acsnano.4c05851}, author = {Xiao, Xiao and Yin, Junyi and Xu, Jing and Tat, Trinny and Chen, Jun}, abstract = {Recent years have witnessed tremendous advances in machine learning techniques for wearable sensors and bioelectronics, which play an essential role in real-time sensing data analysis to provide clinical-grade information for personalized healthcare. To this end, supervised learning and unsupervised learning algorithms have emerged as powerful tools, allowing for the detection of complex patterns and relationships in large, high-dimensional data sets. In this Review, we aim to delineate the latest advancements in machine learning for wearable sensors, focusing on key developments in algorithmic techniques, applications, and the challenges intrinsic to this evolving landscape. Additionally, we highlight the potential of machine-learning approaches to enhance the accuracy, reliability, and interpretability of wearable sensor data and discuss the opportunities and limitations of this emerging field. Ultimately, our work aims to provide a roadmap for future research endeavors in this exciting and rapidly evolving area.} }
@article{WOS:000431737300083, title = {A machine learning model with human cognitive biases capable of learning from small and biased datasets}, journal = {SCIENTIFIC REPORTS}, volume = {8}, year = {2018}, issn = {2045-2322}, doi = {10.1038/s41598-018-25679-z}, author = {Taniguchi, Hidetaka and Sato, Hiroshi and Shirakawa, Tomohiro}, abstract = {Human learners can generalize a new concept from a small number of samples. In contrast, conventional machine learning methods require large amounts of data to address the same types of problems. Humans have cognitive biases that promote fast learning. Here, we developed a method to reduce the gap between human beings and machines in this type of inference by utilizing cognitive biases. We implemented a human cognitive model into machine learning algorithms and compared their performance with the currently most popular methods, naive Bayes, support vector machine, neural networks, logistic regression and random forests. We focused on the task of spam classification, which has been studied for a long time in the field of machine learning and often requires a large amount of data to obtain high accuracy. Our models achieved superior performance with small and biased samples in comparison with other representative machine learning methods.} }
@article{WOS:000270620400005, title = {Some single-machine and m-machine flowshop scheduling problems with learning considerations}, journal = {INFORMATION SCIENCES}, volume = {179}, pages = {3885-3892}, year = {2009}, issn = {0020-0255}, doi = {10.1016/j.ins.2009.07.011}, author = {Lee, Wen-Chiung and Wu, Chin-Chia}, abstract = {Scheduling with learning effect has drawn many researchers' attention since Biskup [D. Biskup, Single-machine scheduling with learning considerations, European journal of Opterational Research 115 (1999) 173-178] introduced the concept of learning into the scheduling field. Biskup [D. Biskup, A state-of-the-art review on scheduling with learning effect, European journal of Opterational Research 188 (2008) 315-329] classified the learning approaches in the literature into two main streams. He claimed that the position-based learning seems to be a realistic model for machine learning, while the sum-of-processing-time-based learning is a model for human learning. In some realistic situations, both the machine and human learning might exist simultaneously. For example, robots with neural networks are used in computers, motor vehicles, and many assembly lines. The actions of a robot are constantly modified through self-learning in processing the jobs. On the other hand, the operators in the control center learn how to give the commands efficiently through working experience. In this paper, we propose a new learning model that unifies the two main approaches. We show that some single-machine problems and some specified flowshop problems are polynomially solvable. (c) 2009 Elsevier Inc. All rights reserved.} }
@article{WOS:000412361900042, title = {Imbalance Learning Machine-Based Power System Short-Term Voltage Stability Assessment}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {13}, pages = {2533-2543}, year = {2017}, issn = {1551-3203}, doi = {10.1109/TII.2017.2696534}, author = {Zhu, Lipeng and Lu, Chao and Dong, Zhao Yang and Hong, Chao}, abstract = {In terms of machine learning-based power system dynamic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a challenging class imbalance problem. Besides, short-term feature extraction from scarce instability seems extremely difficult for conventional learning machines. Faced with such a dilemma, this paper develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is embedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribution. Cost-sensitive learning is employed to intensify bias toward those scarce yet valuable unstable cases. Furthermore, an incremental learning strategy is put forward for online monitoring, contributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the assessment scheme.} }
@article{WOS:000425074300032, title = {Applying spark based machine learning model on streaming big data for health status prediction}, journal = {COMPUTERS \\& ELECTRICAL ENGINEERING}, volume = {65}, pages = {393-399}, year = {2018}, issn = {0045-7906}, doi = {10.1016/j.compeleceng.2017.03.009}, author = {Nair, Lekha R. and Shetty, Sujala D. and Shetty, Siddhanth D.}, abstract = {Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000519206300017, title = {Analysis of non-iterative phase retrieval based on machine learning}, journal = {OPTICAL REVIEW}, volume = {27}, pages = {136-141}, year = {2020}, issn = {1340-6000}, doi = {10.1007/s10043-019-00574-8}, author = {Nishizaki, Yohei and Horisaki, Ryoichi and Kitaguchi, Katsuhisa and Saito, Mamoru and Tanida, Jun}, abstract = {In this paper, we analyze a machine-learning-based non-iterative phase retrieval method. Phase retrieval and its applications have been attractive research topics in optics and photonics, for example, in biomedical imaging, astronomical imaging, and so on. Most conventional phase retrieval methods have used iterative processes to recover phase information; however, the calculation speed and convergence with these methods are serious issues in real-time monitoring applications. Machine-learning-based methods are promising for addressing these issues. Here, we numerically compare conventional methods and a machine-learning-based method in which a convolutional neural network is employed. Simulations with several conditions show that the machine-learning-based method realizes fast and robust phase recovery compared with the conventional methods. We also numerically demonstrate machine-learning-based phase retrieval from noisy measurements with a noisy training data set for improving the noise robustness. The machine-learning-based approach used in this study may increase the impact of phase retrieval, which is useful in various fields, where phase retrieval has been used as a fundamental tool.} }
@article{WOS:000368151700007, title = {Classification of textile fabrics by use of spectroscopy-based pattern recognition methods}, journal = {SPECTROSCOPY LETTERS}, volume = {49}, pages = {96-102}, year = {2016}, issn = {0038-7010}, doi = {10.1080/00387010.2015.1089446}, author = {Sun, Xudong and Zhou, Mingxing and Sun, Yize}, abstract = {The combination of near-infrared spectroscopy and pattern recognition methods, including soft independent modeling of class analogy, least squares support machine, and extreme learning machine, was employed for textile fabrics classification. The fabrics of cotton, viscose, acrylic, polyamide, polyester, and blend fabric of cotton-viscose were divided into training and prediction sets (60: 60) for developing models and evaluating the classification abilities of the models. The classification accuracy and speed of soft independent modeling of class analogy, least squares support machine, and extreme learning machine were compared. Both least squares support machine and extreme learning machine achieved the classification accuracy of 100\\% for the prediction set. However, extreme learning machine performed much faster than least squares support machine, which suggested that extreme learning machine may be a promising method for real-time textile fabrics classification with a comparable accuracy based on near-infrared spectroscopy. Moreover, it might have commercial and regulatory potential to avoid time-consuming work, and costly and laborious chemical analysis for textile fabrics classification.} }
@article{WOS:000605202300001, title = {Second opinion needed: communicating uncertainty in medical machine learning}, journal = {NPJ DIGITAL MEDICINE}, volume = {4}, year = {2021}, issn = {2398-6352}, doi = {10.1038/s41746-020-00367-3}, author = {Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L.}, abstract = {There is great excitement that medical artificial intelligence (AI) based on machine learning (ML) can be used to improve decision making at the patient level in a variety of healthcare settings. However, the quantification and communication of uncertainty for individual predictions is often neglected even though uncertainty estimates could lead to more principled decision-making and enable machine learning models to automatically or semi-automatically abstain on samples for which there is high uncertainty. In this article, we provide an overview of different approaches to uncertainty quantification and abstention for machine learning and highlight how these techniques could improve the safety and reliability of current ML systems being used in healthcare settings. Effective quantification and communication of uncertainty could help to engender trust with healthcare workers, while providing safeguards against known failure modes of current machine learning approaches. As machine learning becomes further integrated into healthcare environments, the ability to say ``I'm not sure'' or ``I don't know'' when uncertain is a necessary capability to enable safe clinical deployment.} }
@article{WOS:000526850800009, title = {Machine learning in nephrology: scratching the surface}, journal = {CHINESE MEDICAL JOURNAL}, volume = {133}, pages = {687-698}, year = {2020}, issn = {0366-6999}, doi = {10.1097/CM9.0000000000000694}, author = {Li Qi and Fan Qiu-Ling and Han Qiu-Xia and Geng Wen-Jia and Zhao Huan-Huan and Ding Xiao-Nan and Yan Jing-Yao and Zhu Han-Yu}, abstract = {Machine learning shows enormous potential in facilitating decision-making regarding kidney diseases. With the development of data preservation and processing, as well as the advancement of machine learning algorithms, machine learning is expected to make remarkable breakthroughs in nephrology. Machine learning models have yielded many preliminaries to moderate and several excellent achievements in the fields, including analysis of renal pathological images, diagnosis and prognosis of chronic kidney diseases and acute kidney injury, as well as management of dialysis treatments. However, it is just scratching the surface of the field; at the same time, machine learning and its applications in renal diseases are facing a number of challenges. In this review, we discuss the application status, challenges and future prospects of machine learning in nephrology to help people further understand and improve the capacity for prediction, detection, and care quality in kidney diseases.} }
@article{WOS:000644443200006, title = {From Server-Based to Client-Based Machine Learning: A Comprehensive Survey}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3424660}, author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua}, abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users' local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users' sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.} }
@article{WOS:000466446500036, title = {Machine learning-assisted directed protein evolution with combinatorial libraries}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {116}, pages = {8852-8858}, year = {2019}, issn = {0027-8424}, doi = {10.1073/pnas.1901979116}, author = {Wu, Zachary and Kan, S. B. Jennifer and Lewis, Russell D. and Wittmann, Bruce J. and Arnold, Frances H.}, abstract = {To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si-H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93\\% and 79\\% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.} }
@article{WOS:000608918500001, title = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {21}, year = {2020}, issn = {1532-4435}, author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle}, abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.} }
@article{WOS:000969320000010, title = {Machine learning analysis: general features, requirements and cardiovascular applications}, journal = {MINERVA CARDIOLOGY AND ANGIOLOGY}, volume = {70}, pages = {67-74}, year = {2022}, issn = {2724-5683}, doi = {10.23736/S2724-5683.21.05637-4}, author = {Ricciardi, Carlo and Cuocolo, Renato and Megna, Rosario and Cesarelli, Mario and Petretta, Mario}, abstract = {Artificial intelligence represents the science which will probably change the future of medicine by solving actually challenging issues. In this special article, the general features of machine learning are discussed. First, a background explanation regarding the division of artificial intelligence, machine learning and deep learning is given and a focus on the structure of machine learning subgroups is shown. The traditional process of a machine learning analysis is described, starting from the collection of data, across features engineering, modelling and till the validation and deployment phase. Due to the several applications of machine learning performed in literature in the last decades and the lack of some guidelines, the need of a standardization for reporting machine learning analysis results emerged. Some possible standards for reporting machine learning results are identified and discussed deeply; these are related to study population (number of subjects), repeatability of the analysis, validation, results, comparison with current practice. The way to the use of machine learning in clinical practice is open and the hope is that, with emerging technology and advanced digital and computational tools, available from hospitalization and subsequently after discharge, it will also be possible, with the help of increasingly powerful hardware, to build assistance strategies useful in clinical practice.} }
@article{WOS:000936226800001, title = {A Machine Learning-Combined Flexible Sensor for Tactile Detection and Voice Recognition}, journal = {ACS APPLIED MATERIALS \\& INTERFACES}, volume = {15}, pages = {12551-12559}, year = {2023}, issn = {1944-8244}, doi = {10.1021/acsami.2c22287}, author = {Xie, Jiawang and Zhao, Yuzhi and Zhu, Dezhi and Li, Jiaqun and Qiao, Ming and He, Guangzhi and Deng, Shengfa and Yan, Jianfeng}, abstract = {Intelligent sensors have attracted substantial attention for various applications, including wearable electronics, artificial intelligence, healthcare monitoring, and human-machine interactions. However, there still remains a critical challenge in developing a multifunctional sensing system for complex signal detection and analysis in practical applications. Here, we develop a machine learning-combined flexible sensor for real-time tactile sensing and voice recognition through laser-induced graphitization. The intelligent sensor with a triboelectric layer can convert local pressure to an electrical signal through a contact electrification effect without external bias, which has a characteristic response behavior when exposed to various mechanical stimuli. With the special patterning design, a smart human-machine interaction controlling system composed of a digital arrayed touch panel is constructed to control electronic devices. Based on machine learning, the real-time monitoring and recognition of the changes of voice are achieved with high accuracy. The machine learning-empowered flexible sensor provides a promising platform for the development of flexible tactile sensing, real-time health detection, human-machine interaction, and intelligent wearable devices.} }
@article{WOS:001174091000001, title = {Revolutionizing physics: a comprehensive survey of machine learning applications}, journal = {FRONTIERS IN PHYSICS}, volume = {12}, year = {2024}, doi = {10.3389/fphy.2024.1322162}, author = {Suresh, Rahul and Bishnoi, Hardik and Kuklin, Artem V. and Parikh, Atharva and Molokeev, Maxim and Harinarayanan, R. and Gharat, Sarvesh and Hiba, P.}, abstract = {In the context of the 21st century and the fourth industrial revolution, the substantial proliferation of data has established it as a valuable resource, fostering enhanced computational capabilities across scientific disciplines, including physics. The integration of Machine Learning stands as a prominent solution to unravel the intricacies inherent to scientific data. While diverse machine learning algorithms find utility in various branches of physics, there exists a need for a systematic framework for the application of Machine Learning to the field. This review offers a comprehensive exploration of the fundamental principles and algorithms of Machine Learning, with a focus on their implementation within distinct domains of physics. The review delves into the contemporary trends of Machine Learning application in condensed matter physics, biophysics, astrophysics, material science, and addresses emerging challenges. The potential for Machine Learning to revolutionize the comprehension of intricate physical phenomena is underscored. Nevertheless, persisting challenges in the form of more efficient and precise algorithm development are acknowledged within this review.} }
@article{WOS:001040009600001, title = {No more free lunch: The increasing popularity of machine learning and financial market efficiency}, journal = {ECONOMIC AND POLITICAL STUDIES-EPS}, volume = {12}, pages = {34-57}, year = {2024}, issn = {2095-4816}, doi = {10.1080/20954816.2023.2230622}, author = {Feng, Jian and Liu, Xin}, abstract = {In this paper, we show that the increasing popularity of machine learning improves market efficiency. By analysing the performance of a set of popular machine learning-based investment strategies, we find that profits from these strategies experience significant declines since the wide adoption of machine learning techniques, especially for profits based on the more preferred method of neural networks. These declines mainly come from long legs. Using the `machine learning' Google search index as a proxy for machine learning-based trading intensity, we find that returns from the neural networks-based long-short and long-only strategies are weaker following high levels of machine learning intensity, while no relation is found between machine learning intensity and the short-only neural networks-based strategy.} }
@article{WOS:000523484900001, title = {Machine learning and clinical epigenetics: a review of challenges for diagnosis and classification}, journal = {CLINICAL EPIGENETICS}, volume = {12}, year = {2020}, issn = {1868-7075}, doi = {10.1186/s13148-020-00842-4}, author = {Rauschert, S. and Raubenheimer, K. and Melton, P. E. and Huang, R. C.}, abstract = {Background Machine learning is a sub-field of artificial intelligence, which utilises large data sets to make predictions for future events. Although most algorithms used in machine learning were developed as far back as the 1950s, the advent of big data in combination with dramatically increased computing power has spurred renewed interest in this technology over the last two decades. Main body Within the medical field, machine learning is promising in the development of assistive clinical tools for detection of e.g. cancers and prediction of disease. Recent advances in deep learning technologies, a sub-discipline of machine learning that requires less user input but more data and processing power, has provided even greater promise in assisting physicians to achieve accurate diagnoses. Within the fields of genetics and its sub-field epigenetics, both prime examples of complex data, machine learning methods are on the rise, as the field of personalised medicine is aiming for treatment of the individual based on their genetic and epigenetic profiles. Conclusion We now have an ever-growing number of reported epigenetic alterations in disease, and this offers a chance to increase sensitivity and specificity of future diagnostics and therapies. Currently, there are limited studies using machine learning applied to epigenetics. They pertain to a wide variety of disease states and have used mostly supervised machine learning methods.} }
@article{WOS:001389793200001, title = {Machine learning of weighted superposition attraction algorithm for optimization diesel engine performance and emission fueled with butanol-diesel biofuel}, journal = {AIN SHAMS ENGINEERING JOURNAL}, volume = {15}, year = {2024}, issn = {2090-4479}, doi = {10.1016/j.asej.2024.103126}, author = {Veza, Ibham and Karaoglan, Aslan Deniz and Akpinar, Sener and Spraggon, Martin and Idris, Muhammad}, abstract = {Machine learning (ML) is a subset of artificial intelligence (AI) and computer science that employs data and algorithms and mimics human learning to self-enhance its accuracy. In biofuel research, butanol is widely recognized as a prospective alternative biofuel. Butanol addition in diesel or combustion engine has been more and more studied recently. Gaining a comprehensive comprehension of butanol performance and emission characteristics using machine learning approach is an essential milestone in investigating alcohol-based biofuel addition in diesel engines. However, few studies investigated butanol effect on diesel engine emissions using machine learning for optimization. A novel optimization study is needed. This work aims to investigate the newly developed and efficient machine learning, weighted superposition attraction (WSA) algorithm, to optimize the emission and performance of diesel engines fuelled with butanol-diesel biofuel. Mathematical modeling between the factors (butanol (vol.\\%) and BMEP (bar)) and the responses (BTE (\\%), BSFC (g/kWh), Exhaust Temperature Texh (oC), NOx (g/kWh), CO (g/kWh), HC (g/kWh), and Smoke Opacity (\\%)) are governed using regression modeling. The optimized and best factor levels are determined employing the machine learning of WSA Algorithm. Confirmations are carried out. Optimization results indicate that the BTE is maximized, and the remainder of the responses are minimized.} }
@article{WOS:000701254700001, title = {Autism Spectrum Disorder Studies Using fMRI Data and Machine Learning: A Review}, journal = {FRONTIERS IN NEUROSCIENCE}, volume = {15}, year = {2021}, doi = {10.3389/fnins.2021.697870}, author = {Liu, Meijie and Li, Baojuan and Hu, Dewen}, abstract = {Machine learning methods have been frequently applied in the field of cognitive neuroscience in the last decade. A great deal of attention has been attracted to introduce machine learning methods to study the autism spectrum disorder (ASD) in order to find out its neurophysiological underpinnings. In this paper, we presented a comprehensive review about the previous studies since 2011, which applied machine learning methods to analyze the functional magnetic resonance imaging (fMRI) data of autistic individuals and the typical controls (TCs). The all-round process was covered, including feature construction from raw fMRI data, feature selection methods, machine learning methods, factors for high classification accuracy, and critical conclusions. Applying different machine learning methods and fMRI data acquired from different sites, classification accuracies were obtained ranging from 48.3\\% up to 97\\%, and informative brain regions and networks were located. Through thorough analysis, high classification accuracies were found to usually occur in the studies which involved task-based fMRI data, single dataset for some selection principle, effective feature selection methods, or advanced machine learning methods. Advanced deep learning together with the multi-site Autism Brain Imaging Data Exchange (ABIDE) dataset became research trends especially in the recent 4 years. In the future, advanced feature selection and machine learning methods combined with multi-site dataset or easily operated task-based fMRI data may appear to have the potentiality to serve as a promising diagnostic tool for ASD.} }
@article{WOS:001336722300165, title = {Quantum Machine Learning for Additive Manufacturing Process Monitoring}, journal = {MANUFACTURING LETTERS}, volume = {41}, pages = {1415-1422}, year = {2024}, issn = {2213-8463}, doi = {10.1016/j.mfglet.2024.09.168}, author = {Choi, Eunsik and Sul, Jinhwan and Kim, Jungin E. and Hong, Sungjin and Gonzalez, Beatriz Izquierdo and Cembellin, Pablo and Wang, Yan}, abstract = {Machine learning is useful for analyzing and monitoring complex manufacturing processes. However, it has several limitations including the curse-of-dimensionality and lack of training data. In this paper, we propose a quantum machine learning strategy to tackle these challenges. Quantum support vector machine is applied to identify the states of machines in fused filament fabrication process based on acoustic emission data. Quantum convolutional neural network is used to detect spatters in laser powder bed fusion process based on coaxial optical images. Our results show that quantum machine learning can achieve the similar accuracy levels of predictions by classical machine learning counterparts, but with exponentially fewer parameters. (c) 2024 The Authors. Published by ELSEVIER Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)} }
@article{WOS:001301816000001, title = {An interpretable machine learning-based pitting corrosion depth prediction model for steel drinking water pipelines}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {190}, pages = {571-585}, year = {2024}, issn = {0957-5820}, doi = {10.1016/j.psep.2024.08.038}, author = {Kim, Taehyeon and Kim, Kibum and Hyung, Jinseok and Park, Haekeum and Oh, Yoojin and Koo, Jayong}, abstract = {Steel pipes are a crucial element of the water supply system and are necessary for safely delivering large quantities of water from purification plants to consumers. Corrosion is a significant factor that deteriorates the interior and exterior of the steel pipes. Although the effectiveness of machine learning has been demonstrated in various fields, machine learning has rarely been used to identify corrosion mechanisms in buried steel pipes. A hybrid machine-learning-based corrosion depth prediction model was developed by integrating a corrosion depth trend prediction model based only on elapsed years with machine-learning algorithms. Shapley additive explanation (SHAP) was used to analyze the hybrid machine-learning-based corrosion depth prediction models, revealing corrosion mechanisms and explaining the interactions among influencing factors through global and local interpretations. The SHAP local interpretation showed that the hybrid machine-learning-based corrosion depth prediction models can effectively capture the interrelationship between soil and water corrosiveness.} }
@article{WOS:000382418300016, title = {Machine learning for medical images analysis}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {33}, pages = {91-93}, year = {2016}, issn = {1361-8415}, doi = {10.1016/j.media.2016.06.002}, author = {Criminisi, A.}, abstract = {This article discusses the application of machine learning for the analysis of medical images. Specifically: (i) We show how a special type of learning models can be thought of as automatically optimized, hierarchically-structured, rule-based algorithms, and (ii) We discuss how the issue of collecting large labelled datasets applies to both conventional algorithms as well as machine learning techniques. The size of the training database is a function of model complexity rather than a characteristic of machine learning methods. Crown Copyright (C) 2016 Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000485090400075, title = {Using Artificial Intelligence To Forecast Water Oxidation Catalysts}, journal = {ACS CATALYSIS}, volume = {9}, pages = {8383-8387}, year = {2019}, issn = {2155-5435}, doi = {10.1021/acscatal.9b01985}, author = {Palkovits, Regina and Palkovits, Stefan}, abstract = {Artificial intelligence and various types of machine learning are of increasing interest not only in the natural sciences but also in a wide range of applied and engineering sciences. In this study, we rethink the view on combinatorial heterogeneous catalysis and combine machine learning methods with combinatorial approaches in electrocatalysis. Several machine learning methods were used to forecast water oxidation catalysts on the basis of literature published data sets and data from our own work. The machine learning models exhibit a decent prediction precision based on the data sets available and confirm that even simple models are suitable for good forecasts.} }
@article{WOS:000793810100003, title = {Hyperspectral Anomaly Detection Based on Machine Learning: An Overview}, journal = {IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING}, volume = {15}, pages = {3351-3364}, year = {2022}, issn = {1939-1404}, doi = {10.1109/JSTARS.2022.3167830}, author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei}, abstract = {Hyperspectral anomaly detection (HAD) is an important hyperspectral image application. HAD can find pixels with anomalous spectral signatures compared with their neighbor background without any prior information. While most of the existed researches are related to statistic-based and distance-based techniques, by summarizing the background samples with certain models, and then, finding the very few outliers by various distance metrics, this review focuses on the HAD based on machine learning methods, which have witnessed remarkable progress in the recent years. In particular, these studies can generally be grouped into the traditional machine learning and deep-learning-based methods. Several representative HAD methods, including both traditional machine and deep-learning-based methods, are then conducted on four real HSIs in the experiments. Finally, conclusions regarding HAD are summarized, and prospects and future development direction are discussed.} }
@article{WOS:000282915500002, title = {The security of machine learning}, journal = {MACHINE LEARNING}, volume = {81}, pages = {121-148}, year = {2010}, issn = {0885-6125}, doi = {10.1007/s10994-010-5188-5}, author = {Barreno, Marco and Nelson, Blaine and Joseph, Anthony D. and Tygar, J. D.}, abstract = {Machine learning's ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.} }
@article{WOS:000660860700001, title = {Quantum machine learning and quantum biomimetics: A perspective}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {1}, year = {2020}, doi = {10.1088/2632-2153/ab9803}, author = {Lamata, Lucas}, abstract = {Quantum machine learning has emerged as an exciting and promising paradigm inside quantum technologies. It may permit, on the one hand, to carry out more efficient machine learning calculations by means of quantum devices, while, on the other hand, to employ machine learning techniques to better control quantum systems. Inside quantum machine learning, quantum reinforcement learning aims at developing `intelligent' quantum agents that may interact with the outer world and adapt to it, with the strategy of achieving some final goal. Another paradigm inside quantum machine learning is that of quantum autoencoders, which may allow one for employing fewer resources in a quantum device via a training process. Moreover, the field of quantum biomimetics aims at establishing analogies between biological and quantum systems, to look for previously inadvertent connections that may enable useful applications. Two recent examples are the concepts of quantum artificial life, as well as of quantum memristors. In this Perspective, we give an overview of these topics, describing the related research carried out by the scientific community.} }
@article{WOS:000674276800001, title = {Securing Machine Learning in the Cloud: A Systematic Review of Cloud Machine Learning Security}, journal = {FRONTIERS IN BIG DATA}, volume = {3}, year = {2020}, doi = {10.3389/fdata.2020.587139}, author = {Qayyum, Adnan and Ijaz, Aneeqa and Usama, Muhammad and Iqbal, Waleed and Qadir, Junaid and Elkhatib, Yehia and Al-Fuqaha, Ala}, abstract = {With the advances in machine learning (ML) and deep learning (DL) techniques, and the potency of cloud computing in offering services efficiently and cost-effectively, Machine Learning as a Service (MLaaS) cloud platforms have become popular. In addition, there is increasing adoption of third-party cloud services for outsourcing training of DL models, which requires substantial costly computational resources (e.g., high-performance graphics processing units (GPUs)). Such widespread usage of cloud-hosted ML/DL services opens a wide range of attack surfaces for adversaries to exploit the ML/DL system to achieve malicious goals. In this article, we conduct a systematic evaluation of literature of cloud-hosted ML/DL models along both the important dimensions-attacks and defenses-related to their security. Our systematic review identified a total of 31 related articles out of which 19 focused on attack, six focused on defense, and six focused on both attack and defense. Our evaluation reveals that there is an increasing interest from the research community on the perspective of attacking and defending different attacks on Machine Learning as a Service platforms. In addition, we identify the limitations and pitfalls of the analyzed articles and highlight open research issues that require further investigation.} }
@article{WOS:000416161200002, title = {Ensemble of Efficient Minimal Learning Machines for Classification and Regression}, journal = {NEURAL PROCESSING LETTERS}, volume = {46}, pages = {751-766}, year = {2017}, issn = {1370-4621}, doi = {10.1007/s11063-017-9587-5}, author = {Mesquita, Diego P. P. and Gomes, Joao P. P. and Souza Junior, Amauri H.}, abstract = {Minimal Learning Machine (MLM) is a recently proposed supervised learning algorithm with performance comparable to most state-of-the-art machine learning methods. In this work, we propose ensemble methods for classification and regression using MLMs. The goal of ensemble strategies is to produce more robust and accurate models when compared to a single classifier or regression model. Despite its successful application, MLM employs a computationally intensive optimization problem as part of its test procedure (out-of-sample data estimation). This becomes even more noticeable in the context of ensemble learning, where multiple models are used. Aiming to provide fast alternatives to the standard MLM, we also propose the Nearest Neighbor Minimal Learning Machine and the Cubic Equation Minimal Learning Machine to cope with classification and single-output regression problems, respectively. The experimental assessment conducted on real-world datasets reports that ensemble of fast MLMs perform comparably or superiorly to reference machine learning algorithms.} }
@article{WOS:000608126900013, title = {Prediction Machines: Applied Machine Learning for Therapeutic Protein Design and Development}, journal = {JOURNAL OF PHARMACEUTICAL SCIENCES}, volume = {110}, pages = {665-681}, year = {2021}, issn = {0022-3549}, doi = {10.1016/j.xphs.2020.11.034}, author = {Kamerzell, Tim J. and Middaugh, C. Russell}, abstract = {The rapid growth in technological advances and quantity of scientific data over the past decade has led to several challenges including data storage and analysis. Accurate models of complex datasets were previously difficult to develop and interpret. However, improvements in machine learning algorithms have since enabled unparalleled classification and prediction capabilities. The application of machine learning can be seen throughout diverse industries due to their ease of use and interpretability. In this review, we describe popular machine learning algorithms and highlight their application in pharmaceutical protein development. Machine learning models have now been applied to better understand the nonlinear concentration dependent viscosity of protein solutions, predict protein oxidation and deamidation rates, classify sub-visible particles and compare the physical stability of proteins. We also applied several machine learning algorithms using previously published data and describe models with improved predictions and classification. The authors hope that this review can be used as a resource to others and encourage continued application of machine learning algorithms to problems in pharmaceutical protein development.} }
@article{WOS:000628311200001, title = {Machine Learning and Novel Biomarkers for the Diagnosis of Alzheimer's Disease}, journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}, volume = {22}, year = {2021}, issn = {1661-6596}, doi = {10.3390/ijms22052761}, author = {Chang, Chun-Hung and Lin, Chieh-Hsin and Lane, Hsien-Yuan}, abstract = {Background: Alzheimer's disease (AD) is a complex and severe neurodegenerative disease that still lacks effective methods of diagnosis. The current diagnostic methods of AD rely on cognitive tests, imaging techniques and cerebrospinal fluid (CSF) levels of amyloid-beta 1-42 (A beta 42), total tau protein and hyperphosphorylated tau (p-tau). However, the available methods are expensive and relatively invasive. Artificial intelligence techniques like machine learning tools have being increasingly used in precision diagnosis. Methods: We conducted a meta-analysis to investigate the machine learning and novel biomarkers for the diagnosis of AD. Methods: We searched PubMed, the Cochrane Central Register of Controlled Trials, and the Cochrane Database of Systematic Reviews for reviews and trials that investigated the machine learning and novel biomarkers in diagnosis of AD. Results: In additional to A beta and tau-related biomarkers, biomarkers according to other mechanisms of AD pathology have been investigated. Neuronal injury biomarker includes neurofiliament light (NFL). Biomarkers about synaptic dysfunction and/or loss includes neurogranin, BACE1, synaptotagmin, SNAP-25, GAP-43, synaptophysin. Biomarkers about neuroinflammation includes sTREM2, and YKL-40. Besides, d-glutamate is one of coagonists at the NMDARs. Several machine learning algorithms including support vector machine, logistic regression, random forest, and naive Bayes) to build an optimal predictive model to distinguish patients with AD from healthy controls. Conclusions: Our results revealed machine learning with novel biomarkers and multiple variables may increase the sensitivity and specificity in diagnosis of AD. Rapid and cost-effective HPLC for biomarkers and machine learning algorithms may assist physicians in diagnosing AD in outpatient clinics.} }
@article{WOS:000485885700001, title = {MACHINE LEARNING METHODS FOR SYSTEMIC RISK ANALYSIS IN FINANCIAL SECTORS}, journal = {TECHNOLOGICAL AND ECONOMIC DEVELOPMENT OF ECONOMY}, volume = {25}, pages = {716-742}, year = {2019}, issn = {2029-4913}, doi = {10.3846/tede.2019.8740}, author = {Kou, Gang and Chao, Xiangrui and Peng, Yi and Alsaadi, Fawaz E. and Herrera-Viedma, Enrique}, abstract = {Financial systemic risk is an important issue in economics and financial systems. Trying to detect and respond to systemic risk with growing amounts of data produced in financial markets and systems, a lot of researchers have increasingly employed machine learning methods. Machine learning methods study the mechanisms of outbreak and contagion of systemic risk in the financial network and improve the current regulation of the financial market and industry. In this paper, we survey existing researches and methodologies on assessment and measurement of financial systemic risk combined with machine learning technologies, including big data analysis, network analysis and sentiment analysis, etc. In addition, we identify future challenges, and suggest further research topics. The main purpose of this paper is to introduce current researches on financial systemic risk with machine learning methods and to propose directions for future work.} }
@article{WOS:000705637500013, title = {Machine learning models for forecasting power electricity consumption using a high dimensional dataset}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {187}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2021.115917}, author = {Albuquerque, Pedro C. and Cajueiro, Daniel O. and Rossi, Marina D. C.}, abstract = {We use regularized machine learning models to forecast Brazilian power electricity consumption for short and medium terms. We compare our models to benchmark specifications such as Random Walk and Autoregressive Integrated Moving Average. Our results show that machine learning methods, especially Random Forest and Lasso Lars, give more accurate forecasts for all horizons. Random Forest and Lasso Lars managed to keep up with the trend and the seasonality for various time horizons. The gain in predicting PEC using machine learning models relative to the benchmarks is considerably higher for the very short-term. Machine learning variable selection further shows that lagged consumption values are extremely important for very short-term forecasting due to the series high autocorrelation. Other variables such as weather and calendar variables are important for longer time horizons.} }
@article{WOS:000752230700001, title = {Machine learning for brain signal analysis}, journal = {INTERNATIONAL JOURNAL OF BIOLOGY AND CHEMISTRY}, volume = {14}, pages = {4-11}, year = {2021}, issn = {2218-7979}, doi = {10.26577/ijbch.2021.v14.i2.01}, author = {Makhmet, A. S. and Sharaev, M. G. and Dyusembaev, A. E. and Kustubayeva, A. M.}, abstract = {Machine learning (ML) is an effective tool for analysing signals from the human brain. Machine Learning techniques provide new insight into the understanding of brain function in healthy subjects and patients with neurological and mental disorders. Here we introduce the application of machine learning to resonance imaging (fMRI) and Electroencephalography (EEG). The article provides a brief overview of the theoretical concept of machine learning and its types: supervised, unsupervised and reinforcement learning. The potential of machine learning applications in pathology is discussed. Differences between EEG and fMRI methods regarding machine learning application and an overview of the techniques employed in different research studies are reviewed. The new machine learning methods invented for analysis of brain signals in the resting ate and during the performance of the different cognitive tasks would be useful and worth considering in other domains, not limited to medicine.} }
@article{WOS:000209236900011, title = {Gradient boosting machines, a tutorial}, journal = {FRONTIERS IN NEUROROBOTICS}, volume = {7}, year = {2013}, issn = {1662-5218}, doi = {10.3389/fnbot.2013.00021}, author = {Natekin, Alexey and Knoll, Alois}, abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.} }
@article{WOS:000664641500020, title = {COVID-19 detection using federated machine learning}, journal = {PLOS ONE}, volume = {16}, year = {2021}, issn = {1932-6203}, doi = {10.1371/journal.pone.0252573}, author = {Salam, Mustafa Abdul and Taha, Sanaa and Ramadan, Mohamed}, abstract = {The current COVID-19 pandemic threatens human life, health, and productivity. AI plays an essential role in COVID-19 case classification as we can apply machine learning models on COVID-19 case data to predict infectious cases and recovery rates using chest x-ray. Accessing patient's private data violates patient privacy and traditional machine learning model requires accessing or transferring whole data to train the model. In recent years, there has been increasing interest in federated machine learning, as it provides an effective solution for data privacy, centralized computation, and high computation power. In this paper, we studied the efficacy of federated learning versus traditional learning by developing two machine learning models (a federated learning model and a traditional machine learning model)using Keras and TensorFlow federated, we used a descriptive dataset and chest x-ray (CXR) images from COVID-19 patients. During the model training stage, we tried to identify which factors affect model prediction accuracy and loss like activation function, model optimizer, learning rate, number of rounds, and data Size, we kept recording and plotting the model loss and prediction accuracy per each training round, to identify which factors affect the model performance, and we found that softmax activation function and SGD optimizer give better prediction accuracy and loss, changing the number of rounds and learning rate has slightly effect on model prediction accuracy and prediction loss but increasing the data size did not have any effect on model prediction accuracy and prediction loss. finally, we build a comparison between the proposed models' loss, accuracy, and performance speed, the results demonstrate that the federated machine learning model has a better prediction accuracy and loss but higher performance time than the traditional machine learning model.} }
@article{WOS:000432883500001, title = {A machine learning framework to forecast wave conditions}, journal = {COASTAL ENGINEERING}, volume = {137}, pages = {1-10}, year = {2018}, issn = {0378-3839}, doi = {10.1016/j.coastaleng.2018.03.004}, author = {James, Scott C. and Zhang, Yushan and O'Donncha, Fearghal}, abstract = {A machine learning framework is developed to estimate ocean-wave conditions. By supervised training of machine learning models on many thousands of iterations of a physics-based wave model, accurate representations of significant wave heights and period can be used to predict ocean conditions. A model of Monterey Bay was used as the example test site; it was forced by measured wave conditions, ocean-current nowcasts, and reported winds. These input data along with model outputs of spatially variable wave heights and characteristic period were aggregated into supervised learning training and test data sets, which were supplied to machine learning models. These machine learning models replicated wave heights from the physics-based model with a root-mean-squared error of 9 cm and correctly identify over 90\\% of the characteristic periods for the test-data sets. Impressively, transforming model inputs to outputs through matrix operations requires only a fraction (< 1/1, 000 th) of the computation time compared to forecasting with the physics-based model.} }
@article{WOS:000829738400001, title = {Materials Discovery With Machine Learning and Knowledge Discovery}, journal = {FRONTIERS IN CHEMISTRY}, volume = {10}, year = {2022}, issn = {2296-2646}, doi = {10.3389/fchem.2022.930369}, author = {Oliveira Jr, Osvaldo N. and Oliveira, Maria Cristina F.}, abstract = {Machine learning and other artificial intelligence methods are gaining increasing prominence in chemistry and materials sciences, especially for materials design and discovery, and in data analysis of results generated by sensors and biosensors. In this paper, we present a perspective on this current use of machine learning, and discuss the prospects of the future impact of extending the use of machine learning to encompass knowledge discovery as an essential step towards a new paradigm of machine-generated knowledge. The reasons why results so far have been limited are given with a discussion of the limitations of machine learning in tasks requiring interpretation. Also discussed is the need to adapt the training of students and scientists in chemistry and materials sciences, to better explore the potential of artificial intelligence capabilities.} }
@article{WOS:000445712400274, title = {Machine Learning in Agriculture: A Review}, journal = {SENSORS}, volume = {18}, year = {2018}, doi = {10.3390/s18082674}, author = {Liakos, Konstantinos G. and Busato, Patrizia and Moshou, Dimitrios and Pearson, Simon and Bochtis, Dionysis}, abstract = {Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.} }
@article{WOS:000841672300001, title = {Machine Learning Approaches to TCR Repertoire Analysis}, journal = {FRONTIERS IN IMMUNOLOGY}, volume = {13}, year = {2022}, issn = {1664-3224}, doi = {10.3389/fimmu.2022.858057}, author = {Katayama, Yotaro and Yokota, Ryo and Akiyama, Taishin and Kobayashi, Tetsuya J.}, abstract = {Sparked by the development of genome sequencing technology, the quantity and quality of data handled in immunological research have been changing dramatically. Various data and database platforms are now driving the rapid progress of machine learning for immunological data analysis. Of various topics in immunology, T cell receptor repertoire analysis is one of the most important targets of machine learning for assessing the state and abnormalities of immune systems. In this paper, we review recent repertoire analysis methods based on machine learning and deep learning and discuss their prospects.} }
@article{WOS:000477891100005, title = {Machine learning in autistic spectrum disorder behavioral research: A review and ways forward}, journal = {INFORMATICS FOR HEALTH \\& SOCIAL CARE}, volume = {44}, pages = {278-297}, year = {2019}, issn = {1753-8157}, doi = {10.1080/17538157.2017.1399132}, author = {Thabtah, Fadi}, abstract = {Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.} }
@article{WOS:000450513100004, title = {eDoctor: machine learning and the future of medicine}, journal = {JOURNAL OF INTERNAL MEDICINE}, volume = {284}, pages = {603-619}, year = {2018}, issn = {0954-6820}, doi = {10.1111/joim.12822}, author = {Handelman, G. S. and Kok, H. K. and Chandra, R. V. and Razavi, A. H. and Lee, M. J. and Asadi, H.}, abstract = {Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer-aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine.} }
@article{WOS:000430994500003, title = {A robust extreme learning machine for modeling a small-scale turbojet engine}, journal = {APPLIED ENERGY}, volume = {218}, pages = {22-35}, year = {2018}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2018.02.175}, author = {Zhao, Yong-Ping and Hu, Qian-Kun and Xu, Jian-Guo and Li, Bing and Huang, Gong and Pan, Ying-Ting}, abstract = {In this paper, a robust extreme learning machine is proposed. In comparison with the original extreme learning machine and the regularized extreme learning machine, this robust algorithm minimizes both the mean and variance of modeling errors in the objective function to overcome the bias-variance dilemma. As a result, its generalization performance and robustness are enhanced, and these merits are further proved theoretically. In addition, this proposed algorithm can keep the same computational efficiency as the original extreme learning machine and the regularized extreme learning machine. Then, several benchmark data sets are used to test the effectiveness and soundness of the proposed algorithm. Finally, it is employed to model a real small-scale turbojet engine. This engine is fit well. Especially, on the idle phase, where the signal-to-noise ratio is low and it is very hard to model, the proposed algorithm performs well and its robustness is sufficiently showcased. All in all, the proposed algorithm provides a candidate technique for modeling real systems.} }
@article{WOS:000448616200004, title = {Machine Learning in Compiler Optimization}, journal = {PROCEEDINGS OF THE IEEE}, volume = {106}, pages = {1879-1901}, year = {2018}, issn = {0018-9219}, doi = {10.1109/JPROC.2018.2817118}, author = {Wang, Zheng and O'Boyle, Michael}, abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.} }
@article{WOS:001239078200005, title = {Interpretable Machine Learning for Discovery: Statistical Challenges and Opportunities}, journal = {ANNUAL REVIEW OF STATISTICS AND ITS APPLICATION}, volume = {11}, pages = {97-121}, year = {2024}, issn = {2326-8298}, doi = {10.1146/annurev-statistics-040120-030919}, author = {Allen, Genevera I. and Gan, Luqin and Zheng, Lili}, abstract = {New technologies have led to vast troves of large and complex data sets across many scientific domains and industries. People routinely use machine learning techniques not only to process, visualize, and make predictions from these big data, but also to make data-driven discoveries. These discoveries are often made using interpretable machine learning, or machine learning models and techniques that yield human-understandable insights. In this article, we discuss and review the field of interpretable machine learning, focusing especially on the techniques, as they are often employed to generate new knowledge or make discoveries from large data sets.We outline the types of discoveries that can be made using interpretable machine learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science.We discuss validation both from a practical perspective, reviewing approaches based on data-splitting and stability, as well as from a theoretical perspective, reviewing statistical results on model selection consistency and uncertainty quantification via statistical inference. Finally, we conclude by highlighting open challenges in using interpretable machine learning techniques to make discoveries, including gaps between theory and practice for validating data-driven discoveries.} }
@article{WOS:001486910100001, title = {Quantum machine learning based wind turbine condition monitoring: State of the art and future prospects}, journal = {ENERGY CONVERSION AND MANAGEMENT}, volume = {332}, year = {2025}, issn = {0196-8904}, doi = {10.1016/j.enconman.2025.119694}, author = {Zhang, Zhefeng and Wu, Yueqi and Ma, Xiandong}, abstract = {Wind energy, as a popular renewable resource, has gained extensive development and application in recent decades. Effective condition monitoring and fault diagnosis are crucial for ensuring the reliable operation of wind turbines. While conventional machine learning methods have been widely used in wind turbine condition monitoring, these approaches often face challenges such as complex feature extraction, limited model generalization, and high computational costs when dealing with large-scale, high-dimensional, and complex datasets. The emergence of quantum computing has opened up a new paradigm of machine learning algorithms. Quantum machine learning combines the advantages of quantum computing and machine learning, with the potential to surpass classical computational capabilities. This paper firstly reviews applications and limitations of the state-of-the-art machine learning-based condition monitoring techniques for wind turbines. It then reviews the fundamentals of quantum computing, quantum machine learning algorithms and their applications, covering quantum-based feature extraction, classification and regression for fault detection and the use of quantum neural networks for predictive maintenance. Through comparison, it is observed that quantum machine learning methods, even without extensive optimization, can achieve accuracy levels comparable to those of optimized conventional machine learning approaches. The challenges of applying quantum machine learning are also addressed, along with the future research and development prospects. The objective of this review is to fill a gap in the published literature by providing a new paradigm approach for wind turbine condition monitoring. By promoting quantum machine learning in this field, the reliability and efficiency of wind power systems are ultimately sought to be enhanced.} }
@article{WOS:000472796800001, title = {Machine Learning Methods for Ranking}, journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING}, volume = {29}, pages = {729-761}, year = {2019}, issn = {0218-1940}, doi = {10.1142/S021819401930001X}, author = {Rahangdale, Ashwini and Raut, Shital}, abstract = {Learning-to-rank is one of the learning frameworks in machine learning and it aims to organize the objects in a particular order according to their preference, relevance or ranking. In this paper, we give a comprehensive survey for learning-to-rank. First, we discuss the different approaches along with different machine learning methods such as regression, SVM, neural network-based, evolutionary, boosting method. In order to compare different approaches: we discuss the characteristics of each approach. In addition to that, learning-to-rank algorithms combine with other machine learning paradigms such as semi-supervised learning, active learning, reinforcement learning and deep learning. The learning-to-rank models employ with parallel or big data analytics to review computational and storage advantage. Many real-time applications use learning-to-rank for preference learning. In regard to this, we introduce some representative works. Finally, we highlighted future directions to investigate learning-to-rank methods.} }
@article{WOS:001059216400001, title = {An integrated machine learning framework with uncertainty quantification for three-dimensional lithological modeling from multi-source geophysical data and drilling data}, journal = {ENGINEERING GEOLOGY}, volume = {324}, year = {2023}, issn = {0013-7952}, doi = {10.1016/j.enggeo.2023.107255}, author = {Zhang, Zhiqiang and Wang, Gongwen and Carranza, Emmanuel John M. and Liu, Chong and Li, Junjian and Fu, Chao and Liu, Xinxing and Chen, Chao and Fan, Junjie and Dong, Yulong}, abstract = {Nowadays, it is commonplace for geological surveys to integrate multi-source geophysical data and drilling data in order to construct three-dimensional (3D) lithological models. In this context, manual translation of complex geophysical data into parameters used for 3D lithological modeling is challenging. Machine learning has recently shown great potential in 3D lithological modeling. However, the performance of machine learning algorithm is influenced by the imbalance in number of categories of lithological samples. In addition, the uncertainty associated with 3D lithological modeling by machine learning has rarely been quantified. This study presents a novel integrated machine learning framework to address the imbalance issue and to quantify uncertainty in 3D lithological modeling. As its novelty, our integrated machine learning framework can subdivide total uncertainty into aleatoric and epistemic uncertainties in the 3D lithological modeling procedure by stochastic gradient Langevin boosting. Another innovation of this study is the use of Bayesian hyperparameter optimization for automatic tuning of hyperparameters of the integrated machine learning framework. The 3D lithological and uncertainty modeling case study in the Jiaojia-Sanshandao gold district of China demonstrated the superiority of our proposed integrated machine learning framework. The proposed framework has great potential in integrating multi-source geophysical and drilling data for 3D lithological and uncertainty modeling in engineering geology.} }
@article{WOS:000888574800002, title = {Tiny Machine Learning for Resource-Constrained Microcontrollers}, journal = {JOURNAL OF SENSORS}, volume = {2022}, year = {2022}, issn = {1687-725X}, doi = {10.1155/2022/7437023}, author = {Immonen, Riku and Hamalainen, Timo}, abstract = {We use 250 billion microcontrollers daily in electronic devices that are capable of running machine learning models inside them. Unfortunately, most of these microcontrollers are highly constrained in terms of computational resources, such as memory usage or clock speed. These are exactly the same resources that play a key role in teaching and running a machine learning model with a basic computer. However, in a microcontroller environment, constrained resources make a critical difference. Therefore, a new paradigm known as tiny machine learning had to be created to meet the constrained requirements of the embedded devices. In this review, we discuss the resource optimization challenges of tiny machine learning and different methods, such as quantization, pruning, and clustering, that can be used to overcome these resource difficulties. Furthermore, we summarize the present state of tiny machine learning frameworks, libraries, development environments, and tools. The benchmarking of tiny machine learning devices is another thing to be concerned about; these same constraints of the microcontrollers and diversity of hardware and software turn to benchmark challenges that must be resolved before it is possible to measure performance differences reliably between embedded devices. We also discuss emerging techniques and approaches to boost and expand the tiny machine learning process and improve data privacy and security. In the end, we form a conclusion about tiny machine learning and its future development.} }
@article{WOS:000555753600001, title = {The Number of Confirmed Cases of Covid-19 by using Machine Learning: Methods and Challenges}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {2645-2653}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09472-8}, author = {Ahmad, Amir and Garhwal, Sunita and Ray, Santosh Kumar and Kumar, Gagan and Malebary, Sharaf Jameel and Barukab, Omar Mohammed}, abstract = {Covid-19 is one of the biggest health challenges that the world has ever faced. Public health policy makers need the reliable prediction of the confirmed cases in future to plan medical facilities. Machine learning methods learn from the historical data and make predictions about the events. Machine learning methods have been used to predict the number of confirmed cases of Covid-19. In this paper, we present a detailed review of these research papers. We present a taxonomy that groups them in four categories. We further present the challenges in this field. We provide suggestions to the machine learning practitioners to improve the performance of machine learning methods for the prediction of confirmed cases of Covid-19.} }
@article{WOS:000678361100002, title = {Pairing conceptual modeling with machine learning}, journal = {DATA \\& KNOWLEDGE ENGINEERING}, volume = {134}, year = {2021}, issn = {0169-023X}, doi = {10.1016/j.datak.2021.101909}, author = {Maass, Wolfgang and Storey, Veda C.}, abstract = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.} }
@article{WOS:001079720800001, title = {The rise of the machines: A state-of-the-art technical review on process modelling and machine learning within hydrogen production with carbon capture}, journal = {GAS SCIENCE AND ENGINEERING}, volume = {118}, year = {2023}, issn = {2949-9097}, doi = {10.1016/j.jgsce.2023.205104}, author = {Davies, William George and Babamohammadi, Shervan and Yang, Yang and Soltani, Salman Masoudi}, abstract = {This study aims to present a compendious yet technical scrutiny of the current trends in process modelling as well as the implementation of machine learning within combined hydrogen production and carbon capture (i.e. blue hydrogen). The paper is intended to accurately portray the role that machine learning is anticipated to play within research and development in blue hydrogen production in the forthcoming years. This covers the implementation of machine learning at both material and process development levels. The paper provides a concise overview of the current trends in blue hydrogen production, as well as an intro to machine learning and process modelling within the same context. We have reinforced our paper by first summarising a brief description of the key ``tools'' used in machine learning and process modelling, before painstakingly examining the imple-mentation of these techniques in blue hydrogen production and the less-discovered merits and de-merits.Ultimately, the paper depicts a clear picture of the advancements in machine learning and the major role it is expected to play in accelerating research and development in blue hydrogen production on both material and process development fronts. The paper strives to shed some light on the key advantages that machine learning has to offer in blue hydrogen for future research work.} }
@article{WOS:000518864800005, title = {A Framework of Using Machine Learning Approaches for Short-Term Solar Power Forecasting}, journal = {JOURNAL OF ELECTRICAL ENGINEERING \\& TECHNOLOGY}, volume = {15}, pages = {561-569}, year = {2020}, issn = {1975-0102}, doi = {10.1007/s42835-020-00346-4}, author = {Munawar, Usman and Wang, Zhanle}, abstract = {Various machine learning approaches are widely applied for short-term solar power forecasting, which is highly demanded for renewable energy integration and power system planning. However, appropriate selection of machine learning models and data features is a significant challenge. In this study, a framework is developed to quantitatively evaluate various models and feature selection methods, and the best combination for short-term solar power forecasting is discovered. More specifically, the machine learning methods include the random forest, artificial neural network and extreme gradient boosting (XGBoost), and the feature selection techniques include the feature importance and principle component analysis (PCA). All possible combinations of these machine learning and feature selection methods are developed and evaluated for solar power forecasting. The best ensemble of machine learning methods and feature selection techniques is identified for solar power forecasting in Hawaii, US. Simulation results show that the XGBoost method with features selected by the PCA method outperforms the other approaches. In addition, the random forest and XGBoost models have rarely been used for short-term solar forecasting. This framework can be used to select appropriate machine learning approaches for short-term solar power forecasting and the simulation results can be used as a baseline for comparison.} }
@article{WOS:000982564700004, title = {Comparison of Machine Learning Based on Category Theory}, journal = {JOURNAL OF WEB ENGINEERING}, volume = {22}, pages = {41-54}, year = {2023}, issn = {1540-9589}, doi = {10.13052/jwe1540-9589.2213}, author = {Zhao, Heng and Chen, Yixing and Fu, Xianghua}, abstract = {In recent years, machine learning has been widely used in data analysis of network engineering. The increasing types of model and data enhance the complexity of machine learning. In this paper, we propose a mathematical structure based on category theory as a combination of machine learning that combines multiple theories of data mining. We aim to study machine learning from the perspective of classification theory. Category theory utilizes mathematical language to connect the various structures of machine learning. We implement the representation of machine learning with category theory. In the experimental section, slice categories and functors are introduced in detail to model the data preprocessing. We use functors to preprocess the benchmark dataset and evaluate the accuracy of nine machine learning models. A key contribution is the representation of slice categories. This study provides a structural perspective of machine learning and a general method for the combination of category theory and machine learning.} }
@article{WOS:000721705800032, title = {Documentation to facilitate communication between dataset creators and consumers}, journal = {COMMUNICATIONS OF THE ACM}, volume = {64}, pages = {86-92}, year = {2021}, issn = {0001-0782}, doi = {10.1145/3458723}, author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume, III, Hal and Crawford, Kate}, abstract = {DATA PLAYS A critical role in machine learning. Every machine learning model is trained and evaluated using data, quite often in the form of static datasets. The characteristics of these datasets fundamentally influence a model's behavior: a model is unlikely to perform well in the wild if its deployment context does not match its training or evaluation datasets, or if these datasets reflect unwanted societal biases. Mismatches like this can have especially severe consequences when machine learning models are used in high-stakes domains, such as criminal justice,(1,13,24) hiring,(19) critical infrastructure,(11,21) and finance.(18) Even in other domains, mismatches may lead to loss of revenue or public relations setbacks. Of particular concern are recent examples showing that machine learning models can reproduce or amplify unwanted societal biases reflected in training datasets.(4,5,12) For these and other reasons, the World Economic Forum suggests all entities should document the provenance, creation, and use of machine learning datasets to avoid discriminatory outcomes.(25) Although data provenance has been studied} }
@article{WOS:001269821000001, title = {Applications of Machine Learning to Optimize Tennis Performance: A Systematic Review}, journal = {APPLIED SCIENCES-BASEL}, volume = {14}, year = {2024}, doi = {10.3390/app14135517}, author = {Sampaio, Tatiana and Oliveira, Joao P. and Marinho, Daniel A. and Neiva, Henrique P. and Morais, Jorge E.}, abstract = {(1) Background: Tennis has changed toward power-driven gameplay, demanding a nuanced understanding of performance factors. This review explores the role of machine learning in enhancing tennis performance. (2) Methods: A systematic search identified articles utilizing machine learning in tennis performance analysis. (3) Results: Machine learning applications show promise in psychological state monitoring, talent identification, match outcome prediction, spatial and tactical analysis, and injury prevention. Coaches can leverage wearable technologies for personalized psychological state monitoring, data-driven talent identification, and tactical insights for informed decision-making. (4) Conclusions: Machine learning offers coaches insights to refine coaching methodologies and optimize player performance in tennis. By integrating these insights, coaches can adapt to the demands of the sport by improving the players' outcomes. As technology progresses, continued exploration of machine learning's potential in tennis is warranted for further advancements in performance optimization.} }
@article{WOS:000685103600001, title = {Training machine learning models on climate model output yields skillful interpretable seasonal precipitation forecasts}, journal = {COMMUNICATIONS EARTH \\& ENVIRONMENT}, volume = {2}, year = {2021}, doi = {10.1038/s43247-021-00225-4}, author = {Gibson, Peter B. and Chapman, William E. and Altinok, Alphan and Delle Monache, Luca and DeFlorio, Michael J. and Waliser, Duane E.}, abstract = {Seasonal forecasting skill in machine learning methods that are trained on large climate model ensembles can compete with, or out-compete, existing dynamical models, while retaining physical interpretability. A barrier to utilizing machine learning in seasonal forecasting applications is the limited sample size of observational data for model training. To circumvent this issue, here we explore the feasibility of training various machine learning approaches on a large climate model ensemble, providing a long training set with physically consistent model realizations. After training on thousands of seasons of climate model simulations, the machine learning models are tested for producing seasonal forecasts across the historical observational period (1980-2020). For forecasting large-scale spatial patterns of precipitation across the western United States, here we show that these machine learning-based models are capable of competing with or outperforming existing dynamical models from the North American Multi Model Ensemble. We further show that this approach need not be considered a `black box' by utilizing machine learning interpretability methods to identify the relevant physical processes that lead to prediction skill.} }
@article{WOS:000710922600002, title = {A survey of machine learning in credit risk}, journal = {JOURNAL OF CREDIT RISK}, volume = {17}, pages = {1-62}, year = {2021}, issn = {1744-6619}, doi = {10.21314/JCR.2021.008}, author = {Breeden, Joseph L.}, abstract = {Machine learning algorithms have come to dominate several industries. After decades of resistance from examiners and auditors, machine learning is now moving from the research desk to the application stack for credit scoring and a range of other applications in credit risk. This migration is not without novel risks and challenges. Much of the research is now shifting from how best to make the models to how best to use the models in a regulator-compliant business context. This paper surveys the impressively broad range of machine learning methods and application areas for credit risk. In the process of that survey, we create a taxonomy to think about how different machine learning components are matched to create specific algorithms. The reasons for where machine learning succeeds over simple linear methods are explored through a specific lending example. Throughout, we highlight open questions, ideas for improvements and a framework for thinking about how to choose the best machine learning method for a specific problem.} }
@article{WOS:000630189900078, title = {Evaluation of Sentiment Analysis based on AutoML and Traditional Approaches}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {12}, pages = {612-618}, year = {2021}, issn = {2158-107X}, author = {Mahima, K. T. Y. and Ginige, T. N. D. S. and De Zoysa, Kasun}, abstract = {AutoML or Automated Machine Learning is a set of tools to reduce or eliminate the necessary skills of a data scientist to build machine learning or deep learning models. Those tools are able to automatically discover the machine learning models and pipelines for the given dataset within very low interaction of the user. This concept was derived because developing a machine learning or deep learning model by applying the traditional machine learning methods is time-consuming and sometimes it is challenging for experts as well. Moreover, present AutoML tools are used in most of the areas such as image processing and sentiment analysis. In this research, the authors evaluate the implementation of a sentiment analysis classification model based on AutoML and Traditional approaches. For the evaluation, this research used both deep learning and machine learning approaches. To implement the sentiment analysis models HyperOpt SkLearn, TPot as AutoML libraries and, as the traditional method, Scikit learn libraries were used. Moreover for implementing the deep learning models Keras and Auto-Keras libraries used. In the implementation process, to build two binary classification and two multi-class classification models using the above- mentioned libraries. Thereafter evaluate the findings by each AutoML and Traditional approach. In this research, the authors were able to identify that building a machine learning or a deep learning model manually is better than using an AutoML approach.} }
@article{WOS:000899437900001, title = {Hydrogel and Machine Learning for Soft Robots' Sensing and Signal Processing: A Review}, journal = {JOURNAL OF BIONIC ENGINEERING}, volume = {20}, pages = {845-857}, year = {2023}, issn = {1672-6529}, doi = {10.1007/s42235-022-00320-y}, author = {Wang, Shuyu and Sun, Zhaojia}, abstract = {The soft robotics field is on the rise. The highly adaptive robots provide the opportunity to bridge the gap between machines and people. However, their elastomeric nature poses significant challenges to the perception, control, and signal processing. Hydrogels and machine learning provide promising solutions to the problems above. This review aims to summarize this recent trend by first assessing the current hydrogel-based sensing and actuation methods applied to soft robots. We outlined the mechanisms of perception in response to various external stimuli. Next, recent achievements of machine learning for soft robots' sensing data processing and optimization are evaluated. Here we list the strategies for implementing machine learning models from the perspective of applications. Last, we discuss the challenges and future opportunities in perception data processing and soft robots' high level tasks.} }
@article{WOS:000331851700015, title = {TOSELM: Timeliness Online Sequential Extreme Learning Machine}, journal = {NEUROCOMPUTING}, volume = {128}, pages = {119-127}, year = {2014}, issn = {0925-2312}, doi = {10.1016/j.neucom.2013.02.047}, author = {Gu, Yang and Liu, Junfa and Chen, Yiqiang and Jiang, Xinlong and Yu, Hanchao}, abstract = {For handling data and training model, existing machine learning methods do not take timeliness problem into consideration. Timeliness here means the data distribution or the data trend changes with time passing by. Based on timeliness management scheme, a novel machine learning algorithm Timeliness Online Sequential Extreme Learning Machine (TOSELM) is proposed, which improves Online Sequential Extreme Learning Machine (OSELM) with central tendency and dispersion characteristics of data to deal with timeliness problem. The performance of proposed algorithm has been validated on several simulated and realistic datasets, and experimental results show that TOSELM utilizing adaptive weight scheme and iteration scheme can achieve higher learning accuracy, faster convergence and better stability than other machine learning methods. (C) 2013 Elsevier B.V. All rights reserved.} }
@article{WOS:000352350000001, title = {Experimental Realization of a Quantum Support Vector Machine}, journal = {PHYSICAL REVIEW LETTERS}, volume = {114}, year = {2015}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.114.140504}, author = {Li, Zhaokai and Liu, Xiaomei and Xu, Nanyang and Du, Jiangfeng}, abstract = {The fundamental principle of artificial intelligence is the ability of machines to learn from previous experience and do future work accordingly. In the age of big data, classical learning machines often require huge computational resources in many practical cases. Quantum machine learning algorithms, on the other hand, could be exponentially faster than their classical counterparts by utilizing quantum parallelism. Here, we demonstrate a quantum machine learning algorithm to implement handwriting recognition on a four-qubit NMR test bench. The quantum machine learns standard character fonts and then recognizes handwritten characters from a set with two candidates. Because of the wide spread importance of artificial intelligence and its tremendous consumption of computational resources, quantum speedup would be extremely attractive against the challenges of big data.} }
@article{WOS:000494359400009, title = {Machine Learning for Intelligent Authentication in 5G and Beyond Wireless Networks}, journal = {IEEE WIRELESS COMMUNICATIONS}, volume = {26}, pages = {55-61}, year = {2019}, issn = {1536-1284}, doi = {10.1109/MWC.001.1900054}, author = {Fang, He and Wang, Xianbin and Tomasin, Stefano}, abstract = {The 5G and beyond wireless networks are critical to support diverse vertical applications by connecting heterogeneous devices and machines, which directly increase vulnerability for various spoofing attacks. Conventional cryptographic and physical layer authentication techniques are facing some challenges in complex dynamic wireless environments, including significant security overhead, low reliability, as well as difficulties in pre-designing a precise authentication model, providing continuous protection, and learning time-varying attributes. In this article, we envision new authentication approaches based on machine learning techniques by opportunistically leveraging physical layer attributes, and introduce intelligence to authentication for more efficient security provisioning. Machine learning paradigms for intelligent authentication design are presented, namely for parametric/non-parametric and supervised/ unsupervised/reinforcement learning algorithms. In a nutshell, the machine-learning-based intelligent authentication approaches utilize specific features in the multi-dimensional domain for achieving cost-effective, more reliable, model-free, continuous, and situation-aware device validation under unknown network conditions and unpredictable dynamics.} }
@article{WOS:000732559000001, title = {Early diagnosis of rice plant disease using machine learning techniques}, journal = {ARCHIVES OF PHYTOPATHOLOGY AND PLANT PROTECTION}, volume = {55}, pages = {259-283}, year = {2022}, issn = {0323-5408}, doi = {10.1080/03235408.2021.2015866}, author = {Sharma, Mayuri and Kumar, Chandan Jyoti and Deka, Aniruddha}, abstract = {There is an incredible progress in machine learning applications in the field of agricultural research. Detection of various diseases, deficiencies, and factors impacting crops' productivity is one of the major ongoing research in this field. This paper considers various machine learning and deep learning techniques (transfer learning) for rice disease detection. In this study three different rice diseases viz. bacterial blight, rice blast, and brown spot are considered. A detailed comparative analysis of the results indicates the superiority of transfer learning techniques over conventional machine learning techniques. It is observed that InceptionResNetV2 achieves the best result followed by XceptionNet. This work can be incorporated in assisting the farmers for early diagnosis of rice disease so that future course of action may be taken on time. For future studies, efforts should be directed to work with bigger datasets so as to generalize the findings of the experiment.} }
@article{WOS:000525375800050, title = {A review of machine learning kernel methods in statistical process monitoring}, journal = {COMPUTERS \\& INDUSTRIAL ENGINEERING}, volume = {142}, year = {2020}, issn = {0360-8352}, doi = {10.1016/j.cie.2020.106376}, author = {Apsemidis, Anastasios and Psarakis, Stelios and Moguerza, Javier M.}, abstract = {The complexity of modern problems turns increasingly larger in industrial environments, so the classical process monitoring techniques have to adapt to deal with those problems. This is one of the reasons why new Machine and Statistical Learning methodologies have become very popular in the statistical community. Specifically, this article is focused on machine learning kernel methods techniques in the process monitoring field. After explaining the idea of kernel methods we thoroughly examine the process monitoring articles that make use of kernel models and the way in which these models are combined with other Machine Learning approaches. Finally, we summarize the whole picture of the literature and mention some remarkable points.} }
@article{WOS:000745560100002, title = {Machine Learning Applications in Drug Repurposing}, journal = {INTERDISCIPLINARY SCIENCES-COMPUTATIONAL LIFE SCIENCES}, volume = {14}, pages = {15-21}, year = {2022}, issn = {1913-2751}, doi = {10.1007/s12539-021-00487-8}, author = {Yang, Fan and Zhang, Qi and Ji, Xiaokang and Zhang, Yanchun and Li, Wentao and Peng, Shaoliang and Xue, Fuzhong}, abstract = {The coronavirus disease (COVID-19) has led to an rush to repurpose existing drugs, although the underlying evidence base is of variable quality. Drug repurposing is a technique by taking advantage of existing known drugs or drug combinations to be explored in an unexpected medical scenario. Drug repurposing, hence, plays a vital role in accelerating the pre-clinical process of designing novel drugs by saving time and cost compared to the traditional de novo drug discovery processes. Since drug repurposing depends on massive observed data from existing drugs and diseases, the tremendous growth of publicly available large-scale machine learning methods supplies the state-of-the-art application of data science to signaling disease, medicine, therapeutics, and identifying targets with the least error. In this article, we introduce guidelines on strategies and options of utilizing machine learning approaches for accelerating drug repurposing. We discuss how to employ machine learning methods in studying precision medicine, and as an instance, how machine learning approaches can accelerate COVID-19 drug repurposing by developing Chinese traditional medicine therapy. This article provides a strong reasonableness for employing machine learning methods for drug repurposing, including during fighting for COVID-19 pandemic.} }
@article{WOS:001162350100001, title = {Exploring QCD matter in extreme conditions with Machine Learning}, journal = {PROGRESS IN PARTICLE AND NUCLEAR PHYSICS}, volume = {135}, year = {2024}, issn = {0146-6410}, doi = {10.1016/j.ppnp.2023.104084}, author = {Zhou, Kai and Wang, Lingxiao and Pang, Long -Gang and Shi, Shuzhe}, abstract = {In recent years, machine learning has emerged as a powerful computational tool and novel problem -solving perspective for physics, offering new avenues for studying strongly interacting QCD matter properties under extreme conditions. This review article aims to provide an overview of the current state of this intersection of fields, focusing on the application of machine learning to theoretical studies in high energy nuclear physics. It covers diverse aspects, including heavy ion collisions, lattice field theory, and neutron stars, and discuss how machine learning can be used to explore and facilitate the physics goals of understanding QCD matter. The review also provides a commonality overview from a methodology perspective, from data -driven perspective to physics -driven perspective. We conclude by discussing the challenges and future prospects of machine learning applications in high energy nuclear physics, also underscoring the importance of incorporating physics priors into the purely data -driven learning toolbox. This review highlights the critical role of machine learning as a valuable computational paradigm for advancing physics exploration in high energy nuclear physics.} }
@article{WOS:000478732400005, title = {Machine learning by unitary tensor network of hierarchical tree structure}, journal = {NEW JOURNAL OF PHYSICS}, volume = {21}, year = {2019}, issn = {1367-2630}, doi = {10.1088/1367-2630/ab31ef}, author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and Garcia, Raul Blazquez and Su, Gang and Lewenstein, Maciej}, abstract = {The resemblance between the methods used in quantum-many body physics and in machine learning has drawn considerable attention. In particular, tensor networks (TNs) and deep learning architectures bear striking similarities to the extent that TNs can be used for machine learning. Previous results used one-dimensional TNs in image recognition, showing limited scalability and flexibilities. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multi-scale entanglement renormalization ansatz. This approach introduces mathematical connections among quantum many-body physics, quantum information theory, and machine learning. While keeping the TN unitary in the training phase, TN states are defined, which encode classes of images into quantum many-body states. We study the quantum features of the TN states, including quantum entanglement and fidelity. We find these quantities could be properties that characterize the image classes, as well as the machine learning tasks.} }
@article{WOS:000840062800002, title = {A survey on machine learning in array databases}, journal = {APPLIED INTELLIGENCE}, volume = {53}, pages = {9799-9822}, year = {2023}, issn = {0924-669X}, doi = {10.1007/s10489-022-03979-2}, author = {Villarroya, Sebastian and Baumann, Peter}, abstract = {This paper provides an in-depth survey on the integration of machine learning and array databases. First,machine learning support in modern database management systems is introduced. From straightforward implementations of linear algebra operations in SQL to machine learning capabilities of specialized database managers designed to process specific types of data, a number of different approaches are overviewed. Then, the paper covers the database features already implemented in current machine learning systems. Features such as rewriting, compression, and caching allow users to implement more efficient machine learning applications. The underlying linear algebra computations in some of the most used machine learning algorithms are studied in order to determine which linear algebra operations should be efficiently implemented by array databases. An exhaustive overview of array data and relevant array database managers is also provided. Those database features that have been proven of special importance for efficient execution of machine learning algorithms are analyzed in detail for each relevant array database management system. Finally, current state of array databases capabilities for machine learning implementation is shown through two example implementations in Rasdaman and SciDB.} }
@article{WOS:000444245700003, title = {REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES}, journal = {QUANTUM INFORMATION \\& COMPUTATION}, volume = {18}, pages = {51-74}, year = {2018}, issn = {1533-7146}, author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S. and Ronaghe, Pooya}, abstract = {We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.} }
@article{WOS:000562309400001, title = {Incremental Cost-Sensitive Support Vector Machine With Linear-Exponential Loss}, journal = {IEEE ACCESS}, volume = {8}, pages = {149899-149914}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3015954}, author = {Ma, Yue and Zhao, Kun and Wang, Qi and Tian, Yingjie}, abstract = {Incremental learning or online learning as a branch of machine learning has attracted more attention recently. For large-scale problems and dynamic data problem, incremental learning overwhelms batch learning, because of its efficient treatment for new data. However, class imbalance problem, which always appears in online classification brings a considerable challenge for incremental learning. The serious class imbalance problem may directly lead to a useless learning system. Cost-sensitive learning is an important learning paradigm for class imbalance problems and widely used in many applications. In this article, we propose an incremental cost-sensitive learning method to tackle the class imbalance problems in the online situation. This proposed algorithm is based on a novel cost-sensitive support vector machine, which uses the Linear-exponential (LINEX) loss to implement high cost for minority class and low cost for majority class. Using the half-quadratic optimization, we first put forward the algorithm for the cost-sensitive support vector machine, called CSLINEX-SVM*. Then we propose the incremental cost-sensitive algorithm, ICSL-SVM. The results of numeric experiments demonstrate that the proposed incremental algorithm outperforms some conventional batch algorithms except the proposed CSLINEX-SVM*.} }
@article{WOS:001129260900004, title = {Machine learning and pre-medical education}, journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE}, volume = {129}, year = {2022}, issn = {0933-3657}, doi = {10.1016/j.artmed.2022.102313}, author = {Kolachalama, Vijaya B.}, abstract = {Machine learning and artificial intelligence (AI)-driven technologies are contributing significantly to various facets of medicine and care management. It is likely that the next generation of healthcare professionals will be confronted with a series of innovations that are powered by AI, and they may not have sufficient time during their professional tenure to learn about the underlying machine learning frameworks that are driving these systems. Educating the aspiring clinicians and care providers with the right foundational courses in machine learning as part of postsecondary education will likely transform them as high-tech physicians and care providers of the future.} }
@article{WOS:000486611500019, title = {Machine Learning for the Interventional Radiologist}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {213}, pages = {782-784}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.19.21527}, author = {Meek, Ryan D. and Lungren, Matthew P. and Gichoya, Judy W.}, abstract = {OBJECTIVE. The purpose of this article is to describe key potential areas of application of machine learning in interventional radiology. CONCLUSION. Machine learning, although in the early stages of development within the field of interventional radiology, has great potential to influence key areas such as image analysis, clinical predictive modeling, and trainee education. A proactive approach from current interventional radiologists and trainees is needed to shape future directions for machine learning and artificial intelligence.} }
@article{WOS:000464239400001, title = {Genetic algorithms for computational materials discovery accelerated by machine learning}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, issn = {2057-3960}, doi = {10.1038/s41524-019-0181-4}, author = {Jennings, Paul C. and Lysgaard, Steen and Hummelshoj, Jens Strabo and Vegge, Tejs and Bligaard, Thomas}, abstract = {Materials discovery is increasingly being impelled by machine learning methods that rely on pre-existing datasets. Where datasets are lacking, unbiased data generation can be achieved with genetic algorithms. Here a machine learning model is trained on-the-fly as a computationally inexpensive energy predictor before analyzing how to augment convergence in genetic algorithm-based approaches by using the model as a surrogate. This leads to a machine learning accelerated genetic algorithm combining robust qualities of the genetic algorithm with rapid machine learning. The approach is used to search for stable, compositionally variant, geometrically similar nanoparticle alloys to illustrate its capability for accelerated materials discovery, e.g., nanoalloy catalysts. The machine learning accelerated approach, in this case, yields a 50-fold reduction in the number of required energy calculations compared to a traditional ``brute force'' genetic algorithm. This makes searching through the space of all homotops and compositions of a binary alloy particle in a given structure feasible, using density functional theory calculations.} }
@article{WOS:000455128100005, title = {What Machine Learning Can Learn from Foresight: A Human-Centered Approach For machine learning-based forecast efforts to succeed, they must embrace lessons from corporate foresight to address human and organizational challenges.}, journal = {RESEARCH-TECHNOLOGY MANAGEMENT}, volume = {62}, pages = {30-33}, year = {2019}, issn = {0895-6308}, doi = {10.1080/08956308.2019.1541725}, author = {Crews, Christian}, abstract = {Overview: Machine learning applications in business that return forecasts or predictions of future market or consumer behavior must pay attention to nontechnical aspects of how those forecasts are created and used by leaders. Machine learning projects can generate better forecasts that have greater effect by embracing key methods developed through almost 50 years of corporate foresight practice to improve the adoption and use of forecasts in organizations.} }
@article{WOS:000452544100061, title = {Data Integration and Machine Learning: A Natural Synergy}, journal = {PROCEEDINGS OF THE VLDB ENDOWMENT}, volume = {11}, pages = {2094-2097}, year = {2018}, issn = {2150-8097}, doi = {10.14778/3229863.3229876}, author = {Dong, Xin Luna and Rekatsinas, Theodoros}, abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.} }
@article{WOS:001102779100001, title = {Machine Learning for the Control and Monitoring of Electric Machine Drives: Advances and Trends}, journal = {IEEE OPEN JOURNAL OF INDUSTRY APPLICATIONS}, volume = {4}, pages = {188-214}, year = {2023}, doi = {10.1109/OJIA.2023.3284717}, author = {Zhang, Shen and Wallscheid, Oliver and Porrmann, Mario}, abstract = {This review article systematically summarizes the existing literature on utilizing machine learning (ML) techniques for the control and monitoring of electric machine drives. It is anticipated that with the rapid progress in learning algorithms and specialized embedded hardware platforms, ML-based data-driven approaches will become standard tools for the automated high-performance control and monitoring of electric drives. In addition, this article also provides some outlook toward promoting its widespread application in the industry with a focus on deploying ML algorithms onto embedded system-on-chip field-programmable gate array devices.} }
@article{WOS:001380875400005, title = {Landscape of machine learning evolution: privacy-preserving federated learning frameworks and tools}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {58}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-11036-2}, author = {Nguyen, Giang and Sainz-Pardo Diaz, Judith and Calatrava, Amanda and Berberi, Lisana and Lytvyn, Oleksandr and Kozlov, Valentin and Tran, Viet and Molto, German and Lopez Garcia, Alvaro}, abstract = {Machine learning is one of the most widely used technologies in the field of Artificial Intelligence. As machine learning applications become increasingly ubiquitous, concerns about data privacy and security have also grown. The work in this paper presents a broad theoretical landscape concerning the evolution of machine learning and deep learning from centralized to distributed learning, first in relation to privacy-preserving machine learning and secondly in the area of privacy-enhancing technologies. It provides a comprehensive landscape of the synergy between distributed machine learning and privacy-enhancing technologies, with federated learning being one of the most prominent architectures. Various distributed learning approaches to privacy-aware techniques are structured in a review, followed by an in-depth description of relevant frameworks and libraries, more particularly in the context of federated learning. The paper also highlights the need for data protection and privacy addressed from different approaches, key findings in the field concerning AI applications, and advances in the development of related tools and techniques.} }
@article{WOS:000463601900005, title = {Nurses ``Seeing Forest for the Trees'' in the Age of Machine Learning Using Nursing Knowledge to Improve Relevance and Performance}, journal = {CIN-COMPUTERS INFORMATICS NURSING}, volume = {37}, pages = {203-212}, year = {2019}, issn = {1538-2931}, doi = {10.1097/CIN.0000000000000508}, author = {Kwon, Jae Yung and Karim, Mohammad Ehsanul and Topaz, Maxim and Currie, Leanne M.}, abstract = {Although machine learning is increasingly being applied to support clinical decision making, there is a significant gap in understanding what it is and how nurses should adopt it in practice. The purpose of this case study is to show how one application of machine learning may support nursing work and to discuss how nurses can contribute to improving its relevance and performance. Using data from 130 specialized hospitals with 101 766 patients with diabetes, we applied various advanced statistical methods (known as machine learning algorithms) to predict early readmission. The best-performing machine learning algorithm showed modest predictive ability with opportunities for improvement. Nurses can contribute to machine learning algorithms by (1) filling data gaps with nursing-relevant data that provide personalized context about the patient, (2) improving data preprocessing techniques, and (3) evaluating potential value in practice. These findings suggest that nurses need to further process the information provided by machine learning and apply ``Wisdom-in-Action'' to make appropriate clinical decisions. Nurses play a pivotal role in ensuring that machine learning algorithms are shaped by their unique knowledge of each patient's personalized context. By combining machine learning with unique nursing knowledge, nurses can provide more visibility to nursing work, advance nursing science, and better individualize patient care. Therefore, to successfully integrate and maximize the benefits of machine learning, nurses must fully participate in its development, implementation, and evaluation.} }
@article{WOS:001232580600001, title = {Machine Learning and Deep Learning Strategies for Chinese Hamster Ovary Cell Bioprocess Optimization}, journal = {FERMENTATION-BASEL}, volume = {10}, year = {2024}, doi = {10.3390/fermentation10050234}, author = {Baako, Tiffany-Marie D. and Kulkarni, Sahil Kaushik and McClendon, Jerome L. and Harcum, Sarah W. and Gilmore, Jordon}, abstract = {The use of machine learning and deep learning has become prominent within various fields of bioprocessing for countless modeling and prediction tasks. Previous reviews have emphasized machine learning applications in various fields of bioprocessing, including biomanufacturing. This comprehensive review highlights many of the different machine learning and multivariate analysis techniques that have been utilized within Chinese hamster ovary cell biomanufacturing, specifically due to their rising significance in the industry. Applications of machine and deep learning within other bioprocessing industries are also briefly discussed.} }
@article{WOS:000618359200001, title = {Comparison of machine learning and deep learning algorithms for hourly global/diffuse solar radiation predictions}, journal = {INTERNATIONAL JOURNAL OF ENERGY RESEARCH}, volume = {46}, pages = {10052-10073}, year = {2022}, issn = {0363-907X}, doi = {10.1002/er.6529}, author = {Bamisile, Olusola and Oluwasanmi, Ariyo and Ejiyi, Chukwuebuka and Yimen, Nasser and Obiora, Sandra and Huang, Qi}, abstract = {Due to the advancement and wide adoption/application of solar-based technologies, the prediction of solar irradiance has attracted research attention in recent years. In this study, the predictive performance of machine learning models is compared with that of deep learning models for both global solar radiation (GSR) and diffuse solar radiation (DSR) prediction. Different studies have proposed the use of different models for solar radiation prediction. While some used machine learning models, the use of deep learning algorithms were considered by others. Although these algorithms were concluded to be appropriate for solar radiation prediction, variation in their performances brings about an intriguing quest to compare and determine the most appropriate algorithm. The three most common deep learning models in the literature namely; artificial neural network, convolutional neural network, and recurrent neural network (RNN) are considered within the scope of this study. Also, two traditional machine learning models namely polynomial regression and support vector regression (SVR) is considered as well as an ensemble machine learning model called random forest. These models have been applied to four different locations in Nigeria and the typical meteorological year data for 12 years in an hourly time step was used to train/test the model developed. Results from this study show that deep learning models have a better GSR and DSR prediction accuracy in comparison to machine learning models. However, the duration for training and testing the machine learning models (except SVR) is shorter than that of deep learning models making it more desirable for low computational applications. The application of RNN for GSR prediction in Yobe (with an r value of 0.9546 and root means square error/mean absolute error of 82.22 W/m(2)/36.52 W/m(2)) had the overall best model performance of all the models developed in this study. This study contributes to the existing literature in this field as it highlights the disparities between machine learning and deep learning algorithms application for solar radiation forecast.} }
@article{WOS:001028542300001, title = {A state-of-the-art review on the utilization of machine learning in nanofluids, solar energy generation, and the prognosis of solar power}, journal = {ENGINEERING ANALYSIS WITH BOUNDARY ELEMENTS}, volume = {155}, pages = {62-86}, year = {2023}, issn = {0955-7997}, doi = {10.1016/j.enganabound.2023.06.003}, author = {Singh, Santosh Kumar and Tiwari, Arun Kumar and Paliwal, H. K.}, abstract = {In the contemporary data-driven era, the fields of machine learning, deep learning, big data, statistics, and data science are essential for forecasting outcomes and getting insights from data. This paper looks at how machine learning approaches can be used to anticipate solar power generation, assess heat exchanger heat transfer efficiency, and predict the thermo-physical properties of nanofluids. The review specifically focuses on the potential use of machine learning in solar thermal applications, perovskites, and photovoltaic power forecasting. Predictions of nanofluid characteristics and device performance may be more accurately made with the development of machine learning algorithms. The use of machine learning in the creation of new perovskites and the assessment of their effectiveness and stability is also included in the review. Additionally, the paper explores developments in artificial intelligence, particularly deep learning, in this area and offers insights into techniques for forecasting solar power, including PV production, cloud motion, and weather classification.} }
@article{WOS:000704195500004, title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review}, journal = {COMPUTER COMMUNICATIONS}, volume = {178}, pages = {37-63}, year = {2021}, issn = {0140-3664}, doi = {10.1016/j.comcom.2021.07.009}, author = {Miglani, Arzoo and Kumar, Neeraj}, abstract = {Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.} }
@article{WOS:000494273000002, title = {Machine Learning Education for Artists, Musicians, and Other Creative Practitioners}, journal = {ACM TRANSACTIONS ON COMPUTING EDUCATION}, volume = {19}, year = {2019}, issn = {1946-6226}, doi = {10.1145/3294008}, author = {Fiebrink, Rebecca}, abstract = {This article aims to lay a foundation for the research and practice of machine learning education for creative practitioners. It begins by arguing that it is important to teach machine learning to creative practitioners and to conduct research about this teaching, drawing on related work in creative machine learning, creative computing education, and machine learning education. It then draws on research about design processes in engineering and creative practice to motivate a set of learning objectives for students who wish to design new creative artifacts with machine learning. The article then draws on education research and knowledge of creative computing practices to propose a set of teaching strategies that can be used to support creative computing students in achieving these objectives. Explanations of these strategies are accompanied by concrete descriptions of how they have been employed to develop new lectures and activities, and to design new experiential learning and scaffolding technologies, for teaching some of the first courses in the world focused on teaching machine learning to creative practitioners. The article subsequently draws on data collected from these courses-an online course as well as undergraduate and masters-level courses taught at a university-to begin to understand how this curriculum supported student learning, to understand learners' challenges and mistakes, and to inform future teaching and research.} }
@article{WOS:000401022300001, title = {Addressing uncertainty in atomistic machine learning}, journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS}, volume = {19}, pages = {10978-10985}, year = {2017}, issn = {1463-9076}, doi = {10.1039/c7cp00375g}, author = {Peterson, Andrew A. and Christensen, Rune and Khorshidi, Alireza}, abstract = {Machine-learning regression has been demonstrated to precisely emulate the potential energy and forces that are output from more expensive electronic-structure calculations. However, to predict new regions of the potential energy surface, an assessment must be made of the credibility of the predictions. In this perspective, we address the types of errors that might arise in atomistic machine learning, the unique aspects of atomistic simulations that make machine-learning challenging, and highlight how uncertainty analysis can be used to assess the validity of machine-learning predictions. We suggest this will allow researchers to more fully use machine learning for the routine acceleration of large, high-accuracy, or extended-time simulations. In our demonstrations, we use a bootstrap ensemble of neural network-based calculators, and show that the width of the ensemble can provide an estimate of the uncertainty when the width is comparable to that in the training data. Intriguingly, we also show that the uncertainty can be localized to specific atoms in the simulation, which may offer hints for the generation of training data to strategically improve the machine-learned representation.} }
@article{WOS:001130161000001, title = {Machine Learning and Genetic Algorithms: A case study on image reconstruction}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {284}, year = {2024}, issn = {0950-7051}, doi = {10.1016/j.knosys.2023.111194}, author = {Cavallaro, Claudia and Cutello, Vincenzo and Pavone, Mario and Zito, Francesco}, abstract = {In this research, we investigate the application of machine learning techniques to optimization problems and propose a novel integration between metaheuristics and machine learning for the problem of image reconstruction. We propose a modified version of the standard genetic algorithm that uses machine learning to quickly drive the search towards good solutions by dynamically adjusting its parameters. We conducted experiments to compare the performance of our proposed algorithm with other metaheuristic algorithms, including Tabu Search, Iterated Local Search, and Artificial Immune System. Our results demonstrate the effectiveness of our algorithm in finding better solutions and in achieving faster convergence times compared to the other algorithms. The significant computational time difference between the standard genetic algorithm and the genetic algorithm with machine learning highlights the innovation of our approach and its potential to improve real-world applications.} }
@article{WOS:000784962100001, title = {A Review of Data-Driven Machinery Fault Diagnosis Using Machine Learning Algorithms}, journal = {JOURNAL OF VIBRATION ENGINEERING \\& TECHNOLOGIES}, volume = {10}, pages = {2481-2507}, year = {2022}, issn = {2523-3920}, doi = {10.1007/s42417-022-00498-9}, author = {Cen, Jian and Yang, Zhuohong and Liu, Xi and Xiong, Jianbin and Chen, Honghua}, abstract = {Purpose This article aims to systematically review the recent research advances in data-driven machinery fault diagnosis based on machine learning algorithms, and provide valuable guidance for future research directions in this field. Methods This article reviews the research results of data-driven fault diagnosis methods of recent years, and it includes the application status and research progress of machinery fault diagnosis in three frameworks: shallow machine learning (SML), deep learning (DL), and transfer learning (TL). Many publications on this topic are classified and summarized. The related theories, application research, advantages, and disadvantages of several main algorithms under each framework are discussed. Results It has shown that SML-based diagnosis models are simple, reliable, and fast to train. For relatively uncomplicated systems, SML-based diagnosis models still have important applications. For diagnosis tasks with large amounts of training samples and the pursuit of higher accuracy, DL-based diagnosis models can provide end-to-end diagnostic services for complex systems as well as compound faults. TL-based diagnosis models can realize knowledge transfer across conditions, machines, and even fields to solve the problems of data scarcity and sample imbalance that often occur in fault diagnosis. However, in the face of increasingly complex engineering systems, the applications of machine learning algorithms in machinery fault diagnosis are still challenging. Conclusions In future research, the fusion of different machine learning frameworks could solve the problems of inadequate feature extraction and slow training of diagnostic models. Transformer neural network based on pure attention mechanism breaks through the shortcomings of LSTM neural network which cannot be computed in parallel, and it is a worthy research direction in the field of fault diagnosis. In addition, machinery fault diagnosis method based on machine learning algorithms also has great potential for improvement in transferability, federated transfer learning, and strong noise background. These proposed future research directions can provide new ideas for researchers to promote the development of machine learning algorithms in machinery fault diagnosis.} }
@article{WOS:000933899400002, title = {Applications of machine learning in relativistic heavy ion physics}, journal = {SCIENTIA SINICA-PHYSICA MECHANICA \\& ASTRONOMICA}, volume = {52}, year = {2022}, issn = {1674-7275}, doi = {10.1360/SSPMA-2021-0321}, author = {Zhou Meng and Luo YiQun and Song HuiChao}, abstract = {Recently, with rapid hardware and algorithms development, machine learning has been widely used as a significant data analysis method. This article reviews the application of di fferent machine learning algorithms in heavy ion collisions, including impact parameter prediction, nuclear deformation parameter prediction, phase transitions classification, fluid evolution simulation, etc. The machine learning algorithms comprise classic machine learning algorithms, such as ensemble learning and principal component analysis, and deep learning algorithms, such as convolutional neural networks and point cloud networks. Because of the excellent performance and e fficiency of machine learning, these applications will receive much attention in our field.} }
@article{WOS:000609117400001, title = {An inclusive survey on machine learning for CRM: a paradigm shift}, journal = {DECISION}, volume = {47}, pages = {447-457}, year = {2020}, issn = {0304-0941}, doi = {10.1007/s40622-020-00261-7}, author = {Singh, Narendra and Singh, Pushpa and Gupta, Mukul}, abstract = {Customer relationship management (CRM) is the tool to enhance customer relationship in any business. Due to the exponential growth of data volume, in any field, it is significant to develop new techniques to discover the customer knowledge, automation of the system and moreover customer satisfaction to win customer lifetime value. CRM with machine learning could bring a catalytic change in business. Several supervised and unsupervised machine learning techniques are utilized to improve the customer experience and profitability of business. This paper reviews the available literature on the CRM with machine learning techniques for customer identification, customer attraction, and customer retention and customer development. This study reveals that supervised learning techniques are 48.48\\% utilized, unsupervised learning techniques are utilized 15.15\\%, and 9.09\\% utilized other techniques in CRM. Paradigm is also shifted toward the deep learning from machine learning as 28.28\\% text has been reported to deep learning. Decision tree-based algorithm and support vector machine algorithms are most utilized algorithm of supervised learning. E-commerce and telecommunication sectors are the most important areas identified with the exponential growth of the users and hence need a suitable machine learning techniques for customer satisfaction and business profitability.} }
@article{WOS:001238605200001, title = {Interpretable machine learning for creditor recovery rates}, journal = {JOURNAL OF BANKING \\& FINANCE}, volume = {164}, year = {2024}, issn = {0378-4266}, doi = {10.1016/j.jbankfin.2024.107187}, author = {Nazemi, Abdolreza and Fabozzi, Frank J.}, abstract = {Machine learning methods have achieved great success in modeling complex patterns in finance such as asset pricing and credit risk that enable them to outperform statistical models. In addition to the predictive accuracy of machine learning methods, the ability to interpret what a model has learned is crucial in the finance industry. We address this challenge by adapting interpretable machine learning to the context of corporate bond recovery rate modeling. In addition to the best performance, we show the value of interpretable machine learning by finding drivers of recovery rates and their relationship that cannot be discovered by the use of traditional machine learning methods. Our findings are financially meaningful and consistent with the findings in the existing credit risk literature.} }
@article{WOS:000471643600004, title = {Stochastic Gradient Descent and Its Variants in Machine Learning}, journal = {JOURNAL OF THE INDIAN INSTITUTE OF SCIENCE}, volume = {99}, pages = {201-213}, year = {2019}, issn = {0970-4140}, doi = {10.1007/s41745-019-0098-4}, author = {Netrapalli, Praneeth}, abstract = {Stochastic gradient descent (SGD) is a fundamental algorithm which has had a profound impact on machine learning. This article surveys some important results on SGD and its variants that arose in machine learning.} }
@article{WOS:000523935300001, title = {A survey of surveys on the use of visualization for interpreting machine learning models}, journal = {INFORMATION VISUALIZATION}, volume = {19}, pages = {207-233}, year = {2020}, issn = {1473-8716}, doi = {10.1177/1473871620904671}, author = {Chatzimparmpas, Angelos and Martins, Rafael M. and Jusufi, Ilir and Kerren, Andreas}, abstract = {Research in machine learning has become very popular in recent years, with many types of models proposed to comprehend and predict patterns and trends in data originating from different domains. As these models get more and more complex, it also becomes harder for users to assess and trust their results, since their internal operations are mostly hidden in black boxes. The interpretation of machine learning models is currently a hot topic in the information visualization community, with results showing that insights from machine learning models can lead to better predictions and improve the trustworthiness of the results. Due to this, multiple (and extensive) survey articles have been published recently trying to summarize the high number of original research papers published on the topic. But there is not always a clear definition of what these surveys cover, what is the overlap between them, which types of machine learning models they deal with, or what exactly is the scenario that the readers will find in each of them. In this article, we present a meta-analysis (i.e. a ``survey of surveys'') of manually collected survey papers that refer to the visual interpretation of machine learning models, including the papers discussed in the selected surveys. The aim of our article is to serve both as a detailed summary and as a guide through this survey ecosystem by acquiring, cataloging, and presenting fundamental knowledge of the state of the art and research opportunities in the area. Our results confirm the increasing trend of interpreting machine learning with visualizations in the past years, and that visualization can assist in, for example, online training processes of deep learning models and enhancing trust into machine learning. However, the question of exactly how this assistance should take place is still considered as an open challenge of the visualization community.} }
@article{WOS:000962500000001, title = {Differentiation of Bone Metastasis in Elderly Patients With Lung Adenocarcinoma Using Multiple Machine Learning Algorithms}, journal = {CANCER CONTROL}, volume = {30}, year = {2023}, issn = {1073-2748}, doi = {10.1177/10732748231167958}, author = {Zhou, Cheng-Mao and Wang, Ying and Xue, Qiong and Zhu, Yu}, abstract = {ObjectiveWe tested the performance of general machine learning and joint machine learning algorithms in the classification of bone metastasis, in patients with lung adenocarcinoma.MethodsWe used R version 3.5.3 for statistical analysis of the general information, and Python to construct machine learning models.ResultsWe first used the average classifiers of the 4 machine learning algorithms to rank the features and the results showed that race, sex, whether they had surgery and marriage were the first 4 factors affecting bone metastasis. Machine learning results in the training group: for area under the curve (AUC), except for RF and LR, the AUC values of all machine learning classifiers were greater than .8, but the joint algorithm did not improve the AUC for any single machine learning algorithm. Among the results related to accuracy and precision, the accuracy of other machine learning classifiers except the RF algorithm was higher than 70\\%, and only the precision of the LGBM algorithm was higher than 70\\%. Machine learning results in the test group: Similarly, for areas under the curve (AUC), except RF and LR, the AUC values for all machine learning classifiers were greater than .8, but the joint algorithm did not improve the AUC value for any single machine learning algorithm. For accuracy, except for the RF algorithm, the accuracy of other machine learning classifiers was higher than 70\\%. The highest precision for the LGBM algorithm was .675.ConclusionThe results of this concept verification study show that machine learning algorithm classifiers can distinguish the bone metastasis of patients with lung cancer. This will provide a new research idea for the future use of non-invasive technology to identify bone metastasis in lungcancer. However, more prospective multicenter cohort studies are needed.} }
@article{WOS:001221463000016, title = {Machine Learning in Materials Chemistry: An Invitation}, journal = {MACHINE LEARNING WITH APPLICATIONS}, volume = {8}, year = {2022}, doi = {10.1016/j.mlwa.2022.100265}, author = {Packwood, Daniel and Nguyen, Linh Thi Hoai and Cesana, Pierluigi and Zhang, Guoxi and Staykov, Aleksandar and Fukumoto, Yasuhide and Nguyen, Dinh Hoa}, abstract = {Materials chemistry is being profoundly influenced by the uptake of machine learning methodologies. Machine learning techniques, in combination with established techniques from computational physics, promise to accelerate the discovery of new materials by elucidating complex structure-property relationships from massive material databases. Despite exciting possibilities, further methodological developments call for a greater synergism between materials chemists, physicists, and engineers on one side, with computer science and math majors on the other. In this review, we provide a non -exhaustive account of machine learning in materials chemistry for computer scientists and applied mathematicians, with an emphasis on molecule datasets and materials chemistry problems. The first part of this review provides a tutorial on how to prepare such datasets for subsequent model building, with an emphasis on the construction of feature vectors. We also provide a self-contained introduction to density functional theory, a method from computational physics which is widely used to generate datasets and compute response variables. The second part reviews two machine learning methodologies which represent the status quo in materials chemistry at present - kernelized machine learning and Bayesian machine learning - and discusses their application to real datasets. In the third part of the review, we introduce some emerging machine learning techniques which have not been widely adopted by materials scientists and therefore present potential avenues for computer science and applied math majors. In the final concluding section, we discuss some recent machine learning -based approaches to real materials discovery problems and speculate on some promising future directions.} }
@article{WOS:000635324300001, title = {RETRACTED: User acceptance of machine learning models - Integrating several important external variables with technology acceptance model (Retracted Article)}, journal = {INTERNATIONAL JOURNAL OF ELECTRICAL ENGINEERING EDUCATION}, year = {2021}, issn = {0020-7209}, doi = {10.1177/00207209211005271}, author = {Zhang, Xiaohang and Wang, Yuan and Li, Zhengren}, abstract = {Machine learning models enable data-based decision-making in many areas and have attracted extensive attention. By testing the factors that influence the adoption of machine learning models, this study expands the scope of machine learning models in information technology adoption research. Based on the machine learning background and Technology Acceptance Model, this study integrates the necessary external variables, proposes a research model, and further verifies the validity of the model through the survey of 192 users of machine learning models. The results showed that organizational factors, trust, perceived usefulness, and perceived ease of use are positively correlated with the attitude of machine learning models. Moreover, our findings show that the interpretability of the model has an important positive effect on trust. The factors examined in this study are the basis for the development and use of reliable machine learning models. And it has important practical significance for promoting user adoption of machine learning model. Meanwhile, these theoretical studies also provide a strong literature support for the adoption of machine learning models and fill the theoretical research gap in this field.} }
@article{WOS:000357246100026, title = {Multi-task proximal support vector machine}, journal = {PATTERN RECOGNITION}, volume = {48}, pages = {3249-3257}, year = {2015}, issn = {0031-3203}, doi = {10.1016/j.patcog.2015.01.014}, author = {Li, Ya and Tian, Xinmei and Song, Mingli and Tao, Dacheng}, abstract = {With the explosive growth of the use of imagery, visual recognition plays an important role in many applications and attracts increasing research attention. Given several related tasks, single-task learning learns each task separately and ignores the relationships among these tasks. Different from single-task learning, multi-task learning can explore more information to learn all tasks jointly by using relationships among these tasks. In this paper, we propose a novel multi-task learning model based on the proximal support vector machine. The proximal support vector machine uses the large-margin idea as does the standard support vector machines but with looser constraints and much lower computational cost. Our multi-task proximal support vector machine inherits the merits of the proximal support vector machine and achieves better performance compared with other popular multi-task learning models. Experiments are conducted on several multi-task learning datasets, including two classification datasets and one regression dataset. All results demonstrate the effectiveness and efficiency of our proposed multi-task proximal support vector machine. (C) 2015 Elsevier Ltd. All rights reserved.} }
@article{WOS:001326938800003, title = {A mini review of machine learning in inorganic phosphors}, journal = {JOURNAL OF MATERIALS INFORMATICS}, volume = {2}, year = {2022}, doi = {10.20517/jmi.2022.21}, author = {Jiang, Lipeng and Jiang, Xue and Lv, Guocai and Su, Yanjing}, abstract = {Machine learning has promoted the rapid development of materials science. In this review, we provide an overview of recent advances in machine learning for inorganic phosphors. We take two aspects of material properties prediction and optimization based on iterative experiments as entry points to outline the applications of machine learning for inorganic phosphors in terms of Debye temperature prediction and luminescence intensity and thermal stability optimization. By analyzing the machine learning methods and their application objectives, current problems are summarized and suggestions for subsequent development are proposed.} }
@article{WOS:000360416900003, title = {An improved cuckoo search based extreme learning machine for medical data classification}, journal = {SWARM AND EVOLUTIONARY COMPUTATION}, volume = {24}, pages = {25-49}, year = {2015}, issn = {2210-6502}, doi = {10.1016/j.swevo.2015.05.003}, author = {Mohapatra, P. and Chakravarty, S. and Dash, P. K.}, abstract = {Machine learning techniques are being increasingly used for detection and diagnosis of diseases for its accuracy and efficiency in pattern classification. In this paper, improved cuckoo search based extreme learning machine (ICSELM) is proposed to classify binary medical datasets. Extreme learning machine (ELM) is widely used as a learning algorithm for training single layer feed forward neural networks (SLFN) in the field of classification. However, to make the model more stable, an evolutionary algorithm improved cuckoo search (ICS) is used to pre-train ELM by selecting the input weights and hidden biases. Like ELM, Moore-Penrose (MP) generalized inverse is used in ICSELM to analytically determines the output weights. To evaluate the effectiveness of the proposed model, four benchmark datasets, i.e. Breast Cancer, Diabetes, Bupa and Hepatitis from the UCI Repository of Machine Learning are used. A number of useful performance evaluation measures including accuracy, sensitivity, specificity, confusion matrix, Gmean, F-score and norm of the output weights as well as the area under the receiver operating characteristic (ROC) curve are computed. The results are analyzed and compared with both ELM based models like ELM, on-line sequential extreme learning algorithm (OSELM), CSELM and other artificial neural networks i.e. multi-layered perceptron (MLP), MLPCS, MLPICS and radial basis function neural network (RBFNN), RBFNNCS, RBFNNICS. The experimental results demonstrate that the ICSELM model outperforms other models. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000546324500005, title = {Exploring chemical compound space with quantum-based machine learning}, journal = {NATURE REVIEWS CHEMISTRY}, volume = {4}, pages = {347-358}, year = {2020}, doi = {10.1038/s41570-020-0189-9}, author = {von Lilienfeld, O. Anatole and Mueller, Klaus-Robert and Tkatchenko, Alexandre}, abstract = {Machine-learning techniques have enabled, among many other applications, the exploration of molecular properties throughout chemical space. The specific development of quantum-based approaches in machine learning can now help us unravel new chemical insights. Rational design of compounds with specific properties requires understanding and fast evaluation of molecular properties throughout chemical compound space - the huge set of all potentially stable molecules. Recent advances in combining quantum-mechanical calculations with machine learning provide powerful tools for exploring wide swathes of chemical compound space. We present our perspective on this exciting and quickly developing field by discussing key advances in the development and applications of quantum-mechanics-based machine-learning methods to diverse compounds and properties, and outlining the challenges ahead. We argue that significant progress in the exploration and understanding of chemical compound space can be made through a systematic combination of rigorous physical theories, comprehensive synthetic data sets of microscopic and macroscopic properties, and modern machine-learning methods that account for physical and chemical knowledge.} }
@article{WOS:000995149000001, title = {Twenty Years of Machine-Learning-Based Text Classification: A Systematic Review}, journal = {ALGORITHMS}, volume = {16}, year = {2023}, doi = {10.3390/a16050236}, author = {Palanivinayagam, Ashokkumar and El-Bayeh, Claude Ziad and Damasevicius, Robertas}, abstract = {Machine-learning-based text classification is one of the leading research areas and has a wide range of applications, which include spam detection, hate speech identification, reviews, rating summarization, sentiment analysis, and topic modelling. Widely used machine-learning-based research differs in terms of the datasets, training methods, performance evaluation, and comparison methods used. In this paper, we surveyed 224 papers published between 2003 and 2022 that employed machine learning for text classification. The Preferred Reporting Items for Systematic Reviews (PRISMA) statement is used as the guidelines for the systematic review process. The comprehensive differences in the literature are analyzed in terms of six aspects: datasets, machine learning models, best accuracy, performance evaluation metrics, training and testing splitting methods, and comparisons among machine learning models. Furthermore, we highlight the limitations and research gaps in the literature. Although the research works included in the survey perform well in terms of text classification, improvement is required in many areas. We believe that this survey paper will be useful for researchers in the field of text classification.} }
@article{WOS:000751673300089, title = {Integration of AI and Machine Learning in Radiotherapy QA}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {3}, year = {2020}, doi = {10.3389/frai.2020.577620}, author = {Chan, Maria F. and Witztum, Alon and Valdes, Gilmer}, abstract = {The use of machine learning and other sophisticated models to aid in prediction and decision making has become widely popular across a breadth of disciplines. Within the greater diagnostic radiology, radiation oncology, and medical physics communities promising work is being performed in tissue classification and cancer staging, outcome prediction, automated segmentation, treatment planning, and quality assurance as well as other areas. In this article, machine learning approaches are explored, highlighting specific applications in machine and patient-specific quality assurance (QA). Machine learning can analyze multiple elements of a delivery system on its performance over time including the multileaf collimator (MLC), imaging system, mechanical and dosimetric parameters. Virtual Intensity-Modulated Radiation Therapy (IMRT) QA can predict passing rates using different measurement techniques, different treatment planning systems, and different treatment delivery machines across multiple institutions. Prediction of QA passing rates and other metrics can have profound implications on the current IMRT process. Here we cover general concepts of machine learning in dosimetry and various methods used in virtual IMRT QA, as well as their clinical applications.} }
@article{WOS:000569585500001, title = {Using Machine Learning Models and Actual Transaction Data for Predicting Real Estate Prices}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175832}, author = {Pai, Ping-Feng and Wang, Wen-Chang}, abstract = {Real estate price prediction is crucial for the establishment of real estate policies and can help real estate owners and agents make informative decisions. The aim of this study is to employ actual transaction data and machine learning models to predict prices of real estate. The actual transaction data contain attributes and transaction prices of real estate that respectively serve as independent variables and dependent variables for machine learning models. The study employed four machine learning models-namely, least squares support vector regression (LSSVR), classification and regression tree (CART), general regression neural networks (GRNN), and backpropagation neural networks (BPNN), to forecast real estate prices. In addition, genetic algorithms were used to select parameters of machine learning models. Numerical results indicated that the least squares support vector regression outperforms the other three machine learning models in terms of forecasting accuracy. Furthermore, forecasting results generated by the least squares support vector regression are superior to previous related studies of real estate price prediction in terms of the average absolute percentage error. Thus, the machine learning-based model is a substantial and feasible way to forecast real estate prices, and the least squares support vector regression can provide relatively competitive and satisfactory results.} }
@article{WOS:000797054700007, title = {Machine Learning in Causal Inference: Application in Pharmacovigilance}, journal = {DRUG SAFETY}, volume = {45}, pages = {459-476}, year = {2022}, issn = {0114-5916}, doi = {10.1007/s40264-022-01155-6}, author = {Zhao, Yiqing and Yu, Yue and Wang, Hanyin and Li, Yikuan and Deng, Yu and Jiang, Guoqian and Luo, Yuan}, abstract = {Monitoring adverse drug events or pharmacovigilance has been promoted by the World Health Organization to assure the safety of medicines through a timely and reliable information exchange regarding drug safety issues. We aim to discuss the application of machine learning methods as well as causal inference paradigms in pharmacovigilance. We first reviewed data sources for pharmacovigilance. Then, we examined traditional causal inference paradigms, their applications in pharmacovigilance, and how machine learning methods and causal inference paradigms were integrated to enhance the performance of traditional causal inference paradigms. Finally, we summarized issues with currently mainstream correlation-based machine learning models and how the machine learning community has tried to address these issues by incorporating causal inference paradigms. Our literature search revealed that most existing data sources and tasks for pharmacovigilance were not designed for causal inference. Additionally, pharmacovigilance was lagging in adopting machine learning-causal inference integrated models. We highlight several currently trending directions or gaps to integrate causal inference with machine learning in pharmacovigilance research. Finally, our literature search revealed that the adoption of causal paradigms can mitigate known issues with machine learning models. We foresee that the pharmacovigilance domain can benefit from the progress in the machine learning field.} }
@article{WOS:000534638300001, title = {Applications of Machine Learning to Friction Stir Welding Process Optimization}, journal = {JURNAL KEJURUTERAAN}, volume = {32}, pages = {171-186}, year = {2020}, issn = {0128-0198}, doi = {10.17576/jkukm-2020-32(2)-01}, author = {Nasir, Tauqir and Asmael, Mohammed and Zeeshan, Qasim and Solyali, Davut}, abstract = {Machine learning (ML) is a branch of artificial intelligent which involve the study and development of algorithm for computer to learn from data. A computational method used in machine learning to learn or get directly information from data without relying on a prearranged model equation. The applications of ML applied in the domains of all industries. In the field of manufacturing the ability of ML approach is utilized to predict the failure before occurrence. FSW and FSSW is an advanced form of friction welding and it is a solid state joining technique which is mostly used to weld the dissimilar alloys. FSW, FSSW has become a dominant joining method in aerospace, railway and ship building industries. It observed that the number of applications of machine learning increased in FSW, FSSW process which sheared the Machine-learning approaches like, artificial Neural Network (ANN), Regression model (RSM), Support Vector Machine (SVM) and Adaptive Neuro-Fuzzy Inference System (ANFIS). The main purpose of this study is to review and summarize the emerging research work of machine learning techniques in FSW and FSSW. Previous researchers demonstrate that the Machine Learning applications applied to predict the response of FSW and FSSW process. The prediction in error percentage in result of ANN and RSM model in overall is less than 5\\%. In comparison between ANN/RSM the obtain result shows that ANN is provide better and accurate than RSM. In application of SVM algorithm the prediction accuracy found 100\\% for training and testing process.} }
@article{WOS:001003539600001, title = {Application of Machine Learning in Fuel Cell Research}, journal = {ENERGIES}, volume = {16}, year = {2023}, doi = {10.3390/en16114390}, author = {Su, Danqi and Zheng, Jiayang and Ma, Junjie and Dong, Zizhe and Chen, Zhangjie and Qin, Yanzhou}, abstract = {A fuel cell is an energy conversion device that utilizes hydrogen energy through an electrochemical reaction. Despite their many advantages, such as high efficiency, zero emissions, and fast startup, fuel cells have not yet been fully commercialized due to deficiencies in service life, cost, and performance. Efficient evaluation methods for performance and service life are critical for the design and optimization of fuel cells. The purpose of this paper was to review the application of common machine learning algorithms in fuel cells. The significance and status of machine learning applications in fuel cells are briefly described. Common machine learning algorithms, such as artificial neural networks, support vector machines, and random forests are introduced, and their applications in fuel cell performance prediction and optimization are comprehensively elaborated. The review revealed that machine learning algorithms can be successfully used for performance prediction, service life prediction, and fault diagnosis in fuel cells, with good accuracy in solving nonlinear problems. Combined with optimization algorithms, machine learning models can further carry out the optimization of design and operating parameters to achieve multiple optimization goals with good accuracy and efficiency. It is expected that this review paper could help the reader comprehend the state of the art of machine learning applications in fuel fuels and shed light on further development directions in fuel cell research.} }
@article{WOS:000695210000001, title = {A review on utilizing machine learning technology in the fields of electronic emergency triage and patient priority systems in telemedicine: Coherent taxonomy, motivations, open research challenges and recommendations for intelligent future work}, journal = {COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE}, volume = {209}, year = {2021}, issn = {0169-2607}, doi = {10.1016/j.cmpb.2021.106357}, author = {Salman, Omar H. and Taha, Zahraa and Alsabah, Muntadher Q. and Hussein, Yaseein S. and Mohammed, Ahmed S. and Aal-Nouman, Mohammed}, abstract = {Background: With the remarkable increasing in the numbers of patients, the triaging and prioritizing patients into multi-emergency level is required to accommodate all the patients, save more lives, and manage the medical resources effectively. Triaging and prioritizing patients becomes particularly challenging especially for the patients who are far from hospital and use telemedicine system. To this end, the researchers exploiting the useful tool of machine learning to address this challenge. Hence, carrying out an intensive investigation and in-depth study in the field of using machine learning in E-triage and patient priority are essential and required. Objectives: This research aims to (1) provide a literature review and an in-depth study on the roles of machine learning in the fields of electronic emergency triage (E-triage) and prioritize patients for fast healthcare services in telemedicine applications. (2) highlight the effectiveness of machine learning methods in terms of algorithms, medical input data, output results, and machine learning goals in remote healthcare telemedicine systems. (3) present the relationship between machine learning goals and the electronic triage processes specifically on the: triage levels, medical features for input, outcome results as outputs, and the relevant diseases. (4), the outcomes of our analyses are subjected to organize and propose a cross-over taxonomy between machine learning algorithms and telemedicine structure. (5) present lists of motivations, open research challenges and recommendations for future intelligent work for both academic and industrial sectors in telemedicine and remote healthcare applications. Methods: An intensive research is carried out by reviewing all articles related to the field of E-triage and remote priority systems that utilise machine learning algorithms and sensors. We have searched all related keywords to investigate the databases of Science Direct, IEEE Xplore, Web of Science, PubMed, and Medline for the articles, which have been published from January 2012 up to date. Results: A new crossover matching between machine learning methods and telemedicine taxonomy is proposed. The crossover-taxonomy is developed in this study to identify the relationship between machine learning algorithm and the equivalent telemedicine categories whereas the machine learning algorithm has been utilized. The impact of utilizing machine learning is composed in proposing the telemedicine architecture based on synchronous (real-time/ online) and asynchronous (store-and-forward / offline) structure. In addition to that, list of machine learning algorithms, list of the performance metrics, list of inputs data and outputs results are presented. Moreover, open research challenges, the benefits of utilizing machine learning and the recommendations for new research opportunities that need to be addressed for the synergistic integration of multidisciplinary works are organized and presented accordingly. Discussion: The state-of-the-art studies on the E-triage and priority systems that utilise machine learning algorithms in telemedicine architecture are discussed. This approach allows the researchers to understand the modernisation of healthcare systems and the efficient use of artificial intelligence and machine learning. In particular, the growing worldwide population and various chronic diseases such as heart chronic diseases, blood pressure and diabetes, require smart health monitoring systems in E-triage and priority systems, in which machine learning algorithms could be greatly beneficial. Conclusions: Although research directions on E-triage and priority systems that use machine learning algorithms in telemedicine vary, they are equally essential and should be considered. Hence, we provide a comprehensive review to emphasise the advantages of the existing research in multidisciplinary works of artificial intelligence, machine learning and healthcare services. (c) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000669541400017, title = {Implementation of Quantum Machine Learning for Electronic Structure Calculations of Periodic Systems on Quantum Computing Devices}, journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING}, volume = {61}, pages = {2667-2674}, year = {2021}, issn = {1549-9596}, doi = {10.1021/acs.jcim.1c00294}, author = {Sureshbabu, Shree Hari and Sajjan, Manas and Oh, Sangchul and Kais, Sabre}, abstract = {Quantum machine learning algorithms, the extensions of machine learning to quantum regimes, are believed to be more powerful as they leverage the power of quantum properties. Quantum machine learning methods have been employed to solve quantum many-body systems and have demonstrated accurate electronic structure calculations of lattice models, molecular systems, and recently periodic systems. A hybrid approach using restricted Boltzmann machines and a quantum algorithm to obtain the probability distribution that can be optimized classically is a promising method due to its efficiency and ease of implementation. Here, we implement the benchmark test of the hybrid quantum machine learning on the IBM-Q quantum computer to calculate the electronic structure of typical two-dimensional crystal structures: hexagonal-boron nitride and graphene. The band structures of these systems calculated using the hybrid quantum machine learning approach are in good agreement with those obtained by the conventional electronic structure calculations. This benchmark result implies that the hybrid quantum machine learning method, empowered by quantum computers, could provide a new way of calculating the electronic structures of quantum many-body systems.} }
@article{WOS:001306200600001, title = {Machine learning to predict the production of bio-oil, biogas, and biochar by pyrolysis of biomass: a review}, journal = {ENVIRONMENTAL CHEMISTRY LETTERS}, volume = {22}, pages = {2669-2698}, year = {2024}, issn = {1610-3653}, doi = {10.1007/s10311-024-01767-7}, author = {Khandelwal, Kapil and Nanda, Sonil and Dalai, Ajay K.}, abstract = {The world energy consumption has increased by + 195\\% since 1970 with more than 80\\% of the energy mix originating from fossil fuels, thus leading to pollution and global warming. Alternatively, pyrolysis of modern biomass is considered carbon neutral and produces value-added biogas, bio-oils, and biochar, yet actual pyrolysis processes are not fully optimized. Here, we review the use of machine learning to improve the pyrolysis of lignocellulosic biomass, with emphasis on machine learning algorithms and prediction of product characteristics. Algorithms comprise regression analysis, artificial neural networks, decision trees, and the support vector machine. Machine learning allows for the prediction of yield, quality, surface area, reaction kinetics, techno-economics, and lifecycle assessment of biogas, bio-oil, and biochar. The robustness of machine learning techniques and engineering applications are discussed.} }
@article{WOS:000670096500001, title = {Machine learning to advance the prediction, prevention and treatment of eating disorders}, journal = {EUROPEAN EATING DISORDERS REVIEW}, volume = {29}, pages = {683-691}, year = {2021}, issn = {1072-4133}, doi = {10.1002/erv.2850}, author = {Wang, Shirley B.}, abstract = {Machine learning approaches are just emerging in eating disorders research. Promising early results suggest that such approaches may be a particularly promising and fruitful future direction. However, there are several challenges related to the nature of eating disorders in building robust, reliable and clinically meaningful prediction models. This article aims to provide a brief introduction to machine learning and to discuss several such challenges, including issues of sample size, measurement, imbalanced data and bias; I also provide concrete steps and recommendations for each of these issues. Finally, I outline key outstanding questions and directions for future research in building, testing and implementing machine learning models to advance our prediction, prevention, and treatment of eating disorders. Highlights Machine learning holds significant promise to advance eating disorders research Some key considerations for responsible machine learning application in eating disorders research include issues of sample size, measurement, imbalanced data and bias Future research should prioritize external validation of machine learning models} }
@article{WOS:000658810900038, title = {QUBO formulations for training machine learning models}, journal = {SCIENTIFIC REPORTS}, volume = {11}, year = {2021}, issn = {2045-2322}, doi = {10.1038/s41598-021-89461-4}, author = {Date, Prasanna and Arthur, Davis and Pusey-Nazzaro, Lauren}, abstract = {Training machine learning models on classical computers is usually a time and compute intensive process. With Moore's law nearing its inevitable end and an ever-increasing demand for large-scale data analysis using machine learning, we must leverage non-conventional computing paradigms like quantum computing to train machine learning models efficiently. Adiabatic quantum computers can approximately solve NP-hard problems, such as the quadratic unconstrained binary optimization (QUBO), faster than classical computers. Since many machine learning problems are also NP-hard, we believe adiabatic quantum computers might be instrumental in training machine learning models efficiently in the post Moore's law era. In order to solve problems on adiabatic quantum computers, they must be formulated as QUBO problems, which is very challenging. In this paper, we formulate the training problems of three machine learning models-linear regression, support vector machine (SVM) and balanced k-means clustering-as QUBO problems, making them conducive to be trained on adiabatic quantum computers. We also analyze the computational complexities of our formulations and compare them to corresponding state-of-the-art classical approaches. We show that the time and space complexities of our formulations are better (in case of SVM and balanced k-means clustering) or equivalent (in case of linear regression) to their classical counterparts.} }
@article{WOS:000571258300007, title = {Machine learning for active matter}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {2}, pages = {94-103}, year = {2020}, doi = {10.1038/s42256-020-0146-9}, author = {Cichos, Frank and Gustavsson, Kristian and Mehlig, Bernhard and Volpe, Giovanni}, abstract = {The availability of large datasets has boosted the application of machine learning in many fields and is now starting to shape active-matter research as well. Machine learning techniques have already been successfully applied to active-matter data-for example, deep neural networks to analyse images and track objects, and recurrent nets and random forests to analyse time series. Yet machine learning can also help to disentangle the complexity of biological active matter, helping, for example, to establish a relation between genetic code and emergent bacterial behaviour, to find navigation strategies in complex environments, and to map physical cues to animal behaviours. In this Review, we highlight the current state of the art in the application of machine learning to active matter and discuss opportunities and challenges that are emerging. We also emphasize how active matter and machine learning can work together for mutual benefit. This Review surveys machine learning techniques that are currently developed for a range of research topics in biological and artificial active matter and also discusses challenges and exciting opportunities. This research direction promises to help disentangle the complexity of active matter and gain fundamental insights for instance in collective behaviour of systems at many length scales from colonies of bacteria to animal flocks.} }
@article{WOS:000918184200006, title = {Crop yield prediction using machine learning techniques}, journal = {ADVANCES IN ENGINEERING SOFTWARE}, volume = {175}, year = {2023}, issn = {0965-9978}, doi = {10.1016/j.advengsoft.2022.103326}, author = {Iniyan, S. and Varma, V. Akhil and Naidu, Ch Teja}, abstract = {Machine Learning is a successful dynamic device for foreseeing crop yields, just as for choosing which harvests to plant and what to do about them during the developing season. Since it operates with a large amount of data produced by several variables, the farming system is highly complicated. Methods of machine learning can aid intelligent system decision-making. The following paper investigates a variety of methods for predicting crop yields using a variety of soil and environmental variables. The main purpose of this project is to make a machine learning model make predictions. By taking into account several variables, machine learning algorithms can help farmers decide which crop to grow in addition to increasing yield. Farmers can benefit from yield estimation because it allows them to minimize crop loss and obtain the best prices for their crops. A machine learning model may be descriptive or predictive, depending on the research question and study objectives.} }
@article{WOS:000636526200001, title = {Fault diagnosis of various rotating equipment using machine learning approaches - A review}, journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART E-JOURNAL OF PROCESS MECHANICAL ENGINEERING}, volume = {235}, pages = {629-642}, year = {2021}, issn = {0954-4089}, doi = {10.1177/0954408920971976}, author = {Manikandan, S. and Duraivelu, K.}, abstract = {Fault diagnosis of various rotating equipment plays a significant role in industries as it guarantees safety, reliability and prevents breakdown and loss of any source of energy. Early identification is a fundamental aspect for diagnosing the faults which saves both time and costs and in fact it avoids perilous conditions. Investigations are being carried out for intelligent fault diagnosis using machine learning approaches. This article analyses various machine learning approaches used for fault diagnosis of rotating equipment. In addition to this, a detailed study of different machine learning strategies which are incorporated on various rotating equipment in the context of fault diagnosis is also carried out. Mainly, the benefits and advance patterns of deep neural network which are applied to multiple components for fault diagnosis are inspected in this study. Finally, different algorithms are proposed to propagate the quality of fault diagnosis and the conceivable research ideas of applying machine learning approaches on various rotating equipment are condensed in this article.} }
@article{WOS:000588277100001, title = {Tutorial: Applying Machine Learning in Behavioral Research}, journal = {PERSPECTIVES ON BEHAVIOR SCIENCE}, volume = {43}, pages = {697-723}, year = {2020}, issn = {2520-8969}, doi = {10.1007/s40614-020-00270-y}, author = {Turgeon, Stephanie and Lanovaz, Marc J.}, abstract = {Machine-learning algorithms hold promise for revolutionizing how educators and clinicians make decisions. However, researchers in behavior analysis have been slow to adopt this methodology to further develop their understanding of human behavior and improve the application of the science to problems of applied significance. One potential explanation for the scarcity of research is that machine learning is not typically taught as part of training programs in behavior analysis. This tutorial aims to address this barrier by promoting increased research using machine learning in behavior analysis. We present how to apply the random forest, support vector machine, stochastic gradient descent, and k-nearest neighbors algorithms on a small dataset to better identify parents of children with autism who would benefit from a behavior analytic interactive web training. These step-by-step applications should allow researchers to implement machine-learning algorithms with novel research questions and datasets.} }
@article{WOS:001283397300001, title = {Towards advanced uncertainty and sensitivity analysis of building energy performance using machine learning techniques}, journal = {JOURNAL OF BUILDING PERFORMANCE SIMULATION}, volume = {17}, pages = {655-662}, year = {2024}, issn = {1940-1493}, doi = {10.1080/19401493.2024.2387071}, author = {Tian, Wei}, abstract = {Uncertainty analysis quantifies the inherently uncertain nature of building energy performance, whereas sensitivity analysis identifies key factors to explain variations in building energy performance. With the ability to handle complex relationships, machine learning techniques offer an effective approach to more accurate and reliable uncertainty and sensitivity analysis. This paper provides valuable insights into the current state and future prospects of machine learning-based uncertainty and sensitivity analysis for building energy performance. The development of machine learning-based uncertainty analysis is discussed from three perspectives: observational data-based probabilistic prediction, surrogate model-based uncertainty quantification, and inverse uncertainty quantification. Variance-based sensitivity analysis using surrogate machine learning models decomposes output variance associated with each input. In contrast, machine learning-based variable importance refers to the change of model predictive performance using model-specific or model-agnostic approaches. Finally, future research directions on machine learning-based uncertainty and sensitivity analysis of building energy performance are presented.} }
@article{WOS:000557303500009, title = {Machine Learning With Neuroimaging: Evaluating Its Applications in Psychiatry}, journal = {BIOLOGICAL PSYCHIATRY-COGNITIVE NEUROSCIENCE AND NEUROIMAGING}, volume = {5}, pages = {791-798}, year = {2020}, issn = {2451-9022}, doi = {10.1016/j.bpsc.2019.11.007}, author = {Nielsen, Ashley N. and Barch, Deanna M. and Petersen, Steven E. and Schlaggar, Bradley L. and Greene, Deanna J.}, abstract = {Psychiatric disorders are complex, involving heterogeneous symptomatology and neurobiology that rarely involves the disruption of single, isolated brain structures. In an attempt to better describe and understand the complexities of psychiatric disorders, investigators have increasingly applied multivariate pattern classification approaches to neuroimaging data and in particular supervised machine learning methods. However, supervised machine learning approaches also come with unique challenges and trade-offs, requiring additional study design and interpretation considerations. The goal of this review is to provide a set of best practices for evaluating machine learning applications to psychiatric disorders. We discuss how to evaluate two common efforts: 1) making predictions that have the potential to aid in diagnosis, prognosis, and treatment and 2) interrogating the complex neurophysiological mechanisms underlying psychopathology. We focus here on machine learning as applied to functional connectivity with magnetic resonance imaging, as an example to ground discussion. We argue that for machine learning classification to have translational utility for individual-level predictions, investigators must ensure that the classification is clinically informative, independent of confounding variables, and appropriately assessed for both performance and generalizability. We contend that shedding light on the complex mechanisms underlying psychiatric disorders will require consideration of the unique utility, interpretability, and reliability of the neuroimaging features (e.g., regions, networks, connections) identified from machine learning approaches. Finally, we discuss how the rise of large, multisite, publicly available datasets may contribute to the utility of machine learning approaches in psychiatry.} }
@article{WOS:000577262800006, title = {Retrospective on a decade of machine learning for chemical discovery}, journal = {NATURE COMMUNICATIONS}, volume = {11}, year = {2020}, doi = {10.1038/s41467-020-18556-9}, author = {von Lilienfeld, O. Anatole and Burke, Kieron}, abstract = {Over the last decade, we have witnessed the emergence of ever more machine learning applications in all aspects of the chemical sciences. Here, we highlight specific achievements of machine learning models in the field of computational chemistry by considering selected studies of electronic structure, interatomic potentials, and chemical compound space in chronological order.} }
@article{WOS:000428618900040, title = {Using human brain activity to guide machine learning}, journal = {SCIENTIFIC REPORTS}, volume = {8}, year = {2018}, issn = {2045-2322}, doi = {10.1038/s41598-018-23618-6}, author = {Fong, Ruth C. and Scheirer, Walter J. and Cox, David D.}, abstract = {Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of ``neurally-weighted'' machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data.} }
@article{WOS:000928006100002, title = {Context-Aware Machine Learning for Intelligent Transportation Systems: A Survey}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {24}, pages = {17-36}, year = {2023}, issn = {1524-9050}, doi = {10.1109/TITS.2022.3216462}, author = {Huang, Guang-Li and Zaslavsky, Arkady and Loke, Seng W. and Abkenar, Amin and Medvedev, Alexey and Hassani, Alireza}, abstract = {Context awareness adds intelligence to and enriches data for applications, services and systems while enabling underlying algorithms to sense dynamic changes in incoming data streams. Context-aware machine learning is often adopted in intelligent services by endowing meaning to Internet of Things(IoT)/ubiquitous data. Intelligent transportation systems (ITS) are at the forefront of applying context awareness with marked success. In contrast to non-context-aware machine learning models, context-aware machine learning models often perform better in traffic prediction/classification and are capable of supporting complex and more intelligent ITS decision-making. This paper presents a comprehensive review of recent studies in context-aware machine learning for intelligent transportation, especially focusing on road transportation systems. State-of-the-art techniques are discussed from several perspectives, including contextual data (e.g., location, time, weather, road condition and events), applications (i.e., traffic prediction and decision making), modes (i.e., specialised and general), learning methods (e.g., supervised, unsupervised, semi-supervised and transfer learning). Two main frameworks of context-aware machine learning models are summarised. In addition, open challenges and future research directions of developing context-aware machine learning models for ITS are discussed, and a novel context-aware machine learning layered engine (CAMILLE) architecture is proposed as a potential solution to address identified gaps in the studied body of knowledge.} }
@article{WOS:001130213200001, title = {Machine Learning Algorithm for Malware Detection: Taxonomy, Current Challenges, and Future Directions}, journal = {IEEE ACCESS}, volume = {11}, pages = {141045-141089}, year = {2023}, issn = {2169-3536}, doi = {10.1109/ACCESS.2023.3256979}, author = {Gorment, Nor Zakiah and Selamat, Ali and Cheng, Lim Kok and Krejcar, Ondrej}, abstract = {Malware has emerged as a cyber security threat that continuously changes to target computer systems, smart devices, and extensive networks with the development of information technologies. As a result, malware detection has always been a major worry and a difficult issue, owing to shortcomings in performance accuracy, analysis type, and malware detection approaches that fail to identify unexpected malware attacks. This paper seeks to conduct a thorough systematic literature review (SLR) and offer a taxonomy of machine learning methods for malware detection that considers these problems by analyzing 77 chosen research works related to malware detection using machine learning algorithm. The research investigates malware and machine learning in the context of cybersecurity, including malware detection taxonomy and machine learning algorithm classification into numerous categories. Furthermore, the taxonomy was used to evaluate the most recent machine learning algorithm and analysis. The paper also examines the obstacles and associated concerns encountered in malware detection and potential remedies. Finally, to address the related issues that would motivate researchers in their future work, an empirical study was utilized to assess the performance of several machine learning algorithms.} }
@article{WOS:000639523700001, title = {Artificial intelligence to deep learning: machine intelligence approach for drug discovery}, journal = {MOLECULAR DIVERSITY}, volume = {25}, pages = {1315-1360}, year = {2021}, issn = {1381-1991}, doi = {10.1007/s11030-021-10217-3}, author = {Gupta, Rohan and Srivastava, Devesh and Sahu, Mehar and Tiwari, Swati and Ambasta, Rashmi K. and Kumar, Pravir}, abstract = {Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure-activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. Graphic abstract The primary concern associated with drug design and development is time consumption and production cost. Further, inefficiency, inaccurate target delivery, and inappropriate dosage are other hurdles that inhibit the process of drug delivery and development. With advancements in technology, computer-aided drug design integrating artificial intelligence algorithms can eliminate the challenges and hurdles of traditional drug design and development. Artificial intelligence is referred to as superset comprising machine learning, whereas machine learning comprises supervised learning, unsupervised learning, and reinforcement learning. Further, deep learning, a subset of machine learning, has been extensively implemented in drug design and development. The artificial neural network, deep neural network, support vector machines, classification and regression, generative adversarial networks, symbolic learning, and meta-learning are examples of the algorithms applied to the drug design and discovery process. Artificial intelligence has been applied to different areas of drug design and development process, such as from peptide synthesis to molecule design, virtual screening to molecular docking, quantitative structure-activity relationship to drug repositioning, protein misfolding to protein-protein interactions, and molecular pathway identification to polypharmacology. Artificial intelligence principles have been applied to the classification of active and inactive, monitoring drug release, pre-clinical and clinical development, primary and secondary drug screening, biomarker development, pharmaceutical manufacturing, bioactivity identification and physiochemical properties, prediction of toxicity, and identification of mode of action.} }
@article{WOS:000668074700001, title = {An Ensemble Machine Learning Model for Enhancing the Prediction Accuracy of Energy Consumption in Buildings}, journal = {ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING}, volume = {47}, pages = {4105-4117}, year = {2022}, issn = {2193-567X}, doi = {10.1007/s13369-021-05927-7}, author = {Ngoc-Tri Ngo and Anh-Duc Pham and Thi Thu Ha Truong and Ngoc-Son Truong and Nhat-To Huynh and Tuan Minh Pham}, abstract = {Predicting building energy use is necessary for energy planning, management, and conservation. It is difficult to achieve accurate prediction results due to the inherent complexity of building thermal characteristics and occupant behavior. Machine learning has been recently applied for predicting energy consumption. Improving its predictive accuracy and generalization ability is essential. Therefore, this study proposed a machine learning model for an ensemble approach to forecasting energy consumption in non-residential buildings. Various datasets from non-residential buildings were collected to assess the predictive performance. Artificial neural networks, support vector regression, and M5Rules models were used as baseline models in this study. Evaluation results have confirmed the effectiveness of the ensemble machine learning model in the next 24-h energy consumption prediction in buildings. The mean absolute error (MAE) and mean absolute percentage error (MAPE) obtained by the ensemble machine learning model were 2.858 kWh and 16.141 kWh, respectively. The ensemble machine learning model can improve the MAE by 123.4\\% and the MAPE by 209.3\\% as compared to baseline models. This study contributes to highlighting the advantages of machine learning applications for the building sector. Ensemble machine learning models can be proposed as an effective method for forecasting energy consumption in buildings.} }
@article{WOS:000929540300001, title = {Insights into the Application of Machine Learning in Reservoir Engineering: Current Developments and Future Trends}, journal = {ENERGIES}, volume = {16}, year = {2023}, doi = {10.3390/en16031392}, author = {Wang, Hai and Chen, Shengnan}, abstract = {In the past few decades, the machine learning (or data-driven) approach has been broadly adopted as an alternative to scientific discovery, resulting in many opportunities and challenges. In the oil and gas sector, subsurface reservoirs are heterogeneous porous media involving a large number of complex phenomena, making their characterization and dynamic prediction a real challenge. This study provides a comprehensive overview of recent research that has employed machine learning in three key areas: reservoir characterization, production forecasting, and well test interpretation. The results show that machine learning can automate and accelerate many reservoirs engineering tasks with acceptable level of accuracy, resulting in more efficient and cost-effective decisions. Although machine learning presents promising results at this stage, there are still several crucial challenges that need to be addressed, such as data quality and data scarcity, the lack of physics nature of machine learning algorithms, and joint modelling of multiple data sources/formats. The significance of this research is that it demonstrates the potential of machine learning to revolutionize the oil and gas sector by providing more accurate and efficient solutions for challenging problems.} }
@article{WOS:000760455600001, title = {Machine Learning in Assessing the Performance of Hydrological Models}, journal = {HYDROLOGY}, volume = {9}, year = {2022}, doi = {10.3390/hydrology9010005}, author = {Rozos, Evangelos and Dimitriadis, Panayiotis and Bellos, Vasilis}, abstract = {Machine learning has been employed successfully as a tool virtually in every scientific and technological field. In hydrology, machine learning models first appeared as simple feed-forward networks that were used for short-term forecasting, and have evolved into complex models that can take into account even the static features of catchments, imitating the hydrological experience. Recent studies have found machine learning models to be robust and efficient, frequently outperforming the standard hydrological models (both conceptual and physically based). However, and despite some recent efforts, the results of the machine learning models require significant effort to interpret and derive inferences. Furthermore, all successful applications of machine learning in hydrology are based on networks of fairly complex topology that require significant computational power and CPU time to train. For these reasons, the value of the standard hydrological models remains indisputable. In this study, we suggest employing machine learning models not as a substitute for hydrological models, but as an independent tool to assess their performance. We argue that this approach can help to unveil the anomalies in catchment data that do not fit in the employed hydrological model structure or configuration, and to deal with them without compromising the understanding of the underlying physical processes.} }
@article{WOS:000640517400008, title = {Human and Machine Learning}, journal = {COMPUTATIONAL ECONOMICS}, volume = {57}, pages = {889-909}, year = {2021}, issn = {0927-7099}, doi = {10.1007/s10614-018-9803-z}, author = {Kao, Ying-Fang and Venkatachalam, Ragupathy}, abstract = {In this paper, we consider learning by human beings and machines in the light of Herbert Simon's pioneering contributions to the theory of Human Problem Solving. Using board games of perfect information as a paradigm, we explore differences in human and machine learning in complex strategic environments. In doing so, we contrast theories of learning in classical game theory with computational game theory proposed by Simon. Among theories that invoke computation, we make a further distinction between computable and computational or machine learning theories. We argue that the modern machine learning algorithms, although impressive in terms of their performance, do not necessarily shed enough light on human learning. Instead, they seem to take us further away from Simon's lifelong quest to understand the mechanics of actual human behaviour.} }
@article{WOS:000727217500001, title = {Artificial Intelligence Methodologies for Data Management}, journal = {SYMMETRY-BASEL}, volume = {13}, year = {2021}, doi = {10.3390/sym13112040}, author = {Serey, Joel and Quezada, Luis and Alfaro, Miguel and Fuertes, Guillermo and Vargas, Manuel and Ternero, Rodrigo and Sabattin, Jorge and Duran, Claudia and Gutierrez, Sebastian}, abstract = {This study analyses the main challenges, trends, technological approaches, and artificial intelligence methods developed by new researchers and professionals in the field of machine learning, with an emphasis on the most outstanding and relevant works to date. This literature review evaluates the main methodological contributions of artificial intelligence through machine learning. The methodology used to study the documents was content analysis; the basic terminology of the study corresponds to machine learning, artificial intelligence, and big data between the years 2017 and 2021. For this study, we selected 181 references, of which 120 are part of the literature review. The conceptual framework includes 12 categories, four groups, and eight subgroups. The study of data management using AI methodologies presents symmetry in the four machine learning groups: supervised learning, unsupervised learning, semi-supervised learning, and reinforced learning. Furthermore, the artificial intelligence methods with more symmetry in all groups are artificial neural networks, Support Vector Machines, K-means, and Bayesian Methods. Finally, five research avenues are presented to improve the prediction of machine learning.} }
@article{WOS:000441141200005, title = {Discrete space reinforcement learning algorithm based on support vector machine classification}, journal = {PATTERN RECOGNITION LETTERS}, volume = {111}, pages = {30-35}, year = {2018}, issn = {0167-8655}, doi = {10.1016/j.patrec.2018.04.012}, author = {An, Yuexuan and Ding, Shifei and Shi, Songhui and Li, Jingcan}, abstract = {When facing discrete space learning problems, the traditional reinforcement learning algorithms often have the problems of slow convergence and poor convergence accuracy. Deep reinforcement learning needs a large number of learning samples in its learning process, so it often faces with the problems that the algorithm is difficult to converge and easy to fall into local minimums. In view of the above problems, we apply support vector machines classification to reinforcement learning, and propose an algorithm named Advantage Actor-Critic with Support Vector Machine Classification (SVM-A2C). Our algorithm adopts the actor-critic framework and uses the support vector machine classification as a result of the actor's action output, while Critic uses the advantage function to improve and optimize the parameters of support vector machine. In addition, since the environment is changing all the time in reinforcement learning, it is difficult to find a global optimal solution for the support vector machines, the gradient descent method is applied to optimize the parameters of support vector machine. So that the agent can quickly learn a more precise action selection policy. Finally, the effectiveness of the proposed method is proved by the classical experimental environment of reinforcement learning. It is proved that the algorithm proposed in this paper has shorter episodes to convergence and more accurate results than other algorithms. (C) 2018 Elsevier B.V. All rights reserved.} }
@article{WOS:000401789000024, title = {A Review of Machine Learning Applications in Veterinary Field}, journal = {KAFKAS UNIVERSITESI VETERINER FAKULTESI DERGISI}, volume = {23}, pages = {673-680}, year = {2017}, issn = {1300-6045}, doi = {10.9775/kvfd.2016.17281}, author = {Cihan, Pinar and Gokce, Erhan and Kalipsiz, Oya}, abstract = {Machine learning is a sub field of artificial intelligence which allows forecasting through learning past behaviors and rules from old data. In today's world, machine learning is being used almost in any fields such as education, medicine, veterinary, banking, telecommunication, security, and bio-medical sciences. In human health, although machine learning is generally preferred particularly in predicting diseases and identifying respective risk factors, it is obvious that there are a limited number of publications where this method was applied on veterinary or indicates whether it is correct and applicable. In this review, it was observed that the neural network, logistic regression, linear regression, multiple regression, principle component analysis and k-means methods were frequently used in examined publications and machine learning application in veterinary field upward momentum. Additionally, it was observed that recent developments in the field of machine learning (deep learning, ensemble learning, voice recognition, emotion recognition, etc.) is still new in the field of veterinary. In this review, publications are examined under clustering, classification, regression, multivariate data analysis and image processing topics. This review aims at providing basic information on machine learning and to increase the number of multidisciplinary publications on computer sciences/engineering and veterinary field.} }
@article{WOS:000342248100012, title = {Online sequential extreme learning machine with kernels for nonstationary time series prediction}, journal = {NEUROCOMPUTING}, volume = {145}, pages = {90-97}, year = {2014}, issn = {0925-2312}, doi = {10.1016/j.neucom.2014.05.068}, author = {Wang, Xinying and Han, Min}, abstract = {In this paper, an online sequential extreme learning machine with kernels (OS-ELMK) has been proposed for nonstationary time series prediction. An online sequential learning algorithm, which can learn samples one-by-one or chunk-by-chunk, is developed for extreme learning machine with kernels. A limited memory prediction strategy based on the proposed OS-ELMK is designed to model the nonstationary time series. Performance comparisons of OS-ELMK with other existing algorithms are presented using artificial and real life nonstationary time series data. The results show that the proposed OS-ELMK produces similar or better accuracies with at least an order-of-magnitude reduction in the learning time. (C) 2014 Elsevier B.V. All rights reserved.} }
@article{WOS:000506280400001, title = {Machine-learning-based damage identification methods with features derived from moving principal component analysis}, journal = {MECHANICS OF ADVANCED MATERIALS AND STRUCTURES}, volume = {27}, pages = {1789-1802}, year = {2020}, issn = {1537-6494}, doi = {10.1080/15376494.2019.1710308}, author = {Zhang, Ge and Tang, Liqun and Liu, Zejia and Zhou, Licheng and Liu, Yiping and Jiang, Zhenyu}, abstract = {This paper aims to propose machine-learning-based damage identification methods with features derived from moving principal component analysis (MPCA) to improve the damage identification performance for engineering structures. Previously, machine learning algorithms have usually used structural responses as inputs directly. These methods show low damage identification capabilities and are susceptible to noise. In this paper, the eigenvectors of structural responses derived from MPCA are employed as inputs instead. Several traditional machine learning algorithms are applied for verification. The results demonstrate that as compared to strains and frequencies, their eigenvectors as inputs for machine learning algorithms render better performances for damage identification.} }
@article{WOS:000601113600025, title = {How Machine Learning Accelerates the Development of Quantum Dots?†}, journal = {CHINESE JOURNAL OF CHEMISTRY}, volume = {39}, pages = {181-188}, year = {2021}, issn = {1001-604X}, doi = {10.1002/cjoc.202000393}, author = {Peng, Jia and Muhammad, Ramzan and Wang, Shu-Liang and Zhong, Hai-Zheng}, abstract = {With the rapid developments in the field of information technology, the material research society is looking for an alternate scientific route to the traditional methods of trial and error in material research and process development. Machine learning emerges as a new research paradigm to accelerate the application-oriented material discovery. Quantum dots are expanded as functional nanomaterials to enhance cutting-edge photonic technology. However, they suffer from uncertainty in industrial fabrication and application. Here, we discuss how machine learning accelerates the development of quantum dots. The basic principles and operation procedures of machine learning are described with a few representative examples of quantum dots. We emphasize how machine learning contributes to the optimization of synthesis and the analysis of material characterizations. To conclude, we give a short perspective discussing the problems of combining machine learning and quantum dots.} }
@article{WOS:000500381500027, title = {Applications of machine learning methods for engineering risk assessment - A review}, journal = {SAFETY SCIENCE}, volume = {122}, year = {2020}, issn = {0925-7535}, doi = {10.1016/j.ssci.2019.09.015}, author = {Hegde, Jeevith and Rokseth, Borge}, abstract = {The purpose of this article is to present a structured review of publications utilizing machine learning methods to aid in engineering risk assessment. A keyword search is performed to retrieve relevant articles from the databases of Scopus and Engineering Village. The search results are filtered according to seven selection criteria. The filtering process resulted in the retrieval of one hundred and twenty-four relevant research articles. Statistics based on different categories from the citation database is presented. By reviewing the articles, additional categories, such as the type of machine learning algorithm used, the type of input source used, the type of industry targeted, the type of implementation, and the intended risk assessment phase are also determined. The findings show that the automotive industry is leading the adoption of machine learning algorithms for risk assessment. Artificial neural networks are the most applied machine learning method to aid in engineering risk assessment. Additional findings from the review process are also presented in this article.} }
@article{WOS:000379822600001, title = {Machine learning in manufacturing: advantages, challenges, and applications}, journal = {PRODUCTION AND MANUFACTURING RESEARCH-AN OPEN ACCESS JOURNAL}, volume = {4}, pages = {23-45}, year = {2016}, doi = {10.1080/21693277.2016.1192517}, author = {Wuest, Thorsten and Weimer, Daniel and Irgens, Christopher and Thoben, Klaus-Dieter}, abstract = {The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.} }
@article{WOS:000510903200023, title = {Diagnosing Rotating Machines With Weakly Supervised Data Using Deep Transfer Learning}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {16}, pages = {1688-1697}, year = {2020}, issn = {1551-3203}, doi = {10.1109/TII.2019.2927590}, author = {Li, Xiang and Zhang, Wei and Ding, Qian and Li, Xu}, abstract = {Rotating machinery fault diagnosis problems have been well-addressed when sufficient supervised data of the tested machine are available using the latest data-driven methods. However, it is still challenging to develop effective diagnostic method with insufficient training data, which is highly demanded in real-industrial scenarios, since high-quality data are usually difficult and expensive to collect. Considering the underlying similarities of rotating machines, data mining on different but related equipments potentially benefit the diagnostic performance on the target machine. Therefore, a novel transfer learning method for diagnostics based on deep learning is proposed in this article, where the diagnostic knowledge learned from sufficient supervised data of multiple rotating machines is transferred to the target equipment with domain adversarial training. Different from the existing studies, a more generalized transfer learning problem with different label spaces of domains is investigated, and different fault severities are also considered in fault diagnostics. The experimental results on four datasets validate the effectiveness of the proposed method, and show it is feasible and promising to explore different datasets to improve diagnostic performance.} }
@article{WOS:000521984900003, title = {Presenting machine learning model information to clinical end users with model facts labels}, journal = {NPJ DIGITAL MEDICINE}, volume = {3}, year = {2020}, issn = {2398-6352}, doi = {10.1038/s41746-020-0253-3}, author = {Sendak, Mark P. and Gao, Michael and Brajer, Nathan and Balu, Suresh}, abstract = {There is tremendous enthusiasm surrounding the potential for machine learning to improve medical prognosis and diagnosis. However, there are risks to translating a machine learning model into clinical care and clinical end users are often unaware of the potential harm to patients. This perspective presents the ``Model Facts'' label, a systematic effort to ensure that front-line clinicians actually know how, when, how not, and when not to incorporate model output into clinical decisions. The ``Model Facts'' label was designed for clinicians who make decisions supported by a machine learning model and its purpose is to collate relevant, actionable information in 1-page. Practitioners and regulators must work together to standardize presentation of machine learning model information to clinical end users in order to prevent harm to patients. Efforts to integrate a model into clinical practice should be accompanied by an effort to clearly communicate information about a machine learning model with a ``Model Facts'' label.} }
@article{WOS:001441035000001, title = {Advancements in cache management: a review of machine learning innovations for enhanced performance and security}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {8}, year = {2025}, doi = {10.3389/frai.2025.1441250}, author = {Krishna, Keshav}, abstract = {Machine learning techniques have emerged as a promising tool for efficient cache management, helping optimize cache performance and fortify against security threats. The range of machine learning is vast, from reinforcement learning-based cache replacement policies to Long Short-Term Memory (LSTM) models predicting content characteristics for caching decisions. Diverse techniques such as imitation learning, reinforcement learning, and neural networks are extensively useful in cache-based attack detection, dynamic cache management, and content caching in edge networks. The versatility of machine learning techniques enables them to tackle various cache management challenges, from adapting to workload characteristics to improving cache hit rates in content delivery networks. A comprehensive review of various machine learning approaches for cache management is presented, which helps the community learn how machine learning is used to solve practical challenges in cache management. It includes reinforcement learning, deep learning, and imitation learning-driven cache replacement in hardware caches. Information on content caching strategies and dynamic cache management using various machine learning techniques in cloud and edge computing environments is also presented. Machine learning-driven methods to mitigate security threats in cache management have also been discussed.} }
@article{WOS:000641162100002, title = {A Comprehensive Survey on Graph Neural Networks}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS}, volume = {32}, pages = {4-24}, year = {2021}, issn = {2162-237X}, doi = {10.1109/TNNLS.2020.2978386}, author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.}, abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.} }
@article{WOS:000572537200011, title = {Machine learning in materials genome initiative: A review}, journal = {JOURNAL OF MATERIALS SCIENCE \\& TECHNOLOGY}, volume = {57}, pages = {113-122}, year = {2020}, issn = {1005-0302}, doi = {10.1016/j.jmst.2020.01.067}, author = {Liu, Yingli and Niu, Chen and Wang, Zhuo and Gan, Yong and Zhu, Yan and Sun, Shuhong and Shen, Tao}, abstract = {Discovering new materials with excellent performance is a hot issue in the materials genome initiative. Traditional experiments and calculations often waste large amounts of time and money and are also limited by various conditions. Therefore, it is imperative to develop a new method to accelerate the discovery and design of new materials. In recent years, material discovery and design methods using machine learning have attracted much attention from material experts and have made some progress. This review first outlines available materials database and material data analytics tools and then elaborates on the machine learning algorithms used in materials science. Next, the field of application of machine learning in materials science is summarized, focusing on the aspects of structure determination, performance prediction, fingerprint prediction, and new material discovery. Finally, the review points out the problems of data and machine learning in materials science and points to future research. Using machine learning algorithms, the authors hope to achieve amazing results in material discovery and design. (C) 2020 Published by Elsevier Ltd on behalf of The editorial office of Journal of Materials Science \\& Technology.} }
@article{WOS:000823964300001, title = {Machine-designed biotherapeutics: opportunities, feasibility and advantages of deep learning in computational antibody discovery}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {23}, year = {2022}, issn = {1467-5463}, doi = {10.1093/bib/bbac267}, author = {Wilman, Wiktoria and Wrobel, Sonia and Bielska, Weronika and Deszynski, Piotr and Dudzic, Pawel and Jaszczyszyn, Igor and Kaniewski, Jedrzej and Mlokosiewicz, Jakub and Rouyan, Anahita and Satlawa, Tadeusz and Kumar, Sandeep and Greiff, Victor and Krawczyk, Konrad}, abstract = {Antibodies are versatile molecular binders with an established and growing role as therapeutics. Computational approaches to developing and designing these molecules are being increasingly used to complement traditional lab-based processes. Nowadays, in silico methods fill multiple elements of the discovery stage, such as characterizing antibody-antigen interactions and identifying developability liabilities. Recently, computational methods tackling such problems have begun to follow machine learning paradigms, in many cases deep learning specifically. This paradigm shift offers improvements in established areas such as structure or binding prediction and opens up new possibilities such as language-based modeling of antibody repertoires or machine-learning-based generation of novel sequences. In this review, we critically examine the recent developments in (deep) machine learning approaches to therapeutic antibody design with implications for fully computational antibody design.} }
@article{WOS:000498675500022, title = {How to Read Articles That Use Machine Learning Users' Guides to the Medical Literature}, journal = {JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION}, volume = {322}, pages = {1806-1816}, year = {2019}, issn = {0098-7484}, doi = {10.1001/jama.2019.16489}, author = {Liu, Yun and Chen, Po-Hsuan Cameron and Krause, Jonathan and Peng, Lily}, abstract = {In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.} }
@article{WOS:001026236800015, title = {Quantum machine learning beyond kernel methods}, journal = {NATURE COMMUNICATIONS}, volume = {14}, year = {2023}, doi = {10.1038/s41467-023-36159-y}, author = {Jerbi, Sofiene and Fiderer, Lukas J. and Poulsen Nautrup, Hendrik and Kuebler, Jonas M. and Briegel, Hans J. and Dunjko, Vedran}, abstract = {Machine learning algorithms based on parametrized quantum circuits are prime candidates for near-term applications on noisy quantum computers. In this direction, various types of quantum machine learning models have been introduced and studied extensively. Yet, our understanding of how these models compare, both mutually and to classical models, remains limited. In this work, we identify a constructive framework that captures all standard models based on parametrized quantum circuits: that of linear quantum models. In particular, we show using tools from quantum information theory how data re-uploading circuits, an apparent outlier of this framework, can be efficiently mapped into the simpler picture of linear models in quantum Hilbert spaces. Furthermore, we analyze the experimentally-relevant resource requirements of these models in terms of qubit number and amount of data needed to learn. Based on recent results from classical machine learning, we prove that linear quantum models must utilize exponentially more qubits than data re-uploading models in order to solve certain learning tasks, while kernel methods additionally require exponentially more data points. Our results provide a more comprehensive view of quantum machine learning models as well as insights on the compatibility of different models with NISQ constraints. Comparing the capabilities of different quantum machine learning protocols is difficult. Here, the authors show that different learning models based on parametrized quantum circuits can all be seen as quantum linear models, thus driving general conclusions on their resource requirements and capabilities.} }
@article{WOS:000636768300001, title = {Radiomic and Genomic Machine Learning Method Performance for Prostate Cancer Diagnosis: Systematic Literature Review}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {23}, year = {2021}, issn = {1438-8871}, doi = {10.2196/22394}, author = {Castaldo, Rossana and Cavaliere, Carlo and Soricelli, Andrea and Salvatore, Marco and Pecchia, Leandro and Franzese, Monica}, abstract = {Background: Machine learning algorithms have been drawing attention at the joining of pathology and radiology in prostate cancer research. However, due to their algorithmic learning complexity and the variability of their architecture, there is an ongoing need to analyze their performance. Objective: This study assesses the source of heterogeneity and the performance of machine learning applied to radiomic, genomic, and clinical biomarkers for the diagnosis of prostate cancer. One research focus of this study was on clearly identifying problems and issues related to the implementation of machine learning in clinical studies. Methods: Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) protocol, 816 titles were identified from the PubMed, Scopus, and OvidSP databases. Studies that used machine learning to detect prostate cancer and provided performance measures were included in our analysis. The quality of the eligible studies was assessed using the QUADAS-2 (quality assessment of diagnostic accuracy studies-version 2) tool. The hierarchical multivariate model was applied to the pooled data in a meta-analysis. To investigate the heterogeneity among studies, I-2 statistics were performed along with visual evaluation of coupled forest plots. Due to the internal heterogeneity among machine learning algorithms, subgroup analysis was carried out to investigate the diagnostic capability of machine learning systems in clinical practice. Results: In the final analysis, 37 studies were included, of which 29 entered the meta-analysis pooling. The analysis of machine learning methods to detect prostate cancer reveals the limited usage of the methods and the lack of standards that hinder the implementation of machine learning in clinical applications. Conclusions: The performance of machine learning for diagnosis of prostate cancer was considered satisfactory for several studies investigating the multiparametric magnetic resonance imaging and urine biomarkers; however, given the limitations indicated in our study, further studies are warranted to extend the potential use of machine learning to clinical settings. Recommendations on the use of machine learning techniques were also provided to help researchers to design robust studies to facilitate evidence generation from the use of radiomic and genomic biomarkers.} }
@article{WOS:000530096900011, title = {Privacy preservation for machine learning training and classification based on homomorphic encryption schemes}, journal = {INFORMATION SCIENCES}, volume = {526}, pages = {166-179}, year = {2020}, issn = {0020-0255}, doi = {10.1016/j.ins.2020.03.041}, author = {Li, Jing and Kuang, Xiaohui and Lin, Shujie and Ma, Xu and Tang, Yi}, abstract = {In recent years, more and more machine learning algorithms depend on the cloud computing. When a machine learning system is trained or classified in the cloud environment, the cloud server obtains data from the user side. Then, the privacy of the data depends on the service provider, it is easy to induce the malicious acquisition and utilization of data. On the other hand, the attackers can detect the statistical characteristics of machine learning data and infer the parameters of machine learning model through reverse attacks. Therefore, it is urgent to design an effective encryption scheme to protect the data's privacy without breaking the performance of machine learning. In this paper, we propose a novel homomorphic encryption framework over non-abelian rings, and define the homomorphism operations in ciphertexts space. The scheme can achieve one-way security based on the Conjugacy Search Problem. After that, a homomorphic encryption was proposed over a matrix-ring. It supports real numbers encryption based on the homomorphism of 2-order displacement matrix coding function and achieves fast ciphertexts homomorphic comparison without decrypting any ciphetexts operations' intermediate result. Furthermore, we use the scheme to realize privacy preservation for machine learning training and classification in data ciphertexts environment. The analysis shows that our proposed schemes are efficient for encryption/decryption and homomorphic operations. (C) 2020 Elsevier Inc. All rights reserved.} }
@article{WOS:000663165500001, title = {River: machine learning for streaming data in Python}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {22}, year = {2021}, issn = {1532-4435}, author = {Montiel, Jacob and Halford, Max and Mastelini, Saulo Martiello and Bolmier, Geoffrey and Sourty, Raphael and Vaysse, Robin and Zouitine, Adil and Gomes, Heitor Murilo and Read, Jesse and Abdessalem, Talel and Bifet, Albert}, abstract = {River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of two popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.} }
@article{WOS:001411195100001, title = {Causal machine learning for supply chain risk prediction and intervention planning}, journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH}, volume = {63}, pages = {5629-5648}, year = {2025}, issn = {0020-7543}, doi = {10.1080/00207543.2025.2458121}, author = {Wyrembek, Mateusz and Baryannis, George and Brintrup, Alexandra}, abstract = {The ultimate goal for developing machine learning models in supply chain management is to make optimal interventions. However, most machine learning models identify correlations in data rather than inferring causation, making it difficult to systematically plan for better outcomes. In this article, we propose and evaluate the use of causal machine learning for developing supply chain risk intervention models, and demonstrate its use with a case study in supply chain risk management in the maritime engineering sector. Our findings highlight that causal machine learning enhances decision-making processes by identifying changes that can be achieved under different supply chain interventions, allowing `what-if' scenario planning. We therefore propose different machine learning developmental pathways for predicting risk and planning for interventions to minimise risk and outline key steps for supply chain researchers to explore causal machine learning and harness its capabilities.} }
@article{WOS:000705187000001, title = {Exploiting machine learning for bestowing intelligence to microfluidics}, journal = {BIOSENSORS \\& BIOELECTRONICS}, volume = {194}, year = {2021}, issn = {0956-5663}, doi = {10.1016/j.bios.2021.113666}, author = {Zheng, Jiahao and Cole, Tim and Zhang, Yuxin and Kim, Jeeson and Tang, Shi-Yang}, abstract = {Intelligent microfluidics is an emerging cross-discipline research area formed by combining microfluidics with machine learning. It uses the advantages of microfluidics, such as high throughput and controllability, and the powerful data processing capabilities of machine learning, resulting in improved systems in biotechnology and chemistry. Compared to traditional microfluidics using manual analysis methods, intelligent microfluidics needs less human intervention, and results in a more user-friendly experience with faster processing. There is a paucity of literature reviewing this burgeoning and highly promising cross-discipline. Therefore, we herein comprehensively and systematically summarize several aspects of microfluidic applications enabled by machine learning. We list the types of microfluidics used in intelligent microfluidic applications over the last five years, as well as the machine learning algorithms and the hardware used for training. We also present the most recent advances in key technologies, developments, challenges, and the emerging opportunities created by intelligent microfluidics.} }
@article{WOS:000457664700017, title = {Generalized class-specific kernelized extreme learning machine for multiclass imbalanced learning}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {121}, pages = {244-255}, year = {2019}, issn = {0957-4174}, doi = {10.1016/j.eswa.2018.12.024}, author = {Raghuwanshi, Bhagat Singh and Shukla, Sanyam}, abstract = {Class imbalanced learning is a well-known issue, which exists in real-world applications. Datasets that have skewed class distribution raise hindrance to the traditional learning algorithms. Traditional classifiers give the same importance to all the samples, which leads to the prediction biased towards the majority classes. To solve this intrinsic deficiency, numerous strategies have been proposed such as weighted extreme learning machine (WELM), weighted support vector machine (WSVM), class-specific extreme learning machine (CS-ELM) and class-specific kernelized extreme learning machine (CSKELM). This work focuses on multiclass imbalance problems, which are more difficult compared to the binary class imbalance problems. Kernelized extreme learning machine (KELM) yields better results compared to the traditional extreme learning machine (ELM), which uses random input parameters. This work presents a generalized CSKELM (GCSKELM), the extension of our recently proposed CSKELM, which addresses the multiclass imbalanced problems more effectively. The proposed GCSKELM can be applied directly to solve the multiclass imbalanced problems. GCSKELM with Gaussian kernel function avoids the non-optimal hidden node problem associated with CS-ELM and other existing variants of ELM. The proposed work also has less computational cost in contrast with kernelized WELM (KWELM) for multiclass imbalanced learning. This work employs class-specific regularization parameters, which are determined by employing class proportion. The extensive experimental analysis shows that the proposed work obtains promising generalization performance in contrast with the other state-of-the-art imbalanced learning methods. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:000425010500008, title = {On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products}, journal = {BIG DATA}, volume = {5}, pages = {246-255}, year = {2017}, issn = {2167-6461}, doi = {10.1089/big.2016.0051}, author = {Varshney, Kush R. and Alemzadeh, Homa}, abstract = {Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.} }
@article{WOS:001169793700001, title = {Machine learning and IoT - Based predictive maintenance approach for industrial applications}, journal = {ALEXANDRIA ENGINEERING JOURNAL}, volume = {88}, pages = {298-309}, year = {2024}, issn = {1110-0168}, doi = {10.1016/j.aej.2023.12.065}, author = {Elkateb, Sherien and Metwalli, Ahmed and Shendy, Abdelrahman and Abu-Elanien, Ahmed E. B.}, abstract = {Unplanned outage in industry due to machine failures can lead to significant production losses and increased maintenance costs. Predictive maintenance methods use the data collected from IoT-enabled devices installed in working machines to detect incipient faults and prevent major failures. In this study, a predictive maintenance system based on machine learning algorithms, specifically AdaBoost, is presented to classify different types of machines stops in real-time with application in knitting machines. The data collected from the machines include machine speeds and steps, which were pre-processed and fed into the machine learning model to classify six types of machines stops: gate stop, feeder stop, needle stop, completed roll stop, idle stop, and lycra stop. The model is trained and optimized using a combination of hyperparameter tuning and cross-validation techniques to achieve an accuracy of 92\\% on the test set. The results demonstrate the potential of the proposed system to accurately classify machine stops and enable timely maintenance actions; thereby, improving the overall efficiency and productivity of the textile industry.} }
@article{WOS:000799624800014, title = {Deceptive Logic Locking for Hardware Integrity Protection Against Machine Learning Attacks}, journal = {IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS}, volume = {41}, pages = {1716-1729}, year = {2022}, issn = {0278-0070}, doi = {10.1109/TCAD.2021.3100275}, author = {Sisejkovic, Dominik and Merchant, Farhad and Reimann, Lennart M. and Leupers, Rainer}, abstract = {Logic locking has emerged as a prominent key-driven technique to protect the integrity of integrated circuits. However, novel machine-learning-based attacks have recently been introduced to challenge the security foundations of locking schemes. These attacks are able to recover a significant percentage of the key without having access to an activated circuit. This article address this issue through two focal points. First, we present a theoretical model to test locking schemes for key-related structural leakage that can be exploited by machine learning. Second, based on the theoretical model, we introduce D-MUX: a deceptive multiplexer-based logic-locking scheme that is resilient against structure-exploiting machine learning attacks. Through the design of D-MUX, we uncover a major fallacy in the existing multiplexer-based locking schemes in the form of a structural-analysis attack. Finally, an extensive cost evaluation of D-MUX is presented. To the best of our knowledge, D-MUX is the first machine-learning-resilient locking scheme capable of protecting against all known learning-based attacks. Hereby, the presented work offers a starting point for the design and evaluation of future-generation logic locking in the era of machine learning.} }
@article{WOS:000701874800010, title = {Adversarial machine learning in Network Intrusion Detection Systems}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {186}, year = {2021}, issn = {0957-4174}, doi = {10.1016/j.eswa.2021.115782}, author = {Alhajjar, Elie and Maxwell, Paul and Bastian, Nathaniel}, abstract = {Adversarial examples are inputs to a machine learning system intentionally crafted by an attacker to fool the model into producing an incorrect output. These examples have achieved a great deal of success in several domains such as image recognition, speech recognition and spam detection. In this paper, we study the nature of the adversarial problem in Network Intrusion Detection Systems (NIDS). We focus on the attack perspective, which includes techniques to generate adversarial examples capable of evading a variety of machine learning models. More specifically, we explore the use of evolutionary computation (particle swarm optimization and genetic algorithm) and deep learning (generative adversarial networks) as tools for adversarial example generation. To assess the performance of these algorithms in evading a NIDS, we apply them to two publicly available data sets, namely the NSL-KDD and UNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo simulation. The results show that our adversarial example generation techniques cause high misclassification rates in eleven different machine learning models, along with a voting classifier. Our work highlights the vulnerability of machine learning based NIDS in the face of adversarial perturbation.} }
@article{WOS:000883846400002, title = {Human vs. supervised machine learning: Who learns patterns faster?}, journal = {COGNITIVE SYSTEMS RESEARCH}, volume = {76}, pages = {78-92}, year = {2022}, issn = {2214-4366}, doi = {10.1016/j.cogsys.2022.09.002}, author = {Kuehl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik}, abstract = {The capabilities of supervised machine learning (SML), especially compared to human abilities, are being discussed in scientific research and in the usage of SML. This study provides an answer to how learning performance differs between humans and machines when there is limited training data. We have designed an experiment in which 44 humans and three different machine learning algorithms identify patterns in labeled training data and have to label instances according to the patterns they find. The results show a high dependency between performance and the underlying patterns of the task. Whereas humans perform relatively similarly across all patterns, machines show large performance differences for the various patterns in our experiment. After seeing 20 instances in the experiment, human performance does not improve anymore, which we relate to theories of cognitive overload. Machines learn slower but can reach the same level or may even outperform humans in 2 of the 4 of used patterns. However, machines need more instances compared to humans for the same results. The performance of machines is comparably lower for the other 2 patterns due to the difficulty of combining input features.} }
@article{WOS:000534712200003, title = {A stacked ensemble learning model for intrusion detection in wireless network}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {15387-15395}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-020-04986-5}, author = {Rajadurai, Hariharan and Gandhi, Usha Devi}, abstract = {Intrusion detection pretended to be a major technique for revealing the attacks and guarantee the security on the network. As the data increases tremendously every year on the Internet, a single algorithm is not sufficient for the network security. Because, deploying a single learning approach may suffer from statistical, computational and representational issues. To eliminate these issues, this paper combines multiple machine learning algorithms called stacked ensemble learning, to detect the attacks in a better manner than conventional learning, where a single algorithm is used to identify the attacks. The stacked ensemble system has been taken the benchmark data set, NSL-KDD, to compare its performance with other popular machine learning algorithms such as ANN, CART, random forest, SVM and other machine learning methods proposed by researchers. The experimental results show that stacked ensemble learning is a proper technique for classifying attacks than other existing methods. And also, the proposed system shows better accuracy compare to other intrusion detection models.} }
@article{WOS:000444816000005, title = {Machine Learning for the Developing World}, journal = {ACM TRANSACTIONS ON MANAGEMENT INFORMATION SYSTEMS}, volume = {9}, year = {2018}, issn = {2158-656X}, doi = {10.1145/3210548}, author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur}, abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.} }
@article{WOS:001025397400001, title = {A review on the applications of machine learning and deep learning in agriculture section for the production of crop biomass raw materials}, journal = {ENERGY SOURCES PART A-RECOVERY UTILIZATION AND ENVIRONMENTAL EFFECTS}, volume = {45}, pages = {9178-9201}, year = {2023}, issn = {1556-7036}, doi = {10.1080/15567036.2023.2232322}, author = {Peng, Wei and Karimi Sadaghiani, Omid}, abstract = {The application of biomass, as an energy resource, depends on four main steps of production, pre-treatment, bio-refinery, and upgrading. This work reviews Machine Learning applications in the biomass production step with focusing on agriculture crops. By investigating numerous related works, it is concluded that there is a considerable reviewing gap in collecting the applications of Machine Learning in crop biomass production. To fill this gap by the current work, the origin of biomass raw materials is explained, and the application of Machine Learning in this section is scrutinized. Then, the kinds and resources of biomass as well as the role of machine learning in these fields are reviewed. Meanwhile, the sustainable production of farming-origin biomass and the effective factors in this issue are explained, and the application of Machine Learning in these areas are surveyed. Summarily, after analysis of numerous papers, it is concluded that Machine Learning and Deep Learning are widely utilized in crop biomass production areas to enhance the crops production quantity, quality, and sustainability, improve the predictions, decrease the costs, and diminish the products losses. According to the statistical analysis, in 19\\% of the studies conducted about the application of Machine Learning and Deep Learning in crop biomass raw materials, Artificial Neural Network (ANN) algorithm has been applied. Afterward, the Random Forest (RF) and Super Vector Machine (SVM) are the second and third most-utilized algorithms applied in 17\\% and 15\\% of studies, respectively. Meanwhile, 26\\% of studies focused on the applications of Machine Learning and Deep Learning in the sugar crops. At the second and third places, the starchy crops and algae with 23\\% and 21\\% received more attention of researchers in the utilization of Machine Learning and Deep Learning techniques.} }
@article{WOS:000642962200001, title = {Federated Quantum Machine Learning}, journal = {ENTROPY}, volume = {23}, year = {2021}, doi = {10.3390/e23040460}, author = {Chen, Samuel Yen-Chi and Yoo, Shinjae}, abstract = {Distributed training across several quantum computers could significantly improve the training time and if we could share the learned model, not the data, it could potentially improve the data privacy as the training would happen where the data is located. One of the potential schemes to achieve this property is the federated learning (FL), which consists of several clients or local nodes learning on their own data and a central node to aggregate the models collected from those local nodes. However, to the best of our knowledge, no work has been done in quantum machine learning (QML) in federation setting yet. In this work, we present the federated training on hybrid quantum-classical machine learning models although our framework could be generalized to pure quantum machine learning model. Specifically, we consider the quantum neural network (QNN) coupled with classical pre-trained convolutional model. Our distributed federated learning scheme demonstrated almost the same level of trained model accuracies and yet significantly faster distributed training. It demonstrates a promising future research direction for scaling and privacy aspects.} }
@article{WOS:000621832300030, title = {Nowcasting GDP using machine-learning algorithms: A real-time assessment}, journal = {INTERNATIONAL JOURNAL OF FORECASTING}, volume = {37}, pages = {941-948}, year = {2021}, issn = {0169-2070}, doi = {10.1016/j.ijforecast.2020.10.005}, author = {Richardson, Adam and Mulder, Thomas van Florenstein and Vehbi, Tugrul}, abstract = {Can machine-learning algorithms help central banks understand the current state of the economy? Our results say yes! We contribute to the emerging literature on forecasting macroeconomic variables using machine-learning algorithms by testing the nowcast performance of common algorithms in a full `real-time' setting-that is, with real-time vintages of New Zealand GDP growth (our target variable) and real-time vintages of around 600 predictors. Our results show that machine-learning algorithms are able to significantly improve over a simple autoregressive benchmark and a dynamic factor model. We also show that machine-learning algorithms have the potential to add value to, and in one case improve on, the official forecasts of the Reserve Bank of New Zealand. (C) 2020 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000616713000005, title = {A Comprehensive Review on Medical Diagnosis Using Machine Learning}, journal = {CMC-COMPUTERS MATERIALS \\& CONTINUA}, volume = {67}, pages = {1997-2014}, year = {2021}, issn = {1546-2218}, doi = {10.32604/cmc.2021.014943}, author = {Bhavsar, Kaustubh Arun and Abugabah, Ahed and Singla, Jimmy and AlZubi, Ahmad Ali and Bashir, Ali Kashif and Nikita}, abstract = {The unavailability of sufficient information for proper diagnosis, incomplete or miscommunication between patient and the clinician, or among the healthcare professionals, delay or incorrect diagnosis, the fatigue of clinician, or even the high diagnostic complexity in limited time can lead to diagnostic errors. Diagnostic errors have adverse effects on the treatment of a patient. Unnecessary treatments increase the medical bills and deteriorate the health of a patient. Such diagnostic errors that harm the patient in various ways could be minimized using machine learning. Machine learning algorithms could be used to diagnose various diseases with high accuracy. The use of machine learning could assist the doctors in making decisions on time, and could also be used as a second opinion or supporting tool. This study aims to provide a comprehensive review of research articles published from the year 2015 to mid of the year 2020 that have used machine learning for diagnosis of various diseases. We present the various machine learning algorithms used over the years to diagnose various diseases. The results of this study show the distribution of machine learning methods by medical disciplines. Based on our review, we present future research directions that could be used to conduct further research.} }
@article{WOS:000792828100003, title = {Machine learning techniques for pavement condition evaluation}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {136}, year = {2022}, issn = {0926-5805}, doi = {10.1016/j.autcon.2022.104190}, author = {Sholevar, Nima and Golroo, Amir and Esfahani, Sahand Roghani}, abstract = {Pavement management systems play a significant role in country's economy since road authorities are concerned about preserving their priceless road assets for a longer time to save maintenance costs. An essential part of such systems is how to collect and analyze pavement condition data. This paper reviews the state-of-the-art techniques in pavement condition data evaluation using machine learning techniques, more specifically, the application of machine learning methods: image classification, object detection, and segmentation in pavement distress assessment is investigated. Furthermore, the pavement automated data collection tools and pavement condition indices have been reviewed from the lens of machine learning applications. The review concludes that the overall trends in pavement condition evaluation is to apply machine learning techniques although there are some limitations not only in detection of few pavement distresses with complicated patterns but also in indication of the severity and density of distresses leading to avenues for future research.} }
@article{WOS:001199751600001, title = {Theoretical Calculation Assisted by Machine Learning Accelerate Optimal Electrocatalyst Finding for Hydrogen Evolution Reaction}, journal = {CHEMELECTROCHEM}, volume = {11}, year = {2024}, issn = {2196-0216}, doi = {10.1002/celc.202400084}, author = {Zhang, Yuefei and Liu, Xuefei and Wang, Wentao}, abstract = {Electrocatalytic hydrogen evolution reaction (HER) is a promising strategy to solve and mitigate the coming energy shortage and global environmental pollution. Searching for efficient electrocatalysts for HER remains challenging through traditional trial-and-error methods from numerous potential material candidates. Theoretical high throughput calculation assisted by machine learning is a possible method to screen excellent HER electrocatalysts effectively. This will pave the way for high-efficiency and low-price electrocatalyst findings. In this review, we comprehensively introduce the machine learning workflow and standard models for hydrogen reduction reactions. This mainly illustrates how machine learning is used in catalyst filtration and descriptor exploration. Subsequently, several applications, including surface electrocatalysts, two-dimensional (2D) electrocatalysts, and single/dual atom electrocatalysts using machine learning in electrocatalytic HER, are highlighted and introduced. Finally, the corresponding challenge and perspective for machine learning in electrocatalytic hydrogen reduction reactions are concluded. We hope this critical review can provide a comprehensive understanding of machine learning for HER catalyst design and guide the future theoretical and experimental investigation of HER catalyst findings. In this review, we comprehensively introduce the machine learning workflow and standard models for hydrogen reduction reactions (HER). This mainly illustrates how machine learning is used in catalyst filtration and descriptor exploration. Several applications, including surface electrocatalysts, two-dimensional electrocatalysts, and single/dual atom electrocatalysts using machine learning in electrocatalytic HER, are highlighted and introduced. image} }
@article{WOS:000702351700085, title = {A deep learning approach for imbalanced crash data in predicting highway-rail grade crossings accidents}, journal = {RELIABILITY ENGINEERING \\& SYSTEM SAFETY}, volume = {216}, year = {2021}, issn = {0951-8320}, doi = {10.1016/j.ress.2021.108019}, author = {Gao, Lu and Lu, Pan and Ren, Yihao}, abstract = {Accurate accident prediction for highway-rail grade crossings (HRGCs) is critically important for assisting at-grade safety improvement decision making. Numerous machine-learning methods were developed focusing on predicting accidents and identifying contributing physical and operational characteristics. A more advanced deep learning-based model is explored as a more accurate means of predicting HRGC crashes compared to machine learning-based approaches. In particular, the prediction performance of the convolution neural network (CNN) model is compared to the most commonly used machine learning methods, such as decision tree (DT) and random forests (RF). A 19-year HRGCs data in North Dakota (ND) is used in this study. Training a machine learning model on an imbalanced data (e.g., unequal distribution of labeled data in accident and no-accident classes) introduce unique challenges for accurate prediction especially for minority class. In this paper, a resampling approach was used to address the imbalanced data issue. Various performance measurements are used to compare the models' prediction performance. The results indicate that resampling the imbalanced dataset significantly improves the recall rate. The results also show that the proposed deep learning-based approach which deepens the layer levels and adapts to the training dataset has better prediction performance compared to other machine learning-based methods.} }
@article{WOS:000436866400004, title = {People's Councils for Ethical Machine Learning}, journal = {SOCIAL MEDIA + SOCIETY}, volume = {4}, year = {2018}, issn = {2056-3051}, doi = {10.1177/2056305118768303}, author = {McQuillan, Dan}, abstract = {Machine learning is a form of knowledge production native to the era of big data. It is at the core of social media platforms and everyday interactions. It is also being rapidly adopted for research and discovery across academia, business, and government. This article will explores the way the affordances of machine learning itself, and the forms of social apparatus that it becomes a part of, will potentially erode ethics and draw us in to a drone-like perspective. Unconstrained machine learning enables and delimits our knowledge of the world in particular ways: the abstractions and operations of machine learning produce a ``view from above'' whose consequences for both ethics and legality parallel the dilemmas of drone warfare. The family of machine learning methods is not somehow inherently bad or dangerous, nor does implementing them signal any intent to cause harm. Nevertheless, the machine learning assemblage produces a targeting gaze whose algorithms obfuscate the legality of its judgments, and whose iterations threaten to create both specific injustices and broader states of exception. Given the urgent need to provide some kind of balance before machine learning becomes embedded everywhere, this article proposes people's councils as a way to contest machinic judgments and reassert openness and discourse.} }
@article{WOS:000654535500001, title = {Machine learning paradigm for structural health monitoring}, journal = {STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL}, volume = {20}, pages = {1353-1372}, year = {2021}, issn = {1475-9217}, doi = {10.1177/1475921720972416}, author = {Bao, Yuequan and Li, Hui}, abstract = {Structural health diagnosis and prognosis is the goal of structural health monitoring. Vibration-based structural health monitoring methodology has been extensively investigated. However, the conventional vibration-based methods find it difficult to detect damages of actual structures because of a high incompleteness in the monitoring information (the number of sensors is much fewer with respect to the number of degrees of freedom of a structure), intense uncertainties in the structural conditions and monitoring systems, and coupled effects of damage and environmental actions on modal parameters. It is a truth that the performance and conditions of a structure must be embedded in the monitoring data (vehicles, wind, etc.; acceleration, displacement, cable force, strain, images, videos, etc.). Therefore, there is a need to develop completely novel structural health diagnosis and prognosis methodology based on the various monitoring data. Machine learning provides the advanced mathematical frameworks and algorithms that can help discover and model the performance and conditions of a structure through deep mining of monitoring data. Thus, machine learning takes an opportunity to establish novel machine learning paradigm for structural health diagnosis and prognosis theory termed the machine learning paradigm for structural health monitoring. This article sheds light on principles for machine learning paradigm for structural health monitoring with some examples and reviews the existing challenges and open questions in this field.} }
@article{WOS:000700585500007, title = {An efficient parallel machine learning-based blockchain framework}, journal = {ICT EXPRESS}, volume = {7}, pages = {300-307}, year = {2021}, issn = {2405-9595}, doi = {10.1016/j.icte.2021.08.014}, author = {Tsai, Chun-Wei and Chen, Yi-Ping and Tang, Tzu-Chieh and Luo, Yu-Chen}, abstract = {The unlimited possibilities of machine learning have been shown in several successful reports and applications. However, how to make sure that the searched results of a machine learning system are not tampered by anyone and how to prevent the other users in the same network environment from easily getting our private data are two critical research issues when we immerse into powerful machine learning-based systems or applications. This situation is just like other modern information systems that confront security and privacy issues. The development of blockchain provides us an alternative way to address these two issues. That is why some recent studies have attempted to develop machine learning systems with blockchain technologies or to apply machine learning methods to blockchain systems. To show what the combination of blockchain and machine learning is capable of doing, in this paper, we proposed a parallel framework to find out suitable hyperparameters of deep learning in a blockchain environment by using a metaheuristic algorithm. The proposed framework also takes into account the issue of communication cost, by limiting the number of information exchanges between miners and blockchain. (C) 2021 The Korean Institute of Communications and Information Sciences (KICS). Publishing services by Elsevier B.V.} }
@article{WOS:001395051600001, title = {Network embedding: The bridge between water distribution network hydraulics and machine learning}, journal = {WATER RESEARCH}, volume = {273}, year = {2025}, issn = {0043-1354}, doi = {10.1016/j.watres.2024.123011}, author = {Zhou, Xiao and Guo, Shuyi and Xin, Kunlun and Tang, Zhenheng and Chu, Xiaowen and Fu, Guangtao}, abstract = {Machine learning has been increasingly used to solve management problems of water distribution networks (WDNs). A critical research gap, however, remains in the effective incorporation of WDN hydraulic characteristics in machine learning. Here we present a new water distribution network embedding (WDNE) method that transforms the hydraulic relationships of WDN topology into a vector form to be best suited for machine learning algorithms. The nodal relationships are characterized by local structure, global structure and attribute information. A conjoint use of two deep auto-encoder embedding models ensures that the hydraulic relationships and attribute information are simultaneously preserved and are effectively utilized by machine learning models. WDNE provides a new way to bridge WDN hydraulics with machine learning. It is first applied to a pipe burst localization problem. The results show that it can increase the performance of machine learning algorithms, and enable a lightweight machine learning algorithm to achieve better accuracy with less training data compared with a deep learning method reported in the literature. Then, applications in node grouping problems show that WDNE enables machine learning algorithms to make use of WDN hydraulic information, and integrates WDN structural relationships to achieve better grouping results. The results highlight the potential of WDNE to enhance WDN management by improving the efficiency of machine learning models and broadening the range of solvable problems. Codes are available at https://github.com/ZhouGroupHFUT/WDNE} }
@article{WOS:000929283300001, title = {An Overview of Machine Learning, Deep Learning, and Reinforcement Learning-Based Techniques in Quantitative Finance: Recent Progress and Challenges}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app13031956}, author = {Sahu, Santosh Kumar and Mokhade, Anil and Bokde, Neeraj Dhanraj}, abstract = {Forecasting the behavior of the stock market is a classic but difficult topic, one that has attracted the interest of both economists and computer scientists. Over the course of the last couple of decades, researchers have investigated linear models as well as models that are based on machine learning (ML), deep learning (DL), reinforcement learning (RL), and deep reinforcement learning (DRL) in order to create an accurate predictive model. Machine learning algorithms can now extract high-level financial market data patterns. Investors are using deep learning models to anticipate and evaluate stock and foreign exchange markets due to the advantage of artificial intelligence. Recent years have seen a proliferation of the deep reinforcement learning algorithm's application in algorithmic trading. DRL agents, which combine price prediction and trading signal production, have been used to construct several completely automated trading systems or strategies. Our objective is to enable interested researchers to stay current and easily imitate earlier findings. In this paper, we have worked to explain the utility of Machine Learning, Deep Learning, Reinforcement Learning, and Deep Reinforcement Learning in Quantitative Finance (QF) and the Stock Market. We also outline potential future study paths in this area based on the overview that was presented before.} }
@article{WOS:000866803100001, title = {Deep Learning and Machine Learning with Grid Search to Predict Later Occurrence of Breast Cancer Metastasis Using Clinical Data}, journal = {JOURNAL OF CLINICAL MEDICINE}, volume = {11}, year = {2022}, doi = {10.3390/jcm11195772}, author = {Jiang, Xia and Xu, Chuhan}, abstract = {Background: It is important to be able to predict, for each individual patient, the likelihood of later metastatic occurrence, because the prediction can guide treatment plans tailored to a specific patient to prevent metastasis and to help avoid under-treatment or over-treatment. Deep neural network (DNN) learning, commonly referred to as deep learning, has become popular due to its success in image detection and prediction, but questions such as whether deep learning outperforms other machine learning methods when using non-image clinical data remain unanswered. Grid search has been introduced to deep learning hyperparameter tuning for the purpose of improving its prediction performance, but the effect of grid search on other machine learning methods are under-studied. In this research, we take the empirical approach to study the performance of deep learning and other machine learning methods when using non-image clinical data to predict the occurrence of breast cancer metastasis (BCM) 5, 10, or 15 years after the initial treatment. We developed prediction models using the deep feedforward neural network (DFNN) methods, as well as models using nine other machine learning methods, including naive Bayes (NB), logistic regression (LR), support vector machine (SVM), LASSO, decision tree (DT), k-nearest neighbor (KNN), random forest (RF), AdaBoost (ADB), and XGBoost (XGB). We used grid search to tune hyperparameters for all methods. We then compared our feedforward deep learning models to the models trained using the nine other machine learning methods. Results: Based on the mean test AUC (Area under the ROC Curve) results, DFNN ranks 6th, 4th, and 3rd when predicting 5-year, 10-year, and 15-year BCM, respectively, out of 10 methods. The top performing methods in predicting 5-year BCM are XGB (1st), RF (2nd), and KNN (3rd). For predicting 10-year BCM, the top performers are XGB (1st), RF (2nd), and NB (3rd). Finally, for 15-year BCM, the top performers are SVM (1st), LR and LASSO (tied for 2nd), and DFNN (3rd). The ensemble methods RF and XGB outperform other methods when data are less balanced, while SVM, LR, LASSO, and DFNN outperform other methods when data are more balanced. Our statistical testing results show that at a significance level of 0.05, DFNN overall performs comparably to other machine learning methods when predicting 5-year, 10-year, and 15-year BCM. Conclusions: Our results show that deep learning with grid search overall performs at least as well as other machine learning methods when using non-image clinical data. It is interesting to note that some of the other machine learning methods, such as XGB, RF, and SVM, are very strong competitors of DFNN when incorporating grid search. It is also worth noting that the computation time required to do grid search with DFNN is much more than that required to do grid search with the other nine machine learning methods.} }
@article{WOS:000805800300001, title = {Novel Graph-Based Machine Learning Technique to Secure Smart Vehicles in Intelligent Transportation Systems}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {24}, pages = {8483-8491}, year = {2023}, issn = {1524-9050}, doi = {10.1109/TITS.2022.3174333}, author = {Gupta, Brij Bhooshan and Gaurav, Akshat and Marin, Enrique Cano and Alhalabi, Wadee}, abstract = {Intelligent Transport Systems (ITS) is a developing technology that will significantly alter the driving experience. In such systems, smart vehicles and Road-Side Units (RSUs) communicate through the VANET. Safety apps use these data to identify and prevent hazardous situations in real-time. Detection of malicious nodes and attack traffic in Intelligent Transportation Systems (ITS) is a current research subject. Recently, researchers are proposing graph-based machine learning techniques to identify malicious users in the ITS environment, through which it is easy to analyze the network traffic and detect the malicious devices. Therefore, graph-based machine learning techniques could be a technique that efficiently detect malicious nodes in the ITS environment. In this context, this article aims to provide a technique for resolving authentication and security issues in ITS using lightweight cryptography and graph-based machine learning. Our solution uses the concepts of identity based authentication technique and graph-based machine learning in order to provide authentication and security to the smart vehicle in ITS. By authenticating smart vehicles in ITS and identifying various cyber threats, our proposed method substantially contributes to the development of intelligent transportation communication environment.} }
@article{WOS:000803735500004, title = {Embedding metric learning into an extreme learning machine for scene recognition}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {203}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.117505}, author = {Wang, Chen and Peng, Guohua and De Baets, Bernard}, abstract = {Metric learning can be very useful to improve the performance of a distance-dependent classifier. However, separating metric learning from the classifier learning possibly degenerates the performance, for instance in scene recognition, especially for some complicated scene images. To address this issue, we propose to embed metric learning into an extreme learning machine (EML-ELM) to tackle scene recognition. Specifically, metric learning is conducted to fully explore discriminative features by taking into account all label information, rendering samples of the same class more compact and those of different classes more separable. An extreme learning machine is employed as a classifier thanks to its effectiveness and fast learning process. By explicitly embedding metric learning into an extreme learning machine, we can jointly learn the discriminative features and an effective classifier in a unified framework, thereby improving the recognition performance for complicated scene images. Extensive experiments on four benchmark scene datasets demonstrate the competitive performance of EML-ELM in comparison with state-of-the-art methods.} }
@article{WOS:000470880900001, title = {Machine Learning Topological Phases with a Solid-State Quantum Simulator}, journal = {PHYSICAL REVIEW LETTERS}, volume = {122}, year = {2019}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.122.210503}, author = {Lian, Wenqian and Wang, Sheng-Tao and Lu, Sirui and Huang, Yuanyuan and Wang, Fei and Yuan, Xinxing and Zhang, Wengang and Ouyang, Xiaolong and Wang, Xin and Huang, Xianzhi and He, Li and Chang, Xiuying and Deng, Dong-Ling and Duan, Luming}, abstract = {We report an experimental demonstration of a machine learning approach to identify exotic topological phases, with a focus on the three-dimensional chiral topological insulators. We show that the convolutional neural networks-a class of deep feed-forward artificial neural networks with widespread applications in machine learning-can be trained to successfully identify different topological phases protected by chiral symmetry from experimental raw data generated with a solid-state quantum simulator. Our results explicitly showcase the exceptional power of machine learning in the experimental detection of topological phases, which paves a way to study rich topological phenomena with the machine learning toolbox.} }
@article{WOS:000639862400012, title = {Optimized Extreme Learning Machine for Intelligent Spectrum Sensing in 5G systems}, journal = {JOURNAL OF COMMUNICATIONS TECHNOLOGY AND ELECTRONICS}, volume = {66}, pages = {322-332}, year = {2021}, issn = {1064-2269}, doi = {10.1134/S1064226921040045}, author = {Kansal, P. and Kumar, A. and Gangadharappa, M.}, abstract = {A two-level learned distributed networking (LDN) structure that uses existing machine learning (ML) algorithms and the novel Optimized Extreme Learning Machine (OELM) algorithm to perform intelligent spectrum sensing for 5G systems has been proposed and implemented. This novel technique uses input vectors like received signal strength indicator, the distance between Cognitive Radio users and gateways, and energy vectors to train the model. Extreme Learning Machine optimized by BAT algorithm outperforms the existing Machine Learning techniques in terms of detection accuracy, false alarm, detection probability and cross validation curves at different SNR scenarios.} }
@article{WOS:000564994700001, title = {Soul and machine (learning)}, journal = {MARKETING LETTERS}, volume = {31}, pages = {393-404}, year = {2020}, issn = {0923-0645}, doi = {10.1007/s11002-020-09538-4}, author = {Proserpio, Davide and Hauser, John R. and Liu, Xiao and Amano, Tomomichi and Burnap, Alex and Guo, Tong and Lee, Dokyun (DK) and Lewis, Randall and Misra, Kanishka and Schwarz, Eric and Timoshenko, Artem and Xu, Lilei and Yoganarasimhan, Hema}, abstract = {Machine learning is bringing us self-driving cars, medical diagnoses, and language translation, but how can machine learning help marketers improve marketing decisions? Machine learning models predict extremely well, are scalable to ``big data,'' and are a natural fit to analyze rich media content, such as text, images, audio, and video. Examples of current marketing applications include identification of customer needs from online data, accurate prediction of consumer response to advertising, personalized pricing, and product recommendations. But without the human input and insight-the soul-the applications of machine learning are limited. To create competitive or cooperative strategies, to generate creative product designs, to be accurate for ``what-if'' and ``but-for'' applications, to devise dynamic policies, to advance knowledge, to protect consumer privacy, and avoid algorithm bias, machine learning needs a soul. The brightest future is based on the synergy of what the machine can do well and what humans do well. We provide examples and predictions for the future.} }
@article{WOS:000651052400024, title = {Machine learning for molecular thermodynamics}, journal = {CHINESE JOURNAL OF CHEMICAL ENGINEERING}, volume = {31}, pages = {227-239}, year = {2021}, issn = {1004-9541}, doi = {10.1016/j.cjche.2020.10.044}, author = {Ding, Jiaqi and Xu, Nan and Manh Tien Nguyen and Qiao, Qi and Shi, Yao and He, Yi and Shao, Qing}, abstract = {Thermodynamic properties of complex systems play an essential role in developing chemical engineering processes. It remains a challenge to predict the thermodynamic properties of complex systems in a wide range and describe the behavior of ions and molecules in complex systems. Machine learning emerges as a powerful tool to resolve this issue because it can describe complex relationships beyond the capacity of traditional mathematical functions. This minireview will summarize some fundamental concepts of machine learning methods and their applications in three aspects of the molecular thermodynamics using several examples. The first aspect is to apply machine learning methods to predict the thermodynamic properties of a broad spectrum of systems based on known data. The second aspect is to integer machine learning and molecular simulations to accelerate the discovery of materials. The third aspect is to develop machine learning force field that can eliminate the barrier between quantum mechanics and all-atom molecular dynamics simulations. The applications in these three aspects illustrate the potential of machine learning in molecular thermodynamics of chemical engineering. We will also discuss the perspective of the broad applications of machine learning in chemical engineering. (C) 2021 The Chemical Industry and Engineering Society of China, and Chemical Industry Press Co., Ltd. All rights reserved.} }
@article{WOS:001096766000001, title = {Digital geotechnics: from data-driven site characterisation towards digital transformation and intelligence in geotechnical engineering}, journal = {GEORISK-ASSESSMENT AND MANAGEMENT OF RISK FOR ENGINEERED SYSTEMS AND GEOHAZARDS}, volume = {18}, pages = {8-32}, year = {2024}, issn = {1749-9518}, doi = {10.1080/17499518.2023.2278136}, author = {Wang, Yu and Tian, Hua-Ming}, abstract = {Geotechnical engineering is experiencing a paradigm shift towards digital transformation and intelligence, driven by Industry 4.0 and emerging digital technologies, such as machine learning. However, development and application of machine learning are relatively slow in geotechnical practice, because extensive training databases are a key to the success of machine learning, but geotechnical data are often small and ugly, leading to the difficulty in developing a suitable training database required for machine learning. In addition, convincing examples from real projects are rare that demonstrate the immediate added value of machine learning to geotechnical practices. To facilitate digital transformation and machine learning in geotechnical engineering, this study proposes to develop a project-specific training database that leverages on digital transformation of geotechnical workflow and reflects both project-specific data collected from various stages of the geotechnical workflow and domain knowledge in geotechnical practices, such as soil mechanics, numerical analysis principles, and prior engineering experience and judgment. A real ground improvement project is presented to illustrate the proposed method and demonstrate the added value of digital transformation and machine learning in geotechnical practices.} }
@article{WOS:001003548300006, title = {Machine learning and deep learning techniques for breast cancer diagnosis and classification: a comprehensive review of medical imaging studies}, journal = {JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY}, volume = {149}, pages = {10473-10491}, year = {2023}, issn = {0171-5216}, doi = {10.1007/s00432-023-04956-z}, author = {Radak, Mehran and Lafta, Haider Yabr and Fallahi, Hossein}, abstract = {BackgroundBreast cancer is a major public health concern, and early diagnosis and classification are critical for effective treatment. Machine learning and deep learning techniques have shown great promise in the classification and diagnosis of breast cancer.PurposeIn this review, we examine studies that have used these techniques for breast cancer classification and diagnosis, focusing on five groups of medical images: mammography, ultrasound, MRI, histology, and thermography. We discuss the use of five popular machine learning techniques, including Nearest Neighbor, SVM, Naive Bayesian Network, DT, and ANN, as well as deep learning architectures and convolutional neural networks.ConclusionOur review finds that machine learning and deep learning techniques have achieved high accuracy rates in breast cancer classification and diagnosis across various medical imaging modalities. Furthermore, these techniques have the potential to improve clinical decision-making and ultimately lead to better patient outcomes.} }
@article{WOS:000794708000001, title = {Rainfall Prediction System Using Machine Learning Fusion for Smart Cities}, journal = {SENSORS}, volume = {22}, year = {2022}, doi = {10.3390/s22093504}, author = {Rahman, Atta-ur and Abbas, Sagheer and Gollapalli, Mohammed and Ahmed, Rashad and Aftab, Shabib and Ahmad, Munir and Khan, Muhammad Adnan and Mosavi, Amir}, abstract = {Precipitation in any form-such as rain, snow, and hail-can affect day-to-day outdoor activities. Rainfall prediction is one of the challenging tasks in weather forecasting process. Accurate rainfall prediction is now more difficult than before due to the extreme climate variations. Machine learning techniques can predict rainfall by extracting hidden patterns from historical weather data. Selection of an appropriate classification technique for prediction is a difficult job. This research proposes a novel real-time rainfall prediction system for smart cities using a machine learning fusion technique. The proposed framework uses four widely used supervised machine learning techniques, i.e., decision tree, Naive Bayes, K-nearest neighbors, and support vector machines. For effective prediction of rainfall, the technique of fuzzy logic is incorporated in the framework to integrate the predictive accuracies of the machine learning techniques, also known as fusion. For prediction, 12 years of historical weather data (2005 to 2017) for the city of Lahore is considered. Pre-processing tasks such as cleaning and normalization were performed on the dataset before the classification process. The results reflect that the proposed machine learning fusion-based framework outperforms other models.} }
@article{WOS:000615376800001, title = {Machine Learning Control Based on Approximation of Optimal Trajectories}, journal = {MATHEMATICS}, volume = {9}, year = {2021}, doi = {10.3390/math9030265}, author = {Diveev, Askhat and Konstantinov, Sergey and Shmalko, Elizaveta and Dong, Ge}, abstract = {The paper is devoted to an emerging trend in control-a machine learning control. Despite the popularity of the idea of machine learning, there are various interpretations of this concept, and there is an urgent need for its strict mathematical formalization. An attempt to formalize the concept of machine learning is presented in this paper. The concepts of an unknown function, work area, training set are introduced, and a mathematical formulation of the machine learning problem is presented. Based on the presented formulation, the concept of machine learning control is considered. One of the problems of machine learning control is the general synthesis of control. It implies finding a control function that depends on the state of the object, which ensures the achievement of the control goal with the optimal value of the quality criterion from any initial state of some admissible region. Supervised and unsupervised approaches to solving a problem based on symbolic regression methods are considered. As a computational example, a problem of general synthesis of optimal control for a spacecraft landing on the surface of the Moon is considered as supervised machine learning control with a training set.} }
@article{WOS:000995071700009, title = {Green-IN Machine Learning at a Glance}, journal = {COMPUTER}, volume = {56}, pages = {35-43}, year = {2023}, issn = {0018-9162}, doi = {10.1109/MC.2023.3254646}, author = {Gutierrez, Maria and Moraga, Ma Angeles and Garcia, Felix and Calero, Coral}, abstract = {The use of machine learning (ML) algorithms has an environmental impact to be fully considered. This article presents a green-in-driven approach to the development of ML models. The aim thereof is to meet operational requirements while ensuring a suitable tradeoff between performance/reliability and energy consumption.} }
@article{WOS:001139775100136, title = {MLpronto: A tool for democratizing machine learning}, journal = {PLOS ONE}, volume = {18}, year = {2023}, doi = {10.1371/journal.pone.0294924}, author = {Tjaden, Jacob and Tjaden, Brian}, abstract = {The democratization of machine learning is a popular and growing movement. In a world with a wealth of publicly available data, it is important that algorithms for analysis of data are accessible and usable by everyone. We present MLpronto, a system for machine learning analysis that is designed to be easy to use so as to facilitate engagement with machine learning algorithms. With its web interface, MLpronto requires no computer programming or machine learning background, and it normally returns results in a matter of seconds. As input, MLpronto takes a file of data to be analyzed. MLpronto then executes some of the more commonly used supervised machine learning algorithms on the data and reports the results of the analyses. As part of its execution, MLpronto generates computer programming code corresponding to its machine learning analysis, which it also supplies as output. Thus, MLpronto can be used as a no-code solution for citizen data scientists with no machine learning or programming background, as an educational tool for those learning about machine learning, and as a first step for those who prefer to engage with programming code in order to facilitate rapid development of machine learning projects. MLpronto is freely available for use at https://mlpronto.org/.} }
@article{WOS:000947665000001, title = {Machine Learning Adoption in Educational Institutions: Role of Internet of Things and Digital Educational Platforms}, journal = {SUSTAINABILITY}, volume = {15}, year = {2023}, doi = {10.3390/su15054000}, author = {Li, Jiuxiang and Wang, Rufeng}, abstract = {The ever-increasing development of information technologies has led to the adoption of advanced learning techniques. In this regard, e-learning and machine learning are two of the emerging instructional means for educational institutes. The current study investigates the role of the Internet of Things (IoT) and digital educational platforms (DEPs) in the adoption of machine learning. The present research additionally investigated the function of DEPs as mediators between IoT and machine learning adoption. The department chairs or heads of 310 departments at 91 Chinese institutions provided the information. In order to analyze the data, we used SPSS 25.0 and SEM (structural equation modeling). The results demonstrated how crucial an impact IoT has on DEPs and the uptake of machine learning. DEPs directly affect machine learning adoption and also act as mediators. The findings also support the mediating role of DEPs in the IoT and machine learning adoption link. The current study contributes to both theory and practical management by examining how IoT is helpful for achieving machine learning adoption. Based on the responses of 91 educational departments, this is a unique study of the mechanisms to achieve machine learning adoption through IoT and DEPs.} }
@article{WOS:000756609900013, title = {Using Machine Learning for measuring democracy: A practitioners guide and a new updated dataset for 186 countries from 1919 to 2019}, journal = {EUROPEAN JOURNAL OF POLITICAL ECONOMY}, volume = {70}, year = {2021}, issn = {0176-2680}, doi = {10.1016/j.ejpoleco.2021.102047}, author = {Grundler, Klaus and Krieger, Tommy}, abstract = {We provide a comprehensive overview of the literature on the measurement of democracy and present an extensive update of the Machine Learning indicator of Grundler and Krieger (2016). Four improvements are particularly notable: First, we produce a continuous and a dichotomous version of the Machine Learning democracy indicator. Second, we calculate intervals that reflect the degree of measurement uncertainty. Third, we refine the conceptualization of the Machine Learning Index. Finally, we significantly expand the data coverage by providing democracy indices for 186 countries in the period from 1919 to 2019.} }
@article{WOS:001230482400001, title = {Accommodating Machine Learning Algorithms in Professional Service Firms}, journal = {ORGANIZATION STUDIES}, volume = {45}, year = {2024}, issn = {0170-8406}, doi = {10.1177/01708406241252930}, author = {Faulconbridge, James R. and Sarwar, Atif and Spring, Martin}, abstract = {Machine learning algorithms, as one form of artificial intelligence, are significant for professional work because they create the possibility for some predictions, interpretations and judgements that inform decision-making to be made by algorithms. However, little is known about whether it is possible to transform professional work to incorporate machine learning while also addressing negative responses from professionals whose work is changed by inscrutable algorithms. Through original empirical analysis of the effects of machine learning algorithms on the work of accountants and lawyers, this paper identifies the role of accommodating machine learning algorithms in professional service firms. Accommodating machine learning algorithms involves strategic responses that both justify adoption in the context of the possibilities and new contributions of machine learning algorithms and respond to the algorithms' limitations and opaque and inscrutable nature. The analysis advances understanding of the processes that enable or inhibit the cooperative adoption of artificial intelligence in professional service firms and develops insights relevant when examining the long-term impacts of machine learning algorithms as they become ever more sophisticated.} }
@article{WOS:000573712500001, title = {A translucent box: interpretable machine learning in ecology}, journal = {ECOLOGICAL MONOGRAPHS}, volume = {90}, year = {2020}, issn = {0012-9615}, doi = {10.1002/ecm.1422}, author = {Lucas, Tim C. D.}, abstract = {Machine learning has become popular in ecology but its use has remained restricted to predicting, rather than understanding, the natural world. Many researchers consider machine learning algorithms to be a black box. These models can, however, with careful examination, be used to inform our understanding of the world. They are translucent boxes. Furthermore, the interpretation of these models can be an important step in building confidence in a model or in a specific prediction from a model. Here I review a number of techniques for interpreting machine learning models at the level of the system, the variable, and the individual prediction as well as methods for handling non-independent data. I also discuss the limits of interpretability for different methods and demonstrate these approaches using a case example of understanding litter sizes in mammals.} }
@article{WOS:000955359500001, title = {Artificial intelligence and machine learning overview in pathology \\& laboratory medicine: A general review of data preprocessing and basic supervised concepts}, journal = {SEMINARS IN DIAGNOSTIC PATHOLOGY}, volume = {40}, pages = {71-87}, year = {2023}, issn = {0740-2570}, doi = {10.1053/j.semdp.2023.02.002}, author = {Albahra, Samer and Gorbett, Tom and Robertson, Scott and D'Aleo, Giana and Ockunzzi, Samuel and Lallo, Daniel and Hu, Bo and Rashidi, Hooman H.}, abstract = {Machine learning (ML) is becoming an integral aspect of several domains in medicine. Yet, most pathologists and laboratory professionals remain unfamiliar with such tools and are unprepared for their inevitable integration. To bridge this knowledge gap, we present an overview of key elements within this emerging data science discipline. First, we will cover general, well-established concepts within ML, such as data type concepts, data preprocessing methods, and ML study design. We will describe common supervised and unsupervised learning algorithms and their associated common machine learning terms (provided within a comprehensive glossary of terms that are discussed within this review). Overall, this review will offer a broad overview of the key concepts and algorithms in machine learning, with a focus on pathology and laboratory medicine. The objective is to provide an updated useful reference for those new to this field or those who require a refresher.} }
@article{WOS:000877748800001, title = {The Role of Machine Learning in Tribology: A Systematic Review}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {30}, pages = {1345-1397}, year = {2023}, issn = {1134-3060}, doi = {10.1007/s11831-022-09841-5}, author = {Paturi, Uma Maheshwera Reddy and Palakurthy, Sai Teja and Reddy, N. S.}, abstract = {The machine learning (ML) approach, motivated by artificial intelligence (AI), is an inspiring mathematical algorithm that accurately simulates many engineering processes. Machine learning algorithms solve nonlinear and complex relationships through data training; additionally, they can infer previously unknown relationships, allowing for a simplified model and estimation of hidden data. Unlike other statistical tools, machine learning does not impose process parameter restrictions and yields an accurate association between input and output parameters. Tribology is a branch of surface science concerned with studying and managing friction, lubrication, and wear on relatively interacting surfaces. While AI-based machine learning approaches have been adopted in tribology applications, modern tribo-contact simulation requires a deliberate decomposition of complex design challenges into simpler sub-threads, thereby identifying the relationships between the numerous interconnected features and processes. Numerous studies have established that artificial intelligence techniques can accurately model tribological processes and their properties based on various process parameters. The primary objective of this review is to conduct a thorough examination of the role of machine learning in tribological research and pave the way for future researchers by providing a specific research direction. In terms of future research directions and developments, the expanded application of artificial intelligence and various machine learning methods in tribology has been emphasized, including the characterization and design of complex tribological systems. Additionally, by combining machine learning methods with tribological experimental data, interdisciplinary research can be conducted to understand efficient resource utilization and resource conservation better. At the conclusion of this article, a detailed discussion of the limitations and future research opportunities associated with implementing various machine learning algorithms in tribology and its interdisciplinary fields is presented.} }
@article{WOS:000737752600001, title = {Intelligent on-demand design of phononic metamaterials}, journal = {NANOPHOTONICS}, volume = {11}, pages = {439-460}, year = {2022}, issn = {2192-8606}, doi = {10.1515/nanoph-2021-0639}, author = {Jin, Yabin and He, Liangshu and Wen, Zhihui and Mortazavi, Bohayra and Guo, Hongwei and Torrent, Daniel and Djafari-Rouhani, Bahram and Rabczuk, Timon and Zhuang, Xiaoying and Li, Yan}, abstract = {With the growing interest in the field of artificial materials, more advanced and sophisticated functionalities are required from phononic crystals and acoustic metamaterials. This implies a high computational effort and cost, and still the efficiency of the designs may be not sufficient. With the help of third-wave artificial intelligence technologies, the design schemes of these materials are undergoing a new revolution. As an important branch of artificial intelligence, machine learning paves the way to new technological innovations by stimulating the exploration of structural design. Machine learning provides a powerful means of achieving an efficient and accurate design process by exploring nonlinear physical patterns in high-dimensional space, based on data sets of candidate structures. Many advanced machine learning algorithms, such as deep neural networks, unsupervised manifold clustering, reinforcement learning and so forth, have been widely and deeply investigated for structural design. In this review, we summarize the recent works on the combination of phononic metamaterials and machine learning. We provide an overview of machine learning on structural design. Then discuss machine learning driven on-demand design of phononic metamaterials for acoustic and elastic waves functions, topological phases and atomic-scale phonon properties. Finally, we summarize the current state of the art and provide a prospective of the future development directions.} }
@article{WOS:001088970300001, title = {Assessing rainfall prediction models: Exploring the advantages of machine learning and remote sensing approaches}, journal = {ALEXANDRIA ENGINEERING JOURNAL}, volume = {82}, pages = {16-25}, year = {2023}, issn = {1110-0168}, doi = {10.1016/j.aej.2023.09.060}, author = {Latif, Sarmad Dashti and Hazrin, Nur Alyaa Binti and Koo, Chai Hoon and Ng, Jing Lin and Chaplot, Barkha and Huang, Yuk Feng and El-Shafie, Ahmed and Ahmed, Ali Najah}, abstract = {Using a comparison of three different major types, the best predictive model was determined. Statistical models and machine learning algorithms automatically learn and improve based on data. Deep learning uses neural networks to learn complex data patterns and relationships. A combination of satellite imagery, radar data, and ground-based observations are used and using aircraft or satellites, and remote sensing (RS) collects data on distant objects or locations. Satellites and radar are used to gather regional precipitation data for hybrid models. An algorithm trained on historical rainfall measurements would then process the data. Using remote monitoring instrument input features, the machine-learning model can predict precipitation. Evaluation of machine learning regression methods is based on the degree of agreement between predicted and observed values. The RMSE, R2, and MAE statistical measures check on the precision of a prediction or forecasting model. Machine learning excels at rainfall prediction regardless of climate or timescale. As one of the more popular models for predicting rainfall, the LSTM models demonstrate their superiority. Remote sensing and hybrid predictive models should be investigated further due to their scarcity.} }
@article{WOS:000695342100003, title = {Federated Learning: A Distributed Shared Machine Learning Method}, journal = {COMPLEXITY}, volume = {2021}, year = {2021}, issn = {1076-2787}, doi = {10.1155/2021/8261663}, author = {Hu, Kai and Li, Yaogen and Xia, Min and Wu, Jiasheng and Lu, Meixia and Zhang, Shuai and Weng, Liguo}, abstract = {Federated learning (FL) is a distributed machine learning (ML) framework. In FL, multiple clients collaborate to solve traditional distributed ML problems under the coordination of the central server without sharing their local private data with others. This paper mainly sorts out FLs based on machine learning and deep learning. First of all, this paper introduces the development process, definition, architecture, and classification of FL and explains the concept of FL by comparing it with traditional distributed learning. Then, it describes typical problems of FL that need to be solved. On the basis of classical FL algorithms, several federated machine learning algorithms are briefly introduced, with emphasis on deep learning and classification and comparisons of those algorithms are carried out. Finally, this paper discusses possible future developments of FL based on deep learning.} }
@article{WOS:000509281100001, title = {Machine Learning in Psychometrics and Psychological Research}, journal = {FRONTIERS IN PSYCHOLOGY}, volume = {10}, year = {2020}, issn = {1664-1078}, doi = {10.3389/fpsyg.2019.02970}, author = {Orru, Graziella and Monaro, Merylin and Conversano, Ciro and Gemignani, Angelo and Sartori, Giuseppe}, abstract = {Recent controversies about the level of replicability of behavioral research analyzed using statistical inference have cast interest in developing more efficient techniques for analyzing the results of psychological experiments. Here we claim that complementing the analytical workflow of psychological experiments with Machine Learning-based analysis will both maximize accuracy and minimize replicability issues. As compared to statistical inference, ML analysis of experimental data is model agnostic and primarily focused on prediction rather than inference. We also highlight some potential pitfalls resulting from adoption of Machine Learning based experiment analysis. If not properly used it can lead to over-optimistic accuracy estimates similarly observed using statistical inference. Remedies to such pitfalls are also presented such and building model based on cross validation and the use of ensemble models. ML models are typically regarded as black boxes and we will discuss strategies aimed at rendering more transparent the predictions.} }
@article{WOS:000423046300023, title = {Information discriminative extreme learning machine}, journal = {SOFT COMPUTING}, volume = {22}, pages = {677-689}, year = {2018}, issn = {1432-7643}, doi = {10.1007/s00500-016-2372-y}, author = {Yan, Deqin and Chu, Yonghe and Zhang, Haiying and Liu, Deshan}, abstract = {Extreme learning machine (ELM) has become one of the new research hotspots in the field of pattern recognition and machine learning. However, the existing extreme learning machine algorithms cannot better use identification information of data. Aiming at solving this problem, we propose a regularized extreme learning machine (algorithm) based on discriminative information (called IELM). In order to evaluate and verify the effectiveness of the proposed method, experiments use widely used image data sets. The comparative experimental results show that the proposed algorithm in the paper can significantly improve the classification performance and generalization ability of ELM.} }
@article{WOS:000737368700001, title = {Assessing Surface Water Flood Risks in Urban Areas Using Machine Learning}, journal = {WATER}, volume = {13}, year = {2021}, doi = {10.3390/w13243520}, author = {Li, Zhufeng and Liu, Haixing and Luo, Chunbo and Fu, Guangtao}, abstract = {Urban flooding is a devastating natural hazard for cities around the world. Flood risk mapping is a key tool in flood management. However, it is computationally expensive to produce flood risk maps using hydrodynamic models. To this end, this paper investigates the use of machine learning for the assessment of surface water flood risks in urban areas. The factors that are considered in machine learning models include coordinates, elevation, slope gradient, imperviousness, land use, land cover, soil type, substrate, distance to river, distance to road, and normalized difference vegetation index. The machine learning models are tested using the case study of Exeter, UK. The performance of machine learning algorithms, including naive Bayes, perceptron, artificial neural networks (ANNs), and convolutional neural networks (CNNs), is compared based on a spectrum of indicators, e.g., accuracy, F-beta score, and receiver operating characteristic curve. The results obtained from the case study show that the flood risk maps can be accurately generated by the machine learning models. The performance of models on the 30-year flood event is better than 100-year and 1000-year flood events. The CNNs and ANNs outperform the other machine learning algorithms tested. This study shows that machine learning can help provide rapid flood mapping, and contribute to urban flood risk assessment and management.} }
@article{WOS:001101098500001, title = {Machine learning in physics: A short guide}, journal = {EPL}, volume = {144}, year = {2023}, issn = {0295-5075}, doi = {10.1209/0295-5075/ad0575}, author = {Rodrigues, Francisco A.}, abstract = {Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives. Copyright (c) 2023 EPLA} }
@article{WOS:000838722000001, title = {Oil futures volatility predictability: New evidence based on machine learning models}, journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS}, volume = {83}, year = {2022}, issn = {1057-5219}, doi = {10.1016/j.irfa.2022.102299}, author = {Lu, Xinjie and Ma, Feng and Xu, Jin and Zhang, Zehui}, abstract = {This paper comprehensively examines the connection between oil futures volatility and the financial market based on a model-rich environment, which contains traditional predicting models, machine learning models, and combination models. The results highlight the efficiency of machine learning models for oil futures volatility forecasting, particularly the ensemble models and neural network models. Most interestingly, we consider the ``forecast combination puzzle'' in machine learning models, and find that combination models continue to have more satisfactory performances in all types of situations. We also discuss the model interpretability and each indicator's contribution to the prediction. Our paper provides new insights for machine learning methods' applications in futures market volatility prediction, which is helpful for academics, policy-makers, and investors.} }
@article{WOS:001166813800001, title = {Advances in Machine Learning Processing of Big Data from Disease Diagnosis Sensors}, journal = {ACS SENSORS}, volume = {9}, pages = {1134-1148}, year = {2024}, issn = {2379-3694}, doi = {10.1021/acssensors.3c02670}, author = {Lu, Shasha and Yang, Jianyu and Gu, Yu and He, Dongyuan and Wu, Haocheng and Sun, Wei and Xu, Dong and Li, Changming and Guo, Chunxian}, abstract = {Exploring accurate, noninvasive, and inexpensive disease diagnostic sensors is a critical task in the fields of chemistry, biology, and medicine. The complexity of biological systems and the explosive growth of biomarker data have driven machine learning to become a powerful tool for mining and processing big data from disease diagnosis sensors. With the development of bioinformatics and artificial intelligence (AI), machine learning models formed by data mining have been able to guide more sensitive and accurate molecular computing. This review presents an overview of big data collection approaches and fundamental machine learning algorithms and discusses recent advances in machine learning and molecular computational disease diagnostic sensors. More specifically, we highlight existing modular workflows and key opportunities and challenges for machine learning to achieve disease diagnosis through big data mining.} }
@article{WOS:000911819300007, title = {Neurology education in the era of artificial intelligence}, journal = {CURRENT OPINION IN NEUROLOGY}, volume = {36}, pages = {51-58}, year = {2023}, issn = {1350-7540}, doi = {10.1097/WCO.0000000000001130}, author = {Kedar, Sachin and Khazanchi, Deepak}, abstract = {Purpose of reviewThe practice of neurology is undergoing a paradigm shift because of advances in the field of data science, artificial intelligence, and machine learning. To ensure a smooth transition, physicians must have the knowledge and competence to apply these technologies in clinical practice. In this review, we describe physician perception and preparedness, as well as current state for clinical applications of artificial intelligence and machine learning in neurology.Recent findingsDigital health including artificial intelligence-based/machine learning-based technology has made significant inroads into various aspects of healthcare including neurological care. Surveys of physicians and healthcare stakeholders suggests an overall positive perception about the benefits of artificial intelligence/machine learning in clinical practice. This positive perception is tempered by concerns for lack of knowledge and limited opportunities to build competence in artificial intelligence/machine learning technology. Literature about neurologist's perception and preparedness towards artificial intelligence/machine learning-based technology is scant. There are very few opportunities for physicians particularly neurologists to learn about artificial intelligence/machine learning-based technology.Neurologists have not been surveyed about their perception and preparedness to adopt artificial intelligence/machine learning-based technology in clinical practice. We propose development of a practical artificial intelligence/machine learning curriculum to enhance neurologists' competence in these newer technologies.} }
@article{WOS:000498861800006, title = {Machine learning approaches for wind speed forecasting using long-term monitoring data: a comparative study}, journal = {SMART STRUCTURES AND SYSTEMS}, volume = {24}, pages = {733-744}, year = {2019}, issn = {1738-1584}, doi = {10.12989/sss.2019.24.6.733}, author = {Ye, X. W. and Ding, Y. and Wan, H. P.}, abstract = {Wind speed forecasting is critical for a variety of engineering tasks, such as wind energy harvesting, scheduling of a wind power system, and dynamic control of structures (e.g., wind turbine, bridge, and building). Wind speed, which has characteristics of random, nonlinear and uncertainty, is difficult to forecast. Nowadays, machine learning approaches (generalized regression neural network (GRNN), back propagation neural network (BPNN), and extreme learning machine (ELM)) are widely used for wind speed forecasting. In this study, two schemes are proposed to improve the forecasting performance of machine learning approaches. One is that optimization algorithms, i.e., cross validation (CV), genetic algorithm (GA), and particle swarm optimization (P SO), are used to automatically find the optimal model parameters. The other is that the combination of different machine learning methods is proposed by finite mixture (FM) method. Specifically, CV-GRNN, GA-BPNN, PSO-ELM belong to optimization algorithm-assisted machine learning approaches, and FM is a hybrid machine learning approach consisting of GRNN, BPNN, and ELM. The effectiveness of these machine learning methods in wind speed forecasting are fully investigated by one-year field monitoring data, and their performance is comprehensively compared.} }
@article{WOS:000446413400006, title = {Comparing Multiple Machine Learning Algorithms and Metrics for Estrogen Receptor Binding Prediction}, journal = {MOLECULAR PHARMACEUTICS}, volume = {15}, pages = {4361-4370}, year = {2018}, issn = {1543-8384}, doi = {10.1021/acs.molpharmaceut.8b00546}, author = {Russo, Daniel P. and Zorn, Kimberley M. and Clark, Alex M. and Zhu, Hao and Ekins, Sean}, abstract = {Many chemicals that disrupt endocrine function have been linked to a variety of adverse biological outcomes. However, screening for endocrine disruption using in vitro or in vivo approaches is costly and time-consuming. Computational methods, e.g., quantitative structure activity relationship models, have become more reliable due to bigger training sets, increased computing power, and advanced machine learning algorithms, such as multilayered artificial neural networks. Machine learning models can be used to predict compounds for endocrine disrupting capabilities, such as binding to the estrogen receptor (ER), and allow for prioritization and further testing. In this work, an exhaustive comparison of multiple machine learning algorithms, chemical spaces, and evaluation metrics for ER binding was performed on public data sets curated using in-house cheminformatics software (Assay Central). Chemical features utilized in modeling consisted of binary fingerprints (ECFP6, FCFP6, ToxPrint, or MACCS keys) and continuous molecular descriptors from RDKit. Each feature set was subjected to classic machine learning algorithms (Bernoulli Naive Bayes, AdaBoost Decision Tree, Random Forest, Support Vector Machine) and Deep Neural Networks (DNN). Models were evaluated using a variety of metrics: recall, precision, F1-score, accuracy, area under the receiver operating characteristic curve, Cohen's Kappa, and Matthews correlation coefficient. For predicting compounds within the training set, DNN has an accuracy higher than that of other methods; however, in 5-fold cross validation and external test set predictions, DNN and most classic machine learning models perform similarly regardless of the data set or molecular descriptors used. We have also used the rank normalized scores as a performance-criteria for each machine learning method, and Random Forest performed best on the validation set when ranked by metric or by data sets. These results suggest classic machine learning algorithms may be sufficient to develop high quality predictive models of ER activity.} }
@article{WOS:001136727600004, title = {A primer on the use of machine learning to distil knowledge from data in biological psychiatry}, journal = {MOLECULAR PSYCHIATRY}, year = {2024}, issn = {1359-4184}, doi = {10.1038/s41380-023-02334-2}, author = {Quinn, Thomas P. and Hess, Jonathan L. and Marshe, Victoria S. and Barnett, Michelle M. and Hauschild, Anne-Christin and Maciukiewicz, Malgorzata and Elsheikh, Samar S. M. and Men, Xiaoyu and Schwarz, Emanuel and Trakadis, Yannis J. and Breen, Michael S. and Barnett, Eric J. and Zhang-James, Yanli and Ahsen, Mehmet Eren and Cao, Han and Chen, Junfang and Hou, Jiahui and Salekin, Asif and Lin, I, Ping- and Nicodemus, Kristin K. and Meyer-Lindenberg, Andreas and Bichindaritz, Isabelle and Faraone, Stephen V. and Cairns, Murray J. and Pandey, Gaurav and Mueller, Daniel J. and Glatt, Stephen J. and Machine Learning Psychiat MLPsych}, abstract = {Applications of machine learning in the biomedical sciences are growing rapidly. This growth has been spurred by diverse cross-institutional and interdisciplinary collaborations, public availability of large datasets, an increase in the accessibility of analytic routines, and the availability of powerful computing resources. With this increased access and exposure to machine learning comes a responsibility for education and a deeper understanding of its bases and bounds, borne equally by data scientists seeking to ply their analytic wares in medical research and by biomedical scientists seeking to harness such methods to glean knowledge from data. This article provides an accessible and critical review of machine learning for a biomedically informed audience, as well as its applications in psychiatry. The review covers definitions and expositions of commonly used machine learning methods, and historical trends of their use in psychiatry. We also provide a set of standards, namely Guidelines for REporting Machine Learning Investigations in Neuropsychiatry (GREMLIN), for designing and reporting studies that use machine learning as a primary data-analysis approach. Lastly, we propose the establishment of the Machine Learning in Psychiatry (MLPsych) Consortium, enumerate its objectives, and identify areas of opportunity for future applications of machine learning in biological psychiatry. This review serves as a cautiously optimistic primer on machine learning for those on the precipice as they prepare to dive into the field, either as methodological practitioners or well-informed consumers.} }
@article{WOS:000412062500001, title = {Progressive sampling-based Bayesian optimization for efficient and automatic machine learning model selection}, journal = {HEALTH INFORMATION SCIENCE AND SYSTEMS}, volume = {5}, year = {2017}, issn = {2047-2501}, doi = {10.1007/s13755-017-0023-z}, author = {Zeng, Xueqiang and Luo, Gang}, abstract = {Purpose: Machine learning is broadly used for clinical data analysis. Before training a model, a machine learning algorithm must be selected. Also, the values of one or more model parameters termed hyper-parameters must be set. Selecting algorithms and hyper-parameter values requires advanced machine learning knowledge and many labor-intensive manual iterations. To lower the bar to machine learning, miscellaneous automatic selection methods for algorithms and/or hyper-parameter values have been proposed. Existing automatic selection methods are inefficient on large data sets. This poses a challenge for using machine learning in the clinical big data era. Methods: To address the challenge, this paper presents progressive sampling-based Bayesian optimization, an efficient and automatic selection method for both algorithms and hyper-parameter values. Results: We report an implementation of the method. We show that compared to a state of the art automatic selection method, our method can significantly reduce search time, classification error rate, and standard deviation of error rate due to randomization. Conclusions: This is major progress towards enabling fast turnaround in identifying high-quality solutions required by many machine learning-based clinical data analysis tasks.} }
@article{WOS:000663077000011, title = {FeARH: Federated machine learning with anonymous random hybridization on electronic medical records}, journal = {JOURNAL OF BIOMEDICAL INFORMATICS}, volume = {117}, year = {2021}, issn = {1532-0464}, doi = {10.1016/j.jbi.2021.103735}, author = {Cui, Jianfei and Zhu, He and Deng, Hao and Chen, Ziwei and Liu, Dianbo}, abstract = {Electrical medical records are restricted and difficult to centralize for machine learning model training due to privacy and regulatory issues. One solution is to train models in a distributed manner that involves many parties in the process. However, sometimes certain parties are not trustable, and in this project, we aim to propose an alternative method to traditional federated learning with central analyzer in order to conduct training in a situation without a trustable central analyzer. The proposed algorithm is called ``federated machine learning with anonymous random hybridization (abbreviated as `FeARH')'', using mainly hybridization algorithm to degenerate the integration of connections between medical record data and models' parameters by adding randomization into the parameter sets shared to other parties. Based on our experiment, our new algorithm has similar AUCROC and AUCPR results compared with machine learning in a centralized manner and original federated machine learning.} }
@article{WOS:000483476800034, title = {Applications of Machine Learning Approaches in Emergency Medicine; a Review Article}, journal = {ARCHIVES OF ACADEMIC EMERGENCY MEDICINE}, volume = {7}, year = {2019}, author = {Shafaf, Negin and Malek, Hamed}, abstract = {Using artificial intelligence and machine learning techniques in different medical fields, especially emergency medicine is rapidly growing. In this paper, studies conducted in the recent years on using artificial intelligence in emergency medicine have been collected and assessed. These studies belonged to three categories: prediction and detection of disease; prediction of need for admission, discharge and also mortality; and machine learning based triage systems. In each of these categories, the most important studies have been chosen and accuracy and results of the algorithms have been briefly evaluated by mentioning machine learning techniques and used datasets.} }
@article{WOS:000675886300003, title = {Multi-source transfer learning network to complement knowledge for intelligent diagnosis of machines with unseen faults}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {162}, year = {2022}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2021.108095}, author = {Yang, Bin and Xu, Songci and Lei, Yaguo and Lee, Chi-Guhn and Stewart, Edward and Roberts, Clive}, abstract = {Most of the current successes of deep transfer learning-based fault diagnosis require two assumptions: 1) the health state set of source machines should overlap that of target machines; 2) the number of target machine samples is balanced across health states. However, such assumptions are unrealistic in engineering scenarios, where target machines suffer from fault types that are not seen in source machines and the target machines are mostly in a healthy state with only occasional faults. As a result, the diagnostic knowledge from source machines may not cover all fault types of target machines nor address imbalanced target samples. Therefore, we propose a framework, called a multi-source transfer learning network (MSTLN), to aggregate and transfer diagnostic knowledge from multiple source machines by combining multiple partial distribution adaptation sub-networks (PDA-Subnets) and a multi-source diagnostic knowledge fusion module. The former weights target samples by counter-balancing factors to jointly adapt partial distributions of source and target pairs, and the latter releases negative effects due to discrepancy among multiple source machines and further fuses diagnostic decisions output from multiple PDA-Subnets. Two case studies demonstrate that MSTLN can reduce the misdiagnosis rate and obtain better transfer performance for imbalanced target samples than other conventional methods.} }
@article{WOS:000405536900025, title = {Twin extreme learning machines for pattern classification}, journal = {NEUROCOMPUTING}, volume = {260}, pages = {235-244}, year = {2017}, issn = {0925-2312}, doi = {10.1016/j.neucom.2017.04.036}, author = {Wan, Yihe and Song, Shiji and Huang, Gao and Li, Shuang}, abstract = {Extreme learning machine (ELM) is an efficient and effective learning algorithm for pattern classification. For binary classification problem, traditional ELM learns only one hyperplane to separate different classes in the feature space. In this paper, we propose a novel twin extreme learning machine (TELM) to simultaneously train two ELMs with two nonparallel classification hyperplanes. Specifically, TELM first utilizes the random feature mapping mechanism to construct the feature space, and then two nonparallel separating hyperplanes are learned for the final classification. For each hyperplane, TELM jointly minimizes its distance to one class and requires it to be far away from the other class. TELM incorporates the idea of twin support vector machine (TSVM) into the basic framework of ELM, thus TELM could have the advantages of the both algorithms. Moreover, compared to TSVM, TELM has fewer optimization constraint variables but with better classification performance. We also introduce a successive over-relaxation technique to speed up the training of our algorithm. Comprehensive experimental results on a large number of datasets verify the effectiveness and efficiency of TELM. (C) 2017 Published by Elsevier B.V.} }
@article{WOS:000747895100011, title = {Evaluating machine learning models for sepsis prediction: A systematic review of methodologies}, journal = {ISCIENCE}, volume = {25}, year = {2022}, doi = {10.1016/j.isci.2021.103651}, author = {Deng, Hong-Fei and Sun, Ming-Wei and Wang, Yu and Zeng, Jun and Yuan, Ting and Li, Ting and Li, Di-Huan and Chen, Wei and Zhou, Ping and Wang, Qi and Jiang, Hua}, abstract = {Studies for sepsis prediction using machine learning are developing rapidly in medical science recently. In this review, we propose a set of new evaluation criteria and reporting standards to assess 21 qualified machine learning models for quality analysis based on PRISMA. Our assessment shows that (1.) the definition of sepsis is not consistent among the studies; (2.) data sources and data preprocessing methods, machine learning models, feature engineering, and inclusion types vary widely among the studies; (3.) the closer to the onset of sepsis, the higher the value of AUROC is; (4.) the improvement in AUROC is primarily due to using machine learning as a feature engineering tool; (5.) deep neural networks coupled with Sepsis-3 diagnostic criteria tend to yield better results on the time series data collected from patients with sepsis. The new evaluation criteria and reporting standards will facilitate the development of improved machine learning models for clinical applications.} }
@article{WOS:000618935000001, title = {Applying machine learning approach in recycling}, journal = {JOURNAL OF MATERIAL CYCLES AND WASTE MANAGEMENT}, volume = {23}, pages = {855-871}, year = {2021}, issn = {1438-4957}, doi = {10.1007/s10163-021-01182-y}, author = {Erkinay Ozdemir, Merve and Ali, Zaara and Subeshan, Balakrishnan and Asmatulu, Eylem}, abstract = {Waste generation has been increasing drastically based on the world's population and economic growth. This has significantly affected human health, natural life, and ecology. The utilization of limited natural resources, and the harming of the earth in the process of mineral extraction, and waste management have far exceeded limits. The recycling rate are continuously increasing; however, assessments show that humans will be creating more waste than ever before. Some difficulties during recycling include the significant expense involved during the separation of recyclable waste from non-disposable waste. Machine learning is the utilization of artificial intelligence (AI) that provides a framework to take as a structural improvement of the fact without being programmed. Machine learning concentrates on the advancement of programs that can obtain the information and use it to learn to make future decisions. The classification and separation of materials in a mixed recycling application in machine learning is a division of AI that is playing an important role for better separation of complex waste. The primary purpose of this study is to analyze AI by focusing on machine learning algorithms used in recycling systems. This study is a compilation of the most recent developments in machine learning used in recycling industries.} }
@article{WOS:000826145900001, title = {Machine learning-driven credit risk: a systemic review}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {14327-14339}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-022-07472-2}, author = {Shi, Si and Tse, Rita and Luo, Wuman and D'Addona, Stefano and Pau, Giovanni}, abstract = {Credit risk assessment is at the core of modern economies. Traditionally, it is measured by statistical methods and manual auditing. Recent advances in financial artificial intelligence stemmed from a new wave of machine learning (ML)-driven credit risk models that gained tremendous attention from both industry and academia. In this paper, we systematically review a series of major research contributions (76 papers) over the past eight years using statistical, machine learning and deep learning techniques to address the problems of credit risk. Specifically, we propose a novel classification methodology for ML-driven credit risk algorithms and their performance ranking using public datasets. We further discuss the challenges including data imbalance, dataset inconsistency, model transparency, and inadequate utilization of deep learning models. The results of our review show that: 1) most deep learning models outperform classic machine learning and statistical algorithms in credit risk estimation, and 2) ensemble methods provide higher accuracy compared with single models. Finally, we present summary tables in terms of datasets and proposed models.} }
@article{WOS:001255993200001, title = {Strategies to Enrich Electrochemical Sensing Data with Analytical Relevance for Machine Learning Applications: A Focused Review}, journal = {SENSORS}, volume = {24}, year = {2024}, doi = {10.3390/s24123855}, author = {Kang, Mijeong and Kim, Donghyeon and Kim, Jihee and Kim, Nakyung and Lee, Seunghun}, abstract = {In this review, recent advances regarding the integration of machine learning into electrochemical analysis are overviewed, focusing on the strategies to increase the analytical context of electrochemical data for enhanced machine learning applications. While information-rich electrochemical data offer great potential for machine learning applications, limitations arise when sensors struggle to identify or quantitatively detect target substances in a complex matrix of non-target substances. Advanced machine learning techniques are crucial, but equally important is the development of methods to ensure that electrochemical systems can generate data with reasonable variations across different targets or the different concentrations of a single target. We discuss five strategies developed for building such electrochemical systems, employed in the steps of preparing sensing electrodes, recording signals, and analyzing data. In addition, we explore approaches for acquiring and augmenting the datasets used to train and validate machine learning models. Through these insights, we aim to inspire researchers to fully leverage the potential of machine learning in electroanalytical science.} }
@article{WOS:000884481200001, title = {Interpretable machine learning methods for predictions in systems biology from omics data}, journal = {FRONTIERS IN MOLECULAR BIOSCIENCES}, volume = {9}, year = {2022}, doi = {10.3389/fmolb.2022.926623}, author = {Sidak, David and Schwarzerova, Jana and Weckwerth, Wolfram and Waldherr, Steffen}, abstract = {Machine learning has become a powerful tool for systems biologists, from diagnosing cancer to optimizing kinetic models and predicting the state, growth dynamics, or type of a cell. Potential predictions from complex biological data sets obtained by ``omics `` experiments seem endless, but are often not the main objective of biological research. Often we want to understand the molecular mechanisms of a disease to develop new therapies, or we need to justify a crucial decision that is derived from a prediction. In order to gain such knowledge from data, machine learning models need to be extended. A recent trend to achieve this is to design ``interpretable `` models. However, the notions around interpretability are sometimes ambiguous, and a universal recipe for building well-interpretable models is missing. With this work, we want to familiarize systems biologists with the concept of model interpretability in machine learning. We consider data sets, data preparation, machine learning methods, and software tools relevant to omics research in systems biology. Finally, we try to answer the question: ``What is interpretability? `` We introduce views from the interpretable machine learning community and propose a scheme for categorizing studies on omics data. We then apply these tools to review and categorize recent studies where predictive machine learning models have been constructed from non-sequential omics data.} }
@article{WOS:001068492000001, title = {An empirical evaluation of extreme learning machine uncertainty quantification for automated breast cancer detection}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, year = {2023}, issn = {0941-0643}, doi = {10.1007/s00521-023-08992-1}, author = {Muduli, Debendra and Kumar, Rakesh Ranjan and Pradhan, Jitesh and Kumar, Abhinav}, abstract = {Early detection and diagnosis are the key factors in decreasing the breast cancer mortality rate in medical image analysis. A randomized learning technique called extreme learning machine (ELM) plays a vital role in learning the single hidden layer feed-forward network with fast learning speed and good generalization. The input weight and bias are randomly generated and fixed during the ELM training phase, and subsequently, the analytical procedure determines the output weight. The extreme learning machine's learning ability is based on three uncertainty factors: the number of hidden nodes, an input weight initialization, and the type of activation function in the hidden layer. Various breast classification works have experimented with extreme learning machine techniques and did not investigate the following factors. This paper evaluates the extreme learning machine model's performance with different configurations on the standard ultra-sound breast cancer dataset, BUSI. The proposed extreme learning machine configuration model experimented on original and filtered ultra-sound images. A fivefold stratified cross-validation scheme is applied here to enhance the model's generalization performance. The proposed computer-aided diagnosis (CAD) model provides 100\\% accuracy with the best extreme learning machine configurations. Then, we compare the classification results of the proposed model with typical variants of extreme learning machines like Hybrid ELM (HELM), online-sequential ELM (OS-ELM), Weighted ELM, and complex ELM (CELM). The experimental results demonstrate that the proposed extreme learning machine model is superior to existing models, offering good generalization without any feature extraction or reduction method.} }
@article{WOS:000445815100007, title = {A Tutorial on Machine Learning for Interactive Pedagogical Systems}, journal = {INTERNATIONAL JOURNAL OF SERIOUS GAMES}, volume = {5}, pages = {79-112}, year = {2018}, issn = {2384-8766}, doi = {10.17083/ijsg.v5i3.256}, author = {Melo, Francisco S. and Mascarenhas, Samuel and Paiva, Ana}, abstract = {This paper provides a short introduction to the field of machine learning for interactive pedagogical systems. Departing from different examples encountered in interactive pedagogical systems-such as intelligent tutoring systems or serious games-we go over several representative families of methods in machine learning, introducing key concepts in this field. We discuss common challenges in machine learning and how current methods address such challenges. Conversely, by anchoring our presentation on actual interactive pedagogical systems, highlight how machine learning can benefit the development of such systems.} }
@article{WOS:000729816900003, title = {Prediction of occurrence of extreme events using machine learning}, journal = {EUROPEAN PHYSICAL JOURNAL PLUS}, volume = {137}, year = {2022}, issn = {2190-5444}, doi = {10.1140/epjp/s13360-021-02249-3}, author = {Meiyazhagan, J. and Sudharsan, S. and Venkatesan, A. and Senthilvelan, M.}, abstract = {Machine learning models play a vital role in the prediction task in several fields of study. In this work, we utilize the ability of machine learning algorithms to predict the occurrence of extreme events in a nonlinear mechanical system. Extreme events are rare events that occur ubiquitously in nature. We consider four machine learning models, namely Logistic Regression, Support Vector Machine, Random Forest and Multi-Layer Perceptron in our prediction task. We train these four machine learning models using training set data and compute the performance of each model using the test set data. We show that the Multi-Layer Perceptron model performs better among the four models in the prediction of extreme events in the considered system. The persistent behaviour of the considered machine learning models is cross-checked with randomly shuffled training set and test set data.} }
@article{WOS:000542942700029, title = {Blockchained On-Device Federated Learning}, journal = {IEEE COMMUNICATIONS LETTERS}, volume = {24}, pages = {1279-1283}, year = {2020}, issn = {1089-7798}, doi = {10.1109/LCOMM.2019.2921755}, author = {Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun}, abstract = {By leveraging blockchain, this letter proposes a blockchained federated learning (BlockFL) architecture where local learning model updates are exchanged and verified. This enables on-device machine learning without any centralized training data or coordination by utilizing a consensus mechanism in blockchain. Moreover, we analyze an end-to-end latency model of BlockFL and characterize the optimal block generation rate by considering communication, computation, and consensus delays.} }
@article{WOS:000453925000014, title = {Peering Into the Black Box of Artificial Intelligence: Evaluation Metrics of Machine Learning Methods}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {212}, pages = {38-43}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.18.20224}, author = {Handelman, Guy S. and Kok, Hong Kuan and Chandra, Ronil V. and Razavi, Amir H. and Huang, Shiwei and Brooks, Mark and Lee, Michael J. and Asadi, Hamed}, abstract = {OBJECTIVE. Machine learning (ML) and artificial intelligence (AI) are rapidly becoming the most talked about and controversial topics in radiology and medicine. Over the past few years, the numbers of ML- or AI-focused studies in the literature have increased almost exponentially, and ML has become a hot topic at academic and industry conferences. However, despite the increased awareness of ML as a tool, many medical professionals have a poor understanding of how ML works and how to critically appraise studies and tools that are presented to us. Thus, we present a brief overview of ML, explain the metrics used in ML and how to interpret them, and explain some of the technical jargon associated with the field so that readers with a medical background and basic knowledge of statistics can feel more comfortable when examining ML applications. CONCLUSION. Attention to sample size, overfitting, underfitting, cross validation, as well as a broad knowledge of the metrics of machine learning, can help those with little or no technical knowledge begin to assess machine learning studies. However, transparency in methods and sharing of algorithms is vital to allow clinicians to assess these tools themselves.} }
@article{WOS:000424191300048, title = {Active learning machine learns to create new quantum experiments}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {115}, pages = {1221-1226}, year = {2018}, issn = {0027-8424}, doi = {10.1073/pnas.1714936115}, author = {Melnikov, Alexey A. and Nautrup, Hendrik Poulsen and Krenn, Mario and Dunjko, Vedran and Tiersch, Markus and Zeilinger, Anton and Briegel, Hans J.}, abstract = {How useful can machine learning be in a quantum laboratory? Here we raise the question of the potential of intelligent machines in the context of scientific research. A major motivation for the present work is the unknown reachability of various entanglement classes in quantum experiments. We investigate this question by using the projective simulation model, a physics-oriented approach to artificial intelligence. In our approach, the projective simulation system is challenged to design complex photonic quantum experiments that produce high-dimensional entangled multiphoton states, which are of high interest in modern quantum experiments. The artificial intelligence system learns to create a variety of entangled states and improves the efficiency of their realization. In the process, the system autonomously (re)discovers experimental techniques which are only now becoming standard in modern quantum optical experiments-a trait which was not explicitly demanded from the system but emerged through the process of learning. Such features highlight the possibility that machines could have a significantly more creative role in future research.} }
@article{WOS:000700566400002, title = {Boltzmann machine learning with a variational quantum algorithm}, journal = {PHYSICAL REVIEW A}, volume = {104}, year = {2021}, issn = {2469-9926}, doi = {10.1103/PhysRevA.104.032413}, author = {Shingu, Yuta and Seki, Yuya and Watabe, Shohei and Endo, Suguru and Matsuzaki, Yuichiro and Kawabata, Shiro and Nikuni, Tetsuro and Hakoshima, Hideaki}, abstract = {A Boltzmann machine is a powerful tool for modeling probability distributions that govern the training data. A thermal equilibrium state is typically used for the Boltzmann machine learning to obtain a suitable probability distribution. The Boltzmann machine learning consists of calculating the gradient of the loss function given in terms of the thermal average, which is the most time-consuming procedure. Here, we propose a method to implement the Boltzmann machine learning by using noisy intermediate-scale quantum devices. We prepare an initial pure state that contains all possible computational basis states with the same amplitude, and we apply a variational imaginary time simulation. Readout of the state after the evolution in the computational basis approximates the probability distribution of the thermal equilibrium state that is used for the Boltzmann machine learning. We perform the numerical simulations of our scheme and confirm that the Boltzmann machine learning works well. Our scheme leads to a significant step toward an efficient machine learning using quantum hardware.} }
@article{WOS:001293612000015, title = {Creativity and Machine Learning: A Survey}, journal = {ACM COMPUTING SURVEYS}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3664595}, author = {Franceschelli, Giorgio and Musolesi, Mirco}, abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.} }
@article{WOS:001109664000001, title = {Tiny Machine Learning: Progress and Futures}, journal = {IEEE CIRCUITS AND SYSTEMS MAGAZINE}, volume = {23}, pages = {8-34}, year = {2023}, issn = {1531-636X}, doi = {10.1109/MCAS.2023.3302182}, author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Han, Song}, abstract = {Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today's ``large'' model might be tomorrow's ``tiny'' model. The scope of TinyML should evolve and adapt over time.} }
@article{WOS:000730729600001, title = {Predictably Unequal? The Effects of Machine Learning on Credit Markets}, journal = {JOURNAL OF FINANCE}, volume = {77}, pages = {5-47}, year = {2022}, issn = {0022-1082}, doi = {10.1111/jofi.13090}, author = {Fuster, Andreas and Goldsmith-Pinkham, Paul and Ramadorai, Tarun and Walther, Ansgar}, abstract = {Innovations in statistical technology in functions including credit-screening have raised concerns about distributional impacts across categories such as race. Theoretically, distributional effects of better statistical technology can come from greater flexibility to uncover structural relationships or from triangulation of otherwise excluded characteristics. Using data on U.S. mortgages, we predict default using traditional and machine learning models. We find that Black and Hispanic borrowers are disproportionately less likely to gain from the introduction of machine learning. In a simple equilibrium credit market model, machine learning increases disparity in rates between and within groups, with these changes attributable primarily to greater flexibility.} }
@article{WOS:000604147200010, title = {Quantum adversarial machine learning}, journal = {PHYSICAL REVIEW RESEARCH}, volume = {2}, year = {2020}, doi = {10.1103/PhysRevResearch.2.033212}, author = {Lu, Sirui and Duan, Lu-Ming and Deng, Dong-Ling}, abstract = {Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and recently has attracted tremendous attention across different communities. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals another perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.} }
@article{WOS:000487122600008, title = {Importance of machine learning for enhancing ecological studies using information-rich imagery}, journal = {ENDANGERED SPECIES RESEARCH}, volume = {39}, pages = {91-104}, year = {2019}, issn = {1863-5407}, doi = {10.3354/esr00958}, author = {Dujon, Antoine M. and Schofield, Gail}, abstract = {There is increasing demand for efficient ways to process large volumes of data from visual-based remote-technology, such as unmanned aerial vehicles (UAVs) in ecology and conservation, with machine learning methods representing a promising avenue to address varying user demands. Here, we evaluated current trends in how machine learning and UAVs are used to process imagery data for detecting animals and vegetation across habitats, placing emphasis on their utility for endangered species. We reviewed 213 publications that used UAVs at 256 study sites, of which just 89 (42 \\%) used machine learning to assess the visual data. We evaluated geographical and temporal trends and identified how each technology is used at a global scale. We also identified the most commonly encountered machine-learning methods, including potential reasons for their limited use in ecology and possible solutions. Thirteen out of the 17 habitats defined by the International Union for Conservation of Nature (IUCN) habitat classification scheme were monitored using UAVs, while 12 habitats were monitored using both UAVs and machine learning. Our results show that, while machine learning is already being used across many habitat types, it is primarily restricted to more uniform habitats at present. Out of 173 plant and animal species monitored using UAV surveys, 30 were of conservation concern, with machine learning being used to assess UAV imagery data for 9 of these species. In conclusion, we anticipate that the joint use of UAVs and machine learning for ecological research and conservation will expand as machine learning methods become more accessible.} }
@article{WOS:000874987300005, title = {Image-based machine learning for materials science}, journal = {JOURNAL OF APPLIED PHYSICS}, volume = {132}, year = {2022}, issn = {0021-8979}, doi = {10.1063/5.0087381}, author = {Zhang, Lei and Shao, Shaofeng}, abstract = {Materials research studies are dealing with a large number of images, which can now be facilitated via image-based machine learning techniques. In this article, we review recent progress of machine learning-driven image recognition and analysis for the materials and chemical domains. First, the image-based machine learning that facilitates the property prediction of chemicals or materials is discussed. Second, the analysis of nanoscale images including those from a scanning electron microscope and a transmission electron microscope is discussed, which is followed by the discussion about the identification of molecular structures via image recognition. Subsequently, the image-based machine learning works to identify and classify various practical materials such as metal, ceramics, and polymers are provided, and the image recognition for a range of real-scenario device applications such as solar cells is provided in detail. Finally, suggestions and future outlook for image based machine learning for classification and prediction tasks in the materials and chemical science are presented. This article highlights the importance of the integration of the image-based machine learning method into materials and chemical science and calls for a large-scale deployment of image-based machine learning methods for prediction and classification of images in materials and chemical science. Published under an exclusive license by AIP Publishing.} }
@article{WOS:001162062000001, title = {A comprehensive review of machine learning and its application to dairy products}, journal = {CRITICAL REVIEWS IN FOOD SCIENCE AND NUTRITION}, volume = {65}, pages = {1878-1893}, year = {2025}, issn = {1040-8398}, doi = {10.1080/10408398.2024.2312537}, author = {Freire, Paulina and Freire, Diego and Licon, Carmen C.}, abstract = {Machine learning (ML) technology is a powerful tool in food science and engineering offering numerous advantages, from recognizing patterns and predicting outcomes to customizing and adjusting to individual needs. Its further development can enable researchers and industries to significantly enhance the efficiency of dairy processing while providing valuable insights into the field. This paper presents an overview of the role of machine learning in the dairy industry and its potential to improve the efficiency of dairy processing. We performed a systematic search for articles published between January 2003 and January 2023 related to machine learning in dairy products and highlighted the algorithms used. 48 studies are discussed to assist researchers in identifying the best methods that could be applied in their field and providing relevant ideas for future research directions. Moreover, a step-by-step guide to the machine learning process, including a classification of different machine learning algorithms, is provided. This review focuses on state-of-the-art machine learning applications in milk products and their transformation into other dairy products, but it also presents future perspectives and conclusions. The study serves as a valuable guide for individuals in the dairy industry interested in learning about or getting involved with ML.} }
@article{WOS:000435400900032, title = {Arabic Text Categorization using Machine Learning Approaches}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {9}, pages = {226-230}, year = {2018}, issn = {2158-107X}, author = {Alshammari, Riyad}, abstract = {Arabic Text categorization is considered one of the severe problems in classification using machine learning algorithms. Achieving high accuracy in Arabic text categorization depends on the preprocessing techniques used to prepare the data set. Thus, in this paper, an investigation of the impact of the preprocessing methods concerning the performance of three machine learning algorithms, namely, Naive Bayesian, DMNBtext and C4.5 is conducted. Results show that the DMNBtext learning algorithm achieved higher performance compared to other machine learning algorithms in categorizing Arabic text.} }
@article{WOS:000802887500001, title = {Matrix product state pre-training for quantum machine learning}, journal = {QUANTUM SCIENCE AND TECHNOLOGY}, volume = {7}, year = {2022}, issn = {2058-9565}, doi = {10.1088/2058-9565/ac7073}, author = {Dborin, James and Barratt, Fergus and Wimalaweera, Vinul and Wright, Lewis and Green, Andrew G.}, abstract = {Hybrid quantum-classical algorithms are a promising candidate for developing uses for NISQ devices. In particular, parametrised quantum circuits (PQCs) paired with classical optimizers have been used as a basis for quantum chemistry and quantum optimization problems. Tensor network methods are being increasingly used as a classical machine learning tool, as well as a tool for studying quantum systems. We introduce a circuit pre-training method based on matrix product state machine learning methods, and demonstrate that it accelerates training of PQCs for both supervised learning, energy minimization, and combinatorial optimization.} }
@article{WOS:000890647400007, title = {Machine Learning for Computer Systems and Networking: A Survey}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3523057}, author = {Kanakis, Marios Evangelos and Khalili, Ramin and Wang, Lin}, abstract = {Machine learning (ML) has become the de-facto approach for various scientific domains such as computer vision and natural language processing. Despite recent breakthroughs, machine learning has only made its way into the fundamental challenges in computer systems and networking recently. This article attempts to shed light on recent literature that appeals for machine learning-based solutions to traditional problems in computer systems and networking. To this end, we first introduce a taxonomy based on a set of major research problem domains. Then, we present a comprehensive review per domain, where we compare the traditional approaches against the machine learning-based ones. Finally, we discuss the general limitations of machine learning for computer systems and networking, including lack of training data, training overhead, real-time performance, and explainability, and reveal future research directions targeting these limitations.} }
@article{WOS:000931530800001, title = {A hybrid agent-based machine learning method for human-centred energy consumption prediction}, journal = {ENERGY AND BUILDINGS}, volume = {283}, year = {2023}, issn = {0378-7788}, doi = {10.1016/j.enbuild.2023.112797}, author = {Qiao, Qingyao and Yunusa-Kaltungo, Akilu}, abstract = {Occupant behaviour has significant impacts on the performance of machine learning algorithms when predicting building energy consumption. Due to a variety of reasons (e.g., underperforming building energy management systems or restrictions due to privacy policies), the availability of occupational data has long been an obstacle that hinders the performance of machine learning algorithms in predicting building energy consumption. Therefore, this study proposed an agent-based machine learning model whereby agent-based modelling was employed to generate simulated occupational data as input features for machine learning algorithms for building energy consumption prediction. Boruta feature selection was also introduced in this study to select all relevant features. The results indicated that the perfor-mances of machine learning algorithms in predicting building energy consumption were significantly improved when using simulated occupational data, with even greater improvements after conducting Boruta feature selection.(c) 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:000715130600001, title = {Comparison of different machine learning models for mass appraisal of real estate}, journal = {SURVEY REVIEW}, volume = {55}, pages = {32-43}, year = {2023}, issn = {0039-6265}, doi = {10.1080/00396265.2021.1996799}, author = {Bilgilioglu, Suleyman Sefa and Yilmaz, Haci Murat}, abstract = {The present study aimed to compare five machine learning techniques, namely, artificial neural network (ANN), support vector machine (SVM), chi-square automatic interaction detection (CHAID), classification and regression tree (CART), and random forest (RF) for mass appraisal of real estate. Firstly, 1982 precedent data was collected throughout the entire study area for train and test models. Secondly, a total of 68 variables were considered for the mass appraisal. Subsequently, the five machine learning techniques were applied. Finally, the receiver operating characteristic (ROC) and various statistical methods were applied to compare five machine learning techniques.} }
@article{WOS:000540819200006, title = {Using machine learning in physics-based simulation of fire}, journal = {FIRE SAFETY JOURNAL}, volume = {114}, year = {2020}, issn = {0379-7112}, doi = {10.1016/j.firesaf.2020.102991}, author = {Lattimer, B. Y. and Hodges, J. L. and Lattimer, A. M.}, abstract = {There is a current need to provide rapid, high fidelity predictions of fires to support hazard/risk assessments, use sparse data to understand conditions, and develop mitigation strategies. Machine learning is one approach that has been used to provide rapid predictions based on large amounts of data in business, robotics, and image analysis; however, there have been limited applications to support physics-based or science applications. This paper provides a general overview of machine learning with details on specific techniques being explored for performing low-cost, high fidelity fire predictions. Examples of using both dimensionality reduction (reduced-order models) and deep learning with neural networks are provided. When compared with CFD results, these initial studies show that machine learning can provide full-field predictions 2-3 orders of magnitude faster than CFD simulations. Further work is needed to improve machine learning accuracy and extend these models to more general scenarios.} }
@article{WOS:000630618200001, title = {Machine learning in continuous casting of steel: a state-of-the-art survey}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {33}, pages = {1561-1579}, year = {2022}, issn = {0956-5515}, doi = {10.1007/s10845-021-01754-7}, author = {Cemernek, David and Cemernek, Sandra and Gursch, Heimo and Pandeshwar, Ashwini and Leitner, Thomas and Berger, Matthias and Klosch, Gerald and Kern, Roman}, abstract = {Continuous casting is the most important route for the production of steel today. Due to the physical, mechanical, and chemical components involved in the production, continuous casting is a very complex process, pushing conventional methods of monitoring and control to their limits. In recent years, this complexity and the increasing global competition created a demand for new methods to monitor and control the continuous casting process. Due to the success and associated rise of machine learning techniques in recent years, machine learning nowadays plays an essential role in monitoring and controlling complex processes. This publication presents a scientific survey of machine learning techniques for the analysis of the continuous casting process. We provide an introduction to both the involved fields: an overview of machine learning, and an overview of the continuous casting process. Therefore, we first analyze the existing work concerning machine learning in continuous casting of steel and then synthesize the common concepts into categories, supporting the identification of common use cases and approaches. This analysis is concluded with the elaboration of challenges, potential solutions, and a future outlook of further research directions.} }
@article{WOS:000850207400010, title = {A Review on Machine Learning Strategies for Real-World Engineering Applications}, journal = {MOBILE INFORMATION SYSTEMS}, volume = {2022}, year = {2022}, issn = {1574-017X}, doi = {10.1155/2022/1833507}, author = {Jhaveri, Rutvij H. and Revathi, A. and Ramana, Kadiyala and Raut, Roshani and Dhanaraj, Rajesh Kumar}, abstract = {Huge amounts of data are circulating in the digital world in the era of the Industry 5.0 revolution. Machine learning is experiencing success in several sectors such as intelligent control, decision making, speech recognition, natural language processing, computer graphics, and computer vision, despite the requirement to analyze and interpret data. Due to their amazing performance, Deep Learning and Machine Learning Techniques have recently become extensively recognized and implemented by a variety of real-time engineering applications. Knowledge of machine learning is essential for designing automated and intelligent applications that can handle data in fields such as health, cyber-security, and intelligent transportation systems. There are a range of strategies in the field of machine learning, including reinforcement learning, semi-supervised, unsupervised, and supervised algorithms. This study provides a complete study of managing real-time engineering applications using machine learning, which will improve an application's capabilities and intelligence. This work adds to the understanding of the applicability of various machine learning approaches in real-world applications such as cyber security, healthcare, and intelligent transportation systems. This study highlights the research objectives and obstacles that Machine Learning approaches encounter while managing real-world applications. This study will act as a reference point for both industry professionals and academics, and from a technical standpoint, it will serve as a benchmark for decision-makers on a range of application domains and real-world scenarios.} }
@article{WOS:000396957800011, title = {Entropy-based matrix learning machine for imbalanced data sets}, journal = {PATTERN RECOGNITION LETTERS}, volume = {88}, pages = {72-80}, year = {2017}, issn = {0167-8655}, doi = {10.1016/j.patrec.2017.01.014}, author = {Zhu, Changming and Wang, Zhe}, abstract = {Imbalance problem occurs when negative class contains many more patterns than that of positive class. Since conventional Support Vector Machine (SVM) and Neural Networks (NN) have been proven not to effectively handle imbalanced data, some improved learning machines including Fuzzy SVM (FSVM) have been proposed. FSVM applies a fuzzy membership to each training pattern such that different patterns can give different contributions to the learning machine. However, how to evaluate fuzzy membership becomes the key point to FSVM. Moreover, these learning machines present disadvantages to process matrix patterns. In order to process matrix patterns and to tackle the imbalance problem, this paper proposes an entropy-based matrix learning machine for imbalanced data sets, adopting the Matrix-pattern oriented Ho-Kashyap learning machine with regularization learning (MatMHKS) as the base classifier. The new leaning machine is named EMatMHKS and its contributions are: (1) proposing a new entropy-based fuzzy membership evaluation approach which enhances the importance of patterns, (2) guaranteeing the importance of positive patterns and get a more flexible decision surface. Experiments on real-world imbalanced data sets validate that EMatMHKS outperforms compared learning machines. (C) 2017 Elsevier B.V. All rights reserved.} }
@article{WOS:000495542300001, title = {Can machine learning on economic data better forecast the unemployment rate?}, journal = {APPLIED ECONOMICS LETTERS}, volume = {27}, pages = {1434-1437}, year = {2020}, issn = {1350-4851}, doi = {10.1080/13504851.2019.1688237}, author = {Kreiner, Aaron and Duca, John V.}, abstract = {Using FRED data, a machine-learning model outperforms the Survey of Professional Forecasters and other models since 2001 in forecasting the unemployment rate.} }
@article{WOS:000540984000015, title = {Machine learning, the kidney, and genotype-phenotype analysis}, journal = {KIDNEY INTERNATIONAL}, volume = {97}, pages = {1141-1149}, year = {2020}, issn = {0085-2538}, doi = {10.1016/j.kint.2020.02.028}, author = {Sealfon, Rachel S. G. and Mariani, Laura H. and Kretzler, Matthias and Troyanskaya, Olga G.}, abstract = {With biomedical research transitioning into data-rich science, machine learning provides a powerful toolkit for extracting knowledge from large-scale biological data sets. The increasing availability of comprehensive kidney omics compendia (transcriptomics, proteomics, metabolomics, and genome sequencing), as well as other data modalities such as electronic health records, digital nephropathology repositories, and radiology renal images, makes machine learning approaches increasingly essential for analyzing human kidney data sets. Here, we discuss how machine learning approaches can be applied to the study of kidney disease, with a particular focus on how they can be used for understanding the relationship between genotype and phenotype.} }
@article{WOS:000454421100001, title = {A Review on Industrial Applications of Machine Learning}, journal = {INTERNATIONAL JOURNAL OF DISASTER RECOVERY AND BUSINESS CONTINUITY}, volume = {9}, pages = {1-9}, year = {2018}, issn = {2005-4289}, doi = {10.14257/ijdrbc.2018.9.01}, author = {Rao, N. Thirupathi}, abstract = {Machine learning is the rapidly growing technology in the field of almost all recent technologies in the market. With the successful application of machine learning in almost all the recent technologies, the growth in all the areas was splendid. The growth in those areas has crossed the expectations of the scientists. Recently, the application of machine learning in the areas of medicine and pharma is growing in the recent times in a rapid fast. In the current paper, the authors represent the seven applications or the areas in the field of medicine and pharma where the applications of the machine learning were implementing and good results are obtaining.} }
@article{WOS:000567789900003, title = {A Taxonomy of ML for Systems Problems}, journal = {IEEE MICRO}, volume = {40}, pages = {8-16}, year = {2020}, issn = {0272-1732}, doi = {10.1109/MM.2020.3012883}, author = {Maas, Martin}, abstract = {Machine learning has the potential to significantly improve systems, but only under certain conditions. We describe a taxonomy to help identify whether or not machine learning should be applied to particular systems problems, and which approaches are most promising. We believe that this taxonomy can help practitioners and researchers decide how to most effectively use machine learning in their systems, and provide the community with a framework and vocabulary to discuss different approaches for applying machine learning in systems.} }
@article{WOS:000853623600001, title = {RETRACTED: Performance analysis of machine learning algorithms in heart disease prediction (Retracted Article)}, journal = {CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS}, volume = {30}, pages = {335-343}, year = {2022}, issn = {1063-293X}, doi = {10.1177/1063293X221125231}, author = {Dhasaradhan, K. and Jaichandran, R.}, abstract = {This work presents performance analysis of machine learning algorithms such as logistic regression, naive bayes, decision tree, k nearest neighbour, random forest, support vector machine, and extreme gradient boosting in heart disease prediction. Machine learning algorithms are implemented in python using Scikit learn library in Jupiter notebook. Experiments are conducted by training and testing machine learning algorithms using kaggle heart disease dataset under six test cases. Performance of machine learning algorithms are evaluated using accuracy, precision, recall, F1 score and ROC as metrics. Results show random forest reported high accuracy, precision, recall, F1 score and ROC in heart disease prediction compared to other machine learning algorithms in all six test cases. Results show RF is effective in heart disease prediction in Case 3 with 80\\% train data and 20\\% test data.} }
@article{WOS:001139560900001, title = {Batteries temperature prediction and thermal management using machine learning: An overview}, journal = {ENERGY REPORTS}, volume = {10}, pages = {2277-2305}, year = {2023}, issn = {2352-4847}, doi = {10.1016/j.egyr.2023.08.043}, author = {Al Miaari, Ahmad and Ali, Hafiz Muhammad}, abstract = {Batteries, particularly lithium-ion batteries, play an important role in powering our modern world, from portable devices to electric vehicles and renewable energy storage. However, during charging and discharging, they generate heat due to chemical reactions within them. This heat can lead to reduced performance, shortened lifespan, and even safety risks if not properly managed. To address this problem, Machine learning has been emerged as a changing tool in battery technology due to its ability to analyze large datasets that can be used in predicting battery temperatures and enhancing their thermal management. In this work, we address machine learning features along with a look at its various learning categories, frameworks, and applications. In a comprehensive study, various machine learning methods and neural networks used in battery temperature prediction and thermal management are analyzed and discussed along with its various training algorithms. Moreover, the paper reviews and summarizes various research publications examining battery temperature prediction and battery thermal management using the various machine learning algorithms. As a result, there is no superior machine learning algorithm for battery temperature prediction and thermal management, as the performance of the model may vary depending on the data set, training algorithm, and other parameters. However, among these machine learning algorithms researchers are preferring to use artificial neural networks due to its accuracy and model complexity. In particular, artificial neural network integrated with proper cooling technology can reduce the battery temperature by more than 25\\%.} }
@article{WOS:000558696600001, title = {Applying DevOps Practices of Continuous Automation for Machine Learning}, journal = {INFORMATION}, volume = {11}, year = {2020}, doi = {10.3390/info11070363}, author = {Karamitsos, Ioannis and Albarhami, Saeed and Apostolopoulos, Charalampos}, abstract = {This paper proposes DevOps practices for machine learning application, integrating both the development and operation environment seamlessly. The machine learning processes of development and deployment during the experimentation phase may seem easy. However, if not carefully designed, deploying and using such models may lead to a complex, time-consuming approaches which may require significant and costly efforts for maintenance, improvement, and monitoring. This paper presents how to apply continuous integration (CI) and continuous delivery (CD) principles, practices, and tools so as to minimize waste, support rapid feedback loops, explore the hidden technical debt, improve value delivery and maintenance, and improve operational functions for real-world machine learning applications.} }
@article{WOS:000852913100007, title = {Differentiation of Plastics by Combining Raman Spectroscopy and Machine Learning}, journal = {JOURNAL OF APPLIED SPECTROSCOPY}, volume = {89}, pages = {790-798}, year = {2022}, issn = {0021-9037}, doi = {10.1007/s10812-022-01426-1}, author = {Yang, Y. and Zhang, W. and Wang, Zh and Li, Y.}, abstract = {We combined Raman spectroscopy with machine learning for the classification of 11 plastic samples. A confocal Raman system with an excitation wavelength of 532 nm was used to collect the Raman spectral data of plastic samples and principal component analysis was used for feature extraction. The prediction models of plastic classification based on three machine learning algorithms are compared. The results show that all three machine learning algorithms are able to classify 11 plastics well. This indicates that the combination of Raman spectroscopy and machine learning has great potential in the rapid and nondestructive classification of plastics.} }
