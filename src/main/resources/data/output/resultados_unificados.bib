@inproceedings{10.1145/3744199.3744635, title = {Automated Video Segmentation Machine Learning Pipeline}, booktitle = {Proceedings of the Digital Production Symposium}, year = {2025}, isbn = {9798400720086}, doi = {10.1145/3744199.3744635}, url = {https://doi.org/10.1145/3744199.3744635}, author = {Merz, Johannes and Fostier, Lucien}, keywords = {Image Processing, Video Segmentation, Object Detection, Machine Learning}, abstract = {Visual effects (VFX) production often struggles with slow, resource-intensive mask generation. This paper presents an automated video segmentation pipeline that creates temporally consistent instance masks. It employs machine learning for: (1) flexible object detection via text prompts, (2) refined per-frame image segmentation and (3) robust video tracking to ensure temporal stability. Deployed using containerization and leveraging a structured output format, the pipeline was quickly adopted by our artists. It significantly reduces manual effort, speeds up the creation of preliminary composites, and provides comprehensive segmentation data, thereby enhancing overall VFX production efficiency.} }
@inproceedings{10.1145/3769394.3769411, title = {Sustainable Machine Learning: Course 1}, booktitle = {Proceedings of the Conference on 6th ACM Europe Summer School on Data Science}, pages = {17}, year = {2025}, doi = {10.1145/3769394.3769411}, url = {https://doi.org/10.1145/3769394.3769411}, author = {Kemme, Bettina}, abstract = {Machine learning has become increasingly data and processing hungry. A recent report from the International Energy Agency projects that the electricity demand for data centers specialized in AI will more than quadruple by 2030. As such, it has become a pressing need to include energy awareness and environmental sustainability into the Machine Learning life cycle. In fact, a considerable amount of research efforts have been conducted in the last years in this direction.The first part of this tutorial will discuss various mechanisms to assess the environmental impact of machine learning, from power and energy consumption to carbon footprint. This will be put in relation to more traditional performance metrics used in the research literature, from the "goodness" of a ML solution, measured by metrics such as accuracy, to systems performance metrics such as runtime, throughput and scalability. From there, the tutorial will present several concrete research efforts for a quantitative analysis of the environmental footprint of various ML tasks.The second part of the tutorial will outline recent solutions to tackle the huge energy consumption of modern ML. For instance, there have been an increasing number of research efforts to make both the learning and the inference tasks more efficient while providing similar performance in terms of traditional ML performance metrics such as accuracy. A further line of research focuses on adjusting the infrastructure or the execution of ML tasks to be more energy aware, e.g., through scheduling approaches.} }
@inproceedings{10.1145/3757110.3757152, title = {Telecom Customer Churn Prediction with Explainable Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {246--250}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757152}, url = {https://doi.org/10.1145/3757110.3757152}, author = {Yuan, Jiaming and Liu, Hao}, keywords = {CatBoost, SHAP, customer churn, machine learning, telecommunication}, abstract = {In the fast-paced era of digital transformation, the telecoms industry is grappling with an increasing issue of customer churn, which endangers stable revenue and operational efficiency. This paper therefore proposes an interpretable machine learning framework that aims to identify customers who are likely to churn more accurately, while also enhancing the interpretability of the model. The performance of the model is evaluated using the publicly available Iranian churn dataset by means of data preprocessing (duplicate removal, outlier detection, feature selection and class imbalance mitigation), training of seven machine learning models tuned with hyper-parameters and selection of five key performance indicators. Experimental results demonstrate that the CatBoost model achieves excellent performance, with an accuracy of 0.9554, precision of 0.9010, recall of 0.8072, F1 score of 0.8503 and AUC-ROC of 0.9823 — surpassing the performance of the other six models. Additionally, this study explored the stacking model, which ultimately proved to be inferior to the CatBoost model. Finally, the optimal model was interpreted using the SHAP technique. The analysis identified three key characteristics affecting customer churn: seconds of use (negative correlation), call failures (positive correlation) and frequency of SMS (negative correlation). This led to the derivation of an operational strategy for customer retention in the telecommunications industry.} }
@article{10.1145/3704807, title = {Collaborative Distributed Machine Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3704807}, url = {https://doi.org/10.1145/3704807}, author = {Jin, David and Kannengieer, Niclas and Rank, Sascha and Sunyaev, Ali}, keywords = {Collaborative distributed machine learning (CDML), privacy-enhancing technologies (PETs), assisted learning, federated learning (FL), split learning, swarm learning, multi-agent systems (MAS)}, abstract = {Various collaborative distributed machine learning (CDML) systems, including federated learning systems and swarm learning systems, with different key traits were developed to leverage resources for the development and use of machine learning models in a confidentiality-preserving way. To meet use case requirements, suitable CDML systems need to be selected. However, comparison between CDML systems to assess their suitability for use cases is often difficult. To support comparison of CDML systems and introduce scientific and practical audiences to the principal functioning and key traits of CDML systems, this work presents a CDML system conceptualization and CDML archetypes.} }
@article{10.1145/3767157, title = {How do Machine Learning Models Change?}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3767157}, url = {https://doi.org/10.1145/3767157}, author = {Casta\~no, Joel and Caba\~nas, Rafael and Salmer\'on, Antonio and Lo, David and Mart\'nez-Fern\'andez, Silverio}, keywords = {ML Software Evolution, ML Model Changes, ML Software Releases, Commit Type Classification, Bayesian Networks in Software Engineering}, abstract = {The proliferation of Machine Learning (ML) models and their open-source implementations has transformed Artificial Intelligence research and applications. Platforms like Hugging Face (HF) enable this evolving ecosystem, yet a large-scale longitudinal study of how these models change is lacking. This study addresses this gap by analyzing over 680,000 commits from 100,000 models and 2,251 releases from 202 of these models on HF using repository mining and longitudinal methods. We apply an extended ML change taxonomy to classify commits and use Bayesian networks to model temporal patterns in commit and release activities. Our findings show that commit activities align with established data science methodologies, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM), emphasizing iterative refinement. Release patterns tend to consolidate significant updates, particularly in model outputs, sharing, and documentation, distinguishing them from granular commits. Furthermore, projects with higher popularity exhibit distinct evolutionary paths, often starting from a more mature baseline with fewer foundational commits in their public history. In contrast, those with intensive collaboration show unique documentation and technical evolution patterns. These insights enhance the understanding of model changes on community platforms and provide valuable guidance for best practices in model maintenance.} }
@inproceedings{10.1145/3721251.3736530, title = {Introduction To Generative Machine Learning}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Labs}, year = {2025}, isbn = {9798400715501}, doi = {10.1145/3721251.3736530}, url = {https://doi.org/10.1145/3721251.3736530}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {This is an intermediate level course for attendees to gain a strong understanding of the basic principles of generative AI. The course will help build intuition around several topics with easy-to-understand explanations and examples from some of the prevalent algorithms and models including Autoencoders, CNN, Diffusion Models, Transformers, and NeRFs.} }
@inproceedings{10.1145/3767052.3767072, title = {Machine Learning Approaches to Creditworthiness Classification}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {127--134}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767072}, url = {https://doi.org/10.1145/3767052.3767072}, author = {Hua, Tianhao}, keywords = {Credit scoring, Random forest, Supervised learning, Support vector machine}, abstract = {Credit risk evaluation is fundamental in financial decision-making, directly influencing lending strategies and default prevention. With the growing availability of structured financial data, machine learning methods have become increasingly prominent in building predictive credit scoring models. This research evaluates Decision Tree, Random Forest, and SVM classifiers for creditworthiness assessment. The Statlog (German Credit Data) dataset from UCI is used with a standardized preprocessing pipeline. Each model was trained and tested on the same dataset split and evaluated using standard classification metrics such as accuracy, precision, recall, and F1 score. Results show that the Random Forest classifier achieved the highest overall performance, particularly in identifying good credit applicants. At the same time, the Decision Tree maintained interpretability, and SVM offered a balanced trade-off. The findings highlight key considerations for model selection in credit scoring applications and suggest ensemble methods as strong candidates for future deployment.} }
@article{10.1145/3761822, title = {Sample Selection Bias in Machine Learning for Healthcare}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761822}, url = {https://doi.org/10.1145/3761822}, author = {Chauhan, Vinod Kumar and Clifton, Lei and Sala\"un, Achille and Lu, Huiqi Yvonne and Branson, Kim and Schwab, Patrick and Nigam, Gaurav and Clifton, David A.}, keywords = {Sample Selection Bias, Target Population, Machine Learning, Healthcare, Risk Prediction}, abstract = {While machine learning algorithms hold promise for personalised medicine, their clinical adoption remains limited, partly due to biases that can compromise the reliability of predictions. In this article, we focus on sample selection bias (SSB), a specific type of bias where the study population is less representative of the target population, leading to biased and potentially harmful decisions. Despite being well-known in the literature, SSB remains scarcely studied in machine learning for healthcare. Moreover, the existing machine learning techniques try to correct the bias mostly by balancing distributions between the study and the target populations, which may result in a loss of predictive performance. To address these problems, our study illustrates the potential risks associated with SSB by examining SSB’s impact on the performance of machine learning algorithms. Most importantly, we propose a new research direction for addressing SSB, based on the target population identification rather than the bias correction. Specifically, we propose two independent networks (T-Net) and a multitasking network (MT-Net) for addressing SSB, where one network/task identifies the target subpopulation which is representative of the study population and the second makes predictions for the identified subpopulation. Our empirical results with synthetic and semi-synthetic datasets highlight that SSB can lead to a large drop in the performance of an algorithm for the target population as compared with the study population, as well as a substantial difference in the performance for the target subpopulations that are representative of the selected and the non-selected patients from the study population. Furthermore, our proposed techniques demonstrate robustness across various settings, including different dataset sizes, event rates and selection rates, outperforming the existing bias correction techniques.} }
@inproceedings{10.1145/3712256.3737464, title = {Rethinking Efficiency in Machine Learning}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {1}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3737464}, url = {https://doi.org/10.1145/3712256.3737464}, author = {Alonso-Betanzos, Amparo}, keywords = {green AI, scalable machine learning, location = NH Malaga Hotel, Malaga, Spain}, abstract = {The success of Artificial Intelligence (AI) has so far relied on developing increasingly precise models. However, this has come at the cost of greater complexity, requiring a higher number of parameters to estimate. As a result, model transparency and explainability have diminished, while the energy demands for training and deployment have skyrocketed. It is estimated that by 2030, AI could account for more than 30\% of the planet's total energy consumption.In this context, green and responsible AI has emerged as a promising alternative, characterized by lower carbon footprints, reduced model sizes, decreased computational complexity, and improved transparency. Various strategies can help achieve these goals, such as improving data quality, developing more energy-efficient execution models, and optimizing energy efficiency in model training and inference. These innovation approaches highlight the potential of green AI to challenge the prevailing paradigm of ever-growing models.} }
@article{10.1145/3705309, title = {Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3705309}, url = {https://doi.org/10.1145/3705309}, author = {Noei, Shayan and Li, Heng and Zou, Ying}, keywords = {Code Refactoring, Refactoring Detection, Python Refactoring, Machine Learning, Code Quality}, abstract = {Refactoring aims to improve the quality of software without altering its functional behaviors. Understanding developers’ refactoring activities is essential to improve software maintainability. The use of machine learning (ML) libraries and frameworks in software systems has significantly increased in recent years, making the maximization of their maintainability crucial. Due to the data-driven nature of ML libraries and frameworks, they often undergo a different development process compared to traditional projects. As a result, they may experience various types of refactoring, such as those related to the data. The state-of-the-art refactoring detection tools have not been tested in the ML technical domain, and they are not specifically designed to detect ML-specific refactoring types (e.g., data manipulation) in ML projects; therefore, they may not adequately find all potential refactoring operations, specifically the ML-specific refactoring operations. Furthermore, a vast number of ML libraries and frameworks are written in Python, which has limited tooling support for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations with relatively high precision. In contrast, for other languages such as Java, state-of-the-art tools are capable of detecting a much more comprehensive list of refactorings. For example, Rminer can detect 99 types of refactoring for Java projects. Inspired by previous work that leverages commit messages to detect refactoring, we introduce MLRefScanner, a prototype tool that applies ML techniques to detect refactoring commits in ML Python projects. MLRefScanner detects commits involving both ML-specific refactoring operations and additional refactoring operations beyond the scope of state-of-the-art refactoring detection tools. To demonstrate the effectiveness of our approach, we evaluate MLRefScanner on 199 ML open source libraries and frameworks and compare MLRefScanner against other refactoring detection tools for Python projects. Our findings show that MLRefScanner outperforms existing tools in detecting refactoring-related commits, achieving an overall precision of 94\% and recall of 82\% for identifying refactoring-related commits. MLRefScanner can identify commits with ML-specific and additional refactoring operations compared to state-of-the-art refactoring detection tools. When combining MLRefScanner with PyRef, we can further increase the precision and recall to 95\% and 99\%, respectively. MLRefScanner provides a valuable contribution to the Python ML community, as it allows ML developers to detect refactoring-related commits more effectively in their ML Python projects. Our study sheds light on the promising direction of leveraging machine learning techniques to detect refactoring activities for other programming languages or technical domains where the commonly used rule-based refactoring detection approaches are not sufficient.} }
@inproceedings{10.1145/3736181.3747139, title = {Introducing Machine Learning to Children in Nigeria}, booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 1}, pages = {204--210}, year = {2025}, isbn = {9798400719295}, doi = {10.1145/3736181.3747139}, url = {https://doi.org/10.1145/3736181.3747139}, author = {John, Avong Emmanuel and Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday}, keywords = {basic education, children, machine learning education, nigeria, location = Gaborone, Botswana}, abstract = {This study explores the potential of machine learning (ML) education for children in Nigeria, addressing the significant dearth of such initiatives in Africa. We propose the design of ML intervention programs aimed at creating engaging learning experiences for children aged 7 to 16 years, who have no prior exposure to ML concepts. Through hands-on activities using online platforms, LearningML, which introduces classification techniques and image/text recognition and DoodleIt, which focuses on teaching convolutional neural networks, we investigate how these tools facilitate learning in a non-formal setting. Employing a mixed-method approach that includes pre- and post-surveys, as well as participant interviews, we analyze the data using descriptive and thematic methods. Our findings reveal that children grasp the importance of data model, training and other fundamental ML concepts, demonstrating that user-friendly ML tools can ignite curiosity and foster a desire to learn. The implications of this study are significant for advancing ML education among young learners in Africa.} }
@article{10.1145/3737650, title = {Race Against the Machine Learning Courses}, journal = {ACM Trans. Intell. Syst. Technol.}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3737650}, url = {https://doi.org/10.1145/3737650}, author = {Deshpande, Riddhi and Mlombwa, Donald and Celi, Leo Anthony and Gallifant, Jack and D’couto, Helen}, keywords = {1.32 Tutoring and educational systems, 1 Systems and Applications, 2.6 Highly scalable AI algorithms, 2 AI Technology, 1.20 Medical and health systems, 1 Systems and Applications, 1.14 AI in science, 1 Systems and Applications}, abstract = {Despite the rapid integration of AI in healthcare, a critical gap exists in current machine learning courses: the lack of education on identifying and mitigating bias in datasets. This oversight risks perpetuating existing health disparities through biased AI models. Analyzing 11 prominent online courses, we found only 5 addressed dataset bias, often dedicating minimal time compared to technical aspects. This paper urges course developers to prioritize education on data context, equipping learners with the tools to critically evaluate the origin, collection methods, and potential biases inherent in the data. This approach fosters the creation of fair algorithms and the incorporation of diverse data sources, ultimately mitigating the harmful effects of bias in healthcare AI. While this analysis focused on publicly available courses, it underscores the urgency of addressing bias in all healthcare machine learning education. Early intervention in algorithm development is crucial to prevent the amplification of dataset and model bias, ensuring responsible and equitable AI implementation in healthcare.} }
@inproceedings{10.1145/3674029.3674033, title = {Diabetes Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {16--20}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674033}, url = {https://doi.org/10.1145/3674029.3674033}, author = {Tian, Stephanie and Hui, Guanghui}, keywords = {Binary classification, Deep Neural Network (DNN), Diabetes prediction, Machine Learning (ML), Modified Sigmoid Function, location = Oslo, Norway}, abstract = {Machine learning (ML) techniques for healthcare informatics provide health professional insight into disease development. Many healthcare topics are suitable for ML research, such as diabetes prediction and classification. Common ML approaches use a classification method to predict the outcome of the disease for given test data, though these solutions tend to have limited accuracy rates. Further tuning with extra manipulation of the dataset helps improve the model's accuracy to a certain level, but this requires certain professional knowledge in the medical domain. In this research, we propose using a DNN (Deep Neural Network) approach to predict the outcome of diabetes from the test data. Based on the dataset statistics, we simply transform 1D diabetes test data arrays to 2D Farrays without complex medical knowledge. We use a 2D convolution function to extract the features for prediction in addition to modifying the final stage activation function, to which the response is similar to a unit step function for binary classification problems. Our DNN model prediction accuracy has improved over the known non-deep learning classification models.} }
@inproceedings{10.1145/3711896.3737860, title = {8th Workshop on Machine Learning in Finance}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6288--6289}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737860}, url = {https://doi.org/10.1145/3711896.3737860}, author = {Nagrecha, Saurabh and Chaturvedi, Isha and Kumar, Senthil and Chawla, Nitesh and Das, Mahashweta and Yadav, Daksha and Rodriguez-Serrano, Jose A. and Kurshan, Eren}, keywords = {ai, finance, genai, machine learning, location = Toronto ON, Canada}, abstract = {The financial industry leverages machine learning in more ways than just finding the right alpha signal. It grapples with supply chains, business processes, marketing, churn, fraud, and money laundering, all while maintaining compliance with the various regulatory frameworks it is beholden to. Due to the sheer volume of wealth being handled by the financial industry and its critical role in everyday life, it has been a lucrative target for a wide spectrum of ever-evolving bad actors. With each successive iteration of this workshop, we have attempted to capture the breadth of these actors - fraudsters, money launderers, market manipulators, and potentially nation-state-level risks. The emerging advances in Generative AI make this a particularly exciting time to host this workshop. GenAI offers groundbreaking approaches to handling the various data types prevalent in the financial sector. From a security point of view, bad actors are actively using Generative AI creatively to thwart conventional defenses (e.g. voice cloning, better synthetic identities), and this workshop's audience would benefit from commonly applicable defenses \&amp; best practices against such threats. Last but not the least, there is now an increasing willingness from the financial industry towards deeper engagement and data sharing with academia.} }
@inproceedings{10.1145/3696271.3696274, title = {Customer Clusterization using Machine Learning Approach}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {15--19}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696274}, url = {https://doi.org/10.1145/3696271.3696274}, author = {Purnamasari, Fanindia and Putri Nasution, Umaya Ramadhani and Elveny, Marischa and Hayatunnufus, Hayatunnufus}, keywords = {clustering, customer segmentation, k-means, machine learning, silhouette}, abstract = {Understanding customer is crucial for marketing strategies and increasing customer satisfaction in today's business environment. One method to fulfill of marketing strategies is segment customer based on their purchasing habits and demographic characteristics. This study describes a complete approach to customer segmentation based on K-means clustering, an unsupervised machine learning algorithm. There are three stages namely preprocessing to select feature and variable is used to develop clustering model, clustering model implementation, and validation of model. There are four clusters that compare the relationship of marital status and recency to the grocery purchases (product) made by each customer to find out which ingredients we will use to make better products for customers.} }
@inproceedings{10.1145/3623509.3633370, title = {Embodied Machine Learning}, booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}, year = {2024}, isbn = {9798400704024}, doi = {10.1145/3623509.3633370}, url = {https://doi.org/10.1145/3623509.3633370}, author = {Bakogeorge, Alexander and Imtiaz, Syeda Aniqa and Abu Hantash, Nour and Manshaei, Roozbeh and Mazalek, Ali}, keywords = {Embodied interaction, collaboration, machine learning, medical data, tabletop interaction, trust, location = Cork, Ireland}, abstract = {Machine learning becomes more prevalent in specialized domains such as medicine and biology every year, but domain expert trust in machine learning continues to lag behind. Researchers have explored increasing rational trust in AI but little research exists focusing on systems that foster affective and normative trust between domain experts and data scientists who create the models. Tools like Project Jupyter have attempted to bridge this gap between data scientists and domain experts, but failed to see uptake in applied fields or to promote collaboration through co-located synchronous work. To address this we present a proof-of-concept tabletop interactive machine learning system for synchronous, co-located model fine tuning. We tested our system with biology experts and data scientists on a cell biology dataset. Results show that our system promotes interactions between domain experts, data scientists, and the model-in-training and fosters domain expert affective and normative trust in the resulting AI model.} }
@inproceedings{10.1145/3711896.3736560, title = {Data Heterogeneity Modeling for Trustworthy Machine Learning}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6086--6095}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3736560}, url = {https://doi.org/10.1145/3711896.3736560}, author = {Liu, Jiashuo and Cui, Peng}, keywords = {data heterogeneity, out-of-distribution generalization, stability, trustworthy machine learning, location = Toronto ON, Canada}, abstract = {Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline-from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.} }
@inproceedings{10.1145/3641525.3663630, title = {The Idealized Machine Learning Pipeline (IMLP) for Advancing Reproducibility in Machine Learning}, booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability}, pages = {110--120}, year = {2024}, isbn = {9798400705304}, doi = {10.1145/3641525.3663630}, url = {https://doi.org/10.1145/3641525.3663630}, author = {Zheng, Yantong and Stodden, Victoria}, keywords = {Computational Reproducibility, CyberInfrastructure, Human Factors, Machine Learning, Open Code, Open Data, Reproducibility Policy, location = Rennes, France}, abstract = {We investigate the influence of Human Factors on reproducible machine learning, using a novel “Idealized Machine Learning Pipeline” we introduce as a conceptual framework. The study of Human Factors seeks to ensure that systems meet the needs and expectations of people who use and interact with these systems. It also, importantly, seeks to ensure that the capabilities and limitations of those people are accommodated by the system. As increasing the reproducibility of Machine Learning continues to be a community priority, we believe an improved understanding the Human Factors associated with the complex human-machine system of the Machine Learning pipeline could help facilitiate useful steps toward reproducibility. To do this, we first define a practical abstraction of the steps that comprise a typical Machine Learning pipeline for a well-known use case, from raw data through to model estimation used for inference and prediction, that we call the “Idealized Machine Learning Pipeline.” We emphasize that our proposed “Idealized Machine Learning Pipeline” is intended as an abstraction, rather than a directive or description of all Machine Learning workflows, and meant to harmonize and integrate different viewpoints, expertise, and other elements vital to Machine Learning research. Our goal is to enable the research community to coalesce around priorities for theoretical and applied research on reproducible Machine Learning including tools, frameworks, comparisons, and policies, and provide a foundation for communicating and teaching its myriad aspects as an integrated whole. We define the principal steps as follows: 0) Documentation, 1) Problem Definition, 2) Input Data, 3) Data Preparation, 4) Feature Selection, 5) Model Training, 6) Model Evaluation, 7) Preservation \&amp; Publication. To then understand barriers and opportunities attending to the adoption of reproducible Machine Learning pipelines, we leverage the “Idealized Machine Learning Pipeline” for our chosen use case to identify and motivate relevant Human Factors that affect various steps in a reproducible Machine Learning pipeline. We find the identified Human Factors fall into three groups: Incentives; Training; and Error-based Human Factors.} }
@inproceedings{10.1145/3631461.3632516, title = {Distributed Machine Learning}, booktitle = {Proceedings of the 25th International Conference on Distributed Computing and Networking}, pages = {4--7}, year = {2024}, isbn = {9798400716737}, doi = {10.1145/3631461.3632516}, url = {https://doi.org/10.1145/3631461.3632516}, author = {Chatterjee, Bapi}, keywords = {Distributed Machine Learning, Federated Learning, Machine Learning, location = Chennai, India}, abstract = {We explore the landscape of distributed machine learning, focusing on advancements, challenges, and potential future directions in this rapidly evolving field. We delve into the motivation for distributed machine learning, its essential techniques, real-world applications, and open research questions. The theoretical discussion will give an overview of proving the convergence of popular Stochastic Gradient Descent (SGD) Algorithms to train contemporary machine learning models, including the deep learning models with the assumption of non-convexity, in a distributed setting. We will specify the convergence of data parallel SGD for various distributed systems properties, such as asynchronous and compressed communication. We will also discuss distributed machine learning techniques such as model parallelism and tensor parallelism to train large language models (LLMs).} }
@inproceedings{10.1145/3748355.3748363, title = {Empowering machine-learning assisted kernel decisions with eBPFML}, booktitle = {Proceedings of the 3rd Workshop on EBPF and Kernel Extensions}, pages = {28--30}, year = {2025}, isbn = {9798400720840}, doi = {10.1145/3748355.3748363}, url = {https://doi.org/10.1145/3748355.3748363}, author = {Sodhi, Prabhpreet Singh and Liargkovas, Georgios and Kaffes, Kostis}, keywords = {Operating systems, eBPF, hardware acceleration, machine learning, location = Coimbra, Portugal}, abstract = {Machine-learning (ML) techniques can optimize core operating system paths---scheduling, I/O, power, and memory---yet practical deployments remain rare. Existing prototypes either (i) bake simple heuristics directly into the kernel or (ii) off-load inference to user space to exploit discrete accelerators, both of which incur unacceptable engineering or latency cost. We argue that eBPF, the Linux kernel's safe, hot-swappable byte-code runtime, is the missing substrate for moderately complex in-kernel ML. We present eBPFML, a design that (1) extends the eBPF instruction set with matrix-multiply helpers, (2) leverages upcoming CPU matrix engines such as Intel Advanced Matrix Extensions (AMX) through the eBPF JIT, and (3) retains verifier guarantees and CO-RE portability.} }
@article{10.1145/3729432, title = {Performance Evaluation for Detecting and Alleviating Biases in Predictive Machine Learning Models}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3729432}, url = {https://doi.org/10.1145/3729432}, author = {Khakurel, Utsab and Abdelmoumin, Ghada and Rawat, Danda B.}, keywords = {machine learning, bias, identification, mitigation, predictions, classification, ethical AI}, abstract = {Machine Learning (ML) is widely used in various domains but is susceptible to biases that can lead to unfair decisions. Bias can arise from biased data, algorithms, or data collection processes, making it crucial to develop methods that ensure fairness. This article introduces the Detect and Alleviate Bias (DAB) framework, a novel approach designed to identify and mitigate bias in ML models, focusing on sensitive attributes such as gender and race. The key contributions of DAB include: (1) a holistic pipeline that combines data pre-processing, model enhancement, situation testing, and bias mitigation techniques; (2) the application of Counterfactual Fairness testing to detect individual biases; and (3) the integration of multiple bias mitigation strategies to improve fairness in binary classification tasks. We demonstrate the practical implications of DAB through empirical experiments on two widely used datasets, showing that it reduces bias and improves fairness with a slight compromise in model performance, with a significant potential for making an impact in sensitive domains such as healthcare and criminal justice. The results show that pre-processing and post-processing mitigation techniques achieve the best improvements in fairness for unrepresentative datasets, while in-processing methods are more effective in enhancing fairness for representative datasets; however, an inverse relationship between model performance and fairness is observed.} }
@inproceedings{10.1145/3711896.3737246, title = {Maturity Framework for Enhancing Machine Learning Quality}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {4296--4307}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737246}, url = {https://doi.org/10.1145/3711896.3737246}, author = {Castelli, Angelantonio and Chouliaras, Georgios Christos and Goldenberg, Dmitri}, keywords = {machine learning maturity framework, machine learning quality, quality framework, reproducibility, location = Toronto ON, Canada}, abstract = {With the rapid integration of Machine Learning (ML) in business applications and processes, it is crucial to ensure the quality, reliability and reproducibility of such systems. We suggest a methodical approach towards ML system quality assessment and introduce a structured Maturity framework for governance of ML. We emphasize the importance of quality in ML and the need for rigorous assessment, driven by issues in ML governance and gaps in existing frameworks. Our primary contribution is a comprehensive open-sourced quality assessment method, validated with empirical evidence, accompanied by a systematic maturity framework tailored to ML systems. Drawing from applied experience at Booking.com, we discuss challenges and lessons learned during large-scale adoption within organizations. The study presents empirical findings, highlighting quality improvement trends and showcasing business outcomes. The maturity framework for ML systems, aims to become a valuable resource to reshape industry standards and enable a structural approach to improve ML maturity in any organization.} }
@article{10.1145/3701031, title = {Overview of Multimodal Machine Learning}, journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.}, volume = {24}, year = {2025}, issn = {2375-4699}, doi = {10.1145/3701031}, url = {https://doi.org/10.1145/3701031}, author = {Al-Zoghby, Aya M. and Al-Awadly, Esraa Mohamed K. and Ebada, Ahmed Ismail and Awad, Wael A.}, keywords = {Multimodal Machine Learning(MML), Natural Language Processing(NLP), Deep Learning(DL), Fusion Techniques, Cross-Modal}, abstract = {Human nature is fundamentally driven by the need for interaction and attention, which are fulfilled through various sensory modalities, including hearing, sight, touch, taste, and smell. These senses enable us to perceive, understand, and engage with the world around us. The quality and depth of our interactions change considerably when we use multiple senses simultaneously, highlighting the importance of multimodal interactions in our daily lives. In the realm of technology, multimodal integration offers immense value, as it aims to create systems that can replicate or complement these natural human abilities for enhanced interaction.This article explores the significance of spatial multimodalities in machine learning, highlighting their role in improving model performance in applications such as autonomous driving, healthcare, and virtual assistants. It addresses challenges like the complexity of fusing diverse sensory data types and proposes solutions such as advanced data fusion techniques, adaptive learning algorithms, and transformer architectures. The goal is to provide an overview of state-of-the-art research and future directions for advancing human–computer interaction.} }
@inproceedings{10.1145/3696271.3696276, title = {Smart Drying with Machine Learning Methods}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {27--33}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696276}, url = {https://doi.org/10.1145/3696271.3696276}, author = {Chandra, Nicholas Li Jian and Chen, Zhiyuan and Law, Chung Lim}, keywords = {Fuzzy Logic, Moisture Content, Random Forest, Smart Drying, Support Vector Machine}, abstract = {This research aims to improve the prediction of process kinetics in food drying by evaluating and comparing various machine learning models and a hybrid model. Traditional methods for modeling the thermal processes in drying agricultural commodities often fall short due to their complexity. This study investigates four commonly used algorithms in this research domain—Artificial Neural Networks (ANN), Fuzzy Logic (FL), Random Forest (RF), Support Vector Machine (SVM)—and one hybrid model, Fuzzy SVM (FSVM). Data preprocessing included handling missing values, scaling numerical features, and one-hot encoding categorical variables. The models were evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²). Results indicated that the RF model with 100 decision trees and the ANN with 1000 epochs provided promising accuracy. However, the FSVM hybrid model did not demonstrate enhanced predictive capabilities beyond those of its base models.} }
@inproceedings{10.1145/3658644.3690862, title = {Privacy Analyses in Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {5110--5112}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690862}, url = {https://doi.org/10.1145/3658644.3690862}, author = {Ye, Jiayuan}, abstract = {Machine learning models sometimes memorize sensitive training data features, posing privacy risks. To control such privacy risks, Dwork et al. proposed the definition of differential privacy (DP) to measure the privacy risks of an algorithm. However, existing DP models either have significantly lower accuracy than their non-private variants or are computationally expensive to train by requiring to incorporate a large amount of public prior knowledge in terms of data or a large pre-trained model. Thus, the fundamental problem of efficiently training an accurate model while preserving DP is not fully addressed. To tackle this problem, we investigate the potential of improving privacy analysis of machine learning algorithms, thus subsequently allowing improved privacy-utility trade-off. First, we observe that the standard DP bound is not tight for large, overparameterized models. Specifically, the DP bound worsens with the number of iterations, and the privacy-accuracy trade-off worsens with the model dimension. This is despite the algorithm converging during training and the finite dimension of training data space. Such potential untightness is more severe for a realistic adversary that does not observe all model parameters, where prior works suggest empirical privacy amplification, and we investigate theoretically. Finally, we take a close look at the privacy risk of each model prediction about individual training data and analyze how to attribute privacy risk to the properties of the data and the choice of model (e.g., architectures). If successful, our research would enable tighter and more informative privacy bounds for differentially private learning, thus in turn allowing improved privacy-utility trade-offs.} }
@inproceedings{10.1145/3747227.3747245, title = {Research on Intrusion Detection Based on Interpretable Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {109--114}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747245}, url = {https://doi.org/10.1145/3747227.3747245}, author = {Chen, Mao and Ma, Bowen and Jiang, Hao}, keywords = {Deep neural networks, Explainable machine learning, Feature importance analysis, Intrusion detection, Port scanning}, abstract = {Intrusion Detection Systems (IDS) grapple with challenges such as delayed updates and a high false positive rate in traditional rule-based approaches under high-dimensional network traffic data. Although deep learning has significantly improved detection accuracy, the conflict between its "black box" nature and traceability requirements still persists. This paper focuses on port scanning attack detection and provides a “dual-wheel-driven” solution of "architecture optimization and interpretability enhancement" to build a high-precision interpretable learning framework. To tackle the vanishing gradient and overfitting problems of traditional Deep Neural Networks (DNN), an enhanced DNN model architecture is designed: through dynamic residual structure, "expansion-compression" design, and adaptive regularization strategy, the learning ability of high-dimensional network traffic features is improved. Moreover, to address the common "black-box" problem in current models, this paper introduces a feature importance quantification method based on gradient-weighting, which integrates dynamic prediction confidence weights to enhance the interpretability analysis of model decisions. In this study, the CICIDS-2017 dataset is chosen for training and evaluation. The results indicated that the enhanced DNN model had significantly better accuracy on the test set than traditional DNN and traditional machine learning models. More importantly, the enhanced DNN model can also identify key attack features, providing high-precision and transparent methodological support for cybersecurity defense.} }
@inproceedings{10.1145/3708360.3708380, title = {Research on Multi-Factor Investment Strategies Utilizing Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {123--126}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708380}, url = {https://doi.org/10.1145/3708360.3708380}, author = {Wang, Bojing}, keywords = {machine learning, machine learning, multi-factor, stock selection strategy}, abstract = {In today's complex and changeable financial market, stock selection strategies have always been the focus of investors' attention. The application of machine learning technology in finance is becoming increasingly widespread. As a classic investment approach, the multi-factor stock selection strategy, when combined with machine learning techniques, is expected to enhance the accuracy and effectiveness of stock selection. This paper uses the overall and individual stock performance of the Shanghai 50 index from June 2023 to June 2024 as a case study to analyze a multi-factor stock selection strategy based on machine learning.} }
@article{10.1145/3708479, title = {Bayesian Machine Learning Meets Formal Methods: An Application to Spatio-Temporal Data}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3708479}, url = {https://doi.org/10.1145/3708479}, author = {Vana-G\"ur, Laura and Visconti, Ennio and Nenzi, Laura and Cadonna, Annalisa and Kastner, Gregor}, keywords = {Bayesian predictive inference, spatio-temporal models, formal verification methods, posterior predictive verification, urban mobility}, abstract = {We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in machine learning, with formal methods, rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.} }
@article{10.1145/3768158, title = {Physics-informed Machine Learning for Medical Image Analysis}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3768158}, url = {https://doi.org/10.1145/3768158}, author = {Banerjee, Chayan and Nguyen, Kien and Salvado, Olivier and Tran, Truyen and Fookes, Clinton}, keywords = {Physics-informed, physics-guided, physics-constrained, PINNs, neural networks, medical image analysis}, abstract = {The incorporation of physical information in machine learning frameworks is transforming medical image analysis (MIA). Integrating fundamental knowledge and governing physical laws not only improves analysis performance but also enhances the model’s robustness and interpretability. This work presents a systematic review of over 100 articles on the utility of PINNs dedicated to MIA (PIMIA) tasks. We propose a unified taxonomy to investigate what physics knowledge and processes are modeled, how they are represented, and the strategies to incorporate them into MIA models. We delve deep into a wide range of image analysis tasks, from imaging, generation, prediction, inverse imaging (super-resolution and reconstruction), registration, and image analysis (segmentation and classification). For each task, we thoroughly examine and present the central physics-guided operation, the region of interest (with respect to human anatomy), the corresponding imaging modality, the datasets used for model training, the deep network architectures employed, and the primary physical processes, equations, or principles utilized. Additionally, we also introduce a novel metric to compare the performance of PIMIA methods across different tasks and datasets. Based on this review, we summarize and distill our perspectives on the challenges, and highlight open research questions and directions for future research.} }
@inbook{10.1145/3696630.3728523, title = {Learning to Edit Interactive Machine Learning Notebooks}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {681--685}, year = {2025}, isbn = {9798400712760}, url = {https://doi.org/10.1145/3696630.3728523}, author = {Jin, Bihui and Wang, Jiayue and Nie, Pengyu}, abstract = {Machine learning (ML) developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Notebooks provide a convenient tool for writing ML pipelines and interactively observing outputs. However, maintaining notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the ML pipeline code. Moreover, there is no existing benchmark related to developer edits on notebooks.In this paper, we present early results of the first study on learning to edit ML pipeline code in notebooks using large language models (LLMs). We collect the first dataset of 48,398 notebook edits derived from 20,095 revisions of 792 ML-related GitHub repositories. Our dataset captures granular details of file-level and cell-level modifications, offering a foundation for understanding real-world maintenance patterns in ML pipelines. We observe that the edits on notebooks are highly localized. Although LLMs have been shown to be effective on general-purpose code generation and editing, our results reveal that the same LLMs, even after finetuning, have low accuracy on notebook editing, demonstrating the complexity of real-world ML pipeline maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing LLMs' capabilities in engineering ML code.} }
@article{10.5555/3722577.3722593, title = {Localized debiased machine learning: efficient inference on quantile treatment effects and beyond}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Kallus, Nathan and Mao, Xiaojie and Uehara, Masatoshi}, keywords = {causal inference, Neyman orthogonality, cross-fitting, instrumental variables, conditional value at risk, expectiles}, abstract = {We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisance functions that depend on the target parameter as an input. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Existing approaches based on flexibly estimating the nuisances and plugging in the estimates, such as debiased machine learning (DML), require we learn the nuisance at all possible inputs. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the target parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or exible relationships, as we demonstrate in empirical studies.} }
@inproceedings{10.1145/3749096.3750027, title = {Towards Blind Quantum Machine Learning in Entanglement Networks}, booktitle = {Proceedings of the 2nd Workshop on Quantum Networks and Distributed Quantum Computing}, pages = {8--13}, year = {2025}, isbn = {9798400720970}, doi = {10.1145/3749096.3750027}, url = {https://doi.org/10.1145/3749096.3750027}, author = {de Abreu, Diego Medeiros and Abel\'em, Ant\^onio}, keywords = {Blind Quantum Computing, Quantum Networks, location = Coimbra, Portugal}, abstract = {Blind Quantum Computation (BQC) enables clients to delegate quantum computations to a quantum server while maintaining the privacy of their data and algorithms, even when the server is untrusted. In this work, we extend BQC frameworks to Quantum Machine Learning (QML) by implementing a network of entangled clients and a quantum server. Specifically, we explore the integration of Variational Quantum Classifiers (VQC) and Quantum Convolutional Neural Networks (QCNNs) within this paradigm. Our proposed model allows clients to perform classical preprocessing and optimization locally while leveraging the quantum server for computationally expensive quantum tasks. The entanglement-based network is managed by a controller that dynamically allocates resources according to the BQC protocol, allowing secure and efficient execution. We present simulation results indicating the feasibility of this approach, including an analysis of network efficiency and resource consumption, alongside the F1 score on QML benchmark datasets.} }
@inproceedings{10.1145/3637528.3671488, title = {Machine Learning in Finance}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6703}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671488}, url = {https://doi.org/10.1145/3637528.3671488}, author = {Akoglu, Leman and Chawla, Nitesh and Domingo-Ferrer, Josep and Kurshan, Eren and Kumar, Senthil and Naware, Vidyut and Rodriguez-Serrano, Jose A. and Chaturvedi, Isha and Nagrecha, Saurabh and Das, Mahashweta and Faruquie, Tanveer}, keywords = {ai, finance, genai, machine learning, location = Barcelona, Spain}, abstract = {This workshop aims to explore the intersection of Generative AI with the rich tapestry of financial data types, seeking to uncover new methodologies and techniques that can enhance predictive analytics, fraud detection, and customer insights across the sector. By harnessing these advancements in AI, we can pave the way to not only understand customer behavior but also anticipate their needs more effectively, leading to superior customer outcomes and more personalized services. Our objective is to shed light on the challenges and opportunities presented by the diverse data formats in finance. We aim to bridge the gap between the dominance of traditional models for tabular data analysis and the emerging potential of Generative AI to revolutionize the treatment of time series, click streams, and other unstructured data forms.} }
@inproceedings{10.1145/3759023.3759127, title = {Using Machine Learning Algorithms to Classify Recyclable Waste}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759127}, url = {https://doi.org/10.1145/3759023.3759127}, author = {Sibanda, Mthokozisi and Bradshaw, Karen}, keywords = {Image classification, Transfer learning, Convolutional neural network}, abstract = {Waste management poses a significant global challenge, with 90\% of South Africa’s waste directed to landfills, leading to environmental and health risks. In many regions in Africa, manual waste sorting exposes workers to hazardous conditions, underscoring the need for safer, more efficient recycling practices. This research explores the possibility of removing human involvement in waste recycling through the application of image processing and machine learning techniques, specifically using convolutional neural networks (CNNs) with transfer learning, to automate the sorting of recyclable materials to identify recyclable waste. By accurately classifying waste into categories such as paper, plastics, glass, metals, cardboard, and other waste, the proposed model aims to reduce landfill waste, promote recycling, and create safer working environments.Several proof-of-concept pretrained models are used for binary and multi-class classification tasks, highlighting the performance of CNNs in distinguishing waste types. While binary classification demonstrates high accuracy for specific waste categories, multi-class classification reflects real-world complexities in sorting mixed waste streams. Though limited in its effectiveness for class imbalance, data augmentation points to potential improvements through oversampling techniques. Ultimately, this research could contribute to the development of an automated waste sorting system, enhancing recycling efficiency and reducing the environmental footprint of waste disposal.} }
@inproceedings{10.1145/3718391.3718417, title = {Malware Traffic Analysis using Machine Learning}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {62--67}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3718417}, url = {https://doi.org/10.1145/3718391.3718417}, author = {Ji, Jie and Mogos, Gabriela}, keywords = {Machine learning, malware, security}, abstract = {Malware refers to computer code or software that is installed and operated on a user's computer or other terminal without explicit notification or permission, engaging in activities such as stealing, encrypting, modifying, and deleting data, and monitoring the legitimate rights and interests of users. The types of malwares include viruses, worms, Trojans, ransomware, spyware, and so on. Different types of malwares have different attack methods and can cause different damages, resulting in potential financial losses for users. Five machine learning algorithms were used to conduct comparative analysis and find the best performing model to predict potential malware traffic issues in networks. We used the CIC-IDS-2017 dataset, Pearson correlation coefficient to select features and 5-fold cross validation to evaluate the model's generalization ability.} }
@inproceedings{10.1145/3715275.3732195, title = {The Data Minimization Principle in Machine Learning}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {3075--3093}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732195}, url = {https://doi.org/10.1145/3715275.3732195}, author = {Ganesh, Prakhar and Tran, Cuong and Shokri, Reza and Fioretto, Ferdinando}, keywords = {Data minimization, Privacy, Data Protection Regulations}, abstract = {The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches. Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations. However, its implementation remains a challenge due to the lack of a rigorous formulation. Our paper addresses this gap and introduces an optimization framework to operationalize the legal definitions of data minimization. It adapts several optimization algorithms to perform data minimization in machine learning and conducts a comprehensive evaluation of their compliance with minimization objectives and their impact on user privacy. Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for data minimization approaches that account for multiple facets of real-world privacy risks.} }
@inproceedings{10.1145/3702653.3744293, title = {Supporting Structured Problem-Solving in Machine Learning Education}, booktitle = {Proceedings of the 2025 ACM Conference on International Computing Education Research V.2}, pages = {63--64}, year = {2025}, isbn = {9798400713415}, doi = {10.1145/3702653.3744293}, url = {https://doi.org/10.1145/3702653.3744293}, author = {Witt, Clemens}, keywords = {Machine Learning Education, Problem-Solving Strategies, Stealth Assessment}, abstract = {This PhD project investigates how students develop problem-solving strategies in machine learning (ML) education. Applying a design-based research approach, it integrates stealth assessment techniques and adaptive feedback mechanisms to foster more structured and systematic engagement with complex ML learning tasks. Initial empirical findings inform the iterative development of a transferable framework to support systematic learning processes across diverse ML contexts, thereby advancing understanding of students’ problem-solving in ML education.} }
@inproceedings{10.1145/3670474.3685973, title = {Machine Learning for High Sigma Analog Designs (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685973}, url = {https://doi.org/10.1145/3670474.3685973}, author = {Jallepalli, Srinivas}, keywords = {Monte Carlo: machine learning, high sigma, importance sampling, parametric yield, scaled sigma sampling, statistical blockade, location = Salt Lake City, UT, USA}, abstract = {Monte Carlo simulations have been the gold standard for assessing parametric yields of analog, mixed signal, and RF circuits as they offer one of the most direct representations of the variation induced by semiconductor manufacturing. However, Monte Carlo analyses are often too expensive for understanding high sigma yields with fewer defects than 1000ppm. To quantify the impact of rare events on circuit yield, we need insights into their probability densities. All rare event sampling techniques that seek to provide this insight employ a machine learning flow of some kind. The various implementations of importance sampling and statistical blockade, for example, try to locate the rare event populations in parametric space through input domain mapping. Despite their popularity, they can sometimes pose a significant challenge, especially when the dimensionality of the input variation space is high, as both feature selection and machine learning can be non-trivial. A good alternative to machine learning in the input parametric domain is the innovative scaled sigma sampling technique that leverages machine learning of the probability density differences produced by scaling input standard deviations. This paper reviews these key approaches for determining the high sigma yields of analog circuits.} }
@inproceedings{10.1145/3745133.3745174, title = {Forecasting ESG Index Based on Machine Learning Methods}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Information Systems}, pages = {241--246}, year = {2025}, isbn = {9798400714375}, doi = {10.1145/3745133.3745174}, url = {https://doi.org/10.1145/3745133.3745174}, author = {Zhao, Juantong}, keywords = {ESG, machine learning, sustainable development, time series prediction}, abstract = {ESG (Environmental, Social, and Governance) is an indicator of corporate sustainability and responsible performance in terms of environmental, social and governance, as well as an important framework for measuring corporate non-financial performance, which is important for balancing economic efficiency with global sustainable development. This study constructs the impact factor system of ESG index in the aspects of environment, society and governance, and then constructs six machine learning models to predict ESG index. The results show that LSTM model performs best, and MSE is 0.14-25.14 times better than other machine learning models BP, CNN, GRU, RNN and MLP. This study provides a scalable, intelligent solution for ESG assessment and reveals the potential value of deep learning in sustainable finance.} }
@article{10.5555/3722479.3722535, title = {Finiteness Considerations in Machine Learning}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {250--262}, year = {2024}, issn = {1937-4771}, author = {Jackson, Jeffrey C.}, abstract = {Many machine learning textbooks include at least some coverage of one or both of the No Free Lunch theorems for learning and probably-approximately correct generalization error bounds. However, it is not a simple matter to reconcile the implications of these two topics and provide advice to students (and practitioners) regarding when learning claims such as "this learned hypothesis will be at least 95\% accurate on previously-unseen data" can reasonably be made. This paper shows how finiteness considerations can potentially provide such a reconciliation. It also suggests that finiteness considerations can be used to simplify certain generalization error bounds by eliminating their reliance on the VC-dimension of hypothesis classes, which might be of independent pedagogical interest.} }
@proceedings{10.1145/3721146, title = {EuroMLSys '25: Proceedings of the 5th Workshop on Machine Learning and Systems}, year = {2025}, isbn = {9798400715389}, abstract = {EuroMLSys gathers AI researchers and practitioners to share innovative advancements in software infrastructure, tools, design principles, theoretical foundations, algorithms, and applications—all viewed from a systems-oriented perspective and harnessing the power of machine learning.} }
@article{10.1145/3616537, title = {Byzantine Machine Learning: A Primer}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3616537}, url = {https://doi.org/10.1145/3616537}, author = {Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael}, keywords = {Byzantine machine learning, distributed SGD, robust aggregation}, abstract = {The problem of Byzantine resilience in distributed machine learning, a.k.a. Byzantine machine learning, consists of designing distributed algorithms that can train an accurate model despite the presence of Byzantine nodes—that is, nodes with corrupt data or machines that can misbehave arbitrarily. By now, many solutions to this important problem have been proposed, most of which build upon the classical stochastic gradient descent scheme. Yet, the literature lacks a unified structure of this emerging field. Consequently, the general understanding on the principles of Byzantine machine learning remains poor. This article addresses this issue by presenting a primer on Byzantine machine learning. In particular, we introduce three pillars of Byzantine machine learning, namely the concepts of breakdown point, robustness, and gradient complexity, to curate the efficacy of a solution. The introduced systematization enables us to (i) bring forth the merits and limitations of the state-of-the-art solutions, and (ii) pave a clear path for future advancements in this field.} }
@inproceedings{10.1145/3760544.3764125, title = {Machine Learning-Based Distance Estimation for Molecular Communication}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {134--138}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764125}, url = {https://doi.org/10.1145/3760544.3764125}, author = {Cheng, Zhen and Zheng, Jianlong and Xu, Ziyan}, keywords = {molecular communication, distance estimation, machine learning, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication (MC) transmits information through the release, diffusion, and reception of molecules, holding great potential in the field of drug delivery. In an MC system, the prediction of the distance between the transmitter and the receiver is crucial for the receiver's resource consumption. Traditional distance detection strategies mainly focus on known channel state information (CSI). To address this limitation, this paper proposes a method for estimating the distance between the transmitters and the receiver in MC system with unknown CSI using a deep neural network (DNN) model. We employ Monte Carlo simulation to capture the positions of molecules in a three-dimensional environment. The dataset is generated based on the molecular coordinates at each position. Numerical results indicate that the DNN model can accurately estimate the distance between the transmitters and the receiver, demonstrating good detection capabilities and generalization ability. Additionally, the minimum distance between the transmitters and the receiver's boundary also affects the accuracy of the distance estimation.} }
@inproceedings{10.1145/3728199.3728288, title = {Analysis of Passenger Satisfaction Evaluation Metrics Based on Machine Learning}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {538--543}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728288}, url = {https://doi.org/10.1145/3728199.3728288}, author = {Wang, Xiucui and Dumlao, Menchita F.}, keywords = {HistGBDT model, Passenger satisfaction, machine learning algorithms}, abstract = {The research in this paper is based on a dataset on airline passenger satisfaction downloaded from the Kaggle data platform, which contains 26 characteristics of passengers, totaling nearly 130,000 records. Using this data set, eight machine learning algorithms are used, such as GBDT, Decision tree, AdaBoost, XGBoost, Random forest, Extremerandom tree, CatBoost, HistGBDT. By analyzing the evaluation indexes of these 8 machine learning algorithms, it is concluded that HistGBDT algorithm has higher values in precision and accuracy. This result page verifies that HistGBDT model can improve the predictive ability of passenger satisfaction. Increase the generalization ability of their models. It also shows that the model is superior in predicting airline satisfaction. Through this result analysis, we can help airlines to better provide passengers with better quality services and meet their different needs, so as to provide airlines with more personalized services and decisions, and enhance their competitive advantages.} }
@inproceedings{10.1145/3728199.3728243, title = {Machine Learning Based Structural Design and Performance Prediction of Advanced Materials}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {269--272}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728243}, url = {https://doi.org/10.1145/3728199.3728243}, author = {You, Weichen}, keywords = {Graph neural network, Intelligent optimisation, Machine learning, Material structure design, Performance prediction}, abstract = {This paper explores the core methods of machine learning in material modelling, structure-property correlation analysis and optimal design, including data pre-processing, feature engineering, algorithm selection and model training strategies. Combining deep learning, graph neural network (GNN) and gradient boosting decision tree (GBDT) methods, accurate prediction of material properties can be achieved and material structures can be optimised by Bayesian optimisation, genetic algorithms and other intelligent search strategies. It is shown that the machine learning-based material design method can significantly improve the efficiency of material screening and accelerate the development process of new materials.} }
@inproceedings{10.1145/3722237.3722398, title = {Machine Learning and Graduate Education in Finance}, booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education}, pages = {937--943}, year = {2025}, isbn = {9798400712692}, doi = {10.1145/3722237.3722398}, url = {https://doi.org/10.1145/3722237.3722398}, author = {Zhang, Kan and Wang, Fengqingyang and Wang, Suze}, keywords = {Causal Inference, Data Analysis, Financial Education, Machine Learning, Prediction}, abstract = {This paper discusses the application of machine learning in graduate education in finance, emphasizing its importance in data analysis, prediction, and causal inference. The paper points out that machine learning can enhance students' professional competence, expand their knowledge areas, and enhance their work abilities. Through theoretical teaching and research applications, machine learning methods can help process large amounts of unstructured data and combine with traditional econometric methods to improve the accuracy and explanatory power of research. The survey results support the effectiveness of machine learning teaching. At the end of the paper, it is proposed to continuously update the teaching content of machine learning technology, while paying attention to its limitations, in order to promote the improvement of students' comprehensive abilities.} }
@inproceedings{10.1145/3673791.3698439, title = {Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, booktitle = {Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region}, pages = {299--302}, year = {2024}, isbn = {9798400707247}, doi = {10.1145/3673791.3698439}, url = {https://doi.org/10.1145/3673791.3698439}, author = {Diaz, Fernando and Drozdov, Andrew and Kim, To Eun and Salemi, Alireza and Zamani, Hamed}, keywords = {information retrieval, machine learning, location = Tokyo, Japan}, abstract = {Retrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.} }
@inproceedings{10.1145/3641555.3705218, title = {Mathematics for Machine Learning: A Bridge Course}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1437--1438}, year = {2025}, isbn = {9798400705328}, doi = {10.1145/3641555.3705218}, url = {https://doi.org/10.1145/3641555.3705218}, author = {Deng, Samuel}, keywords = {bridge course, curriculum, machine learning education, mathematics education, location = Pittsburgh, PA, USA}, abstract = {We present Mathematics for Machine Learning, a one-semester mathematics course designed to strengthen students' foundations before further study and research in machine learning (ML) and data science. Oftentimes, the mathematical prerequisites needed for serious study of ML are taught in a disjointed manner. Our course is designed to bridge this gap and provide emphasis on concepts heavily employed in modern ML, such as spectral analysis in linear algebra or convex optimization in calculus. We structured our course around the three "pillars'' of math that underlie much of modern ML: (i) linear algebra, (ii) calculus and optimization, and (iii) probability and statistics. Weaving each of these together is a central story --- all concepts, ideas, and proofs are introduced relative to two ubiquitous concepts in machine learning: least squares regression and gradient descent, providing a consistent anchoring narrative and constant motivation for mathematical ideas.} }
@inproceedings{10.1145/3746972.3747002, title = {Hybrid Machine Learning for Used Car Price Determinants}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {181--186}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747002}, url = {https://doi.org/10.1145/3746972.3747002}, author = {Feng, Jianli}, keywords = {Gradient Descent, Logistic Regression, Machine Learning, Multiple Linear Regression, Support Vector Machine}, abstract = {This study develops a methodological framework for used car valuation through comparative analysis of regression techniques. Initial factor significance evaluation was conducted using Multiple Linear Regression (MLR) and Logistic Regression (LR), identifying ten key pricing determinants. Twelve regression models were subsequently implemented with these predictors, revealing LR's superior performance over MLR in coefficient stability and interpretability. The predictive phase comparatively evaluates Gradient Descent (GD) and Support Vector Machine (SVM) algorithms, with systematic residual analysis demonstrating SVM's optimal prediction accuracy. The findings provide empirical guidance for machine learning applications in automotive residual value estimation.} }
@article{10.1145/3761823, title = {Integration of fNIRS and Machine Learning for Identifying Parkinson’s Disease}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761823}, url = {https://doi.org/10.1145/3761823}, author = {Sousani, Maryam and Rojas, Raul Fernandez and Preston, Elisabeth and Ghahramani, Maryam}, keywords = {Parkinson’s Disease, fNIRS, Machine Learning, Biomarker}, abstract = {Parkinson’s disease (PD) is a neurodegenerative disorder where early diagnosis is crucial for effective management. However, current diagnostic methods are often invasive or delayed, hindering early intervention. This study evaluates the effectiveness of combining functional near-infrared spectroscopy (fNIRS) with machine learning to distinguish individuals with PD from age-matched controls.Data were collected using fNIRS from 28 people with PD and 32 age-matched controls while performing the Timed Up and Go (TUG) test under three conditions: simple TUG, cognitive dual-task TUG and motor dual-task TUG. Changes in cerebral blood oxygenation in the prefrontal cortex (PFC) were analysed using four machine learning models: Support Vector Machine (SVM), K-Nearest Neighbours (KNN), Random Forest (RF) and Extreme Gradient Boosting (XGB), along with statistical analyses. Two feature selection models identified key features and channels for differentiating PD from controls.The SVM model achieved the highest accuracy (0.85 (pm) 0.35) in distinguishing PD from CG. Feature selection and statistical analysis showed that dual-task activities were more effective than simple tasks for distinguishing PD from CG. Specific PFC subregions exhibited distinct activation patterns, which could serve as potential biomarkers for PD detection. Combining fNIRS with machine learning shows promise for PD diagnosis, with dual-task activities enhancing accuracy. Further investigation into PFC subregion behaviour could reveal stronger biomarkers.} }
@inproceedings{10.1145/3724363.3729032, title = {Are Interactive Visualizations in Machine Learning Education Helping Students?}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {2--8}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729032}, url = {https://doi.org/10.1145/3724363.3729032}, author = {Rentea, Ilinca and Migut, Gosia and Krijthe, Jesse}, keywords = {controlled experiment, education, interactive visualizations, knowledge gain, machine learning, motivation, location = Nijmegen, Netherlands}, abstract = {With the fast integration of Machine Learning (ML) across industries, effective pedagogical strategies are essential for teaching this complex and evolving field. Machine Learning is now widely integrated into various university programs and introduced at earlier educational stages, including high school and secondary school. However, ML pedagogy lacks standardized teaching methods compared to other science-related subjects, which have established norms for topic introduction, teaching tools, and assessment methods. Inspired by other fields, this research explores the use of interactive visualizations in teaching ML topics, more specifically in teaching Gradient Descent (GD) and Principal Component Analysis (PCA). The target population consists of Computer Science and Engineering Bachelor students who have not yet followed any Machine Learning courses but have foundational knowledge in calculus, linear algebra, and statistics. The evaluation measures knowledge gained and student motivation, compared to a static version of the materials. Results show a significant positive effect in knowledge related to PCA with interactive visualizations, but no differences in knowledge gain for GD or in learning motivation for either topic. With these results, we contribute to the body of evidence-based teaching methods in Machine Learning and identify further research needed to generalize the effect of interactive visualizations as a teaching method for teaching ML basic concepts.} }
@inproceedings{10.1145/3711896.3737858, title = {3rd Workshop on Causal Inference and Machine Learning in Practice}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6290--6291}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737858}, url = {https://doi.org/10.1145/3711896.3737858}, author = {Lee, Jeong-Yoon and Pan, Jing and Wu, Yifeng and Harinen, Totte and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Yin, Sichao and Stevenson, Roland and Wang, Jingshen and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, keywords = {causal inference, machine learning, location = Toronto ON, Canada}, abstract = {The 3rd Workshop on Causal Inference and Machine Learning in Practice at KDD 2025 aims to bring together researchers, industry professionals, and practitioners to explore the application of causal inference within machine learning models. As causal machine learning techniques gain traction across industries, practical challenges related to trustworthiness, robustness, and fairness remain at the forefront. This workshop will provide a forum to discuss methodologies for evaluating causal models in real-world scenarios and explore innovative applications that integrate causal inference with generative AI (GenAI) and large language models (LLMs). Topics of interest include using GenAI and LLMs to facilitate causal inference tasks and leveraging causal inference techniques for evaluating and improving GenAI/LLM models. Building on the success of the previous workshop editions at KDD 2023 and KDD 2024, which attracted over 200 and 250 participants, respectively, this workshop will continue fostering collaboration between academia and industry. Through invited talks, contributed papers, and interactive discussions, we will address key challenges and opportunities at the intersection of causal inference and machine learning. As the field continues to evolve, this workshop serves as a crucial platform for knowledge exchange and innovation, driving forward the application of causal techniques in machine learning and AI.} }
@article{10.1145/3719663, title = {Machine Learning for Infectious Disease Risk Prediction: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3719663}, url = {https://doi.org/10.1145/3719663}, author = {Liu, Mutong and Liu, Yang and Liu, Jiming}, keywords = {Machine learning, data-driven modeling, epidemiology-inspired learning, infectious disease risk prediction, transmission dynamics characterization}, abstract = {Infectious diseases place a heavy burden on public health worldwide. In this article, we systematically investigate how machine learning (ML) can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation for using ML for infectious disease risk prediction. Next, we describe the development and application of various ML models for infectious disease risk prediction, categorizing them according to the models’ alignment with vital public health concerns specific to two distinct phases of infectious disease propagation: (1) the pandemic and epidemic phases (the P-E phases) and (2) the endemic and elimination phases (the E-E phases), with each presenting its own set of critical questions. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluations. We conclude with a discussion of open questions and future directions.} }
@article{10.1145/3725809, title = {Differentiable Economics: Strategic Behavior, Mechanisms, and Machine Learning}, journal = {Commun. ACM}, volume = {68}, pages = {80--88}, year = {2025}, issn = {0001-0782}, doi = {10.1145/3725809}, url = {https://doi.org/10.1145/3725809}, author = {Bichler, Martin and Parkes, David C.}, abstract = {Economics studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these agents. Game theory had a substantial impact on economic modeling because it allows us to model the outcome of such economic interaction while taking the incentives of individual agents into account. Mechanism design does the same when designing the rules of economic institutions. Unfortunately, these economic models have turned out to be computationally hard to solve. For example, finding equilibrium in some incomplete-information models of markets with continuous valuation and action spaces are hard in PP, and designing a revenue-maximizing multi-item auction is #P-hard. This computational complexity poses a fundamental barrier in modeling economic systems but is worst-case and considers non-generic instances. Differentiable economics describes a new approach to solving these central problems in the economic sciences. It uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems. In particular, neural networks and learning algorithms such as Stochastic Gradient Descent have been shown to be very effective. Machine learning has led to breakthroughs in many sciences, and it also holds the potential to fundamentally alter how we analyze and design economic systems.A new approach to solving central problems in the economic sciences uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems.} }
@article{10.1145/3643456, title = {Pitfalls in Machine Learning for Computer Security}, journal = {Commun. ACM}, volume = {67}, pages = {104--112}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3643456}, url = {https://doi.org/10.1145/3643456}, author = {Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad}, abstract = {With the growing processing power of computing systems and the increasing availability of massive datasets, machine learning algorithms have led to major breakthroughs in many different areas. This development has influenced computer security, spawning a series of work on learning-based security systems, such as for malware detection, vulnerability discovery, and binary code analysis. Despite great potential, machine learning in security is prone to subtle pitfalls that undermine its performance and render learning-based systems potentially unsuitable for security tasks and practical deployment.In this paper, we look at this problem with critical eyes. First, we identify common pitfalls in the design, implementation, and evaluation of learning-based security systems. We conduct a study of 30 papers from top-tier security conferences within the past 10 years, confirming that these pitfalls are widespread in the current security literature. In an empirical analysis, we further demonstrate how individual pitfalls can lead to unrealistic performance and interpretations, obstructing the understanding of the security problem at hand. As a remedy, we propose actionable recommendations to support researchers in avoiding or mitigating the pitfalls where possible. Furthermore, we identify open problems when applying machine learning in security and provide directions for further research.} }
@article{10.1145/3733838, title = {Practitioners and Bias in Machine Learning: A Study}, journal = {ACM Trans. Interact. Intell. Syst.}, volume = {15}, year = {2025}, issn = {2160-6455}, doi = {10.1145/3733838}, url = {https://doi.org/10.1145/3733838}, author = {Cinca, Robert and Costanza, Enrico and Musolesi, Mirco}, keywords = {ML Bias, Operationalizing Bias, machine learning, machine learning practitioners, interview study}, abstract = {The increasing adoption of machine learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with 22 participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.} }
@inproceedings{10.1145/3674029.3674050, title = {Machine Learning Tool for Wildlife Image Classification}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {127--132}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674050}, url = {https://doi.org/10.1145/3674029.3674050}, author = {Seljebotn, Karoline and Lawal, Isah A.}, keywords = {Camera Traps, Deep Learning, Wildlife Classification, location = Oslo, Norway}, abstract = {Wildlife researchers gather a large amount of image data during fieldwork. Reviewing this data is time-consuming and requires specialized expertise. To address this issue, machine learning models can automatically classify animals in these images. This study introduces a new method for classifying animals in both benchmark and camera trap images using a single model. The model achieved a top-1 accuracy of 93\% for benchmark images and 56\% for camera trap images previously unseen. The model was integrated into a web application, making it accessible to wildlife researchers without programming knowledge.} }
@inproceedings{10.1145/3627673.3679095, title = {Data Quality-aware Graph Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5534--5537}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679095}, url = {https://doi.org/10.1145/3627673.3679095}, author = {Wang, Yu and Ding, Kaize and Liu, Xiaorui and Kang, Jian and Rossi, Ryan and Derr, Tyler}, keywords = {data-centric artificial intelligence, graph machine learning, location = Boise, ID, USA}, abstract = {Recent years have seen a significant shift in Artificial Intelligence from model-centric to data-centric approaches, highlighted by the success of large foundational models. Following this trend, despite numerous innovations in graph machine learning model design, graph-structured data often suffers from data quality issues, jeopardizing the progress of Data-centric AI in graph-structured applications. Our proposed tutorial addresses this gap by raising awareness about data quality issues within the graph machine-learning community. We provide an overview of existing topology, imbalance, bias, limited data, and abnormality issues in graph data. Additionally, we highlight recent developments in foundational graph models that focus on identifying, investigating, mitigating, and resolving these issues.} }
@article{10.1145/3773898, title = {Eye-Tracking Indicators of Novice Programmers’ Proficiency: A Machine Learning Approach}, journal = {ACM Trans. Comput. Educ.}, year = {2025}, doi = {10.1145/3773898}, url = {https://doi.org/10.1145/3773898}, author = {Ahsan, Zubair and Obaidellah, Unaizah}, keywords = {Machine Learning, Computer Education, Human-Computer Interaction}, abstract = {This study investigates the efficacy of machine learning algorithms in classifying different levels of programming expertise among 60 first-year undergraduate computer science students from Asian demographic backgrounds using eye-tracking data. Existing studies offer limited detail on the construction and selection of feature sets for machine learning modeling. The study identifies Total Fixation Duration (TFD) and Total Visit Duration (TVD) as robust indicators for machine learning models when distinguishing between high and low performers (two levels) achieving accuracy as high as 76\%. However, performance declines notably when classifying expertise into three levels (high, average, and low), with performance dropping below 50\%, indicating that binary labels yield more reliable predictions than finer-grained categorization. Our findings suggest that such fixation-based metrics can provide real-time insights into student engagement and potentially cognitive effort, offering opportunities for adaptive instruction and targeted support. Hence, this model can be utilized for real-time screening of novice students during programming tasks in classroom settings, allowing educators to identify students requiring additional support, thereby enhancing programming education. Future research should address study limitations by increasing sample size, diversifying participant demographics, and cross-validating model performance with students’ grades.} }
@inproceedings{10.1145/3747227.3747246, title = {A Review of Machine Learning Algorithms Applied to Reservoir Exploration and Prediction}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {115--120}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747246}, url = {https://doi.org/10.1145/3747227.3747246}, author = {Lu, Bing and Wang, Hanqing and Zhao, Huilan and Song, Huilan and Li, Yan}, keywords = {Deep learning, Low-permeability reservoirs, Machine learning, Reservoir prediction}, abstract = {Reservoir exploration and prediction are highly complex tasks. They require knowledge from multiple disciplines. Key challenges include interpreting multidimensional geological data and evaluating low-permeability reservoirs. Traditional methods mainly rely on porosity and permeability. However, they often fail to reflect the heterogeneity and true storage capacity of tight reservoirs. In recent years, machine learning has become a promising alternative. Algorithms such as Support Vector Machines (SVM), Random Forests (RF), clustering, and deep learning models like CNNs and RNNs provide new tools for reservoir analysis. These methods are well-suited for handling complex, nonlinear, and high-dimensional data. In this work, we review recent developments in the use of machine learning for reservoir evaluation. It highlights how these techniques improve quantitative assessment and support better decision-making in oil and gas development.} }
@inproceedings{10.1145/3670865.3673573, title = {Machine Learning-Powered Course Allocation}, booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation}, pages = {1099}, year = {2024}, isbn = {9798400707049}, doi = {10.1145/3670865.3673573}, url = {https://doi.org/10.1145/3670865.3673573}, author = {Soumalias, Ermis and Zamanlooy, Behnoosh and Weissteiner, Jakob and Seuken, Sven}, keywords = {course allocation, preference elicitation, combinatorial assignment, location = New Haven, CT, USA}, abstract = {We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7\%--11\% and 17\%--29\%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM while making the upgrade process simple for universities and seamless for their students.} }
@article{10.1145/3736751, title = {Maintainability and Scalability in Machine Learning: Challenges and Solutions}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3736751}, url = {https://doi.org/10.1145/3736751}, author = {Shivashankar, Karthik and Al Hajj, Ghadi and Martini, Antonio}, keywords = {Machine learning, deep learning, maintainability, scalability}, abstract = {Rapid advancements in Machine Learning (ML) introduce unique maintainability and scalability challenges. Our research addresses the evolving challenges and identifies ML maintainability and scalability solutions by conducting a thorough literature review of over 17,000 papers, ultimately refining our focus to 124 relevant sources that meet our stringent selection criteria. We present a catalogue of 41 Maintainability and 13 Scalability challenges and solutions across Data, Model Engineering and the overall development of ML applications and systems. This study equips practitioners with insights on building robust ML applications, laying the groundwork for future research on improving ML system robustness at different workflow stages.} }
@article{10.5555/3648699.3648808, title = {Dimensionless machine learning: imposing exact units equivariance}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Villar, Soledad and Yao, Weichi and Hogg, David W. and Blum-Smith, Ben and Dumitrascu, Bianca}, abstract = {Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods that are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.} }
@article{10.1145/3709705, title = {Modyn: Data-Centric Machine Learning Pipeline Orchestration}, journal = {Proc. ACM Manag. Data}, volume = {3}, year = {2025}, doi = {10.1145/3709705}, url = {https://doi.org/10.1145/3709705}, author = {B\"other, Maximilian and Robroek, Ties and Gsteiger, Viktor and Holzinger, Robin and Ma, Xianzhe and T\"oz\"un, Pnar and Klimovic, Ana}, keywords = {data-centric ai, machine learning pipelines, online learning}, abstract = {In real-world machine learning (ML) pipelines, datasets are continuously growing. Models must incorporate this new training data to improve generalization and adapt to potential distribution shifts. The cost of model retraining is proportional to how frequently the model is retrained and how much data it is trained on, which makes the naive approach of retraining from scratch each time impractical. We present Modyn, a data-centric end-to-end machine learning platform. Modyn's ML pipeline abstraction enables users to declaratively describe policies for continuously training a model on a growing dataset. Modyn pipelines allow users to apply data selection policies (to reduce the number of data points) and triggering policies (to reduce the number of trainings). Modyn executes and orchestrates these continuous ML training pipelines. The system is open-source and comes with an ecosystem of benchmark datasets, models, and tooling. We formally discuss how to measure the performance of ML pipelines by introducing the concept of composite models, enabling fair comparison of pipelines with different data selection and triggering policies. We empirically analyze how various data selection and triggering policies impact model accuracy, and also show that Modyn enables high throughput training with sample-level data selection.} }
@inproceedings{10.1145/3749566.3749567, title = {Prediction of employment market trends for college students based on machine learning}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {1--5}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749567}, url = {https://doi.org/10.1145/3749566.3749567}, author = {Shao, Chen and Wang, Xinyan and Qiao, Shengnan and Liu, Shipeng}, keywords = {STEM field, employment market trends, machine learning}, abstract = {This article provides an in-depth prediction and analysis of the employment market trends for college students through machine learning algorithms. By collecting and processing relevant employment data from the past decade, a predictive model including time series analysis and regression analysis was established, aiming to provide valuable information on employment market trends for college students, educational institutions, and governments, and to provide scientific basis for understanding and adapting to the rapidly changing employment environment. The research results indicate that the development trend of the employment market for college students is influenced by various factors, among which economic environment, technological progress, and improvement in education level play a decisive role.} }
@inproceedings{10.1145/3664475.3664574, title = {Machine Learning \&amp; Neural Networks}, booktitle = {ACM SIGGRAPH 2024 Courses}, year = {2024}, isbn = {9798400706837}, doi = {10.1145/3664475.3664574}, url = {https://doi.org/10.1145/3664475.3664574}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {Use and development of computer systems that are able to learn and adapt without following explicit instructions by using algorithms and statistical models to analyze and draw inferences from patterns in data.} }
@article{10.1145/3735969, title = {Instrumental Variables in Causal Inference and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3735969}, url = {https://doi.org/10.1145/3735969}, author = {Wu, Anpeng and Kuang, Kun and Xiong, Ruoxuan and Wu, Fei}, keywords = {Causal machine learning, instrumental variable, control function, unmeasured confounders}, abstract = {Causal inference is the process of drawing conclusions about causal relationships between variables using a combination of assumptions, study designs, and estimation strategies. In machine learning, causal inference is crucial for uncovering the mechanisms behind complex systems and making informed decisions. This article provides a comprehensive overview of using Instrumental Variables (IVs) in causal inference and machine learning, with a focus on addressing unobserved confounding that affects both treatment and outcome variables. We review identification conditions under standard assumptions in the IV literature. In this article, we explore three key research areas of IV methods: Two-Stage Least Squares (2SLS) regression, control function (CFN) approaches, and recent advances in IV learning methods. These methods cover both classical causal inference approaches and recent advancements in machine learning research. Additionally, we provide a summary of available datasets and algorithms for implementing these methods. Furthermore, we introduce a variety of applications of IV methods in real-world scenarios. Lastly, we identify open problems and suggest future research directions to further advance the field. A toolkit of reviewed IV methods with machine learning (MLIV) is available at .} }
@inbook{10.1145/3728725.3728813, title = {Loan Default Prediction Based on Machine Learning Approaches}, booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security}, pages = {557--564}, year = {2025}, isbn = {9798400713453}, url = {https://doi.org/10.1145/3728725.3728813}, author = {Cai, Xinyu and Dai, Wenbo and Lu, Jingyu}, abstract = {To address the credit risk losses incurred by commercial banks due to loan defaults, this study utilizes the loan default prediction dataset from the Alibaba Tianchi platform to develop machine learning models for predicting customer defaults, aiming to mitigate credit risk. Given the characteristics of class imbalance and high dimensionality of loan data, data preprocessing and exploratory data analysis are conducted. Based on a comparative analysis of various models, seven machine learning algorithms that demonstrate superior performance are selected for experimental comparison, including Decision Tree, Random Forest, AdaBoost, Bagging, XGBoost, LightGBM, and CatBoost. The results indicate that ensemble learning algorithms exhibit higher accuracy and predictive performance compared to single algorithms, with the CatBoost model performing best across various indicators, including AUC. The study identifies key features highly correlated with loan defaults, including loan grade, annual income, loan amount, credit history length, and debt-to-income ratio.} }
@inproceedings{10.1145/3701716.3715280, title = {Towards Democratized Machine Learning: A Semantic Web Approach}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {697--700}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715280}, url = {https://doi.org/10.1145/3701716.3715280}, author = {Klironomos, Antonis}, keywords = {knowledge graphs, machine learning, semantic web, similarity measures, location = Sydney NSW, Australia}, abstract = {The rapid growth of machine learning (ML) research has produced a vast and expanding collection of algorithms, datasets, and pipelines available on the Web. However, fragmented and dispersed documentation of these resources hampers accessibility, transparency, and effective use, posing challenges for users seeking to understand, adapt, and create ML pipelines. To address these challenges, we leverage Knowledge Graphs (KGs) and ontologies to represent ML pipelines as executable KGs (ExeKGs). This approach fosters an intuitive understanding of pipeline components and their relationships while defined constraints streamline the creation of valid and efficient pipelines. Furthermore, the structure of our KGs enables intelligent exploration and discovery of relevant ML artifacts, including pipelines and datasets. By incorporating KG-based ML techniques, we enhance the discovery and reuse of these artifacts. To consolidate these functionalities and provide users with an intuitive interface, we are developing ExeKGLab, a GUI-based platform for interacting with ExeKGs. This thesis explores the potential of KGs to democratize the ML landscape. We present our ongoing efforts to build a KG for ML, emphasizing its role in simplifying pipeline design, enhancing comprehension, and enabling smart exploration. By creating a structured and interconnected framework, our approach seeks to bridge gaps in accessibility and foster a more collaborative ML ecosystem. We invite discussion and feedback to advance this promising direction for future ML research.} }
@inproceedings{10.1145/3749566.3749594, title = {Predicting New York City Rent through Machine Learning —— Based on Airbnb Data}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {115--122}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749594}, url = {https://doi.org/10.1145/3749566.3749594}, author = {Gong, Youzhe}, keywords = {Airbnb, feature importance analysis, machine learning, rent prediction}, abstract = {This research aims to predict the rent of Airbnb listings in New York City through machine learning models and analyze the key factors may influence the prices. Utilizing Airbnb data from New York City for the year 2024, the study employs four models: Ridge Regression, Decision Tree, Random Forest, and XGBoost. Through feature engineering and parameter tuning, the models’ performances were optimized and compared. The Random Forest model was determined to perform the best, achieving the lowest test set RMSE of 29.6926. Additionally, the study reveals the impacts of key variables such as the number of rooms, location, number of amenities, and minimum stay requirements on rent prediction, based on feature importance ranking and OLS regression results. The significance of this study lies in providing an effective method for housing price prediction and offering a reference for hosts to set reasonable pricing mechanisms.} }
@inproceedings{10.1145/3736733.3736744, title = {Explanations for Machine Learning Pipelines under Data Drift}, booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics}, year = {2025}, isbn = {9798400719592}, doi = {10.1145/3736733.3736744}, url = {https://doi.org/10.1145/3736733.3736744}, author = {Hasan, Jahid and Pradhan, Romila}, keywords = {pipeline robustness, data preparation, explainable AI, data drift, location = Intercontinental Berlin, Berlin, Germany}, abstract = {Ensuring the robustness of data preprocessing pipelines is essential for maintaining the reliability of machine learning model performance in the face of real-world data shifts. Traditional methods optimize preprocessing sequences for specific datasets but often overlook their vulnerability to future data variations. This research introduces a vulnerability score to quantify the susceptibility of preprocessing components to data shift. We propose a Linear Regression approach to establish a predictive relationship between the vulnerability of the pipeline components and changes in the model's performance. The generated relationships act as explanations for practitioners of the system and help them quantify the robustness of the pipeline to data shift. For a given pipeline, we generate an explanation that highlights a tolerable threshold beyond which a component is considered shift-vulnerable and is likely to contribute to performance degradation. For the shift-vulnerable scenarios, we further suggest a new pipeline for system maintainers that preserves the model performance without retraining. The proposed framework delivers a risk-aware assessment, empowering practitioners to anticipate potential performance changes and adapt their pipeline strategies accordingly. Experimental results on several real-world datasets generate valid explanations for pipeline robustness and demonstrate the opportunities in this field of research.} }
@proceedings{10.1145/3733965, title = {WiseML '25: Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning}, year = {2025}, isbn = {9798400715310}, abstract = {We are delighted to welcome you to the ACM Workshop on Wireless Security and Machine Learning (WiseML) 2025. Continuing its tradition as a premier forum, WiseML brings together researchers and practitioners from the machine learning, privacy, security, wireless communications, and networking communities worldwide. The workshop serves as a dynamic platform for presenting cutting-edge research, exchanging innovative ideas, and fostering collaborations that advance these rapidly evolving fields. This year's event will be held in Arlington, VA, USA, and will feature a single-track program to encourage focused and engaging discussions. This year's call for papers attracted submissions from Europe, Asia, and the United States, which were carefully reviewed by 17 technical program committee (TPC) members representing both academia and industrial research labs. We are proud to present an outstanding technical program featuring eight papers that address a broad spectrum of topics in security, privacy, and adversarial machine learning as applied to wireless networks, mobile communications, 5G/IoT systems, cloud and edge computing, vehicular networks, and emerging applications.} }
@inproceedings{10.1145/3724363.3729107, title = {Student Perspectives on the Challenges in Machine Learning}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {9--15}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729107}, url = {https://doi.org/10.1145/3724363.3729107}, author = {Sibia, Naaz and Richardson, Amber and Gao, Alice and Petersen, Andrew and Zhang, Lisa}, keywords = {artificial intelligence education, barriers and challenges, computing education, machine learning education, retention, student success, support, theory and practice, location = Nijmegen, Netherlands}, abstract = {Machine learning (ML) has become increasingly important for students, yet university-level ML courses are often perceived as challenging and time-intensive. This study explores the perceived challenges and motivations of students in a university ML course to inform curricular and teaching strategies. Through 5 surveys conducted in two instances of a 12-week introductory ML course, we examined students' engagement with both theoretical and practical aspects of ML. Results indicate that while students initially express strong interest in applying ML concepts, their reported interests can shift toward theoretical foundations. Challenges in both theory and practice are reported, including difficulties in mathematical notation and vectorization of gradient components, as well as model implementation. Students also discuss the time commitment required in a course with both theoretical and practical content. We recommend aligning course content with student motivations, providing targeted support for mathematical notation and vectorization, and balancing theoretical depth with practical application.} }
@inproceedings{10.1145/3670474.3685972, title = {When Device Modeling Meets Machine Learning: Opportunities and Challenges (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685972}, url = {https://doi.org/10.1145/3670474.3685972}, author = {Zhang, Lining and Peng, Baokang and Li, Yu and Liu, Hengyi and Dai, Wu and Wang, Runsheng}, keywords = {artificial neural network, compact model, device modeling, machine learning, location = Salt Lake City, UT, USA}, abstract = {Device modeling is essential for circuit simulations and designs in terms of constructing the circuit matrix equations of KCL and KVL. While there are classical methodologies, machine learning techniques are promising to bring innovations in the landscape of device modeling. This work reviews the device modeling from a top-down perspective, covering two different interpretations of modeling. Then the recent process in the domain-specific machine learning approaches is briefly summarized for logic and memory devices. The challenges ahead, for the machine learning model to support the industry's practical needs, are analyzed. A concept of fusion model, by deeply merging device physics and neural networks, is also explained.} }
@inproceedings{10.5555/3712729.3712752, title = {LLM Enhanced Machine Learning Estimators for Classification}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {288--298}, year = {2025}, isbn = {9798331534202}, author = {Wu, Yuhang and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, abstract = {Pre-trained large language models (LLM) have emerged as a powerful tool for simulating various scenarios and generating informative output given specific instructions and multimodal input. In this work, we analyze the specific use of LLM to enhance a classical supervised machine learning method for classification problems. We propose a few approaches to integrate LLM into a classical machine learning estimator to further enhance the prediction performance. We examine the performance of the proposed approaches through both standard supervised learning binary classification tasks, and a transfer learning task where the test data observe distribution changes compared to the training data. Numerical experiments using four publicly available datasets are conducted and suggest that using LLM to enhance classical machine learning estimators can provide significant improvement on prediction performance.} }
@inproceedings{10.1145/3718751.3718943, title = {Multivariate machine learning algorithm for stock return prediction modeling Machine learning algorithm for stock return prediction}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {1168--1172}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718943}, url = {https://doi.org/10.1145/3718751.3718943}, author = {Liu, Zile}, keywords = {BP neural network, Double machine learning algorithm, Multiple linear regression, Stock return rate}, abstract = {With the increase of market risk and volatility, it has become an important topic to evaluate stock return rate by using multi-algorithm comprehensive prediction. [Methods]: Multiple linear regression model, BP neural network model and double machine learning (DML) model were used to model stock return. [Data]: Stock weekly regular data of China A-share listed companies from 2010 to 2020 were used. [Results]: The interpretation degree of the model was 46\% by using multiple linear regression model. The F score of BP neural network model is 92\%. The degree of model explanation using DML model is more than 99\%.} }
@article{10.1145/3664595, title = {Creativity and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3664595}, url = {https://doi.org/10.1145/3664595}, author = {Franceschelli, Giorgio and Musolesi, Mirco}, keywords = {Computational creativity, machine learning, generative deep learning, creativity evaluation methods}, abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.} }
@inproceedings{10.1145/3749566.3749612, title = {Predictive Modeling of Airbnb Listing Prices in Boston Using Machine Learning Techniques}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {223--229}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749612}, url = {https://doi.org/10.1145/3749566.3749612}, author = {Tang, Jiaqi}, keywords = {Airbnb prices, Boston, Feature importance, Machine learning, Regression models}, abstract = {This paper aims to build a price prediction model based on the Airbnb official website listing indicators. Airbnb is currently a popular online platform that provides short-term and long-term homestays in multiple countries and regions. It plays the role of an intermediary and charges commission from each booking. This research combines predictive algorithms and explainable machine learning techniques to forecast the prices of Airbnb listings in the Boston, the United States, after the dataset is cleaned and preprocessed. Through multi-model comparison, this research finds the best performing model from a variety of methods. In addition, this study also evaluates the importance of features in the best performing model. Extreme Gradient Boosting (XGBoost) model outperforms other models on both training and test datasets, with a Mean Squared Error (MSE) and Mean Absolute Error (MAE) of 0.1536 and 0.2808 on the test set, respectively. Factors such as “is_entire_home”, “number_of_reviews”, “number of records presented by neighborhoods”, “minimum_nights”, and “pop_density” contribute the most to the prediction of listing prices. Overall, this study provides practical suggestions for Airbnb to optimize its listing strategy and improve its price recommendation system.} }
@article{10.1145/3729234, title = {Privacy-Preserving Machine Learning Based on Cryptography: A Survey}, journal = {ACM Trans. Knowl. Discov. Data}, volume = {19}, year = {2025}, issn = {1556-4681}, doi = {10.1145/3729234}, url = {https://doi.org/10.1145/3729234}, author = {Chen, Congcong and Wei, Lifei and Xie, Jintao and Shi, Yang}, keywords = {Machine learning, secure multi-party computation, homomorphic encryption, privacy preserving, cryptography}, abstract = {Machine learning has profoundly influenced various aspects of our lives. However, privacy breaches have caused significant unease and concern among the general public. Preserving the privacy of sensitive data during the training and inference phases of machine learning is a key challenge. Cryptography-based privacy-preserving machine learning (crypto-based PPML) offers a viable solution to this challenge. In this article, we studied over 100 publications on crypto-based PPML frameworks published between 2016 and 2024, including 55 client-server architecture frameworks and 64 multi-party architecture frameworks. We provide a comprehensive overview of these frameworks, highlighting their features across various dimensions. Furthermore, we conduct an in-depth analysis, delving into scenarios, privacy goals, threat models, and optimization techniques that underpin these innovative solutions. We also discuss the challenges in the field of crypto-based PPML, including aspects of security and privacy, efficiency, and availability and usability. Finally, we offer an outlook on future research directions, aiming to provide valuable insights for both scholars and practitioners.} }
@inproceedings{10.1145/3747227.3747273, title = {Intelligent Agricultural Greenhouse Control System Based on Internet of Things and Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {292--300}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747273}, url = {https://doi.org/10.1145/3747227.3747273}, author = {Xiao, Yu and Gong, Jiangchuan and Wang, Yanze and Wang, Cangqing}, keywords = {Agricultural greenhouse, Internet of Things (IoT), Machine Learning, RNN model}, abstract = {This study endeavors to conceptualize and execute a sophisticated agricultural greenhouse control system grounded in the amalgamation of the Internet of Things (IoT) and machine learning. Through meticulous monitoring of intrinsic environmental parameters within the greenhouse and the integration of machine learning algorithms, the conditions within the greenhouse are aptly modulated. The envisaged outcome is an enhancement in crop growth efficiency and yield, accompanied by a reduction in resource wastage. In the backdrop of escalating global population figures and the escalating exigencies of climate change, agriculture confronts unprecedented challenges. Conventional agricultural paradigms have proven inadequate in addressing the imperatives of food safety and production efficiency. Against this backdrop, greenhouse agriculture emerges as a viable solution, proffering a controlled milieu for crop cultivation to augment yields, refine quality, and diminish reliance on natural resources. Nevertheless, greenhouse agriculture contends with a gamut of challenges. Traditional greenhouse management strategies, often grounded in experiential knowledge and predefined rules, lack targeted personalized regulation, thereby resulting in resource inefficiencies. The exigencies of real-time monitoring and precise control of the greenhouse's internal environment gain paramount importance with the burgeoning scale of agriculture. To redress this challenge, the study introduces IoT technology and machine learning algorithms into greenhouse agriculture, aspiring to institute an intelligent agricultural greenhouse control system conducive to augmenting the efficiency and sustainability of agricultural production.} }
@inproceedings{10.1145/3712255.3716543, title = {Machine Learning Assisted Evolutionary Multi-objective Optimization}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1025--1041}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716543}, url = {https://doi.org/10.1145/3712255.3716543}, author = {Deb, Kalyanmoy and Saxena, Dhish Kumar and Mittal, Sukrit} }
@inproceedings{10.1145/3676642.3729206, title = {Has Machine Learning for Systems Reached an Inflection Point?}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages = {3--4}, year = {2025}, isbn = {9798400710803}, doi = {10.1145/3676642.3729206}, url = {https://doi.org/10.1145/3676642.3729206}, author = {Maas, Martin}, keywords = {cluster scheduling, code optimization, data centers, large language models, machine learning for systems, memory allocation, storage systems, location = Rotterdam, Netherlands}, abstract = {A wide range of research areas - from natural language processing to computer vision and software engineering - have been (or are being) revolutionized by machine learning and artificial intelligence. Each of these areas went through an inflection point where they transitioned from ML as one of many approaches to ML becoming a predominant approach of the field. No example symbolizes this better than the AlexNet paper from 2012, which fundamentally transformed the field of computer vision. Computer systems remain a notable exception. In this talk, I will discuss emerging trends in the ML for Systems domain, how systems differ from these other areas, and what an ''AlexNet Moment'' for systems might look like. Along the way, I will describe a framework for categorizing work in the field and discuss emerging research problems and opportunities.} }
@article{10.1145/3763132, title = {AccelerQ: Accelerating Quantum Eigensolvers with Machine Learning on Quantum Simulators}, journal = {Proc. ACM Program. Lang.}, volume = {9}, year = {2025}, doi = {10.1145/3763132}, url = {https://doi.org/10.1145/3763132}, author = {Bensoussan, Avner and Chachkarova, Elena and Even-Mendoza, Karine and Fortz, Sophie and Lenihan, Connor}, keywords = {Genetic Algorithms, Machine Learning, Optimisation, Quantum Computing, Quantum Program Analysis, Search-based Software Engineering}, abstract = {We present AccelerQ, a framework for automatically tuning quantum eigensolver (QE) implementations–these are quantum programs implementing a specific QE algorithm–using machine learning and search-based optimisation. Rather than redesigning quantum algorithms or manually tweaking the code of an already existing implementation, AccelerQ treats QE implementations as black-box programs and learns to optimise their hyperparameters to improve accuracy and efficiency by incorporating search-based techniques and genetic algorithms (GA) alongside ML models to efficiently explore the hyperparameter space of QE implementations and avoid local minima. Our approach leverages two ideas: 1) train on data from smaller, classically simulable systems, and 2) use program-specific ML models, exploiting the fact that local physical interactions in molecular systems persist across scales, supporting generalisation to larger systems. We present an empirical evaluation of AccelerQ on two fundamentally different QE implementations: ADAPT-QSCI and QCELS. For each, we trained a QE predictor model, a lightweight XGBoost Python regressor, using data extracted classically from systems of up to 16 qubits. We deployed the model to optimise hyperparameters for executions on larger systems of 20-, 24-, and 28-qubit Hamiltonians, where direct classical simulation becomes impractical. We observed a reduction in error from 5.48\% to 5.3\% with only the ML model and further to 5.05\% with GA for ADAPT-QSCI, and from 7.5\% to 6.5\%, with no additional gain with GA for QCELS. Given inconclusive results for some 20- and 24-qubit systems, we recommend further analysis of training data concerning Hamiltonian characteristics. Nonetheless, our results highlight the potential of ML and optimisation techniques for quantum programs and suggest promising directions for integrating software engineering methods into quantum software stacks.} }
@inproceedings{10.1145/3747227.3747228, title = {Comparison and Prediction of Earth Pressure Based on Multiple Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {1--5}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747228}, url = {https://doi.org/10.1145/3747227.3747228}, author = {Li, Daimao and Xiao, Haohan and Guo, Qinghua and Han, Ke and Chen, Siyang}, keywords = {Data Processing, Earth Pressure, Machine Learning Algorithms, TBM}, abstract = {This paper investigates the application of machine learning algorithms in processing data from the first 1500 excavation segments of an Earth Pressure Balanced (EPB) Shield Tunnel Boring Machine (TBM). First, various models developed based on classical algorithms such as Decision Trees, Neural Networks, and Support Vector Machines are introduced, including Random Forest, XGBoost, and BPNN. The paper then elaborates on key data processing steps, noting that all models, except for the three Decision Tree-based models, require data normalization according to specific formulas. The model evaluation criteria are defined, encompassing multiple metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Additionally, the paper discusses how the dataset partitioning method varies based on the characteristics of the models and the configuration of hyperparameters. Finally, a comparative analysis of the predictive performance of each model is presented. The results show that, with the exception of ELM, most models perform similarly and meet engineering requirements. The different models exhibit varying strengths and weaknesses across different excavation segments, providing valuable insights for the application and improvement of machine learning models in related engineering projects.} }
@inbook{10.1145/3745238.3745358, title = {Pricing Strategy Optimization by Machine Learning in E-commerce}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {760--765}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745358}, author = {Liu, Quan and Song, Yunkui}, abstract = {This paper introduces the concept of pricing strategy and its importance in e-commerce, analyzes the limitations of traditional pricing methods. Subsequently, the article elaborates on the application of machine learning in E-commerce pricing strategies. In addition, the article also presents the key technologies for optimizing pricing strategies using machine learning. Finally, it looks forward to the development trend of future pricing systems.} }
@article{10.1145/3728474, title = {Machine Learning for Blockchain Data Analysis: Progress and Opportunities}, journal = {Distrib. Ledger Technol.}, year = {2025}, doi = {10.1145/3728474}, url = {https://doi.org/10.1145/3728474}, author = {Azad, Poupak and Akcora, Cuneyt and Khan, Arijit}, keywords = {Machine Learning, Blockchain, Cryptocurrency, Graph Neural Networks, Temporal Data, Smart Contracts}, abstract = {Blockchain technology has rapidly emerged to mainstream attention. At the same time, its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data. Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts. Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity, such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies. These unique characteristics present opportunities and challenges for machine learning on blockchain data.On the one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for improving blockchain technology, such as e-crime detection and trends prediction. On the other hand, we shed light on blockchain's pivotal role by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem. This paper is a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field.} }
@inproceedings{10.1145/3694860.3694868, title = {Dementia Deterioration Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Cloud and Big Data Computing}, pages = {54--59}, year = {2024}, isbn = {9798400717253}, doi = {10.1145/3694860.3694868}, url = {https://doi.org/10.1145/3694860.3694868}, author = {Dawood Almardoud, Layla and Tawfik, Hissam and Majzoub, Sohaib}, keywords = {Dementia prognosis, Interpretable models, Machine Learning, Mild Cognitive Impairment, Permutation Importance}, abstract = {Dementia is a disease that imposes medical, social, and economic challenges on medical professionals, caregivers, and the patients themselves. Dementia monitoring and prognosis are critical factors besides dementia diagnosis. However, recent studies on dementia prognosis involve people with diagnosed dementia and not non-demented with a high risk of being demented, like people with cognitive difficulties. The aim of this paper is to use Machine Learning (ML) algorithms to predict patients with the risk of deterioration from medical histories containing clinical, cognitive, and profile data collected from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The best model was the Random Forest model with a sensitivity of 0.79, accuracy of 0.77, specificity of 0.76, F1-score of 0.78, and an AUROC of 0.83. Moreover, the model was interpreted through permutation importance. Using the permutation importance tool, the study highlighted the strong effect of diagnosis information and specific symptoms like muscle pain for dementia deterioration prediction.} }
@inproceedings{10.1145/3760544.3764127, title = {Breath Patterns as Signals: A Machine Learning-based Molecular Communication Perspective}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {22--27}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764127}, url = {https://doi.org/10.1145/3760544.3764127}, author = {Bhattacharjee, Sunasheer and Pal, Saswati and Scheepers, P\'eter and Dressler, Falko}, keywords = {biological system, diagnostics, communication system model, machine learning, molecular communication, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication is a core pillar of the Internet of Bio-Nano Things. Exhaled breath, rich in water vapor, offers a viable medium for air-based molecular communication. This paper presents a low-cost, non-invasive approach using a DHT22 sensor to classify breath patterns, namely Eupnea, Bradypnea, and Tachypnea. Humidity and temperature signals from the mouth and nose are processed using machine learning (ML). The model achieves strong classification performance, showing that ML can effectively distinguish breath patterns despite sensor constraints.} }
@inproceedings{10.1145/3637528.3671442, title = {Practical Machine Learning for Streaming Data}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6418--6419}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671442}, url = {https://doi.org/10.1145/3637528.3671442}, author = {Gomes, Heitor Murilo and Bifet, Albert}, keywords = {classification, concept drift, data streams, prediction intervals, regression, semi-supervised learning, location = Barcelona, Spain}, abstract = {Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/.} }
@article{10.1145/3744237, title = {Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3744237}, url = {https://doi.org/10.1145/3744237}, author = {Zhong, Zhiqiang and Barkova, Anastasia and Mottin, Davide}, keywords = {Graph machine learning, knowledge-augmented methods, drug discovery, knowledge database, knowledge graph}, abstract = {Artificial Intelligence has become integral to intelligent drug discovery, with Graph Machine Learning (GML) emerging as a powerful structure-based method for modelling graph-structured biomedical data and investigating their properties. However, GML faces challenges such as limited interpretability and heavy dependency on abundant high-quality training data. On the other hand, knowledge-based methods leverage biomedical knowledge databases, e.g., Knowledge Graphs (KGs), to explore unknown knowledge. Nevertheless, KG construction is resource-intensive and often neglects crucial structural information in biomedical data. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with scarce training data. Nevertheless, a systematic definition for this burgeoning research direction is yet to be established. This survey formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery and organises collected KaGML works into four categories following a novel-defined taxonomy. We also present a comprehensive overview of long-standing drug discovery principles and provide the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases. To facilitate research in this promptly emerging field, we share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.} }
@article{10.1145/3723356, title = {PredicTor: A Global, Machine Learning Approach to Tor Path Selection}, journal = {ACM Trans. Priv. Secur.}, volume = {28}, year = {2025}, issn = {2471-2566}, doi = {10.1145/3723356}, url = {https://doi.org/10.1145/3723356}, author = {Barton, Armon and Walsh, Timothy and Imani, Mohsen and Ming, Jiang and Wright, Matthew}, keywords = {Anonymous communications, Tor, machine learning, path selection, network performance}, abstract = {Tor users derive anonymity in part from the size of the Tor user base, but Tor struggles to attract and support more users due to performance limitations. Previous works have proposed modifications to Tor’s path selection algorithm to enhance both performance and security, but many proposals have unintended consequences due to incorporating information related to client location. We instead propose selecting paths using a global view of the network, independent of client location, and we propose doing so with a machine learning classifier to predict the performance of a given path before building a circuit. We show through a variety of simulated and live experimental settings, across different time periods, that this approach can significantly improve performance compared to Tor’s default path selection algorithm and two previously proposed approaches. In addition to evaluating the security of our approach with traditional metrics, we propose a novel anonymity metric that captures information leakage resulting from location-aware path selection, and we show that our path selection approach leaks no more information than the default path selection algorithm.} }
@inproceedings{10.1145/3724363.3729085, title = {K-12 Students' (Mis-)Conceptions of Machine Learning Paradigms}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {347--353}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729085}, url = {https://doi.org/10.1145/3724363.3729085}, author = {Kr\"uger, Jan Jakob and Gromann, Gabriel and Dengel, Andreas}, keywords = {ai literacy, computer science curriculum, k-12 education, machine learning, student misconceptions}, abstract = {Understanding Artificial Intelligence (AI) is on its way to becoming a key competence in the coming years due to its rapid advancements and growing societal impact. Learning about different paradigms in AI, particularly in machine learning, is crucial to enabling students to critically engage with and shape an AI-driven world. The study investigates how students of three grade levels (5th, 8th, and 11th) understand AI concepts, addressing the challenge of widespread misconceptions and the limited presence of AI in school curricula. A deductive qualitative analysis was used to analyze the students' preconceptions. The results indicate that younger students typically exhibit minimal or anthropomorphic views, reflecting limited exposure to basic AI principles. Older students show somewhat more advanced, yet still partial, understandings, with misconceptions persisting across all groups. The findings underscore the importance of integrating AI literacy into school curricula and aligning instruction with students' developmental stages. For younger learners, hands-on activities can introduce basic AI functionality while dispelling human-like attributions. Older students benefit from exploring ethical, technical, and societal dimensions of AI. This research highlights the need for age-appropriate AI education to foster informed, responsible users and creators of AI systems.} }
@inproceedings{10.1145/3696271.3696272, title = {Predicting Foreign Exchange EUR/USD Direction Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {1--9}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696272}, url = {https://doi.org/10.1145/3696271.3696272}, author = {Guyard, Kevin Cedric and Deriaz, Michel}, keywords = {Bayesian search, Forex prediction, Machine learning, Meta estimator}, abstract = {The Foreign Exchange market is a significant market for speculators, characterized by substantial transaction volumes and high volatility. Accurately predicting the directional movement of currency pairs is essential for formulating a sound financial investment strategy. This paper conducts a comparative analysis of various machine learning models for predicting the daily directional movement of the EUR/USD currency pair in the Foreign Exchange market. The analysis includes both decorrelated and non-decorrelated feature sets using Principal Component Analysis. Additionally, this study explores meta-estimators, which involve stacking multiple estimators as input for another estimator, aiming to achieve improved predictive performance. Ultimately, our approach yielded a prediction accuracy of 58.52\% for one-day ahead forecasts, coupled with an annual return of 32.48\% for the year 2022.} }
@inproceedings{10.1145/3701047.3701067, title = {Comparative Analysis of Various Machine Learning Techniques for Rockburst Risk Prediction}, booktitle = {Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, pages = {105--109}, year = {2025}, isbn = {9798400711688}, doi = {10.1145/3701047.3701067}, url = {https://doi.org/10.1145/3701047.3701067}, author = {Yu, Hongtao and Li, Mingyao and Xiao, Haohan and Cao, Ruilang}, keywords = {Data processing, Feature clustering, Machine learning, Model evaluation, Rockburst risk prediction}, abstract = {This paper investigates the application of machine learning algorithms in rockburst risk prediction models. It begins with an overview of fundamental machine learning principles, followed by a detailed analysis of common classification algorithms, including Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), Random Forest (RF), and AdaBoost. Subsequently, the paper examines the use of clustering algorithms for feature extraction and reorganization. The technical process for developing accurate rockburst risk prediction models is outlined, encompassing data preprocessing, feature clustering, reorganization, and model evaluation. The findings offer valuable insights and references for the integration of machine learning in rockburst risk prediction, as well as innovative approaches for research and practical applications in related fields.} }
@inproceedings{10.1145/3728199.3728203, title = {Data Analysis of College Practice Teaching Based on Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {18--23}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728203}, url = {https://doi.org/10.1145/3728199.3728203}, author = {Wang, Xinwu}, keywords = {association rule analysis, cluster analysis, college practice teaching, data mining, machine learning algorithm}, abstract = {With the rapid development of information technology, especially the wide application of big data and artificial intelligence technology, the field of education is undergoing unprecedented changes. Among them, practical teaching in colleges and universities, as an important part of modern education system, is facing severe challenges. However, the traditional practice teaching mode often has some problems, such as uneven distribution of resources, disjointed teaching content and market demand, and single teaching method, which limit the improvement of practice teaching effect. In this regard, based on the actual needs, this paper will deeply analyze the application feasibility of artificial intelligence and machine learning in the reform of practical teaching in colleges and universities, and put forward a set of data analysis and processing scheme of practical teaching in colleges and universities, aiming at improving the effectiveness and personalized level of practical teaching through the integration, mining and analysis of multi-source data. Practice has proved that machine learning algorithms such as fuzzy C-means (FCM) clustering analysis and frequent pattern growth (FP-growth) correlation analysis are adopted in the scheme, and the data involved in college practice teaching are deeply analyzed, and the feasibility and effectiveness of the model are verified by practical tests, which provides scientific basis and decision support for college practice teaching.} }
@inproceedings{10.1145/3745812.3745873, title = {Predictive Analytics for Chemotherapy Effectiveness Using Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745873}, url = {https://doi.org/10.1145/3745812.3745873}, author = {kaur, Lakhwinder and Bamne, Shrikrishna N. and Dehankar, Jiwan and Khetani, Vinit and Pawar, S K and Goyal, Dinesh, -}, keywords = {Cancer Treatment, Chemotherapy, Machine Learning, Personalized Medicine, Predictive Analytics}, abstract = {Abstract: Chemotherapy is still an important part of treating cancer, but how well it works for each patient depends on things like the type of tumor, their genetics, and their general health. Accurately predicting how chemotherapy will work is necessary to make treatment plans work better, lower side effects, and raise patient mortality rates. This study looks into how machine learning (ML) methods can be used in predictive analytics to figure out how well treatment works. Machine learning models like Support Vector Machines (SVM), Random Forests, and Neural Networks are used to figure out which patients are most likely to respond to or not respond to chemotherapy. They do this by looking at information about each patient's medical history, genomic information, treatment plans, and the characteristics of their tumors. The information used to train and test the model comes from clinical studies and patient records. This makes sure that it has a wide range of cancer types and patient traits. To deal with complicated, high-dimensional data and find key drivers of treatment results, both controlled and untrained learning methods are used together. The models are judged by their performance measures, which include their accuracy, precision, recall, and the area under the receiver operating characteristic curve (AUC). The results show that prediction models based on machine learning can accurately predict how well chemotherapy will work, which lets doctors make personalized treatment plans. The results show that using predictive analytics in clinical decision- making could make cancer procedures more efficient and effective, reducing side effects and improving patient outcomes.} }
@article{10.1145/3757743, title = {Enhancing Blockchain Scalability using Off-Chain and Machine Learning Techniques}, journal = {J. Emerg. Technol. Comput. Syst.}, year = {2025}, issn = {1550-4832}, doi = {10.1145/3757743}, url = {https://doi.org/10.1145/3757743}, author = {Pawar, Manjula and Patil, Prakashgoud and Hiremath, P.S.}, keywords = {Blockchain, Scalability, Machine learning, Off-chain, KNN classification, Artificial Intelligence}, abstract = {Blockchain Technology is a nascent technology that possesses attributes such as immutability, security, transparency, openness, and decentralization. It is widely used in industry and business applications. Though it has the best features, it still suffers from some main characteristics, such as scalability and privacy. Scalability is measured through throughput (transactions per second), space,cost and latency. Bitcoin and Ethereum, which are prominent Blockchain platforms, carry out 7 and 20 transactions per second, respectively. This is much less than popular platforms such as Visa, PayPal, and Amazon, which perform thousands of transactions per second. Therefore, this paper presents comprehensive study of scalability improving techniques for Blockchain and case studies for improving scalability by using some of the techniques. The scalability of Blockchain systems can be enhanced by on-chain, off-chain, and machine learning algorithms.The proposed methodology improves the scalability using off-chain technique for supply chain management and KNN classification for healthcare domain.} }
@inproceedings{10.1145/3716368.3735251, title = {Enhancing Modern SAT Solver With Machine Learning Method}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {886--892}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735251}, url = {https://doi.org/10.1145/3716368.3735251}, author = {Chen, Guanting and Wang, Jia}, keywords = {SAT Solver, CDCL, Machine Learning, GNN}, abstract = {Satisfiability (SAT), a well-known NP-complete problem, has been widely studied and drives numerous research fields. State-of-the-art SAT solvers rely on the Conflict-Driven Clause Learning (CDCL) algorithm to solve practical SAT instances with two possible outcomes – a solution for SAT instances or a proof proving no solution exists for UNSAT instances. While many heuristics were manually designed to improve CDCL in the past, recent efforts focus on applying machine and deep learning models, e.g. Graph Neural Networks (GNN), with the hope to make heuristics more effective and adaptive. Nevertheless, the demand for significant GPU resources and the effectiveness in a broader set of SAT and UNSAT instances remain the major challenges. In this paper, we present a GNN-based algorithm that predicts at the same time backbone variables for SAT instances and UNSAT-core variables for UNSAT instances. Leveraging offline model inference, our trained GNN model, and so the whole SAT solver, is able to run entirely on CPU, removing the need of GPU resources. Experimental results confirm that with our algorithm, a modern SAT solver is able to solve up to 5\% and 7\% more instances for different baseline solvers.} }
@article{10.1145/3711713, title = {Applications of Certainty Scoring for Machine Learning Classification and Out-of-Distribution Detection}, journal = {ACM Trans. Probab. Mach. Learn.}, year = {2025}, doi = {10.1145/3711713}, url = {https://doi.org/10.1145/3711713}, author = {Berenbeim, Alexander M. and Cobb, Adam D. and Roy, Anirban and Jha, Susmit and Bastian, Nathaniel D.}, keywords = {Machine Learning, Uncertainty Quantification, Out-of-Distribution Detection, Computer Vision, Network Security}, abstract = {Quantitative characterizations and estimations of uncertainty are of fundamental importance for machine learning classification, particularly in safety-critical settings where continuous real-time monitoring requires explainable and reliable scoring. Reliance on the maximum a posteriori principle to determine label classification can obscure the certainty of a label assignment. We develop a theoretical framework for quantitative scores of certainty and competence based on predicted probability estimates, formally prove their properties, and empirically confirm the inferential power of these properties across different data modalities, tasks and model architectures. Our theoretical results establish that competent models have distinct distributions of certainty for true and false positives conditioned on inputs similar to training and testing data, and prove that this framework provides a reliable means to infer the quality of model predictions and detect false positives. Our empirical results bear out that there are distinct distributions of certainty scores on training and holdout data, as well as data that is a priori out-of-distribution. For expert models, at least 62.1\% of false positives could be identified when using a cut-off at at the bottom 5\% TP threshold. Further, we found a strong negative correlation between empirical competence and the FPR95TPR rate for EnergyBased out-of-distribution (OOD) detectors. Finally, we developed two forms of an OOD detector that were able to reliably distinguish in-distribution data from OOD data for both frequentist and Bayesian models, performing better on average than previous state-of-the-art EnergyBased OOD detection methods, and improving upon the baseline Monte Carlo Dropout AUPR-OUT performance on average by 14.4\% and 16.5\%, and reducing the FPR95TPR by 54.2\% and 37.6\%.} }
@inproceedings{10.1145/3746027.3764193, title = {Toward Fast and Exact Machine Learning Platform for Big Data}, booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia}, pages = {14372}, year = {2025}, isbn = {9798400720352}, doi = {10.1145/3746027.3764193}, url = {https://doi.org/10.1145/3746027.3764193}, author = {Fujiwara, Yasuhiro}, keywords = {ai-based data analysis, big data, computational pruning, scalable machine learning, location = Dublin, Ireland}, abstract = {Data has become the foundation of knowledge, and many companies are growing interested in harnessing AI-based data analysis to unlock its value. The volume of digital data is increasing at an unprecedented pace: market research reports estimate that global data volume, approximately 12.5 zettabytes in 2014, will reach around 180 zettabytes by 2025. Extracting patterns and trends from such big data is crucial for enabling data-driven decision-making. However, a key challenge lies in the enormous computational costs required for large-scale analysis, due to the inherent complexities of the task. Approximate methods are often employed to reduce these costs, but they inevitably trade exactness for efficiency. To overcome this limitation, our research aims to develop a machine learning platform that delivers both speed and accuracy. The core of our platform is computational pruning. This talk will introduce three representative pruning strategies. Specifically, it first introduces a pruning method that uses upper and lower bounds to omit computations. This method efficiently identifies unnecessary processes by using upper and lower bounds of scores to skip unnecessary computations. Next, this talk introduces a method to terminate computations that cannot yield solutions. This method maintains patterns that failed during the search process to avoid repeated futile searches. This talk finally introduces a method that prunes computations through optimistic processing. This method temporarily removes a constraint to find a solution quickly and then verifies if the obtained solution meets the constraint. These strategies can open the path to data analysis techniques that are both efficient and exact, ultimately empowering companies to make more reliable and timely decisions in today's increasingly data-driven world.} }
@inproceedings{10.1145/3757110.3757193, title = {Attenuation characteristics and machine learning predictions for metamaterial foundation systems}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {501--505}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757193}, url = {https://doi.org/10.1145/3757110.3757193}, author = {Feng, Li and Pi, Saiqi}, keywords = {Bandgap, Local resonance, Metamaterial, Neural network}, abstract = {The ambient vibrations caused by trains have become more serious. It is imperative to isolate those vibration. Thus, this proposes a novel metamaterial foundation based on local resonance theory for reducing vibration induced by trains and establishes a neural network model. The bandgaps of in-plane vibration and out-plane vibration are 30.1∼52.1Hz and 13.8∼25.5Hz. This indicates that the metamaterial foundation can be used to reduce vibrations induced by trains, and the neural network model has good performance, can accurately predict the bandgap based on structural and material parameters.} }
@inproceedings{10.1145/3706628.3708848, title = {Resource Scheduling for Real-Time Machine Learning}, booktitle = {Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays}, pages = {50}, year = {2025}, isbn = {9798400713965}, doi = {10.1145/3706628.3708848}, url = {https://doi.org/10.1145/3706628.3708848}, author = {Singh, Suyash Vardhan and Ahmad, Iftakhar and Andrews, David and Huang, Miaoqing and Downey, Austin R. J. and Bakos, Jason D.}, keywords = {hardware acceleration, high-level synthesis (hls), real-time control systems, resource scheduling, location = Monterey, CA, USA}, abstract = {Data-driven physics models offer the potential for substantially increasing the sample rate for applications in high-rate cyberphys- ical systems, such as model predictive control, structural health monitoring, and online smart sensing. Making this practical re- quires new model deployment tools that search for networks with maximum accuracy while meeting both real-time performance and resource constraints. Tools that generate customized architectures for machine learning models, such as HLS4ML and FINN, require manual control over latency and cost trade-offs for each layer. This poster describes a proposed end-to-end framework that combines Bayesian optimization for neural architecture search with Integer Linear Optimization of layer cost-latency trade-off using HLS4ML ''reuse factors''. The proposed framework is shown in Fig. 1 and consists of a performance model training phase and two model deployment stages. The performance model training phase generates training data and trains a model to predict the resource cost and latency of an HLS4ML deployment of a given layer and associated reuse factor on a given FPGA. The first model deployment stage takes training, test, and validation data for a physical system-in this case, the Dynamic Reproduction of Projectiles in Ballistic Environments for Advanced Research (DROPBEAR) dataset-and searches the hyper- parameter space for Pareto optimal models with respect to latency and workload, as measured by the number of multiplies required for one forward pass. For each of the models generated, a second stage uses the performance model to optimize the reuse factor of each layer to guarantee that the whole model meets the resource constraint while minimizing end-to-end latency. Table 1 shows the benefit of the reuse factor optimizer that comprises the second stage of the model deployment phase, The results compare the performance of a baseline stochastic search to that of our proposed optimizer for an example model consisting of four convolutional layers, three LSTM layers, and one dense layer. The results show sample stochastic search runs having 1K, 10K, 100K, and 1M trials over a total search space of 209 million reuse factor permutations. The stochastic search reaches a point of diminishing returns with latency 205 𝜂 while the optimizer achieves a latency of 190 𝜂 and requires roughly 1000X less search time.} }
@article{10.1145/3749116.3749122, title = {Graph Data Management and Graph Machine Learning: Synergies and Opportunities}, journal = {SIGMOD Rec.}, volume = {54}, pages = {28--42}, year = {2025}, issn = {0163-5808}, doi = {10.1145/3749116.3749122}, url = {https://doi.org/10.1145/3749116.3749122}, author = {Khan, Arijit and Ke, Xiangyu and Wu, Yinghui}, abstract = {The ubiquity of machine learning, particularly deep learning, applied to graphs is evident in applications ranging from cheminformatics (drug discovery) and bioinformatics (protein interaction prediction) to knowledge graph-based query answering, fraud detection, and social network analysis. Concurrently, graph data management deals with the research and development of effective, efficient, scalable, robust, and user-friendly systems and algorithms for storing, processing, and analyzing vast quantities of heterogeneous and complex graph data. Our survey provides a comprehensive overview of the synergies between graph data management and graph machine learning, illustrating how they intertwine and mutually reinforce each other across the entire spectrum of the graph data science and machine learning pipeline. Specifically, the survey highlights two crucial aspects: (1) How graph data management enhances graph machine learning, including contributions such as improved graph neural network performance through graph data cleaning, scalable graph embedding, efficient graph-based vector data management, robust graph neural networks, user-friendly explainability methods; and (2) how graph machine learning, in turn, aids in graph data management, with a focus on applications like query answering over knowledge graphs and various data science tasks. We discuss pertinent open problems and delineate crucial research directions.} }
@inproceedings{10.1145/3680256.3721313, title = {DMML: A Machine-learning Performance Model for Data Migration}, booktitle = {Companion of the 16th ACM/SPEC International Conference on Performance Engineering}, pages = {136--143}, year = {2025}, isbn = {9798400711305}, doi = {10.1145/3680256.3721313}, url = {https://doi.org/10.1145/3680256.3721313}, author = {Ghaneshirazi, Hasti and Hamouda, Fares and Fokaefs, Marios and Haouari, Wejdene and Jania, Dariusz}, keywords = {data migration, data transfer time, decision tree, feature engineering, gradient boosting, hyperparameter tuning, machine learning, model evaluation, predictive modeling, random forest, regression models, xgboost, location = Toronto ON, Canada}, abstract = {Data migration at scale can be a daunting task. It may require significant resources and time, which must be taken from value-adding activities of an enterprise. Besides errors may occur, which can jeopardize the integrity of the data and waste resources. Accurately estimating data migration time and resource performance is critical for optimizing time, cost, and risk in large-scale data transfers. In this paper, we propose the use of machine learning to create performance models for data migration. We utilize DMBench, a benchmarking and load testing tool specifically tailored for data migrations, to generate data, simulating various data migration scenarios with different data sizes, vCPUs, RAM size, and data compression types. We experimented with multiple ML algorithms and showed the effect of hyperparameter tuning in the model's accuracy. Our results show that the XGBoost is the most accurate and consistent across the different scenarios. We demonstrate the model building process and its evaluation on an industrial case study.} }
@article{10.1145/3723157, title = {Machine Learning for Identifying Risk in Financial Statements: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3723157}, url = {https://doi.org/10.1145/3723157}, author = {Zavitsanos, Elias and Spyropoulou, Eirini and Giannakopoulos, George and Paliouras, Georgios}, keywords = {Risk assessment, misstatement detection, financial distress, bankruptcy prediction, fraud detection, financial reports, financial statements, machine learning, data mining, auditing}, abstract = {The work herein reviews the scientific literature on Machine Learning approaches for financial risk assessment using financial reports. We identify two prominent use cases that constitute fundamental risk factors for a company, namely misstatement detection and financial distress prediction. We further categorize the related work along four dimensions that can help highlight the peculiarities and challenges of the domain. Specifically, we group the related work based on (a) the input features used by each method, (b) the sources providing the labels of the data, (c) the evaluation approaches used to confirm the validity of the methods, and (d) the machine learning methods themselves. This categorization facilitates a technical overview of risk detection methods, revealing common patterns, methodologies, significant challenges, and opportunities for further research in the field.} }
@inproceedings{10.1145/3709026.3709071, title = {Regularization of Machine Learning and Linear Algebra}, booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence}, pages = {327--332}, year = {2025}, isbn = {9798400718182}, doi = {10.1145/3709026.3709071}, url = {https://doi.org/10.1145/3709026.3709071}, author = {Liu, Shuang and Kabanikhin, Sergey Igorevich and Strijhak, Sergei Vladimirovich}, keywords = {system of linear algebraic equations, machine learning, linear neural network.}, abstract = {This paper explores the connection between systems of linear algebraic equations (SLAE) and machine learning methods, including regularization techniques, to establish a more novel neural network model based on linear neural networks. The goal is to construct a weight matrix for the neural network, which, by simulating the process of finding pseudo-solutions to SLAE, can generate the optimal answer for any input data. In this new neural network model, linear operations are performed first, followed by nonlinear operations, ultimately yielding an optimized weight matrix that serves as the pseudo-solution to the SLAE. The paper demonstrates how linear neural networks can be simplified to SLAE, how adding nonlinear layers to the linear neural network model can improve accuracy, and how machine learning methods can be used to find pseudo-solutions to SLAE.} }
@inproceedings{10.1145/3674029.3674031, title = {Prediction of Mobile Phone Prices using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {6--10}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674031}, url = {https://doi.org/10.1145/3674029.3674031}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco and Panwar, Disha and Shree, Shadeeksha}, abstract = {This research investigates upon the prediction of mobile phone prices based on various factors through the applications of multiple machine learning algorithms. Leveraging linear regression, decision tree regressor, random forest regressor, gradient boosting regressor, voting\&nbsp;regressor and support vector regressor. This work explores the effectiveness in capturing the intricate relationships between the pricing of mobile phones in the market and the various factors affecting it which may be based on the hardware, software, the brand value, etc. The experimental results reveal distinct strengths and limitations of each algorithm, with the ensemble-based voting regressor demonstrating superior predictive performance with a training accuracy of 93.21\% and testing accuracy of 88.98\%. Gradient boosting regressor overfits the model with a training accuracy of 100\% and testing accuracy of 97.91\% and the linear regression model is observed to be the least accurate with a training and testing accuracy of 7.77\% and 7.12\% respectively. This research lays the groundwork for informed algorithm selection and implementation in the development of advanced mobile price prediction systems.} }
@inproceedings{10.1145/3732801.3732854, title = {Malicious URL Detection with Explainable Machine Learning Techniques}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {293--299}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732854}, url = {https://doi.org/10.1145/3732801.3732854}, author = {Wang, Bolun}, keywords = {Explainability, Machine Learning, Malicious URL Detection}, abstract = {Malicious Uniform Resource Locator (URL) detection research has been actively worked on and the focus is on achieving higher and more accurate results with machine learning and artificial intelligence approaches. Unfortunately, existing research on this problem has not yet been completed completely, and there remains a research gap in malicious URL detection in developing effective approaches that are capable of dealing with evasive techniques used by the attackers to conceal harmful URLs. This study explores an effective technique of detecting malicious URL detection with machine learnings with explainability. In particular, three advanced ML models are applied on one real parameters URL dataset, Logistic regression (LR), decision trees (DT) and Random Forest (RF) are employed. The results show that RF has the best detection accuracy and the best performance in terms of explain ability.} }
@inproceedings{10.1145/3627673.3679103, title = {Hands-On Introduction to Quantum Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5507--5510}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679103}, url = {https://doi.org/10.1145/3627673.3679103}, author = {Chen, Samuel Yen-Chi and Kim, Joongheon}, keywords = {quantum architecture search, quantum machine learning, quantum neural networks, reinforcement learning, variational quantum circuits, location = Boise, ID, USA}, abstract = {This tutorial offers a hands-on introduction into the captivating field of quantum machine learning (QML). Beginning with the bedrock of quantum information science (QIS)-including essential elements like qubits, single and multiple qubit gates, measurements, and entanglement-the session swiftly progresses to foundational QML concepts. Participants will explore parameterized or variational circuits, data encoding or embedding techniques, and quantum circuit design principles. Delving deeper, attendees will examine various QML models, including the quantum support vector machine (QSVM), quantum feed-forward neural network (QNN), and quantum convolutional neural network (QCNN). Pushing boundaries, the tutorial delves into cutting-edge QML models such as quantum recurrent neural networks (QRNN) and quantum reinforcement learning (QRL), alongside privacy-preserving techniques like quantum federated machine learning, bolstered by concrete programming examples. Throughout the tutorial, all topics and concepts are brought to life through practical demonstrations executed on a quantum computer simulator. Designed with novices in mind, the content caters to those eager to embark on their journey into QML. Attendees will also receive guidance on further reading materials, as well as software packages and frameworks to explore beyond the session.} }
@inproceedings{10.1145/3744367.3744386, title = {The Application and Development of Machine Learning in Depression Diagnosis}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems}, pages = {113--116}, year = {2025}, isbn = {9798400715068}, doi = {10.1145/3744367.3744386}, url = {https://doi.org/10.1145/3744367.3744386}, author = {Wang, Huaixiong and Wang, Jian}, keywords = {Application Research, Depression, Machine Learning}, abstract = {The analysis scrutinizes the integration of machine learning in depression diagnosis, noting its impact on early detection, individualized care, and predictive analytics.The application of machine learning boosts diagnostic accuracy via the examination of clinical, psychological, and neuroimaging data.This study delineates supervised, unsupervised, and reinforcement learning approaches, exploring their deployment in data-driven diagnostics, neuroimaging analytics, and natural language processing (NLP).In addition, the predictive power of machine learning aids in the anticipation of treatment efficacy, facilitating the design of tailored treatment approaches.Despite its bright prospects, unresolved challenges pertain to data integrity, privacy concerns, and model interpretive issues.For wider deployment, further clinical trials and interdepartmental collaboration are imperative.For wider deployment, further clinical trials and interdepartmental collaboration are imperative.} }
@inbook{10.1145/3757749.3757773, title = {Research on Concrete Strength Based on Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {149--154}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757773}, author = {Wei, Wanhua}, abstract = {Under the "Dual Carbon" strategic background, there is an increasingly urgent demand for synergistic optimization between low-carbon production and high-performance concrete. To address the limitations of traditional strength prediction methods, including insufficient analysis of multi-component interactions and high trial mix costs, this study proposes the use of Random Forest, Gradient Boosting Decision Tree and Stacking models to investigate the nonlinear effects of various components on concrete compressive strength and establish a concrete strength prediction system. Empirical analysis demonstrates that the Stacking model significantly outperforms other models. Feature importance analysis reveals that Age In Days, Cement Component and Water Component are dominant factors affecting concrete strength, while Fly Ash Component shows relatively lower influence. This model provides a data-driven decision-making tool for industrial solid waste recycling and low-carbon concrete mix design.} }
@inproceedings{10.1145/3708778.3708783, title = {Machine Learning-Based Hyper-Heuristics: A Clear Insight}, booktitle = {Proceedings of the 2024 7th International Conference on Computational Intelligence and Intelligent Systems}, pages = {29--37}, year = {2025}, isbn = {9798400717437}, doi = {10.1145/3708778.3708783}, url = {https://doi.org/10.1145/3708778.3708783}, author = {Bouazza, Wassim}, keywords = {Hyper-heuristics, Machine Learning, Optimization, Reinforcement Learning, Meta-Learning, Heuristic Selection, State-of-the-Art Review, Automated Problem Solving}, abstract = {In various scientific and engineering disciplines, decision-making processes frequently rely on optimization strategies to identify the best solution from a multitude of potential options. Traditional optimization methods have proven effective for many challenges; however, they face substantial difficulties when applied to complex, dynamic, and large-scale scenarios. Hyper-heuristics (HHs) have emerged as a promising alternative, offering a flexible and adaptive approach by optimizing the selection or creation of heuristics rather than directly tackling problem solutions. The integration of machine learning (ML) with hyper-heuristics has significantly propelled this field forward, enabling data-driven methods to enhance the selection and creation of heuristics. This article provides a comprehensive overview of the current landscape of machine learning-based hyper-heuristics. It categorizes and analyzes existing approaches, emphasizing the various ML techniques employed and their specific applications. Furthermore, it identifies emerging trends and future research directions aimed at enhancing the efficiency, adaptability, and robustness of HH methodologies through advanced ML techniques. By consolidating these insights, this review aims to equip researchers and practitioners with a deep understanding of the evolving landscape and potential opportunities for leveraging machine learning in the design of optimization heuristics.} }
@inproceedings{10.1145/3674029.3674049, title = {Rating Prediction of Football Players using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {121--126}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674049}, url = {https://doi.org/10.1145/3674029.3674049}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco}, abstract = {This research Analysis the prediction of football player ratings through the application of diverse machine learning algorithms. Rating systems for sports teams have gathered considerable attention in academic research. The approach used by the authors of this paper serves as an effort to streamline scouts and performance analytics. Leveraging linear regression, decision tree regressor, random forest regressor, gradient boosting regressor, support vector regressor, voting regressor, ridge regression, lasso regression, k-nearest neighbours’ regression, Huber regression and elastic-net regression. The Analysis explores the efficiency of each algorithm and concludes that Support Vector Regressor algorithm performs the best with 91.84\% accuracy on the testing data followed by the Gradient Boosting Regressor with 90.78\%, Voting Regressor with 91.68\% and Random Forest Regressor with 88.89\%. Apart from them the K-Nearest Neighbours Regression Algorithm highly overfits the model with 100\% accuracy on the training set and 70.71\%. The conclusions drawn underscore the critical importance of judiciously selecting algorithms tailored to the specific characteristics of the dataset for precise and reliable player rating predictions.} }
@inproceedings{10.1145/3723498.3723843, title = {Diverse Level Generation via Machine Learning of Quality Diversity}, booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games}, year = {2025}, isbn = {9798400718564}, doi = {10.1145/3723498.3723843}, url = {https://doi.org/10.1145/3723498.3723843}, author = {Sfikas, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N.}, keywords = {Procedural content generation, machine learning, novelty search, quality diversity, strategy maps}, abstract = {Can we replicate the power of evolutionary algorithms in discovering good and diverse game content via generative machine learning (ML) techniques? This question could subvert current trends in procedural content generation (PCG) and beyond. By learning the behavior of quality-diversity (QD) evolutionary algorithms through ML, we stand to overcome the computational challenges inherent in QD search and ensure that the benefits of QD search are reproduced by efficient generative models. We introduce a novel, end-to-end methodology named Machine Learning of Quality Diversity (MLQD) which is executed in two steps. First, tailored QD evolution creates large and diverse training datasets from the ground up. Second, sophisticated ML architectures such as the Transformer learn the datasets’ underlying distributions, resulting in generative models that can emulate QD search via stochastic inference. We test MLQD on the use-case of generating strategy game map sketches, a task characterized by stringent constraints and a multidimensional feature space. Our findings are promising, demonstrating that the Transformer architecture can capture both the diversity and the quality traits of the training sets, successfully reproducing the behavior of a range of tested QD algorithms. This marks a significant advancement in our quest to automate the creation of high-quality, diverse game content, pushing the boundaries of what is possible in PCG and generative AI at large.} }
@article{10.1145/3728369, title = {Prevention of Data Poisonous Threats on Machine Learning Models in e-Health}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3728369}, url = {https://doi.org/10.1145/3728369}, author = {Alruwaili, Etidal and Moulahi, Tarek}, keywords = {Machine Learning, e-health, attack, poisonous threats, prevention}, abstract = {Machine learning is widely used across various fields, including e-health, to enhance efficiency, classify events, and make accurate predictions, such as diagnosing diseases and prescribing medications. However, machine learning models are increasingly vulnerable to data poisoning attacks, which manipulate training data to degrade model accuracy and cause incorrect predictions. This study focuses on detecting data poisoning in e-health applications by simulating label-flipping attacks at different rates (5\%, 25\%, 50\%, 75\%) on breast cancer and diabetes datasets. The performance of machine learning models in disease detection was evaluated before and after poisoning, alongside their ability to detect poisoned data. Results show that models perform significantly better on clean data, with a marked deterioration at higher poisoning rates (50\%–75\%). The Random Forest (RF) and Gradient Boosting (GB) models proved most effective in detecting poisoned data, particularly at higher rates of poisoning. Conversely, the Logistic Regression (LR) and Multi-layer Perceptron (MLP) models tended to overgeneralize, leading to false positives, especially in the breast cancer dataset. This study highlights the importance of safeguarding ML models in e-health from data poisoning threats.} }
@inproceedings{10.1145/3721146.3721935, title = {Machine Learning-based Deep Packet Inspection at Line Rate for RDMA on FPGAs}, booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems}, pages = {148--155}, year = {2025}, isbn = {9798400715389}, doi = {10.1145/3721146.3721935}, url = {https://doi.org/10.1145/3721146.3721935}, author = {Heer, Maximilian Jakob and Ramhorst, Benjamin and Alonso, Gustavo}, keywords = {FPGA, remote direct memory access (RDMA), deep packet inspection, machine learning, location = World Trade Center, Rotterdam, Netherlands}, abstract = {FPGAs are becoming increasingly important in the cloud and data centers, especially as network-attached accelerators or reconfigurable Network Interface Cards (NICs). In the cloud, Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCEv2) has emerged as the de facto standard protocol for data transport due to its low latency and high throughput. However, RDMA has several access control weaknesses limiting its applicability in the cloud. In this paper, we explore using machine learning-based deep packet inspection (DPI) as an enhancement to an open-source FPGA RDMA stack. The ultra low-latency ML model is integrated on the RDMA datapath and allows for detection of specific content in RDMA payloads (e.g., executables) at a line rate of 100Gbps while using less than 1\% of the available resources. Compared with existing work, our solution operates on the full message payload, at the transport level, and on a complete RDMA stack without sacrificing compatibility with RoCEv2 and its native performance characteristics, proving its potential as an end-to-end solution.} }
@inproceedings{10.1145/3747227.3747259, title = {Fine-Grained Seismic Damage Assessment of Buildings Based on Machine Learning and Evidential Reasoning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {189--196}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747259}, url = {https://doi.org/10.1145/3747227.3747259}, author = {Zhang, Ying and Guo, Hong-Mei and Yin, Wen-Gang and Zhao, Zhen and He, Zong-Hang}, keywords = {Belief rule base, Damage level, Evidential reasoning, Fine-grained seismic damage assessment, Individual building, Machine learning}, abstract = {Existing seismic damage assessment methods for buildings often suffer from coarse results (based on groups of buildings), complex processes, and poor adaptability. To address these limitations, this paper leverages the advantages of machine learning in complex data processing and algorithmic adaptability, integrates expert knowledge, and combines qualitative and quantitative information to propose a fine-grained seismic damage assessment method for individual buildings based on machine learning and evidential reasoning. Validation using historical seismic damage cases demonstrates that this method enables rapid, accurate, and intelligent assessment of building seismic damage. It outputs all possible damage levels and corresponding probabilities for individual buildings under the combined effects of multiple seismic factors. These quantitative, fine-grained results provide critical support for pre-earthquake risk perception, post-earthquake rapid damage mapping, and precision disaster management.} }
@inproceedings{10.1145/3696673.3723076, title = {Obstructive Lung Disease Classification from Electrocardiograms Using Machine Learning}, booktitle = {Proceedings of the 2025 ACM Southeast Conference}, pages = {221--226}, year = {2025}, isbn = {9798400712777}, doi = {10.1145/3696673.3723076}, url = {https://doi.org/10.1145/3696673.3723076}, author = {Abdoulaye Soumana, Aboubacar and Lamichhane, Prajwol and Shabbir, Mehlam and Liu, Xudong and Nasseri, Mona and Helgeson, Scott}, keywords = {obstructive lung disease classification, machine learning, deep learning, electrocardiogram, digital health, health informatics, location = Southeast Missouri State University, Cape Girardeau, MO, USA}, abstract = {Pulmonary diseases, such as chronic obstructive pulmonary disease (COPD) and asthma, are among the leading causes of death in the US. These lung diseases often are diagnosed by pulmonologists using physical exam (e.g., lung auscultation) and objective measurement of lung function with pulmonary function testing (PFT). These extensive tests, however, can be inaccessible to many patients due to limited resources and availability. Nowadays hand-held medical devices (e.g., electrocardiogram (ECG) monitors) are already available to such patients and can yield ECG data that potentially could be used for diagnosis. In this paper, we explore the use of easily accessible ECGs to train machine learning models to classify obstructive lung disease (OLD). Not only do we utilize the time-series raw ECG directly, we also define and employ eleven features derived from the PQRST sequences of the ECGs. To this end, we develop and experiment with two approaches: deep neural network models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)) trained using the ECG signals directly, and non-neural models (e.g., support vector machines (SVMs) and logistic regression) trained using the derived features from ECGs. In the task of classifying whether a patient has OLD or not, our results show that deep neural network models outperformed the non-neural models, though the difference is within 3\% on accuracy and F1-score metrics.} }
@inproceedings{10.1145/3689609.3699919, title = {Abstract Domains for Machine Learning Verification (Keynote)}, booktitle = {Proceedings of the 10th ACM SIGPLAN International Workshop on Numerical and Symbolic Abstract Domains}, pages = {1}, year = {2024}, isbn = {9798400712173}, doi = {10.1145/3689609.3699919}, url = {https://doi.org/10.1145/3689609.3699919}, author = {Urban, Caterina}, abstract = {Machine learning (ML) software is increasingly being deployed in high-stakes and sensitive applications, raising important challenges related to safety, privacy, and fairness. In response, ML verification has quickly gained traction within the formal methods community, particularly through techniques like abstract interpretation. However, much of this research has progressed with minimal dialogue and collaboration with the ML community, where it often goes underappreciated. In this talk, we advocate for closing this gap by surveying possible ways to make formal methods more appealing to the ML community. We will survey our recent and ongoing work in the design and development of abstract domains for machine learning verification, and discuss research questions and avenues for future work in this context.} }
@article{10.1145/3757892.3757908, title = {Re-Evaluating Storage Carbon Emissions In Machine Learning Workloads}, journal = {SIGENERGY Energy Inform. Rev.}, volume = {5}, pages = {111--117}, year = {2025}, doi = {10.1145/3757892.3757908}, url = {https://doi.org/10.1145/3757892.3757908}, author = {Kopczyk, Dorota and Chandra, Abhishek}, keywords = {sustainable computing, ML system, storage carbon emissions, SSD vs HDD, embodied carbon, operational carbon, carbon-aware ML infrastructure}, abstract = {As machine learning (ML) workloads grow in scale, the carbon impact of data storage is underexplored. Despite the dominance of solid-state drives (SSDs) in ML pipelines for their performance benefits, the environmental trade-offs with traditional hard disk drives (HDDs) are not well understood. We compare the performance and total carbon cost of SSDs and HDDs in ML training workloads. To evaluate carbon impact driven by storage, we use the MLPerf Storage benchmark along with carbon emissions data from two energy grids. We find that although SSDs have significantly higher embodied emissions, their lower operational carbon and faster runtimes make them more efficient for I/O-bound ML workloads—especially once data exceeds memory capacity. While SSDs generally amortize their carbon cost over time, often outperforming HDDs in total emissions, there are caveats. When considering regional energy mix, results suggest that carbon-aware ML infrastructure should consider workload size, memory constraints, and grid intensity—not just device specifications.} }
@inproceedings{10.1145/3647750.3647755, title = {Machine Learning-based Models for Predicting Defective Packages}, booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing}, pages = {25--31}, year = {2024}, isbn = {9798400716546}, doi = {10.1145/3647750.3647755}, url = {https://doi.org/10.1145/3647750.3647755}, author = {Wang, Yushuo and Mo, Ran and Zhang, Yao}, keywords = {Code Metrics, Defective Packages Prediction, Machine Learning, location = Singapore, Singapore}, abstract = {Software defects are often expensive to fix, especially when they are identified late in development. Packages encapsulate logical functionality and are often developed by particular teams. Package-level defect prediction provides insights into defective designs or implementations in a system early. However, there is little work studying how to build prediction models at the package level. In this paper, we develop prediction models by using seven machine-learning algorithms and code metrics. After evaluating our approach on 20 open-source projects, we have presented that we can build effective models for predicting defective packages by using an appropriate set of metrics. However, there is no single set of metrics that can be generalized across all projects. Our study demonstrates the potential for machine-learning models to enable effective package-level defect prediction. This can guide testing and quality assurance to efficiently locate and fix defects.} }
@inproceedings{10.1145/3723178.3723313, title = {Panic Attack Frequency Detection Using Machine Learning Algorithm}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {1014--1025}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723313}, url = {https://doi.org/10.1145/3723178.3723313}, author = {Nasiruddin, Kazi Md and Haque, Md. Jawadul and Hasan, Md. Jahid and Ahmed Shams, Md. Shayed and Shefat, Syed Nafiul}, keywords = {Classification, HoeffdingTree, J48, Machine Learning, Na\"ve Bayes, Panic attack frequency, Prediction, REPTree, Weka}, abstract = {Abstract:An individual's life can be heavily influenced by mental disorders. Machine learning has shown itself to be a possible tool for the prediction and treatment of some diseases. In this study, different machine learning classifiers were used to predict the number of panic episodes using different datasets in various patients. This dataset contained clinical and demographic details for 500 cases. Preparing data, employing different algorithms (e.g., J48, Naive Bayes, HoeffdingTree, REPTree, Ensemble methods), and evaluating model performance indicators are all important steps within this process.The findings showed that machine learning models exhibited accurate predictions of panic attack frequencies; all performance indexes have been led by the J48 decision tree algorithm. At the ‘Often’ frequency level, J48 had 94.8\% accuracy, 0.897 precision, and an F1-score of 0.933, while the recall score was found to be 0.973, i.e., for the same ‘Often’ level. For ‘Often,’ however, Naive Bayes showed the highest precision (0.992) but the lowest overall performance. These results suggest the potential for predicting the frequency of panic attacks through instructional machine learning models, which may assist with patient care and clinical decision making. Nevertheless, more investigation and study are needed to increase precision and generalizability among various demographics.} }
@article{10.5555/3722577.3722945, title = {Localisation of regularised and multiview support vector machine learning}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Gheondea, Aurelian and Tilki, Cankat}, keywords = {operator valued reproducing kernel Hilbert spaces, manifold co-regularised and multiview learning, support vector machine learning, loss functions, representer theorem}, abstract = {We prove some representer theorems for a localised version of a semisupervised, manifold regularised and multiview support vector machine learning problem introduced by H.Q. Minh, L. Bazzani, and V. Murino, Journal of Machine Learning Research, 17(2016) 1-72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional underlying Hilbert spaces are considered. We show that the general framework allows infinite dimensional Hilbert spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that lead to systems of partially nonlinear equations for which some Newton's approximation methods based on the interior point method can be used. Some numerical experiments are performed on a toy model that illustrate the tractability of the methods that we propose.} }
@inproceedings{10.1145/3670474.3685970, title = {Machine Learning VLSI CAD Experiments Should Consider Atomic Data Groups}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685970}, url = {https://doi.org/10.1145/3670474.3685970}, author = {Gunter, Andrew David and Wilton, Steven}, keywords = {CAD, FPGA, VLSI, information leakage, machine learning, routing, supervised learning, location = Salt Lake City, UT, USA}, abstract = {Machine learning (ML) has proved useful across a wide range of applications in the very-large-scale integration computer-aided design (VLSI CAD) domain. To avoid overestimating ML models' generalization capabilities for real-world deployments, best practices utilize realistic data and avoid test set information leakage during ML model preparation. In this paper we identify a further consideration, atomic data groups, which are sets of very highly correlated data that may also lead to such overestimation if not accounted for in train-test splits during model evaluation. We investigate the potential impact of atomic data groups in experimental design through a case study of field-programmable gate array (FPGA) routing. Our investigations show that model performance in deployment is overestimated by 38\% in this case study when atomic data groups are ignored. We hope that these results motivate other ML CAD practitioners to be critical of their train-test splits and identify when atomic data groups are relevant to their model evaluations.} }
@inproceedings{10.1145/3726302.3731695, title = {The Second Tutorial on Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {4130--4133}, year = {2025}, isbn = {9798400715921}, doi = {10.1145/3726302.3731695}, url = {https://doi.org/10.1145/3726302.3731695}, author = {Diaz, Fernando and Drozdov, Andrew and Kim, To Eun and Salemi, Alireza and Zamani, Hamed}, keywords = {framework, information retrieval, machine learning, location = Padua, Italy}, abstract = {Retrieval-Enhanced Machine Learning (REML) refers to the use of information retrieval (IR) methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability, and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but not limited to, NLP. What is unique to our approach is the use of consistent notations to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at a website. Building on the momentum of our successful workshop at SIGIR 2023 and our tutorial at SIGIR-AP 2024, this year's tutorial features updated content with an emphasis on retrieval technologies used across the broader ML community. We also highlight their role in emerging, future-facing applications such as language agents and evolving scenarios where the extensive body of knowledge from IR can provide critical insights and capabilities.} }
@inproceedings{10.1145/3672608.3707891, title = {Data Balancing for Mitigating Sampling Bias in Machine Learning}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {1204--1212}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707891}, url = {https://doi.org/10.1145/3672608.3707891}, author = {Inoc\^encio J\'unior, Ronaldo and Basgalupp, M\'arcio and Ludermir, Teresa and Lorena, Ana Carolina}, keywords = {machine learning, fairness, data balancing, location = Catania International Airport, Catania, Italy}, abstract = {We increasingly integrate technology into our daily activities, and using Machine Learning (ML) algorithms in various domains has become a common practice. However, in crucial sectors where algorithmic decisions significantly impact people's lives, there is a need to scrutinize these decisions more carefully. Using these algorithms in critical areas, such as courtrooms, raises concerns about potential bias and prejudice, directly affecting the justice and partiality of these tools. There is an urge to create algorithms supporting ethical decisions. This paper proposes using data balancing techniques to mitigate the sample bias present in datasets, aiming to make subsequent ML algorithm training more impartial. A version of the ADASYN algorithm is developed, which performs data balancing at both the class level and at the level of protected attributes, enhancing the diversity and representativeness of the protected groups in the datasets. Experimental results show the technique can promote greater fairness in the predictions of different ML models while keeping a good trade-off with overall accuracy.} }
@inproceedings{10.1145/3721146.3721957, title = {Utilizing Large Language Models for Ablation Studies in Machine Learning and Deep Learning}, booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems}, pages = {230--237}, year = {2025}, isbn = {9798400715389}, doi = {10.1145/3721146.3721957}, url = {https://doi.org/10.1145/3721146.3721957}, author = {Sheikholeslami, Sina and Ghasemirahni, Hamid and Payberah, Amir H. and Wang, Tianze and Dowling, Jim and Vlassov, Vladimir}, keywords = {ablation studies, machine learning, deep learning, deep neural networks, feature ablation, model ablation, large language models, location = World Trade Center, Rotterdam, Netherlands}, abstract = {In Machine Learning (ML) and Deep Learning (DL) research, ablation studies are typically performed to provide insights into the individual contribution of different building blocks and components of an ML/DL system (e.g., a deep neural network), as well as to justify that certain additions or modifications to an existing ML/DL system can result in the proposed improved performance. Although dedicated frameworks for performing ablation studies have been introduced in recent years, conducting such experiments is still associated with requiring tedious, redundant work, typically involving maintaining redundant and nearly identical versions of code that correspond to different ablation trials. Inspired by the recent promising performance of Large Language Models (LLMs) in the generation and analysis of ML/DL code, in this paper we discuss the potential of LLMs as facilitators of ablation study experiments for scientific research projects that involve or deal with ML and DL models. We first discuss the different ways in which LLMs can be utilized for ablation studies and then present the prototype of a tool called AblationMage, that leverages LLMs to semi-automate the overall process of conducting ablation study experiments. We showcase the usability of AblationMage as a tool through three experiments, including one in which we reproduce the ablation studies from a recently published applied DL paper.} }
@article{10.1145/3586991, title = {Machine Learning Sensors}, journal = {Commun. ACM}, volume = {66}, pages = {25--28}, year = {2023}, issn = {0001-0782}, doi = {10.1145/3586991}, url = {https://doi.org/10.1145/3586991}, author = {Warden, Pete and Stewart, Matthew and Plancher, Brian and Katti, Sachin and Reddi, Vijay Janapa}, abstract = {A design paradigm for the future of intelligent sensors.} }
@inproceedings{10.1145/3674029.3674030, title = {Machine Learning Application for Real-Time Simulator}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {1--5}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674030}, url = {https://doi.org/10.1145/3674029.3674030}, author = {Hadadi, Azadeh and Chardonnet, Jean-R\'emy and Guillet, Christophe and Ovtcharova, Jivka}, keywords = {artificial intelligence, auto-adaptive systems, real-time systems, location = Oslo, Norway}, abstract = {This paper presents a groundbreaking research initiative that focuses on the development of an intelligent architecture for Adaptive Virtual Reality Systems (AVRS) in immersive virtual environments. The primary objective of this architecture is to enable real-time artificial intelligence training and adapt the virtual environment based on user states or external parameters. In a case study focused on detecting cybersickness, an undesired side effect in immersive virtual environments, we utilized this architecture to train an artificial intelligence model and personalize it for individual users in a driving simulator application. By leveraging the capabilities of this architecture, we can optimize virtual reality experiences for individual users, leading to increased comfort. We evaluated the system’s performance in terms of memory usage, CPU and GPU usage, temperature monitoring, frame rate, and network performance, and our results demonstrated the efficiency of our proposed architecture.} }
@article{10.1145/3764582, title = {A Survey of Quantum Machine Learning: Foundations, Algorithms, Frameworks, Data and Applications}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3764582}, url = {https://doi.org/10.1145/3764582}, author = {Rodr\'guez-D\'az, Francesc and Guti\'errez-Avil\'es, David and Troncoso, Alicia and Mart\'nez-\'Alvarez, Francisco}, keywords = {Quantum machine learning, quantum foundations, quantum algorithms, quantum computing frameworks, quantum datasets, quantum applications}, abstract = {Quantum machine learning combines quantum computing with machine learning to solve complex computational problems more efficiently than classical approaches. This survey provides an introduction to the foundations, algorithms, frameworks, data and applications of quantum machine learning, serving as a resource for researchers and practitioners. We begin by reviewing existing surveys to identify gaps that this work addresses, followed by a detailed discussion of the foundational principles of quantum mechanics and machine learning essential for quantum machine learning. Key algorithms are examined, highlighting their mechanisms, advantages, and applications across various domains. Current frameworks and platforms for implementing quantum machine learning algorithms are explored, emphasizing their unique features and suitability for different contexts. Existing quantum datasets for practical usage are also reported and commented on. This survey also reviews over 135 articles, categorized into theoretical and practical contributions, to identify key advances, limitations, and application areas within quantum machine learning. Critical challenges such as hardware limitations, error rates, and scalability are analyzed to detect the obstacles that must be addressed for practical deployment. By synthesizing these elements into a structured overview, this survey aims at serving as both an introduction and a guide for advancing research and development in this disruptive field.} }
@inproceedings{10.1145/3712255.3716512, title = {Evolutionary Computation meets Machine Learning for Combinatorial Optimisation}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1539--1561}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716512}, url = {https://doi.org/10.1145/3712255.3716512}, author = {Mei, Yi and Raidl, G\"unther} }
@inproceedings{10.1145/3712255.3734234, title = {Evidential Fuzzy Rule-Based Machine Learning to Quantify Classification Uncertainty}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {69--70}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734234}, url = {https://doi.org/10.1145/3712255.3734234}, author = {Shiraishi, Hiroki and Ishibuchi, Hisao and Nakata, Masaya}, keywords = {learning fuzzy-classifier systems, evidential reasoning, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Learning Fuzzy-Classifier Systems (LFCSs), also known as evolutionary fuzzy rule-based machine learning, combine evolutionary algorithms with fuzzy rules to create interpretable models. However, traditional inference schemes lack mechanisms to quantify uncertainty in predictions. This paper introduces a novel class inference scheme based on the Dempster-Shafer Theory of Evidence that explicitly models epistemic uncertainty through an "I don't know" state. By calculating belief masses for each class hypothesis and uncertainty, our approach enhances robustness in real-world applications. Experiments on real-world datasets demonstrate statistically significant improvements in classification performance compared to conventional approaches. Our method forms smoother decision boundaries and provides quantitative measures of prediction confidence, addressing a critical gap in reliable decision-making for fuzzy rule-based machine learning.This paper summarizes the ACM TELO article: Hiroki Shiraishi, Hisao Ishibuchi, and Masaya Nakata. 2025. A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems. ACM Transactions on Evolutionary Learning and Optimization. https://doi.org/10.1145/3717613 [7]. Our implementation is available at https://github.com/YNU-NakataLab/jUCS.} }
@article{10.1145/3765735, title = {Applications and Challenges of Fairness APIs in Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3765735}, url = {https://doi.org/10.1145/3765735}, author = {Das, Ajoy and Uddin, Gias and Chowdhury, Shaiful and Akhond, Mostafijur Rahman and Hemmati, Hadi}, keywords = {bias, api, github, fairness}, abstract = {Machine Learning software systems are frequently used in our day-to-day lives. Some of these systems are used in various sensitive environments to make life-changing decisions. Therefore, it is crucial to ensure that these AI/ML systems do not make any discriminatory decisions for any specific groups or populations. In that vein, different bias detection and mitigation open-source software libraries (aka API libraries) are being developed and used. In this paper, we conduct a qualitative study to understand in what scenarios these open-source fairness APIs are used in the wild, how they are used, and what challenges the developers of these APIs face while developing and adopting these libraries. We have analyzed 204 GitHub repositories (from a list of 1885 candidate repositories) which used 13 APIs that are developed to address bias in ML software. We found that these APIs are used for two primary purposes (i.e., learning and solving real-world problems), targeting 17 unique use-cases. Our study suggests that developers are not well-versed in bias detection and mitigation; they face lots of troubleshooting issues, and frequently ask for opinions and resources. Our findings can be instrumental for future bias-related software engineering research, and for guiding educators in developing more state-of-the-art curricula.} }
@inproceedings{10.1145/3696687.3696700, title = {Precise Issuance of Meituan Merchants’ Coupons with Machine Learning}, booktitle = {Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, pages = {71--75}, year = {2024}, isbn = {9798400709876}, doi = {10.1145/3696687.3696700}, url = {https://doi.org/10.1145/3696687.3696700}, author = {Zhang, Xue and Qiu, Jie and Li, Bo}, abstract = {With the popularity of mobile Internet, the “Online-to-Offline” (O2O) business model has become popular. Issuing coupons to attract new customer registrations and keep old customers active is an important marketing tool for O2O companies. But the random distribution of coupons can be annoying to those non-target customers. For merchants, the transition of issuing coupons to merchants will not only increase the promotion cost but also have a negative effect on their brand reputation. The purpose of this study is to analyze transaction data and build a model to predict the redemption of coupons, so as to achieve the precise issue of coupons by merchants. We use machine learning to analyze the consumption data and extract features from five categories: coupons, merchants, consumers, consumers-merchants, and other categories. A total of 44 features are extracted and the XGBoost (eXtreme Gradient Boosting) model is adopted. It has been verified that the prediction results of the application of the XGBoost model can nearly increase 50\% net profits of the merchants.} }
@inproceedings{10.1145/3643796.3648455, title = {JetTrain: IDE-Native Machine Learning Experiments}, booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments}, pages = {59--61}, year = {2024}, isbn = {9798400705809}, doi = {10.1145/3643796.3648455}, url = {https://doi.org/10.1145/3643796.3648455}, author = {Trofimov, Artem and Kostyukov, Mikhail and Ugdyzhekov, Sergei and Ponomareva, Natalia and Naumov, Igor and Melekhovets, Maksim}, keywords = {integrated development environment, machine learning, MLOps, location = Lisbon, Portugal}, abstract = {Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.} }
@inproceedings{10.1145/3716368.3735221, title = {A Machine Learning-Assisted Placement Flow with Pin Accessibility Awareness}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {221--226}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735221}, url = {https://doi.org/10.1145/3716368.3735221}, author = {Hsieh, Min-Feng and Wang, Ting-Chi}, keywords = {Placement, Pin Accessibility, Machine Learning}, abstract = {Pin accessibility is a critical concern in placement, as poor accessibility can lead to routing difficulties and design rule violations (DRVs). This paper proposes a pin accessibility-aware placement flow that uses dynamic cell margin generation and nonuniform blockage to improve pin accessibility and routing performance. Additionally, we develop a machine learning-based parameter tuner to optimize the tuning process. Our approach significantly improves post-routing results, enhancing worst negative slack by 72.12\%, total negative slack by 73.06\%, reducing wirelength by 2.48\%, eliminating DRVs by 100\%, and reducing power consumption by 1.31\% and execution time of placement, clock tree synthesis, and routing by 15.97\% compared with a default commercial placement flow.} }
@inproceedings{10.1145/3757110.3757159, title = {Construction of Student Safety Management System based on Big Data and Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {283--286}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757159}, url = {https://doi.org/10.1145/3757110.3757159}, author = {Tang, Jiabing}, keywords = {Big Data, Machine Learning, Security Management, System Building}, abstract = {With the development of science and technology, more and more industries begin to involve big data. This paper aims to explore and build a student safety management model based on big data and machine learning. On the basis of summarizing the basic theories, research processes and basic paradigms involved in the previous studies of "data mining and knowledge discovery" and "information extraction and information analysis", a data-driven PMDA-Diki public security intelligence analysis and extraction system model is independently proposed. This model describes the basic evolution path of data processing flow and data existence form in the process of transforming multi-source data into security intelligence, and also defines the operation steps of extracting public security intelligence based on data, namely: By means of "processing → data mining → knowledge discovery → algorithm activation", the data can be gradually extracted into the required public security information according to the order of "data → information → knowledge → intelligence". Big data technology, in short, is the ability to extract valuable information quickly from a wide variety of types of data. Combining with rich contour feature visible light image, using the scale invariant feature transform (SIFT) algorithm for image registration fusion processing, image fusion is both advantages, through the deconvolution of fusion image feature extraction of feature extraction algorithm is exclusive feature mapping matrix, different from traditional random initialization convolution neural network learning method of convolution kernels, An improved neural network identification model, through the characteristics of the migration study will get mapping matrix as the initial convolution convolution neural network model of nuclear matrix, using the improved network model testing of the visible light and infrared images of repeated iterative learning, training a good model can improve the students' safety equipment management system identification accuracy, So as to realize the intelligent identification of equipment. Through the above research, it can realize the intelligent identification and fault warning of safety management equipment, and transform the traditional manual monitoring mode into intelligent monitoring. The rationality and effectiveness of the proposed method are verified by an example analysis of the measured data.} }
@inbook{10.1145/3672608.3707742, title = {ASML-REG: Automated Machine Learning for Data Stream Regression}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {440--447}, year = {2025}, isbn = {9798400706295}, url = {https://doi.org/10.1145/3672608.3707742}, author = {Verma, Nilesh and Bifet, Albert and Pfahringer, Bernhard and Bahri, Maroua}, abstract = {Online learning scenarios present a significant challenge for AutoML techniques due to the dynamic nature of data distributions, where the optimal model and configuration may change over time. While most research in machine learning for data streams has primarily focused on classification algorithms, regression methods have received significantly less attention. To address this gap, we propose ASML-REG, an Automated Streaming Machine Learning framework designed specifically for regression tasks on data streams. ASML-REG continuously explores a vast and diverse space of pipeline configurations, adapting to evolving data by focusing on the current best design, performing adaptive random searches in promising areas, and maintaining an ensemble of top-performing pipelines. Our experiments with real and synthetic datasets demonstrate that ASML-REG significantly outperforms current state-of-the-art data stream regression algorithms.} }
@article{10.1145/3728368, title = {Fairness Challenges in the Design of Machine Learning Applications for Healthcare}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3728368}, url = {https://doi.org/10.1145/3728368}, author = {Ryan, Seamus and Cai, Wanling and Bowman, Robert and Doherty, Gavin}, keywords = {Fairness, Healthcare, Machine Learning, Interviews}, abstract = {Machine learning-augmented applications have the potential to be powerful tools for decision-making in healthcare. However, healthcare is a complex domain that presents many challenges. These challenges, such as medical errors, clinician–patient relationships and treatment preferences, must be addressed to ensure fairness in ML-augmented healthcare applications. To better understand the influence these challenges have on fairness, 16 experienced engineers and designers with domain knowledge in healthcare technology were interviewed about how they would prioritise fairness in 3 healthcare scenarios (well-being improvement, chronic illness management, acute illness treatment). Using a template analysis, this work identifies the key considerations in the creation of fair ML for healthcare. These considerations clustered into categories related to technology, healthcare context and user perspectives. To explore these categories, we propose the stakeholder fairness conceptual model. This framework aids designers and developers in understanding the complex considerations that stem from the building, management and evaluation of ML-augmented healthcare applications, and how they affect the expectations of fairness. This work then discusses how this model may be applied when the health technology is directly provisioned to users, without a healthcare provider managing its use or adoption. This article contributes to the understanding of fairness requirements in healthcare, including the effect of healthcare errors, clinician-application collaboration and how the evaluation of healthcare technology becomes part of the fairness design process.} }
@inproceedings{10.1145/3674029.3674032, title = {An Analysis of Car Price Prediction using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {11--15}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674032}, url = {https://doi.org/10.1145/3674029.3674032}, author = {Bhatnagar, Parth and Lokesh, Gururaj Harinahalli and Shreyas, J and Flammini, Francesco and Gautam, Shivansh}, abstract = {This research paper explores machine learning techniques, such as voting regressors, gradient boosting regressors, random forest regressors, decision tree regressors, and support vector regressors, for car predicting the car price. Each machine learning technique has its own unique advantages and disadvantages, with the voting regressor exhibiting the best results. Methodologically, GridSearchCV is used to tune hyperparameters on a dataset of more than 200 automobiles, each with 26 parameters. The outcomes demonstrate the predictive power of regression and ensemble techniques, providing insightful information to practitioners in the business and academics alike. The training accuracies range from 16.87\% (MAPE) for Linear Regression, 96.78\% for Decision Tree Regressor, 96.49\% for Random Forest Regressor, 97.84\% for Gradient Boosting Regressor,95.8\% for Voting Regressor, 81.89\% for Support Vector Regressor, notably the testing accuracies vary from 19.44\% (MAPE) for Linear Regression, 87.76\% for Decision Tree Regressor, 89.75\% for Random Forest Regressor, 88.67\% for Gradient Boosting Regressor, 88.02\% for Voting Regressor, 79.55\% for Support Vector Regressor.} }
@inproceedings{10.1145/3690771.3690781, title = {Anomaly and Pattern Detection Using Advanced Machine Learning Models}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {20--26}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690781}, url = {https://doi.org/10.1145/3690771.3690781}, author = {Nacchanandana, Piyavachara and Suleiman, Basem and Anaissi, Ali and Picones, Gio}, keywords = {Generative Adversarial Network, Federated Learning, Federated Averaging, FedProx, Federated Personalisation, Datasets, Neutral Networks, Gaze Detection, Text Tagging, Anomaly Detection, Federated Median}, abstract = {This research addresses a unique intersection in the field of machine learning: the convergence of anomaly detection and decentralized learning. Although each domain has been extensively studied in isolation, their combination remains relatively unexplored. This study pioneers in investigating the practicality and efficiency of decentralized anomaly detection. We achieve this by integrating the GANomaly architecture with Federated Learning, a novel approach that allows us to analyze the impact of various factors on system performance. The core of our investigation revolves around four aggregation methods: Federated Averaging, FedProx, Federated Personalization, and the newly introduced Federated Median. We meticulously evaluate the efficacy of these methods within a combined GANomaly and Federated Learning framework, focusing on their performance across different levels of local computation and varying proportions of straggler clients per round. This research not only fills a gap in the existing literature, but also offers new insights into the optimization of decentralized learning systems for effective anomaly detection.} }
@article{10.1145/3616865, title = {Fairness in Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3616865}, url = {https://doi.org/10.1145/3616865}, author = {Caton, Simon and Haas, Christian}, keywords = {Fairness, accountability, transparency, machine learning}, abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.} }
@article{10.1613/jair.1.15665, title = {The Human in Interactive Machine Learning: Analysis and Perspectives for Ambient Intelligence}, journal = {J. Artif. Int. Res.}, volume = {81}, year = {2025}, issn = {1076-9757}, doi = {10.1613/jair.1.15665}, url = {https://doi.org/10.1613/jair.1.15665}, author = {Delcourt, Kevin and Trouilhet, Sylvie and Arcangeli, Jean-Paul and Adreit, Francoise}, abstract = {As the vision of Ambient Intelligence (AmI) becomes more feasible, the challenge of designing effective and usable human-machine interaction in this context becomes increasingly important. Interactive Machine Learning (IML) offers a set of techniques and tools to involve end-users in the machine learning process, making it possible to build more trustworthy and adaptable ambient systems. In this paper, our focus is on exploring approaches to effectively integrate and assist human users within ML-based AmI systems. Through a survey of key IML-related contributions, we identify principles for designing effective human-AI interaction in AmI applications. We apply them to the case of Opportunistic Composition, which is an approach to achieve AmI, to enhance collaboration between humans and Artificial Intelligence. Our study highlights the need for user-centered and context-aware design, and provides insights into the challenges and opportunities of integrating IML techniques into AmI systems.} }
@inproceedings{10.1145/3728199.3728270, title = {Compensation mechanism and machine learning application for cooperative watershed management in pollution control}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {423--430}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728270}, url = {https://doi.org/10.1145/3728199.3728270}, author = {Zhuo, Fenglian}, keywords = {Compensation Mechanism, Decision Tree, LSTM, Machine Learning, RNN, Random Forest}, abstract = {The management of urban and rural watersheds faces challenges due to outdated methods, highlighting the need to strengthen cost-sharing mechanisms and establish an ecological compensation framework. In this study, a comprehensive analysis was conducted using cooperative game theory and machine learning techniques in the Fujian Minjiang River Basin as an example. Based on the average tree solution (A-T solution), a fairer cost-sharing scheme is proposed, and a practical ecological compensation model is constructed by integrating the pollution intensity compensation factor. In addition, a machine learning approach was used to predict wastewater discharge trends and identify factors affecting water quality. The Random Forest model identified key factors including 'water temperature ((^ circ C ))', 'chemical oxygen demand (COD) (mg/L)' and 'dissolved oxygen (mg/L)'. Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) models were used to predict industrial wastewater discharges in Putian City from 1994 to 2022. The results suggest that regional geography has a significant impact on cost sharing and that coalition constraints contribute to a more equitable and balanced distribution of costs. The results of the study highlight the importance of pollution intensity in determining management costs and the need to develop compensation criteria. This take into account regional pollution levels to improve watershed management coordination. The LSTM model showed the highest accuracy with an R2 of 0.09, which exceeded the performance of traditional linear regression. This study provides insights that can inform improved watershed management, equitable cost sharing and ecological compensation, while highlighting the benefits of combining machine learning with traditional methods to improve forecasting and decision making.} }
@article{10.1145/3774418, title = {MLKAPS: Machine Learning and Adaptive Sampling for HPC Kernel Auto-tuning}, journal = {ACM Trans. Archit. Code Optim.}, year = {2025}, issn = {1544-3566}, doi = {10.1145/3774418}, url = {https://doi.org/10.1145/3774418}, author = {Jam, Mathys Eliott and Petit, Eric and de Oliveira Castro, Pablo and Defour, David and Henry, Greg and Jalby, William}, keywords = {Autotuning, input-aware optimization, Math Library}, abstract = {Many High-Performance Computing (HPC) libraries rely on decision trees to select the best kernel hyperparameters at runtime, depending on the input and environment. However, finding optimized configurations for each input and environment is challenging and requires significant manual effort and computational resources. This paper presents MLKAPS, a tool that automates this task using machine learning and adaptive sampling techniques. MLKAPS generates decision trees that tune HPC kernels’ design parameters to achieve efficient performance for any user input. MLKAPS scales to large input and design spaces, outperforming similar state-of-the-art auto-tuning tools in tuning time and mean speedup. We demonstrate the benefits of MLKAPS on the highly optimized Intel®MKL dgetrf LU kernel and show that MLKAPS finds blindspots in the manual tuning of HPC experts. It improves over (85\% ) of the inputs with a geomean speedup of 1.31. On the Intel®MKL dgeqrf QR kernel, MLKAPS improves performance on (85\% ) of the inputs with a geomean speedup of 1.18.} }
@article{10.1145/3652029, title = {Applied Machine Learning for Information Security}, journal = {Digital Threats}, volume = {5}, year = {2024}, doi = {10.1145/3652029}, url = {https://doi.org/10.1145/3652029}, author = {Samtani, Sagar and Raff, Edward and Anderson, Hyrum}, keywords = {Applied machine learning, deep learning, artificial intelligence, information security, cybersecurity}, abstract = {Information security has undoubtedly become a critical aspect of modern cybersecurity practices. Over the past half-decade, numerous academic and industry groups have sought to develop machine learning, deep learning, and other areas of artificial intelligence-enabled analytics into information security practices. The Conference on Applied Machine Learning (CAMLIS) is an emerging venue that seeks to gather researchers and practitioners to discuss applied and fundamental research on machine learning for information security applications. In 2021, CAMLIS partnered with ACM Digital Threats: Research and Practice (DTRAP) to provide opportunities for authors of accepted CAMLIS papers to submit their research for consideration into ACM DTRAP via a Special Issue on Applied Machine Learning for Information Security. This editorial summarizes the results of this Special Issue.} }
@article{10.1145/3688841, title = {An Exploratory Study on Machine Learning Model Management}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3688841}, url = {https://doi.org/10.1145/3688841}, author = {Latendresse, Jasmine and Abedu, Samuel and Abdellatif, Ahmad and Shihab, Emad}, keywords = {Software engineering, machine learning, model management}, abstract = {Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9\% of the identified activities belong to the maintenance category, with activities like refactoring (20.5\%) and documentation (18.3\%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3\%) and bug management (14.9\%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift toward automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models.} }
@inproceedings{10.1145/3731569.3764818, title = {LithOS: An Operating System for Efficient Machine Learning on GPUs}, booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles}, pages = {1--17}, year = {2025}, isbn = {9798400718700}, doi = {10.1145/3731569.3764818}, url = {https://doi.org/10.1145/3731569.3764818}, author = {Coppock, Patrick H. and Zhang, Brian and Solomon, Eliot H. and Kypriotis, Vasilis and Yang, Leon and Sharma, Bikash and Schatzberg, Dan and Mowry, Todd C. and Skarlatos, Dimitrios}, abstract = {The rapid growth of machine learning (ML) has made GPUs indispensable in datacenters and underscores the urgency of improving their efficiency. However, balancing diverse model demands with high utilization remains a fundamental challenge. Transparent, fine-grained GPU resource management that maximizes utilization, energy efficiency, and isolation requires an OS approach. This paper introduces LithOS, a first step towards a GPU OS.LithOS includes the following new abstractions and mechanisms for efficient GPU management: (i) a novel TPC Scheduler that supports spatial scheduling at the granularity of individual TPCs, unlocking efficient TPC stealing between workloads; (ii) a transparent kernel atomizer to reduce head-of-line blocking and allow dynamic resource reallocation mid-execution; (iii) a lightweight hardware right-sizing mechanism that dynamically determines the minimal TPC resources needed per atom; and (iv) a transparent power management mechanism that reduces power consumption based upon in-flight work characteristics.We build LithOS in Rust and evaluate its performance across a broad set of deep learning environments, comparing it to state-of-the-art solutions from NVIDIA and prior research. For inference stacking, LithOS reduces tail latencies by 13 compared to MPS; compared to the best-performing SotA, it reduces tail latencies by 4 while improving aggregate goodput by 1.3. Furthermore, in hybrid inference-training stacking, LithOS reduces tail latencies by 4.7 compared to MPS; compared to the best-performing SotA, it reduces tail latencies by 1.18 while improving aggregate throughput by 1.35. Finally, for a modest performance hit under 4\%, LithOS's hardware right-sizing provides a quarter of GPU capacity savings on average, while for a 7\% hit, LithOS's transparent power management delivers a quarter of GPU total energy savings on average. Overall, LithOS transparently increases GPU efficiency, establishing a foundation for future OS research on GPUs.} }
@inbook{10.1145/3760023.3760041, title = {Machine Learning-Driven Social Network Analysis of Logistic Standardization Development}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {100--107}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760041}, author = {Wang, Qi and Jin, Zongzhen}, abstract = {Logistics has been developing for nearly a hundred years. Over time, the industry is booming and covering more aspects. In the process of development, many countries also began to summarize the experience of logistics, formulate standards, and restrain and regulate the behavior of enterprises. The formulation of international standards can play a coordinating role, collect information, make logistics develop in a better direction, and play a more important role in the world. This paper applied the social network analysis method and finds that the standards of each country are difficult to integrate as international standards, and the construction of the standard system has been obstructed. This paper will give three possible solutions to overcome those defects. Additionally, this study integrates machine learning algorithms such as Random Forest and Support Vector Machines (SVM) to analyze the correlation between national logistics standards and their adoption in international frameworks. These algorithms enhance the predictive accuracy of standardization barriers and provide data-driven insights for future policy formulation.} }
@inproceedings{10.1145/3736731.3746153, title = {Learning from Irreproducibility: Introducing Data Leakage Case Studies for Machine Learning Education}, booktitle = {Proceedings of the 3rd ACM Conference on Reproducibility and Replicability}, pages = {224--228}, year = {2025}, isbn = {9798400719585}, doi = {10.1145/3736731.3746153}, url = {https://doi.org/10.1145/3736731.3746153}, author = {Fund, Fraida and Saeed, Mohamed and Malik, Shaivi and Ishak, Kyrillos}, keywords = {reproducibility, education, machine learning, data leakage}, abstract = {Data leakage remains a pervasive issue in machine learning (ML), especially when applied to science, leading to overly optimistic performance estimates and irreproducible findings. Despite its prevalence, data leakage receives limited attention in ML education, in part due to the lack of accessible, hands-on teaching resources. To address this gap, we developed interactive learning modules in which students reproduce examples from academic publications that are affected by data leakage, then repeat the evaluation without the data leakage error to see how the finding is affected. These modules were deployed by the authors in two introductory machine learning courses, enabling students to explore common forms of leakage and their impact on model reliability. Following their engagement with these materials, student feedback highlighted increased awareness of subtle pitfalls that can compromise machine learning workflows.} }
@inproceedings{10.1145/3678698.3678702, title = {Financial Big data Visualization: A Machine Learning Perspective}, booktitle = {Proceedings of the 17th International Symposium on Visual Information Communication and Interaction}, year = {2024}, isbn = {9798400709678}, doi = {10.1145/3678698.3678702}, url = {https://doi.org/10.1145/3678698.3678702}, author = {Dong, Xiaodan and Huang, Weidong and Wang, Jitong}, keywords = {Machine Learning, Visualization, Telematics, Big Data}, abstract = {In today’s technology-driven environment, the exponential growth of big data underscores the importance of visualizing and analyzing it to derive actionable insights. This need spans across industrial sectors, with particular importance in the financial industry. While numerous modern models and algorithms have been developed, and utilized in diverse applications, it is crucial to classify these methodologies for users to identify the most suitable ones. In this paper, we embark on a selective review to streamline the classification of financial big data visualization methodologies from a machine learning perspective and explore the latest trends. We categorize techniques based on two key elements: the modeling stage and the nature of big data. The analytical stage divides methods into three phases: pre-model building, during-model building, and post-model building. Additionally, the characteristics of big data play an important role in shaping methodologies. We delve into three primary types of big data—structured, semi-structured, and unstructured and identify the most popular financial data types within each category in current society. We also discuss and highlight some research opportunities that we hope could be useful for visual analytics researchers.} }
@inproceedings{10.1145/3728424.3760766, title = {CAVIR Cognitive Assessment in VR: An Eye-tracking and Machine Learning Approach}, booktitle = {Proceedings of the 2nd International Workshop on Multimedia Computing for Health and Medicine}, pages = {13--19}, year = {2025}, isbn = {9798400718366}, doi = {10.1145/3728424.3760766}, url = {https://doi.org/10.1145/3728424.3760766}, author = {Vulpe-Grigorasi, Adrian and Slijepcevi\'c, Djordje and Sch\"offer, Lucas and Wanner, Sophia and Jespersen, Andreas E. and Miskowiak, Kamilla W. and Leung, Vanessa}, keywords = {eye tracking, machine learning, digital biomarkers, cognitive assessment, virtual reality, cavir, location = Ireland}, abstract = {Assessment of cognitive function in virtual reality (VR) is an emerging area of research that offers immersive and ecologically valid environments to evaluate cognitive skills in more realistic and engaging contexts. In this study, we extend this paradigm by integrating eye-tracking data to analyze gaze patterns during five distinct cognitive tasks within the Cognitive Assessment in Virtual Reality (CAVIR) environment. By extracting and modeling features from raw eye-tracking signals, we demonstrate that these tasks can be effectively distinguished based on gaze behavior. Our machine learning experiments show that a classifier trained to differentiate the five tasks achieved an accuracy of 70\%, while a model trained to classify the broader cognitive categories of verbal memory, processing speed, and attention achieved an accuracy of 89\%. These results indicate that eye tracking provides valuable behavioral digital biomarkers for cognitive function differentiation in VR-based assessments. This approach highlights the potential for developing non-invasive, gaze-driven cognitive profiling tools in virtual environments, paving the way for more personalized and adaptive neurocognitive diagnostics.} }
@article{10.1145/3757484, title = {Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3757484}, url = {https://doi.org/10.1145/3757484}, author = {Ng, Lynnette Hui Xian and Jaidka, Kokil and Tay, Kai Yuan and Chhaya, Niyati}, keywords = {crowdsourcing, diplomacy, discourse, machine learning, natural language processing, negotiation}, abstract = {Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14\% on held-out data and 12\% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction.} }
@inbook{10.1145/3729706.3729724, title = {Improved Data Flow Matching in SDN Using Machine Learning}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {120--124}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729724}, author = {Wang, Shuo and Liu, Jiaxin and Yang, Man}, abstract = {Data flow subsequence matching is crucial in Software-Defined Networking (SDN) due to the growing demand for efficient data handling. Traditional methods face limitations in accurately and rapidly matching data subsequences, especially under the complexities introduced by non-linear patterns. This paper proposes a machine learning-based algorithm designed for high-accuracy data flow subsequence matching in SDN. The algorithm utilizes feature extraction and transformation for each subsequence and applies similarity measurement techniques to improve matching precision. Furthermore, it enhances a neural network structure by converting a simple neural network into a modular network, improving convergence and performance. Experimental analysis demonstrates that the proposed algorithm achieves shorter feature transformation times, high accuracy in similarity measurement, and improved matching rates across diverse datasets, surpassing traditional methods. This solution provides a reliable approach to optimizing data flow management in SDN.} }
@inproceedings{10.1145/3658644.3690194, title = {Evaluations of Machine Learning Privacy Defenses are Misleading}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {1271--1284}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690194}, url = {https://doi.org/10.1145/3658644.3690194}, author = {Aerni, Michael and Zhang, Jie and Tram\`er, Florian}, keywords = {DP-SGD, audit, machine learning, membership inference, privacy, location = Salt Lake City, UT, USA}, abstract = {Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).} }
@inproceedings{10.1145/3745812.3745842, title = {Predictive Analytics in Diabetes Care: A Machine Learning Approach}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745842}, url = {https://doi.org/10.1145/3745812.3745842}, author = {Bisht, Smita and Saxena, Vivek and Goyal, Dinesh and Sharma, Himanshi}, keywords = {Decision Tree, Diabetes Prediction, Healthcare Analytics, Logistic Regression, Machine Learning, Python, Random Forest, Support Vector Machine}, abstract = {Diabetes is not only a hassle; it is one of the fast-growing ailments worldwide. Early detection and treatment are therefore, highly important in order to save lives and avoid complications later on. In the course of this study, we mentioned the potentiality of machine learning (ML) toward the prediction of diabetes. Four different ML techniques, namely Logistic Regression, Decision Trees, Random Forest, and Support Vector Machine (SVM) were analysed while coding in Python. The idea was to find out, with the help of the Pima Indians Diabetes data collection, if diabetes was predictable.Predictive modelling appears to be very powerful in medicine, as this study demonstrates. Future improvement in predicting diabetes may come from reasonably large datasets and perhaps from newer machine learning algorithms. Starting with a cleaning of the data, we made comparisons of all models based on accuracy, precision, recall, and F1-score. Random Forest was the winner! Highest prediction accuracy, but really all models did well, especially this one.} }
@inproceedings{10.1145/3746709.3746911, title = {Machine learning-based electronic confrontation system performance evaluation and optimization}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {1184--1188}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746911}, url = {https://doi.org/10.1145/3746709.3746911}, author = {Yuan, Ye and Zhou, Xin and Chen, Qingte}, keywords = {Electronic confrontation, Machine learning, Parameter optimization, Performance evaluation}, abstract = {Electronic confrontation is a key technology in modern warfare, and its complexity and uncertainty brought challenges to system performance evaluation and optimization. This article studies the performance evaluation and optimization method of electronic confrontation system based on machine learning. In terms of performance evaluation, a data-driven assessment framework is proposed. The construction of electronic confrontation efficiency is realized through feature engineering and machine learning model construction. Experimental results show, This method can accurately evaluate the combat effectiveness of electronic confrontation equipment and strategies in complex electromagnetic environments, where the performance of neural network models is the best. In terms of performance optimization, the electronic confrontation system parameter optimization method based on Bayesian optimization and electronic confrontation strategy optimization model based on deep reinforcement learning. The study of this article enriches the theoretical and methods of machine learning in the field of electronic confrontation, and provides new ideas for the intelligent development of the electronic confrontation system.} }
@inproceedings{10.1145/3659677.3659701, title = {Crop Recommendation using Machine Learning Algorithms}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659701}, url = {https://doi.org/10.1145/3659677.3659701}, author = {El Barrak, Khaoula and Lakhal, Said and Abdoun, Othman}, keywords = {Agriculture, Crop Recommendation System, Machine Learning, location = Meknes, AA, Morocco}, abstract = {Agricultural productivity is an essential factor in ensuring food security and meeting the growing demands of the world’s population. However, farmers face many challenges such as limited access to information, which can significantly impact their crop yield. A common problem among farmers is not choosing the right crop based on their soil’s needs. This article investigates the efficacy of various machine learning (ML) algorithms in developing a crop recommendation system. The study focuses on algorithms including Decision Trees, Random Forest, Support Vector Machine, K-Nearest Neighbors, Naive Bayes, LightGBM, and Logistic Regression, analyzing their performance metrics such as Accuracy, Precision, Recall, and F1 Score. The research utilizes a dataset encompassing soil and environmental parameters crucial for crop selection. Through rigorous evaluation, Random Forest, Naive Bayes, and LightGBM emerge as the top-performing algorithms, exhibiting high Accuracy and F1 Score for plant recommendation. These findings provide valuable insights for developing robust crop recommendation systems, aiding farmers in optimizing yields and mitigating risks in agricultural practices.} }
@article{10.1145/3742471, title = {Myoelectric Prosthetic Hands: A Review of Muscle Synergy, Machine Learning and Edge Computing}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3742471}, url = {https://doi.org/10.1145/3742471}, author = {Farag, Hamdy O. and Gaber, Mohamed Medhat and Awad, Mohammed Ibrahim and Elhady, Nancy E.}, keywords = {Myoelectric control, upper limb prostheses, prosthetic assessment, muscle synergies, edge deployment, machine learning, sensor fusion, neural network optimization}, abstract = {Over the past decade, the integration of electromyography (EMG) techniques with machine learning has significantly advanced prosthetic device control. Researchers have developed sophisticated deep learning classifiers for gesture recognition and created EMG controllers capable of simultaneous proportional control across multiple degrees of freedom. However, the increasing complexity of these machine learning models demands greater computational power, creating challenges for real-time deployment on embedded prosthetic controllers. Various optimization techniques - including hyperdimensional computing, pruning, and quantization - have demonstrated effectiveness in reducing computational requirements while preserving system performance. Concurrently, biomedical research has explored muscle and task synergies as methods to simplify inputs for machine learning models. This review examines synergy extraction in upper limb prosthetics research and identifies the need for standardized hardware specifications to facilitate proper validation and comparison of research outcomes. Furthermore, it explores how optimization techniques from Internet of Things (IoT) applications could enhance EMG controllers in biomedical settings. The analysis identifies sensor fusion and high-density EMG as particularly promising approaches for achieving robust, generalized control of upper limb prosthetics.} }
@inproceedings{10.1145/3711542.3711573, title = {Constructing Depression Prediction Model using Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {413--418}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711573}, url = {https://doi.org/10.1145/3711542.3711573}, author = {Nuipian, Vatinee and Hanumas, Sorawit and Plangklang, Kannika}, keywords = {Decision Trees, Depression prediction, Machine learning, Natural language, Recommendation system}, abstract = {Depression is a prevalent mental health disorder with significant impacts on individuals and society. Given the increasing presence of depressive expressions on social media, this study constructs a predictive model to classify depression-related content using machine learning algorithms. Six algorithms for comparing a model, such as Decision Trees, Random Forest, K-nearest neighbors, Support Vector Machines (SVM), Logistic Regression, and Convolutional Neural Networks (CNNs), are used to identify the most effective model for accurately predicting depressive posts. The study utilized a dataset from Twitter, balancing depressive and non-depressive texts for unbiased training. Text preprocessing involved tokenization, stop word removal, and feature extraction using TF-IDF. Evaluation metrics such as accuracy, precision, recall, and F1-score were applied to assess performance. The results showed that Decision Trees and Logistic Regression performed best, with accuracy exceeding 99\%. These models were implemented in a prototype for real-time depression risk prediction. Future work aims to expand the model's applicability to Thai language texts, enhancing support for mental health care} }
@inproceedings{10.1145/3728725.3728770, title = {Optimization of Price Strategy by Machine Learning in E-commerce}, booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security}, pages = {284--288}, year = {2025}, isbn = {9798400713453}, doi = {10.1145/3728725.3728770}, url = {https://doi.org/10.1145/3728725.3728770}, author = {Liu, Quan and Song, Yunkui}, keywords = {E-commerce, Machine learning, Pinduoduo, Price strategy, Taobao}, abstract = {Machine learning algorithms can monitor market dynamics, user behavior and competitor prices in real time, and dynamically adjust commodity prices based on this information to maximize profits. By analyzing historical sales data, market price data, user behavior data, etc., machine learning models can predict the demand and price sensitivity of goods to develop optimal pricing strategies. This study takes Pinduoduo and Taobao as examples to analyze the application of machine learning in E-commerce price strategy optimization, in order to provide useful reference for the E-commerce platforms.} }
@article{10.1145/3732786, title = {Graph Machine Learning in the Era of Large Language Models (LLMs)}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {16}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3732786}, url = {https://doi.org/10.1145/3732786}, author = {Wang, Shijie and Huang, Jiani and Chen, Zhikai and Song, Yu and Tang, Wenzhuo and Mao, Haitao and Fan, Wenqi and Liu, Hui and Liu, Xiaorui and Yin, Dawei and Li, Qing}, keywords = {Graph Machine Learning, Large Language Models (LLMs), Pre-training and Fine-tuning, Prompting, and Representation Learning}, abstract = {Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graphs. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications, such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML’s generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations, such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph Heterophily and Out-of-Distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.} }
@inproceedings{10.1145/3711896.3737870, title = {Machine Learning on Graphs in the Era of Generative Artificial Intelligence}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6296--6297}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737870}, url = {https://doi.org/10.1145/3711896.3737870}, author = {Wang, Yu and Zhang, Yu and Guo, Zhichun and Shomer, Harry and Han, Haoyu and Derr, Tyler and Ahmed, Nesreen K. and Halappanavar, Mahantesh and Tang, Jiliang}, keywords = {generative artificial intelligence, graph machine learning, location = Toronto ON, Canada}, abstract = {Graphs, which encode pairwise relations between entities, serve as a fundamental data structure across real-world domains. Many critical applications can be formulated as graph-based tasks, and graph machine learning (GML), from the shallow embedding models to graph neural networks and further advanced to the most powerful graph transformers, has been well-established to automate knowledge discovery and decision-making on graphs. In parallel, the recent emergence of large foundational models has driven machine learning into a new era of Generative Artificial Intelligence (Gen-AI), and this revolution presents both unprecedented opportunities and profound challenges for the well-established GML paradigms. However, few investigations have analyzed and envisioned how GML should evolve to harness these opportunities, address these challenges, and embrace this new Gen-AI era. To fill in this gap, we organize the first international Workshop on Machine Learning on Graphs in the Era of Generative Artificial Intelligence (MLoG-GenAI), held in connection with the 31st ACM Conference on Knowledge Discovery and Data Mining, which provides a venue to gather academic researchers and industry practitioners to discuss and picture the development of GML in the new Gen-AI era.} }
@inproceedings{10.1145/3630106.3658914, title = {Insights From Insurance for Fair Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {407--421}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658914}, url = {https://doi.org/10.1145/3630106.3658914}, author = {Fr\"ohlich, Christian and Williamson, Robert C.}, keywords = {aggregates, fair machine learning, insurance, responsibility, statistics, location = Rio de Janeiro, Brazil}, abstract = {We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.} }
@inproceedings{10.1145/3706594.3726983, title = {Stop Wasting your Cache! Bringing Machine Learning into Cache Computing}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {86--89}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3726983}, url = {https://doi.org/10.1145/3706594.3726983}, author = {Petrolo, Vincenzo and Guella, Flavia and Caon, Michele and Masera, Guido and Martina, Maurizio}, keywords = {In-Cache Computing, Near-memory Computing, Deep Neural Networks}, abstract = {The rapid evolution of Machine Learning (ML) workloads, particularly Deep Neural Networks (DNNs) and Transformer-based models, has intensified demands on computing architectures, highlighting the limitations of traditional von Neumann systems due to the memory bottleneck. To address these challenges, this paper investigates the mapping of fundamental Machine Learning (ML) operations onto ARCANE, a Near-Memory Computing (NMC)-based architecture that integrates Vector Processing Units (VPUs) directly within the data cache. ARCANE offers a flexible ISA-extension (xmnmc) abstracting memory management, effectively reducing data movement and enhancing performance. We specifically explore the acceleration capabilities of ARCANE when executing fundamental Deep Neural Network (DNN) and Transformer-based operations. Experimental results show that, with a contained area overhead, ARCANE achieves consistent speedups, delivering up to 150 improvement in 2D convolution, 305 in Linear layer, and over 32 in Fused-Weight Self-Attention (FWSA), compared to conventional CPU approaches. These findings underline ARCANE’s significant benefits in supporting efficient deployment of edge-oriented Machine Learning (ML) workloads.} }
@article{10.1145/3730576, title = {A Comprehensive Survey on Machine Learning Driven Material Defect Detection}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3730576}, url = {https://doi.org/10.1145/3730576}, author = {Bai, Jun and Wu, Di and Shelley, Tristan and Schubel, Peter and Twine, David and Russell, John and Zeng, Xuesen and Zhang, Ji}, keywords = {Material defect detection, composites manufacturing, machine learning, deep learning, computer vision, machine vision}, abstract = {Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.} }
@inproceedings{10.1145/3732365.3732410, title = {A Survey of Machine Learning Approaches for Malware Detec-tion}, booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering}, pages = {269--273}, year = {2025}, isbn = {9798400713613}, doi = {10.1145/3732365.3732410}, url = {https://doi.org/10.1145/3732365.3732410}, author = {Wu, Yibiao and Zhuang, Honglin and Jia, Yetao and Zhang, Yuting}, keywords = {Deep Learning, Detection technology, LLM, Machine Learning, Malware}, abstract = {At present, due to the rapid evolution of malicious code and the rapid progress of science and technology, the security of information system has been put forward higher requirements. This research systematically reviews the current detection technologies for malicious code, from classical machine learning algorithms to some existing mature algorithms, and introduces machine learning, deep learning, and other technologies on this basis. In addition, malicious code detection technology based on large-scale language modeling (LLMS) is also studied. This study focuses on the advantages, limitations, and problems in practical applications of various detection techniques, and compares traditional methods, machine learning, and deep learning methods, and looks forward to future work.} }
@article{10.1145/3729376, title = {Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729376}, url = {https://doi.org/10.1145/3729376}, author = {Nguyen Tung, Lam and Cho, Steven and Du, Xiaoning and Neelofar, Neelofar and Terragni, Valerio and Ruberto, Stefano and Aleti, Aldeida}, keywords = {Explainability, Oracle Problem, SE4AI, Trustworthy Text Classifier}, abstract = {Machine learning (ML) for text classification has been widely used in various domains, such as toxicity detection, chatbot consulting, and review analysis. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Several studies indicate that traditional uncertainty metrics, such as model confidence, and performance metrics, like accuracy, are insufficient to build human trust in ML models. These models often learn spurious correlations during training and predict based on them during inference. When deployed in the real world, where such correlations are absent, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are made reasonably based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. So far, due to the lack of automated trustworthiness oracles, the assessment requires manual validation, based on the decision process disclosed by explanation methods. However, this approach is time-consuming, error-prone, and not scalable. To address this problem, we propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanation methods to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. As a demonstration of its practical usefulness, we also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. We compare TOKI with a naive baseline based solely on model confidence. To evaluate their alignment with human judgement, experiments are conducted on human-created ground truths of approximately 8,000 predictions. Additionally, we compare the effectiveness of TOKI-guided adversarial attack method with A2T, a state-of-the-art adversarial attack method for text classification. Results show that (1) relying on prediction uncertainty metrics, such as model confidence, cannot effectively distinguish between trustworthy and untrustworthy predictions, (2) TOKI achieves 142\% higher accuracy than the naive baseline, and (3) TOKI-guided adversarial attack method is more effective with fewer perturbations than A2T.} }
@inproceedings{10.1145/3736251.3747313, title = {Building Trust in AI-Powered Assessment Through Explainable Machine Learning Models}, booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 2}, pages = {403--404}, year = {2025}, isbn = {9798400719424}, doi = {10.1145/3736251.3747313}, url = {https://doi.org/10.1145/3736251.3747313}, author = {Ayanwale, Musa Adekunle}, keywords = {ai in higher education, educational assessment, explainable artificial intelligence, learning management system, student engagement, location = Gaborone, Botswana}, abstract = {The increasing integration of artificial intelligence (AI)-based assessment systems in higher education offers efficiency and scalability in grading and feedback but raises concerns about transparency, fairness, and student trust. This study investigates the role of Explainable AI (XAI) in improving trust and engagement within AI-driven assessment environments, focusing on the Thuto Learning Management System (LMS) at the National University of Lesotho (NUL). A cross-sectional survey of 850 undergraduate students, who had engaged with Thuto LMS for at least two semesters, was conducted using a 55-item questionnaire measuring 11 constructs, including digital self-efficacy, AI literacy, ethics awareness, and feedback perceptions. Assessment outcomes were retrieved from LMS records and binarised into pass/fail classifications. Three supervised machine learning models-Logistic Regression, Random Forest, and K-Nearest Neighbours-were developed to predict assessment outcomes, and post hoc interpretability was achieved using SHAP and DALEX frameworks. Logistic Regression demonstrated the highest predictive accuracy (73.7\%), while feature importance analyses revealed that feedback usefulness, discussion engagement, and digital self-efficacy were the strongest predictors of academic success. Findings underscore the potential of XAI for promoting fairness, transparency, and learner trust in AI-powered educational systems, particularly in underexplored African higher education contexts.} }
@inproceedings{10.1145/3671127.3699676, title = {A Machine Learning Approach to Benchmark Thermal Comfort}, booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {363--368}, year = {2024}, isbn = {9798400707063}, doi = {10.1145/3671127.3699676}, url = {https://doi.org/10.1145/3671127.3699676}, author = {Jacob, Jeslu Celine and Pandit, Debapratim and Sen, Joy}, keywords = {Machine learning, Personal comfort, Skin temperature, Thermal comfort, location = Hangzhou, China}, abstract = {Thermal comfort models are used as benchmarks in air conditioning design to decide on temperature setpoints. Most existing thermal comfort models are designed for representative occupants considering static environmental conditions. However, inter-personal and intra-personal factors influence thermal preference of occupants. Hence assuming standard thermal comfort conditions lead to uncomfortable thermal environments. With advancement in sensing technology and computation resources there exists an opportunity to model personal thermal preferences of occupants using physiological data in addition to environmental data. This study evaluates the use of machine learning models to predict personal thermal comfort with high accuracy. The results show that classification models such as decision trees, random forest, k-nearest neighbor, neural networks and adaboost can be used to model personal thermal comfort with accuracies higher than 70\%. Random forest models perform the best with accuracies higher than 75\%.} }
@inproceedings{10.1145/3679240.3734628, title = {Machine Learning for Building-Level Heat Risk Mapping}, booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems}, pages = {341--346}, year = {2025}, isbn = {9798400711251}, doi = {10.1145/3679240.3734628}, url = {https://doi.org/10.1145/3679240.3734628}, author = {Domiter, Andrea and Keshav, Srinivasan}, keywords = {Heatwaves, Thermal Vulnerability, Urban Occupants, ML}, abstract = {Climate change is intensifying the frequency and severity of heat waves, increasing risks to public health and energy systems worldwide. Although building simulations can reveal the types of buildings whose occupants are most at risk, they rarely pinpoint the locations of these structures. We present a data-driven workflow that labels real-world buildings from satellite imagery and building characteristics, estimates thermal performance under heatwaves, and visualizes risks on a detailed map. Through a case study of 10 UK cities, we show that our approach can be used to identify areas of concentrated heat risk, providing valuable insights for guiding public health strategies and informing future research into grid reliability and thermal equity.} }
@inproceedings{10.1145/3587135.3592769, title = {Machine Learning Application Benchmark}, booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers}, pages = {229--235}, year = {2023}, isbn = {9798400701405}, doi = {10.1145/3587135.3592769}, url = {https://doi.org/10.1145/3587135.3592769}, author = {Koch, Andreas and Petry, Michael and Ghiglione, Max and Raoofy, Amir and Dax, Gabriel and Furano, Gianluca and Werner, Martin and Trinitis, Carsten and Langer, Martin}, keywords = {FPGA, Machine learning, benchmark, datasets, neural networks, power consumption, location = Bologna, Italy}, abstract = {This paper presents the MLAB project, a research and development activity funded by ESA General Support Technology Programme under the lead of Airbus Defence and Space GmbH, with the goal of developing a machine learning application benchmark for space applications. First, the need for a benchmark dedicated to machine learning applications in spacecraft is explained, and examples of applications are described including their design challenges. Then the benchmark design is presented, including the rules of the metrics, guidelines and scenarios for references. These scenarios include a description of the reference workloads that have been selected during the activity as representative for spacecraft applications. Lastly, the submission concept is introduced.} }
@inbook{10.1145/3658617.3703144, title = {Leveraging Machine Learning Techniques for Traditional EDA Workflow Enhancement}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {676--682}, year = {2025}, isbn = {9798400706356}, url = {https://doi.org/10.1145/3658617.3703144}, author = {Cho, Jinoh and Im, Jaekyung and Lee, Jaeseung and Min, Kyungjun and Park, Seonghyeon and Seo, Jaemin and Yoon, Jongho and Kang, Seokhyeong}, abstract = {As technology nodes advance and feature sizes shrink, the increasing complexity of design rules and routing congestion has resulted in greater design challenges and rising costs. Machine learning (ML) models offer significant potential to enhance design quality by enabling early prediction and optimization during the design flow. However, only a few works have validated the effectiveness of ML model when integrated to the traditional design flow. This paper will cover the effectiveness of ML-enhanced design workflow with some practical applications. Additionally, we will address which problems should be solved to achieve successful ML integration.} }
@inproceedings{10.1145/3665939.3665960, title = {Transparent Data Preprocessing for Machine Learning}, booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics}, pages = {1--6}, year = {2024}, isbn = {9798400706936}, doi = {10.1145/3665939.3665960}, url = {https://doi.org/10.1145/3665939.3665960}, author = {Strasser, Sebastian and Klettke, Meike}, keywords = {data preprocessing, data profiles, change profiles, transparency, location = Santiago, AA, Chile}, abstract = {Data preprocessing is an important task in machine learning which can significantly improve model outcomes. However, evaluating the impact of data preprocessing is often difficult. There is a need for tools which make it transparent to the user on how certain transformations conducted in preprocessing affect the data. Thus, we propose a vision of a transparency system for data preprocessing that provides insights into data preparation pipelines. Our envisioned system consists of a Python library which enables users to log transformations and processed data. Subsequently, the system generates summaries of the data which was processed in the pipeline and so-called change profiles which capture the changes conducted in each processing step. These abstractions offer insight into the transformations and their effects on data. Additionally, the system includes an user interface where users can interactively discover the implemented pipeline and the changes made during preprocessing. This paper presents an initial concept of such a system. It also examines further challenges related to making preprocessing transparent and discusses potential solutions to address these challenges.} }
@inproceedings{10.1145/3696630.3731614, title = {Causal Models in Requirement Specifications for Machine Learning: A vision}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {1402--1405}, year = {2025}, isbn = {9798400712760}, doi = {10.1145/3696630.3731614}, url = {https://doi.org/10.1145/3696630.3731614}, author = {Heyn, Hans-Martin and Mao, Yufei and Wei, Roland and Knauss, Eric}, keywords = {AI engineering, causal modelling, data requirements, requirements engineering, location = Clarion Hotel Trondheim, Trondheim, Norway}, abstract = {Specifying data requirements for machine learning (ML) software systems remains a challenge in requirements engineering (RE). This vision paper explores causal modelling as an RE activity that allows the systematic integration of prior domain knowledge into the design of ML software systems. We propose a workflow to elicit low-level model and data requirements from high-level prior knowledge using causal models. The approach is demonstrated on an industrial fault detection system. This paper outlines future research needed to establish causal modelling as an RE practice.} }
@article{10.1145/3656021.3656029, title = {Fair Machine Learning Post Affirmative Action}, journal = {SIGCAS Comput. Soc.}, volume = {52}, pages = {22}, year = {2024}, issn = {0095-2737}, doi = {10.1145/3656021.3656029}, url = {https://doi.org/10.1145/3656021.3656029}, author = {Perello, Nick and Grabowicz, Przemyslaw}, abstract = {The U.S. Supreme Court, in a 6-3 decision on June 29, effectively ended the use of race in college admissions [1]. Indeed, national polls found that a plurality of Americans - 42\%, according to a poll conducted by the University of Massachusetts [2] - agree that the policy should be discontinued, while 33\% support its continued use in admissions decisions. As scholars of fair machine learning, we ponder how the Supreme Court decision shifts points of focus in the field. The most popular fair machine learning methods aim to achieve some form of "impact parity" by diminishing or removing the correlation between decisions and protected attributes, such as race or gender, similarly to the 80\% rule of thumb of the Equal Employment Opportunity Commision. Impact parity can be achieved by reversing historical discrimination, which corresponds to affirmative actions, or by diminishing or removing the influence of the attributes correlated with the protected attributes, which is impractical as it severely undermines model accuracy. Besides, impact disparity is not necessarily a bad thing, e.g., African-American patients suffer from a higher rate of chronic illnesses than White patients and, hence, it may be justified to admit them to care programs at a proportionally higher rate [3]. The U.S. burden-shifting framework under Title VII offers solutions alternative to impact parity. To determine employment discrimination, U.S. courts rely on the McDonnell-Douglas burden-shifting framework where the explanations, justifications, and comparisons of employment practices play a central role. Can similar methods be applied in machine learning?} }
@inbook{10.1145/3715014.3724051, title = {Machine Learning and Big Data on Raspberry Pi: A Performance Evaluation}, booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems}, pages = {648--649}, year = {2025}, isbn = {9798400714795}, url = {https://doi.org/10.1145/3715014.3724051}, author = {Chen, Tan and Liu, Dawei}, abstract = {In this paper we present a performance evaluation of machine learning and big data technologies in edge computing. Existing research focuses on deep learning methods on high-end edge computing devices. There have been few reports on classical machine learning methods on common edge computing devices. To close this gap, we evaluate support vector machine (SVM) and random forest (RF) on a Raspberry Pi platform. Our evaluation includes method execution time and data storage time. The latter is considered an important component in machine learning execution lifecycle and has not been jointly evaluated in existing research.Some interesting observations include the significant impact of the data storage engine on efficiency, and how RF can extend the performance bottleneck of edge devices. These findings show that, in today's common edge computing devices, reasonable algorithm selection and data optimization strategies are essential to fully exploit the potential of the device.} }
@inbook{10.1145/3756423.3756471, title = {Research on Optimization of Food Process Parameters Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing}, pages = {297--301}, year = {2025}, isbn = {9798400714351}, url = {https://doi.org/10.1145/3756423.3756471}, author = {Chen, Xiangjun and Shi, Qinhong}, abstract = {The article is limited to machine learning and discusses parameter optimisation in the bread production process. This paper introduces the basic technical parameters used in bread manufacturing: the moisture ratio, yeast fermentation time, kneading rate and baking temperature, as well as baking time, and how these parameters affect bread quality. The research collected plenty of process parameters and quality index through experiments, and established a prediction model of regression analysis, support vector machine and random forest based on machine learning theory. Meanwhile, the parameters were modified with genetic optimization and particle swarm optimization. It is possible to accurately analyze the relationship between the different process parameters and the quality of bread, optimize the production process, and upgrade the product quality, which has applicational value to the food production.} }
@inproceedings{10.1145/3708657.3708769, title = {Machine Learning vs. Transformer: Indonesia News Classification}, booktitle = {Proceedings of the 2024 10th International Conference on Communication and Information Processing}, pages = {699--704}, year = {2025}, isbn = {9798400717444}, doi = {10.1145/3708657.3708769}, url = {https://doi.org/10.1145/3708657.3708769}, author = {Harditya, Michael and Purnamasari, Prima Dewi}, keywords = {text classification, evaluation, machine learning, BART, BERT, na\"ve bayes classification}, abstract = {The shift from traditional newspapers to digital media in Indonesia has led to challenges in news publication, including the spread of less credible or fake news. This paper addresses the problem by evaluating text classification methods to improve news tagging and eventually improve the publishing process. Specifically, it compares the performance of Na\"ve Bayes and Transformer models (BERT and BART) for multiclass classification of Indonesian news headlines. Using a dataset of 150,466 news articles, the study preprocesses the data, tokenizes it, and trains the classifiers. Results show that Na\"ve Bayes with unigram preprocessing achieves comparable accuracy to BERT and BART, highlighting the potential of simpler models in certain contexts. The findings suggest that both traditional and advanced models can be effective, depending on the dataset and preprocessing methods used.} }
@article{10.1145/3716376, title = {Cost-effective Missing Value Imputation for Data-effective Machine Learning}, journal = {ACM Trans. Database Syst.}, volume = {50}, year = {2025}, issn = {0362-5915}, doi = {10.1145/3716376}, url = {https://doi.org/10.1145/3716376}, author = {Chai, Chengliang and Jin, Kaisen and Tang, Nan and Fan, Ju and Miao, Dongjing and Wang, Jiayi and Luo, Yuyu and Li, Guoliang and Yuan, Ye and Wang, Guoren}, keywords = {Data-centric AI, machine learning, data cleaning, coreset selection}, abstract = {Given a dataset with incomplete data (e.g., missing values), training a machine learning model over the incomplete data requires two steps. First, it requires a data-effective step that cleans the data in order to improve the data quality (and the model quality on the cleaned data). Second, it requires a data-efficient step that selects a core subset of the data (called coreset) such that the trained models on the entire data and the coreset have similar model quality, in order to save the computational cost of training. The first-data-effective-then-data-efficient methods are too costly, because they are expensive to clean the whole data; while the first-data-efficient-then-data-effective methods have low model quality, because they cannot select high-quality coreset for incomplete data.In this article, we investigate the problem of coreset selection over incomplete data for data-effective and data-efficient machine learning. The essential challenge is how to model the incomplete data for selecting high-quality coreset. To this end, we propose the GoodCore framework towards selecting a good coreset over incomplete data with low cost. To model the unknown complete data, we utilize the combinations of possible repairs as possible worlds of the incomplete data. Based on possible worlds, GoodCore selects an expected optimal coreset through gradient approximation without training ML models. We formally define the expected optimal coreset selection problem, prove its NP-hardness, and propose a greedy algorithm with an approximation ratio. To make GoodCore more efficient, we propose optimization methods that incorporate human-in-the-loop imputation or automatic imputation method into our framework. Moreover, a group-based strategy is utilized to further accelerate the coreset selection with incomplete data given large datasets. Experimental results show the effectiveness and efficiency of our framework with low cost.} }
@inproceedings{10.1145/3725843.3756092, title = {SkipReduce: (Interconnection) Network Sparsity to Accelerate Distributed Machine Learning}, booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture}, pages = {643--658}, year = {2025}, isbn = {9798400715730}, doi = {10.1145/3725843.3756092}, url = {https://doi.org/10.1145/3725843.3756092}, author = {Kasan, Hans and Abts, Dennis and Choi, Jungwook and Kim, John}, abstract = {The interconnection network is a critical component for building scalable systems, as its communication bandwidth directly impacts the collective communication performance of distributed training. In this work, we exploit interconnection network sparsity (or communication sparsity) to address challenges of communication performance and scalability. In particular, we identify how gradients (or packets) during communication can be randomly skipped with minimal impact on accuracy. However, skipping gradients in fine granularity (or individually) results in a loss of gradient information without improving communication performance, due to the synchronous nature of collective communication. Thus, we propose coarse-grained skipping where gradient slices are skipped, which enables skipping of some AllReduce steps to accelerate communication. In particular, we propose SkipReduce collective communication that intentionally skips random gradients during AllReduce. However, a naive implementation of SkipReduce can degrade accuracy by repeatedly skipping gradients from the same node, which introduces bias. To mitigate this accuracy loss, we show how randomizing the skipped gradient slices improves training accuracy with negligible additional runtime. We also observe that not all layers have similar communication sparsity and propose applying SkipReduce selectively where only the sparse layers (or gradients) are skipped to minimize the accuracy impact of SkipReduce. Compared to prior work on communication acceleration, SkipReduce can be seamlessly integrated into existing collective communication libraries with minimal overhead. We implement SkipReduce on top of NCCL’s ring-based AllReduce algorithm. Our results show that this method accelerates collective communication while preserving final training accuracy. Compared to baseline AllReduce, SkipReduce provides up to a 1.58 speedup in time-to-accuracy. Beyond this performance gain in data parallelism, this work also discusses the broader implications of SkipReduce, including its application to other parallelism strategies and logical topologies, as well as its benefits as a model regularizer.} }
@article{10.1145/3769292, title = {Machine Learning Systems: A Survey from a Data-Oriented Perspective}, journal = {ACM Comput. Surv.}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3769292}, url = {https://doi.org/10.1145/3769292}, author = {Cabrera, Christian and Paleyes, Andrei and Thodoroff, Pierre and Lawrence, Neil}, keywords = {Artificial Intelligence, Machine Learning, Real-World Deployment, Systems Architecture, Data-Oriented Architecture.}, abstract = {Engineers are deploying ML models as parts of real-world systems with the upsurge of AI technologies. Real-world environments challenge the deployment of such systems because these environments produce large amounts of heterogeneous data, and users require increasingly efficient responses. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-Oriented Architecture (DOA) is an emerging style that better equips systems to integrate ML models. Even though papers on deployed ML-based systems do not mention DOA, their authors make design decisions that implicitly follow DOA. Implicit decisions create a knowledge gap, limiting practitioners’ ability to implement ML-based systems. This paper surveys why, how, and to what extent practitioners have adopted DOA to implement ML-based systems. We overcome the knowledge gap by answering these questions and explicitly showing the design decisions and practices behind these systems. The survey follows a well-known systematic and semi-automated methodology for reviewing papers in software engineering. The majority of reviewed works partially adopt DOA. Such an adoption enables systems to address big data management, low-latency processing, resource management, security, and privacy requirements. Based on these findings, we formulate practical advice to facilitate the deployment of ML-based systems.} }
@inproceedings{10.1145/3670474.3685969, title = {Enabling Risk Management of Machine Learning Predictions for FPGA Routability}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685969}, url = {https://doi.org/10.1145/3670474.3685969}, author = {Gunter, Andrew David and Thomas, Maya and Ghanathe, Nikhil Pratap and Wilton, Steven}, keywords = {CAD, FPGA, ML, confidence, early exit, risk, routing, uncertainty, location = Salt Lake City, UT, USA}, abstract = {Machine Learning (ML) models sometimes make inaccurate predictions for the routability of field-programmable gate array (FPGA) circuit designs. This risks time wasted attempting to route an unroutable design or the premature termination of a routable design's compilation. While improving model accuracy is beneficial, we explore a complementary approach to mitigate the risk of inaccurate predictions by assessing the confidence of ML models. This approach could allow individuals to customize their own trade-off for the competing risks of wasted time and premature compilation termination. In this paper, we introduce a novel mixture of experts ML system for FPGA routability prediction and further quantify the confidence calibration of this system to determine its suitability as a risk management tool. We evaluate our prediction system for the purpose of enabling user risk management in FPGA routability prediction, comparing against a baseline inspired by prior work. Our evaluation finds our approach to achieve almost 2 the precision in risk trade-off between time wasted on unroutable designs and premature termination of routable designs.} }
@inproceedings{10.1145/3658644.3690373, title = {TabularMark: Watermarking Tabular Datasets for Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {3570--3584}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690373}, url = {https://doi.org/10.1145/3658644.3690373}, author = {Zheng, Yihao and Xia, Haocheng and Pang, Junyuan and Liu, Jinfei and Ren, Kui and Chu, Lingyang and Cao, Yang and Xiong, Li}, keywords = {data ownership, machine learning, tabular dataset, watermark, location = Salt Lake City, UT, USA}, abstract = {Watermarking is broadly utilized to protect ownership of shared data while preserving data utility. However, existing watermarking methods for tabular datasets fall short on the desired properties (detectability, non-intrusiveness, and robustness) and only preserve data utility from the perspective of data statistics, ignoring the performance of downstream ML models trained on the datasets. Can we watermark tabular datasets without significantly compromising their utility for training ML models while preventing attackers from training usable ML models on attacked datasets?In this paper, we propose a hypothesis testing-based watermarking scheme, TabularMark. Data noise partitioning is utilized for data perturbation during embedding, which is adaptable for numerical and categorical attributes while preserving the data utility. For detection, a custom-threshold one proportion z-test is employed, which can reliably determine the presence of the watermark. Experiments on real-world and synthetic datasets demonstrate the superiority of TabularMark in detectability, non-intrusiveness, and robustness.} }
@article{10.1145/3762199, title = {What is new, and what is old, in fairness and machine learning}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3762199}, url = {https://doi.org/10.1145/3762199}, author = {Hu, Lily}, keywords = {Algorithmic fairness, Algorithmic decision-making, Political Philosophy}, abstract = {This article explores the issue of normative distinctiveness in machine learning decision systems alongside Solon Barocas, Moritz Hardt, and Arvind Narayanan's landmark book Fairness and Machine Learning. What, if anything, is different this time, with the rise of machine learning-based aids to bureaucratic decision-making? I show how a focus on normative distinctiveness can obscure from view a much more significant upshot of machine learning: that the mere existence of feasible alternatives presses new justificatory demands not just on the design of new technical systems but on the prevailing human-centered decision regime. I argue that depoliticizing conventional bureaucratic structures of decision-making leads to a missed opportunity for a broader normative reevaluation of what we owe to each other in a world of expanded practical possibility.} }
@inproceedings{10.1145/3725843.3756126, title = {Misam: Machine Learning Assisted Dataflow Selection in Accelerators for Sparse Matrix Multiplication}, booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture}, pages = {824--838}, year = {2025}, isbn = {9798400715730}, doi = {10.1145/3725843.3756126}, url = {https://doi.org/10.1145/3725843.3756126}, author = {Yadav, Sanjali and Namjoo, Amirmahdi and Asgari, Bahar}, keywords = {Flexible SpGEMM accelerators, FPGA Reconfiguration, Decision Tree.}, abstract = {The performance of Sparse Matrix-Matrix Multiplication (SpGEMM), a foundational operation in scientific computing and machine learning, is highly sensitive to the diverse and dynamic sparsity patterns of its input matrices. While specialized hardware accelerators improve efficiency, their reliance on fixed dataflows, each optimized for a narrow sparsity regime, results in suboptimal performance on real-world workloads. Even recent flexible accelerators that support multiple dataflows face two critical limitations: (1) the lack of a fast and principled mechanism for runtime dataflow selection, and (2) the area overhead and hardware underutilization incurred to provide that flexibility. We present Misam, a machine learning framework that addresses these challenges to enable adaptive and hardware-efficient SpGEMM acceleration. Misam employs a lightweight decision tree to dynamically predict the optimal hardware configuration from matrix features. To overcome hardware underutilization, Misam leverages FPGA reconfigurability to deploy specialized, resource-efficient bitstreams on demand. This process is governed by an intelligent reconfiguration engine that evaluates whether the anticipated performance gain justifies the overhead of switching hardware configurations. Misam’s dynamic approach yields up to a 10.76 speedup by judiciously reconfiguring. Misam demonstrates that a synergistic combination of machine learning-based prediction and judicious hardware reconfiguration can achieve high performance across a wide spectrum of sparsity patterns, bridging the gap between specialized efficiency and general-purpose adaptability.} }
@inproceedings{10.1145/3555041.3589337, title = {Mixed Methods Machine Learning}, booktitle = {Companion of the 2023 International Conference on Management of Data}, pages = {3--4}, year = {2023}, isbn = {9781450395076}, doi = {10.1145/3555041.3589337}, url = {https://doi.org/10.1145/3555041.3589337}, author = {Murdock, Vanessa}, keywords = {UX research, e-commerce, mixed-methods research, shopping, location = Seattle, WA, USA}, abstract = {Machine learning is ubiquitous: many of our everyday interactions, both online and offline, are backed by machine learning. Typically, machine learned systems start as an idea from the business or engineering team for a service or an app that helps the customer achieve a goal. The app is built iteratively, starting with the minimum lovable version, and undergoes several rounds of improvements to become more sophisticated. Success is measured with an online A/B test on live traffic, on the assumption that if customers engage with the app, it is serving their needs.We propose a different approach to developing such systems, that employs mixed-methods research to understand what to build, and how to make it satisfying and helpful for the customer. The Mixed Methods Machine Learning (MXML) paradigm, starts with a user study, to understand how people behave in an everyday setting (such as shopping for groceries in a grocery store), and to identify points of friction that can be automated, or experiences that can be made more enjoyable. The study observations are mapped to interactions recorded in the system's behavioral log data, which is the basis for the machine learned system. Mapping the study observations to the log data is a key step in directing the machine learning to solve a customer problem. The MXML system is evaluated with a follow-on user study, in addition to the traditional online A/B test, to assess whether the system is satisfying, helpful and delightful. In this talk we present the MXML paradigm, with real-world examples.} }
@inproceedings{10.1145/3728199.3728237, title = {Machine Learning-Based Multi-User Semi-Blind Signal Detection in Non-Orthogonal Multiple Access}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {234--239}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728237}, url = {https://doi.org/10.1145/3728199.3728237}, author = {Li, Shuangyuan and Liu, Chang and Si, Pengbo and Li, Meng}, keywords = {Bit error rate, Channel estimation, Machine learning, Multi-user signal detection, Non-orthogonal multiple access}, abstract = {With the rapid development of fifth generation (5G) and sixth generation (6G) wireless communication technologies, non-orthogonal multiple access (NOMA) systems have become crucial to improve spectral efficiency and support more user connections. However, multi-user signal detection faces challenges due to complex channel environments and signal interference, and efficient and accurate detection methods need to be developed. In this paper, we propose a novel machine learning (ML) algorithm for semi-blind signal detection in NOMA uplink systems. Uniquely, the algorithm does not require separate channel estimation and can directly and accurately recover user signals received at the base station. Specifically, a convolutional neural network long term memory (CNN-LSTM) framework is used to analyse the synthetic signals received at the base station and estimate the channel state information. The estimation results are then fed into a deep neural network (DNN) based on joint maximum likelihood (JML) detection. By integrating with the synthetic signals, the transmitted signals of each user are detected separately. We evaluate the detection performance of both users under different modulation schemes. Simulation results show that using the proposed machine learning algorithm, our approach achieves significantly lower BER than traditional serial interference cancellation methods and the best JML detection performance.} }
@inproceedings{10.1145/3697090.3699865, title = {Impact of Data Anonymization in Machine Learning Models}, booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing}, pages = {188--191}, year = {2024}, isbn = {9798400717406}, doi = {10.1145/3697090.3699865}, url = {https://doi.org/10.1145/3697090.3699865}, author = {Pimenta, Ivo and Silva, Douglas and Moura, Evellin and Silveira, Matheus and Gomes, Rafael Lopes}, keywords = {Artificial Intelligence, Privacy, Anonymization}, abstract = {Data leakage compromises companies’ confidentiality and directly impacts existing privacy laws, where it is necessary to ensure integration with legacy systems to avoid harming the performance of their services while efficiently using the data for training machine learning (ML) models. Within this context, this work applies a technique to anonymize the data and train ML models in the context of DDoS attacks, aiming to evaluate the performance of this anonymization technique.} }
@inproceedings{10.1145/3695053.3731053, title = {LightML: A Photonic Accelerator for Efficient General Purpose Machine Learning}, booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture}, pages = {18--33}, year = {2025}, isbn = {9798400712616}, doi = {10.1145/3695053.3731053}, url = {https://doi.org/10.1145/3695053.3731053}, author = {Liu, Liang and Kari, Sadra Rahimi and Xin, Xin and Youngblood, Nathan and Zhang, Youtao and Yang, Jun}, keywords = {Optical Computing Crossbar, Coherent Photonic Multiplication, Ultralow-Energy Machine Learning}, abstract = {The rapid integration of AI technologies into everyday life across sectors such as healthcare, autonomous driving, and smart home applications requires extensive computational resources, placing strain on server infrastructure and incurring significant costs.We present LightML, the first system-level photonic crossbar design, optimized for high-performance machine learning applications. This work provides the first complete memory and buffer architecture carefully designed to support the high-speed photonic crossbar, achieving over 80\% utilization. LightML\&nbsp;also introduces solutions for key ML functions, including large-scale matrix multiplication (MMM), element-wise operations, non-linear functions, and convolutional layers. Delivering 325 TOP/s at only 3 watts, LightML offers significant improvements in speed and power efficiency, making it ideal for both edge devices and dense data center workloads.} }
@inproceedings{10.1145/3767052.3767068, title = {Machine Learning Models for Predicting Second-hand House Prices: A Comparative Study}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {99--106}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767068}, url = {https://doi.org/10.1145/3767052.3767068}, author = {Jiang, Haoran}, keywords = {LightGBM, Linear Regression, Machine learning, Random Forest, Ridge Regression, Second-hand house price prediction, XGBoost}, abstract = {This paper presents a comparative analysis of the application of machine learning models in predicting the prices of second-hand houses. It underscores the importance of the real estate market within the economy and points out the advantages of precise predictions for various stakeholders, including homebuyers, real estate agents, and financial institutions. Traditional methods frequently fail to grasp the intricate interactions that impact house prices, whereas machine learning techniques have garnered attention for their capability to handle such complexities. The study made use of the “Shanghai Second-hand Housing Prices from 2024 Anjuke Website” dataset from Kaggle, which comprises 175,135 records, each having nearly 30 features. After undergoing preprocessing procedures like duplicate removal, feature transformation, outlier removal, missing value imputation, feature encoding, feature engineering, and data standardization, the data was divided into training and testing sets. Four machine learning models were compared: Linear Regression, Ridge Regression, Random Forest, and XGBoost. The findings revealed that Random Forest and XGBoost surpassed the linear models, with Random Forest attaining the lowest Mean Absolute Error (MAE) and the highest R² Score. The study concluded that Random Forest was the most effective model for predicting the prices of second-hand houses in Shanghai. Even though the results were promising, the study acknowledged certain limitations such as the temporal and geographical constraints of the dataset and the reliance on historical data. Future research ought to concentrate on real-time data integration, model updating, incorporating additional data sources, and exploring advanced techniques to improve the performance and interpretability of the model.} }
@inproceedings{10.1145/3691573.3691592, title = {Machine Learning Applied to Locomotion in Virtual Reality}, booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality}, pages = {134--139}, year = {2024}, isbn = {9798400709791}, doi = {10.1145/3691573.3691592}, url = {https://doi.org/10.1145/3691573.3691592}, author = {Sakabe, Fernando Kenji and Ayres, Fabio Jos\'e and Soares, Luciano Pereira}, keywords = {Feet Tracking, Machine Learning, User Navigation, Virtual Locomotion, Walking Pattern Recognition, Walking in Virtual Reality, location = Manaus, Brazil}, abstract = {The objective of this research project is to recognize a user’s walking pattern on a treadmill-like platform for navigation in Virtual Reality (VR) environments. To achieve this, a walking recognition software solution was developed using Convolutional Neural Networks (CNNs), a type of Machine Learning (ML) architecture. The ML model was trained on a dataset collected from users’ movements in a virtual simulation, tracking the positions and rotations of their feet as they walked in various directions and orientations. Tracking was accomplished using 6 degrees of freedom (6DoF) trackers placed on the users’ feet. The neural network achieved a 94\% accuracy rate during testing. Due to the network’s lightweight configuration, the response time is, on average, 0.1 seconds, demonstrating its potential as an efficient natural controller for real-time user navigation in multiple directions.} }
@inproceedings{10.1145/3714334.3714341, title = {Machine Learning-Based Income Inequality Prediction: A Case Study}, booktitle = {Proceedings of the 2024 2nd International Conference on Artificial Intelligence, Systems and Network Security}, pages = {34--39}, year = {2025}, isbn = {9798400711237}, doi = {10.1145/3714334.3714341}, url = {https://doi.org/10.1145/3714334.3714341}, author = {Ji, Jiexin}, keywords = {Income Inequality, Income Prediction, Machine Learning, Socioeconomic Factors, XGBoost}, abstract = {Income inequality is an important socioeconomic issue, especially in countries such as the United States, where income distribution gaps continue to widen. Predicting income levels and understanding the factors that contribute to these gaps can provide valuable insights for policymakers and researchers. This paper explores the application of XGBoost (Extreme Gradient Boosting) modeling to income level prediction. The task consists of predicting whether an individual's annual income exceeds $50,000 based on their socioeconomic attributes. After preprocessing the dataset (including dealing with missing values and encoding categorical features), the XGBoost model was trained and optimized using hyper-parameter tuning and cross-validation. The ROC AUC score was 0.93, highlighting the ability of the trained machine learning model to effectively discriminate between income categories.} }
@inproceedings{10.1145/3737821.3749562, title = {SweePix! A Machine Learning Driven Gallery Cleaning App Adapted to User Behavior}, booktitle = {Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction}, year = {2025}, isbn = {9798400719707}, doi = {10.1145/3737821.3749562}, url = {https://doi.org/10.1145/3737821.3749562}, author = {Sahin undefinedppoliti, Hatice and Bender-Saebelkampf, Christian and Abdenebaoui, Larbi}, keywords = {machine learning, image processing, gallery cleaning, distributed learning, similarity, aesthetics, personalization}, abstract = {As individuals accumulate large numbers of images, many of which are redundant or low-quality, identifying and deleting unnecessary photos becomes a time-consuming task. This study explores a gallery cleaning app that aims to use machine learning to help users manage their digital photo collections more efficiently. The app adapts to individual preferences by learning from user behavior, offering personalized suggestions for deletions. We evaluate the effectiveness of the app in cleaning participants’ galleries and examine whether its personalization features impact the overall cleaning process. While personalization did not significantly improve the results in the current study, the findings contribute to the growing field of Human-Centered AI, demonstrating how adaptive systems may improve user experiences by aligning with individual needs in AI-driven applications.} }
@inproceedings{10.1145/3650215.3650355, title = {IO Behaviour Analysis Based on Machine Learning}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {805--808}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650355}, url = {https://doi.org/10.1145/3650215.3650355}, author = {Pan, Weibo and Li, Fei and Li, Sheng and Zhao, Yongcai and Zhang, Jun and Yang, Di and Wan, Xiang}, abstract = {This article uses machine learning algorithms to predict the latency of an IO request without additional internal information on the device, treating it as a black box to model equipment performance prediction. Different from algorithms such as CART in machines used in amounts of IO performance prediction, this paper mainly analyzes the impact of attributes on prediction performance and implements and compares the performance of multiple algorithms on different workloads. Based on the GBDT algorithm and the attribute selection algorithm, it analyzes different metrics (attribute sets). It proposes that using a “lastdelay” attribute can improve the prediction effect and takes removing the distance attribute from SSDs based on traditional metrics into consideration. According to unstable effects on SSDs and unstable performance of the distance attribute, this paper points out the necessity of using different metrics for different loads and different devices.} }
@inproceedings{10.1145/3641554.3701803, title = {Larger than Life In-Class Demonstrations for Introductory Machine Learning}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {220--226}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701803}, url = {https://doi.org/10.1145/3641554.3701803}, author = {Chai, Henry and Gormley, Matthew R.}, keywords = {data science, education, machine learning, unplugged demos, location = Pittsburgh, PA, USA}, abstract = {This paper presents a collection of in-class demonstrations for an introductory machine learning (ML) class. Each demonstration engages students actively in visualizing the behavior of a machine learning algorithm in order to build an intuitive understanding. These demonstrations are in direct contrast to purely slide- or whiteboard- based presentations of the same concepts by being student-paced and highly interactive, leveraging the physical space of the classroom. We developed demonstrations for six common ML methods: decision trees, k-nearest neighbors, the Perceptron, stochastic gradient descent (SGD), neural networks, and multi-armed bandits. Survey data from two semesters show that our demonstrations enhance student retention of and engagement with the material, relative to lectures without similar in-class demonstrations. Our demonstrations use readily available materials and student volunteers, making them easily reproducible for any educator seeking to complement their existing ML course.} }
@inproceedings{10.1145/3725899.3725948, title = {Motivation Factor Analytic for Training Business Using Machine Learning}, booktitle = {Proceedings of the 2025 8th International Conference on Software Engineering and Information Management}, pages = {184--189}, year = {2025}, isbn = {9798400710438}, doi = {10.1145/3725899.3725948}, url = {https://doi.org/10.1145/3725899.3725948}, author = {Kulkarineetham, Suwanee and Amsupan, Satanphop and Maliyaem, Maleerat}, keywords = {K-nearest neighbors, Machine learning, Motivation factor analytic, Principal component analysis, Random forest}, abstract = {In today's competitive and technology-driven business environment, ongoing employee training is essential for professional growth and organizational success. This research aims to analyze and identify the key motivational factors that drive both self-sponsored and company-sponsored participants to enroll in marketing training programs used Principal Component Analysis (PCA) and using machine learning models K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF) to predict the participants returned to enroll training. Dataset was collected through surveys, processed using PCA to reduce dimensionality, and analyzed to enhance the interpretability of features such as career development motivations and company-driven policies. The predictive models purposed high accuracy, with KNN model based on PCA achieving 97.09\% for the company-sponsored participants and RF model based on PCA achieving 96.00\% for the self-sponsored participants. This research aims to identify key motivational factors for training program enrollment among self-sponsored and company-sponsored participants and predict the likelihood of participants returning for further training based on motivational factors. The insights offer valuable guidance for training providers to tailor their programs to participant motivations, effectively supporting talent development in the digital economy.} }
@article{10.1145/3747347, title = {Understanding Open Source Contributor Profiles in Popular Machine Learning Libraries}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3747347}, url = {https://doi.org/10.1145/3747347}, author = {Liu, Jiawen and Zhang, Haoxiang and Zou, Ying}, keywords = {Open Source Software, Developer Profiles, Collaborative Software Development, Machine Learning Libraries}, abstract = {With the increasing popularity of machine learning (ML), many open source software (OSS) contributors are attracted to developing and adopting ML approaches. Comprehensive understanding of ML contributors is crucial for successful ML OSS development and maintenance. Without such knowledge, there is a risk of inefficient resource allocation and hindered collaboration in ML OSS projects. Existing research focuses on understanding the difficulties and challenges perceived by ML contributors through user surveys. There is a lack of understanding of ML contributors based on their activities recorded in the software repositories. In this paper, we aim to understand ML contributors by identifying contributor profiles in ML libraries. We further study contributors’ OSS engagement from four aspects: workload composition, work preferences, technical importance, and ML-specific vs SE contributions. By investigating 11,949 contributors from 8 popular ML libraries (i.e., TensorFlow, PyTorch, scikit-learn, Keras, MXNet, Theano/Aesara, ONNX, and deeplearning4j), we categorize them into four contributor profiles: Core-Nighttime, Core-Daytime, Peripheral-Nighttime, and Peripheral-Daytime. We find that: 1) project experience, authored files, collaborations, pull requests comments received and approval ratio, and geographical location are significant features of all profiles; 2) contributors in Core profiles exhibit significantly different OSS engagement compared to Peripheral profiles; 3) contributors’ work preferences and workload compositions are significantly correlated with project popularity; and 4) long-term contributors evolve towards making fewer, constant, balanced and less technical contributions.} }
@inproceedings{10.1145/3675888.3676049, title = {Android Malware Detection System using Machine Learning}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {186--191}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676049}, url = {https://doi.org/10.1145/3675888.3676049}, author = {Kaur, Amanpreet and Lal, Sangeeta and Goel, Shruti and Pandey, Mrinal and Agarwal, Astha}, abstract = {Detecting Android malware is imperative for safeguarding user privacy, securing data, and preserving device performance. Consequently, numerous studies have underscored the complexities associated with Android malware detection, prompting a multidimensional approach to tackle these challenges effectively. This research leverages machine learning techniques, emphasizing feature extraction, classification algorithms, and both supervised and unsupervised learning methodologies. The exploration begins with in-depth Exploratory Data Analysis (EDA) to gain insights into the dataset, paving the way for informed decision-making. Principal Component Analysis (PCA) is employed for dimensionality reduction, a pivotal step in handling the multivariate nature of the data. The integration of API calls, clustering, and anomaly detection further enriches the model's capability to discern between benign and malicious applications. Crucially, the study delves into the intricacies of sampling, evaluation, and the Confusion Matrix to quantify the model's performance accurately. The utilization of diverse classification algorithms, including Support Vector Machines (SVM), Multi-Layer Perceptrons (MLP), Random Forest, GaussianNB, Decision Tree, and Logistic Regression, underscores the comprehensive nature of the approach. These algorithms collectively contribute to a robust and versatile Android malware detection model capable of adapting to varying threat scenarios. The dataset employed for training and evaluation is sourced from Kaggle, encompassing 29,999 Android applications categorized as benign or malicious based on permissions sought. Current detection methods, deemed resource-intensive and exhaustive, face the challenge of keeping pace with the relentless evolution of new malware strains. This research seeks to address this gap by proposing a sophisticated, machine learning-driven model that not only enhances accuracy but also demonstrates efficiency and adaptability in the face of a dynamic threat landscape.} }
@inproceedings{10.1145/3652963.3655064, title = {Machine Learning Systems are Bloated and Vulnerable}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, booktitle = {Abstracts of the 2024 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems}, volume = {8}, pages = {37--38}, year = {2024}, isbn = {9798400706240}, doi = {10.1145/3652963.3655064}, url = {https://doi.org/10.1145/3652963.3655064}, author = {Zhang, Huaifeng and Alhanahnah, Mohannad and Ahmed, Fahmi Abdulqadir and Fatih, Dyako and Leitner, Philipp and Ali-Eldin, Ahmed}, keywords = {machine learning systems, software debloating, location = Venice, Italy}, abstract = {Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from operating systems and applications to containers. Containers are lightweight virtualization technologies used to package code and dependencies, providing portable, reproducible and isolated environments. For their ease of use, data scientists often utilize machine learning containers to simplify their workflow. However, this convenience comes at a cost: containers are often bloated with unnecessary code and dependencies, resulting in very large sizes. In this paper, we analyze and quantify bloat in machine learning containers. We develop MMLB, a framework for analyzing bloat in software systems, focusing on machine learning containers. MMLB measures the amount of bloat at both the container and package levels, quantifying the sources of bloat. In addition, MMLB integrates with vulnerability analysis tools and performs package dependency analysis to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from TensorFlow, PyTorch, and Nvidia, we show that bloat accounts for up to 80\% of machine learning container sizes, increasing container provisioning times by up to 370\% and exacerbating vulnerabilities by up to 99\%. For more detail, see the full paper, ~citezhang2024machine.} }
@inproceedings{10.1145/3630106.3658983, title = {Achieving Reproducibility in EEG-Based Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {1464--1474}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658983}, url = {https://doi.org/10.1145/3630106.3658983}, author = {Kinahan, Sean and Saidi, Pouria and Daliri, Ayoub and Liss, Julie and Berisha, Visar}, keywords = {EEG, Machine Learning, Reproducibility, location = Rio de Janeiro, Brazil}, abstract = {Despite the inherent complexity of electroencephalogram (EEG) data characterized by its high dimensionality, artifactual noise, and biological variability, many machine learning (ML) studies claim impressive performance in decoding or classifying EEG signals. Recently, several studies have highlighted that flawed data analysis is a prevalent issue in the literature, leading to irreproducible results and exaggerated claims. To address this issue, we propose a framework that addresses three primary obstacles in EEG ML research: data leakage, data scarcity, and flawed model selection. We introduce the EEG ML Model Card, a standardized and transparent EEG ML model documentation tool that aims to directly address these pitfalls and enhance reproducibility and trustworthiness in EEG ML research.} }
@inproceedings{10.1145/3593013.3594000, title = {‘Affordances’ for Machine Learning}, booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency}, pages = {324--332}, year = {2023}, isbn = {9798400701924}, doi = {10.1145/3593013.3594000}, url = {https://doi.org/10.1145/3593013.3594000}, author = {Davis, Jenny L}, keywords = {AI Alignment, Affordances, Design Studies, Machine Learning, Mechanisms and Conditions Framework, Principles-to-Practice, location = Chicago, IL, USA}, abstract = {The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.} }
@inbook{10.1145/3730436.3730535, title = {Empirical Study on Machine Learning Testing Based on Mutation Testing}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {608--613}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730535}, author = {Che, Zijie and Zhao, Ruilian and Wang, Weiwei}, abstract = {In the context of machine learning projects, the inherent randomness of machine learning algorithms and the intuitive nature of developer test assertions frequently result in unstable and ineffective test cases. Mutation testing is a proven method to enhance the efficacy of test cases. However, the implementation of mutation testing for machine learning is confronted with challenges such as the generation of a substantial number of mutants and suboptimal execution efficiency. This paper undertakes a comprehensive analysis of six prominent open-source machine learning projects, optimises their test cases through the utilisation of mutation testing, and conducts a detailed investigation into the performance of various mutants. The objective is to provide recommendations and assistance in the selection of effective mutants in machine learning projects. The primary conclusions of this study are as follows: 1) The predominant cause of the failure of machine learning mutants is the substantial semantic discrepancies between the mutant program statements and the original program. 2) In existing machine learning projects, the proportion of mutants eliminated by test assertions is minimal, with an average of merely 4.39\% of mutants being eliminated. 3) Mutation operators involving numeric and operator types are highly effective in adjusting the boundaries of test assertions.} }
@inproceedings{10.1145/3706890.3706940, title = {Machine Learning-Based Survival Prediction Model for Melanoma}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {289--295}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3706940}, url = {https://doi.org/10.1145/3706890.3706940}, author = {Jing, Zijie and Wang, Jianghong}, keywords = {Machine learning, Melanoma, Survival}, abstract = {The high growth rate of melanoma poses a huge challenge to healthcare delivery worldwide. At this stage, rapid and accurate diagnosis and timely treatment of melanoma is crucial. In this study, we utilized multiple metrics to predict survival in melanoma patients. We built multiple models of different types of machine learning, deep learning, and integrated learning, of which the best results were Logistic regression, Long short-term memory, and Categorical boosting, with AUCs of 0.9267, 0.9365, and 0.9417, respectively. We performed interpretability analyses of these models to improve the models' applicability in the clinic. The conclusions obtained from the analysis have important implications for the treatment of melanoma. This study represents a significant advancement in the use of machine learning methods to predict the survival status of individuals with melanoma, providing a new way of thinking about melanoma treatment and prognosis.} }
@article{10.1145/3639368, title = {Fairness-Driven Private Collaborative Machine Learning}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {15}, year = {2024}, issn = {2157-6904}, doi = {10.1145/3639368}, url = {https://doi.org/10.1145/3639368}, author = {Pessach, Dana and Tassa, Tamir and Shmueli, Erez}, keywords = {Privacy, algorithmic fairness, collaborative machine learning, federated learning, secure multi-party computation}, abstract = {The performance of machine learning algorithms can be considerably improved when trained over larger datasets. In many domains, such as medicine and finance, larger datasets can be obtained if several parties, each having access to limited amounts of data, collaborate and share their data. However, such data sharing introduces significant privacy challenges. While multiple recent studies have investigated methods for private collaborative machine learning, the fairness of such collaborative algorithms has been overlooked. In this work, we suggest a feasible privacy-preserving pre-process mechanism for enhancing fairness of collaborative machine learning algorithms. An extensive evaluation of the proposed method shows that it is able to enhance fairness considerably with only a minor compromise in accuracy.} }
@article{10.1145/3708495, title = {Distributed Machine Learning in Edge Computing: Challenges, Solutions and Future Directions}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708495}, url = {https://doi.org/10.1145/3708495}, author = {Tu, Jingke and Yang, Lei and Cao, Jiannong}, keywords = {Edge computing, distributed machine learning, model optimization, data heterogeneity, communication constraints}, abstract = {Distributed machine learning on edges is widely used in intelligent transportation, smart home, industrial manufacturing, and underground pipe network monitoring to achieve low latency and real time data processing and prediction. However, the presence of a large number of sensing and edge devices with limited computing, storage, and communication capabilities prevents the deployment of huge machine learning models and hinders its application. At the same time, although distributed machine learning on edges forms an emerging and rapidly growing research area, there has not been a systematic survey on this topic. The article begins by detailing the challenges of distributed machine learning in edge environments, such as limited node resources, data heterogeneity, privacy, security issues, and summarizes common metrics for model optimization. We then present a detailed analysis of parallelism patterns, distributed architectures, and model communication and aggregation schemes in edge computing. we subsequently present a comprehensive classification and intensive description of node resource-constrained processing, heterogeneous data processing, attacks and protection of privacy. The article ends by summarizing the applications of distributed machine learning in edge computing and presenting problems and challenges for further research.} }
@inproceedings{10.1145/3578356.3592581, title = {Actionable Data Insights for Machine Learning}, booktitle = {Proceedings of the 3rd Workshop on Machine Learning and Systems}, pages = {1--7}, year = {2023}, isbn = {9798400700842}, doi = {10.1145/3578356.3592581}, url = {https://doi.org/10.1145/3578356.3592581}, author = {Wu, Ming-Chuan and B\"ahr, Manuel and Braun, Nils and Honauer, Katrin}, keywords = {data insights, interactive diagnostics, data-centric machine learning, location = Rome, Italy}, abstract = {Artificial Intelligence (AI) and Machine Learning (ML) have made tremendous progress in the recent decade and have become ubiquitous in almost all application domains. Many recent advancements in the ease-of-use of ML frameworks and the low-code model training automations have further reduced the threshold for ML model building. As ML algorithms and pre-trained models become commodities, curating the appropriate training datasets and model evaluations remain critical challenges. However, these tasks are labor-intensive and require ML practitioners to have bespoke data skills. Based on the feedback from different ML projects, we built ADIML (Actionable Data Insights for ML) - a holistic data toolset. The goal is to democratize data-centric ML approaches by removing big data and distributed system barriers for engineers. We show in several case studies how the application of ADIML has helped solve specific data challenges and shorten the time to obtain actionable insights.} }
@inproceedings{10.1145/3708360.3708388, title = {Construction of Hotel Energy Consumption Prediction and Management Scheme Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {174--178}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708388}, url = {https://doi.org/10.1145/3708360.3708388}, author = {Xue, Wenting}, keywords = {Energy consumption prediction, Hotel energy management, Machine learning, Multi model integration}, abstract = {As demand for energy management in the hotel increases, energy efficiency and energy consumption costs are important research directions. This paper introduces the energy consumption prediction and management plan of the hotel based on machine learning, and adopts the hierarchical structure design. The power consumption prediction using the depth learning model such as LSTM, Gru, TCN, and so on is optimized by using the dynamic weight distribution strategy. The system integrates an adaptive alarm mechanism, provides real-time energy monitoring, and provides optimization suggestions. Experimental results show that this method has high predictive accuracy and stability under different loading conditions, effectively reducing the energy consumption of the hotel and improving the energy management efficiency. As a result, the energy management system based on machine learning has a high degree of adaptability under complicated circumstances and provides a wide range of applications.} }
@inproceedings{10.1145/3646547.3689668, title = {Poster: Predicting Internet Shutdowns - A Machine Learning Approach}, booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference}, pages = {763--764}, year = {2024}, isbn = {9798400705922}, doi = {10.1145/3646547.3689668}, url = {https://doi.org/10.1145/3646547.3689668}, author = {Zirikana, Jules and Phokeer, Amreesh}, keywords = {internet shutdowns, random forest, shutdown risk prediction, location = Madrid, Spain}, abstract = {Internet shutdowns, often enforced by governments to control communication and access to information, have significant socio-political and economic implications. This study presents a machine learning approach to predict the likelihood of internet shutdowns, developing an Internet Shutdown Risk Score using public datasets from 125 countries. A Random Forest classifier, achieving an AUC of 0.97, was used to calculate risk scores. Key features were identified using the Shapley algorithm, highlighting factors like political unrest, economic conditions, and digital infrastructure. Case studies in Pakistan, India, and Sudan demonstrate rising shutdown risks due to protests from 2019 to 2022. Globally, the Internet Shutdown Risk Index has been consistently high since 2019, indicating increased threats of internet shutdowns in politically unstable regions.} }
@inproceedings{10.1145/3715335.3735471, title = {Bridging the Gap: Climate Scientists' Concerns and Expectations for Machine Learning}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {284--296}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735471}, url = {https://doi.org/10.1145/3715335.3735471}, author = {Wu, Siyi and Easterbrook, Steve M.}, keywords = {Machine Learning, Climate Science, Socio-Technical Systems}, abstract = {As a field with huge volumes of spatio-temporal data and challenging computational bottlenecks, climate science stands to benefit from recent advances in Machine Learning. But ML represents both an opportunity and a threat. While ML has the potential to overcome technical barriers in studying and predicting the climate system, it also up-ends long-standing scientific norms for how climate knowledge is produced, validated, and communicated. Acceptance of these new methods in the field therefore depends on how experts from different fields collaborate and how they address concerns over applicability, interpretability, and scientific rigor. To explore the socio-technical setting in which ML is being adopted by climate scientists, we conducted an interview study to identify how climate scientists are using (or resisting the use of) ML in their work, what concerns and challenges they face, and how what expectations they have for the potential of AI in climate science. The results show that while some climate scientists embrace ML for its computational advantages and potential to enhance climate modeling, others remain skeptical due to concerns about interpretability, validation, and alignment with established scientific practices. Participants highlighted the need for transparent, domain-informed AI methodologies and emphasized barriers to interdisciplinary collaboration between climate scientists and AI researchers. These findings underscore that AI’s role in climate science is not predetermined but actively shaped by domain experts through their expectations, concerns, and engagement with these technologies.} }
@inproceedings{10.1145/3722212.3724483, title = {Ninth Workshop on Data Management for End-to-End Machine Learning (DEEM)}, booktitle = {Companion of the 2025 International Conference on Management of Data}, pages = {878--879}, year = {2025}, isbn = {9798400715648}, doi = {10.1145/3722212.3724483}, url = {https://doi.org/10.1145/3722212.3724483}, author = {Grafberger, Stefan and Hulsebos, Madelon and Interlandi, Matteo and Shankar, Shreya}, keywords = {data management, machine learning, systems, location = Berlin, Germany}, abstract = {The DEEM'25 workshop (Data Management for End-to-End Machine Learning) is held on Friday, June 27th, in conjunction with SIGMOD/PODS 2025. DEEM brings together researchers and practitioners at the intersection of applied machine learning, data management, and systems research, with the goal of discussing the arising data management issues in ML application scenarios. The workshop solicits regular research papers (8 pages) describing preliminary and ongoing research results, including industrial experience reports of end-to-end ML deployments, related to DEEM topics. In addition, DEEM 2025 has a category for short papers (4 pages) as a forum for sharing interesting use cases, problems, datasets, benchmarks, visionary ideas, system designs, preliminary results, and descriptions of system components and tools related to end-to-end ML pipelines. This year, the workshop received 18 high-quality submissions on diverse topics relevant to DEEM.} }
@inbook{10.1145/3676641.3716249, title = {Relax: Composable Abstractions for End-to-End Dynamic Machine Learning}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2}, pages = {998--1013}, year = {2025}, isbn = {9798400710797}, url = {https://doi.org/10.1145/3676641.3716249}, author = {Lai, Ruihang and Shao, Junru and Feng, Siyuan and Lyubomirsky, Steven and Hou, Bohan and Lin, Wuwei and Ye, Zihao and Jin, Hongyi and Jin, Yuchen and Liu, Jiawei and Jin, Lesheng and Cai, Yaxing and Jiang, Ziheng and Wu, Yong and Park, Sunghyun and Srivastava, Prakalp and Roesch, Jared and Mowry, Todd C. and Chen, Tianqi}, abstract = {Dynamic shape computations have become critical in modern machine learning workloads, especially in emerging large language models. The success of these models has driven the demand for their universal deployment across a diverse set of backend environments. In this paper, we present Relax, a compiler abstraction for optimizing end-to-end dynamic machine learning workloads. Relax introduces a cross-level abstraction that encapsulates computational graphs, loop-level tensor programs, and external library calls in a single representation. Relax also introduces first-class symbolic shape annotations to track dynamic shape computations globally across the program, enabling dynamic shape-aware cross-level optimizations. We build an end-to-end compilation framework using the proposed approach to optimize dynamic shape models. Experimental results on LLMs show that Relax delivers performance competitive with state-of-the-art systems across various GPUs and enables deployment of emerging models to a broader set of emerging environments, including mobile phones, embedded devices, and web browsers.} }
@article{10.14778/3750601.3750695, title = {Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods}, journal = {Proc. VLDB Endow.}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, volume = {18}, pages = {5460--5465}, year = {2025}, issn = {2150-8097}, isbn = {9798400704369}, doi = {10.14778/3750601.3750695}, url = {https://doi.org/10.14778/3750601.3750695}, author = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Adhikari, Saugat}, keywords = {GNN, graph, graph neural network, structure, subgraph, system, vertex, location = Boise, ID, USA}, abstract = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand.In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems. Slides are available at https://github.com/akhlaqueak/VLDB-2025-Tutorial.} }
@inproceedings{10.1145/3708036.3708177, title = {Machine Learning Based Supply Chain Risk Prediction and Management}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {842--847}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708177}, url = {https://doi.org/10.1145/3708036.3708177}, author = {Li, Changxia and Chen, Haolin}, keywords = {machine learning, predictive parsing, security, supply chain risk management}, abstract = {Traditional supply chain risk management methods show limitations when facing unexpected events and complex supply chain networks, however, machine learning techniques that can handle unstructured and multi-dimensional data and extract valuable risk information from complex data sets through efficient algorithms have provided new ideas for supply chain management in recent years. This paper proposes a continuous multi-perspective conceptual framework that aims to systematically explore how machine learning can be effectively applied in supply chain risk prediction and management. Meanwhile, this paper will also investigate the potential contribution of utilizing innovative technologies such as blockchain in enhancing supply chain security and competitiveness, with a view to providing more comprehensive and profound insights into modern supply chain management.} }
@inproceedings{10.1145/3637528.3671478, title = {The Third Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6714--6715}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671478}, url = {https://doi.org/10.1145/3637528.3671478}, author = {Goldenberg, Dmitri and Meir Lador, Shir and Sokolova, Elena and Cheong, Lin Lee and Sukhwani, Mohak and Potdar, Saloni}, keywords = {data science management, genai and compliance, machine learning management, ml product development, location = Barcelona, Spain}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions,including the emerging field of General AI (GenAI), has created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Third KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@inproceedings{10.1145/3766918.3766947, title = {ExHybridNet: An Explainable Hybrid Machine Learning Model for Corporate Financial Risk Prediction}, booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business}, pages = {172--178}, year = {2025}, isbn = {9798400716027}, doi = {10.1145/3766918.3766947}, url = {https://doi.org/10.1145/3766918.3766947}, author = {Luo, Yuexin}, keywords = {Explainability, Financial Risk Prediction, Hybrid Machine learning, Neural Network}, abstract = {In a complex and volatile economic environment, accurately predicting the financial risks of enterprises is of vital importance to investors, creditors and regulatory authorities. This study proposes an interpretable hybrid machine learning model, ExHybridNet, for enterprise financial risk prediction. This model effectively captures dynamic nonlinear relationships in financial data through a three-path fusion architecture of convolutional neural networks, attention mechanism and random forest. ExHybridNet also provides transparent feature importance explanations through SHAP analysis. Based on the financial and audit data of listed companies on the Shenzhen and Beijing stock Exchanges in the CSMAR database from 2005 to 2024, ExHybridNet significantly outperforms traditional machine learning models (logistic regression, random forest, support vector machine), deep learning models (CNN, MLP), and hybrid machine learning (RF-CNN, CNN-Attention) in terms of accuracy, precision, and F1 score. The research results of SHAP analysis have revealed the roles of risk drivers such as high leverage ratio (D/A) and short-term borrowing dependence (STBD), as well as risk mitigation factors such as profit stability (EV) and tangible asset ratio (TAR), providing actionable insights for enterprise financial management, risk prevention and risk control.} }
@inproceedings{10.1145/3669940.3707284, title = {PartIR: Composing SPMD Partitioning Strategies for Machine Learning}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, pages = {794--810}, year = {2025}, isbn = {9798400706981}, doi = {10.1145/3669940.3707284}, url = {https://doi.org/10.1145/3669940.3707284}, author = {Alabed, Sami and Belov, Daniel and Chrzaszcz, Bart and Franco, Juliana and Grewe, Dominik and Maclaurin, Dougal and Molloy, James and Natan, Tom and Norman, Tamara and Pan, Xiaoyue and Paszke, Adam and Rink, Norman A. and Schaarschmidt, Michael and Sitdikov, Timur and Swietlik, Agnieszka and Vytiniotis, Dimitrios and Wee, Joel}, keywords = {distributed systems, language/compiler support for machine learning, machine learning model partitioning, parallelism, scalability, spmd, systems for machine learning, systems for tensor computing, location = Rotterdam, Netherlands}, abstract = {Training modern large neural networks (NNs) requires a combination of parallelization strategies, including data, model, or optimizer sharding. To address the growing complexity of these strategies, we introduce PartIR, a hardware-and-runtime agnostic NN partitioning system. PartIR is: 1) Expressive: It allows for the composition of multiple sharding strategies, whether user-defined or automatically derived; 2) Decoupled: the strategies are separate from the ML implementation; and 3) Predictable: It follows a set of well-defined general rules to partition the NN. PartIR utilizes a schedule-like API that incrementally rewrites the ML program intermediate representation (IR) after each strategy, allowing simulators and users to verify the strategy's performance. PartIR has been successfully used both for training large models and across diverse model architectures, demonstrating its predictability, expressiveness, and performance.} }
@inproceedings{10.1145/3701047.3701072, title = {Research on Key Technologies and Applications of Recommendation System Based on Machine Learning}, booktitle = {Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, pages = {136--141}, year = {2025}, isbn = {9798400711688}, doi = {10.1145/3701047.3701072}, url = {https://doi.org/10.1145/3701047.3701072}, author = {Liu, Dan and Sun, Yuan and Hao, Jianmin and Fu, Lin}, keywords = {hybrid recommendation algorithm model, key technologies, machine learning, recommendation system}, abstract = {In the current highly information-based and digital social environment, numerous technical applications are constantly enriching the sources and communication channels of network information data, so that users are facing the problems of information explosion and information overload while enjoying the convenience of information acquisition. Therefore, the emergence and development of recommendation system has become the only way to solve this problem. However, the conventional recommendation system still has obvious lag when dealing with data sparsity and cold start, and it is difficult to achieve accurate recommendation. Based on the actual application requirements, this paper will comprehensively explore the feasibility of machine learning algorithm in the field of recommendation system, and propose a set of recommendation algorithm construction scheme based on machine learning to make up for the shortcomings of the existing recommendation system. Practice has proved that the hybrid recommendation algorithm model based on decision tree (DT), random forest (RF) and logistic regression (LR) can automatically combine and screen features, mine valuable feature information from sparse data, and use these feature information to make efficient classification prediction, and then generate more comprehensive and accurate recommendation results, so as to improve user satisfaction and the performance of recommendation system.} }
@inproceedings{10.1145/3508352.3561121, title = {Machine Learning for Testing Machine-Learning Hardware: A Virtuous Cycle}, booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design}, year = {2022}, isbn = {9781450392174}, doi = {10.1145/3508352.3561121}, url = {https://doi.org/10.1145/3508352.3561121}, author = {Chaudhuri, Arjun and Talukdar, Jonti and Chakrabarty, Krishnendu}, abstract = {The ubiquitous application of deep neural networks (DNN) has led to a rise in demand for AI accelerators. DNN-specific functional criticality analysis identifies faults that cause measurable and significant deviations from acceptable requirements such as the inferencing accuracy. This paper examines the problem of classifying structural faults in the processing elements (PEs) of systolic-array accelerators. We first present a two-tier machine-learning (ML) based method to assess the functional criticality of faults. While supervised learning techniques can be used to accurately estimate fault criticality, it requires a considerable amount of ground truth for model training. We therefore describe a neural-twin framework for analyzing fault criticality with a negligible amount of ground-truth data. We further describe a topological and probabilistic framework to estimate the expected number of PE's primary outputs (POs) flipping in the presence of defects and use the PO-flip count as a surrogate for determining fault criticality. We demonstrate that the combination of PO-flip count and neural twin-enabled sensitivity analysis of internal nets can be used as additional features in existing ML-based criticality classifiers.} }
@inproceedings{10.1145/3670474.3685943, title = {Enhancing the Capabilities of Quantum Transport Simulations Utilizing Machine Learning Strategies}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685943}, url = {https://doi.org/10.1145/3670474.3685943}, author = {Naseer, Ateeb and Zarkob, Yawar Hayat and Rafiq, Musaib and Nazir, Mohammad Sajid and Ahmad, Owais and Agarwal, Amit and Bhowmick, Somnath and Chauhan, Yogesh Singh}, keywords = {Density functional theory (DFT), NanoTCAD, machine learning (ML), maximally localized Wannier functions (MLWFs), nonequilibrium Green's function (NEGF), novel 2-D materials, quantum transport simulations, location = Salt Lake City, UT, USA}, abstract = {The ongoing pursuit of exploring novel materials for potential future device applications continues to strengthen the vital role of Technology Computer-Aided Design (TCAD) simulations within the device community. However, its computationally intensive and time-consuming nature necessitates novel methodologies to overcome the limitations. Machine learning (ML) is a potential remedy in our contemporary data-driven society. In this work, we present a unique approach based on Neural Networks (NNs) to generate an efficient potential profile guess. The predicted ML potential enhances the convergence of the coupled Schrodinger and Poisson equation and speeds up the simulation process, ~ 2.5x, by reducing the number of self-consistent iterations at each bias point. Moreover, our method demonstrates versatility by providing reasonable prediction capacity and accuracy for grid reduction, doping, and channel material variations.} }
@article{10.5555/3722577.3722856, title = {On doubly robust inference for double machine learning in semiparametric regression}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Dukes, Oliver and Vansteelandt, Stijn and Whitney, David}, keywords = {doubly robust estimation, semiparametric inference, causal inference, conditional independence testing}, abstract = {Due to concerns about parametric model misspecification, there is interest in using machine learning to adjust for confounding when evaluating the causal effect of an exposure on an outcome. Unfortunately, exposure effect estimators that rely on machine learning predictions are generally subject to so-called plug-in bias, which can render naive p-values and confidence intervals invalid. Progress has been made via proposals like targeted minimum loss estimation and more recently double machine learning, which rely on learning the conditional mean of both the outcome and exposure. Valid inference can then be obtained so long as both predictions converge (sufficiently fast) to the truth. Focusing on partially linear regression models, we show that a specific implementation of the machine learning techniques can yield exposure effect estimators that have small bias even when one of the first-stage predictions does not converge to the truth. The resulting tests and confidence intervals are doubly robust. We also show that the proposed estimators may fail to be regular when only one nuisance parameter is consistently estimated; nevertheless, we observe in simulation studies that our proposal can lead to reduced bias and improved confidence interval coverage in moderate-to-large samples.} }
@inproceedings{10.1145/3712255.3716519, title = {Evolutionary Art and Design in the Machine Learning Era}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1207--1247}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716519}, url = {https://doi.org/10.1145/3712255.3716519}, author = {Machado, Penousal and Correia, Jo\~ao} }
@article{10.1145/3773084, title = {Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3773084}, url = {https://doi.org/10.1145/3773084}, author = {Gu, Yang and You, Hengyu and Cao, Jian and Yu, Muran and Fan, Haoran and Qian, Shiyou}, keywords = {Machine Learning Workflows, Large Language Models, Software Engineering, AutoML, Survey}, abstract = {Machine Learning (ML) workflows—spanning data preprocessing and feature engineering, model selection and hyperparameter optimization, and workflow evaluation—are increasingly embedded in complex software systems. Building these workflows manually demands substantial ML expertise, domain knowledge, and engineering effort. Automated ML (AutoML) frameworks address parts of this challenge but often suffer from constrained search spaces, limited adaptability, and low interpretability. Recent advances in Large Language Models (LLMs) have opened new opportunities to automate and enhance ML workflows by leveraging their capabilities in language understanding, reasoning, interaction, and code generation, posing new practical and theoretical challenges for software engineering (SE). This survey provides the first SE-oriented, stage-wise review of LLM-based ML workflow automation. We introduce a taxonomy covering all three workflow stages, systematically compare and analyze state-of-the-art methods, and synthesize both stage-specific and cross-stage trends. Our analysis yields SE-oriented implications, including the need for robust verification, quality management, context-aware deployment, and risk mitigation, alongside ensuring key quality attributes such as usability, modularity, traceability, and performance. The findings also call for adapting development models, rethinking lifecycle boundaries, and formalizing uncertainty handling to address the probabilistic and collaborative nature of LLM-assisted workflow generation. We further identify major open challenges and outline future research directions to guide the reliable and effective adoption of LLMs in ML workflow development. Our artifacts are publicly available at .} }
@inproceedings{10.1145/3681778.3698784, title = {Rail transit delay forecasting with Causal Machine Learning}, booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Spatiotemporal Causal Analysis}, pages = {1--10}, year = {2024}, isbn = {9798400711541}, doi = {10.1145/3681778.3698784}, url = {https://doi.org/10.1145/3681778.3698784}, author = {Srivastava, Nishtha and Gohil, Bhavesh N. and Ray, Suprio}, keywords = {Arrival time prediction, Average Treatment Effect, Individual Treatment Effect, Public transport, Rail transit, causal ML, location = Atlanta, GA, USA}, abstract = {The rapid evolution of public transport and advances in analytics have significantly transformed the way we enhance transit services. Rail transit systems, celebrated for their comfort, speed, and minimal environmental impact, face ongoing challenges due to persistent delays. We introduce a novel approach that integrates causal inference with machine learning techniques to predict rail transit delays and uncover key causal factors. Utilizing the New Jersey Transit dataset, we apply uplift modeling and causal inference methods to enhance delay predictions. The study employs Individual Treatment Effect (ITE) and Average Treatment Effect (ATE) metrics to interpret and validate the predictions. Our research offers a comprehensive understanding of rail transit delays and provides actionable insights for policymakers, urban planners, and public health officials. By advancing causal analytical techniques, this work aims to improve transit reliability and efficiency on a global scale.} }
@inproceedings{10.1145/3735014.3735894, title = {Graduate Enrollment Information Management System Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {173--177}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735894}, url = {https://doi.org/10.1145/3735014.3735894}, author = {Li, Kainan and Jiang, Hailin}, keywords = {Enrollment information, Graduate student, Machine learning, Management system}, abstract = {Graduate enrollment and admission work are crucial to the higher education system and essential for maintaining academic heritage and innovation. With the rapid advancement of information technology (IT), especially as China's education sector enters a new stage of comprehensive digital transformation, universities face new development opportunities and severe challenges. The traditional enrollment information management system has become outdated regarding processing speed, information security, and intelligent decision-making assistance. It cannot meet the current situation of expanding graduate enrollment scale, complex processes, and diversified needs of candidates. Therefore, this article proposes an innovative design scheme for a graduate enrollment information management system based on machine learning (ML) technology. This solution integrates advanced data mining (DM), natural language processing (NLP), and predictive analysis algorithms to achieve automated collection, intelligent screening, precise matching, and efficient operation of enrollment information. Experimental results have shown that the system effectively improves the efficiency and quality of graduate enrollment work.} }
@proceedings{10.1145/3747227, title = {MLNN '25: Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, year = {2025}, isbn = {9798400714382} }
@article{10.1145/3736579, title = {Machine Learning-Assisted VCD Processing for Accelerated Dynamic Voltage Drop Analysis}, journal = {ACM Trans. Des. Autom. Electron. Syst.}, year = {2025}, issn = {1084-4309}, doi = {10.1145/3736579}, url = {https://doi.org/10.1145/3736579}, author = {Hu, Jingchao and Chen, Yufei and Sun, Songyu and Song, Jianfei and Zhang, Li and Yin, Xunzhao and Jin, Zhou and Zhuo, Cheng}, keywords = {VCD, Machine learning, Power supply, Noise, Profiling}, abstract = {With escalating power integrity challenges in advanced technologies, acquiring accurate dynamic power supply noise through Dynamic Voltage Drop (DVD) analysis becomes increasingly demanding. As noise margins shrink, the use of Value Change Dump (VCD) files for precise DVD analysis is indispensable but computationally expensive. Furthermore, the substantial storage requirements of VCD files, which record digital waveforms from logical simulations, pose significant challenges. In this paper, we propose a machine learning (ML)-assisted VCD processing framework to accelerate DVD analysis and improve data efficiency. Transitions recorded in VCD files are mapped to a Physical Design-Aware Circuit Hierarchy Tree (CHT) for efficient feature extraction. These features are leveraged by an XGBoost-based predictor to identify critical vector time windows within the VCD, significantly reducing simulation complexity. Additionally, Huffman encoding is applied to compress signal names, further optimizing storage utilization. Experimental results show that DVD analysis using our profiled VCD files achieves a speedup of approximately 3.53 with an error margin of only 3.89\%.} }
@inproceedings{10.1109/ICSE-Companion66252.2025.00076, title = {The Balancing Act of Policies in Developing Machine Learning Explanations}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings}, pages = {237--238}, year = {2025}, isbn = {9798331536831}, doi = {10.1109/ICSE-Companion66252.2025.00076}, url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00076}, author = {Tjaden, Jacob}, abstract = {Due to the nature of opaque machine learning (ML) models, software engineers and data scientists struggle to understand how ML models make decisions [1]. Explainability research aims to provide transparency for these models [2] through two types of explanations. Global explanations describe how a model works generally and provide insight into its accuracy, biases, and fairness. Local explanations describe individual predictions made by the model in specific use cases.} }
@inproceedings{10.1145/3626246.3653389, title = {The Hopsworks Feature Store for Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {135--147}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3653389}, url = {https://doi.org/10.1145/3626246.3653389}, author = {de la R\'ua Mart\'nez, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr\"om, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim}, keywords = {arrow flight, duckdb, feature store, mlops, rondb, location = Santiago AA, Chile}, abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference. In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.} }
@inproceedings{10.1145/3679240.3734623, title = {Probabilistic and Explainable Machine Learning for Tabular Power Grid Data}, booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems}, pages = {213--231}, year = {2025}, isbn = {9798400711251}, doi = {10.1145/3679240.3734623}, url = {https://doi.org/10.1145/3679240.3734623}, author = {Nikoltchovska, Alexandra and P\"utz, Sebastian and Li, Xiao and Hagenmeyer, Veit and Sch\"afer, Benjamin}, keywords = {power grid frequency stability, probabilistic machine learning, explainable artificial intelligence, tabular data, deep learning, TabNetProba}, abstract = {Modeling power grid frequency stability is becoming increasingly challenging due to the integration of renewable energy sources. Machine learning approaches, such as gradient-boosted trees, have shown promise in analyzing the complex characteristics of power systems. However, these models are inherently deterministic, providing only point estimates. Meanwhile, the task of capturing the underlying uncertainty, particularly through (deep) probabilistic models, is still underexplored, despite its potential to better account for the stochastic nature of power grid dynamics. In this paper, we first compare the performance of TabNet, a deep learning architecture designed for tabular data, to XGBoost for modeling power grid frequency stability. We then present TabNetProba: a probabilistic extension of TabNet, that enables uncertainty-aware estimates comparable to NGBoost. Using these (trained) models, we leverage explainable artificial intelligence (XAI) to analyze the drivers influencing grid stability and identify sources of uncertainty in two major European synchronous areas: Continental Europe and the Nordic region. Our results demonstrate that TabNetProba achieves competitive performance with state-of-the-art methods while providing reliable uncertainty estimates. We find that load and conventional generation ramps, as well as forecast errors, are the key quantities for modeling and explaining mean stability indicators in both synchronous areas. In Continental Europe, renewable generation emerges as a key factor in explaining model uncertainty, while in the Nordic region, load and generation features dominate uncertainty estimation, allowing for more reliable and interpretable stability estimates for modern power systems.} }
@inproceedings{10.1145/3711542.3711555, title = {Sarcasm Detection of Facebook's Posts Using Machine Learning Models}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {349--354}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711555}, url = {https://doi.org/10.1145/3711542.3711555}, author = {Chiu, Shu-i and Jhou, Ting-Wei}, keywords = {Sarcasm, deep learning, machine learning, social media}, abstract = {The coronavirus disease 2019 (COVID-19) has brought massive challenges to the world, altering people's lives. We conducted a study on people's mental states during Taiwan's National Epidemic Level 3 Alert in 2021. On May 22, 2021, during a regular press conference held by the Taiwan Centers for Disease Control (CDC), the Minister of Health and Welfare introduced the term 'Retrospective Adjustment', which left the entire population in shock. Our approach consists of a two-stage task. Constructing the model is the first stage and detecting sarcasm is the second. First, we integrated TCNN\&nbsp;and BiLSTM models. Second, we focused on misclassified by the model and performed feature engineering based on these misclassified data. After constructing features, we performed well using machine learning models. Finally, the experimental results show that our approach performs well in detecting sarcasm using linguistic and lexical-based features. In the second stage, the LSTM model detects sarcasm, achieving a performance of 0.73. We integrate the results of two stages to adjust accuracy. By improving the accuracy of the misclassified data to 0.6, the overall accuracy for negative posts has increased to 0.76.} }
@inproceedings{10.1145/3655693.3661296, title = {Towards Access Control for Machine Learning Embeddings}, booktitle = {Proceedings of the 2024 European Interdisciplinary Cybersecurity Conference}, pages = {219--220}, year = {2024}, isbn = {9798400716515}, doi = {10.1145/3655693.3661296}, url = {https://doi.org/10.1145/3655693.3661296}, author = {Matzutt, Roman}, keywords = {Attribute-based encryption, access control, embeddings, location = Xanthi, Greece}, abstract = {In this work, we explore the potential to make embeddings, which are becoming an integral part of machine-learning pipelines, shareable with the general public while providing self-contained access control. To this end, we apply attribute-based encryption and discuss a potential application for supply chain management.} }
@inproceedings{10.1145/3759179.3760446, title = {Explainable Machine Learning for Detecting Malicious Student Behavior in Campus Networks}, booktitle = {Proceedings of the 10th International Conference on Cyber Security and Information Engineering}, pages = {299--305}, year = {2025}, isbn = {9798400718632}, doi = {10.1145/3759179.3760446}, url = {https://doi.org/10.1145/3759179.3760446}, author = {Yan, Qi}, keywords = {Anomaly Detection, Campus Network Security, Explainable Machine Learning, LIME, LSTM, Malicious Behavior Detection, SHAP, XGBoost}, abstract = {As digital infrastructure in higher education expands, campus networks face increasing threats from malicious student behaviors such as unauthorized resource access and exam-related cheating. While machine learning models have shown promise in anomaly detection, their lack of interpretability undermines trust and limits deployment in sensitive academic environments. This study proposes a hybrid explainable artificial intelligence (XAI) framework that integrates Extreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) for behavior classification, enhanced by SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) for global and local interpretability. Tested on over 2.3 million real-world campus network sessions, the system achieves an F1-score of 88.7\% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 93.1\%, while increasing administrator trust scores by 38\%. A live deployment during exam periods further demonstrates its practical value, reducing false positives to 10.1\%, cutting average investigation time by 50\%, and supporting proportional policy enforcement. The results highlight the operational, ethical, and governance benefits of embedding explainability into campus cybersecurity systems.} }
@article{10.1145/3687230.3687232, title = {Planter: Rapid Prototyping of In-Network Machine Learning Inference}, journal = {SIGCOMM Comput. Commun. Rev.}, volume = {54}, pages = {2--21}, year = {2024}, issn = {0146-4833}, doi = {10.1145/3687230.3687232}, url = {https://doi.org/10.1145/3687230.3687232}, author = {Zheng, Changgang and Zang, Mingyuan and Hong, Xinpeng and Perreault, Liam and Bensoussane, Riyad and Vargaftik, Shay and Ben-Itzhak, Yaniv and Zilberman, Noa}, keywords = {P4, dimension reduction, in-network computing, machine learning, machine learning compilers, modular framework, programmable switches}, abstract = {In-network machine learning inference provides high throughput and low latency. It is ideally located within the network, power efficient, and improves applications' performance. Despite its advantages, the bar to in-network machine learning research is high, requiring significant expertise in programmable data planes, in addition to knowledge of machine learning and the application area. Existing solutions are mostly one-time efforts, hard to reproduce, change, or port across platforms. In this paper, we present Planter: a modular and efficient open-source framework for rapid prototyping of in-network machine learning models across a range of platforms and pipeline architectures. By identifying general mapping methodologies for machine learning algorithms, Planter introduces new machine learning mappings and improves existing ones. It provides users with several example use cases and supports different datasets, and was already extended by users to new fields and applications. Our evaluation shows that Planter improves machine learning performance compared with previous model-tailored works, while significantly reducing resource consumption and co-existing with network functionality. Planter-supported algorithms run at line rate on unmodified commodity hardware, providing billions of inference decisions per second.} }
@inproceedings{10.1145/3757110.3757209, title = {Research on Machine Learning-Based Prediction Models for Liver Cirrhosis Complicated by Hepatocellular Carcinoma}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {591--597}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757209}, url = {https://doi.org/10.1145/3757110.3757209}, author = {Wang, Weice and Zhang, Xingchen and Chen, Hongyang and You, Linqiang and Shen, Yufeng and Zhang, Han and Lin, Yuxuan and Huang, Yuting and Yang, Changping and Huang, Zhihui}, keywords = {Hepatocellular carcinoma, Machine learning-based prediction models, Regression modeling, SVM}, abstract = {This study aims to develop machine learning-based prediction models for hepatocellular carcinoma (HCC) development in liver cirrhosis patients, analyze potential influencing factors, and evaluate the impact of body weight through regression modeling, thereby providing evidence for clinical prevention and individualized treatment strategies. Utilizing clinical data from 419 cirrhosis patients at a specialized hospital in Fuzhou, we systematically compared the performance of Logistic Regression, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Genetic Algorithm in model construction to optimize prediction accuracy and generalization capability. The results demonstrate that the SVM model achieved superior performance in HCC prediction, with an overall accuracy of 93.6\%, precision of 90.1\%, recall of 98.0\%, and F1-score of 93.9\%. The ANN model also exhibited strong predictive capability, showing training and test set AUC values of 0.86857 and 0.79826 respectively. In contrast, Logistic Regression underperformed in all evaluation metrics compared to SVM and ANN. Notably, the SVM model significantly enhanced screening sensitivity for high-risk HCC cases while demonstrating robustness against sample imbalance. This study successfully established and validated HCC prediction models based on SVM, Logistic Regression, and ANN, with particular emphasis on the superior predictive performance of the SVM framework. These comparative findings provide clinicians with enhanced tools for precise HCC risk assessment, facilitating early identification of high-risk patients and advancing the development of personalized therapeutic strategies.} }
@inproceedings{10.1145/3637528.3672068, title = {EcoVal: An Efficient Data Valuation Framework for Machine Learning}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {2866--2875}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3672068}, url = {https://doi.org/10.1145/3637528.3672068}, author = {Tarun, Ayush and Chundawat, Vikram and Mandal, Murari and Tan, Hong Ming and Chen, Bowei and Kankanhalli, Mohan}, keywords = {data valuation, machine learning, shapley value, location = Barcelona, Spain}, abstract = {Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall value of the data can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as aproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. The code is available at https://github.com/respai-lab/ecoval.} }
@article{10.5555/3648699.3649032, title = {Weisfeiler and Leman go machine learning: the story so far}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten}, keywords = {machine learning for graphs, graph neural networks, Weisfeiler-Leman algorithm, expressivity, equivariance}, abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.} }
@inproceedings{10.1145/3706598.3713524, title = {Perceptions of the Fairness Impacts of Multiplicity in Machine Learning}, booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, year = {2025}, isbn = {9798400713941}, doi = {10.1145/3706598.3713524}, url = {https://doi.org/10.1145/3706598.3713524}, author = {Meyer, Anna P. and Kim, Yea-Seul and D'Antoni, Loris and Albarghouthi, Aws}, keywords = {Fairness, Fairness Perceptions, Fairness in Machine Learning, Multiplicity, Stakeholder Survey}, abstract = {Machine learning (ML) is increasingly used in high-stakes settings, yet multiplicity – the existence of multiple good models – means that some predictions are essentially arbitrary. ML researchers and philosophers posit that multiplicity poses a fairness risk, but no studies have investigated whether stakeholders agree. In this work, we conduct a survey to see how multiplicity impacts lay stakeholders’ – i.e., decision subjects’ – perceptions of ML fairness, and which approaches to address multiplicity they prefer. We investigate how these perceptions are modulated by task characteristics (e.g., stakes and uncertainty). Survey respondents think that multiplicity threatens the fairness of model outcomes, but not the appropriateness of using the model, even though existing work suggests the opposite. Participants are strongly against resolving multiplicity by using a single model (effectively ignoring multiplicity) or by randomizing the outcomes. Our results indicate that model developers should be intentional about dealing with multiplicity in order to maintain fairness.} }
@inproceedings{10.1145/3711896.3737201, title = {Chasing the Timber Trail: Machine Learning to Reveal Harvest Location Misrepresentation}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {4796--4805}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737201}, url = {https://doi.org/10.1145/3711896.3737201}, author = {Sarkar, Shailik and Yousuf, Raquib Bin and Wang, Linhan and Mayer, Brian and Mortier, Thomas and Deklerck, Victor and Truszkowski, Jakub and Simeone, John C. and Norman, Marigold and Saunders, Jade and Lu, Chang-Tien and Ramakrishnan, Naren}, keywords = {Gaussian processes, ML applications, multitask learning, stable isotope ratio analysis (SIRA), uncertainty estimation, location = Toronto ON, Canada}, abstract = {Illegal logging poses a significant threat to global biodiversity, climate stability, and depresses international prices for legal wood harvesting and responsible forest products trade, affecting livelihoods and communities across the globe. Stable isotope ratio analysis (SIRA) is rapidly becoming an important tool for determining the harvest location of traded, organic, products. The spatial pattern in stable isotope ratio values depends on factors such as atmospheric and environmental conditions and can thus be used for geographic origin identification. We present here the results of a deployed machine learning pipeline where we leverage both isotope values and atmospheric variables to determine timber harvest location. Additionally, the pipeline incorporates uncertainty estimation to facilitate the interpretation of harvest location determination for analysts. We present our experiments on a collection of oak (Quercus spp.) tree samples from its global range. Our pipeline outperforms comparable state-of-the-art models determining geographic harvest origin of commercially traded wood products, and has been used by European enforcement agencies to identify harvest location misrepresentation. We also identify opportunities for further advancement of our framework and how it can be generalized to help identify the origin of falsely labeled organic products throughout the supply chain.} }
@inproceedings{10.1145/3626203.3670522, title = {Adversarial Robustness and Explainability of Machine Learning Models}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670522}, url = {https://doi.org/10.1145/3626203.3670522}, author = {Gafur, Jamil and Goddard, Steve and Lai, William}, keywords = {Framework, Machine Learning, eXplainable AI, packaging, location = Providence, RI, USA}, abstract = {The rapid advancement of machine learning has brought forth sophisticated neural network models harnessing computational prowess and vast datasets for diverse applications. Nonetheless, with the proliferation of these complex models, apprehensions have surfaced regarding their resilience, interpretability, and biases. To mitigate these concerns, we propose the “Adversarial Observation” framework, amalgamating explainable and adversarial methodologies for comprehensive neural network scrutiny. By integrating explainable techniques, users gain profound insights into the model’s internal mechanisms, fostering transparency and facilitating bias identification. This framework aims to enhance the trustworthiness and accountability of neural network systems amidst their expanding utility.} }
@inproceedings{10.1145/3716554.3716563, title = {An Intelligent Chatbot in Greek Using Machine Learning Technology}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {57--62}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716563}, url = {https://doi.org/10.1145/3716554.3716563}, author = {Nikologiannis, Orestis and Tsampos, Ioannis and Marakakis, Emmanouil}, keywords = {CCS CONCEPTS • Artificial Intelligence, Machine Learning, Natural Language Processing}, abstract = {In this paper, we present the development of a sophisticated conversational chatbot for querying and answering in Greek using machine learning technology. We have developed an interactive dialogue-based assistant which provides answers in Greek by processing questions in Greek related to an academic domain. The aim of this research work is the development of a conversational framework which will be domain and language independent and its users would be able to use their natural language to communicate and retrieve information from a problem domain. We have used the state-of-the-art Rasa framework which supports conversational Artificial Intelligence. Rasa is a suitable tool for handling natural conversations between a user and a computer, not only because of its ability to detect the intent of its user, but also because it can trigger actions based on the specific user's intent. The key tools for the development of our system are Rasa for natural language understanding, Python for scripting custom actions and Neo4j for efficient data storage and retrieval. In this research work, we have used a university application domain moreover our approach can be used in other problem domains as well. So, an advantage of our approach is its independence from the application domain. Our approach can be used in other languages apart the Greek one which has been used as test language. So, another advantage is the language independence of our approach.} }
@inproceedings{10.1145/3734436.3734453, title = {Machine Learning in Access Control: A Taxonomy [Systematization of Knowledge Paper]}, booktitle = {Proceedings of the 30th ACM Symposium on Access Control Models and Technologies}, pages = {145--156}, year = {2025}, isbn = {9798400715037}, doi = {10.1145/3734436.3734453}, url = {https://doi.org/10.1145/3734436.3734453}, author = {Nobi, Mohammad Nur and Gupta, Maanak and Krishnan, Ram and Rana, Md Shohel and Praharaj, Lopamudra and Abdelsalam, Mahmoud}, keywords = {access control, mlbac, machine learning based access control, location = USA}, abstract = {Developing and managing access control systems is challenging due to the dynamic nature of users, resources, and environments. Recent advancements in machine learning (ML) offer promising solutions for automating the extraction of access control attributes, policy mining, verification, and decision-making. Despite these advancements, the application of ML in access control remains fragmented, resulting in an incomplete understanding of best practices. This work aims to systematize the use of ML in access control by identifying key components where ML can address various access control challenges. We propose a novel taxonomy of ML applications within this domain, highlighting current limitations such as the scarcity of public real-world datasets, the complexities of administering ML-based systems, and the opacity of ML model decisions. Additionally, we outline potential future research directions to guide both new and experienced researchers in effectively integrating ML into access control practices.} }
@article{10.1145/3511299, title = {Interpretable Machine Learning: Moving from mythos to diagnostics}, journal = {Queue}, volume = {19}, pages = {28--56}, year = {2022}, issn = {1542-7730}, doi = {10.1145/3511299}, url = {https://doi.org/10.1145/3511299}, author = {Chen, Valerie and Li, Jeffrey and Kim, Joon Sik and Plumb, Gregory and Talwalkar, Ameet}, abstract = {The emergence of machine learning as a society-changing technology in the past decade has triggered concerns about people's inability to understand the reasoning of increasingly complex models. The field of IML (interpretable machine learning) grew out of these concerns, with the goal of empowering various stakeholders to tackle use cases, such as building trust in models, performing model debugging, and generally informing real human decision-making.} }
@inproceedings{10.1145/3716368.3735169, title = {Optimal Device Sequencing and Kernel Assignment for Multiple Heterogeneous Machine Learning Accelerators}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {746--751}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735169}, url = {https://doi.org/10.1145/3716368.3735169}, author = {Bachhav, Tejas and Kerkar, Amol and Rana, Rahul and Madden, Patrick}, keywords = {Machine Learning Accelerators, Kernel Mapping, Heterogeneous Systems, Device Sequencing}, abstract = {Applications utilizing machine learning and artificial intelligence have exploded in popularity in recent years. This has driven massive changes in system designs, with software systems being tightly coupled to the underlying architecture; co-optimization of both pays huge dividends. The new applications and hardware have also driven massive capital infrastructure investments; the compute and power demands for machine learning are skyrocketing.In this paper, we present an optimal algorithmic approach to map machine learning kernel graphs to multiple machine learning accelerators – and in particular, we focus on heterogeneous sets of accelerators. By integrating devices from different technology generations, the useful life span of each device is increased, lowing the overall cost of achieving high performance. We use benchmarks from a recent ISPD contest, and adapt them to consider multiple accelerators with varying performance characteristics.} }
@inbook{10.1145/3729706.3729742, title = {Using Explainable Machine Learning to Predict Loan Risk in Consumer Finance}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {235--238}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729742}, author = {Zhang, Lixin}, abstract = {The high default rate of consumer finance loans poses a significant challenge in predicting credit risk. This study presents a framework for predicting loan risk by leveraging the strengths of machine learning models. Compared to logistic regression models, LightGBM and CatBoost models demonstrate superior predictive performance. Furthermore, model fusion techniques contribute to improved performance over individual approaches. To improve the interpretability of the predictions, we use Shapley Additive Explanations (SHAP), which clarify the impact of each feature on the model's outputs. Key risk factors identified include gender, days past due, income type, late payment percentage, contract start date, total loan payments, and outstanding amount. By combining the strong predictive capabilities of LightGBM and CatBoost with the transparency provided by SHAP, this study aims to build trust in machine learning applications within the consumer finance sector. Our findings offer valuable recommendations for improving credit risk management practices, ultimately contributing to more reliable and transparent decision-making processes in consumer finance loan assessments.} }
@inproceedings{10.1145/3603165.3607365, title = {Causal Inspired Trustworthy Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023}, pages = {3--4}, year = {2023}, isbn = {9798400702334}, doi = {10.1145/3603165.3607365}, url = {https://doi.org/10.1145/3603165.3607365}, author = {Kuang, Kun}, abstract = {In causality-based trustworthy machine learning, finding mechanisms from data-driven correlation analysis to causal inference and constructing a machine learning framework from correlation-driven to causality-driven are two significant challenges. To address these challenges, we propose a series of innovations, including data-driven causal inference mechanisms, causality-inspired interpretable and stable learning frameworks, causality-based generalizable graph neural network learning frameworks, and other fundamental theories and key technologies. To further support the development of the field, we make the corresponding codes and resources public in the open-source community, including the big data causal inference framework based on instrumental variables (https://github.com/causal-machine-learning-lab/mliv) and the large-scale graph neural network computing and edge-cloud collaborative learning platform (https://github.com/luoxi-model/luoxi_models).} }
@inproceedings{10.1145/3735014.3735871, title = {Machine Learning Application in Stock Portfolio Optimization and Price Prediction}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {40--47}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735871}, url = {https://doi.org/10.1145/3735014.3735871}, author = {Zhou, Yi and Chen, Xiaoxin and Kan, Pu and Wu, Meng and Zhao, Zhenru}, keywords = {LSTM Model, Machine Learning, Portfolio, Price Prediction, SVM Model, XGBoost Model}, abstract = {This article selects the top ten stocks with the highest returns in the CSI A50 index, and first uses the wavelet denoising method to smooth the original stock price data. Compared with VaR, this article uses CVaR to reduce the potential impact of extreme events, which can better reflect tail risk. Based on this, a high return, low-risk investment portfolio was constructed, and three machine learning models LSTM, SVM, and XGBoost were used to predict future stock prices. After comparative analysis, the XGBoost model with the highest prediction accuracy was selected as the final model. The research findings can provide investors with a new investment strategy by combining wavelet analysis, risk management, and machine learning prediction to optimize portfolio construction and predict future returns.} }
@article{10.1145/3716818, title = {Facial Expression Analysis in Parkinsons’s Disease Using Machine Learning: A Review}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3716818}, url = {https://doi.org/10.1145/3716818}, author = {Oliveira, Guilherme and Ngo, Quoc and Passos, Leandro and Jodas, Danilo and Papa, Joao and Kumar, Dinesh}, keywords = {Neurological disorders, hypomimia, Parkinson’s disease, facial expression, artificial intelligence, machine learning, deep learning}, abstract = {Computerised facial expression analysis is performed for a range of social and commercial applications and more recently its potential in medicine such as to detect Parkinson’s Disease (PD) is emerging. This has possibilities for use in telehealth and population screening. The advancement of facial expression analysis using machine learning is relatively recent, with a majority of the published work being post-2019. We have performed a systematic review of the English-based publication on the topic from 2019 to 2024 to capture the trends and identify research opportunities that will facilitate the translation of this technology for recognising Parkinson’s disease. The review shows significant advancements in the field, with facial expressions emerging as a potential biomarker for PD. Different machine learning models, from shallow to deep learning, could detect PD faces. However, the main limitation is the reliance on limited datasets. Furthermore, while significant progress has been made, model generalization must be tested before clinical applications.} }
@inproceedings{10.1145/3716554.3716616, title = {Machine Learning-Based Geometric Interpolation for Fluid-Flow Modelling}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {407--412}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716616}, url = {https://doi.org/10.1145/3716554.3716616}, author = {Aravanis, Theofanis and Chrimatopoulos, Grigorios and Xenos, Michalis and Tzirtzilakis, Efstratios}, keywords = {Computational Fluid Dynamics, Two-Dimensional Fluid Flow, Artificial Neural Networks, Machine Learning}, abstract = {Modelling fluid flows accurately is essential in engineering and science, but it often depends on solving complex Partial Differential Equations (PDEs), which can be computationally demanding. In this study, we leverage Artificial Neural Networks (ANNs) as efficient surrogates for traditional numerical methods, predicting fluid-flow characteristics with significantly reduced computational cost. Building on our recent previous work, we extend our analysis to explore geometric interpolation within a two-dimensional (2D) channel. While we investigate interpolation with respect to the geometry of the fluid-flow problem, we explore extrapolation in terms of Reynolds numbers, thereby validating our Machine-Learning model on fluid flows with Reynolds numbers beyond the training range. The obtained results demonstrate that while training on finer grids yields high accuracy, training on coarser grids still achieves comparable performance with minimal computational effort; this outcome highlights in turn the promise of ANNs for efficient modelling of fluid dynamics. Such capabilities are particularly valuable in contemporary applications such as digital twins, enabling real-time system monitoring and optimization.} }
@inproceedings{10.1145/3640824.3640839, title = {Artificial Aesthetics: Bridging Neuroaesthetics and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Control Engineering and Artificial Intelligence}, pages = {98--101}, year = {2024}, isbn = {9798400707971}, doi = {10.1145/3640824.3640839}, url = {https://doi.org/10.1145/3640824.3640839}, author = {Liang, Ting-Wen and Lau, Bee Theng and White, David and Barron, Deirdre}, keywords = {Aesthetic preference, Deep Learning, Design Object, Machine Learning, Neural Network algorithm, location = Shanghai, China}, abstract = {This paper presents an innovative exploration of neuroscience, aesthetics, and artificial intelligence. This paper discusses the potential of machine learning in enhancing our understanding of the neural underpinnings of aesthetic experiences and artistic creation. Neuroaesthetics seeks to unravel the cerebral processes involved in art perception and emotional engagement. Integrating these insights with the capabilities of advanced ML models, particularly those inspired by human brain architecture, opens new avenues for analyzing and generating art. This interdisciplinary approach leverages neural network algorithms to mimic and extrapolate human aesthetic preferences and interpretations. Our research employs deep machine learning techniques to analyze electroencephalogram data, categorized based on aesthetic preferences. The datasets from prior research studies provide data for examining the neural correlates of aesthetic judgment and experience. By leveraging machine learning algorithms, we uncover intricate patterns within the EEG readings that correlate with participants' aesthetic preferences, thereby deepening our understanding of the neural mechanisms underlying aesthetic appreciation for design objects.} }
@inproceedings{10.1145/3638530.3654405, title = {Benchmark Problems for Machine Learning in Control Synthesis}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {2123--2126}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3654405}, url = {https://doi.org/10.1145/3638530.3654405}, author = {Shmalko, Elizaveta and Diveev, Askhat}, keywords = {evolutionary machine learning, symbolic regression, control, unsupervised learning, location = Melbourne, VIC, Australia}, abstract = {Symbolic regression methods are becoming increasingly popular and they are a worthy competitor to neural networks in many tasks. However, symbolic regression methods have the greatest promise in those problems where there is no training sample and it is necessary to search for a solution to a machine learning problem only on the basis of some given general quality criterion. A wide class of such problems includes control synthesis problems. Machine learning control tasks consist in searching for a vector- function of control according to some criterion. As a rule, there is no training sample for such tasks, except to approximate the operator's actions. However, this approach is unlikely to satisfy the given criterion. Some symbolic regression methods have already shown good results in control problems. But the complexity of the tasks reveals the need to develop new approaches and modifications of existing methods. In this regard, a benchmark of machine learning control problems is proposed to test symbolic regression methods. Problem formulations and reference solutions are presented.} }
@inproceedings{10.1145/3747227.3747275, title = {Development of a Predictive Model for Hydropower Capacity in Jiangsu Province Utilizing Machine Learning Techniques}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {307--310}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747275}, url = {https://doi.org/10.1145/3747227.3747275}, author = {Zhao, Shuya}, keywords = {Hydropower Generation, Jiangsu Province, XGBoost Model}, abstract = {The strategic importance and contradictions of hydropower development have become evident. As an economic powerhouse, Jiangsu Province's energy demand continues to grow, and hydropower, as a clean energy source, holds a significant position under the "dual carbon" goals. In recent years, machine learning has gradually matured in water resource management, but its potential in predicting hydropower capacity has yet to be fully realized. The variables affecting hydropower generation are investigated, and the seasonal fluctuations and long-term trends of hydropower capacity (such as the difference between wet and dry seasons) are analyzed based on the time series data from 2005 to 2024. This provides a basis for dynamically adjusting power generation strategies and fully exploring the potential of XGBoost in hydropower capacity prediction. A systematic approach using the XGBoost model is adopted to reduce the dimension of data and identify the crucial factors that impact water resources. XGBoost model is evaluated using a dataset on water resource management from Jiangsu Province. Accuracy metrics are used to gauge the performance of the XGBoost model. Prediction accuracy achieved by the XGBoost model's training is 83.3\%, with further potential to optimize extended works. However, 100\% accuracy was obtained model's testing. This approach facilitates a balanced water ecology sheet by integrating environmental and economic considerations. XGBoost model identifies total water resources. The systematic machine learning approach helps policymakers in deciding the appropriate way of managing water resources, and infrastructures. Sustainable resource management techniques support decision-making through data-driven insights.} }
@inproceedings{10.1145/3698205.3733941, title = {Digital Cognitive Apprenticeship: Scaling Rigorous Machine Learning Education in High Schools}, booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale}, pages = {286--290}, year = {2025}, isbn = {9798400712913}, doi = {10.1145/3698205.3733941}, url = {https://doi.org/10.1145/3698205.3733941}, author = {Perach, Shai and Alexandron, Giora}, keywords = {blended learning, cdlr-centered framework, cognitive apprenticeship, digital learning resources, educational scaling, high school stem, machine learning education, professional content integration, teacher capacity, location = Palermo, Italy}, abstract = {Current initiatives to introduce machine learning (ML) in high schools typically rely on simplified content tthat avoids mathematical foundations, creating a significant gap between secondary and advanced ML education. We present a fundamentally different approach to scaling rigorous ML education: a blended learning framework enabling advanced STEM track students to learn directly from professional ML resources while addressing the critical challenge of teaching capacity limitations. Our framework strategically positions prominent Contemporary Digital Learning Resources (CDLRs) --- high-quality resources created by professionals for adult learners --- at the center of instruction, redistributing educational responsibilities among teachers, students, and digital resources to transform classroom dynamics. Our framework addresses a critical scaling challenge in ML education: the shortage of teachers with ML expertise. By drawing on Cognitive Apprenticeship, Community of Inquiry, and Self-Efficacy theories, we create structured learning environments that potentially extend apprenticeship models beyond direct expert-novice relationships. Initial implementation across six high schools shows promising results, with external evaluation revealing substantial student achievement and high completion rates. Student data indicates a consistent preference for professional resources over simplified alternatives, suggesting the value of authentic disciplinary engagement for such science-track students when adequately supported. Our ongoing work focuses on developing empirical measures to evaluate how effectively digital resources implement cognitive apprenticeship elements and identifying critical factors affecting successful implementation. Our work contributes to theoretical understanding of cognitive apprenticeship in digital environments and provides empirical evidence for redistributing educational roles around professional learning resources, with implications that extend beyond ML education to other rapidly evolving technical fields where traditional teacher preparation may prove insufficient.} }
@article{10.1145/3640313, title = {Machine Learning for Refining Knowledge Graphs: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3640313}, url = {https://doi.org/10.1145/3640313}, author = {Subagdja, Budhitama and Shanthoshigaa, D. and Wang, Zhaoxia and Tan, Ah-Hwee}, keywords = {Knowledge graphs, knowledge graph refinement}, abstract = {Knowledge graph (KG) refinement refers to the process of filling in missing information, removing redundancies, and resolving inconsistencies in KGs. With the growing popularity of KG in various domains, many techniques involving machine learning have been applied, but there is no survey dedicated to machine learning-based KG refinement yet. Based on a novel framework following the KG refinement process, this article presents a survey of machine learning approaches to KG refinement according to the kind of operations in KG refinement, the training datasets, mode of learning, and process multiplicity. Furthermore, the survey aims to provide broad practical insights into the development of fully automated KG refinement.} }
@inproceedings{10.1145/3600211.3604689, title = {Machine Learning practices and infrastructures}, booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {466--481}, year = {2023}, isbn = {9798400702310}, doi = {10.1145/3600211.3604689}, url = {https://doi.org/10.1145/3600211.3604689}, author = {Berman, Glen}, keywords = {infrastructure studies, machine learning, social practice, location = Montr\'eal, QC, Canada}, abstract = {Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers’ have demonstrated to be particularly salient for the societal impact of deployed ML systems.} }
@inproceedings{10.1145/3642970.3655828, title = {The Environmental Cost of Engineering Machine Learning-Enabled Systems: A Mapping Study}, booktitle = {Proceedings of the 4th Workshop on Machine Learning and Systems}, pages = {200--207}, year = {2024}, isbn = {9798400705410}, doi = {10.1145/3642970.3655828}, url = {https://doi.org/10.1145/3642970.3655828}, author = {Chadli, Kouider and Botterweck, Goetz and Saber, Takfarinas}, keywords = {DevOps, Environmental Cost, MLOps, Machine Learning-Enabled Systems, Sustainability, location = Athens, Greece}, abstract = {The integration of Machine Learning (ML) across public and industrial sectors has become widespread, posing unique challenges in comparison to conventional software development methods throughout the lifecycle of ML-Enabled Systems. Particularly, with the rising importance of ML platforms in software operations and the computational power associated with their frequent training, testing, and retraining, there is a growing concern about the sustainability of DevOps practices in the context of Al-enabled software. Despite the increasing interest in this domain, a comprehensive overview that offers a holistic perspective on research related to sustainable AI is currently lacking. This paper addresses this gap by presenting a Systematic Mapping Study that thoroughly examines techniques, tools, and lessons learned to assess and promote environmental sustainability in MLOps practices for ML-Enabled Systems.} }
@inproceedings{10.1145/3643651.3659891, title = {Machine Learning Training on Encrypted Data with TFHE}, booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics}, pages = {71--76}, year = {2024}, isbn = {9798400705564}, doi = {10.1145/3643651.3659891}, url = {https://doi.org/10.1145/3643651.3659891}, author = {Montero, Luis and Frery, Jordan and Kherfallah, Celia and Bredehoft, Roman and Stoian, Andrei}, keywords = {homomorphic encryption, machine learning, quantization, location = Porto, Portugal}, abstract = {We present an approach for outsourcing the training of machine learning (ML) models while preserving data confidentiality from malicious parties. We use fully homomorphic encryption (FHE) to build a unified training framework that works on encrypted data and learns quantized ML models. Our approach finds future applications in collaborative settings involving multiple parties working on confidential data, which can be horizontally or vertically split between data owners. We train logistic regression and multi-layer perceptrons on several datasets and show results that are comparable to the state-of-the-art.} }
@inproceedings{10.1145/3719159.3721223, title = {Towards A Modular End-To-End Machine Learning Benchmarking Framework}, booktitle = {Proceedings of the 3rd International Workshop on Testing Distributed Internet of Things Systems}, pages = {23--26}, year = {2025}, isbn = {9798400715266}, doi = {10.1145/3719159.3721223}, url = {https://doi.org/10.1145/3719159.3721223}, author = {Bayer, Robert and Robroek, Ties and T\"oz\"un, Pinar}, keywords = {Benchmarking, Deep Learning, Edge Computing, location = Rotterdam, Netherlands}, abstract = {Machine learning (ML) benchmarks are crucial for evaluating the performance, efficiency, and scalability of ML systems, especially as the adoption of complex ML pipelines, such as retrieval-augmented generation (RAG), continues to grow. These pipelines introduce intricate execution graphs that require more advanced benchmarking approaches. Additionally, collocating workloads can improve resource efficiency but may introduce contention challenges that must be carefully managed. Detailed insights into resource utilization are necessary for effective collocation and optimized edge deployments. However, existing benchmarking frameworks often fail to capture these critical aspects.We introduce a modular end-to-end ML benchmarking framework designed to address these gaps. Our framework emphasizes modularity and reusability by enabling reusable pipeline stages, facilitating flexible benchmarking across diverse ML workflows. It supports complex workloads and measures their end-to-end performance. The workloads can be collocated, with the framework providing insights into resource utilization and contention between the concurrent workloads.} }
@inproceedings{10.1145/3715275.3732033, title = {Identities are not Interchangeable: The Problem of Overgeneralization in Fair Machine Learning}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {485--497}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732033}, url = {https://doi.org/10.1145/3715275.3732033}, author = {Wang, Angelina}, keywords = {machine learning fairness, discrimination, context specificity, social identities}, abstract = {A key value proposition of machine learning is generalizability: the same methods and model architecture should be able to work across different domains and different contexts. While powerful, this generalization can sometimes go too far, and miss the importance of the specifics. In this work, we look at how fair machine learning has often treated as interchangeable the identity axis along which discrimination occurs. In other words, racism is measured and mitigated the same way as sexism, as ableism, as ageism. Disciplines outside of computer science have pointed out both the similarities and differences between these different forms of oppression, and in this work we draw out the implications for fair machine learning. While certainly not all aspects of fair machine learning need to be tailored to the specific form of oppression, there is a pressing need for greater attention to such specificity than is currently evident. Ultimately, context specificity can deepen our understanding of how to build more fair systems, widen our scope to include currently overlooked harms, and, almost paradoxically, also help to narrow our scope and counter the fear of an infinite number of group-specific methods of analysis.} }
@inproceedings{10.1145/3658617.3703638, title = {Invited Paper: Boosting Standard Cell Library Characterization with Machine Learning}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {385--391}, year = {2025}, isbn = {9798400706356}, doi = {10.1145/3658617.3703638}, url = {https://doi.org/10.1145/3658617.3703638}, author = {Chen, Zhengrui and Guo, Chengjun and Song, Zixuan and Feng, Guozhu and Wang, Shizhang and Zhang, Li and Yin, Xunzhao and Wu, Zhenhua and Yan, Zheyu and Zhuo, Cheng}, keywords = {standard cell, library characterization, machine learning, location = Tokyo, Japan}, abstract = {As VLSI designs grow more complex and transition to smaller process nodes, accurate and efficient library characterization has become increasingly crucial within DTCO and STCO flows. Current open-source tools, however, are constrained to basic library characterization functions and fail to adequately meet modern design demands. In this paper, we review the existing open-source standard cell characterization tools, summarize their limitations, and introduce ZlibBoost---a new open-source framework designed to offer both flexibility and efficiency. We leverage ZlibBoost for LUT index optimization, dynamic power supply noise modeling, and machine learning-based prediction to enhance efficiency and accuracy in library characterization. Experimental results show that such a tool is helpful for both academia and industry to effectively navigate DTCO and STCO challenges.} }
@inproceedings{10.1145/3643834.3660688, title = {A Steampunk Critique of Machine Learning Acceleration}, booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference}, pages = {246--257}, year = {2024}, isbn = {9798400705830}, doi = {10.1145/3643834.3660688}, url = {https://doi.org/10.1145/3643834.3660688}, author = {Cremaschi, Michele and Dorfmann, Max and De Angeli, Antonella}, keywords = {Acceleration, Interactive Art, Making, Repurposing of Outdated Technology, Semiotic, Slow Technology, location = Copenhagen, Denmark}, abstract = {The application of Machine Learning is driven by the techno–capitalist struggle for productivity across various domains, including the creative industry. Sociological research has demonstrated how technology–induced temporality introduces challenges at the individual and societal levels. Art creativity conflicts with speed and mass production. This paper describes Isotta, a critical artefact combining a Mignon typewriter and a Language Model to spark discussion about ML–induced acceleration. Fourteen artists evaluated Isotta in an interview study, and semiotics was used as the analytical lens. Results exposed ideological assumptions around the consequences of technology in the writing realm. We discuss these insights in the context of interactive design in times of techno–capitalistic acceleration. Our findings highlight the significance of temporal factors in designing generative writing interactions and underscore how complex societal challenges can be approached in design through the contrast–eliciting property that outdated technologies offer when juxtaposed with contemporary technologies.} }
@inproceedings{10.1145/3732801.3732807, title = {Machine Learning-Based Academic Performance Prediction: A Comparative Empirical Analysis}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {27--32}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732807}, url = {https://doi.org/10.1145/3732801.3732807}, author = {Wu, Meng and Wei, Yi and Zhang, Yirong and Li, Cailing and Mei, Yelin and Subramaniam, Geetha}, keywords = {Academic Performance, AdaBoost Algorithm, Experimental Design, Machine Learning, Prediction Algorithm}, abstract = {In this study, we explore the role of various machine learning algorithms in predicting student academic performance from online behavior data while overcoming some limitations posed by traditional subjective assessments. The Decision Tree, Plain Bayes and AdaBoost algorithms were run against the dataset developed from hundreds of observations collected from software engineering students. From the evaluation, it is concluded that prediction accuracy using AdaBoost was 73\% which is improved significantly compared to other models. Results thus created give educators some data-driven tools for the early identification of academic risk and optimization of teaching strategies, which extend into a scalable framework for other disciplines in support of improved educational decision-making for successful student outcomes.} }
@inproceedings{10.1145/3765325.3765329, title = {Exploring the Impact of Educational Factors on Career Success through Machine Learning}, booktitle = {Proceedings of the 2025 3rd International Conference on Educational Knowledge and Informatization}, pages = {14--19}, year = {2025}, isbn = {9798400715846}, doi = {10.1145/3765325.3765329}, url = {https://doi.org/10.1145/3765325.3765329}, author = {Sun, Kaiyue and Zhen, Tianjiao}, keywords = {Career Success, Education Factors, Graduate Employability, Machine Learning}, abstract = {The academic performance and skill acquisition of students during university are critical to their career success after graduation. Understanding how different educational factors influence career outcomes is very important, as it enables institutions to design more effective educational strategies and help students navigate their paths toward better employment. However, the influence of diverse educational factors on career outcomes is often interdependent with complex interactions, which conventional data statistical techniques may fail to model. To solve this problem, we train machine learning models to predict career outcomes based on several educational factors, thereby quantifying the relative importance of different academic performance and experiential skills. The work in this paper not only facilitates the understanding of the multifaceted relationship between education and career development but also informs the design of more effective educational strategies to improve graduate employability.} }
@inproceedings{10.1145/3648115.3648123, title = {Accelerating Machine Learning Inference on GPUs with SYCL}, booktitle = {Proceedings of the 12th International Workshop on OpenCL and SYCL}, year = {2024}, isbn = {9798400717901}, doi = {10.1145/3648115.3648123}, url = {https://doi.org/10.1145/3648115.3648123}, author = {Panagou, Ioanna-Maria and Bellas, Nikolaos and Moneta, Lorenzo and Sengupta, Sanjiban}, keywords = {Deep Learning, GPU, Machine Learning, Parallel Computing, SYCL, location = Chicago, IL, USA}, abstract = {Recently, machine learning has established itself as a valuable tool for researchers to analyze their data and draw conclusions in various scientific fields, such as High Energy Physics (HEP). Commonly used machine learning libraries, such as Keras and PyTorch, might provide functionality for inference, but they only support their own models and are constrained by heavy dependencies, which render their deployment on embedded or bare-metal environments infeasible. SOFIE\&nbsp;[3], which stands for System for Optimized Fast Inference code Emit, a part of the ROOT project developed at CERN, creates standalone C++ inference code from an input model in one of the popular machine learning formats. This code is directly invokable from other C++ projects and has minimal dependencies. In this work, we extend the functionality of SOFIE to generate SYCL code for machine learning model inference that can run on various GPU platforms and is only dependent on Intel MKL BLAS and portBLAS libraries.} }
@article{10.1145/3757699, title = {Venire: A Machine Learning-Guided Panel Review System for Community Content Moderation}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3757699}, url = {https://doi.org/10.1145/3757699}, author = {Koshy, Vinay and Choi, Frederick and Chiang, Yi-Shyuan and Sundaram, Hari and Chandrasekharan, Eshwar and Karahalios, Karrie}, keywords = {content moderation, decision-making, human-AI interaction, online communities}, abstract = {Research into community content moderation often assumes that moderation teams govern with a single, unified voice. However, recent work has found that moderators disagree with one another at modest, but concerning rates. The problem is not the root disagreements themselves. Subjectivity in moderation is unavoidable, and there are clear benefits to including diverse perspectives within a moderation team. Instead, the crux of the issue is that, due to resource constraints, moderation decisions end up being made by individual decision-makers. The result is decision-making that is inconsistent, which is frustrating for community members. To address this, we develop Venire, an ML-backed system for panel review on Reddit. Venire uses a machine learning model trained on log data to identify the cases where moderators are most likely to disagree. Venire fast-tracks these cases for multi-person review. Ideally, Venire allows moderators to surface and resolve disagreements that would have otherwise gone unnoticed. We conduct three studies through which we design and evaluate Venire: a set of formative interviews with moderators, technical evaluations on two datasets, and a think-aloud study in which moderators used Venire to make decisions on real moderation cases. Quantitatively, we demonstrate that Venire is able to improve decision consistency and surface latent disagreements. Qualitatively, we find that Venire helps moderators resolve difficult moderation cases more confidently. Venire represents a novel paradigm for human-AI content moderation, and shifts the conversation from replacing human decision-making to supporting it.} }
@proceedings{10.1145/3735654, title = {DEEM '25: Proceedings of the Workshop on Data Management for End-to-End Machine Learning}, year = {2025}, isbn = {9798400719240} }
@inproceedings{10.1145/3724154.3724247, title = {Bankruptcy Prediction of Listed Companies Based on Machine Learning Model}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {563--567}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724247}, url = {https://doi.org/10.1145/3724154.3724247}, author = {Zhang, Rongqiang and Cao, Qilong and Wei, Haohao and Sun, Xin}, keywords = {Corporate bankruptcy risk assessment, Machine learning, XGBoost}, abstract = {Machine learning has found extensive applications in the field of financial research, particularly in corporate finance. This study applies machine learning algorithms to predict corporate bankruptcy risk, employing the XGBoost algorithm to develop a predictive model for listed companies. The model's predictive outcomes are compared with those of logistic regression and random forest algorithms. Results demonstrate that XGBoost outperforms the alternatives in predicting bankruptcy risk, exhibiting significantly superior evaluation metrics. Further analysis identifies net cash flow from financing activities, shareholders' equity turnover, tangible net worth debt ratio, operating profit growth rate, and operating profit ratio as critical factors influencing bankruptcy risk.} }
@inproceedings{10.1145/3689236.3696268, title = {The Application of Machine Learning in Spam Filtering}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {258--262}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3696268}, url = {https://doi.org/10.1145/3689236.3696268}, author = {Liu, Xiangwei and Tang, Liang}, keywords = {feature extraction, machine learning method, natural language processing, python, spam filtering}, abstract = {With the wide application of the Internet, Spam is increasingly rampant, which seriously affects the normal communication and network environment of users. Aiming at this problem, this paper deeply studies the related technology of spam filter. Through the analysis of a large number of spam samples, a variety of effective features are extracted, such as keywords, word frequency, sentence structure and so on. Machine learning algorithms, such as logistic and naive Bayes, are used to construct efficient classification models. At the same time, the model is optimized and improved to improve the understanding and recognition accuracy of semantics. The experimental results show that the proposed filter has significant improvement in accuracy, recall rate and F1 value, which can effectively filter spam and provide users with a cleaner email environment, and has important practical application value.} }
@article{10.1145/3761827, title = {Detecting Smart Ponzi Schemes on Blockchain using Machine Learning: A Comprehensive Survey}, journal = {Distrib. Ledger Technol.}, year = {2025}, doi = {10.1145/3761827}, url = {https://doi.org/10.1145/3761827}, author = {Kumar, Dheeraj and Palaniswami, Marimuthu and Muthukkumarasamy, Vallipuram}, keywords = {Smart Ponzi schemes, Cryptocurrencies, Bitcoin, Ponzi smart contract, Ethereum}, abstract = {Ponzi schemes, a more than a century-old fraud, have recently infiltrated blockchain-based cryptocurrency domain led by an explosion of such schemes in two most popular cryptocurrencies: Bitcoin and Ethereum. On these two platforms alone, the perpetrators of these frauds have fleeced gullible investors of billions of dollars annually. Smart Ponzi schemes are a hazard to these cryptocurrency ecosystems, diminishing investor confidence in these cutting-edge technologies, threatening their integrity, and hindering their growth and broader adaptation. These smart Ponzi schemes have also created a nightmare for law enforcement as tracking and taking countermeasures against fraudsters and recovering the victims’ investment is challenging. Over the years, researchers have utilized significant advances in machine learning and artificial intelligence to detect and promptly caution users against investing in Ponzi schemes on Bitcoin and Ethereum. However, this research still exists in silos, and there is a lack of a detailed survey paper critically analyzing various aspects of the approaches focusing on the menace of smart Ponzi schemes. This paper surveys the state-of-the-art techniques proposed in the literature to detect smart Ponzi schemes on two popular blockchain platforms: Bitcoin and Ethereum. We list, categorize, and discuss papers that contributed benchmark datasets, developed novel features concerning various aspects of smart Ponzi schemes, and proposed novel machine-learning approaches to detect them.} }
@inproceedings{10.1145/3600100.3626271, title = {Thermal Preference Prediction with Machine Learning}, booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {303--304}, year = {2023}, isbn = {9798400702303}, doi = {10.1145/3600100.3626271}, url = {https://doi.org/10.1145/3600100.3626271}, author = {Odeyemi, Julianah and Streblow, Rita}, keywords = {Longitudinal dataset., Machine Learning, Personal Comfort Model, Thermal Preference, location = Istanbul, Turkey}, abstract = {This study aimed to develop a personal comfort model (PCM) using machine learning techniques on an open-access longitudinal dataset. Most prior studies on PCMs were based on controlled climate chamber data, which limits their generalisability to real-world settings. The study examined individual and ensemble classifiers for predicting thermal preference. The Support Vector Classifier (SVC) displayed strong predictive power with an accuracy of 0.843, as well as macro precision (0.724), recall (0.847), and F1 score (0.763). Similarly, the Extra Trees (ET) classifier achieved the highest accuracy of 0.924, with macro precision (0.865), recall (0.915), and F1 score (0.887). Overall, ensemble methods such as Extra Trees, Extreme Gradient Boost, and SVC as an individual model proved effective in predicting thermal preference.} }
@inproceedings{10.1145/3748825.3748867, title = {Application Research on Employee Turnover Prediction Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {256--262}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748867}, url = {https://doi.org/10.1145/3748825.3748867}, author = {Liang, Zhiying and Jin, Aitong and Cai, Yongheng and Gan, Haohong and Liu, Zicong and Wang, Shu}, keywords = {Employee Turnover Prediction, Machine Learning Algorithms}, abstract = {This study examines how employee turnover hinders business operations, strategy execution, and cost-efficiency, while management talent loss impacts long-term growth. In the big data era, talent retention poses a challenge. Using machine learning algorithms, we aim to predict turnover more accurately, aid career planning, and strengthen corporate culture. Through "IBM Human Resources dataset"t, we build single (e.g., Decision Tree) and ensemble (Random Forest, AdaBoost, XGBoost) models to analyze turnover factors and propose solutions for talent retention, resource optimization, and improved incentives.} }
@inproceedings{10.1145/3662739.3662743, title = {Machine learning-based seismic prediction of building structures}, booktitle = {Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications}, pages = {256--261}, year = {2024}, isbn = {9798400718144}, doi = {10.1145/3662739.3662743}, url = {https://doi.org/10.1145/3662739.3662743}, author = {Liu, Shuai and Peng, Hailiang and Deng, Xiaolu}, keywords = {building structure, earthquake, machine learning, neural networks, seismicity prediction, location = Ningbo, China}, abstract = {The impact of earthquakes on building structures is the most direct and significant aspect of earthquake disasters, and the seismic resistance of building structures is a key factor in ensuring human safety and reducing economic losses. Ground shaking caused by earthquakes can create strong impact and vibration, which may lead to building collapse, damage, or deformation. In strong earthquakes, many buildings can suffer severe damage, such as complete collapse of the structure, cracking of walls, bending of columns, sagging of beams, and pose great danger to people. With the rapid development of modern computer technology in terms of hardware and software, the earthquake damage prediction methods and earthquake damage management of buildings have entered a brand new period. In this paper, classical machine learning methods such as decision tree, bagging tree, boosting tree, BP neural network and generalized regression network are used to construct prediction models and evaluate their prediction effects respectively. Combining the principles of different machine learning methods, the shortcomings of different models in small sample data training applications are analyzed, and the generalized regression network is further proposed. The prediction results show that the maximum values of seismic coefficients for buildings with rectangular cross-sections of different aspect ratios occur around an aspect ratio of 2, which is slightly larger than the normative value suggested by 1.4. The value stabilizes around 1.35 when the aspect ratio is greater than 4. The results can be used as a reference for similar structures The results can be used as a reference for the seismic design of similar structures.} }
@inproceedings{10.1145/3711129.3711341, title = {Integrated Machine Learning for Enhanced Supply Chain Risk Prediction}, booktitle = {Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering}, pages = {1254--1259}, year = {2025}, isbn = {9798400710094}, doi = {10.1145/3711129.3711341}, url = {https://doi.org/10.1145/3711129.3711341}, author = {Jin, Tian}, keywords = {Supply chain risk, machine learning, integrated model, data preprocessing, prediction accuracy}, abstract = {Supply chain risk prediction has become increasingly critical as organizations navigate complex and volatile environments characterized by rapid market changes, geopolitical uncertainties, and supply disruptions. In this context, effective risk management is essential for maintaining operational efficiency and competitiveness. This study proposes an innovative integrated model that combines Random Forest, Gradient Boosting Machine (GBM), and Neural Networks to enhance prediction accuracy and reliability in supply chain risk assessment. By employing comprehensive data preprocessing techniques—such as missing value imputation, normalization, and anomaly detection—alongside advanced algorithmic strategies, the model effectively addresses the limitations of traditional approaches. The integration of these diverse machine learning techniques not only leverages their individual strengths but also enhances the model’s adaptability and robustness in varying scenarios.} }
@inproceedings{10.1145/3747227.3747258, title = {Research on the Application of Machine Learning in the Cost Control of Enterprise R\&amp;D Projects}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {183--188}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747258}, url = {https://doi.org/10.1145/3747227.3747258}, author = {Liu, Xiu}, keywords = {BP neural network, Machine learning, predictive analysis model, project cost control}, abstract = {Under the background of global economic integration and increasingly fierce market competition, enterprises are facing enormous cost pressure. Among them, R\&amp;D project is the core driving force of enterprise innovation and development, and its cost control effect is directly related to the competitiveness and profitability of enterprises. However, in the face of complex and changeable market environment and technical conditions, the traditional cost control methods of R\&amp;D projects show insurmountable limitations and can no longer meet the practical application needs of enterprises. In this regard, based on the current problems, this paper deeply analyzes the application feasibility of artificial intelligence and machine learning technology in this field, and puts forward a brand-new cost control framework for enterprise R\&amp;D projects to improve the accuracy and efficiency of cost control. Practice has proved that this framework is based on BP neural network, and through learning and analyzing historical project cost data, it can obtain hidden rules and patterns in the data, provide more accurate prediction and decision support for enterprise's subsequent R\&amp;D project cost control, and enhance the flexibility and response speed of cost control.} }
@inproceedings{10.1145/3750069.3757879, title = {Resilient AI Framework for Secure and Ethical Machine Learning Systems (RAISE) - 1st edition}, booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter}, year = {2025}, isbn = {9798400721021}, doi = {10.1145/3750069.3757879}, url = {https://doi.org/10.1145/3750069.3757879}, author = {Breve, Bernardo and Caruccio, Loredana and Polese, Giuseppe}, keywords = {Resilient AI, Human-Centered AI, Fairness and Accountability, Data Quality and Profiling, Verification and Validation}, abstract = {This document provides a summary of the First Workshop on ‘Resilient AI Framework for Secure and Ethical Machine Learning Systems’ (RAISE 2025), which has been accepted at the 6th Biannual Conference of the Italian SIGCHI Chapter (CHItaly 2025).} }
@inproceedings{10.1145/3716368.3735230, title = {An Efficient Distributed Machine Learning Inference Framework with Byzantine Fault Detection}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {56--63}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735230}, url = {https://doi.org/10.1145/3716368.3735230}, author = {Zhou, Xuan and Mohan, Utkarsh and Liu, Yao and Beerel, Peter}, keywords = {distributed inference, security, machine learning inference security, Byzantine fault, fault detection}, abstract = {The gap between the complexity of the most advanced machine learning (ML) models, e.g., the large language model (LLMs), PaLM, with 540 billion parameters, and what hardware resources at the edge can support is growing. Two approaches to mitigate this gap are leveraging cloud-based ML servers, which introduce widely studied security, privacy, and reliability risks, and distributed inference, in which several local edge-based devices share the computational burden. Motivated by the fact that the security of the distributed inference approach has received far less attention, this paper proposes a low-cost and versatile scheme to add redundancy to distributed inference to mitigate compromised devices that can exhibit faulty or malicious behavior modeled as Byzantine faults. We mathematically derive the number of inferences required to detect the attack as a function of computation overhead and also develop a simulator. The simulation results on an LLM align closely with the theoretical values. Specifically, with a redundancy overhead of 10\% and 4 out of 8 devices in a single layer compromised, the average number of inferences required to detect all malicious devices is only 39.42.} }
@inproceedings{10.1145/3746709.3746883, title = {Research on Tomato Leaf Disease Classification Based on Machine Learning}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {1026--1030}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746883}, url = {https://doi.org/10.1145/3746709.3746883}, author = {Yuting, Xia}, keywords = {Disease classification, YOLOv8, deep learning, optimization algorithm}, abstract = {Timely and accurate detection of plant diseases is an ongoing challenge in agriculture. The occurrence of tomato leaf diseases can affect the normal growth of tomatoes, leading to a reduction in quality and yield, resulting in significant economic losses. Traditional manual identification methods are time consuming and labour intensive, while traditional computer vision identification methods lack portability. Conducting research on tomato leaf disease classification based on machine learning can effectively alleviate the shortcomings of manual classification and traditional image classification methods in practical applications. It can improve the classification efficiency and accuracy, reduce the labour cost, realize the timely detection, identification and solution of diseases, reduce the misuse of pesticides, improve the tomato yield, and meet the national demand for green agriculture. Therefore, it is of great significance. Using deep learning technology to classify and identify tomato leaf diseases, we realised the classification of 10 tomato leaf diseases, while improving the classification efficiency and accuracy.} }
@inproceedings{10.1145/3639479.3639512, title = {Experimental design of emotion recognition based on machine learning}, booktitle = {Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing}, pages = {155--160}, year = {2024}, isbn = {9798400709241}, doi = {10.1145/3639479.3639512}, url = {https://doi.org/10.1145/3639479.3639512}, author = {Yu, Jintao and Jiang, Mingze and Liang, Tingwei}, keywords = {Emotion recognition, Experimental design, Machine learning, Physiological signal, location = Sanya, China}, abstract = {In this paper, a wrist bracelet was used to measure the subjects' heart rate signals and skin electrical signals, and three types of emotions (positive, neutral and negative) were induced by designing an experimental environment. The heart rate and skin electrical signals corresponding to emotions were collected through the bracelet, and the recognition accuracy of the three types of emotions was finally obtained through data preprocessing and the support vector machine model in machine learning. At present, how to build an experimental environment that can fully induce subjects' emotions is a major difficulty in emotion recognition, so this paper provides a specific emotion recognition environment design and detailed steps. Meanwhile, through a large number of experiments, the kernel function in the support vector machine model, namely Gaussian kernel function, is determined, and grid search is introduced to search for hyperparameter C. To get the optimal parameters and results. A total of 20 people were measured in this experiment. The experimental results obtained by SVM model were positive: the recognition accuracy of skin electrical signal was 0.8422, the recognition accuracy of heart rate signal was 0.8345, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion was 0.8832. Negative: the recognition accuracy of skin electrical signal is 0.9812, the recognition accuracy of heart rate signal is 0.9385, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion is 0.9902. Neutral: the recognition accuracy of skin electrical signal was 0.6403, the recognition accuracy of heart rate signal was 0.5308, and the recognition accuracy of skin electrical signal and heart rate signal feature fusion was 0.5438} }
@inproceedings{10.1145/3757110.3757150, title = {Urban Tourism Hotel Recommendation Model Based on Geographic Spatial Machine Learning Algorithm and Spatial Route Planning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {236--241}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757150}, url = {https://doi.org/10.1145/3757110.3757150}, author = {Wang, Jingyi and Zhou, Xiao and Xian, Mengling and Pan, Juan}, keywords = {geographic spatial data, machine learning, spatial route planning, urban hotel recommendation}, abstract = {Based on the analysis of the cost of urban traveling and the constrained tourism demands of tourists, we construct an urban tourism hotel recommendation model based on geographic spatial machine learning algorithm and spatial route planning. Firstly, based on the constraints of urban geographic space, an improved DIANA clustering algorithm is constructed to spatially cluster the scenic spots in the city and obtain a local distribution model of the scenic spots. Secondly, taking the scenic spot cluster as the research scope, a spatial route planning algorithm is constructed to output tour routes starting and ending at a certain hotel, and the optimal complete binary tree algorithm is used to output the cost priority of each route, thereby obtaining the hotel recommendation priority. The experiment proves that the proposed algorithm can effectively output the clusters of scenic spots under geographic spatial constraints, and recommend hotel accommodation with lowest traveling cost for tourists within the clustering range, providing the decision support for tourists’ tourism planning.} }
@article{10.1145/3737284, title = {Keeper: Automated Testing and Fixing of Machine Learning Software - RCR Report}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3737284}, url = {https://doi.org/10.1145/3737284}, author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Maire, Michael and Hoffmann, Henry and Lu, Shan}, abstract = {This artifact aims to provide source code, benchmark suite, results, and materials used in our study “Keeper: Automated Testing and Fixing of Machine Learning Software” [3]. We developed an automated testing and fixing tool Keeper and its IDE plugin for ML software. It automatically detects software defects and attempts to change how ML APIs are used to alleviate software misbehavior. This artifact provides guidelines to set up and execute Keeper, and also guidelines to interpret our evaluation results. We hope this artifact can motivate and help future research to further tackle ML API misuses. All related data are available online.} }
@inproceedings{10.1145/3675888.3676055, title = {Interpretable Machine Learning Techniques for Students Grade Prediction}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {213--225}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676055}, url = {https://doi.org/10.1145/3675888.3676055}, author = {Josephine, Namakula and Cedric, Ahumuza and McDaniel, Ssemwanga Trevor and Kanagwa, Benjamin and Joseph, Tibakanya and Marvin, Ggaliwango}, abstract = {Predicting student grades within an academic cycle is a remarkable opportunity to improve academic results. Robust prediction methods such as use of machine learning allow educational leaders to allocate enough resources and personalized instructions where necessary, fostering improved outcomes in Academia. We have presented various machine-learning models that have helped predict students' grades as an early intervention to determine how well the models achieve the required objectives using a combination of input features and past data or grades. We obtained a student dataset with various demographics, economic, educational factors, and grades from different subjects with their corresponding averages where we came up with 15 models including decision trees, random forest, linear regression, k-nearest neighbor, AdaBoost, Gradient Boosting, Support Vector Machine XGBoost, Lasso, Ridge, Elasticnet and among Deep Neural Networks. Our findings show that Linear Regression based model is the best with a MAPE of 8.14 and R-squared of 0.2536, and the Graph neural networks performed worst with a MAPE of 47.2 and R-squared of -83.7. We achieve computational outcomes indicative of the model's predictive performance. Our results show promising accuracy and generalization capabilities and that these models can predict very well, meaning we can rely on them to figure out how students might perform in academia.} }
@inproceedings{10.1145/3712255.3734339, title = {Interpreting Machine Learning Pipelines Produced by Evolutionary AutoML for Biochemical Property Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1944--1952}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734339}, url = {https://doi.org/10.1145/3712255.3734339}, author = {de S\'a, Alex G. C. and Pappa, Gisele L. and Freitas, Alex A. and Ascher, David B.}, keywords = {automated machine learning (AutoML), cheminformatics, drug discovery, AutoML-generated pipeline interpretability, Bayesian networks, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Machine learning (ML) has been playing a crucial role in drug discovery, mainly through quantitative structure-activity relationship models that relate molecular structures to properties, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties. However, traditional ML approaches often lack customisation to a particular biochemical task and fail to generalise to new biochemical spaces, resulting in reduced predictive performance. Automated machine learning (AutoML) has emerged to address these limitations by automatically selecting the suitable ML pipelines for a given input dataset. Despite its potential, AutoML is underutilised in cheminformatics, and its decisions often lack interpretability, reducing user trust - especially among non-experts. Accordingly, this paper proposes an evolutionary AutoML method for biochemical property prediction that outputs an interpretable model for understanding the evolved ML pipelines. It combines grammar-based genetic programming with Bayesian networks to guide search and enhance the searched pipelines' interpretability. The evaluation on 12 benchmark ADMET datasets showed that the proposed AutoML method obtained similar or better results than three existing methods. Additionally, the interpretable Bayesian network identified, among the ML pipelines' components generated by the AutoML method (i.e. components like biochemical feature extraction methods, preprocessing techniques and ML algorithms), which components affect the ML pipelines' predictive performance.} }
@inproceedings{10.1145/3669940.3707266, title = {Design and Operation of Shared Machine Learning Clusters on Campus}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, pages = {295--310}, year = {2025}, isbn = {9798400706981}, doi = {10.1145/3669940.3707266}, url = {https://doi.org/10.1145/3669940.3707266}, author = {Xu, Kaiqiang and Sun, Decang and Wang, Hao and Ren, Zhenghang and Wan, Xinchen and Liao, Xudong and Wang, Zilong and Zhang, Junxue and Chen, Kai}, keywords = {multi-tenant cluster operations, resource management, shared gpu cluster, location = Rotterdam, Netherlands}, abstract = {The rapid advancement of large machine learning (ML) models has driven universities worldwide to invest heavily in GPU clusters. Effectively sharing these resources among multiple users is essential for maximizing both utilization and accessibility. However, managing shared GPU clusters presents significant challenges, ranging from system configuration to fair resource allocation among users.This paper introduces SING, a full-stack solution tailored to simplify shared GPU cluster management. Aimed at addressing the pressing need for efficient resource sharing with limited staffing, SING enhances operational efficiency by reducing maintenance costs and optimizing resource utilization. We provide a comprehensive overview of its four extensible architectural layers, explore the features of each layer, and share insights from real-world deployment, including usage patterns and incident management strategies.As part of our commitment to advancing shared ML cluster management, we open-source SING's resources to support the development and operation of similar systems.} }
@inproceedings{10.1145/3650203.3663329, title = {AIDB: a Sparsely Materialized Database for Queries using Machine Learning}, booktitle = {Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning}, pages = {23--28}, year = {2024}, isbn = {9798400706110}, doi = {10.1145/3650203.3663329}, url = {https://doi.org/10.1145/3650203.3663329}, author = {Jin, Tengjun and Mittal, Akash and Mo, Chenghao and Fang, Jiahao and Zhang, Chengsong and Dai, Timothy and Kang, Daniel}, abstract = {Analysts and scientists are interested in automatically analyzing the semantic contents of unstructured, non-tabular data (videos, images, text, and audio). These analysts have turned to unstructured data systems leveraging machine learning (ML). The most common method of using ML in analytics systems is to call them as user-defined functions (UDFs). Unfortunately, UDFs can be difficult for query optimizers to reason over. Furthermore, they can be difficult to implement and unintuitive to application users.Instead of specifying ML models via UDFs, we propose specifying mappings between virtual columns in a structured table, where virtual rows are sparsely materialized via ML models. Querying sparsely materialized tables has unique challenges: even the cardinality of tables is unknown ahead of time, rendering a wide range of standard optimization techniques unusable. We propose novel optimizations for accelerating approximate and exact queries over sparsely materialized tables to address these challenges, providing up to 350x cheaper queries. We implement our techniques in AIDB and deploy them in four real-world datasets. Several of these datasets were constructed with collaborators including law professors studying court cases, showing AIDB's wide applicability.} }
@inproceedings{10.1109/SCW63240.2024.00008, title = {Machine Learning Aboard the ADAPT Gamma-Ray Telescope}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {4--10}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00008}, url = {https://doi.org/10.1109/SCW63240.2024.00008}, author = {Htet, Ye and Sudvarg, Marion and Butzel, Andrew and Buhler, Jeremy D. and Chamberlain, Roger D. and Buckley, James H.}, keywords = {machine learning, multi-messenger astrophysics, neural networks, location = Atlanta, GA, USA}, abstract = {The Advanced Particle-astrophysics Telescope (APT) is an orbital mission concept designed to contribute to multi-messenger observations of transient phenomena in deep space. APT will be uniquely able to detect and accurately localize short-duration gamma-ray bursts (GRBs) in the sky in real time. Current detection and analysis systems require resource-intensive ground-based computations; in contrast, APT will perform on-board analysis of GRBs, demanding analytical tools that deliver accurate results under severe size, weight, and power constraints.In this work, we describe a neural network approach in our computation pipeline for GRB localization, demonstrating the capabilities of two neural networks: one to discard signals from background radiation, and one to estimate the uncertainty of GRB source direction constraints associated with individual gamma-ray photons. We validate the accuracy and computational efficiency of our networks using a physical simulation of GRB detection in the Antarctic Demonstrator for APT (ADAPT), a high-altitude balloon-borne prototype for APT.} }
@inproceedings{10.1145/3660853.3660935, title = {Enhanced Network Traffic Classification with Machine Learning Algorithms}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {322--327}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660935}, url = {https://doi.org/10.1145/3660853.3660935}, author = {Najm, Ihab Ahmed and Saeed, Ahmed Hikmat and Ahmad, B.A. and Ahmed, Saadaldeen Rashid and Sekhar, Ravi and Shah, Pritesh and Veena, B.S.}, keywords = {Enhanced real-world network, Network traffic, classification, machine learning algorithms, location = undefinedstanbul, Turkiye}, abstract = {Network traffic classification plays a critical role in maintaining the security and efficiency of modern computer networks. Existing techniques often have problems effectively identifying and establishing network traffic patterns. This research seeks to fill this gap by offering an updated approach to network traffic classification using machine learning algorithms. Our research extends upon earlier studies by focusing on robust feature engineering techniques and applying a broad assortment of machine learning algorithms, including decision trees, random forests, support vector machines, and recurrent neural networks. We employ a comprehensive dataset containing diverse network traffic situations to train. We demonstrate the efficacy of our approach by achieving promising accuracy rates: 93\% for decision trees, 97.89\% for random forests, 91\% for support vector machines, and 89.49\% for recurrent neural networks. Our findings emphasize the promise of machine learning for addressing real-world network traffic classification challenges.} }
@inproceedings{10.1145/3631700.3664913, title = {Towards Exploring Personalized Hyperlink Recommendations Through Machine Learning}, booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization}, pages = {528--533}, year = {2024}, isbn = {9798400704666}, doi = {10.1145/3631700.3664913}, url = {https://doi.org/10.1145/3631700.3664913}, author = {Bompotas, Agorakis and Triantafyllopoulos, Panagiotis and Raptis, George E. and Katsini, Christina and Makris, Christos}, keywords = {Content Overload, Hyperlink Analysis, Machine Learning, Natural Language Processing, Personalization, Recommendation Systems, User Experience, Web Navigation, Web Usability, location = Cagliari, Italy}, abstract = {The Internet offers a wealth of content, making it increasingly difficult for users to navigate website information. The volume of hyperlinks on a website often leaves users struggling with content overload, hindering their ability to find relevant information of high interest. This problem highlights the critical need for tools to improve the user experience by providing personalized hyperlink recommendations on a specific website. This paper introduces HypeRec, a browser extension that attempts to address this problem by leveraging and comparing different machine learning and recommendation algorithms to guide users to content consistent with their interests and preferences. Our approach involves extracting hyperlinks from a webpage and subjecting the corresponding textual content to natural language processing techniques. In this way, it simplifies the users’ navigation within a website and promotes a more intuitive and satisfying web browsing experience.} }
@inproceedings{10.1145/3677779.3677809, title = {Traveling Classification Profiler towards Passport Service Optimization using Machine Learning}, booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {182--187}, year = {2024}, isbn = {9798400709760}, doi = {10.1145/3677779.3677809}, url = {https://doi.org/10.1145/3677779.3677809}, author = {Manurung, Raphael Fransiskus and Sembiring, Jaka and Bandung, Yoanes}, abstract = {The issue of passport misuse poses significant threats to both national and international security, as well as the integrity of a country's immigration system. Enhancing the effectiveness of the immigration system to detect and prevent passport misuse is crucial in anticipating as well as mitigating of the potential threats. This study investigates into the implementation of traveler movement extraction to support passport control mechanisms and mitigate the associated risks. The primary objective is to enhance the quality of information to address vulnerabilities in the passport management system and streamline passport verification processes. This approach focuses on the benefits of leveraging the integration of data from diverse microservices such as passport validation, authentication, and real-time border monitoring through web service and utilizing machine learning to extract the valuable data then transforms into insights. By using real-world data sources ensures the accuracy and fidelity of representations. Notably, most informative variables such as duration, frequency, age, gender, country, and category are extracted to inform the analysis. Results indicate that the random forest classifier algorithm outperforms support vector machine and decision tree algorithms in terms of accuracy, achieving rates of 96\% and above across varying dataset thresholds.} }
@inproceedings{10.1145/3722212.3725636, title = {Navigating Data Errors in Machine Learning Pipelines: Identify, Debug, and Learn}, booktitle = {Companion of the 2025 International Conference on Management of Data}, pages = {813--820}, year = {2025}, isbn = {9798400715648}, doi = {10.1145/3722212.3725636}, url = {https://doi.org/10.1145/3722212.3725636}, author = {Karlas, Bojan and Salimi, Babak and Schelter, Sebastian}, keywords = {data debugging, data errors, data importance, missing data, location = Berlin, Germany}, abstract = {Addressing data errors-such as missing, incorrect, noisy, biased, or out-of-distribution values-is essential to building reliable machine learning (ML) systems. Traditional methods often focus on refining the training process to minimize error symptoms or repairing data errors indiscriminately, without addressing their root causes. These isolated approaches ignore how errors originate and propagate through the interconnected stages of ML pipelines-data preprocessing, model training, and prediction-resulting in superficial fixes and suboptimal solutions. Consequently, they miss the opportunity to understand how data errors impact downstream tasks and to implement targeted, effective interventions. In recent years, the research community has made significant progress in developing holistic approaches to identify the most harmful data errors, prioritize impactful repairs, and reason about their effects when errors cannot be fully resolved. This tutorial surveys prominent work in this area and introduces practical tools designed to address data quality issues across the ML lifecycle. By combining theoretical insights with hands-on demonstrations, attendees will gain actionable strategies to diagnose, repair, and manage data errors, enhancing the reliability, fairness, and transparency of ML systems in real-world applications.} }
@inproceedings{10.1145/3695220.3695229, title = {Tourist Destination Recommendation System based on Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Big Data and Computing}, pages = {58--67}, year = {2024}, isbn = {9798400718205}, doi = {10.1145/3695220.3695229}, url = {https://doi.org/10.1145/3695220.3695229}, author = {Kongpeng, Sumitra and Hanskunatai, Anantaporn}, keywords = {Data mining, Less visited area, Machine learning, Tourist recommendation system, location = Bangkok, Thailand}, abstract = {Thailand has a wide variety of tourist attractions, making it difficult for tourist to choose where to go on vacation. The tourist destination recommendation system is a challenge for creating a system to help recommend tourist destinations that are appropriate for personal. Therefore, the principal aims of this research encompass two distinct objectives: firstly, to create a recommendation system for tourist destinations in Thailand by applying machine learning algorithms; and secondly, to analyze factors influencing tourists' choices of destinations. The dataset was gathered from an online survey conducted via Google Forms, comprising responses from 429 tourists in Thailand. In the experiments, three different types of feature selection methods were applied in a data preprocessing step. In the modeling process, four machine learning algorithms, namely Decision Tree, Random Forest, k-Nearest Neighbors (k-NN), and Multi-Layer Perceptron (MLP), were used to construct the model and compare the predictive performance of the recommendation system based on hit rate and NDCG. The experimental results showed that suggesting tourist destinations in the Central region was the most effective, with the highest hit rate and NDCG compared to other regions. The average hit rate and NDCG for the five regions were 0.8 and 0.59, respectively. In addition, there has been an analysis of key factors influencing destination selection, such as activity, travel month, travel budget, and the age of tourists, to understand their impact on travel choices in each region of Thailand.} }
@inproceedings{10.1145/3655755.3655781, title = {Machine Learning Algorithms for Diabetes Diagnosis Prediction}, booktitle = {Proceedings of the 2024 6th International Conference on Image, Video and Signal Processing}, pages = {192--199}, year = {2024}, isbn = {9798400716829}, doi = {10.1145/3655755.3655781}, url = {https://doi.org/10.1145/3655755.3655781}, author = {Zambrana, Amanda and Fanek, Loreen and Carrera, Pablo Salar and Ali, Md Liakat and Narasareddygari, Mourya Reddy}, keywords = {Accuracy, Diabetes Diagnosis, Machine Learning, Precision, Prediction, Recall, location = Ikuta, Japan}, abstract = {Diabetes is one of the most common health conditions in the United States, affecting more than 37 million U.S. adults. Despite how many people are affected by this disease, a permanent cure does not currently exist. Therefore, it is very important for patients to get the proper diagnosis and treatment. If diabetes is not diagnosed and properly treated to control blood sugar levels, serious health problems can occur. Though undiagnosed cases are very dangerous, still 1 in 5 U.S. adults with diabetes do not know that they have it. Therefore, a method for early detection or prediction of the disease is imperative for healthcare workers to provide timely distribution of treatment. Presently, hospitals collect large volumes of patient data with various tests and provide appropriate treatment based on diagnoses. With big data analytics, hidden patterns can be found in the data that help to predict diagnosis outcomes. Such predictions can help lower the rate of undiagnosed patients. In our paper, we propose a prediction model for classification of diabetes based on various features of patient data. We developed and applied six machine learning algorithms on two diabetes data sets to predict diagnoses. We found that the best algorithms for prediction with our two data sets were Ridge Classifier with an accuracy of 83.12\%, along with Random Forest and Decision Tree classifiers, both achieving accuracies of 95\%.} }
@inproceedings{10.1145/3736539.3754446, title = {Earth Embeddings: Harnessing the Information in Earth Observation Data with Machine Learning}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Frontiers}, year = {2025}, isbn = {9798400719462}, doi = {10.1145/3736539.3754446}, url = {https://doi.org/10.1145/3736539.3754446}, author = {Rolf, Esther and Klemmer, Konstantin and Ruwurm, Marc}, abstract = {Machine learning (ML) for Earth Observation (EO) data is revolutionizing the speed and scope at which science and policy can operate — filling critical data gaps across fields such as ecology and development economics. Helping fuel this progress is a class of ML for EO models that distill global satellite data into compact, multi-purpose representations of the Earth. These “Earth embedding” models include image embeddings designed to capture the unique characteristics of satellite imagery and an emerging class of location encoders that serve as implicit neural representations of Earth’s data. These models are already unlocking new use cases and capabilities, with much research yet to be explored.} }
@inproceedings{10.1145/3656650.3656716, title = {User Insights shaping Machine Learning applied to Archives}, booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces}, year = {2024}, isbn = {9798400717642}, doi = {10.1145/3656650.3656716}, url = {https://doi.org/10.1145/3656650.3656716}, author = {Kasturi, Surya and Shenfield, Alex and Roast, Christopher}, keywords = {Archives, Machine Learning, UI, UX, user-centered designs, location = Arenzano, Genoa, Italy}, abstract = {Archives hold vast amounts of historical and cultural information, but navigating and extracting knowledge can be a daunting task. Machine learning (ML) offers immense potential to unlock these archives, yet its effectiveness hinges on understanding user needs. This paper explores how user insights can shape the development and application of ML in archives. Here “user” refers to editors and publishers who are crucial part of archival sorting and publication in the company. This paper emphasizes the importance of an iterative user centred design process to guide development and ensure user acceptance and empowerment. This approach reveals the distance between user expectations and functional integrity.} }
@inproceedings{10.1145/3707127.3707129, title = {Multi-modal Machine Learning in Gastrointestinal Endoscopy: A Review}, booktitle = {Proceedings of the 2024 11th International Conference on Biomedical and Bioinformatics Engineering}, pages = {10--17}, year = {2025}, isbn = {9798400718274}, doi = {10.1145/3707127.3707129}, url = {https://doi.org/10.1145/3707127.3707129}, author = {Chan, In Neng and Wong, Pak Kin and Yan, Tao and Hu, Yanyan and Chan, Chon In}, keywords = {deep learning, multi-modal learning, upper endoscopy}, abstract = {The gastrointestinal (GI) tract plays a crucial role in the human body but is frequently affected by diseases that pose a significant global health issue. GI endoscopy is the primary diagnostic tool for early disease detection. However, its efficacy is limited by human factors such as fatigue and diagnostic variability. The integration of artificial intelligence (AI) in GI endoscopy, particularly through machine learning (ML) and deep learning (DL), offers a promising solution to enhance diagnostic accuracy and efficiency. Many studies have primarily focused on the sole use of endoscopic images, which limits the potential benefits of more comprehensive analysis with unimodal data. With advancements in DL algorithms that can handle complex data modalities, multi-modal machine learning (MMML) has emerged as an innovative approach that leverages multiple data modalities to enhance model performance by combining diverse information sources. Despite its potential, there is no specific review on the application of MMML in GI endoscopy. This paper reviews the applications of MMML in GI endoscopy and highlights its benefits in addressing the complexities of GI endoscopy tasks. Besides, it also explores the potential clinical applications and analyzes the challenges and provides recommendations for further advancements in this emerging field.} }
@inproceedings{10.1145/3726854.3727285, title = {Exploring Function Granularity for Serverless Machine Learning Application with GPU Sharing}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, booktitle = {Abstracts of the 2025 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems}, volume = {9}, pages = {76--78}, year = {2025}, isbn = {9798400715938}, doi = {10.1145/3726854.3727285}, url = {https://doi.org/10.1145/3726854.3727285}, author = {Hui, Xinning and Xu, Yuanchao and Shen, Xipeng}, keywords = {cloud computing, function granularity, gpu sharing, machine learning for systems, serverless computing, location = Stony Brook, NY, USA, deep learning, function-as-a-service, quality of service}, abstract = {Recent years have witnessed increasing interest in machine learning (ML) inferences on serverless computing due to its auto-scaling and cost-effective properties. However, one critical aspect, function granularity, has been largely overlooked, limiting the potential of serverless ML. This paper explores the impact of function granularity on serverless ML, revealing its important effects on the SLO hit rates and resource costs of serverless applications. It further proposes adaptive granularity as an approach to addressing the phenomenon that no single granularity fits all applications and situations. It explores three predictive models and presents programming tools and runtime extensions to facilitate the integration of adaptive granularity into existing serverless platforms. Experiments show adaptive granularity produces up to a 29.2\% improvement in SLO hit rates and up to a 24.6\% reduction in resource costs over the state-of-the-art serverless ML which uses fixed granularity.} }
@inproceedings{10.1145/3759023.3759101, title = {Enhancing DDoS Detection in Software-Defined Networking: A Machine Learning and Deep Learning Approach}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759101}, url = {https://doi.org/10.1145/3759023.3759101}, author = {Hill, Winston and Mason, Janelle and Aldrich, Bernard and Acquaah, Yaa Takyiwaa and Roy, Kaushik}, keywords = {Software-Defined Networking (SDN), DDoS (Distributed Denial of Service), Machine Learning, Deep Learning, Multiclass Classification}, abstract = {Software-Defined Networking (SDN) enables dynamic policy updates and lowers hardware costs but introduces new vulnerabilities, especially to DDoS attacks. This study analyzes two public DDoS datasets using multiclass classification techniques. A range of machine learning models—SVM, Logistic Regression, Naive Bayes, Decision Trees, KNN, Random Forest, XGBoost, and AdaBoost—are evaluated alongside deep learning models, including LSTM and attention-based LSTM, to enhance detection performance across multiple attack types.} }
@inproceedings{10.1145/3595360.3595859, title = {MLflow2PROV: Extracting Provenance from Machine Learning Experiments}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595859}, url = {https://doi.org/10.1145/3595360.3595859}, author = {Schlegel, Marius and Sattler, Kai-Uwe}, keywords = {W3C PROV, provenance, MLflow, machine learning experiments, location = Seattle, WA, USA}, abstract = {Supporting iterative and explorative workflows for developing machine learning (ML) models, ML experiment management systems (ML EMSs), such as MLflow, are increasingly used to simplify the structured collection and management of ML artifacts, such as ML models, metadata, and code. However, EMSs typically suffer from limited provenance capabilities. As a consequence, it is hard to analyze provenance information and gain knowledge that can be used to improve both ML models and their development workflows. We propose a W3C-PROV-compliant provenance model capturing ML experiment activities that originate from Git and MLflow usage. Moreover, we present the tool MLflow2PROV that extracts provenance graphs according to our model, enabling querying, analyzing, and further processing of collected provenance information.} }
@inproceedings{10.1145/3747912.3747950, title = {Spatio-temporal prediction model of urban waterlogging based on machine learning}, booktitle = {Proceedings of the 2025 International Conference on Software Engineering and Computer Applications}, pages = {243--247}, year = {2025}, isbn = {9798400715136}, doi = {10.1145/3747912.3747950}, url = {https://doi.org/10.1145/3747912.3747950}, author = {Zhang, Hui and Xu, Junsong and Zhang, Lan and Wanyan, Jianfei and Duan, Shikun}, keywords = {Keywords, Urban waterlogging, Machine learning, Hydrodynamic control equation, Improved long Short-Term memory network, Spatio-temporal prediction}, abstract = {This study proposes a spatio-temporal prediction model for urban waterlogging based on machine learning, which predicts the accumulation of urban waterlogging from both temporal and spatial dimensions. The spatio-temporal data set of urban waterlogging was constructed by using the waterlogging inundation results of Infoworks ICM under different recurrence periods of rainstorm and pipeline siltation scenarios. The random forest algorithm and the improved long short-term memory neural network algorithm were adopted to predict the waterlogging accumulation in the study area in terms of time and space. The results show that the spatio-temporal prediction accuracy of the urban waterlogging prediction model in space is 81.7\%, and the Nash efficiency coefficient of time series prediction exceeds 0.8. The waterlogging prediction method proposed in this study can effectively improve the accuracy of waterlogging early warning and response speed.} }
@inproceedings{10.1145/3746237.3746301, title = {Comparative Analysis of Downsampling Techniques for Machine Learning on Cultural Heritage Objects}, booktitle = {Proceedings of the 30th International Conference on 3D Web Technology}, pages = {1--10}, year = {2025}, isbn = {9798400720383}, doi = {10.1145/3746237.3746301}, url = {https://doi.org/10.1145/3746237.3746301}, author = {Tzermia, Chrysoula and Malamos, Athanasios G.}, keywords = {Classification, Downsampling, Point Clouds}, abstract = {Presenting point clouds in web environments is a very demanding process. It requires downsampling of point clouds in specific file formats, such us X3D, that uses web architecture to work across diverse devices and lightweight 3D graphics to ensure web page functionality. This paper evaluates the performance of several point cloud downsampling methods with the objective of identifying those that best balance data reduction for efficient web deployment with the preservation of essential features required for AI and machine learning applications. We present a comparative analysis of Voxel Grid Downsampling (VGD), Uniform Grid Subsampling (UGS), Curvature Preserving Sampling (CPS), Random Sampling (RS) and Farthest Point Sampling (FPS). To assess the impact of each approach on the distribution and structure of sampled point clouds, we conduct a number of experiments by combining SHREC 2021 Cultural Heritage dataset with PointNet.} }
@inproceedings{10.1145/3757110.3757170, title = {Research on Income Assessment of Poor Families in Latin America Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {351--356}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757170}, url = {https://doi.org/10.1145/3757110.3757170}, author = {Hu, Zhongyuan}, keywords = {Bagging model, Income assessment, feature engineering}, abstract = {This study tackles the critical challenge of accurately evaluating incomes in economically vulnerable households by constructing a machine learning framework utilizing Costa Rican household data. The proposed methodology emphasizes systematic feature engineering to identify socioeconomic attributes with strong statistical correlations to income levels, including housing conditions, educational attainment, and employment patterns. By prioritizing these discriminative features through rigorous correlation analysis and dimensionality reduction techniques, the framework substantially optimizes input data quality for predictive modeling. Comparative experiments demonstrate that this feature selection strategy consistently enhances the performance of diverse baseline algorithms, including decision trees, random forests, and gradient boosting machines, achieving notable improvements in prediction accuracy and generalization capability. The findings highlight the importance of domain-informed feature optimization in developing robust socioeconomic assessment tools, offering valuable insights for policymakers designing targeted poverty alleviation programs. This data-driven approach provides a scalable solution for income estimation challenges in developing regions, particularly where traditional survey methods face implementation barriers.} }
@inproceedings{10.1145/3647444.3652435, title = {Machine Learning Algorithms for Advanced Rainfall Prediction}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652435}, url = {https://doi.org/10.1145/3647444.3652435}, author = {Saxena, Vivek and Singh, Uday Pratap and Kumari, Bersha and Khandelwal, Ansh}, keywords = {Keywords— Rainfall forecast, accuracy, machine learning, prediction, location = Jaipur, India}, abstract = {Rainfall forecast is essential in water resource management, agricultural planning, and disaster preparedness. Traditional rainfall forecasting systems have accuracy and lead time constraints. The rise of machine learning (ML) techniques provides a viable route for addressing these issues. This review paper examines current advancements in the application of machine learning for forecasting rainfall. It investigates the difficulties connected with rainfall forecasting, such as the complex and nonlinear nature of precipitation systems, data scarcity, and the requirement for real-time forecasting. The research investigates several machine learning models, data sources, and pre-processing approaches used to improve prediction accuracy. It also covers the incorporation of satellite data, weather radar data, and climate models into ML-based rainfall forecast systems. A comprehensive review of model performance in various geographic locations and climatic situations is offered, along with an assessment of limits and future development potential. This paper intends to assist academics, meteorologists, and policymakers in utilizing ML to provide more accurate and dependable rainfall forecasts, hence improving water resource management and disaster mitigation.} }
@inproceedings{10.1145/3745812.3745822, title = {Anomaly Detection in Transactions using Machine Learning: A Comparative Study}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745822}, url = {https://doi.org/10.1145/3745812.3745822}, author = {Tikoo, Divya and Ramesh, Nandhini and Vaidya, Atharva and Vora, Rutva Ashwin and Mali, Swati}, keywords = {Decision Tree, KNN, Metaverse, Random Forest, SMOTE, SVC}, abstract = {Detecting anomalous transactions in the metaverse is a major prob- lem that has serious security implications. The review of available studies on machine learning models used to detect these anomalies is also scanty. To achieve this, four machine learning approaches (i.e., Decision Trees, Random Forest, Support Vector Classification (SVC), and K-Nearest Neighbors (KNN)) will be evaluated and com- pared in terms of their ability to determine the most effective model for anomaly detection. The dataset contains 78,600 transaction records with fields such as timestamps, addresses, amounts, and risk classifications. This comparative analysis, which considers ac- curacy, precision, F1 score, and recall as evaluation metrics, reveals that Random Forest outperforms the other models. The findings of this study are expected to provide insights for building safer and more secure metaverse environments.} }
@inproceedings{10.1145/3638209.3638229, title = {Image Scenario classification using Machine learning}, booktitle = {Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems}, pages = {130--138}, year = {2024}, isbn = {9798400709067}, doi = {10.1145/3638209.3638229}, url = {https://doi.org/10.1145/3638209.3638229}, author = {Almazrouei, khawla and Bou Nassif, Ali J.}, keywords = {Inception V3, Multilayer Perceptron, Support Vector Machine, and CNN layers, location = Tokyo, Japan}, abstract = {Image Scenario classification is widespread for many IoT applications. Classifying scenario helps in making proper decisions. The study aims at classifying six different scenarios using a deep neural network algorithm. The proposed InceptionV3 classification algorithm could predict the scenarios and achieve 92.00\% accuracy. A quick comparison is shown with the traditional machine learning algorithms, SVM and MLP. The study shows the power of the deep neural algorithm and classifies the scene image dataset with higher precision.} }
@inproceedings{10.1145/3704304.3704306, title = {Machine Learning-Based Crop Recommendation for IoT-Enabled Smart Agriculture}, booktitle = {Proceedings of the 2024 9th International Conference on Cloud Computing and Internet of Things}, pages = {11--15}, year = {2025}, isbn = {9798400717161}, doi = {10.1145/3704304.3704306}, url = {https://doi.org/10.1145/3704304.3704306}, author = {Sonata, Ilvico}, keywords = {Internet of Things, Machine Learning, Random Forest, Sensor, Smart Agriculture}, abstract = {Human food needs are increasing all the time. This condition is in line with the increasing world population and exacerbated by climate change. Agricultural products are one of the sources of food to meet food needs. The use of technology is one of the keys to success in increasing agricultural production. One of the technologies used in agricultural technology is the Internet of Things (IoT). IoT in agricultural technology can be used to monitor soil and weather conditions to ensure that these conditions are in accordance with the needs of agricultural crops. The use of machine learning technology can be combined with IoT to help predict the types of crops that are suitable for planting based on soil and weather conditions. The results of the experiment show that the machine learning model can be used to predict the right types of plants to plant based on soil and weather conditions obtained through IoT. This method can be used to increase agricultural yields.} }
@inproceedings{10.1145/3688268.3688271, title = {Prediction of Undergraduate Success Through Machine Learning Models}, booktitle = {Proceedings of the 2024 12th International Conference on Computer and Communications Management}, pages = {12--18}, year = {2024}, isbn = {9798400718038}, doi = {10.1145/3688268.3688271}, url = {https://doi.org/10.1145/3688268.3688271}, author = {Chaiya, Supap and Songpan, Wararat}, keywords = {logistic regression model, predictive model, strategic planning, success factors}, abstract = {The aim of this study is to investigate the factors that affect the graduation rates of undergraduates at Khon Kaen University. The focus is on students who were admitted through the direct admission test in 2018 and graduated in 2022. The university has recently implemented a new direct admission testing system, which has led to the need for a passing level score for university entrance. To address this issue, the study uses a management-by-fact approach, utilizing data from admissions and registration databases that are analyzed using machine learning models. The research categorizes the influencing factors into three main groups: 1) personal factors, including gender, family income, and parents' occupations; 2) educational factors, such as total admission scores, first-year grade point averages (GPA) for both semesters, and English admission scores; and 3) university service-related factors, including teacher and overall class evaluation scores by students. The study found that the GPAs of the first and second semesters in the first year (GPA1 and GPA2, respectively) had a significant impact on the graduation rates of students. The comparison of ten models showed that logistic regression was the most effective, achieving 88.05\% accuracy in predicting outcomes. This finding highlights the potential of machine learning in educational settings, offering valuable insights for strategic planning and interventions aimed at improving graduation rates.} }
@inproceedings{10.1145/3635638.3635646, title = {Predictive Analysis of NBA Game Outcomes through Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence}, pages = {46--55}, year = {2024}, isbn = {9798400709456}, doi = {10.1145/3635638.3635646}, url = {https://doi.org/10.1145/3635638.3635646}, author = {Wang, Junwen}, keywords = {Machine Learning, NBA Game Outcomes, Predictive Analysis, Sports Analytics, location = Chongqing, China}, abstract = {This study delved into the realm of sports analytics, employing machine learning techniques to predict the outcomes of NBA games based on player performance and team statistics. Through meticulous data collection, filtering, and model comparison, we gained insights into the factors that significantly impact game results. Logistic Regression, Support Vector Machines, Deep Neural Networks (DNN) and Random Forest models were rigorously evaluated, showcasing the power of advanced algorithms in uncovering intricate patterns within the data. The structured methodology of this study provides a versatile framework applicable to various sports analytics scenarios. The study shows the better performance of DNN and Random Forest in predicting the game results and the importance of field goal percentage in the predicting. Limitations still exist in this work, including data quality and model constraints; however, the findings have immediate implications for sports professionals seeking actionable insights. As we look ahead, this research underscores the potential for future advancements, encouraging exploration into more sophisticated algorithms, deeper feature analysis, and the integration of temporal patterns for comprehensive predictive accuracy.} }
@inproceedings{10.1145/3718391.3727887, title = {Study on the models of machine learning for better classification}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {319--324}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3727887}, url = {https://doi.org/10.1145/3718391.3727887}, author = {Cui, Yangfan and Yang, Guangsong and Liu, Yuanfa and Lei, Guowei and Ni, Wenqing}, keywords = {Gaussian mixture models, Random forests, hidden Markov models, noise immunity, over-fitting}, abstract = {Up to now, with the development of artificial intelligence, artificial energy technology has been widely used in medicine, text recognition, intelligent speech and other fields. As an important branch of artificial intelligence, machine learning plays a pivotal role in accumulating knowledge for artificial intelligence. In order to study better machine learning methods, this paper compares three machine learning models, Gaussian mixture model (GMM), random forest (RF) and hidden Markov model (HMM), analyzes and compares their advantages and disadvantages and better application fields in terms of accuracy and complexity, and combines simple algorithms to explore a model with higher accuracy. Experimental results have validated the model} }
@inproceedings{10.1145/3649329.3663510, title = {Invited: Leveraging Machine Learning for Quantum Compilation Optimization}, booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference}, year = {2024}, isbn = {9798400706011}, doi = {10.1145/3649329.3663510}, url = {https://doi.org/10.1145/3649329.3663510}, author = {Ren, Xiangyu and Zhang, Tianyu and Xu, Xiong and Zheng, Yi-Cong and Zhang, Shengyu}, keywords = {quantum computation, compiler optimization, machine learning, qubit routing, location = San Francisco, CA, USA}, abstract = {The design of quantum algorithms typically assumes the availability of an ideal quantum computer, characterized by full connectivity, noiseless operation, and unlimited coherence time. However, Noisy Intermediate-Scale Quantum (NISQ) devices present a stark contrast, with a limited number of qubits, non-negligible quantum operation errors, and stringent constraints on the connectivity of physical qubits within a Quantum Processing Unit (QPU). This necessitates the dynamic remapping of logical qubits to physical qubits within the compiler to facilitate the execution of two-qubit gates in the algorithm. However, this introduces additional operations, consequently reducing the fidelity of the algorithm. Therefore, minimizing the number of added gates becomes crucial. Finding such an optimal routing problem is NP-hard, and the task is conventionally addressed using human-crafted heuristics to search for SWAP sequences, but these lack performance guarantees. In this study, we employ a Seq2Seq machine learning model for the qubit routing task, incorporating a Transformer neural network to learn the routing information in the gate and SWAP sequence. Compared to heuristic search-based algorithms, our approach significantly reduces the overhead of quantum computing resources required to adapt logical circuits to physical circuits executable on specific quantum backend hardware.} }
@inproceedings{10.1145/3660317.3660324, title = {Benchmarking Machine Learning Applications on Heterogeneous Architecture using Reframe}, booktitle = {Proceedings of the 4th Workshop on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy}, pages = {16--22}, year = {2024}, isbn = {9798400706455}, doi = {10.1145/3660317.3660324}, url = {https://doi.org/10.1145/3660317.3660324}, author = {Rae, Christopher and Lee, Joseph K. L. and Richings, James and Weiland, Mich\`ele}, keywords = {HPC, reframe, kubernetes, benchmarking, data science, machine learning, MLPerf, GPU, graphcore, cerebras, location = Pisa, Italy}, abstract = {With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues. Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm. We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures.} }
@inproceedings{10.1145/3701716.3715863, title = {Graph Machine Learning under Distribution Shifts: Adaptation, Generalization and Extension to LLM}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {53--56}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715863}, url = {https://doi.org/10.1145/3701716.3715863}, author = {Wang, Xin and Li, Haoyang and Zhang, Zeyang and Zhu, Wenwu}, keywords = {adaptation, distribution shift, generalization, graph machine learning, large language model, location = Sydney NSW, Australia}, abstract = {Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution (I.D.) hypothesis, i.e., testing and training graph data are sampled from the identical distribution. However, this I.D. hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, several advanced graph machine learning techniques which go beyond the I.D. hypothesis, have made great progress and attracted ever-increasing attention from the research community. This tutorial is to disseminate and promote the recent research achievement on graph out-of-distribution adaptation, graph out-of-distribution generalization, and large language models for tackling distribution shifts, which are exciting and fast-growing research directions in the general field of machine learning and data mining. We will advocate novel, high-quality research findings, as well as innovative solutions to the challenging problems in graph machine learning under distribution shifts and the applications on graphs. This topic is at the core of the scope of The Web Conference, and is attractive to machine learning as well as data mining audience from both academia and industry.} }
@inproceedings{10.1145/3733006.3733009, title = {Diabetes Detection: Predicting Type II Diabetes with Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 International Conference on Health Big Data}, pages = {16--22}, year = {2025}, isbn = {9798400713446}, doi = {10.1145/3733006.3733009}, url = {https://doi.org/10.1145/3733006.3733009}, author = {Zhang, Wenshan}, keywords = {Diabetes Management, Imputation Techniques, Logistic Regression, Neural Network, Random Forest, Ridge Regularization, Support Vector Machine}, abstract = {This project aims to predict type II diabetes using machine learning models and identify the one with the best performance, followed by recommendations for prevention strategies. To enhance data quality and address abnormal values, three separate imputation methods were evaluated, as well as the impact of feature standardization on various models. Logistic regression with Ridge regularization, support vector machine, Random Forest and a simple neural network with one hidden layer were implemented with four models for comparison. Some important aspects of working with the dataset included feature engineering, and hyperparameter-tuning with cross-validation to get the best model fit. Feature importance analysis also provided valuable insights into the relationships between health metrics and diabetes risk.} }
@inproceedings{10.1145/3672608.3707845, title = {SeismicSense: Phase Picking of Seismic Events with Embedded Machine Learning}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {551--559}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707845}, url = {https://doi.org/10.1145/3672608.3707845}, author = {Zainab, Tayyaba and Harms, Laura and Karstens, Jens and Landsiedel, Olaf}, keywords = {seismological data analysis, earthquake detection, TinyML, deep neural networks, low-power, internet of things, edge AI, location = Catania International Airport, Catania, Italy}, abstract = {Analyzing seismic data is essential for understanding natural geological processes and anthropogenic activities, particularly in localizing seismic events. While recent advances in seismic analysis rely heavily on resource-intensive machine learning approaches, these methods are impractical in resource-constrained environments such as underwater, underground, or rural areas. To address this, we introduce SeismicSense, a lightweight neural network (NN)-based solution for sensor-level seismic data analysis. SeismicSense detects seismic events and localizes them by identifying seismic event phases through a cascading architecture. Initially, SeismicSense uses an NN to filter out non-earthquake events, minimizing false positives. Upon identifying an earthquake, it detects the P- and S- phases, which are crucial for determining the origin and magnitude of seismic activity. SeismicSense significantly reduces data transmission by communicating only the arrival times of these phases to the cloud, enabling efficient and selective communication during seismic events. Despite being 20 times smaller than state-of-the-art models and requiring just 186 KB of RAM, SeismicSense achieves exceptional performance, with F1-scores of 99.4\% for earthquake detection, 98\% for P-wave detection, and 96\% for S-wave detection. Additionally, leveraging integer acceleration on modern MCUs enhances efficiency, reducing inference time on Cortex-M MCUs by 18-fold compared to non-accelerated methods, enabling real-time execution.} }
@inproceedings{10.1145/3766918.3766922, title = {Machine Learning-Driven Multi-Factor Quantitative Model: A Study on the Ethereum Market}, booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business}, pages = {19--28}, year = {2025}, isbn = {9798400716027}, doi = {10.1145/3766918.3766922}, url = {https://doi.org/10.1145/3766918.3766922}, author = {Yu, Zhijie}, keywords = {Ethereum, machine learning, multi-factor model, on-chain factors, quantitative trading}, abstract = {This study constructs a machine learning-driven multi-factor model for Ethereum quantitative trading, combining traditional technical indicators (RSI, MACD), on-chain metrics (gas usage, active addresses), and X platform social sentiment to predict short-term returns. Backtesting from Q4 2021 to Q3 2024, using online learning and genetic algorithms for dynamic factor updates, yields a 97\% annualized return, a Sharpe ratio of 2.5, and an information ratio of 1.2, outperforming Ethereum's raw returns. Simulated trading in Q4 2024 (bull market) achieves a 33\% quarterly return with an 18\% maximum drawdown, while Q1 2025 (bear market) records a -10\% quarterly return with a 12\% drawdown, confirming robustness. Technical and sentiment factors drive performance, though a 22\% maximum drawdown in backtesting highlights volatility risks. An optimal Z-score threshold (±1.0) and 4-hour trading frequency balance profitability and costs. Future enhancements include high-frequency mainnet data integration and advanced risk management to strengthen model resilience in Ethereum's volatile market.} }
@article{10.14778/3746405.3746437, title = {ArrayMorph: Optimizing Hyperslab Queries on the Cloud for Machine Learning Pipelines}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {3189--3202}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3746405.3746437}, url = {https://doi.org/10.14778/3746405.3746437}, author = {Jiang, Ruochen and Blanas, Spyros}, abstract = {Cloud storage services such as Amazon S3, Azure Blob Storage, and Google Cloud Storage are widely used to store raw data for machine learning applications. When the data is later processed, the analysis predominantly focuses on regions of interest (such as a small bounding box in a larger image) and discards uninteresting regions. Machine learning applications can significantly accelerate their I/O if they push this data filtering step to the cloud. Prior work has proposed different methods to partially read array (tensor) objects, such as chunking, reading a contiguous byte range, and evaluating a lambda function. No method is optimal; estimating the total time and cost of a data retrieval requires an understanding of the data serialization order, the chunk size and platform-specific properties. This paper introduces ArrayMorph, a cloud-based array data storage system that automatically determines which is the best method to use to retrieve regions of interest from data on the cloud. ArrayMorph formulates data accesses as hyperslab queries, and optimizes them using a multi-phase cost-based approach. ArrayMorph seamlessly integrates with Python/PyTorch-based ML applications, and is experimentally shown to transfer up to 9.8X less data than existing systems. This makes ML applications run up to 1.7X faster and 9X cheaper than prior solutions.} }
@inproceedings{10.1145/3703187.3703200, title = {Improving English Speech Recognition System Accuracy Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial Intelligence}, pages = {73--78}, year = {2024}, isbn = {9798400707254}, doi = {10.1145/3703187.3703200}, url = {https://doi.org/10.1145/3703187.3703200}, author = {Xu, Hui}, keywords = {Accuracy Improvement, Deep Learning, Machine Learning, Reinforcement Learning, Speech Recognition, Transfer Learning}, abstract = {This research significantly advanced English speech recognition technology by leveraging advanced machine learning techniques. The study developed a novel hybrid model combining deep learning, transfer learning, and reinforcement learning, which markedly improved recognition accuracy in complex environments. A diverse 1000-hour dataset was used for training, encompassing various accents and environmental conditions. The model's architecture integrated LSTM, CNN, and attention mechanisms, coupled with innovative feature extraction methods. Results showed substantial improvements across multiple metrics. The overall word error rate was reduced to 11.25\%, outperforming existing benchmark models. Notably, the system achieved 86\% accuracy in noisy environments and 92\% in accent adaptation. For rapid speech, error rates decreased from 20\% to 12\%. These advancements expand the technology's applicability in fields such as healthcare, education, and customer service, paving the way for more robust and versatile speech recognition systems in real-world applications.} }
@inproceedings{10.1145/3737895.3768305, title = {UAV-aided Fast Data Collection via Machine Learning Using AERPAW's Digital Twin}, booktitle = {Proceedings of the ACM Workshop on Wireless Network Testbeds, Experimental Evaluation \&amp; Characterization}, pages = {89--96}, year = {2025}, isbn = {9798400719721}, doi = {10.1145/3737895.3768305}, url = {https://doi.org/10.1145/3737895.3768305}, author = {Sadique, Joarder Jafor and Khan, Mahfizur Rahman and Ibrahim, Ahmed S.}, keywords = {digital twin, data collection, k-means clustering, trajectory optimization, and UAV, location = Kerry Hotel, Hong Kong, Hong Kong, China}, abstract = {Unmanned aerial vehicles (UAVs) have emerged as essential components for 5G and beyond networks. Particularly, UAVs serve as aerial data mules, capable of efficiently collecting and transferring data from geographically dispersed ground-based nodes, such as ground base stations. However, practical limitations like battery life constraints, stringent mission timeliness, and varying wireless conditions present significant challenges for efficient data collection. To reduce the mission completion time, optimizing UAV's trajectory is often considered a primary challenge. In addition, data collection time under hovering mode also greatly affect the mission time. In this work, we propose machine learning (ML)-based UAV-aided data collection model that leverages K-means clustering to identify optimal waypoints near each base station within Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) geofence area, located in Raleigh, North Carolina, USA. By adopting the clustering strategy, our primary goal is to reduce flight time without compromising the high signal-to-noise ratio (SNR) across trajectories. Our proposed model also tackles the challenge of dynamic hovering time adjustment at each waypoint by incorporating an adaptive approach based on instantaneous link quality and data volume. To validate the proposed data collection model, we utilize a digital twin simulation environment from AERPAW. The results emulated over the AERPAW's digital twin demonstrate the effectiveness of the proposed algorithm, ensuring enhanced efficiency in the UAV-aided fast data collection strategy.} }
@inbook{10.1145/3672608.3707756, title = {A Machine Learning-Based Approach For Detecting Malicious PyPI Packages}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {1617--1626}, year = {2025}, isbn = {9798400706295}, url = {https://doi.org/10.1145/3672608.3707756}, author = {Samaana, Haya and Costa, Diego Elias and Shihab, Emad and Abdellatif, Ahmad}, abstract = {Background. In modern software development, the use of external libraries and packages is increasingly prevalent, streamlining the software development process and enabling developers to deploy feature-rich systems with little coding. While this reliance on reusing code offers substantial benefits, it also introduces serious risks for deployed software in the form of malicious packages -harmful and vulnerable code disguised as useful libraries. Aims. Popular ecosystems, such PyPI, receive thousands of new package contributions every week, and distinguishing safe contributions from harmful ones presents a significant challenge. There is a dire need for reliable methods to detect and address the presence of malicious packages in these environments. Method. To address these challenges, we propose a data-driven approach that uses machine learning and static analysis to examine the package's metadata, code, files, and textual characteristics to identify malicious packages. Results. In evaluations conducted within the PyPI ecosystem, we achieved an F1-measure of 0.94 for identifying malicious packages using a stacking ensemble classifier. Conclusions. This tool can be seamlessly integrated into package vetting pipelines and has the capability to flag entire packages, not just malicious function calls. This enhancement strengthens security measures and reduces the manual workload for developers and registry maintainers, thereby contributing to the overall integrity of the ecosystem.} }
@inproceedings{10.1145/3712623.3712646, title = {zkTaylor: Zero Knowledge Proofs for Machine Learning via Taylor Series Transformation}, booktitle = {Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications}, pages = {37--41}, year = {2025}, isbn = {9798400712883}, doi = {10.1145/3712623.3712646}, url = {https://doi.org/10.1145/3712623.3712646}, author = {Pan, Dong and Liu, Kezhen and Li, Bingtao and Zheng, Yongsheng and Ma, Jiren}, keywords = {Exponential function, Machine learning, Taylor expansion, Zero-knowledge Proof}, abstract = {In order to enable more types of machine learning models to use zero-knowledge proofs to enhance their computational verifiability, this study proposes a zero-knowledge machine learning conversion method based on the Taylor series. Firstly, a polynomial expansion of structures with transcendental functions in ordinary machine learning models is performed using Taylor's formula. The corresponding arithmetic circuit descriptions are written in ZKP based on the converted model structures. Finally, the proof body is generated, which allows the verifier to verify the correctness of the results quickly. The basic experimental idea is also given, and the scheme's feasibility is verified, which can be done to provide a verification path for the model without seriously affecting its accuracy.} }
@inproceedings{10.1145/3759023.3759106, title = {Integrating Machine Learning, Internet of Things, and Unmanned Aerial Vehicles for Crop Farming}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759106}, url = {https://doi.org/10.1145/3759023.3759106}, author = {Petja, Moshito Mareme and Langa, Makhulu Relebogile and Moeti, Michael Nthabiseng}, keywords = {Agriculture, Crop Farming, IoT-ML-UAV, Soil Nutrition, Soil Testing.}, abstract = {Agriculture is essential for sustaining the increasing world population. However, enhancing crop yield and resource management continues to be a considerable challenge for farmers. This research study presents a novel machine learning-enabled Internet of Things (IoT) device and Unmanned Aerial Vehicles (UAV) for monitoring soil nutrients and delivering precise crop suggestions. Other studies show the use of UAVs, IoT, and/or machine learning (ML) in the domain of agriculture and yet these technologies were utilized separately, leaving integration an area to explore. This study integrates these technologies, in addition with a mobile application that the farmers can use to acquire tailored recommendations. The prototype employs Analogue Soil Moisture, Nitrogen, Phosphorus, and Potassium (NPK), and pH sensors that are attached to a UAV to monitor soil nutrients. These make it suitable for the UAV to fly to any area of the farm to collect soil samples. The gathered data is conveyed to a local server via the Arduino UNO 3’s gateway protocol. ML algorithms are utilized to analyze the gathered data and produce tailored recommendations, including a potential list of high-yield crops, specific fertilizers, and their quantities based on crop needs and soil nutrients. The mobile application enables the assessment of user acceptability at the farmer’s level. The system’s efficacy is assessed by field studies, contrasting its performance with conventional approaches. The findings illustrate the prototype’s capacity to improve crop yield and maximize resource efficiency, fostering sustainable agricultural methods and food security. This research advances IoT-enabled agriculture by illustrating the efficacy of machine learning approaches in enhancing soil nutrient management, enabling informed decisions regarding crop fertilizers, and evaluating the quality of crops.} }
@proceedings{10.1145/3708360, title = {ICMML '24: Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, year = {2024}, isbn = {9798400711657} }
@inproceedings{10.1145/3696271.3696275, title = {Developing A System for Automatic Prediction of Polycystic Ovary Syndrome Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {20--26}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696275}, url = {https://doi.org/10.1145/3696271.3696275}, author = {Akanbi, Kemi and Adepoju, Odunayo Gabriel and Nti, Kofi Isaac}, keywords = {Diagnosis, Infertility, Machine Learning, Polycystic Ovary Syndrome}, abstract = {Polycystic Ovary Syndrome (PCOS) is a condition that leads to lifelong health problems outside of infertility. The lack of a single, known cause and universal symptoms makes diagnosis challenging. The early and accurate prediction will prevent many subsequent serious and morbid illnesses that can arise from PCOS. Therefore, this study proposes a predictive Machine Learning (ML) model to identify patients at risk of PCOS and alert healthcare professionals, allowing for early intervention. The predictive performance of the Random Forest Classifier, Logistic Regression, Gradient Boost, Adaptive Boost, and XGBoost machine learning algorithms was compared based on accuracy, precision, recall, and F1-score with a publicly available dataset from Kaggle. The experiment was performed in Google Colab. Our experimental results showed a good predictive performance of 90\% across all evaluation metrics. However, Random Forest outperformed all other models achieving 96\% accuracy, precision, recall, and F1-score, respectively. The high accuracy (96\%) obtained by this study suggests that the proposed model could effectively identify patients at risk of PCOS, potentially aiding early diagnosis and intervention.\&nbsp;} }
@inbook{10.1145/3760023.3760045, title = {The Machine Learning-Enhanced DCF Model and Probabilistic Cash Flow Forecasting}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {126--133}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760045}, author = {Lu, Tianyi and Zhao, Yihao}, abstract = {Traditional DCF models rely on a high level of subjective analysts' judgment in cash flow forecasting and traditionally assume perpetual constant growth rates and WACC, which are ill-suited to capture market volatility and tail risks. To bypass these pitfalls, this research suggests a hybrid modeling approach with LSTM networks to distill temporal trends from historical financial data and forecast future cash flows as a first input. These forecasts are then run through a Monte Carlo simulation in order to generate probabilistic distributions, with a jump-diffusion process also included to cover Black Swan events [13]. An empirical investigation with data of the top 500 publicly traded US companies demonstrates that: (1) the machine learning can enhance the valuation range of DCF models; (2) compared to the traditional DCF model, the new approach provides a more complete measure of tail risks [10], (3) statistical tests confirm that valuation results of the two models are different significantly. This study introduces a more integrated risk-return investment decision-making framework, ideally suited to high-volatility industries and periods of market uncertainty [9,14].} }
@inproceedings{10.1145/3731545.3736817, title = {Performance Evaluation of Machine Learning Applications Using WebAssembly Across Different Programming Languages}, booktitle = {Proceedings of the 34th International Symposium on High-Performance Parallel and Distributed Computing}, year = {2025}, isbn = {9798400718694}, doi = {10.1145/3731545.3736817}, url = {https://doi.org/10.1145/3731545.3736817}, author = {Khan, Sallar and Malik, Tania and Hasanov, Khalid}, keywords = {webassembly, wasm, WASI, machine learning applications, Python, C++, rust, location = University of Notre Dame Conference Facilities, Notre Dame, IN, USA}, abstract = {WebAssembly (WASM) has emerged as a promising compilation target for languages traditionally not executed on the web and enable the cross-platform deployment of high-performance applications. While its general use cases have been well studied, the performance implications of executing Machine Learning (ML) workloads via WASM across different programming languages and runtime environments remain relatively unexplored. This paper presents a systematic evaluation of two representative ML models, K-Means and Logistic Regression, implemented in Python, Rust, and C++ and compiled to WASM. These models are executed in two distinct environments: a web browser and the WebAssembly System Interface (WASI), and their execution time and accuracy is compared against each programming language across both environments. This study aims to provide insight into the trade-offs and practical considerations involved in deploying ML workloads using WebAssembly across different language ecosystems and runtime configurations.} }
@proceedings{10.1145/3759928, title = {IPMLP '25: Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, year = {2025}, isbn = {9798400715884} }
@inproceedings{10.1145/3637528.3671483, title = {2nd Workshop on Causal Inference and Machine Learning in Practice}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6726}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671483}, url = {https://doi.org/10.1145/3637528.3671483}, author = {Lee, Jeong-Yoon and Wu, Yifeng and Harinen, Totte and Pan, Jing and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Zheng, Zeyu and Vanchinathan, Hasta and Wang, Yingfei and Stevenson, Roland}, keywords = {causal inference, machine learning, location = Barcelona, Spain}, abstract = {The workshop's rationale stems from the escalating interest in causal inference and machine learning methodologies within various industrial contexts. This surge in demand underscores the importance for both scholars and practitioners to exchange knowledge and best practices regarding the application of these techniques to tackle real-world challenges. Yet, applying causal machine learning techniques in real-world scenarios presents a range of challenges not addressed in the academic literature. This workshop aims to address the challenges for practical causal machine learning and explore new industry use cases. The workshop will provide a forum for practitioners and researchers to exchange ideas and explore new collaborations. Moreover, this workshop aims to capitalize on the success and achievements of the KDD 2023 Workshop titled "Causal Inference and Machine Learning in Practice".} }
@inproceedings{10.1145/3762329.3762353, title = {Workshop temperature and humidity prediction based on machine learning algorithms}, booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence of Things and Computing}, pages = {136--145}, year = {2025}, isbn = {9798400718625}, doi = {10.1145/3762329.3762353}, url = {https://doi.org/10.1145/3762329.3762353}, author = {Yang, Hualun and Ouyang, Shaojun and Sun, Fuyan and Zeng, Xiang and Zeng, Yi and Zhang, Ruiping and Liu, Hui}, keywords = {Feature Engineering, Model Prediction, Random Forest, Temperature and Humidity Prediction, Tobacco Processing Workshop}, abstract = {The tobacco leaf processing workshop lacks constant temperature control, causing its production environment to be susceptible to external climatic influences, leading to fluctuations in product quality. To improve product quality stability, this paper uses historical indoor and outdoor temperature and humidity data from February 2023 to August 2024. Feature engineering was employed to enhance data representation, and a Random Forest (RF) model was used for temperature and humidity prediction. The results indicate that, under optimal parameters, the root mean square error (RMSE) for temperature prediction is 0.01, and for humidity prediction is 0.26, demonstrating high predictive accuracy. By accurately forecasting future workshop temperature and humidity, the system provides data support to operators, effectively improving the accuracy of the moisture setting in the tobacco processing process and ensuring product quality.} }
@article{10.1145/3729394, title = {Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729394}, url = {https://doi.org/10.1145/3729394}, author = {Sharmin, Shaila and Zahid, Anwar Hossain and Bhattacharjee, Subhankar and Igwilo, Chiamaka and Kim, Miryung and Le, Wei}, keywords = {Fuzzing, Machine learning code, Numerical instability, Soft assertions}, abstract = {Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted ”tumor”, but instead, it incorrectly predicted ”no tumor” due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32.} }
@inproceedings{10.1145/3688671.3688762, title = {CLBO: Conditional Local Bayesian Optimization for Automated Machine Learning}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688762}, url = {https://doi.org/10.1145/3688671.3688762}, author = {Paterakis, George and Borboudakis, Giorgos and Paraschakis, Konstantinos and Charonyktakis, Pavlos and Tsamardinos, Ioannis}, keywords = {machine learning, Bayesian optimization, AutoML, optimization}, abstract = {In this paper, we present a novel Bayesian optimization method named Conditional Local Bayesian Optimization (CLBO) designed specifically to address challenges in optimizing Automated Machine Learning (AutoML) tasks. Inspired by a controller-responder architecture, our method leverages a controller that selects promising pipelines based on an acquisition function. Local responders employ Bayesian optimization to refine the search within sub-spaces. CLBO introduces a progressive budget system, which dynamically allocates the optimization budget. Our method utilizes a group of surrogate models to initially optimize a simpler objective function. Notably, CLBO outperforms current state-of-the-art algorithms on 35 classification datasets from the OpenML repository. Finally, CLBO reports unbiased performance estimates through the use of a performance correction method.} }
@inbook{10.1145/3730436.3730437, title = {Ballistic wind speed identification method based on machine learning}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {1--5}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730437}, author = {Zhou, Hanbing and Qu, Wenbo and Zhang, Zechen and Fu, Jian}, abstract = {This study aims to improve the accuracy and reliability of trajectory prediction by using an ensemble learning regression model to accurately identify ballistic wind speed. We train and optimize the ensemble of multiple regression models, and combine the Bayesian optimizer to adjust the hyperparameters to improve the model performance. In this study, two ensemble learning regression models are used to train the horizontal wind and vertical wind respectively, and the five-fold cross-validation method is used to compare the identification effect of the proposed model and the Kalman filter method. The experimental results show that our proposed method has achieved significant improvement in ballistic wind speed identification task, and has higher accuracy and stability than traditional methods. This study provides a new idea and method for improving the accuracy of trajectory prediction, which has important scientific significance and practical application prospects.} }
@inproceedings{10.1145/3641554.3701783, title = {Approachable Machine Learning Education: A Spiral Pedagogy Approach with Experiential Learning}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {924--930}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701783}, url = {https://doi.org/10.1145/3641554.3701783}, author = {Qin, Meiying}, keywords = {computer science education, experiential learning, machine learning, spiral approach, location = Pittsburgh, PA, USA}, abstract = {Machine learning (ML) is an important subject for computer science students to learn due to its broad applications. Introductory courses often present techniques in a linear sequence, resulting in a steep learning curve that can overwhelm students and limit the time for experiential learning through course projects. To address this, I restructured the course using a spiral approach, presenting concepts in three iterations. Each iteration delves deeper into the material and introduces complex computational topics progressively. This method includes a built-in repetition mechanism that reinforces learning and enhances understanding. Moreover, this approach allows time for hands-on projects that apply theory to real-world scenarios, helping students better understand the course materials. The spiral approach was implemented in an ML course at a local university, resulting in positive student feedback and improved course retention rates.} }
@inproceedings{10.1145/3617574.3617859, title = {MLGuard: Defend Your Machine Learning Model!}, booktitle = {Proceedings of the 1st International Workshop on Dependability and Trustworthiness of Safety-Critical Systems with Machine Learned Components}, pages = {10--13}, year = {2023}, isbn = {9798400703799}, doi = {10.1145/3617574.3617859}, url = {https://doi.org/10.1145/3617574.3617859}, author = {Wong, Sheng and Barnett, Scott and Rivera-Villicana, Jessica and Simmons, Anj and Abdelkader, Hala and Schneider, Jean-Guy and Vasa, Rajesh}, keywords = {system validation, error handling, design by contract, ML validation, location = San Francisco, CA, USA}, abstract = {Machine Learning (ML) is used in critical highly regulated and high-stakes fields such as finance, medicine, and transportation. The correctness of these ML applications is important for human safety and economic benefit. Progress has been made on improving ML testing and monitoring of ML. However, these approaches do not provide i) pre/post conditions to handle uncertainty, ii) defining corrective actions based on probabilistic outcomes, or iii) continual verification during system operation. In this paper, we propose MLGuard, a new approach to specify contracts for ML applications. Our approach consists of a) an ML contract specification defining pre/post conditions, invariants, and altering behaviours, b) generated validation models to determine the probability of contract violation, and c) an ML wrapper generator to enforce the contract and respond to violations. Our work is intended to provide the overarching framework required for building ML applications and monitoring their safety.} }
@inproceedings{10.1145/3718751.3718800, title = {Machine learning-based impact analysis of social factors in psychiatric patients}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {308--313}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718800}, url = {https://doi.org/10.1145/3718751.3718800}, author = {Ma, Huanying and Liang, Liang}, keywords = {influencing factors, machine learning, psychiatric patients, social functioning}, abstract = {Machine learning is an interdisciplinary subject involving multiple fields such as probability theory, statistics, approximation theory, and convex analysis, etc. Machine learning can effectively analyze the main factors affecting the recovery and enhancement of social functioning of psychiatric patients, and to analyze the main social factors affecting the level of social functioning of psychiatric patients by using machine learning theory.} }
@inproceedings{10.1145/3688671.3688739, title = {Molecular Simulation of Coarse-grained Systems using Machine Learning}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688739}, url = {https://doi.org/10.1145/3688671.3688739}, author = {Gerakinis, Dimitrios-Paraskevas and Ricci, Eleonora and Giannakopoulos, George and Karkaletsis, Vangelis and Theodorou, Doros N. and Vergadou, Niki}, keywords = {Coarse Graining, Convolutional Graph Neural Networks, Machine Learning, Molecular Simulations, Multiscale Modelling}, abstract = {Hierarchical multiscale methods are essential for molecular simulations of complex chemical systems, such as organic fluids and soft matter systems in order to reach longer length and time scales. Coarse-Graining (CG) is often the basis of multiscale schemes. Machine Learning (ML) techniques have been recently explored for the development of atomistic force fields based on quantum mechanical calculations. However, integrating ML methods into CG force fields for bulk molecular systems is still rare. In this work, Graph Convolutional Neural Network (GCNN) architectures were adopted to develop CG Machine Learned potentials for bulk amorphous systems, implementing a strategy that includes a force-matching scheme using benzene liquid as a test system. The ML-based CG force fields developed were evaluated by conducting molecular dynamics simulations at the CG level, and the extracted properties were compared with the atomistic simulations to assess the effectiveness of the ML CG interaction potentials. The impact of hyperparameters, loss function construction, and GCNN architecture size have been examined, providing valuable insights for ML-based CG approaches in bulk soft matter systems.} }
@inproceedings{10.1145/3569009.3572739, title = {Mix \&amp; Match Machine Learning: An Ideation Toolkit to Design Machine Learning-Enabled Solutions}, booktitle = {Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}, year = {2023}, isbn = {9781450399777}, doi = {10.1145/3569009.3572739}, url = {https://doi.org/10.1145/3569009.3572739}, author = {Jansen, Anniek and Colombo, Sara}, keywords = {ML capabilities, data types, design education, design ideation toolkit, machine learning, tangible user interface, location = Warsaw, Poland}, abstract = {Machine learning (ML) provides designers with a wide range of opportunities to innovate products and services. However, the design discipline struggles to integrate ML knowledge in education and prepare designers to ideate with ML. We propose the Mix \&amp; Match Machine Learning toolkit, which provides relevant ML knowledge in the form of tangible tokens and a web interface to support designers’ ideation processes. The tokens represent data types and ML capabilities. By using the toolkit, designers can explore, understand, combine, and operationalize the capabilities of ML and understand its limitations, without depending on programming or computer science knowledge. We evaluated the toolkit in two workshops with design students, and we found that it supports both learning and ideation goals. We discuss the design implications and potential impact of a hybrid toolkit for ML on design education and practice.} }
@article{10.1145/3624010, title = {A Survey of Privacy Attacks in Machine Learning}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3624010}, url = {https://doi.org/10.1145/3624010}, author = {Rigaki, Maria and Garcia, Sebastian}, keywords = {model inversion, reconstruction, model extraction, property inference, membership inference, machine learning, Privacy}, abstract = {As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 45 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.} }
@inproceedings{10.1145/3747227.3747274, title = {The Challenges and Adaptations of the ‘New Elderly’ in China: A Study Based on SVM Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {301--306}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747274}, url = {https://doi.org/10.1145/3747227.3747274}, author = {Shao, Baowen and Rokeman, Muhammad Ihsan and Wang, Xueqin and Yang, Dongmei and Zhang, Shaohan}, keywords = {Learning adaptation, Machine learning, Mobile learning, New elderly, Support Vector Machine (SVM)}, abstract = {The rapid development of information technology, accompanied with the enormous demand of the “new elderly” for home-acceptance and the emerging number of “new elderly” in China, the connection of mobile learning with the life of new elderly has gained rapid popularity. However, due to different from young learners with lower educational backgrounds or rather closed learning attitudes, there are rare studies applied the approach and computational tool to analyze the adaptation process and main influencing factors between new elderly learning and mobile learning. In this research, we aim to analyze challenges and learning adaptation strategy of the new elderly in China learning by mobile learning, and the influential factors in this field using the computational tool of machine learning. We carried out the qualitative data method for analyzing process, that is semi-structured interview for elderly participants, who show active interest and attitudes of mobile learning, extract the main topics with thematic analysis method and utilize Support Vector Machine (SVM) algorithm for identifying the importance and prediction of influential factors between adaptation level. The interview results shows: (1) New elderly suffered from big challenges of anxiety from new technology, uncertainties in the result of learning and emotion. (2) But simultaneously they demonstrated good adaptation ability and motivation for learning continue. The SVM-based machine learning identified the high or low influence variables contributing adaptation level successfully. The two findings provide implications for “new elderly” education development. Not only the high-level data model but also a new path to show what are the main influencing variables contribute to adaptive learning in the machine learning model.} }
@inproceedings{10.1145/3635638.3635643, title = {Research on Diabetes Prediction Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence}, pages = {29--33}, year = {2024}, isbn = {9798400709456}, doi = {10.1145/3635638.3635643}, url = {https://doi.org/10.1145/3635638.3635643}, author = {Zhao, Kaina and Wang, Zhiping}, keywords = {Diabetes Prediction, Extreme Gradient Boosting Tree, Machine Learning, Random Forest, Support Vector Machine, location = Chongqing, China}, abstract = {Diabetes is an irreversible, chronic metabolic disease, and is now the third most important non-communicable disease threatening human health. Therefore, early diagnosis of diabetes is essential. In this paper, after preprocessing the Pima Indian diabetes dataset from the UCI database, support vector machine (SVM), random forest (RF), extreme gradient boosting tree (XG-BOOST) models and the stacking meta model which is created by combining these three models were used to diagnose the disease. The accuracy of the models SVM, RF, XG-BOOST and stacking were found to be 75.22\%, 72.99\%, 77.88\% and 98.67\%, respectively. F1_score, AUC value and recall were utilized for a detailed analysis of the models' classification results. The obtained results revealed that the stacking model has higher accuracy than single model.} }
@inproceedings{10.1145/3747357.3747391, title = {Extraction of biomarkers from lung adenocarcinoma based on machine learning}, booktitle = {Proceedings of the 2025 International Symposium on Bioinformatics and Computational Biology}, pages = {220--227}, year = {2025}, isbn = {9798400714368}, doi = {10.1145/3747357.3747391}, url = {https://doi.org/10.1145/3747357.3747391}, author = {Zhang, Yiyi and Yang, Yuxia and Chaoluomeng and Li, Wentong and Li, Chuanhong}, keywords = {Biomarkers, Feature extraction, Lung adenocarcinoma, Machine learning, Optimization algorithm}, abstract = {Lung adenocarcinoma is one of the most common types of non-small cell lung cancer, which seriously affects the quality of life and survival rate of patients. In response to this problem, this study aims to optimize the feature extraction algorithm through in-depth analysis of lung adenocarcinoma related biomarkers, so as to improve the ability of early diagnosis and precise treatment. We used Lasso regression, non-negative matrix decomposition (NMF), point double correlation and principal component analysis (PCA) for feature extraction. The reliability and effectiveness of each method were evaluated by cross-validation, ROC curve and PR curve, and their performance in high dimensional data was compared. The optimized LASSO feature extraction method achieves up to 0.15 improvement in PR curve and up to 0.13 improvement in ROC curve. The method significantly improved the ability to identify key genes associated with tumor progression. The study demonstrated the effectiveness of the optimized feature extraction algorithm in the single cell sequencing data of lung adenocarcinoma, and found a series of biomarkers, such as sgrn, itk, ptprc, cst7, hcst,\&nbsp;cnot6l and rgs1\&nbsp;7 key genes, providing a new perspective for understanding the pathogenesis of lung adenocarcinoma. Although the sample size of this study was small and the clinical validation analysis was lacking, our findings provide new directions for future research, especially in the formulation of individualized treatment protocols. Future studies will aim to expand the sample size and further validate the clinical applicability of these biomarkers in combination with clinical data, thereby driving the development of early diagnosis and precision treatment of lung adenocarcinoma.} }
@article{10.1145/3766551, title = {TCAD-Machine Learning Enabled TID Compact Model Development for Commercial SiC MOSFET}, journal = {ACM Trans. Des. Autom. Electron. Syst.}, year = {2025}, issn = {1084-4309}, doi = {10.1145/3766551}, url = {https://doi.org/10.1145/3766551}, author = {Gao, Xujiao and Ray, Jaideep and Rummel, Brian and Glaser, Caleb and Rhoades, Elaine and Young, Joshua and Musson, Larry and Buchheit, Thomas}, keywords = {COTS, SiC MOSFET, TID, TCAD, Charon, Dakota, threshold voltage shift, Bayesian inference, Bayesian calibration, surrogate models, random forests}, abstract = {We propose a TCAD-machine learning coupled approach that combines a TCAD tool (Charon), optimization/uncertainty quantification tool (Dakota), surrogate models, and Bayesian learning capabilities. The coupling approach is used for accurate modeling and calibration of total ionizing dose (TID) induced threshold voltage (Vth) shifts in Commercial-Off-The-Shelf (COTS) semiconductor devices and to develop physics-informed TID compact models. This versatile approach is applied to model the TID effect in an exemplar COTS 3.3 kV SiC power MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor). With the Charon-Dakota coupling, we can determine key device geometry and doping values based on device physics, which are difficult to obtain or not available for COTS devices but important for TCAD simulation; additionally, we can efficiently generate thousands of simulation results in a large parameter space, which makes it possible to develop data-driven surrogate models and perform Bayesian calibration. Utilizing the full tool-coupling approach, we achieve calibrated TCAD simulation models that accurately capture the average TID-induced Vth shifts behavior with total doses and Vth shifts saturation at high doses as observed in experimental data. More importantly, the calibrated TCAD simulations are obtained with determined TID model parameters (e.g., hole trap density and capture cross section) values that contain well quantified uncertainties. Furthermore, we can isolate and quantify the noises that are not captured by the TCAD models but exist in the measured data due to measurements and devices variabilities. Lastly, the calibrated surrogate models are used to develop physics-informed TID compact models. The method is generalizable to other devices and/or radiation conditions with little modifications and can provide well-determined uncertainties.} }
@inproceedings{10.1145/3600046.3600048, title = {Image Watermarking for Machine Learning Datasets}, booktitle = {Proceedings of the Second ACM Data Economy Workshop}, pages = {7--13}, year = {2023}, isbn = {9798400708466}, doi = {10.1145/3600046.3600048}, url = {https://doi.org/10.1145/3600046.3600048}, author = {Maesen, Palle and undefinedsler, Devris and Laoutaris, Nikolaos and Erkin, Zekeriya}, keywords = {machine learning, data ownership, data economy, Watermarking, location = Seattle, WA, USA}, abstract = {Machine learning has received increasing attention for the last decade due to its significant success in classification problems in almost every application domain. For its success, the amount of available data for training plays a crucial role in the creation of a machine-learning model. However, the data-gathering process for machine learning algorithms is a tedious and time-consuming task. In many cases, the developers rely on publicly available datasets, which are not always of high quality. Recently, we are witnessing a data market paradigm where valuable datasets are sold. Thus, once the dataset is created or bought, protecting the dataset against illegal use or (re)sale and establishing intellectual property rights is necessary. In this paper, we investigate the question of deploying well-studied image watermarking techniques to be applied to classification algorithm datasets, without degrading the quality of the dataset. We investigate whether Singular Value Decomposition (SVD)-based techniques from image watermarking could be deployed on machine learning datasets or not. To this end, we chose the watermarking technique described in [8] and applied it to a machine-learning dataset. We provide experimental results on the robustness of the scheme. Our results show that the watermark embedding scheme provides decent imperceptibility and robustness against update, zero-out, and insertion attacks but, it is not successful against deletion attacks. We believe our work can inspire researchers who might want to consider applying well-studied image watermarking techniques to machine learning datasets.} }
@article{10.1145/3670007, title = {Machine Learning with Confidential Computing: A Systematization of Knowledge}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3670007}, url = {https://doi.org/10.1145/3670007}, author = {Mo, Fan and Tarkhani, Zahra and Haddadi, Hamed}, keywords = {Privacy-preserving machine learning, confidential computing, trusted execution environment}, abstract = {Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML’s pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this article, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide (i)\&nbsp;confidentiality guarantees and (ii)\&nbsp;integrity assurances and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, TEE-aware ML, and ML full pipeline guarantees. By providing these potential solutions in our systematization of knowledge, we aim to build the bridge to help achieve a much stronger TEE-enabled ML for privacy guarantees without introducing computation and system costs.} }
@article{10.1145/3617897, title = {Machine Learning (In) Security: A Stream of Problems}, journal = {Digital Threats}, volume = {5}, year = {2024}, doi = {10.1145/3617897}, url = {https://doi.org/10.1145/3617897}, author = {Ceschin, Fabr\'cio and Botacin, Marcus and Bifet, Albert and Pfahringer, Bernhard and Oliveira, Luiz S. and Gomes, Heitor Murilo and Gr\'egio, Andr\'e}, keywords = {Machine learning, cybersecurity, data streams}, abstract = {Machine Learning (ML) has been widely applied to cybersecurity and is considered state-of-the-art for solving many of the open issues in that field. However, it is very difficult to evaluate how good the produced solutions are, since the challenges faced in security may not appear in other areas. One of these challenges is the concept drift, which increases the existing arms race between attackers and defenders: malicious actors can always create novel threats to overcome the defense solutions, which may not consider them in some approaches. Due to this, it is essential to know how to properly build and evaluate an ML-based security solution. In this article, we identify, detail, and discuss the main challenges in the correct application of ML techniques to cybersecurity data. We evaluate how concept drift, evolution, delayed labels, and adversarial ML impact the existing solutions. Moreover, we address how issues related to data collection affect the quality of the results presented in the security literature, showing that new strategies are needed to improve current solutions. Finally, we present how existing solutions may fail under certain circumstances and propose mitigations to them, presenting a novel checklist to help the development of future ML solutions for cybersecurity.} }
@inproceedings{10.1145/3724154.3724167, title = {Construction of Machine Learning-based Grid Marketing Service Analysis Model}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {73--78}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724167}, url = {https://doi.org/10.1145/3724154.3724167}, author = {Li, Jiajin and Hao, Chengcheng and Ma, Yuankui and Wu, Xiaozhi and Lu, Wei and Zhong, Haiting and Xuan, Baolong and Wang, Qiyuan}, keywords = {analysis model, data middle platform, machine learning, marketing services}, abstract = {State Grid Energy Internet Marketing service System (Marketing 2.0) is a new generation of power marketing service system independently developed by State Grid Corporation. The system takes the customer as the center, takes the market as the guidance, takes the digitalization, the network, the intelligence as the guidance, builds the platform system of omni-channel service and the whole business application. It serves the marketing personnel of power supply companies in provinces, cities and counties (districts) across the country, effectively promoting the digital transformation and high-quality development of the marketing business of the State Grid Corporation. Nanjing Nari Digital service Co.,Ltd. completed the design and development of 15 analysis models in accordance with the overall planning requirements of marketing 2.0, and realized the deep integration of digital construction and business needs. This paper introduces the model of customer payment prediction analysis, charge strategy adjustment analysis and charge recovery risk analysis based on machine learning algorithm. Through the integration of artificial intelligence technology, the system has achieved technological innovation, improved model accuracy and business accuracy, and promoted the construction, application and iterative upgrading of the platform.} }
@inproceedings{10.1145/3617184.3618056, title = {Constructing Dynamic Honeypot Using Machine Learning}, booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering}, pages = {116--120}, year = {2023}, isbn = {9798400708800}, doi = {10.1145/3617184.3618056}, url = {https://doi.org/10.1145/3617184.3618056}, author = {Zhang, Yingying and Shi, Yue}, keywords = {Active Defense, Dynamic Honeypot, Machine learning, Network security, location = Putrajaya, Malaysia}, abstract = {Honeypot is an active security defense technology that uses false information to lure attackers into attacking and record their behavior. Traditional honeypots are usually static, and inherent features and services can accelerate attackers' recognition of honeypots, causing them to lose value. We designs a dynamic honeypot based on machine learning, which can adapt to dynamic and constantly changing network environments while improving the authenticity of the honeypot. It automatically generates configuration files, simulates the characteristics and behavior of devices in the network. The method proposed is to achieve active monitoring and defense of network attacks by actively scanning Nmap and obtaining network device information through P0f, and combining feature clustering methods to classify devices and generate honeypot configuration files, active monitoring and defense of network attacks can be achieved. The results shows that this methods can effectively enhance the attack capture ability and camouflage ability of honeypots.} }
@inproceedings{10.1145/3630106.3658923, title = {Diversified Ensembling: An Experiment in Crowdsourced Machine Learning}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {529--545}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658923}, url = {https://doi.org/10.1145/3630106.3658923}, author = {Globus-Harris, Ira and Harrison, Declan and Kearns, Michael and Perona, Pietro and Roth, Aaron}, keywords = {Crowdsourcing, Ensembling Methods, Fairness, location = Rio de Janeiro, Brazil}, abstract = {Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In [12], the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants’ efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams’ approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.} }
@inproceedings{10.1145/3605098.3635919, title = {Detection of Slowloris Attacks using Machine Learning Algorithms}, booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing}, pages = {1321--1330}, year = {2024}, isbn = {9798400702433}, doi = {10.1145/3605098.3635919}, url = {https://doi.org/10.1145/3605098.3635919}, author = {Rios, Vinicius and Inacio, Pedro and Magoni, Damien and Freire, Mario}, keywords = {denial of service (DoS) attack, fuzzy logic, low-rate DoS attack, machine learning, Slowloris, location = Avila, Spain}, abstract = {The Slowloris attack, a variant of the slow Denial-of-Service (DoS) attack, is a stealthy threat that aims to take down web services provided by companies and institutions. It is able to pass through the traditional defense systems, due to the low amount and high latency of its attack traffic, often mimicking legitimate user traffic. Therefore, it is necessary to investigate techniques that can detect and mitigate this type of attack and simultaneously prevent legitimate user traffic from being blocked. In this work, we investigate nine machine learning algorithms for detecting Slowloris attacks, as well as a new combination based on Fuzzy Logic (FL), Random Forest (RF), and Euclidean Distance (ED) that we call FRE. We first generate Slowloris attack traffic traces in various environments. We then assess these algorithms under two scenarios: hyperparameters with default values and optimized hyperparameters. We show that most of these machine learning algorithms perform very well, with the random forest leading to the best classification results with test accuracy values reaching 99.52\%. We also show that our FRE method outperforms all these algorithms, with test accuracy values reaching 99.8\%.} }
@inproceedings{10.1145/3728199.3728220, title = {An Improved and Combined Model for Data Pricing Based on Automated Machine Learning and Knowledge Graph}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {137--141}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728220}, url = {https://doi.org/10.1145/3728199.3728220}, author = {Su, Xinyi and Wang, Xiaodong and Li, Biao and Huang, Xiaoping and Zhong, Yuyanzhen and Shuai, Yong}, keywords = {Automated machine learning models, Data pricing, Feature recommendation model, Knowledge graph}, abstract = {Aiming at the problems of the current data pricing model, such as the difficulty of evaluation, the lack of scientific and unified rules for data transactions, the lack of privacy protection of data, and the poor accuracy of pricing, this dissertation proposes an improved data pricing combination model, which firstly establishes a feature recommendation model based on the knowledge graph, selects the optimal feature combinations, and improves the automated machine learning algorithms to obtain the optimal machine learning regression model for data pricing. Finally, the case study demonstrates that the method proposed in this paper has good applicability and accuracy} }
@article{10.1145/3764578, title = {Emerging Trends in Early Dementia Diagnosis: An Analysis on Advanced Machine Learning Approaches}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3764578}, url = {https://doi.org/10.1145/3764578}, author = {Gami, Badal and Agrawal, Manav and Katarya, Rahul}, keywords = {Alzheimer’s, Artificial Intelligence (AI), dementia, Machine Learning(ML), Natural Language Processing (NLP), neuroimaging}, abstract = {Dementia is the waning of cognitive abilities, which is typically seen with the natural aging process and includes issues with memory, language, and problem-solving abilities. Artificial Intelligence (AI) techniques are one viable method for the diagnosis of dementia. Despite recent advances in dementia informatics research and AI, accurate early diagnoses are still far from ideal. This study focuses on showcasing a comprehensive analysis of emerging AI approaches applied to early dementia diagnosis, highlighting trends across neuroimaging, speech, EEG, and clinical data. The proposed work’s main contributions include a summary of the potential challenges and vulnerabilities with dementia informatics research, a wide range of diagnostic issues in dementia care, a descriptive comparison of the elementary manuscripts judged on evaluation parameters such as precision, responsiveness, and definiteness and an offering of a descriptive set of data for developing Machine Learning (ML) and Deep Learning (DL) models. The manuscript also provides a valuable overview of new avenues for informatics research on dementia and advanced ML. The main objective is to fill a gap in the literature by offering an in-depth analysis and overview of the application of AI in dementia research, providing a foundational roadmap for accelerating impactful, data-driven dementia care solutions.} }
@inproceedings{10.1145/3543873.3589751, title = {Machine Learning for Streaming Media}, booktitle = {Companion Proceedings of the ACM Web Conference 2023}, pages = {759}, year = {2023}, isbn = {9781450394192}, doi = {10.1145/3543873.3589751}, url = {https://doi.org/10.1145/3543873.3589751}, author = {Lamkhede, Sudarshan and Chandar, Praveen and Radosavljevic, Vladan and Goyal, Amit and Luo, Lan}, abstract = {Streaming media has become a popular medium for consumers of all ages, with people spending several hours a day streaming videos, games, music, or podcasts across devices. Most global streaming services have introduced Machine Learning (ML) into their operations to personalize consumer experience, improve content, and further enhance the value proposition of streaming services. Despite the rapid growth, there is a need to bridge the gap between academic research and industry requirements and build connections between researchers and practitioners in the field. This workshop aims to provide a unique forum for practitioners and researchers interested in Machine Learning to get together, exchange ideas and get a pulse for the state of the art in research and burning issues in the industry.} }
@article{10.1145/3633455, title = {Test-Driven Ethics for Machine Learning}, journal = {Commun. ACM}, volume = {67}, pages = {45--47}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3633455}, url = {https://doi.org/10.1145/3633455}, author = {Berente, Nicholas and Kormylo, Cameron and Rosenkranz, Christoph}, abstract = {Encouraging organizations to adapt a test-driven ethical development approach.} }
@article{10.5555/3715602.3715621, title = {A Machine Learning Based Sentiment Analysis for Twitter Data}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {145--157}, year = {2024}, issn = {1937-4771}, author = {Arafat, Kazi Abdullah Al and Creer, Kode and Roni, Mahmudur Rahman and Parvez, Imtiaz}, abstract = {Sentiment analysis, also known as opinion mining, is a computational study of people's opinions, sentiments, evaluations, attitudes, and emotions expressed in textual data. This study explores the application of machine learning algorithms for sentiment analysis on a preprocessed dataset. The study employs Support Vector Machines (SVM), Maximum Entropy (Max Ent), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) for sentiment classification. The process includes feature extraction, model training, and evaluation using standard classification metrics. Out of the four algorithms, SVM, CNN, and RNN scored 97\% accuracy, while Max Ent achieved a slightly lower 95\% accuracy. More specifically, in sentiment analysis, SVM demonstrated overall better performances.} }
@inproceedings{10.1145/3653724.3653764, title = {Prediction of Water's Safety for Consumption by Machine Learning}, booktitle = {Proceedings of the International Conference on Mathematics and Machine Learning}, pages = {234--239}, year = {2024}, isbn = {9798400716973}, doi = {10.1145/3653724.3653764}, url = {https://doi.org/10.1145/3653724.3653764}, author = {Li, Jingyi}, abstract = {Water quality is the most important topic in the world. Most researchers study water using machine learning techniques to determine its potability. This paper applies similar approaches to predict the water potability and discusses why the accuracy differs. Water quality is closely related to people's lives. Cultivated land and industrial production both rely on water. The emission of sewage resulting from industrial production also endanger human health. According to the Logistic Regression, Decision Tree, Random Forest, and Extreme Gradient Boosting models, an analysis was performed to determine the best model for predicting water potability. The Random Forest model is the best model for predicting whether water is potable or not. However, the accuracy for this model is lower. The reasons may be featuring choice, parameter modification, and deficient evaluation standards. For the results, it is recommended to use all the features to train the model in similar research in the future. And set multiple parameters before building the models and compare them.} }
@inproceedings{10.1145/3637528.3671471, title = {Graph Machine Learning Meets Multi-Table Relational Data}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6502--6512}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671471}, url = {https://doi.org/10.1145/3637528.3671471}, author = {Gan, Quan and Wang, Minjie and Wipf, David and Faloutsos, Christos}, keywords = {data augmentation, graph machine learning, graph neural networks, relational databases, tabular modeling, location = Barcelona, Spain}, abstract = {While graph machine learning, and notably graph neural networks (GNNs), have gained immense traction in recent years, application is predicated on access to a known input graph upon which predictive models can be trained. And indeed, within the most widely-studied public evaluation benchmarks such graphs are provided, with performance comparisons conditioned on curated data explicitly adhering to this graph. However, in real-world industrial applications, the situation is often quite different. Instead of a known graph, data are originally collected and stored across multiple tables in a repository, at times with ambiguous or incomplete relational structure. As such, to leverage the latest GNN architectures it is then up to a skilled data scientist to first manually construct a graph using intuition and domain knowledge, a laborious process that may discourage adoption in the first place. To narrow this gap and broaden the applicability of graph ML, we survey existing tools and strategies that can be combined to address the more fundamental problem of predictive tabular modeling over data native to multiple tables, with no explicit relational structure assumed a priori. This involves tracing a comprehensive path through related table join discovery and fuzzy table joining, column alignment, automated relational database (RDB) construction, extracting graphs from RDBs, graph sampling, and finally, graph-centric trainable predictive architectures. Although efforts to build deployable systems that integrate all of these components while minimizing manual effort remain in their infancy, this survey will nonetheless reduce barriers to entry and help steer the graph ML community towards promising research directions and wider real-world impact.} }
@inproceedings{10.1145/3677779.3677842, title = {Research on the Load_breast_cancer data set under Multiple Machine Learning Algorithms}, booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {386--392}, year = {2024}, isbn = {9798400709760}, doi = {10.1145/3677779.3677842}, url = {https://doi.org/10.1145/3677779.3677842}, author = {Song, Meirui and Wu, Shixiao and Huang, Rujuan}, abstract = {breast cancer ranks first in female malignant tumors. Early detection and diagnosis is the key to treatment. This paper uses the open-source load_break_cancer breast cancer data set, mainly uses random forest, support vector machine, logical regression, Gauss naive Bayesian algorithm, BP neural network algorithm, k-neighborhood algorithm and XGBoost algorithm to classify and predict the breast cancer data set, conducts a lot of training and testing on the data set under a variety of machine learning algorithms, analyzes the learning curve in the training process, analyzes the training and testing results, and analyzes the performance of the algorithm processing data, which is of great significance for breast cancer diagnosis and treatment.} }
@inproceedings{10.1145/3715669.3723129, title = {Eye Movements as Indicators of Deception: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3723129}, url = {https://doi.org/10.1145/3715669.3723129}, author = {Foucher, Valentin and de Leon-Martinez, Santiago and Moro, Robert}, keywords = {Eye Movements, Gaze, Pupil, Deception Detection, Concealed Information Test, Machine Learning, Feature Importance}, abstract = {Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74\% in a binary classification task (Revealing vs. Concealing) and 49\% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.} }
@inproceedings{10.1145/3681769.3698586, title = {Machine Learning Model Specification for Cataloging Spatio-Temporal Models (Demo Paper)}, booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data}, pages = {36--39}, year = {2024}, isbn = {9798400711480}, doi = {10.1145/3681769.3698586}, url = {https://doi.org/10.1145/3681769.3698586}, author = {Charette-Migneault, Francis and Avery, Ryan and Pondi, Brian and Omojola, Joses and Vaccari, Simone and Membari, Parham and Peressutti, Devis and Yu, Jia and Sundwall, Jed}, keywords = {Catalog, Machine Learning, STAC, Search, Spatio-Temporal Models, location = Atlanta, GA, USA}, abstract = {The Machine Learning Model (MLM) extension is a specification that extends the SpatioTemporal Asset Catalogs (STAC) framework to catalog machine learning models. This demo paper introduces the goals of the MLM, highlighting its role in improving searchability and reproducibility of geospatial models. The MLM is contextualized within the STAC ecosystem, demonstrating its compatibility and the advantages it brings to discovering relevant geospatial models and describing their inference requirements.A detailed overview of the MLM's structure and fields describes the tasks, hardware requirements, frameworks, and inputs/outputs associated with machine learning models. Three use cases are presented, showcasing the application of the MLM in describing models for land cover classification and image segmentation. These examples illustrate how the MLM facilitates easier search and better understanding of how to deploy models in inference pipelines.The discussion addresses future challenges in extending the MLM to account for the diversity in machine learning models, including foundational and fine-tuned models, multi-modal models, and the importance of describing the data pipeline and infrastructure models depend on. Finally, the paper demonstrates the potential of the MLM to be a unifying standard to enable benchmarking and comparing geospatial machine learning models.} }
@inproceedings{10.1145/3721239.3734127, title = {Machine Learning Meets Lighting: Using Depth Estimation To Build The Light Rigs}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Talks}, year = {2025}, isbn = {9798400715419}, doi = {10.1145/3721239.3734127}, url = {https://doi.org/10.1145/3721239.3734127}, author = {Shlyaev, Sergey}, keywords = {Image based lighting, pipeline, rendering}, abstract = {This paper describes the techniques we use to build the complex light rigs in the Sony Pictures Imageworks lighting pipeline. Typically we receive the panoramic HDRI from the set and need to make a light rig from it. Building a light rig has several steps: extracting area lights from HDRI, placing them in a 3D scene, aligning lights with Lidar from set. We describe how this process can be sped up. For example, the positioning of extracted lights is automated using PatchFusion [Li et\&nbsp;al. 2023], an off-the-shelf machine learning model for high resolution monocular depth estimation. PatchFusion computes accurate metric depth directly from the HDRI. The depth map is used as a distance from light to camera to place the area lights in 3D scene. This removes the need for manual distance measurements or guesswork. Our approach significantly reduces manual labor. The time required to build the light rig goes from hours to about a minute.} }
@inproceedings{10.1145/3715071.3750419, title = {Impact of Time Discrepancies on Machine Learning Performance for Multi-Wearable Human Activity Recognition}, booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers}, pages = {98--105}, year = {2025}, isbn = {9798400714818}, doi = {10.1145/3715071.3750419}, url = {https://doi.org/10.1145/3715071.3750419}, author = {Wolling, Florian and Kostolani, David and Trollmann, Patrick and Michahelles, Florian}, keywords = {action segmentation, cnn, human activity recognition, machine learning, ms-tcn, offset, skew, synchronization, time discrepancy, location = Espoo, Finland}, abstract = {Wearable-based human activity recognition (HAR) has become a relevant tool for identifying everyday activities in various domains like healthcare, sports, and human-computer interaction (HCI). The classification performance can be improved by using multiple complementary sensors, which require accurately matched time bases. Although previous studies on synchronization in HAR suggested that sub-second accuracy is advisable while sub-100 ms accuracy is unnecessary, the specific effect of time discrepancies on machine learning models remained unexplored. We address this with an empirical evaluation of the impact of time discrepancies in multi-wearable HAR. We apply a systematic approach using the example of multi-stage temporal convolutional networks (MS-TCN) for action segmentation, simulating the time discrepancies of time offset and clock skew via rational resampling. Our evaluation spanned 30,025 training and validation runs across different model configurations, totaling over one million core-hours of computation. Our results reveal that time offsets larger than 150 ms should be avoided in training datasets, and offsets beyond 300 ms can already significantly degrade the HAR performance for typical activities of daily living (ADLs). The findings highlight the need for adequate synchronization of training datasets. Our findings have implications for the design and deployment of multi-wearable HAR systems and may extend to other multi-sensor contexts.} }
@inproceedings{10.1145/3716554.3716834, title = {From Volunteers to Voters: Machine Learning Insights into Citizen Engagement}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {358--363}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716834}, url = {https://doi.org/10.1145/3716554.3716834}, author = {Tsoni, Rozita and Tolika, Maria and Karapiperis, Dimitris and Verykios, Vassilios}, abstract = {This study examines the link between volunteering and voting behavior. The main goal is to identify the key factors responsible for the impact on volunteers’ decisions to vote and build a predictive model to assess the likelihood of individual volunteers participating in elections. Data collected from 650 volunteers at the Paris 2024 Olympic Games were analyzed using machine learning methods, with Gradient Boosted Trees achieving the highest accuracy. These findings provide insights for enhancing civic engagement and voter turnout. Their responses were used to train a model aimed at predicting whether the participants had voted in the most recent elections. Various Machine Learning (ML) algorithms based on Decision Trees (DT) and kNN were employed for classification and prediction tasks. A key feature of the proposed approach is the use of an ML pipeline developed with an open-source visual programming tool, streamlining the entire process. The methodology encompasses a pre-processing stage and a modeling stage involving five classifiers. Among the algorithms, the Gradient Boosted Trees method demonstrated the highest performance with an accuracy of 0.92.} }
@inproceedings{10.1145/3718751.3718889, title = {Trading Stocks on RSI and KDJ: A Machine Learning Model}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {846--853}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718889}, url = {https://doi.org/10.1145/3718751.3718889}, author = {Chen, Jiawei}, keywords = {Machine learning, Regression model, Technical financial index}, abstract = {This study aims to forecast future stock prices by establishing statistical models, thereby aiding investors in making more informed investment decisions. Initially, this paper discusses in detail how to select the most suitable technical financial indicators for use as model inputs. Subsequently, statistical models targeted for stock price prediction are constructed using machine learning techniques. These models utilize meticulously filtered historical data to ensure the highest efficacy. Moreover, the study involves verification of the model's application across a variety of stocks, ensuring the universal applicability of the proposed prediction model. The empirical analysis results of this paper indicate that the constructed statistical models are capable of effectively predicting future stock price trends, providing a powerful decision-support tool for different types of investors, and thereby potentially enhancing investors' profitability in the complex and ever-changing financial markets.} }
@inproceedings{10.1145/3715335.3735488, title = {Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {530--546}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735488}, url = {https://doi.org/10.1145/3715335.3735488}, author = {Bhaskar, Mihir and Luo, Jun Tao and Geng, Zihan and Hajra, Asmita and Howell, Junia and Gormley, Matthew R.}, keywords = {historical document understanding, housing data, computer vision, OCR, machine learning, regression}, abstract = {Despite well-documented consequences of the U.S. government’s 1930s housing policies on racial wealth disparities, scholars have struggled to quantify its precise financial effects due to the inaccessibility of historical property appraisal records. Many counties still store these records in physical formats, making large-scale quantitative analysis difficult. We present an approach scholars can use to digitize historical housing assessment data, applying it to build and release a dataset for one county. Starting from publicly available scanned documents, we manually annotated property cards for over 12,000 properties to train and validate our methods. We use OCR to label data for an additional 50,000 properties, based on our two-stage approach combining classical computer vision techniques with deep learning-based OCR. For cases where OCR cannot be applied, such as when scanned documents are not available, we show how a regression model based on building feature data can estimate the historical values, and test the generalizability of this model to other counties. With these cost-effective tools, scholars, community activists, and policy makers can better analyze and understand the historical impacts of redlining.} }
@inproceedings{10.1145/3731806.3731840, title = {Enhancing Panic Attack Predictions: Addressing Data Imbalance in Machine Learning Models}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {1--5}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731840}, url = {https://doi.org/10.1145/3731806.3731840}, author = {Alliandre, Fazrul Ridha and Hikmawati, Erna}, keywords = {Data Balancing Strategy, Decision Tree, Machine Learning, Panic Attack Prediction, Random Forest}, abstract = {Panic disorder (PD) is a prevalent mental health condition characterized by recurring panic attacks, affecting millions globally. Despite advancements in machine learning (ML) for mental health predictions, data imbalance remains a significant challenge, often impairing model performance. This study aims to enhance the accuracy and reliability of panic attack prediction models by implementing a novel data balancing strategy. Using the College Student Mental Health Dataset, ML models—Decision Tree and Random Forest—were evaluated before and after applying data balancing techniques. Initial findings revealed suboptimal model performance due to imbalanced data, with AUC values of 0.434 (Decision Tree) and 0.527 (Random Forest). Post-balancing, significant improvements were observed, with AUC increasing to 0.768 and 0.612, respectively. This study highlights the critical role of data balancing in optimizing ML models, even with limited datasets, and provides a foundation for future research on scalable predictive technologies for mental health disorders. The findings underscore the potential of simple yet effective strategies in advancing early detection systems for mental health conditions.} }
@inproceedings{10.1145/3678299.3678313, title = {A machine learning approach to gesture detection in violin performance}, booktitle = {Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures}, pages = {144--151}, year = {2024}, isbn = {9798400709685}, doi = {10.1145/3678299.3678313}, url = {https://doi.org/10.1145/3678299.3678313}, author = {Lucena, Raquel and Ramirez, Rafael}, keywords = {audio features, gesture prediction, machine learning, pose estimation, violin, location = Milan, Italy}, abstract = {Playing a musical instrument is a highly complex activity. It requires mental and sensorimotor skills, which are learned during a long trajectory. Learning to play an instrument typically involves long periods of practice without teacher supervision. Systems providing feedback on the student’s performance during these self-study periods hold significant potential for improving the learning process. We present a machine learning approach to assess the correctness of body gestures in violin performances. We collect images and audio of several violinists performing correct and incorrect postures, extract image and audio descriptors, and apply machine learning algorithms to classify violin gestures. Finally, a real-time feedback system designed for pedagogical use is implemented, in which users receive visual feedback about whether the given gesture is performed properly or not. The system not only has the potential to facilitate the development of sensorimotor skills essential for playing violin, but also can enhance the learning experience for musicians, potentially providing benefits in musical education, performance, and health.} }
@inproceedings{10.1145/3700906.3700996, title = {Collaborative scheduling algorithm for full quantity materials based on process and machine learning}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {561--565}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700996}, url = {https://doi.org/10.1145/3700906.3700996}, author = {Gu, Sanlin}, keywords = {Artificial neural network, Full quantity of materials, Process design, Scheduling algorithm}, abstract = {Participants in the supply chain may have different information, leading to incomplete or inaccurate information when making decisions. To this end, a process and machine learning based collaborative scheduling algorithm for all materials is proposed. Design a health monitoring process for material supply chain based on R-tree dynamic indexing algorithm. Based on this, artificial neural networks in machine learning are applied to mine the data of the entire material supply chain. Through data mining, various data in the supply chain can be integrated and analyzed to improve information transparency and accuracy, and reduce information asymmetry. Adopting a dual layer scheduling model to achieve dual layer collaborative scheduling of materials. The experimental results show that the research method effectively improves the accuracy of data mining in the entire material supply chain, and the utilization rate of materials under this method is always higher than 95\%.} }
@inproceedings{10.1145/3729706.3729821, title = {Machine Learning for Predicting Tensile Properties of Fiber-Reinforced Metal Matrix Composites}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {729--733}, year = {2025}, isbn = {9798400712715}, doi = {10.1145/3729706.3729821}, url = {https://doi.org/10.1145/3729706.3729821}, author = {Gan, Guorong and Liu, Qiqing and Leng, Guoyang and Wang, Zhengcui and Lai, Daohui}, keywords = {Fiber-Metal Matrix Composites, Machine Learning, Process Optimization, Tensile Properties}, abstract = {Fiber-reinforced metal matrix composites (FMMCs) hold broad application prospects in numerous fields due to their excellent comprehensive properties. The tensile properties of these materials, as a key performance indicator, play a decisive role in their engineering applications. Therefore, leveraging the data processing advantages and the ability to identify complex relationships of machine learning (ML) technology can significantly enhance the study of the tensile properties of fiber-metal matrix composites. This paper provides a comprehensive overview of machine learning algorithms and models, as well as the current status of ML in predicting the performance of fiber-reinforced composites. It discusses the application of ML technology in predicting the tensile properties of fiber-metal matrix composites, optimizing manufacturing processes, and elucidating the relationship between microstructure and performance. Additionally, it explores the challenges faced by ML in predicting material properties and offers insights into future development directions, aiming to provide a reference for advancing research and applications in this field.} }
@inproceedings{10.1145/3748825.3748940, title = {Machine Learning-Assisted Public Opinion Segmentation and Diversified Policy Response Mechanism}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {742--747}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748940}, url = {https://doi.org/10.1145/3748825.3748940}, author = {Chen, Sinian}, keywords = {Graph Neural Network, Policy Response, Public Opinion Segmentation, Variational Autoencoder}, abstract = {In the age of information explosion, governments and related organizations are confronted with two issues: how to accurately grasp the scattered opinions in public and how to make an open and comprehensive policy response on time? In our work, we present an advanced machine learning framework, HT-GAPM (Hierarchical Topic-aware Variational Autoencoders and Graph-structured Adaptive Policy Matcher), for fine-grained opinion segmentation and diverse policy responses. Methodologically, HT-GAPM draws on the framework of state-of-the-art Transformer topic modeling and dynamic graph neural network, and can learn latent representations from multi-source public sentiment texts, which can capture semantic consistency and temporal evolution features, and the Graph-structured Adaptive Policy Matcher is designed to build a heterogeneous policy-opinion graph and accurately match segmented opinion groups with the most relevant policy templates using attention-weighted node representations. This model is the first to propose a feature enhanced dual optimization objective function that jointly improves the accuracy of opinion segmentation and the flexibility of policy response, while bias-sening and policy consistent are controlled via domain-prior regularization terms. Experiments show that the proposed model can get higher quality of opinion segmentation granularity and policy response diversity than state-of-the-art methods.} }
@inproceedings{10.1145/3627703.3629563, title = {Accelerating Privacy-Preserving Machine Learning With GeniBatch}, booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems}, pages = {489--504}, year = {2024}, isbn = {9798400704376}, doi = {10.1145/3627703.3629563}, url = {https://doi.org/10.1145/3627703.3629563}, author = {Huang, Xinyang and Zhang, Junxue and Cheng, Xiaodian and Zhang, Hong and Jin, Yilun and Hu, Shuihai and Tian, Han and Chen, Kai}, keywords = {batch compiler, homomorphic encryption, privacy-preserving machine learning, location = Athens, Greece}, abstract = {Cross-silo privacy-preserving machine learning (PPML) adopt; Partial Homomorphic Encryption (PHE) for secure data combination and high-quality model training across multiple organizations (e.g., medical and financial). However, PHE introduces significant computation and communication overheads due to data inflation. Batch optimization is an encouraging direction to mitigate the problem by compressing multiple data into a single ciphertext. While promising, it is impractical for a large number of cross-silo PPML applications due to the limited vector operations support and severe data corruption.In this paper, we present GeniBatch, a batch compiler that translates a PPML program with PHE into an efficient program with batch optimization. GeniBatch adopts a set of conversion rules to allow PHE programs involving all vector operations required in cross-silo PPML and ensures end-to-end result consistency before/after compiling. By proposing bit-reserving algorithms, GeniBatch avoids bit-overflow for the correctness of compiled programs and maximizes the compression ratio. We have integrated GeniBatch into FATE, a representative cross-silo PPML framework, and provided SIMD APIs to harness hardware acceleration. Experiments across six popular applications show that GeniBatch achieves up to 22.6 speedup and reduces network traffic by 5.4-23.8 for generic cross-silo PPML applications.} }
@inproceedings{10.1145/3638530.3664046, title = {Machine Learning for Evolutionary Computation - the Vehicle Routing Problems Competition}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {13--14}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3664046}, url = {https://doi.org/10.1145/3638530.3664046}, author = {Meng, Weiyao and Qu, Rong and Pillay, Nelishia}, keywords = {machine learning, evolutionary computation, meta-heuristics, vehicle routing, location = Melbourne, VIC, Australia}, abstract = {The Competition of Machine Learning for Evolutionary Computation for Solving Vehicle Routing Problems (ML4VRP) seeks to bring together machine learning and evolutionary computation communities to propose innovative techniques for vehicle routing problems (VRPs), aiming to advance machine learning-assisted evolutionary computation that works well across different instances of the VRPs. This paper overviews the key information of the competition.} }
@inproceedings{10.1145/3690624.3709229, title = {On the Hyperparameter Loss Landscapes of Machine Learning Models: An Exploratory Study}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1}, pages = {555--564}, year = {2025}, isbn = {9798400712456}, doi = {10.1145/3690624.3709229}, url = {https://doi.org/10.1145/3690624.3709229}, author = {Huang, Mingyu and Li, Ke}, keywords = {exploratory data analysis, fitness landscape analysis, hyperparameter optimization, location = Toronto ON, Canada}, abstract = {Previous efforts on hyperparameter optimization (HPO) of machine learning (ML) models predominately focus on algorithmic advances, yet little is known about the topography of the underlying hyperparameter (HP) loss landscape, which plays a fundamental role in governing the search process of HPO. While several works have conducted fitness landscape analysis (FLA) on various ML systems, they are limited to properties of isolated landscape without interrogating the potential structural similarities among landscapes induced on different scenarios. The exploration of such similarities can provide a novel perspective for understanding the mechanism behind modern HPO methods, but has been missing. In this paper, we mapped 1,500 HP loss landscapes of 6 representative ML models on 63 datasets across different fidelity levels, with 11M+ configurations. By conducting exploratory analysis on these landscapes with fine-grained visualizations and dedicated FLA metrics, we observed a similar landscape topography across a wide range of models, datasets, and fidelities, and shed light on the mechanism behind the success of several popular methods in HPO. The artifacts associated with this paper is available at https://github.com/COLA-Laboratory/GraphFLA.} }
@article{10.1145/3698831, title = {CtxPipe: Context-aware Data Preparation Pipeline Construction for Machine Learning}, journal = {Proc. ACM Manag. Data}, volume = {2}, year = {2024}, doi = {10.1145/3698831}, url = {https://doi.org/10.1145/3698831}, author = {Gao, Haotian and Cai, Shaofeng and Dinh, Tien Tuan Anh and Huang, Zhiyong and Ooi, Beng Chin}, keywords = {data analytics pipeline, data preparation}, abstract = {Machine learning models are only as good as their training data. Simple models trained on well-chosen features extracted from the raw data often outperform complex models trained directly on the raw data. Data preparation pipelines, which clean and derive features from the data, are therefore important for machine learning applications. However, constructing such pipelines is a resource-intensive process that involves deep human expertise.Our goal is to design an efficient framework for automatically finding high-quality data preparation pipelines. The main challenge is how to explore a large search space of pipeline components with the objective of computing features that maximize the performance of the downstream models. Existing solutions are limited in terms of feature quality, which results in low accuracies of the downstream models, while incurring significant runtime overhead. We present CtxPipe, a novel framework that addresses the limitations of previous works by leveraging contextual information to improve the pipeline construction process. Specifically, it uses pre-trained embedding models to capture the data semantics, which are then used to guide the selection of pipeline components. We implement CtxPipe with deep reinforcement learning and evaluate it against state-of-the-art automated pipeline construction solutions. Our comprehensive experiments demonstrate that CtxPipe outperforms all of the baselines in both model performance and runtime cost.} }
@inbook{10.1145/3745238.3745484, title = {Stock Decision-Making Based on Machine Learning Models and Z-Scores}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1570--1576}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745484}, author = {Wei, Jiawei}, abstract = {This study examines the performance of various machine learning models in stock decision-making based on the stock Z-scores. We employed four models—OLS linear regression, decision tree regression, support vector machine regression (SVM), and multilayer perceptron neural network (MLP). By leveraging Z-scores calculated by different models, stock trading strategies were constructed, and the models’ risk-adjusted returns and cumulative returns were assessed. The experimental results indicate that the decision tree model outperformed the others overall, especially in the unweighted case where it showed lower max drawdown and relatively lower risk. After weighting, the performances of the SVM and MLP models improved, with the decision tree regression model achieving the highest cumulative return.} }
@inproceedings{10.1109/SCW63240.2024.00127, title = {An Efficient Checkpointing System for Large Machine Learning Model Training}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {896--900}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00127}, url = {https://doi.org/10.1109/SCW63240.2024.00127}, author = {Xu, Wubiao and Huang, Xin and Meng, Shiman and Zhang, Weiping and Guo, Luanzheng and Sato, Kento}, abstract = {Checkpointing is one of the fundamental techniques to resume training while system fails. It has been generally used in various domains, such as high-performance computing (HPC) and machine learning (ML). However, as machine learning models increase in size and complexity rapidly, the cost of checkpointing in ML training became a bottleneck in storage and performance (time). For example, the latest GPT-4 model has massive parameters at the scale of 1.76 trillion. It is highly time and storage consuming to frequently writes the model to checkpoints with more than 1 trillion floating point values to storage. This work aims to understand and attempt to mitigate this problem. First, we characterize the checkpointing interface in a collection of representative large machine learning/language models with respect to storage consumption and performance overhead. Second, we propose the two optimizations: i) A periodic cleaning strategy that periodically cleans up outdated checkpoints to reduce the storage burden; ii) A data staging optimization that coordinates checkpoints between local and shared file systems for performance improvement. The experimental results with GPT-2 variants show that, overall the proposed optimizations significantly reduce the storage consumption to a constant while improves performance by average 2.1 for checkpointing in GPT-2 training.} }
@inproceedings{10.1145/3671151.3671253, title = {Using Machine Learning Methods to Select Stock Factors}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {575--580}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671253}, url = {https://doi.org/10.1145/3671151.3671253}, author = {Wang, Xiaoxi}, abstract = {In the intersection research of quantitative finance and big data, whether stock anomalies still exist under various statistical methods especially machine learning methods, and which characteristics could provide independent information have been widely discussed. To address these questions, this paper uses Fama-Macbeth and machine learning techniques to select stock characteristics and then examine them. Firstly, I construct 40 characteristics in finance and accounting from 1980 to 2016. Then perform Fama-Macbeth regressions to identify those with t\&gt;3. Next run Lasso regressions, and identify a few significant characteristics: B/M, Invest, Size, turnover, spread and illiquid. When loosen the penalty, profit, SUE, ROA, cash, mom1m, and IPO are selected. Other supervised learning methods are also implemented. Linear model, penalized models and boosting perform well out of sample. By using big data techniques and machine learning methods, this paper provides new evidence on the traditional finance research questions.} }
@inproceedings{10.1145/3721888.3722092, title = {Edge Acceleration of LiDAR Frame Transmission with In-network Machine Learning}, booktitle = {Proceedings of the 8th International Workshop on Edge Systems, Analytics and Networking}, pages = {13--18}, year = {2025}, isbn = {9798400715594}, doi = {10.1145/3721888.3722092}, url = {https://doi.org/10.1145/3721888.3722092}, author = {Qian, Peng and Zheng, Changgang and Zilberman, Noa}, keywords = {in-network machine learning, P4, edge computing, vehicle perception, location = World Trade Center, Rotterdam, Netherlands}, abstract = {In real-time vehicle perception scenarios, ensuring timely and stable transmission of LiDAR data between vehicles and the network edge is crucial for accurate object detection. However, the inherent variability of wireless links, coupled with the added impact of vehicle mobility, leads to inevitable packet loss and latency jitter, compromising both the timeliness and accuracy of vehicle perception. To address this challenge, we introduce a packet duplication mechanism on dual wireless links, improving LiDAR frame transmission performance. The solution is driven by an integrated In-Network Machine Learning module at a programmable edge device that dynamically detects performance degradation and controls packet duplication. Through practical implementation and extensive evaluation, it is demonstrated that the proposed packet duplication function can effectively address uncertainties in LiDAR frame transmission, while achieving 50\% reduction in transmission times.} }
@inproceedings{10.1145/3647444.3647843, title = {Qualitative Analysis on Machine Learning Through Biblioshiny}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647843}, url = {https://doi.org/10.1145/3647444.3647843}, author = {M, Vidya and Anifa, Mansurali}, abstract = {Machine learning augments business firms by enhancing their business operations and by reduction in costs. It assists business houses to visualize the historical patterns and to envisage future decisions. Machine learning applies algorithms and models to assess the data patterns. Sentiment analysis is a form of Natural Language Processing for regulating the feedback from different ends. The study analysed 620 global publications pertaining to Machine learning and sentiment analysis indexed in Scopus database. The article demonstrated a Three field plot portrays the relationship between authors, countries, and keywords; authors' influence on sources; word counts and word growth; and a collaborative network of papers. The widely held articles of Machine Learning and Sentiment Analysis is published as journal articles, and the number of publications is continuously increasing. In a ranking most productive nations, India came out with the highest citations (1054), followed by USA (504), and Chine (354). The productive authors in the field of Machine Learning and Sentiment Analysis were Wang X has contributed 5 articles followed by Li Z , Wang Y, Yaqub U, with 4 articles. The Journal Information Processing and Management had 24 Publications around the world and also has the total citations of 763 followed by Artificial Intelligence Review (475 Citations). University of Florida was the contributing majority of 21 articles followed by The Hongkong Polytechnic University with 16 articles.} }
@article{10.1145/3710966, title = {Equality Engine: Fostering Critical Machine Learning Bias Literacy Through a Transformational Game}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3710966}, url = {https://doi.org/10.1145/3710966}, author = {Showkat, Dilruba and Wang, Lingqing and Chan, Laveda and To, Alexandra}, keywords = {AI/ML literacy, ML biases, ML ethics, game design, machine learning, transformational games}, abstract = {Machine Learning (ML) is now integrated from everyday technologies to sophisticated infrastructures, providing fast, efficient, and scalable decision-making services, with increasing evidence of ML perpetuating invisible harms and biases. The hidden and socio-technical nature of ML biases can make them difficult to detect and prevent without proper literacy. To investigate this, we developed a novel online multiplayer board game Equality Engine, where players learn about various ML biases and debiasing techniques. We conducted a mixed-method formative playtest case study to solicit feedback from post-secondary students (N = 12) with a range of ML experience. We found that students' self-reported ML bias-debias knowledge improved significantly after playing the game. The game was perceived as easy to use because of the social interaction and immersion the game enabled. Students would also use the game in the future because of the self-reported knowledge gained from the game. Although these positive results may be influenced by measurement bias, our study contributes to the design of an approachable game, which not only facilitates exposure, collaboration, and opportunities for critical reflection on ML biases but also provides recommendations for future game designs that can facilitate ML ethics discourse and literacy among a broader audience.} }
@inproceedings{10.1145/3643659.3643927, title = {Automated Boundary Identification for Machine Learning Classifiers}, booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing}, pages = {1--8}, year = {2024}, isbn = {9798400705625}, doi = {10.1145/3643659.3643927}, url = {https://doi.org/10.1145/3643659.3643927}, author = {Dobslaw, Felix and Feldt, Robert}, abstract = {AI and Machine Learning (ML) models are increasingly used as (critical) components in software systems, even safety-critical ones. This puts new demands on the degree to which we need to test them and requires new and expanded testing methods. Recent boundary-value identification methods have been developed and shown to automatically find boundary candidates for traditional, non-ML software: pairs of nearby inputs that result in (highly) differing outputs. These can be shown to developers and testers, who can judge if the boundary is where it is supposed to be.Here, we explore how this method can identify decision boundaries of ML classification models. The resulting ML Boundary Spanning Algorithm (ML-BSA) is a search-based method extending previous work in two main ways. We empirically evaluate ML-BSA on seven ML datasets and show that it better spans and thus better identifies the entire classification boundary(ies). The diversity objective helps spread out the boundary pairs more broadly and evenly. This, we argue, can help testers and developers better judge where a classification boundary actually is, compare to expectations, and then focus further testing, validation, and even further training and model refinement on parts of the boundary where behaviour is not ideal.} }
@inproceedings{10.1145/3689031.3717496, title = {Heimdall: Optimizing Storage I/O Admission with Extensive Machine Learning Pipeline}, booktitle = {Proceedings of the Twentieth European Conference on Computer Systems}, pages = {1109--1125}, year = {2025}, isbn = {9798400711961}, doi = {10.1145/3689031.3717496}, url = {https://doi.org/10.1145/3689031.3717496}, author = {Kurniawan, Daniar H. and Putri, Rani Ayu and Qin, Peiran and Zulkifli, Kahfi S. and Sinurat, Ray A. O. and Bhimani, Janki and Madireddy, Sandeep and Kistijantoro, Achmad Imam and Gunawi, Haryadi S.}, keywords = {Distributed systems, File and storage systems, I/O admission control, ML for systems, Operating systems, location = Rotterdam, Netherlands}, abstract = {This paper introduces Heimdall, a highly accurate and efficient machine learning-powered I/O admission policy for flash storage, designed to operate in a black-box manner. We make domain-specific innovations in various ML stages by introducing accurate period-based labeling, 3-stage noise filtering, in-depth feature engineering, and fine-grained tuning, which together improve the decision accuracy from 67\% up to 93\%. We perform various deployment optimizations to reach a sub-μs inference latency and a small, 28KB, memory overhead. With 500 unbiased random experiments derived from production traces, we show Heimdall delivers 15-35\% lower average I/O latency compared to the state of the art and up to 2x faster to a baseline. Heimdall is ready for user-level, in-kernel, and distributed deployments.} }
@proceedings{10.1145/3630048, title = {DistributedML '23: Proceedings of the 4th International Workshop on Distributed Machine Learning}, year = {2023}, isbn = {9798400704475}, abstract = {Following up the prior three successful versions of DistributedML, it is our great honour and pleasure to welcome you again, this time physically in the 4rd edition of the Distributed Machine Learning Workshop (DistributedML '23). The workshop is co-located with the 19th International Conference on emerging Networking EXperiments and Technologies (CoNEXT '23) and held in Paris, France, on the 8th of December 2023.Distributed ML is a rapidly evolving, interdisciplinary field bringing together techniques from Distributed Systems, Networks and Machine Learning. With Deep Learning at the forefront, we are seeing an explosion of AI-driven technologies, from immersive VR experiences and smart digital assistants to advanced robotics and autonomous vehicles. These applications not only challenge the limits of local device capabilities but also many times necessitate a shift towards distributed models of computation to enhance performance and efficiency, while respecting privacy and sustainability. At the cornerstone of innovation, foundational models further push the boundaries of today's computational infrastructure. Therefore, scaling up to support the new training workloads and efficiently deploying Large Language or Vision Models become key research areas.} }
@inproceedings{10.1145/3651671.3651689, title = {Study on The Effect of Encoding Method in Quantum Machine Learning}, booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing}, pages = {94--99}, year = {2024}, isbn = {9798400709234}, doi = {10.1145/3651671.3651689}, url = {https://doi.org/10.1145/3651671.3651689}, author = {Xiong, Qingqing and Jiang, Jinzhe and Li, Chen and Zhang, Xin and Zhao, Yaqian}, keywords = {Data encoding, Metrics, Quantum machine learning, location = Shenzhen, China}, abstract = {Quantum machine learning algorithms use qubit encoding and quantum circuits to perform feature extraction and pattern recognition. However, the choice of data encoding methods can have a significant impact on the model’s performance. To evaluate the effect of different encoding methods in a systematic way, we propose two metrics: distribution distance and distribution radius. These metrics describe how the encoded data distribute in the Hilbert space. We show that there is a positive correlation between prediction accuracy and distribution distance, and a negative correlation between prediction accuracy and distribution radius, both theoretically and experimentally. Based on our findings, we suggest a comparative evaluation of data encoding methods for quantum machine learning, which can help improve the learning efficiency.} }
@inproceedings{10.1145/3745034.3745122, title = {Predicting Low Bone Density Based on Interpretable Machine Learning Models}, booktitle = {Proceedings of the 4th International Conference on Biomedical and Intelligent Systems}, pages = {576--581}, year = {2025}, isbn = {9798400714399}, doi = {10.1145/3745034.3745122}, url = {https://doi.org/10.1145/3745034.3745122}, author = {Zhang, Shuaiqiong and Lu, Jingbo}, keywords = {Low bone density, SHAP, machine learning, osteoporosis}, abstract = {Osteoporosis has an insidious onset and low bone density is an early stage of osteoporosis, in order to recognize and intervene in osteoporosis at an early stage, this study aimed to develop a predictive model for low bone density. This study used National Health and Nutrition Examination Survey (NHANES) data from 2013-2014 and 2017-2018 with Decision Tree (DT), Random Forest (RF), Gradient Boosting Decision Tree (GBDT) and XGBoost algorithms to construct the prediction model, and SHAP algorithm for visual interpretation. The results showed that the best prediction model, XGBoost, had an AUC value of 0.777, which ultimately determined that the characteristics of gender, age, obesity or not, race, marital status, number of co-morbidities, and dietary pattern had a good prediction effect. The prediction model of low BMD constructed in this study can help healthcare professionals to recognize low BMD as early as possible and develop targeted measures to prevent osteoporosis.} }
@inproceedings{10.1145/3647750.3647781, title = {Retailers' Order Decision with Setup Cost using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing}, pages = {37--44}, year = {2024}, isbn = {9798400716546}, doi = {10.1145/3647750.3647781}, url = {https://doi.org/10.1145/3647750.3647781}, author = {Jintanasonti, Pissacha and Dumrongsiri, Aussadavut and Tantiwattanakul, Phattarasaya}, keywords = {Artificial Neural Network, Machine Learning, Mathematical Model, Supply Chain, Wholesale Price Strategy, location = Singapore, Singapore}, abstract = {The objective of this study was to gain valuable insights into retailer behavior and develop a predictive model to inform their purchasing decisions. This process involved a comprehensive analysis of the various factors that influence retailers when they make choices regarding product or service purchases. To gather the data for the study, a simple random sampling method was employed to extract retailer purchase data from a mathematical model using Excel Solver. While Excel Solver can determine optimal solutions quickly but, with hundreds of retailers, reoptimizing the model every time a price is changed it is not practical and the manufacturer must contact trial and error many times to optimize has price. To address this challenge, Artificial Neural Network techniques were utilized to analyze the sample data. The resulting equations were subsequently integrated into manufacturer model to assist manufacturers in forecasting retailer decisions. With an understanding of the expected patterns of retailer behavior, manufacturers can strategically plan their purchase orders to align with different promotion and marketing strategies. The study demonstrated that the model achieved an average minimum cost increase of 5.02\% when tested with a new dataset consisting of 5,000 retailers. Based on these findings, it is recommended that manufacturers adjust their order policies by placing orders at the beginning of period 2 and making the most of the earliest discount period. Furthermore, manufacturers should consider a range of factors, including total holding cost, reorder cost, and expected demand, when formulating their order policies. This comprehensive approach will help manufacturers optimize their purchasing decisions and enhance their overall operational efficiency.} }
@inproceedings{10.1145/3639477.3639746, title = {Resolving Code Review Comments with Machine Learning}, booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice}, pages = {204--215}, year = {2024}, isbn = {9798400705014}, doi = {10.1145/3639477.3639746}, url = {https://doi.org/10.1145/3639477.3639746}, author = {Froemmgen, Alexander and Austin, Jacob and Choy, Peter and Ghelani, Nimesh and Kharatyan, Lera and Surita, Gabriela and Khrapko, Elena and Lamblin, Pascal and Manzagol, Pierre-Antoine and Revaj, Marcus and Tabachnyk, Maxim and Tarlow, Daniel and Villela, Kevin and Zheng, Daniel and Chandra, Satish and Maniatis, Petros}, abstract = {Code reviews are a critical part of the software development process, taking a significant amount of the code authors' and the code reviewers' time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see millions of reviewer comments per year, and authors require an average of ~60 minutes active shepherding time between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must devote to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code-review process, e.g., by proposing code changes based on a comment's text.We describe our application of recent advances in large sequence models in a real-world setting to automatically resolve code-review comments in the day-to-day development workflow at Google. We present the evolution of this feature from an asynchronous generation of suggested edits after the reviewer sends feedback, to an interactive experience that suggests code edits to the reviewer at review time. In deployment, code-change authors at Google address 7.5\% of all reviewer comments by applying an ML-suggested edit. The impact of this will be to reduce the time spent on code reviews by hundreds of thousands of engineer hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers' productivity and allows them to focus on more creative and complex tasks.} }
@article{10.5555/3722479.3722527, title = {Studying Financial Data with Macroeconomic Factors Using Machine Learning}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {151--163}, year = {2024}, issn = {1937-4771}, author = {Anem, Sai Sravya and Amiruzzaman, Md and Bhuiyan, Ashik Ahmed}, abstract = {This paper focuses on the prediction of stock indices through machine learning, focusing on macroeconomic factors and market sentiment generation. It centers on major US stock index funds, notably the S\&amp;P 500, and their correlation with key economic indicators like GDP, unemployment, CPI, money supply, and retail sales. Utilizing economic data from diverse sources such as the Federal Reserve, NASDAQ, and news websites, the study cleans and transforms datasets to estimate quarterly fund returns. Employing tree-based algorithms, particularly XGBoost, enables accurate predictions. Moreover, the paper evaluates index forecast performance across various market cycles and geopolitical events. It also uses traditional NLP methods and large language models to explore market sentiment generation, offering comprehensive insights. In essence, this paper sheds light on the predictive power of macroeconomic factors on stock indices and the nuances of market sentiment analysis, leveraging both conventional and advanced techniques.} }
@inproceedings{10.1145/3651671.3651683, title = {Automatic Machine Learning based Real Time Multi-Tasking Image Fusion}, booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing}, pages = {327--333}, year = {2024}, isbn = {9798400709234}, doi = {10.1145/3651671.3651683}, url = {https://doi.org/10.1145/3651671.3651683}, author = {Karim, Shahid and Tong, Geng and Li, Jinyang and Yu, Xiaochang and Hao, Jia and Yu, Yiting}, keywords = {automatic ML, imaging systems, multi-tasking image fusion, location = Shenzhen, China}, abstract = {Imaging systems work diversely in the image processing domain, and each system contains specific characteristics. We are developing models to fuse images from different sensors and environments to get promising outcomes for different computer vision applications. The multiple unified models have been developed for multiple tasks such as multi-focus (MF), multi-exposure (ME), and multi-modal (MM) image fusion. The careful tuning of such models is required to get optimal results, which are still not applicable to diverse applications. We propose an automatic machine learning (AML) based multi-tasking image fusion approach to overcome this problem. Initially, we evaluate source images with AML and feed them to the task-based models. Then, the source images are fused with the pre-trained and fine-tuned models. The experimental results authenticate the consequences of our proposed approach compared to generic approaches.} }
@article{10.14778/3742728.3742753, title = {Robust Plan Evaluation Based on Approximate Probabilistic Machine Learning}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {2626--2638}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3742728.3742753}, url = {https://doi.org/10.14778/3742728.3742753}, author = {Kamali, Amin and Kantere, Verena and Zuzarte, Calisto and Corvinelli, Vincent}, abstract = {Query optimizers in RDBMSs search for execution plans expected to be optimal for given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select plans that are suboptimal at runtime, if estimates and assumptions are not valid. Therefore, they do not sufficiently support robust query optimization. Using ML to improve data systems has shown promising results for query optimization. Inspired by this, we propose Robust Query Optimizer, (Roq), a holistic framework based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq includes a novel learned cost model that is designed to predict the cost of query execution and the associated risks and performs query optimization accordingly. We demonstrate that Roq provides significant improvements in robust query optimization compared with the state-of-the-art.} }
@inproceedings{10.1145/3676536.3676696, title = {CAMSHAP: Accelerating Machine Learning Model Explainability with Analog CAM}, booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design}, year = {2025}, isbn = {9798400710773}, doi = {10.1145/3676536.3676696}, url = {https://doi.org/10.1145/3676536.3676696}, author = {Moon, John and Pedretti, Giacomo and Bruel, Pedro and Serebryakov, Sergey and Eldash, Omar and Buonanno, Luca and Graves, Catherine E. and Faraboschi, Paolo and Ignowski, Jim}, keywords = {explainability, shapley additive explanation, XGBoost, content-addressable memory, location = Newark Liberty International Airport Marriott, New York, NY, USA}, abstract = {The recent success of machine learning (ML) models has led to increasing demands for model explanations - why a result was given - along with model predictions. Tree-based ML models are considered more explainable than deep neural networks and higher performers in several domains. However, algorithms computing model explanations are irregular and scale poorly with model size. While many custom accelerators for training and inference have been proposed, little attention has been paid to accelerating model explanations. This lack of explanatory capability has limited the use of these models for real-time decision-making systems in critical fields such as healthcare, autonomous operation and cybersecurity.In this paper, we propose an architecture called CAMSHAP based on analog content-addressable memory (CAM) for accelerating tree-based ML model explanations. Parallel search in CAMs and reduction through an H-Tree network on chip resolve load imbalance and reduction overhead challenges, which limit tree-based ML model efficiency on GPU. We build a cycle-accurate and functional simulator as well as custom instructions to demonstrate our approach. Studies on device variation also show the robustness of CAMSHAP and resilience to device noise. On evaluations across 5 different datasets, CAMSHAP shows 10 higher throughput, 191 smaller latency, and 41 smaller energy consumption compared to GPU for explaining tree-based ML model outputs.} }
@inproceedings{10.1145/3717664.3717668, title = {Exploring Explainable Machine Learning Models for Corporate Investment Decision Prediction}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {18--22}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717668}, url = {https://doi.org/10.1145/3717664.3717668}, author = {Chen, Yiwen}, keywords = {Decision Tree, Investment Forecasting, Machine Learning, Random Forest}, abstract = {In the current economic environment, the investment decisions of firms are of crucial importance to ensure continuous and stable growth. The present study explores the potential of explanatory problem machine learning models in predicting a firm's debt-to-asset ratio (D/A ratio) with the aim of providing better assistance to firms' investment decisions. The study utilizes a substantial dataset, comprising 57,522 corporate data points obtained from the CSMAR database between 2000 and 2022, encompassing over 80 diverse industries. The study employs a multifaceted approach to data preprocessing, incorporating the K-means clustering method, a random forest model to identify the most pertinent feature variables associated with transmission ratios, and a multistage decision tree with pruning for model optimization. The model's accuracy, as gauged by five cross-validations, approaches 99.95\%, suggesting its high precision. The study's findings indicate that the debt-to-equity ratio and enterprise size are pivotal factors influencing enterprise investment decisions. An in-depth examination of these factors offers valuable insights for the formulation of future corporate investment strategies and risk management. Furthermore, the methodology and findings of this study establish a framework for developing students' ability to utilize artificial intelligence to address future challenges in the field of educational technology. The dataset and code can be accessed at https://github.com/yuzengyi/CIDP.} }
@inproceedings{10.1145/3708036.3708233, title = {Predictive Analysis of Vehicle Insurance Demand Using Machine Learning Techniques}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {1193--1197}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708233}, url = {https://doi.org/10.1145/3708036.3708233}, author = {Sun, Mingwei}, keywords = {Data Analyze, Health Insurance, Machine learning, Purchase Perception, Vehicle Insurance}, abstract = {Health insurance and vehicle insurance play a crucial role in reducing economic burdens, alleviating psychological stress, and maintaining social stability. Given the similarities in buyer characteristics for both types of insurance, predicting whether customers who purchase health insurance are interested in vehicle insurance is of significant importance for companies offering both products. By analyzing data from the Kaggle website, this paper employs machine learning techniques, including XGBoost, AdaBoost, and Multi-Layer Perceptron (MLP), to predict customer interest in purchasing vehicle insurance and compares the performance of different models. The study results indicate that AdaBoost performs best in predicting vehicle insurance demand, followed by XGBoost, while MLP performs relatively weaker in this task. These findings provide important insights for insurance companies to optimize marketing strategies, adjust pricing models, and better manage risks. However, future research should focus on integrating more advanced technologies and improving data quality to further enhance the performance of predictive models.} }
@inproceedings{10.1145/3644815.3644943, title = {Engineering Carbon Emission-aware Machine Learning Pipelines}, booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI}, pages = {118--128}, year = {2024}, isbn = {9798400705915}, doi = {10.1145/3644815.3644943}, url = {https://doi.org/10.1145/3644815.3644943}, author = {Husom, Erik Johannes and Sen, Sagar and Goknil, Arda}, abstract = {The proliferation of machine learning (ML) has brought unprecedented advancements in technology, but it has also raised concerns about its environmental impact, particularly concerning carbon emissions. To address the imperative of environmentally responsible ML, we present in this paper a novel ML pipeline, named CEMAI, designed to monitor and analyze carbon emissions across the entire lifecycle of ML model development, from data preparation to training and deployment. Our endeavor involves an exhaustive evaluation process underpinned by three industrial case studies. These case studies are structured around the application of ML models to predict tool wear, estimate remaining useful lifetimes, and detect anomalies in the Industrial Internet of Things (IIoT). Leveraging sensor data originating from CNC machining and broaching operations, our research shows empirically the efficacy of carbon emissions as a dependable metric guiding the configuration of an ML development process. The essence of our approach lies in striking a balance between superior performance and minimal carbon emissions. Our findings reveal the potential to optimize pipeline configurations for ML models in a manner that not only enhances performance but also drastically reduces carbon emissions, thereby underlining the significance of adopting ecologically responsible engineering practices.} }
@article{10.1145/3672451, title = {Keeper: Automated Testing and Fixing of Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {33}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3672451}, url = {https://doi.org/10.1145/3672451}, author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Hoffmann, Henry and Maire, Michael and Lu, Shan}, keywords = {Software testing, machine learning, machine learning API}, abstract = {The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior.Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78\% of end-users and 95\% of developers agree with Keeper’s detection and fixing results.} }
@inproceedings{10.1145/3677052.3698650, title = {Machine Learning-based Relative Valuation of Municipal Bonds}, booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance}, pages = {634--642}, year = {2024}, isbn = {9798400710810}, doi = {10.1145/3677052.3698650}, url = {https://doi.org/10.1145/3677052.3698650}, author = {Saha, Preetha and Lyu, Jasmine and Desai, Dhruv and Chauhan, Rishab and Jeyapaulraj, Jerinsh and Chu, Peter and Sommer, Philip and Mehta, Dhagash}, keywords = {CatBoost, Municipal Bonds, Relative Valuation, Similarity Learning, location = Brooklyn, NY, USA}, abstract = {The trading ecosystem of the Municipal (muni) bond is complex and unique. With nearly 2\% of securities from over a million securities outstanding trading daily, determining the value or relative value of a bond among its peers is challenging. Traditionally, relative value calculation has been done using rule-based or heuristics-driven approaches, which may introduce human biases and often fail to account for complex relationships between the bond characteristics. We propose a data-driven model to develop a supervised similarity framework for the muni bond market based on CatBoost algorithm. This algorithm learns from a large-scale dataset to identify bonds that are similar to each other based on their risk profiles. This allows us to evaluate the price of a muni bond relative to a cohort of bonds with a similar risk profile. We propose and deploy a back-testing methodology to compare various benchmarks and the proposed methods and show that the similarity-based method outperforms both rule-based and heuristic-based methods.} }
@inproceedings{10.1145/3626246.3656000, title = {The Limitations of Data, Machine Learning and Us}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {1--2}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3656000}, url = {https://doi.org/10.1145/3626246.3656000}, author = {Baeza-Yates, Ricardo}, keywords = {ai ethics, bias, legitimacy, ml evaluation, pseudoscience, location = Santiago AA, Chile}, abstract = {Machine learning (ML), particularly deep learning, is being used everywhere. However, not always is applied well or has ethical and/or scientific issues. In this keynote we first do a deep dive in the limitations of supervised ML and data, its key input. We cover small data, datification, bias, and evaluating success instead of harm, among other limitations. The second part is about ourselves using ML, including different types of social limitations and human incompetence such as cognitive biases, pseudoscience, or unethical applications. These limitations have harmful consequences such as discrimination, misinformation, and mental health issues, to mention just a few. In the final part we discuss regulation on the use of AI and responsible principles that can mitigate the problems outlined above.} }
@inproceedings{10.1145/3712255.3734248, title = {Evolutionary Co-Optimization of Rule Shape and Fuzziness in Rule-Based Machine Learning}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {71--72}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3734248}, url = {https://doi.org/10.1145/3712255.3734248}, author = {Shiraishi, Hiroki and Hayamizu, Yohei and Hashiyama, Tomonori and Takadama, Keiki and Ishibuchi, Hisao and Nakata, Masaya}, keywords = {learning classifier systems, knowledge representation, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Rule-based machine learning systems face a fundamental representation challenge: traditional approaches require a priori selection between crisp intervals and/or fuzzy membership functions. This can result in either overly complex fuzzy rule sets or insufficiently expressive crisp rule sets. To address this limitation, we introduce a novel evolutionary approach for learning classifier system (LCS) machine learning algorithms that co-optimizes both rule shape and fuzziness using a four-parameter beta distribution. Our method integrates specialized genetic operators with generalization pressure mechanisms, such as subsumption and crispification operators, to favor crisp, interpretable rules when possible. Experiments on real-world classification tasks demonstrate competitive accuracy compared to state-of-the-art black-box models while maintaining superior interpretability. Our method can automatically determine appropriate rule representations for different feature space regions, evolving toward simpler crisp rules where possible while retaining fuzzy rules only where necessary for handling complex decision boundaries.This paper summarizes the following IEEE TEVC article: Hiroki Shiraishi, Yohei Hayamizu, Tomonori Hashiyama, Keiki Takadama, Hisao Ishibuchi, and Masaya Nakata. 2025. Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems. IEEE Transactions on Evolutionary Computation. https://doi.org/10.1109/TEVC.2025.3550915 [1]. Our implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS.} }
@inproceedings{10.1109/ICSE55347.2025.00066, title = {Answering User Questions about Machine Learning Models through Standardized Model Cards}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {1488--1500}, year = {2025}, isbn = {9798331505691}, doi = {10.1109/ICSE55347.2025.00066}, url = {https://doi.org/10.1109/ICSE55347.2025.00066}, author = {Toma, Tajkia Rahman and Grewal, Balreet and Bezemer, Cor-Paul}, keywords = {machine learning model hubs, model cards, questions \&amp, answers, hugging face, location = Ottawa, Ontario, Canada}, abstract = {Reusing pre-trained machine learning models is becoming very popular due to model hubs such as Hugging Face (HF). However, similar to when reusing software, many issues may arise when reusing an ML model. In many cases, users resort to asking questions on discussion forums such as the HF community forum. In this paper, we study how we can reduce the community's workload in answering these questions and increase the likelihood that questions receive a quick answer. We analyze 11,278 discussions from the HF model community that contain user questions about ML models. We focus on the effort spent handling questions, the high-level topics of discussions, and the potential for standardizing responses in model cards based on a model card template. Our findings indicate that there is not much effort involved in responding to user questions, however, 40.1\% of the questions remain open without any response. A topic analysis shows that discussions are more centered around technical details on model development and troubleshooting, indicating that more input from model providers is required. We show that 42.5\% of the questions could have been answered if the model provider followed a standard model card template for the model card. Based on our analysis, we recommend that model providers add more development-related details on the model's architecture, algorithm, data preprocessing and training code in existing documentation (sub)sections and add new (sub)sections to the template to address common questions about model usage and hardware requirements.} }
@inproceedings{10.1145/3712255.3726556, title = {Evaluating the Generalizability of Machine Learning Pipelines When Using Lexicase or Tournament Selection}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {283--286}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726556}, url = {https://doi.org/10.1145/3712255.3726556}, author = {Hernandez, Jose Guadalupe and Saini, Anil Kumar and Gupta, Ankit and Moore, Jason}, keywords = {AutoML, tournament selection, lexicase selection, parent selection, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Evolutionary Algorithms have been successfully applied in Automated Machine Learning (AutoML) to evolve effective machine learning (ML) pipelines. Here, we use 12 OpenML classification tasks and the AutoML tool TPOT2 to assess the impact of lexicase and tournament selection on the generalizability of pipelines. We use one of five stratified sampling splits to generate training and validation sets; pipelines are trained on the training set, and predictions are made on the validation set. Lexicase and tournament selection use these predictions to identify parents. At the end of a run, TPOT2 returns the pipeline that achieved the best validation accuracy while maintaining the lowest complexity. The generalizability of this pipeline is assessed using the test set provided for an OpenML task. We found that lexicase produced pipelines with higher validation accuracy than tournament selection in all tasks for at least one split. In contrast, tournament selection produced pipelines with greater generalizability for 10 of the 12 tasks on at least one split. For most cases where tournament selection outperformed lexicase on test accuracy, we detected differences in validation accuracy and pipeline complexity.} }
@article{10.1145/3677119, title = {Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3677119}, url = {https://doi.org/10.1145/3677119}, author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag}, keywords = {Explainability in ML, counterfactual explanations, algorithmic recourse, interpretability in ML}, abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this article, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.} }
@inproceedings{10.1145/3600211.3604753, title = {Advancing Health Equity with Machine Learning}, booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {955--956}, year = {2023}, isbn = {9798400702310}, doi = {10.1145/3600211.3604753}, url = {https://doi.org/10.1145/3600211.3604753}, author = {Mhasawade, Vishwali}, keywords = {causal inference, fairness, health disparities, health equity, location = Montr\'eal, QC, Canada}, abstract = {Social privilege in terms of power, wealth, and prestige is the driver of avoidable health inequities. But today, machine learning systems in healthcare are largely focused on data and systems within hospitals and clinics, ignoring the factors that lead to health disparities across communities. The primary goal of my research is to understand the drivers of population health inequity and design fair and equitable machine learning systems for mitigating health disparities. In order to do this, I mainly focus on causal inference and machine learning methods using data from multiple environments, such as geographical locations and hospitals, to identify and address inequities in health and healthcare.} }
@inproceedings{10.1145/3674399.3674469, title = {Access Structure Selection for Knowledge Graphs Based on Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024}, pages = {214--215}, year = {2024}, isbn = {9798400710117}, doi = {10.1145/3674399.3674469}, url = {https://doi.org/10.1145/3674399.3674469}, author = {Qi, Zhixin and Wang, Hongzhi}, keywords = {Index Selection, Knowledge Graph, Machine Learning, Performance Prediction, Physical Design Tuning, Storage Structure, location = Changsha, China}, abstract = {In recent years, the rapid development of machine learning technology has provided opportunities for the automatic access structure selection of knowledge graph data. Considering that machine learning is suitable to describe the complex patterns and solve the complex optimization problems, this paper adopts machine learning techniques to predict the performance of knowledge graph storage structures, tune the storage structure of a knowledge graph, and select the index configurations for a knowledge graph automatically.} }
@inproceedings{10.1145/3658644.3690304, title = {Analyzing Inference Privacy Risks Through Gradients In Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {3466--3480}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690304}, url = {https://doi.org/10.1145/3658644.3690304}, author = {Li, Zhuohang and Lowy, Andrew and Liu, Jing and Koike-Akino, Toshiaki and Parsons, Kieran and Malin, Bradley and Wang, Ye}, keywords = {inference privacy, machine learning, location = Salt Lake City, UT, USA}, abstract = {In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.} }
@inproceedings{10.1145/3742460.3742986, title = {Adaptive Water pH Sensing in Variable Conditions Using Near Infrared Imaging and Machine Learning}, booktitle = {Proceedings of the International Workshop on Environmental Sensing Systems for Smart Cities}, pages = {20--25}, year = {2025}, isbn = {9798400719868}, doi = {10.1145/3742460.3742986}, url = {https://doi.org/10.1145/3742460.3742986}, author = {Khmaissia, Fadoua and Ravi, Nirupama}, keywords = {water quality sensing, near-infrared imaging, machine learning, ubiquitous sensing, location = Hilton Anaheim, Anaheim, CA, USA}, abstract = {We present a preliminary study on non-invasive water pH sensing using consumer-grade near-infrared (NIR) imaging and machine learning, targeting mobile and field applications. Our main contribution is the NIR-pH dataset, which systematically captures NIR reflectance images of water samples with varying pH levels (4.01–9.18) under diverse environmental conditions, including changes in lighting, distance, volume, and container position. Employing an attention-based neural network, our approach achieves 85.8\% accuracy in classifying water pH, with interpretability analyses confirming the model's focus on relevant water surface features. This work aims to establish a foundation for portable, accessible water quality monitoring and introduces a new machine learning task and dataset to support further research in ubiquitous environmental sensing.} }
@inproceedings{10.1145/3686081.3686118, title = {Application of CPU in AI and Machine Learning}, booktitle = {Proceedings of the International Conference on Decision Science \&amp; Management}, pages = {221--224}, year = {2024}, isbn = {9798400718151}, doi = {10.1145/3686081.3686118}, url = {https://doi.org/10.1145/3686081.3686118}, author = {Yu, Renchao}, keywords = {AI, CPU, Heterogeneous, Machine learning, Neural network, System architecture}, abstract = {In recent years, the burgeoning fields of Artificial Intelligence (AI) and Machine Learning (ML) have increasingly permeated daily life, catalyzing significant advancements in technology. This rapid evolution has, in turn, necessitated the continual adaptation and enhancement of Central Processing Units (CPUs), which remain at the forefront of computational hardware. Despite these advancements, there has been a noticeable dearth in scholarly literature focusing on training neural network models exclusively on CPUs. This gap presents a substantial impediment to the exploration and advancement of CPU applications within AI and ML domains. Upon extensive literature review and analysis, it becomes evident that the CPU's role in AI and ML has not only been pivotal but has also retained distinct advantages despite the relative deceleration in its development trajectory. Current trends suggest that modern CPUs are increasingly moving towards an integrated architecture, incorporating specialized modules and fostering synergies with heterogeneous computing units. This paradigm shift underscores a critical question: How can we stimulate CPU development further and leverage its inherent strengths effectively? Addressing this query is essential for pushing the boundaries of CPU technology in the ever-evolving landscape of AI and ML.} }
@inproceedings{10.1145/3660853.3660933, title = {Machine Learning Approaches for Botnet Detection in Network Traffic}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {310--315}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660933}, url = {https://doi.org/10.1145/3660853.3660933}, author = {Salih, Yousif Tareq and Fenjan, Ali and Ahmed, Saadaldeen Rashid and Ali, Hussein and Abdulwahab, Emad.N and Algruri, Sameer and Kurdi, Neesrin Ali and Al-Sarem, Mohammed and Tawfeq, Jamal Fadhil}, keywords = {Botnet Detection, Machine Learning, Network Security, Network Traffic, location = undefinedstanbul, Turkiye}, abstract = {Botnets pose a significant challenge to network security, continually evolving and threatening the integrity of digital infrastructure. Traditional botnet detection methodologies have limitations, prompting the need for innovative approaches. In this paper, we propose a machine learning-based method to effectively detect botnets within network traffic, with a particular focus on IoT devices. Our approach leverages support vector machine (SVM) and regularized logistic regression (rLR) algorithms. Experimental results demonstrate the efficacy of our model in detecting botnet attacks. This research serves as a precursor to countering the daily onslaught of botnet attacks and emphasizes the importance of integrating machine learning techniques into network security.} }
@inproceedings{10.1145/3724154.3724355, title = {Research on highway freight pricing based on machine learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {1235--1240}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724355}, url = {https://doi.org/10.1145/3724154.3724355}, author = {Jin, Chenyu and Lu, Xiaochun and Wu, Xiaoliang}, keywords = {Highway freight, Machine learning, Stacking combination}, abstract = {The logistics industry has become an important support for China's economy, and the road freight transportation business is the mainstay of the domestic logistics market. The highway network has created good conditions for highway freight transportation. With its advantages of flexibility and adaptability, highway transportation has assumed most of the transportation tasks in the domestic freight market and occupied most of the domestic freight market. At present, there are a large number of individual transporters in the road freight market, and there are problems such as oversupply of services and low freight rates. With the increase in transportation costs, the profit margin is getting smaller and smaller. What's more, there is still malicious price competition in the road freight market, which leads to the confusion of freight prices in the road freight market. Based on this, this paper extracts the factors and characteristics that affect the price of highway vehicle transportation, applies the tree model to train after correlation analysis, and Stacks the adjusted model. The results show that the average quotation deviation rate of the model after Stacking fusion is small and the effect is good, which provides an effective price reference for highway vehicle transportation and is conducive to promoting the orderly development of the highway freight market.} }
@inproceedings{10.1145/3643796.3648464, title = {Detecting Security-Relevant Methods using Multi-label Machine Learning}, booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments}, pages = {101--106}, year = {2024}, isbn = {9798400705809}, doi = {10.1145/3643796.3648464}, url = {https://doi.org/10.1145/3643796.3648464}, author = {Johnson, Oshando and Piskachev, Goran and Krishnamurthy, Ranjith and Bodden, Eric}, keywords = {static analysis, software security, machine learning, vulnerability detection, multi-label learning, IntelliJ plugin development, location = Lisbon, Portugal}, abstract = {To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine learning approach has a higher F1-Measure than related approaches. Moreover, the plugin reduces and simplifies the manual effort required when configuring and using static analysis tools.} }
@inproceedings{10.1145/3757749.3757764, title = {Machine Learning-Driven Optimization of Public Health Education Resources and Personalized Learning Systems}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {86--91}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757764}, url = {https://doi.org/10.1145/3757749.3757764}, author = {Wang, Ziquan and Jiang, Yunxia}, keywords = {Artificial intelligence, Machine learning, Optimization of public health education resources, Personalized learning, Support machine vector}, abstract = {As the artificial intelligence (AI) is developing swiftly, it is becoming more common in public health and education. The lack of egalitarian distribution of educational resources and the urgent requirement of the learning personalization has exposed that the intelligent, data-driven AI is desired now. We investigate the AI that has a better ability to ration public health education resources with personalized learning services. By using the mixed methods of case analysis and machine learning, an AI model based on Support Vector Machine (SVM) algorithms is used to analyze the user behavior data, and to calculate the effective educational programs recommendation models for each individual and in different regions based on their requirements. We find that the AI model, in special the one trained with SVM can leverage the efficiency and quality of public health education resources; it can bring high rewards for individual learning services and the early intervention of public health education. Finally, this work also exhibits how the AI model can help even the education resources by advancing the traditional education resource allocation in public health and learning personalization.} }
@article{10.1145/3706029, title = {A Machine Learning Approach to Resolving Conflicts in Physical Human–Robot Interaction}, journal = {J. Hum.-Robot Interact.}, volume = {14}, year = {2025}, doi = {10.1145/3706029}, url = {https://doi.org/10.1145/3706029}, author = {Dincer, Enes Ulas and Al-Saadi, Zaid and Hamad, Yahya M. and Aydin, Yusuf and Kucukyilmaz, Ayse and Basdogan, Cagatay}, keywords = {physical human–robot interaction, dyadic manipulation, conflict resolution, machine learning, classification of interaction behaviors, haptic features, subjective questionnaire, performance metrics}, abstract = {As artificial intelligence techniques become more sophisticated, we anticipate that robots collaborating with humans will develop their own intentions, leading to potential conflicts in interaction. This development calls for advanced conflict resolution strategies in physical human–robot interaction (pHRI), a key focus of our research. We use a machine learning (ML) classifier to detect conflicts during co-manipulation tasks to adapt the robot’s behavior accordingly using an admittance controller. In our approach, we focus on two groups of interactions, namely “harmonious” and “conflicting,” corresponding respectively to the cases of the human and the robot working in harmony to transport an object when they aim for the same target, and human and robot are in conflict when human changes the manipulation plan, e.g. due to a change in the direction of movement or parking location of the object.Co-manipulation scenarios were designed to investigate the efficacy of the proposed ML approach, involving 20 participants. Task performance achieved by the ML approach was compared against three alternative approaches: (a) a rule-based (RB) Approach, where interaction behaviors were rule-derived from statistical distributions of haptic features; (b) an unyielding robot that is proactive during harmonious interactions but does not resolve conflicts otherwise, and (c) a passive robot which always follows the human partner. This mode of cooperation is known as “hand guidance” in pHRI literature and is frequently used in industrial settings for so-called “teaching” a trajectory to a collaborative robot.The results show that the proposed ML approach is superior to the others in task performance. However, a detailed questionnaire administered after the experiments, which contains several metrics, covering a spectrum of dimensions to measure the subjective opinion of the participants, reveals that the most preferred mode of interaction with the robot is surprisingly passive. This preference indicates a strong inclination toward an interaction mode that gives more control to humans and offers less demanding interaction, even if it is not the most efficient in task performance. Hence, there is a clear trade-off between task performance and the preferred mode of interaction of humans with a robot, and a well-balanced approach is necessary for designing effective pHRI systems in the future.} }
@proceedings{10.1145/3749566, title = {IoTML '25: Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, year = {2025}, isbn = {9798400713927} }
@inproceedings{10.1145/3747227.3747276, title = {Research on the construction of the prediction model of economic management students' performance under machine learning technology}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {311--316}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747276}, url = {https://doi.org/10.1145/3747227.3747276}, author = {Wang, Qiankang and Yao, Jun}, keywords = {data mining techniques, economic management, student performance prediction}, abstract = {In order to explore the predictive model of economic management students' performance based on machine learning techniques, three algorithms, namely, Random Forest, Support Vector Regression (SVR) and Long Short-Term Memory Network (LSTM), are used for modeling. The dataset was obtained from 2,000 economics and management students from eight universities across China and contained 28-dimensional features, such as class attendance rate (92.5\% ± 5.8\%), assignment submission rate (94.2\% ± 4.1\%) and MOOC access frequency (2.7 ± 1.3 times per day). Through data preprocessing, feature selection and dimensionality reduction, the LSTM model was finally selected as the best prediction model, with a mean square error (MSE) of 2.73, a root mean square error (RMSE) of 1.65, and a coefficient of determination, R², of 0.89. Compared with the other models, the LSTM had a higher prediction accuracy in the high performance group (MSE = 2.51), and showed better stability and adaptability. The results of the study show that machine learning techniques can effectively capture the complex relationship between student performance and learning behaviors, and provide an accurate method for predicting student performance in the field of education.} }
@inproceedings{10.1145/3704137.3704160, title = {Dynamic Hover Gesture Classification using Photovoltaic Sensor and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {268--275}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704160}, url = {https://doi.org/10.1145/3704137.3704160}, author = {Almania, Nora Abdullah and Alhouli, Sarah Yousef and Sahoo, Deepak Ranjan}, keywords = {Self-Powered Photovoltaic Sensor, Time-series Data, Dynamic Hover Hand Gestures, Machine Learning, Data Augmentation, Random Forest}, abstract = {Self-powered photovoltaic sensor technology has been presented as a gestural interface that could recognise time-series data of dynamic hover gestures using machine learning (ML). To further expand and improve the classification system of dynamic hover hand gestures, we discovered time-series data from multiple subjects and explored the systems’ performance through ML algorithms. In this paper, we collected our own time-series dataset of 3,696 hand gesture samples from 48 multiple subjects using an off-the-shelf photovoltaic sensor and analysed them by applying different preprocessing techniques such as smoothing, augmentation, normalisation, and resampling. Then, we used different ML algorithms such as K-Nearest Neighbors (KNN), Gradient Boosting (GB), Logistic Regression (LR), Decision Tree (DT), and Random Forest (RF) to classify 11 dynamic hover hand gestures. Our findings indicated that RF achieved the highest accuracy among all other ML classifiers, with more than 97\% accuracy. The implications of our study could inform researchers of the potential of this technology for recognising accurate hand gestures trained with multiple subjects, which could help them establish gesture recognition systems for improved accuracy and usability.} }
@inproceedings{10.1145/3626203.3670525, title = {A Comprehensive Cloud Architecture for Machine Learning-enabled Research}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670525}, url = {https://doi.org/10.1145/3626203.3670525}, author = {Stubbs, Joe and Indrakusuma, Dhanny and Garcia, Christian and Halbach, Francois and Hammock, Cody and Freeman, Nathan and Jamthe, Anagha and Packard, Michael and Fields, Alexander and Curbelo, Gilbert}, keywords = {Cloud Computing, GPUs, Machine Learning, location = Providence, RI, USA}, abstract = {The success of machine learning (ML) algorithms, and deep learning in particular, is having a transformative impact on a wide range of research disciplines, from astronomy, materials science, and climate change to bioinformatics, computational health, and animal ecology. At the same time, these new techniques introduce computational modalities that create challenges for academic computing centers and resource providers that have historically focused on asynchronous, batch-computing paradigms. In particular, there is an emergent need for computing models that enable efficient use of specialized hardware such as graphical processing units (GPUs) in the presence of interactive workloads. In this paper, we present a comprehensive, cloud-based architecture comprised of open-source software layers to better meet the needs of modern ML processes and workloads. This framework, deployed at the Texas Advanced Computing Center and in use by various research teams, provides different interfaces at varying levels of abstraction to support and simplify the tasks of users with different backgrounds and expertise, and to efficiently leverage limited GPU resources for these tasks. We present techniques and implementation details for overcoming challenges related to developing and maintaining such an infrastructure which will be of interest to service providers and infrastructure developers alike.} }
@inproceedings{10.1145/3718391.3718440, title = {Comparison of machine learning models for the identification of disc herniation}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {279--284}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3718440}, url = {https://doi.org/10.1145/3718391.3718440}, author = {Huapalla Garcia, Juan and Baldeon Canchaya, Walter and Ovalle, Christian}, keywords = {Classification Models, Disc Herniation, Machine Learning, Medical Diagnosis}, abstract = {The accurate identification of disc herniations is crucial for the proper diagnosis and treatment of low back pain, a health issue affecting millions of people worldwide. Therefore, this study proposes to develop an innovative approach by comparing machine learning models applied to data obtained from a smart belt equipped with advanced sensors. The adopted methodology encompasses data collection, preprocessing, and analysis, evaluating several models, including Logistic Regression, Decision Trees, Naive Bayes, Random Forest, and k-NN. The results reveal that Logistic Regression is the most effective model, demonstrating the best performance metrics in accuracy, AUC, recall, and MCC, with an AUC of 0.763. This model surpasses others in identifying disc herniations and offers balanced and consistent performance. Thus, the utility of machine learning models in clinical practice is validated, proposing a promising tool to improve the diagnosis and treatment of disc herniations. The findings establish a solid foundation for future research, highlighting the importance of exploring this technological approach to contribute to advancements in health and the management of low back pain} }
@article{10.1145/3764944.3764985, title = {MLAM: A Machine Learning-Aided Architectural BottleneckAnalysis Model for x86 Architectures}, journal = {SIGMETRICS Perform. Eval. Rev.}, volume = {53}, pages = {145--152}, year = {2025}, issn = {0163-5999}, doi = {10.1145/3764944.3764985}, url = {https://doi.org/10.1145/3764944.3764985}, author = {Ryoo, Jihyun and Gudukbay Akbulut, Gulsum and Jiang, Huaipan and Tang, Xulong and Akbulut, Suat and Sampson, John and Narayanan, Vijaykrishnan and Taylan Kandemir, Mahmut}, abstract = {The architectural analysis tools that output bottleneck information do not allow knowledge transfer to other applications or architectures. So, we propose a novel tool that can predict an application's bottlenecks for unavailable architectures. We (i) identify the bottleneck characteristics of 44 applications and use this as the dataset for our ML/DL model; (ii) identify the correlations between metrics and bottlenecks to create our tool's initial feature list; (iii) propose an architectural bottleneck analysis model -MLAM - that employs random forest regression (RFR) and multi-layer perceptron (MLP) regression; (iv) present results that indicate MLAM tool can achieve 0.70 (RFR) and 0.72 (MLP) R2 inference accuracy in predicting bottlenecks; (v) present five versions of MLAM, four of which are trained with single architecture data, and one of which is trained with multiple architecture data, to predict bottlenecks for new architectures.} }
@inproceedings{10.1145/3651890.3672243, title = {m3: Accurate Flow-Level Performance Estimation using Machine Learning}, booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference}, pages = {813--827}, year = {2024}, isbn = {9798400706141}, doi = {10.1145/3651890.3672243}, url = {https://doi.org/10.1145/3651890.3672243}, author = {Li, Chenning and Nasr-Esfahany, Arash and Zhao, Kevin and Noorbakhsh, Kimia and Goyal, Prateesh and Alizadeh, Mohammad and Anderson, Thomas E.}, keywords = {network simulation, data center networks, approximation, machine learning, network modeling, location = Sydney, NSW, Australia}, abstract = {Data center network operators often need accurate estimates of aggregate network performance. Unfortunately, existing methods for estimating aggregate network statistics are either inaccurate or too slow to be practical at the data center scale.In this paper, we develop and evaluate a scale-free, fast, and accurate model for estimating data center network tail latency performance for a given workload, topology, and network configuration. First, we show that path-level simulations---simulations of traffic that intersects a given path---produce almost the same aggregate statistics as full network-wide packet-level simulations. We use a simple and fast flow-level fluid simulation in a novel way to capture and summarize essential elements of the path workload, including the effect of cross-traffic on flows on that path. We use this coarse simulation as input to a machine-learning model to predict path-level behavior, and run it on a sample of paths to produce accurate network-wide estimates. Our model generalizes over the choice of congestion control (CC) protocol, CC protocol parameters, and routing. Relative to Parsimon, a state-of-the-art system for rapidly estimating aggregate network tail latency, our approach is significantly faster (5.7), more accurate (45.9\% less error), and more robust.} }
@inproceedings{10.1145/3688671.3688790, title = {Unsupervised machine learning techniques for energy consumption tariff design}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688790}, url = {https://doi.org/10.1145/3688671.3688790}, author = {Tsakiris, Georgios and Virtsionis-Gkalinikis, Nikolaos and Mischos, Stavros and Vrakas, Dimitrios}, keywords = {Time-Of-Use pricing, residential energy consumption, fuzzy clustering, unsupervised machine learning}, abstract = {Dealing with electricity demand fluctuations throughout peak and off-peak periods is challenging for electricity companies. During peak demand times, the grid should be able to match the high consumer needs. Conversely, minimal usage during off-peak periods leads to underutilization of generation capacity. This imbalance challenges utilities to ensure sufficient capacity and devise fair pricing models. The Time-of-Use (ToU) pricing model has emerged as a viable solution in many countries, encouraging consumers to shift their energy consumption from expensive peak hours to more affordable off-peak periods. To this end, this paper proposes unsupervised machine learning methods for designing ToU tariffs using only energy consumption time series data. Additionally, a new metric is introduced to evaluate the adaptability of the ToU methods to fluctuations in energy consumption. To validate the implemented techniques, public datasets from different countries were used.} }
@inproceedings{10.1145/3747227.3747234, title = {Machine Learning-Driven Implicit Evaluation in College English Ideological Education: A Hybrid Corpus-Linguistic and Survey-Based Approach}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {42--47}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747234}, url = {https://doi.org/10.1145/3747227.3747234}, author = {Yang, Wei}, keywords = {Appraisal Theory, Engagement system, Ideological and Political Education in College English, Implicit evaluation, Machine Learning, Support Machine Vector, UAM Corpus Tool}, abstract = {In this paper, discourses in English have been analyzed by means of UAM Corpus Tool, in the light of Appraisal Theory. It counts the distribution of engagement resources and investigates their contributions to expressions of evaluation and intersubjectivity. The questionnaire survey gathers the feedbacks on implicit evaluation from 310 students in the College English Course. It processes the survey data by machine learning, in particular support vector machines (SVM). It presents quantitative perspective on the opinions of students toward the implicit evaluation. The adopted SVM model raises accuracy of students’ opinion, and it reveals hidden patterns in students’ opinions. The study indicates that implicit evaluation of the engagement system serves College English teachers. It makes their lectures more persuasive and promotes students’ critical thinking ability. It also guides students’ values and enhances their ability of international communication.} }
@article{10.1145/3636341.3636360, title = {Adversarial Machine Learning in Recommender Systems}, journal = {SIGIR Forum}, volume = {57}, year = {2023}, issn = {0163-5840}, doi = {10.1145/3636341.3636360}, url = {https://doi.org/10.1145/3636341.3636360}, author = {Merra, Felice Antonio}, abstract = {Recommender systems are ubiquitous. Our digital lives are influenced by their use when, for instance, we select the news to read, the product to buy, the friend to connect with, and the movie to watch. While enormous academic research efforts have been mainly focused on getting high-quality recommendations to reach maximum user satisfaction, little effort has been devoted to studying the integrity and security of these systems. Is there an underlying relationship between the characteristics of the historical user-item interactions and the efficacy of injection of false users/feedback strategies against collaborative models? Can public semantic data be used to perform attacks more potent in raising the recommendability of victim items? Can a malicious user poison or evade the image data of visual recommenders with adversarial perturbed product images? Is the family of model-based recommenders more vulnerable to multi-step gradient-based adversarial perturbations? Furthermore, is the adversarial training robustification still effective in the last scenario? Is this training defense influencing the beyond-accuracy and bias performance?This dissertation intends to pave the way towards more robust recommender systems, beginning with understanding how to robustify a model, what is the cost of robustness in terms of reduction of recommendation accuracy, and which are the novel adversarial risks of modern recommenders. This thesis, getting inspiration from the literature on the security of collaborative models against the insertion of hand-engineered fake profiles and the recent advances of adversarial machine learning methods in other research areas like computer vision, contributes to several directions: (i) the proposal of a practical framework to interpret the impact of data characteristics on the robustness of collaborative recommenders [Deldjoo et al., 2020], (ii) the design of powerful attack strategies using publicly available semantic data [Anelli et al., 2020], (iii) the identification of severe adversarial vulnerabilities of visual-based recommender models where adversaries can break the recommendation integrity by pushing products to the highest recommendation positions with a simple and human-imperceptible perturbation of products' images [Anelli et al., 2021b], (iv) the proposal of robust adversarial perturbation methods capable of completely breaking the accuracy of matrix factorization recommenders [Anelli et al., 2021a], and (v) a formal study that examines the effects of adversarial training in reducing the recommendation quality of state-of-the-art model-based recommenders Anelli et al. [2021c].Awarded by: Politecnico di Bari, Bari, Italy on 24 January 2022.Supervised by: Tommaso DI Noia.Available at: https://iris.poliba.it/retrieve/dd89f8a6-faa4-ccdd-e053-6605fe0a1b87/Adversarial_Machine_Learning_in_Recommender_Systems.pdf.} }
@inproceedings{10.1145/3662158.3662802, title = {Brief Announcement: A Case for Byzantine Machine Learning}, booktitle = {Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing}, pages = {131--134}, year = {2024}, isbn = {9798400706684}, doi = {10.1145/3662158.3662802}, url = {https://doi.org/10.1145/3662158.3662802}, author = {Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael}, keywords = {federated learning, byzantine failure, data poisoning, location = Nantes, France}, abstract = {The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called workers). Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against data poisoning and faulty workers. The problem of Byzantine ML formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm. Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear. It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable. We highlight here some important results on the efficacy of Byzantine robustness for tackling data poisoning. In particular, we discuss cases where, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are optimal even under the weaker threat model of data poisoning.} }
@inproceedings{10.1145/3745238.3745275, title = {A Comparative Machine Learning Framework for Stock Market Forecasting and Dynamic Portfolio Optimization}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {216--221}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745275}, url = {https://doi.org/10.1145/3745238.3745275}, author = {Chen, Zhuoyi}, keywords = {Gradient Boosting, LSTM, Machine Learning, Markowitz Model, Portfolio Optimization, Sliding Window, Turnover Limit}, abstract = {Predicting stock market trends has always been an important area of research, especially in the application of machine learning algorithms. This study aims to predict stock returns in the steel, electronics, food and beverage, and real estate industries using three different machine learning models: linear regression, random forest, and gradient boosting. The research utilizes historical data from 2005 to 2024, including key indicators such as MACD, RSI, and price-to-earnings ratio (PE/TTM), to forecast stock returns. Additionally, the study implements a portfolio optimization model based on the Markowitz model, incorporating different turnover limit to simulate a real trading environment and optimize asset allocation. The results indicate that as turnover limit increase, portfolios with dynamic rebalancing perform overall better across industries compared to others, reflecting the critical role of total fee rate management and turnover limits in maximizing portfolio returns. This study emphasizes the value of combining machine learning models with portfolio optimization strategies to enhance prediction accuracy and asset allocation efficiency.} }
@inproceedings{10.1145/3701716.3715522, title = {Multi-Component Coarsened Graph Learning for Scaling Graph Machine Learning}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {1001--1004}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715522}, url = {https://doi.org/10.1145/3701716.3715522}, author = {Halder, Subhanu and Kumar, Manoj and Kumar, Sandeep}, keywords = {graph coarsening, graph neural networks, graph representation learning, location = Sydney NSW, Australia}, abstract = {Graph coarsening is a reduction technique that approximates a larger graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications. However, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, tree, bipartite structure, or multi-component structure. This work presents an optimization framework for learning coarsened graphs with desirable multi-component structure. The proposed methods are solved efficiently by leveraging block majorization-minimization, log determinant, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework's efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently outperforms state-of-the-art methods.} }
@inproceedings{10.1145/3638529.3654043, title = {Machine Learning-Enhanced Ant Colony Optimization for Column Generation}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {1073--1081}, year = {2024}, isbn = {9798400704949}, doi = {10.1145/3638529.3654043}, url = {https://doi.org/10.1145/3638529.3654043}, author = {Xu, Hongjie and Shen, Yunzhuang and Sun, Yuan and Li, Xiaodong}, keywords = {ant colony optimization, machine learning, column generation, combinatorial optimization, location = Melbourne, VIC, Australia}, abstract = {Column generation (CG) is a powerful technique for solving optimization problems that involve a large number of variables or columns. This technique begins by solving a smaller problem with a subset of columns and gradually generates additional columns as needed. However, the generation of columns often requires solving difficult subproblems repeatedly, which can be a bottleneck for CG. To address this challenge, we propose a novel method called machine learning enhanced ant colony optimization (MLACO), to efficiently generate multiple high-quality columns from a sub-problem. Specifically, we train a ML model to predict the optimal solution of a subproblem, and then integrate this ML prediction into the probabilistic model of ACO to sample multiple high-quality columns. Our experimental results on the bin packing problem with conflicts show that the MLACO method significantly improves the performance of CG compared to several state-of-the-art methods. Furthermore, when our method is incorporated into a Branch-and-Price method, it leads to a significant reduction in solution time.} }
@article{10.1145/3603171, title = {TinyNS: Platform-aware Neurosymbolic Auto Tiny Machine Learning}, journal = {ACM Trans. Embed. Comput. Syst.}, volume = {23}, year = {2024}, issn = {1539-9087}, doi = {10.1145/3603171}, url = {https://doi.org/10.1145/3603171}, author = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Aggarwal, Mohit and Wang, Brian and Han, Liying and Briseno, Julian De Gortari and Srivastava, Mani}, keywords = {Neurosymbolic, neural architecture search, TinyML, AutoML, bayesian, platform-aware}, abstract = {Machine learning at the extreme edge has enabled a plethora of intelligent, time-critical, and remote applications. However, deploying interpretable artificial intelligence systems that can perform high-level symbolic reasoning and satisfy the underlying system rules and physics within the tight platform resource constraints is challenging. In this article, we introduce TinyNS, the first platform-aware neurosymbolic architecture search framework for joint optimization of symbolic and neural operators. TinyNS provides recipes and parsers to automatically write microcontroller code for five types of neurosymbolic models, combining the context awareness and integrity of symbolic techniques with the robustness and performance of machine learning models. TinyNS uses a fast, gradient-free, black-box Bayesian optimizer over discontinuous, conditional, numeric, and categorical search spaces to find the best synergy of symbolic code and neural networks within the hardware resource budget. To guarantee deployability, TinyNS talks to the target hardware during the optimization process. We showcase the utility of TinyNS by deploying microcontroller-class neurosymbolic models through several case studies. In all use cases, TinyNS outperforms purely neural or purely symbolic approaches while guaranteeing execution on real hardware.} }
@inproceedings{10.1145/3757110.3757210, title = {Research on the Prediction Model of Occupational burnout Among Primary Healthcare Workers Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {598--603}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757210}, url = {https://doi.org/10.1145/3757110.3757210}, author = {Chen, Shuang and Li, Zeyi and Sun, Siyu and Hao, Zixu}, keywords = {Decision Tree, Logistic Regression, Prediction Model, Primary Healthcare Workers, Support Vector Machine}, abstract = {Objective: To conduct a questionnaire survey among primary-level medical personnel in Jinan City, Shandong Province, China, and to construct and evaluate machine learning models in predicting job burnout among this group. Methods: A stratified cluster sampling method was adopted in this study to select 1,710 primary-level medical personnel in Jinan City, Shandong Province, as study participants. A general information questionnaire, the Minnesota Satisfaction Questionnaire - Short Form (MSQ-SF), and the Maslach Burnout Inventory (MBI) were utilized to investigate demographic data, job satisfaction, and job burnout. Three machine learning algorithms—Support Vector Machine, Logistic Regression, and Decision Tree—were employed to construct prediction models. Model performance was evaluated, and predictive feature importance was analyzed. Results: The incidence of job burnout among primary-level medical personnel in Jinan City was relatively high. The Support Vector Machine model demonstrated superior performance, with job satisfaction identified as the most significant predictor. Conclusion: It is recommended to fully harness the practical value of the job burnout prediction model for primary-level medical personnel. Early identification of burnout should be prioritized, and targeted measures should be implemented to alleviate burnout and reduce its negative impacts.} }
@proceedings{10.1145/3728199, title = {CNML '25: Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, year = {2025}, isbn = {9798400713231} }
@inbook{10.1145/3718491.3718587, title = {Extreme Weather Prediction and Prevention——Research Based on Machine Learning Method}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {595--600}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718587}, author = {Chao, Xiao}, abstract = {In this paper, machine learning method is used to predict and prevent extreme weather. Firstly, this paper introduces entropy weight -TOPSIS evaluation method and K-means clustering method, and combines LSTM model to evaluate and predict extreme weather conditions. By collecting a large number of eco-environmental data, this paper analyzes the trend and causes of extreme weather changes in China, and discusses it from the perspective of precipitation and land cover. The research results identify six areas with the most severe extreme weather in the future, and provide specific forecast scores. These findings are of great significance for preparing in advance and reducing the negative effects of extreme weather, which can help to take preventive measures, raise public awareness, strengthen infrastructure construction and optimize resource allocation.} }
@article{10.1145/3708320, title = {Adversarial Machine Learning Attacks and Defences in Multi-Agent Reinforcement Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708320}, url = {https://doi.org/10.1145/3708320}, author = {Standen, Maxwell and Kim, Junae and Szabo, Claudia}, keywords = {Security, robustness, perturbation, adversarial machine learning, deep reinforcement learning, defence, mitigation, communication, regularisation, detection, memory, ensemble}, abstract = {Multi-Agent Reinforcement Learning (MARL) is susceptible to Adversarial Machine Learning (AML) attacks. Execution-time AML attacks against MARL are complex due to effects that propagate across time and between agents. To understand the interaction between AML and MARL, this survey covers attacks and defences for MARL, Multi-Agent Learning (MAL), and Deep Reinforcement Learning (DRL). This survey proposes a novel perspective on AML attacks based on attack vectors. This survey also proposes a framework that addresses gaps in current modelling frameworks and enables the comparison of different attacks against MARL. Lastly, the survey identifies knowledge gaps and future avenues of research.} }
@inproceedings{10.1145/3727353.3727501, title = {Machine Learning Analysis of Carbon Inclusion Drivers in Chinese Universities}, booktitle = {Proceedings of the 2025 4th International Conference on Big Data, Information and Computer Network}, pages = {409--414}, year = {2025}, isbn = {9798400712425}, doi = {10.1145/3727353.3727501}, url = {https://doi.org/10.1145/3727353.3727501}, author = {Tang, Dongping and Abdulraheem, Ahmed Ali Ahmed and Wu, Xiaoli}, keywords = {Chinese universities, carbon inclusion, ecological modernization theory, theory of planned behavior}, abstract = {Carbon inclusion is a recent idea where low-carbon and carbon-neutral principles are integrated into the operations and development strategies of Chinese universities. This is especially important since these organizations work to fulfill the national target of reaching carbon neutrality by 2060. In this paper, we investigate the main elements that influence the implementation of carbon inclusion in Chinese institutions in a bid to determine their driving mechanisms. We use Ecological Modernization Theory (EMT) and the Theory of Planned Behavior (TPB) to offer a theoretical framework clarifying both institutional and personal behavior. The EMT emphasizes collaboration between the state and market to achieve environmental sustainability, whereas the TPB highlights the influence of attitudes, social norms, and perceived control on behavior formation. We collected data from colleges in Guangdong province using a quantitative approach. The data was analyzed with advanced machine learning techniques such as clustering and classification models. The analyses revealed distinct university profiles based on sustainability practices, validated hypotheses regarding drivers and barriers, and highlighted critical interactions among factors such as energy efficiency, policy support, and stakeholder engagement. Our study provides actionable insights for university administrators, policymakers, sustainability practitioners and other relevant stakeholders. The results enhance academic discourse and bolsters governmental and societal initiatives that are aimed at attaining carbon neutrality in the Chinese higher education sector.} }
@inproceedings{10.1145/3674805.3686678, title = {An Investigation of How Software Developers Read Machine Learning Code}, booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement}, pages = {165--176}, year = {2024}, isbn = {9798400710476}, doi = {10.1145/3674805.3686678}, url = {https://doi.org/10.1145/3674805.3686678}, author = {Weber, Thomas and Winiker, Christina and Mayer, Sven}, keywords = {code reading, eye tracking, human computer interaction, software developers, location = Barcelona, Spain}, abstract = {Background\&nbsp;Machine Learning plays an ever-growing role in everyday software. This means a paradigmatic shift in how software operators from algorithm-centered software where the developers defines the functionality to data-driven development where behavior is inferred from data. Aims\&nbsp;The goal of our research is to determine how this paradigmatic shift materializes in the written code and whether developers are aware of these changes and how they affect their behavior. Method\&nbsp;To this end, we perform static analysis of 3,515 software repositories to determine structural differences in the code. Following this, we conducted a user study using eye tracking (N=18) to determine how the code reading of developers differs when reading Machine Learning source code versus traditional code. Results\&nbsp;The results show that there are structural differences in the code of this paradigmatically different software. Developers appear to adapt their mental models with growing experience resulting in distinctly different reading patterns. Conclusions\&nbsp;These difference highlight that we cannot treat all code the same but require paradigm-specific, empirically validated support mechanisms to help developers write high-quality code.} }
@inproceedings{10.1145/3735014.3735909, title = {A Machine Learning-Based Prediction Model for High-Speed Railway Track Geometry}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {256--262}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735909}, url = {https://doi.org/10.1145/3735014.3735909}, author = {Zhou, Jing and Liang, Fang}, keywords = {Geometric state of orbit, High-speed railway, Machine learning, Prediction model}, abstract = {This work is devoted to developing a prediction scheme of the high-speed railway (HSR) track geometry state using machine learning technology to improve the efficiency of track maintenance. Many HSR orbit geometric data are systematically collected, and an efficient prediction framework is successfully established by using the maximum likelihood estimation method. Firstly, the research problems are clarified on the technical route, and a perfect data acquisition system is established to ensure the representativeness and reliability of data samples. Then, with the help of systematic feature engineering, the original dataset is deeply mined and preprocessed, and the most predictive feature subset is selected. In the modeling stage, many machine learning algorithms are compared horizontally, and the random forest (RF) algorithm is finally determined and optimized as the core prediction model. After rigorous experimental verification and performance evaluation, the constructed prediction system shows excellent performance in multiple evaluation dimensions, in which the accuracy rate is improved to 95\%, and it also shows excellent generalization performance and robustness. This research result confirms the value of machine learning in the HSR track geometry state prediction field and provides reliable technical support for track maintenance decisions.CCS CONCEPTS • Computing methodologies∼Machine learning∼Machine learning approaches∼Learning in probabilistic graphical models∼Maximum likelihood modeling} }
@inproceedings{10.1109/ICSE55347.2025.00107, title = {Testing and Understanding Deviation Behaviors in FHE-Hardened Machine Learning Models}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {2251--2263}, year = {2025}, isbn = {9798331505691}, doi = {10.1109/ICSE55347.2025.00107}, url = {https://doi.org/10.1109/ICSE55347.2025.00107}, author = {Peng, Yiteng and Wu, Daoyuan and Liu, Zhibo and Xiao, Dongwei and Ji, Zhenlan and Rahmel, Juergen and Wang, Shuai}, abstract = {Fully homomorphic encryption (FHE) is a promising cryptographic primitive that enables secure computation over encrypted data. A primary use of FHE is to support privacy-preserving machine learning (ML) on public cloud infrastructures. Despite the rapid development of FHE-based ML (or HE-ML), the community lacks a systematic understanding of their robustness.In this paper, we aim to systematically test and understand the deviation behaviors of HE-ML models, where the same input causes deviant outputs between FHE-hardened models and their plaintext versions, leading to completely incorrect model predictions. To effectively uncover deviation-triggering inputs under the constraints of expensive FHE computations, we design a novel differential testing tool called HEDiff, which leverages the margin metric on the plaintext model as guidance to drive targeted testing on FHE models. For the identified deviation inputs, we further analyze them to determine whether they exhibit general noise patterns that are transferable. We evaluate HEDiff using three popular HE-ML frameworks, covering 12 different combinations of models and datasets. HEDiff successfully detected hundreds of deviation inputs across almost every tested FHE framework and model. We also quantitatively show that the identified deviation inputs are (visually) meaningful in comparison to regular inputs. Further schematic analysis reveals the root cause of these deviant inputs and allows us to generalize their noise patterns for more directed testing. Our work sheds light on enabling robust HE-ML for real-world usage.} }
@inproceedings{10.1145/3703790.3703801, title = {Sensor-Guided Adaptive Machine Learning on Resource-Constrained Devices}, booktitle = {Proceedings of the 14th International Conference on the Internet of Things}, pages = {90--98}, year = {2025}, isbn = {9798400712852}, doi = {10.1145/3703790.3703801}, url = {https://doi.org/10.1145/3703790.3703801}, author = {Papst, Franz and Kraus, Daniel and Rechberger, Martin and Saukh, Olga}, keywords = {Machine Learning, Sensor Guided, Data Augmentation, Model Adaptation, Resource Efficient}, abstract = {In recent years, the deployment of deep learning models has extended beyond typical cloud environments to resource-constrained devices such as edge devices and smartphones. This shift is driven by their success in learning and detecting patterns in data. However, deep models are often excessively large and lack robustness to minor input transformations. To solve the challenge, deep learning models are often trained with data augmentation, which requires an even larger model to accommodate the additional knowledge. In this paper, we study ways to mitigate these problems by leveraging additional sensing modalities to a) adapt the input data and b) adapt the model for typical transformations. We show that both approaches increase the accuracy of deep learning models by up to 6.21\% and 7.57\% respectively, while using roughly the same number of parameters or even less at inference time. We furthermore study how well these approaches can handle noisy sensor readings.} }
@article{10.1145/3770865.3770866, title = {DIF-PP: Threshold Optimization Informed by IRT Models for Group Fairness in Machine Learning}, journal = {SIGAPP Appl. Comput. Rev.}, volume = {25}, pages = {5--20}, year = {2025}, issn = {1559-6915}, doi = {10.1145/3770865.3770866}, url = {https://doi.org/10.1145/3770865.3770866}, author = {Minatel, Diego and Parmezan, Antonio R. S. and Santos, Nicolas Roque dos and C\'uri, Mariana and de Andrade Lopes, Alneu}, keywords = {bias, classification, DIF, item response theory, post-processing}, abstract = {Machine learning models risk reproducing social biases, so algorithmic decision-making must incorporate principles that prevent discrimination. Post-processing methods, such as threshold optimization, can support this goal, but striking an appropriate trade-off between predictive performance and group equity metrics remains challenging. The recent application of Differential Item Functioning (DIF) in model selection sheds light on its potential as a promising yet unexplored approach to threshold tuning in more unbiased machine learning systems. Building on this premise, this work introduces DIF-PP, a fairness-aware post-processing method that draws on concepts from Item Response Theory (IRT) and DIF for optimizing decision boundaries. DIF-PP represents these thresholds as test items, derives classification characteristic curves through IRT, and uses DIF to identify the most impartial cutoff point. Experimental results with 18 datasets show that DIF-PP consistently outperforms existing methods when we analyze group fairness metrics and predictive performance simultaneously. By combining IRT and DIF, our proposal effectively mitigates discriminatory effects in binary classification, marking a significant advance toward the development of responsible artificial intelligence solutions.} }
@inproceedings{10.1145/3745238.3745505, title = {Enhancing E-commerce Logistics Insights through Machine Learning: A Real-Data Approach}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1709--1716}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745505}, url = {https://doi.org/10.1145/3745238.3745505}, author = {Wang, Xiangcheng}, keywords = {Big data, Data analysis, Distribution efficiency}, abstract = {This study employs real-world data from e-commerce logistics operations to conduct a comprehensive regression analysis on the benefits of e-commerce logistics. Utilizing linear regression and machine learning modeling techniques, the research identifies relevant factors influencing logistics performance. Furthermore, multiple machine learning models are developed to model and predict the risk of delayed order delivery. To enhance the interpretability of the prediction models, feature importance analysis and SHAP-based interpretability analysis are conducted. These analyses elucidate the impact of key factors such as transportation methods and order scheduling on the risk of delayed delivery, thereby offering a novel perspective for the analysis and research of e-commerce logistics.} }
@inbook{10.1145/3718491.3718674, title = {Warehouse Demand Variation Prediction and Optimization Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {1136--1141}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718674}, author = {Lu, Yuting and Jomhari, Nazean and Liao, Shengshi and Goyal, Shyam Bihari}, abstract = {With the intensification of market competition and the diversification of customer demands, efficient inventory management has become a key factor for the success of enterprises. Warehouse demand forecasting, as a core aspect of inventory management, is of great significance for enterprises to reasonably arrange production, optimize inventory levels, and reduce costs. This paper delves into the prediction and optimization methods of warehouse demand changes based on machine learning algorithms, with a particular focus on three powerful algorithms: Random Forest, XGBoost, and LightGBM.the LightGBM model performed the best. The lowest MSE can provide a lower prediction error, and the highest R² value, which is close to 0.786, can better fit the data. The research results fully demonstrate that machine learning algorithms have significant application value and potential in the field of predicting changes in warehouse demand.} }
@inproceedings{10.1145/3716368.3735244, title = {Adversarial Data Poisoning Attack on Quantum Machine Learning in the NISQ Era}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {976--981}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735244}, url = {https://doi.org/10.1145/3716368.3735244}, author = {Kundu, Satwik and Ghosh, Swaroop}, keywords = {Quantum machine learning, noisy intermediate-scale quantum, data poisoning attack, quantum cloud security, noise resilience.}, abstract = {With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a Quantum Indiscriminate Data Poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM_Brisbane’s noise), across various architectures and datasets, QUID achieves up to ( 92\% ) accuracy degradation in model performance compared to baseline models and up to ( 75\% ) accuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding ( 50\% ), demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.} }
@inproceedings{10.1145/3719384.3719387, title = {Enhancing Process Yield Though Quality Prediction Using Machine Learning Techniques}, booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference}, pages = {19--25}, year = {2025}, isbn = {9798400717925}, doi = {10.1145/3719384.3719387}, url = {https://doi.org/10.1145/3719384.3719387}, author = {Wongngern, Nattapon and Phruksaphanrat, Busaba}, keywords = {Decision Analysis, Process improvement, Production Yield, Quality Control}, abstract = {This research investigates the application of Machine learning (ML) techniques to enhance yield of optical modules in manufacturing process of fiber optics. The study focuses on the quality process, which consists of four stages: FCAL, OPM, OPMT, and EXS. The objective is to improve process yield by identifying and removing parts likely to fail testing at an early stage to save time and resources. Initially, data collection and preprocessing were performed. Then, statistical hypothesis testing of mean and variance were utilized for feature selection. After that, various ML techniques were employed to classify parts as either “fail” or “pass,” including Random Forest, XGBoost, Gradient Boosting, Neural Networks, Logistic Regression, Decision Tree, Na\"ve Bayes, and Nearest Neighbors. The research found that Random Forest, XGBoost, and Gradient Boosting models outperformed other models in predicting the quality of part across different stages. Among these, Random Forest and XGBoost were the most effective in improving the cumulative yield of the overall processes. Implementing these models allowed for the early removal of defective parts, resulting in a significant increased in cumulative yield to approximately 97\%.} }
@inproceedings{10.1145/3644116.3644218, title = {Application of machine learning in bipolar disorder}, booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science}, pages = {621--625}, year = {2024}, isbn = {9798400708138}, doi = {10.1145/3644116.3644218}, url = {https://doi.org/10.1145/3644116.3644218}, author = {Hong, Xin and Hu, Maorong}, abstract = {Purpose of review: Machine learning, a hot area of research today, has been attempted to be applied in various fields of clinical medicine. This paper reviews the current status of research on machine learning in the filed of bipolar disorder, including applications in the diagnosis, identification and the accurate treatment of bipolar disorder, and the prediction of suicidal behaviors in bipolar disorder. It also discusses the advantages and shortcomings and prospects of the applications of machine learning in bipolar disorder, in order to provide references for related researches and clinical treatment.Research methods: The literature on the application of machine learning in the field of bipolar disorder in recent years was searched through literature navigation. The aspects of prevention, diagnosis and treatment interventions in bipolar disorder were analyzed.Research conclusion: In recent years, researches on the clinical diagnosis and treatment of bipolar disorder by machine learning have been increasing. However, there are still some limitations and it still needs to be cautious in clinical application. As mental health practitioners, we should actively adapt to and promote the further development of machine learning in the field of bipolar disorder.} }
@inproceedings{10.1145/3746972.3746986, title = {Empirical Analysis and Trend Prediction of National Economic Development Based on Machine Learning Models}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {79--84}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3746986}, url = {https://doi.org/10.1145/3746972.3746986}, author = {Tu, Junhe}, keywords = {GDP forecasting, Support Vector Machine, economic development trends, machine learning, time series}, abstract = {Against the backdrop of increasing global economic uncertainties, accurately predicting the economic trends of a country is of vital importance for formulating effective macroeconomic policies. Gross domestic product (GDP), as a core indicator of economic health, plays a key role in decision-making. However, traditional economic forecasting models, such as autoregressive comprehensive moving average, show obvious limitations when dealing with complex nonlinear economic data. This study explores the application of machine learning models, including support vector machines and long short-term memory, to improve the accuracy of GDP prediction. This study used data from the World Bank's Open database (1960-2020) to compare the predictive performance of traditional models and machine learning models, with a focus on China and the United States. The results demonstrate that machine learning models outperform traditional methods in capturing nonlinear relationships and high-dimensional data, providing more reliable forecasts. The study also highlights the positive correlation between GDP growth and purchasing power parity, offering insights into the interaction between economic growth and monetary purchasing power. This research contributes to the field by providing empirical evidence for selecting economic forecasting methods and offering valuable references for policymakers to promote stable economic growth.} }
@article{10.14778/3705829.3705861, title = {cedar: Optimized and Unified Machine Learning Input Data Pipelines}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {488--502}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3705829.3705861}, url = {https://doi.org/10.14778/3705829.3705861}, author = {Zhao, Mark and Adamiak, Emanuel and Kozyrakis, Christos}, abstract = {The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical due to skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources - or worse - underutilize expensive accelerators.To address these demands, we present cedar, an optimized and unified programming framework for ML input data pipelines. cedar allows users to define a training job's data pipeline using composable operators that support arbitrary ML frameworks and libraries. cedar's extensible optimizer systematically combines and applies performance optimizations to the pipeline. cedar then orchestrates pipeline processing across configurable local and distributed compute resources to efficiently meet the training job's data throughput demands. Across eight pipelines, cedar improves performance by up to 1.87 to 10.65 compared to state-of-the-art input data systems.} }
@inproceedings{10.1145/3732365.3732426, title = {Using machine learning for toxic text detection on Android}, booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering}, pages = {345--349}, year = {2025}, isbn = {9798400713613}, doi = {10.1145/3732365.3732426}, url = {https://doi.org/10.1145/3732365.3732426}, author = {Xue, Yue}, keywords = {Android platform, BERT model, harmful text}, abstract = {Harmful content is gradually emerging under the joint promotion of social media and instant messaging applications, such as hate speeches and fake news, which have a huge impact on social public opinion. Therefore, it is necessary to detect and filter harmful content through effective detection methods to prevent harmful content from causing harm to users. This detection technology is very important for social media, instant messaging applications, and content platforms. To this end, we mainly developed a harmful content detection tool for the Android platform, fine-tuned the BERT model to the Android platform, and finally got a robust scheme to detect harmful content, and got a 98.25\% F1 score.} }
@inproceedings{10.1145/3631295.3631399, title = {Leveraging Intra-Function Parallelism in Serverless Machine Learning}, booktitle = {Proceedings of the 9th International Workshop on Serverless Computing}, pages = {36--41}, year = {2023}, isbn = {9798400704550}, doi = {10.1145/3631295.3631399}, url = {https://doi.org/10.1145/3631295.3631399}, author = {Predoaia, Ionut and Garc\'a-L\'opez, Pedro}, keywords = {Intra-Function Parallelism, Lithops, Machine Learning, Multicore Functions, Serverless, Stateful, location = Bologna, Italy}, abstract = {Running stateful machine learning algorithms with serverless architectures inherently induces overheads, as serverless functions are not directly network-addressable, hence one must rely on a remote storage service for storing the shared state. To hide the access latency to the remote storage, one can employ intra-function parallelism to take advantage of the multicore computing resources of the serverless functions. In this work, we port to serverless two stateful machine learning algorithms, k-means clustering and logistic regression, and then adopt intra-function parallelism to parallelize the execution of the serverless functions. Several experiments have demonstrated that intra-function parallelism delivers performance improvements in serverless machine learning. Improved performances of up to 68\% have been achieved when running k-means on serverless functions that employ intra-function parallelism. We demonstrate with k-means and logistic regression that from a performance perspective it is preferable to execute a smaller number of multiple-vCPUs workers than a larger number of single-vCPU workers, due to decreased synchronization overheads.} }
@proceedings{10.1145/3649403, title = {WiseML '24: Proceedings of the 2024 ACM Workshop on Wireless Security and Machine Learning}, year = {2024}, isbn = {9798400706028}, abstract = {It is our great pleasure to welcome you to the ACM Workshop on Wireless Security and Machine Learning (WiseML) 2024. This year's workshop continues its tradition of being a premier forum to bring together members of the ML, privacy, security, wireless communications, and networking communities from around the world. It provides a platform to share the latest research findings in these emerging and critical areas, fostering the exchange of ideas and promoting research collaborations to advance the state-of-the-art. This year's event will take place in Seoul, South Korea, and the program will feature a single track.} }
@inproceedings{10.1145/3703847.3703865, title = {Systematic Evaluation of Machine Learning-Based Predictive Model for Diabetes}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {99--103}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703865}, url = {https://doi.org/10.1145/3703847.3703865}, author = {Zhang, Yiyuan}, keywords = {Diabetes, Machine Learning, Prediction Model}, abstract = {The paper aims to systematically evaluate the prediction model for diabetes based on machine learning (ML). It conducted literature research in Baidu Scholar, National Library of China, Web of Weipu, CNKI, and Wanfang databases for literature on prediction models for diabetes constructed by ML, and the search period spanned from January 2018 to February 2023. The paper completed literature screening and data extraction independently and used predictive models to construct a research data extraction and quality evaluation checklist (CHARMS) to evaluate the quality of the included literature and screened high-quality literature for discussion. Results: A total of 13 high-quality studies were collected, including 5 ML models, with an area under the ROC curve ranging from 0.720 to 0.97. Laboratory indicators such as age, BMI, blood sugar concentration, waistline, and diabetes history are the main predictive factors. Conclusions: Diabetes prediction models constructed using ML can accurately identify the risk of diabetes, and their predictive performance is superior to traditional risk prediction models. The available literature on the topic exhibits a low overall risk of bias, however, the applicability level of the prediction model is considered average.} }
@inproceedings{10.1145/3681756.3697880, title = {A Multimodal LLM-based Assistant for User-Centric Interactive Machine Learning}, booktitle = {SIGGRAPH Asia 2024 Posters}, year = {2024}, isbn = {9798400711381}, doi = {10.1145/3681756.3697880}, url = {https://doi.org/10.1145/3681756.3697880}, author = {Kawabe, Wataru and Sugano, Yusuke}, keywords = {graphical user interface, machine learning, large language model}, abstract = {This paper proposes a system based on a multimodal large language model (MLLM) to assist non-expert users without prior experience in machine learning (ML) development. The MLLM assistant in our system interactively helps users compile their requirements and create appropriate training data while building an ML model. It has been reported that users often struggle to define training data that comprehensively covers all samples or aligns with their needs. To prevent such failures, the MLLM assistant monitors the user’s interaction process and translates users’ vague needs into concrete ML formulations through chat, ultimately facilitating the creation of appropriate training data.} }
@article{10.1145/3712198, title = {Uncovering Community Smells in Machine Learning-Enabled Systems: Causes, Effects, and Mitigation Strategies}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3712198}, url = {https://doi.org/10.1145/3712198}, author = {Annunziata, Giusy and Lambiase, Stefano and Tamburri, Damian A. and van den Heuvel, Willem-Jan and Palomba, Fabio and Catolino, Gemma and Ferrucci, Filomena and De Lucia, Andrea}, keywords = {Socio-Technical Aspects, ML-Enabled Teams, Partial Least Squares Structural Equation Modeling, PLS-SEM}, abstract = {Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This article addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities.} }
@inproceedings{10.1145/3718751.3718865, title = {Empirical Study on Optimizing Regional Economic Development Strategy Using Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {709--715}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718865}, url = {https://doi.org/10.1145/3718751.3718865}, author = {Yin, Yong and Zhang, Dongyu and Wen, Feiren and Huang, He and Xu, Yueran}, keywords = {Machine Learning, Regional Economic Development, Strategy Optimization}, abstract = {In the contemporary landscape of economic development, the integration of machine learning algorithms presents a promising avenue for refining regional economic strategies. This study embarks on an empirical investigation to explore the efficacy of machine learning algorithms in optimizing regional economic development strategies. By leveraging diverse datasets encompassing economic indicators, demographic information, and regional characteristics, this research employs a range of machine learning techniques including clustering, regression, and predictive modeling. Through rigorous analysis and evaluation, the study seeks to discern patterns, identify key determinants, and formulate actionable insights to enhance regional economic development strategies. The findings of this study contribute to a deeper understanding of the potential of machine learning in informing evidence-based policy decisions and fostering sustainable economic growth at the regional level.} }
@article{10.1145/3688077, title = {Exploring the Frontiers of Machine Learning in Education}, journal = {XRDS}, volume = {31}, pages = {5}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3688077}, url = {https://doi.org/10.1145/3688077}, author = {Li, Jiayi} }
@article{10.1145/3637826, title = {A Machine Learning–Based Readability Model for Gujarati Texts}, journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.}, volume = {23}, year = {2024}, issn = {2375-4699}, doi = {10.1145/3637826}, url = {https://doi.org/10.1145/3637826}, author = {Bhogayata, Chandrakant K.}, keywords = {Readability model, readability rating and level of education, interrater agreement, model comparison, Gujarati texts}, abstract = {This study aims to develop a machine learning–based model to predict the readability of Gujarati texts. The dataset was 50 prose passages from Gujarati literature. Fourteen lexical and syntactic readability text features were extracted from the dataset using a machine learning algorithm of the unigram parts of speech tagger and three Python programming scripts. Two samples of native Gujarati speaking secondary and higher education students rated the Gujarati texts for readability judgment on a 10-point scale of “easy” to “difficult” with the interrater agreement. After dimensionality reduction, seven text features as the independent variables and the mean readability rating as the dependent variable were used to train the readability model. As the students' level of education and gender were related to their readability rating, four readability models for school students, university students, male students, and female students were trained with a backward stepwise multiple linear regression algorithm of supervised machine learning. The trained model is comparable across the raters’ groups. The best model is the university students’ readability rating model. The model is cross-validated. It explains 91\% and 88\% of the variance in readability ratings at training and cross-validation, respectively, and its effect size and power are large and high.} }
@inproceedings{10.1145/3634737.3657026, title = {Cloud-Based Machine Learning Models as Covert Communication Channels}, booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security}, pages = {141--157}, year = {2024}, isbn = {9798400704826}, doi = {10.1145/3634737.3657026}, url = {https://doi.org/10.1145/3634737.3657026}, author = {Krau, Torsten and Stang, Jasper and Dmitrienko, Alexandra}, keywords = {covert channel, machine learning, poisoning attacks, backdoors, location = Singapore, Singapore}, abstract = {While Machine Learning (ML) is one of the most promising technologies in our era, it is prone to a variety of attacks. One of them is covert channels, that enable two parties to stealthily transmit information through carriers intended for different purposes. Existing works only explore covert channels for federated ML. Thereby, communication is established among multiple entities that collaborate to train a model, while relying on access to model internals.This paper presents covert channels within ML models trained and publicly deployed in cloud-based (black-box) environments. The approach relies on targeted poisoning, or backdoor, attacks to encode messages into the model. It incorporates multiple well-chosen backdoors only through dataset poisoning and without requiring access to model internals or the training process. After model deployment, messages can be extracted via inference.We propose three covert channel versions with varying levels of message robustness and capacity while emphasizing minimal extraction effort, minimal pre-shared knowledge, or maximum message stealthiness. We investigate influencing factors affecting embedded backdoors and propose novel techniques to incorporate numerous backdoors simultaneously for message encoding. Experiments across various datasets and model architectures demonstrate message transmission of 20 to 66 bits with minimal error rates.} }
@inproceedings{10.1145/3727582.3728681, title = {Leveraging LLM Enhanced Commit Messages to Improve Machine Learning Based Test Case Prioritization}, booktitle = {Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering}, pages = {45--54}, year = {2025}, isbn = {9798400715945}, doi = {10.1145/3727582.3728681}, url = {https://doi.org/10.1145/3727582.3728681}, author = {Mahmoud, Yara and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta}, keywords = {commit messages, large language model, natural language processing, software testing, test case prioritization, location = Trondheim, Norway}, abstract = {In the rapidly evolving landscape of software development, software testing is critical for maintaining code quality and reducing defects. Effective test case prioritization employs techniques to identify defects early and ensure software quality. New avenues of research have explored using machine learning (ML) to automate the process, most current applications leverage a machine learning model using numerical features to prioritize the test cases. This study investigates the enhancement of this process by incorporating text-based features derived from git commit messages, which often include valuable information about code changes. Given that commit messages are often poorly written and inconsistent, we employ a large language model (LLM) to rewrite these messages based on code diffs, with the aim of improving the quality of their format and the information they contain. We then assess whether these refined commit messages, as an additional feature, contribute to better performance of the test case prioritization model. Our preliminary results indicate that the inclusion of LLM-enhanced commit messages leads to a noticeable improvement in prioritization effectiveness, suggesting a promising avenue for integrating natural language processing techniques in software testing workflows.} }
@article{10.1145/3696352, title = {Machine Learning for Actionable Warning Identification: A Comprehensive Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3696352}, url = {https://doi.org/10.1145/3696352}, author = {Ge, Xiuting and Fang, Chunrong and Li, Xuanye and Sun, Weisong and Wu, Daoyuan and Zhai, Juan and Lin, Shang-Wei and Zhao, Zhihong and Liu, Yang and Chen, Zhenyu}, keywords = {Static analysis warnings, actionable warning identification, literature review}, abstract = {Actionable Warning Identification (AWI) plays a crucial role in improving the usability of static code analyzers. With recent advances in Machine Learning (ML), various approaches have been proposed to incorporate ML techniques into AWI. These ML-based AWI approaches, benefiting from ML’s strong ability to learn subtle and previously unseen patterns from historical data, have demonstrated superior performance. However, a comprehensive overview of these approaches is missing, which could hinder researchers and practitioners from understanding the current process and discovering potential for future improvement in the ML-based AWI community. In this article, we systematically review the state-of-the-art ML-based AWI approaches. First, we employ a meticulous survey methodology and gather 51 primary studies from January 1, 2000 to January 9, 2023. Then, we outline a typical ML-based AWI workflow, including warning dataset preparation, preprocessing, AWI model construction, and evaluation stages. In such a workflow, we categorize ML-based AWI approaches based on the warning output format. In addition, we analyze the key techniques used in each stage, along with their strengths, weaknesses, and distribution. Finally, we provide practical research directions for future ML-based AWI approaches, focusing on aspects such as data improvement (e.g., enhancing the warning labeling strategy) and model exploration (e.g., exploring large language models for AWI).} }
@inbook{10.1145/3696630.3728600, title = {MANILA: A Low-Code Application to Benchmark Machine Learning Models and Fairness-Enhancing Methods}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {1153--1157}, year = {2025}, isbn = {9798400712760}, url = {https://doi.org/10.1145/3696630.3728600}, author = {d'Aloisio, Giordano}, abstract = {This paper presents MANILA, a web-based low-code application to benchmark machine learning models and fairness-enhancing methods and select the one achieving the best fairness and effectiveness trade-off. It is grounded on an Extended Feature Model that models a general fairness benchmarking workflow as a Software Product Line. The constraints defined among the features guide users in creating experiments that do not lead to execution errors. We describe the architecture and implementation of MANILA and evaluate it in terms of expressiveness and correctness.} }
@inproceedings{10.1145/3626253.3635497, title = {Developing Interactive Exercise Materials for Machine Learning Using Spreadsheets}, booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1734--1735}, year = {2024}, isbn = {9798400704246}, doi = {10.1145/3626253.3635497}, url = {https://doi.org/10.1145/3626253.3635497}, author = {Maeda, Atsuhiko}, keywords = {exercise materials, machine learning, spreadsheets, location = Portland, OR, USA}, abstract = {This paper presents a new technique for creating machine-learning exercise materials using spreadsheets. Spreadsheets are often used in teaching machine learning for beginners and non-specialists. However, computation in machine learning models is divided into a learning phase and an inference phase, and conventional spreadsheet-based materials either rely on the software's extensions for the learning phase or can result in huge sheets, which is unsuitable for learners who want to observe it. We realize the learning phase on a spreadsheet that fits within a PC screen, including visualization for better understanding, without using any extensions. Also, this implementation technique makes it possible to execute or pause the learning process interactively. We will show an example of a neural network implementation and discuss the merits and limitations of this technique.} }
@inproceedings{10.1145/3660853.3660879, title = {Advancing Cyber Threat Intelligence through Machine Learning Algorithms}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {111--115}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660879}, url = {https://doi.org/10.1145/3660853.3660879}, author = {Varbanov, Velizar}, keywords = {Cyber Threat Intelligence, Dark Web, Text Classification, location = undefinedstanbul, Turkiye}, abstract = {In the domain of Cyber Threat Intelligence (CTI) the enigmatic depths of the Dark Web are pivotal for the early identification of nascent cyber threats. Yet, the voluminous and sprawling data across hacker forums and illicit marketplaces pose a significant challenge, swamping analysts with a deluge of extraneous information. This paper advocates for the utilization of advanced Machine Learning (ML) algorithms as a strategic tool to distill and classify emergent threats from the Dark Web's vast data landscape. Through rigorous experiments employing a suite of ML models—Logistic Regression, Decision Tree Classification, Gradient Boosting Classifier, and Random Forest Classifier—we evaluated their performance across two meticulously curated datasets. One dataset was rich with data pertinent to cybersecurity, extracted from hacker forums and Dark Web marketplaces, while the other served as a control set. Our results illuminate the profound capability of ML algorithms to effectively navigate and filter through the data quagmire, highlighting threats with precision and thereby significantly optimizing the workflow of analysts. This automation paves the way for a more streamlined, focused approach to CTI, ensuring that cybersecurity operations remain agile and informed in the face of the dynamic and increasingly sophisticated landscape of cyber threats. Through this study, we underscore the transformative potential of ML in revolutionizing CTI methodologies, offering a beacon of efficiency and effectiveness in the ongoing battle against cybercrime.} }
@inbook{10.1145/3711875.3736686, title = {SPATIUM: A Context-Aware Machine Learning Framework for Immersive Spatiotemporal Health Understanding}, booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services}, pages = {759--764}, year = {2025}, isbn = {9798400714535}, url = {https://doi.org/10.1145/3711875.3736686}, author = {Liu, Yang and Jang, SiYoung and Montanari, Alessandro and Kawsar, Fahim}, abstract = {Wearable devices have made significant progress in continuous health monitoring by providing time-series physiological data such as heart rate, respiration, and activity levels. However, most lack spatial awareness, limiting their ability to interpret physiological changes within environmental context—especially indoors where GPS is unreliable. Recent advances in indoor localization, such as Ultra-Wideband (UWB) and visual-inertial odometry, now allow precise spatial positioning in consumer devices. In this paper, we introduce SPATIUM, a context-aware machine learning framework designed to enable immersive spatiotemporal health understanding. SPATIUM integrates multimodal health signals with spatial and environmental data, such as furniture layout, lighting, and temperature, to support more contextually grounded health analysis. We present a proof-of-concept system that combines OmniBuds and UWB localization to collect and visualize spatiotemporal health data in 2D, 3D, and immersive formats. To evaluate the potential impact of spatial features, we conduct a simulation showing that incorporating spatial context improves physiological classification accuracy by up to 26\%. Our results highlight the importance of context-aware, spatially grounded modeling in achieving immersive spatiotemporal health understanding.} }
@inproceedings{10.1145/3658271.3658330, title = {Machine Learning Applied To Fall Detection in the Elderly}, booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems}, year = {2024}, isbn = {9798400709968}, doi = {10.1145/3658271.3658330}, url = {https://doi.org/10.1145/3658271.3658330}, author = {Oliveira, Camila Pereira de and Colombo, Cristiano da Silveira and Nunes, Daniel Jos\'e Ventorim}, keywords = {Elderly, Fall detection, IOT, Machine learning, location = Juiz de Fora, Brazil}, abstract = {Context: With the increase in falls among the elderly, rapid detection becomes essential to reduce fatal risks. This challenge stimulates advancements in machine learning algorithms and IoT technologies in medicine, aiming to improve the safety and longevity of the elderly. Problem: An increase in falls among the elderly is a domestic severe health issue. Solution: Development of an efficient machine learning algorithm using accelerometer data from wearable devices to detect falls in the elderly. SI Theory: The study employs General Systems Theory, not to develop new hardware devices but to integrate and analyze data from accelerometers that already exist in wearable devices. It focuses on the synergy between this hardware data and software algorithms and their interaction with human behavior to enhance the detection of falls in the elderly. Method: This work uses the applied methodology to evaluate KNN, Decision Tree, and MLP algorithms applied to accelerometer data, focusing on accuracy and efficacy. Summary of Results: The MLP model stood out with high efficacy in fall detection, achieving a recall of 97.92\% during the testing and 100\% during the validation phases. This indicates the model’s strong ability to identify falls correctly, a crucial factor for the safety of the elderly. Contributions and Impact in the IS area: Presents an efficient solution for the health of the elderly, with the potential to reduce accidents and improve quality of life. Highlights the importance of validation in natural environments and with diverse individuals for future research.} }
@inproceedings{10.1145/3595360.3595852, title = {Teaching Blue Elephants the Maths for Machine Learning}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595852}, url = {https://doi.org/10.1145/3595360.3595852}, author = {Ruck, Clemens and Sch\"ule, Maximilian Emanuel}, keywords = {in-database machine learning, automatic differentiation, location = Seattle, WA, USA}, abstract = {Code-generation suits well for reverse mode automatic differentiation as it stores each partial derivative as a virtual register. Since the introduction of just-in-time compilation in PostgreSQL, an efficient operator for automatic differentiation seems feasible. Such an operator would allow for in-database machine learning and thus eliminate the need for data extraction.In this paper, we propose automatic differentiation in PostgreSQL: We extend our proposed SQL lambda functions to compute the derivatives even for matrix operations by traversing the expression tree. The evaluation proves that the compiled execution is up to six times faster than the interpreted runs for regression tasks.} }
@inproceedings{10.1145/3748825.3748935, title = {Multi-machine learning algorithms for the co-selection of biomarkers in hepatocellular carcinoma}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {709--715}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748935}, url = {https://doi.org/10.1145/3748825.3748935}, author = {Chen, Junjie and Li, Shuaicheng and Yang, Cheng and Hu, Huanjun}, keywords = {Drug prediction, Feature gene, Liver cancer, Machine learning}, abstract = {Hepatocellular carcinoma is the most common type of primary liver cancer, typically occurring on the basis of pre-existing severe liver diseases, such as hepatitis and cirrhosis. Given its status as a leading cause of mortality worldwide, identifying biomarkers and therapeutic targets associated with hepatocellular carcinoma is of utmost importance. In this study, we performed differential expression analysis on gene expression data from hepatocellular carcinoma patients and utilized a protein-protein interaction network to identify key genes related to the disease. Subsequently, we applied various machine learning algorithms to screen for feature genes, discovering that CYP2B6 and TOP2A are particularly crucial. We then validated our results and found that CYP2B6 and TOP2A are associated with increased mortality rates in hepatocellular carcinoma patients. Finally, we searched for potential drug compounds using the DsigDB database.} }
@article{10.1145/3589506, title = {A Survey on Machine Learning in Hardware Security}, journal = {J. Emerg. Technol. Comput. Syst.}, volume = {19}, year = {2023}, issn = {1550-4832}, doi = {10.1145/3589506}, url = {https://doi.org/10.1145/3589506}, author = {K\"oyl\"u, Troya Cagl and Wedig Reinbrecht, Cezar Rodolfo and Gebregiorgis, Anteneh and Hamdioui, Said and Taouil, Mottaqiallah}, keywords = {Hardware security, machine learning, hardware attacks, hardware countermeasures, neural networks, cybersecurity}, abstract = {Hardware security is currently a very influential domain, where each year countless works are published concerning attacks against hardware and countermeasures. A significant number of them use machine learning, which is proven to be very effective in other domains. This survey, as one of the early attempts, presents the usage of machine learning in hardware security in a full and organized manner. Our contributions include classification and introduction to the relevant fields of machine learning, a comprehensive and critical overview of machine learning usage in hardware security, and an investigation of the hardware attacks against machine learning (neural network) implementations.} }
@inproceedings{10.1145/3637732.3637744, title = {Breast Cancer Detection with Topological Machine Learning}, booktitle = {Proceedings of the 2023 10th International Conference on Biomedical and Bioinformatics Engineering}, pages = {217--222}, year = {2024}, isbn = {9798400708343}, doi = {10.1145/3637732.3637744}, url = {https://doi.org/10.1145/3637732.3637744}, author = {Yadav, Ankur and Nisha, Fnu and Coskunuzer, Baris}, keywords = {Breast Cancer Diagnosis, Cubical Persistence, Machine Learning, Mammogram, Topological Data Analysis, Ultrasound, location = Kyoto, Japan}, abstract = {Screening for breast cancer using mammograms and ultrasound images is an essential but time-consuming and expensive process that requires a trained clinician’s interpretation. To address this issue, machine learning (ML) methods have been developed in recent years as clinical decision-support tools. However, most of these algorithms face challenges related to computational feasibility, reliability, and interpretability. We present a new approach for feature extraction in mammograms and ultrasound images using topological data analysis (TDA) methods. The proposed method uses persistent homology to capture distinct topological patterns in healthy and unhealthy patient images, which are then transformed into powerful feature vectors. These vectors are combined with standard ML techniques to create the Topo-BRCA model, which provides competitive results with state-of-the-art deep learning (DL) models in several benchmark datasets. Unlike most DL models, Topo-BRCA does not require data augmentation or preprocessing and is effective for both small and large datasets. Additionally, the topological feature vectors can easily be integrated into future DL models to enhance their performance further.} }
@inproceedings{10.1145/3689236.3696749, title = {Application of Blockchain Equity System for Machine Learning and Network Security}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {277--286}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3696749}, url = {https://doi.org/10.1145/3689236.3696749}, author = {Peng, Kanghua and Shi, Jincheng and Qiao, Yifang}, keywords = {Blockchain technology, Cybersecurity, Equity system, Machine learning}, abstract = {This paper mainly studies how to apply machine learning and network security to the blockchain equity system to improve the efficiency and privacy of equity transactions of rural economic cooperatives and maintain the stability of rural revitalization. This paper analyzes the potential application value of machine learning and big data algorithms in equity transactions of economic cooperatives, including network security, risk assessment, and smart contracts. This paper proposes a framework scheme of blockchain equity system based on machine learning and big data algorithms, establishes a network security equity system with data tampering, provides a credible data management mechanism, has distributed storage functions, and realizes automatic network consensus and smart contract applications. Experimental results show that the proposed scheme can effectively improve the efficiency and privacy security of equity trading, and provide new ideas and network security support for the development of equity trading.} }
@inproceedings{10.1145/3626246.3654686, title = {Machine Learning for Databases: Foundations, Paradigms, and Open problems}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {622--629}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654686}, url = {https://doi.org/10.1145/3626246.3654686}, author = {Cong, Gao and Yang, Jingyi and Zhao, Yue}, keywords = {learned index, learned query optimization, machine learning for databases, location = Santiago AA, Chile}, abstract = {This tutorial delves into the burgeoning field of Machine Learning for Databases (ML4DB), highlighting its recent progress and the challenges impeding its integration into industrial-grade database management systems. We systematically explore three key themes: the exploration of foundations in ML4DB and their potential for diverse applications, the two paradigms in ML4DB, i.e., using machine learning as a replacement versus enhancement of traditional database components, and the critical open challenges such as improving model efficiency and addressing data shifts. Through an in-depth analysis, including a survey of recent works in major database conferences, this tutorial encapsulates the current state of ML4DB, as well as charts a roadmap for its future development and wider adoption in practical database environments.} }
@inproceedings{10.1145/3724154.3724158, title = {Construction and Application of E-commerce Recommendation Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {21--26}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724158}, url = {https://doi.org/10.1145/3724154.3724158}, author = {Gao, Jing}, keywords = {e-commerce recommendation system, hybrid model, machine learning, personalized recommendation}, abstract = {With the increasingly fierce market competition, e-commerce enterprises are facing many challenges, such as improving product quality, optimizing price strategy and deeply understanding customer needs. Among them, personalized recommendation based on user's historical behavior data has become an important means for many e-commerce companies to enhance user experience, enhance user stickiness and promote sales growth. However, in the face of the increasing data scale, the current e-commerce recommendation model shows a significant lag in feature recognition, recommendation quality and operational efficiency, which is difficult to meet the application needs of the current e-commerce platform. In this regard, this paper will analyze the application status of e-commerce recommendation model, comprehensively explore the application feasibility of machine learning technology in the field of recommendation system, and propose a set of recommendation model construction scheme based on machine learning to realize personalized recommendation and improve the accuracy and user satisfaction of recommendation system. Practice has proved that, compared with the control model, the hybrid e-commerce recommendation model based on machine learning has outstanding performance in three groups of indicators: accuracy, recall and mAP value, which provides new ideas and methods for the function optimization of recommendation system under e-commerce platform.} }
@article{10.5555/3648699.3648841, title = {On tilted losses in machine learning: theory and applications}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia}, keywords = {exponential tilting, empirical risk minimization, value-at-risk, superquantile optimization, fairness, robustness}, abstract = {Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM--tilted empirical risk minimization (TERM)--which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust optimization (DRO). We develop batch and stochastic first-order optimization methods for solving TERM, provide convergence guarantees for the solvers, and show that the framework can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications in machine learning, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. Despite the straightforward modification TERM makes to traditional ERM objectives, we find that the framework can consistently outperform ERM and deliver competitive performance with state-of-the-art, problem-specific approaches.} }
@inproceedings{10.1145/3620666.3651367, title = {ProxiML: Building Machine Learning Classifiers for Photonic Quantum Computing}, booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages = {834--849}, year = {2024}, isbn = {9798400703867}, doi = {10.1145/3620666.3651367}, url = {https://doi.org/10.1145/3620666.3651367}, author = {Ranjan, Aditya and Patel, Tirthak and Silver, Daniel and Gandhi, Harshitta and Tiwari, Devesh}, abstract = {Quantum machine learning has shown early promise and potential for productivity improvements for machine learning classification tasks, but has not been systematically explored on photonics quantum computing platforms. Therefore, this paper presents the design and implementation of ProxiML - a novel quantum machine learning classifier for photonic quantum computing devices with multiple noise-aware design elements for effective model training and inference. Our extensive evaluation on a photonic device (Xanadu's X8 machine) demonstrates the effectiveness of ProxiML machine learning classifier (over 90\% accuracy on a real machine for challenging four-class classification tasks), and competitive classification accuracy compared to prior reported machine learning classifier accuracy on other quantum platforms - revealing the previously unexplored potential of Xanadu's X8 machine.} }
@inproceedings{10.1145/3664476.3670867, title = {RMF: A Risk Measurement Framework for Machine Learning Models}, booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security}, year = {2024}, isbn = {9798400717185}, doi = {10.1145/3664476.3670867}, url = {https://doi.org/10.1145/3664476.3670867}, author = {Schr\"oder, Jan and Breier, Jakub}, keywords = {Adversarial Machine Learning, Backdoor Attacks, ISO/IEC 27004:2016, Machine Learning Security, Risk Measurement, location = Vienna, Austria}, abstract = {Machine learning (ML) models are used in many safety- and security-critical applications nowadays. It is therefore important to measure the security of a system that uses ML as a component. This paper focuses on the field of ML, particularly the security of autonomous vehicles. For this purpose, a technical framework will be described, implemented, and evaluated in a case study. Based on ISO/IEC 27004:2016, risk indicators are utilized to measure and evaluate the extent of damage and the effort required by an attacker. It is not possible, however, to determine a single risk value that represents the attacker’s effort. Therefore, four different values must be interpreted individually.} }
@inproceedings{10.1145/3688671.3688780, title = {Driver Sleepiness Detection Using Machine Learning Models on EEG Data}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688780}, url = {https://doi.org/10.1145/3688671.3688780}, author = {Trigka, Maria and Dritsas, Elias and Mylonas, Phivos}, keywords = {Sleepiness Detection, EEG, Machine Learning}, abstract = {Driver sleepiness is a major cause of road accidents, necessitating effective detection systems to improve safety. This study investigates the use of machine learning (ML) models to automate the detection of driver sleepiness through electroencephalography (EEG) data collected in simulated environments. Various ML models, such as Random Forests (RF), Decision Trees (DT), Logistic Model Trees (LMT) and two Ensemble methods (Bagging and Stacking), were evaluated using 10-fold cross-validation. More specifically, the selected classifiers were trained and tested using EEG data acquired via the NeuroSky MindWave device, including band power, attention, and mediation features to differentiate effectively between Sleepy and Non-sleepy subjects. The Bagging approach demonstrated superior performance among the classifiers, achieving 84.99\% accuracy, 0.849 precision, 0.850 recall, and an Area Under the ROC Curve (AUC) of 0.935.} }
@inproceedings{10.1145/3715931.3715936, title = {Prediction of Anesthesia Resuscitation after Painless Gastroenteroscopy Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Intelligent Medicine and Health}, pages = {27--31}, year = {2025}, isbn = {9798400709616}, doi = {10.1145/3715931.3715936}, url = {https://doi.org/10.1145/3715931.3715936}, author = {Huang, Xueping and He, Qinghua and Zhou, Yun}, keywords = {General anesthesia, Machine learning, Painless gastroenteroscopy, Prediction model}, abstract = {Objective To construct a prediction model for post-anesthesia recovery after endoscopy and validate it. Methods Selection of 2400 surgical patients who underwent painless gastroenteroscopy at Sichuan Electric Power Hospital from September 2019 to September 2024. After data preprocessing, the Logistic regression, random forest (RF), support vector machine (SVM), decision tree (DT), and extreme gradient boosting (XGBoost) model methods were used to construct prediction models. The predictive accuracy of the machine learning models was evaluated by calculating the area under the receiver operating characteristic (ROC) curve, and the contribution of each feature to the prediction result was evaluated by using the Shapley additive explanations (SHAP) method. Results According to the performance indicators, the best model among the five models was the XGBoost regression model, and the nine indicators were found to be closely related to the recovery time of general anesthesia during the feature importance analysis. Conclusion The XGBoost model constructed using multiple data can analyze the complex relationships between variables and outcomes and has good predictive performance and clinical value.} }
@inproceedings{10.1145/3696271.3696277, title = {A More Precise Credit Score Computation Method Based on Big Data and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {34--38}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696277}, url = {https://doi.org/10.1145/3696271.3696277}, author = {Zhang, Liang and Yu, Rujie and Lin, Yifeng and Yang, Yuer}, keywords = {Big Data, Credit Score Computation, Machine Learning}, abstract = {Artificial intelligence is replacing traditional credit scoring systems, processing extensive datasets including financial history, transaction records, credit bureau data, and even the social media behavior of borrowers to promote more accurate credit scores. While traditional methods aimed to promote fair and transparent credit accessibility, there are still some concerns. As more and more factors should be considered to compute the credit score, traditional methods lack the capability of handling big data. Thus, this paper aims to solve the limitations of traditional credit scoring. A solution of using machine learning and big data to improve credit scoring, practical implementations, and challenges is proposed. A linear equation to perform credit score computation is presented.} }
@inproceedings{10.1145/3628797.3628974, title = {Enhancing Intensive Care Patient Prognostics with Machine Learning}, booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology}, pages = {546--553}, year = {2023}, isbn = {9798400708916}, doi = {10.1145/3628797.3628974}, url = {https://doi.org/10.1145/3628797.3628974}, author = {Aissaoui, Wafae and Khennou, Fadoua and Abdellaoui, Abderrahim}, keywords = {Artificial Intelligence, Electronic health records., Machine Learning, Readmission \&amp, Discharge, location = Ho Chi Minh, Vietnam}, abstract = {This article delves into the challenge of foreseeing patient discharges and unplanned returns to the intensive care unit. Its primary objective is to enhance the decision-making process for healthcare providers and administrators, facilitate resource allocation, and elevate the quality of patient care. The focal point of our research involves the prediction of patient discharges and unforeseen readmissions to the intensive care unit, leveraging the comprehensive Medical Information Mart for Intensive Care (MIMIC-III) database. To do this, we evaluated seven distinct machine learning models, aiming to discern their predictive accuracy specifically for ICU readmissions and discharges using the MIMIC-III dataset. Our approach employs a repertoire of machine learning algorithms, encompassing logistic regression, support vector machines (SVM), random forest, and others. These algorithms undergo meticulous training to project the probabilities of both patient discharge and readmission. This training process is reliant on a diverse set of variables, encompassing vital signs, demographic information, and age. In the realm of patient readmissions, the ensemble of Neural Network, SVM, and Random Forest models demonstrates exceptional performance, achieving an impressive accuracy of 99\%. Meanwhile, the Gradient Boosting model showcases remarkably high accuracy, reaching 97\%. In the context of patient discharges, the Random Forest model emerges as the most proficient, boasting an accuracy of approximately 79\%. The findings underscore the effectiveness of these machine learning models in anticipating the likelihood of ICU patient readmissions and discharges. Nonetheless, it’s vital to acknowledge that the outcomes may vary contingent upon the implemented model. Nevertheless, these collective results hold the promise of enhancing patient care and refining administrative practices within medical clinics and hospitals.} }
@inproceedings{10.1145/3711403.3711480, title = {Research on Blended teaching of Artificial Intelligence and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management}, pages = {474--479}, year = {2025}, isbn = {9798400717468}, doi = {10.1145/3711403.3711480}, url = {https://doi.org/10.1145/3711403.3711480}, author = {Guo, Yunying and Li, Xiaofei and Zhang, Tianyu}, keywords = {OBE, achievement, blended teaching, expected learning effect}, abstract = {Based on the results-oriented OBE education concept, in order to cultivate application oriented innovative talents, artificial intelligence and machine learning courses adopt the blended teaching mode. The online learning platform is used to combine offline classroom and online independent learning, and the practical process of blended teaching is described from before, during and after class. Through the scientific setting of KT point and the expected learning effect mapping relationship to assess the level of achievement, we evaluate the impact of blended teaching reform and reflect on our teaching practices. Blended teaching in artificial intelligence courses presents the best scenario for learning, as it combines traditional and online teaching to enhance the understanding and engagement. It offers more personal and flexible learning, more interactions, and practical opportunities for the students to learn.} }
@inproceedings{10.1145/3604237.3626884, title = {Generative Machine Learning for Multivariate Equity Returns}, booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance}, pages = {159--166}, year = {2023}, isbn = {9798400702402}, doi = {10.1145/3604237.3626884}, url = {https://doi.org/10.1145/3604237.3626884}, author = {Tepelyan, Ruslan and Gopal, Achintya}, keywords = {Generative Modeling, Normalizing Flows, Portfolio Optimization, Risk Forecasting, Stock Returns, Variational Autoencoders, location = Brooklyn, NY, USA}, abstract = {The use of machine learning to generate synthetic data has grown in popularity with the proliferation of text-to-image models and especially large language models. The core methodology these models use is to learn the distribution of the underlying data, similar to the classical methods common in finance of fitting statistical models to data. In this work, we explore the efficacy of using modern machine learning methods, specifically conditional importance weighted autoencoders (a variant of variational autoencoders) and conditional normalizing flows, for the task of modeling the returns of equities. The main problem we work to address is modeling the joint distribution of all the members of the S\&amp;P 500, or, in other words, learning a 500-dimensional joint distribution. We show that this generative model has a broad range of applications in finance, including generating realistic synthetic data, volatility and correlation estimation, risk analysis (e.g., value at risk, or VaR, of portfolios), and portfolio optimization.} }
@inproceedings{10.1145/3745238.3745265, title = {RESEARCH ON MACHINE LEARNING METHODS FOR PERSONALIZED LEARNING RECOMMENDATION SYSTEMS}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {153--157}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745265}, url = {https://doi.org/10.1145/3745238.3745265}, author = {Deng, YuJie}, keywords = {Adaptive Recommendation Framework, The Personalised Learning Platform, The application of machine learning methodologies}, abstract = {The present study proposes a personalised recommendation system for students, the function of which is to recommend teaching resources according to students' mastery and interest. Existing information platforms are challenging to utilise for the purpose of recommending teaching resources that align with students' individual conditions, and conventional recommendation technologies frequently encounter issues with accuracy and adaptability. It is evident that a combination of deep neural networks, reinforcement learning mechanisms and domain knowledge graphs is required. Firstly, deep learning methods are employed to extract multi-dimensional features from learning trajectories. Subsequently, reinforcement learning frameworks are utilised to optimise the recommendation decisions in order to facilitate dynamic planning of learning paths. The establishment of a structured knowledge graph enables the system to establish semantic associations between learning resources with a high degree of effectiveness, thereby improving the accuracy of recommendation results. In the face of the challenging issue posed by the limited number of new users, the system has been engineered to ensure the consistency of its recommendation quality in the majority of instances where the initial data is found to be inadequate. This is achieved through the implementation of a domain migration learning scheme.} }
@article{10.1145/3734866, title = {FuMi: A Runtime Fuzz-based Machine Learning Precision Measurement and Testing Framework}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3734866}, url = {https://doi.org/10.1145/3734866}, author = {Zhang, Peng and Papadakis, Mike and Zhou, Yuming}, keywords = {Deep learning, fuzzing testing, software measurement}, abstract = {The rapid evolution of machine learning model training has outpaced the development of corresponding measurement and testing tools, leading to two significant challenges. Firstly, developers of deep learning frameworks struggle to identify operators that fail to meet precision criteria, as these issues may only manifest in a few data points. Secondly, model trainers lack effective methods to estimate precision loss caused by operators during training. To address these issues, we introduce a Pythonic framework inspired by common network layer definitions in deep learning. Our framework includes two new layers: the Fuzz Layer and the Check Layer, designed to aid in measurement and testing. The Fuzz Layer introduces minor perturbations to tensor inputs for any deterministic layer under testing (LUT). The Check Layer then measures precision by analyzing the differences before and after the perturbation. This approach estimates a lower bound of the maximal relative error and alerts developers or trainers of potential bugs if the difference exceeds a predefined tolerance. Additionally, Check Layers can be used independently to conduct precision tests for specific layers, ensuring the precision of operators during runtime. Despite the additional memory and time requirements, this runtime testing ensures proper training of the original model. We demonstrate the utility of our framework, FuMi, through two experiments. First, we tested 21 torch operators across 9 popular machine learning models using PyTorch for various tasks, finding that the conv2d and linear operators often fail to meet precision requirements. Second, to showcase the generalizability of our framework, we tested the ATTENTION operator. By comparing different implementations of state-of-the-art ATTENTION operators, we found that the maximum relative error of the ATTENTION operator is not less than 1\%, which is 13 times larger than that measured by Predoo (a unit test tool). This framework provides a robust solution for identifying precision issues in deep learning models, ensuring reliable and accurate model training.} }
@proceedings{10.1145/3642970, title = {EuroMLSys '24: Proceedings of the 4th Workshop on Machine Learning and Systems}, year = {2024}, isbn = {9798400705410} }
@article{10.1145/3771734, title = {Are There Exceptions to Goodhart's law? On the Moral Justification of Fairness-Aware Machine Learning}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3771734}, url = {https://doi.org/10.1145/3771734}, author = {Weerts, Hilde and Royakkers, Lamb\`er and Pechenizkiy, Mykola}, keywords = {algorithmic fairness, fairness-aware machine learning, ethics, egalitarianism}, abstract = {Fairness-aware machine learning (fair-ml) techniques are algorithmic interventions designed to ensure that individuals who are affected by the predictions of a machine learning model are treated fairly. The problem is often posed as an optimization problem, where the objective is to achieve high predictive performance under a quantitative fairness constraint. However, any attempt to design a fair-ml algorithm must assume a world where Goodhart’s law has an exception: when a fairness measure becomes an optimization constraint, it does not cease to be a good measure. In this paper, we argue that fairness measures are particularly sensitive to Goodhart’s law. Our main contributions are as follows. First, we present a framework for moral reasoning about the justification of fairness metrics. In contrast to existing work, our framework incorporates the belief that whether a distribution of outcomes is fair, depends not only on the cause of inequalities but also on what moral claims decision subjects have to receive a particular benefit or avoid a burden. We use the framework to distil moral and empirical assumptions under which particular fairness metrics correspond to a fair distribution of outcomes. Second, we explore the extent to which employing fairness metrics as a constraint in a fair-ml algorithm is morally justifiable, exemplified by the fair-ml algorithm introduced by Hardt et\&nbsp;al. [21]. We illustrate that enforcing a fairness metric through a fair-ml algorithm often does not result in the fair distribution of outcomes that motivated its use and can even harm the individuals the intervention was intended to protect.} }
@inproceedings{10.1145/3712255.3716531, title = {Automated Machine Learning Tools for Data Science, Modeling, and Algorithm Benchmarking}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1368--1395}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716531}, url = {https://doi.org/10.1145/3712255.3716531}, author = {Urbanowicz, Ryan J.} }
@inproceedings{10.1145/3678957.3685723, title = {Understanding Non-Verbal Irony Markers: Machine Learning Insights Versus Human Judgment}, booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction}, pages = {164--172}, year = {2024}, isbn = {9798400704628}, doi = {10.1145/3678957.3685723}, url = {https://doi.org/10.1145/3678957.3685723}, author = {Spitale, Micol and Catania, Fabio and Panzeri, Francesca}, keywords = {Affective computing, Dataset, Irony detection, Multi-modal, location = San Jose, Costa Rica}, abstract = {Irony detection is a complex task that often stumps both humans, who frequently misinterpret ironic statements, and artificial intelligence (AI) systems. While the majority of AI research on irony detection has concentrated on linguistic cues, the role of non-verbal cues like facial expressions and auditory signals has been largely overlooked. This paper investigates the effectiveness of machine learning models in recognizing irony using solely non-verbal cues. To this end, we conducted the following experiments and analysis: (i) we trained and evaluated some machine-learning models to detect irony; (ii) we compared the results with human interpretations; and (iii) we analysed and identified multi-modal non-verbal irony markers. Our research demonstrates that machine learning models trained on nonverbal data have shown significant promise in detecting irony, outperforming human judgments in this task. Specifically, we found that certain facial action units and acoustic characteristics of speech are key indicators of irony expression. These non-verbal cues, often overlooked in traditional irony detection methods, were effectively identified by machine learning models, leading to improved accuracy in detecting irony.} }
@inproceedings{10.1145/3617023.3617043, title = {Unfairness in Machine Learning for Web Systems Applications}, booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web}, pages = {144--153}, year = {2023}, isbn = {9798400709081}, doi = {10.1145/3617023.3617043}, url = {https://doi.org/10.1145/3617023.3617043}, author = {Minatel, Diego and dos Santos, N\'colas Roque and da Silva, Angelo Cesar Mendes and C\'uri, Mariana and Marcacini, Ricardo Marcondes and Lopes, Alneu de Andrade}, keywords = {Data Bias, Machine Learning, Unfairness Examples, Web Systems, location = Ribeir\~ao Preto, Brazil}, abstract = {Machine learning models are increasingly present in our society; many of these models integrate Web Systems and are directly related to the content we consume daily. Nonetheless, on several occasions, these models have been responsible for decisions that spread prejudices or even decisions, if committed by humans, that would be punishable. After several cases of this nature came to light, research and discussion topics such as Fairness in Machine Learning and Artificial Intelligence Ethics gained a boost of importance and urgency in our society. Thus, one way to make Web Systems fairer in the future is to show how they can currently be unfair. In order to support discussions and be a reference for unfairness cases in machine learning decisions, this work aims to organize in a single document known decision-making that was wholly or partially supported by machine learning models that propagated prejudices, stereotypes, and inequalities in Web Systems. We conceptualize relevant categories of unfairness (such as Web Search and Deep Fake), and when possible, we present the solution adopted by those involved. Furthermore, we discuss approaches to mitigate or prevent discriminatory effects in Web Systems decision-making based on machine learning.} }
@inproceedings{10.1145/3716895.3717018, title = {Mining and analyzing potential customers of bank loans based on machine learning}, booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering}, pages = {694--699}, year = {2025}, isbn = {9798400718007}, doi = {10.1145/3716895.3717018}, url = {https://doi.org/10.1145/3716895.3717018}, author = {Cheng, Yuxian and Bai, Hua}, keywords = {Bank loan, Customer persona, Data mining, Machine learning}, abstract = {This paper predicts whether a customer will apply for a bank loan based on the customer persona created by machine learning. The data generated in the loan ac-tivities of a foreign commercial bank are pre-processed and the machine learning model parameters are tuned by grid search. Several machine learning algorithms such as logistic regression, random forest, KNN and XGBoost were evaluated by accuracy, AUC, F1, Kappa and other indicators, and it was concluded that Ran-dom forest under stratified sampling was the best. This paper uses the form of decision tree to construct detailed customer personas for precision marketing.} }
@article{10.1145/3728887, title = {Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3728887}, url = {https://doi.org/10.1145/3728887}, author = {Risse, Niklas and Liu, Jing and B\"ohme, Marcel}, keywords = {LLM, ML4VD, benchmark, context, data quality, function, machine learning, software security, spurious correlations, vulnerability detection}, abstract = {According to our survey of machine learning for vulnerability detection (ML4VD), 9 in every 10 papers published in the past five years define ML4VD as a function-level binary classification problem: Given a function, does it contain a security flaw? From our experience as security researchers, faced with deciding whether a given function makes the program vulnerable to attacks, we would often first want to understand the context in which this function is called. In this paper, we study how often this decision can really be made without further context and study both vulnerable and non-vulnerable functions in the most popular ML4VD datasets. We call a function vulnerable if it was involved in a patch of an actual security flaw and confirmed to cause the program’s vulnerability. It is non-vulnerable otherwise. We find that in almost all cases this decision cannot be made without further context. Vulnerable functions are often vulnerable only because a corresponding vulnerability-inducing calling context exists while non-vulnerable functions would often be vulnerable if a corresponding context existed. But why do ML4VD techniques achieve high scores even though there is demonstrably not enough information in these samples? Spurious correlations: We find that high scores can be achieved even when only word counts are available. This shows that these datasets can be exploited to achieve high scores without actually detecting any security vulnerabilities. We conclude that the prevailing problem statement of ML4VD is ill-defined and call into question the internal validity of this growing body of work. Constructively, we call for more effective benchmarking methodologies to evaluate the true capabilities of ML4VD, propose alternative problem statements, and examine broader implications for the evaluation of machine learning and programming analysis research.} }
@inproceedings{10.1145/3678884.3681920, title = {Understanding the Perceptions and Practices of the Machine Learning Professionals in Bangladesh}, booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing}, pages = {647--652}, year = {2024}, isbn = {9798400711145}, doi = {10.1145/3678884.3681920}, url = {https://doi.org/10.1145/3678884.3681920}, author = {Nowshin, Afroza and Shiba, Shammi Akhter and Saha, Pratyasha and Sadeque, Farig and Haque, S M Taiabul}, keywords = {artificial intelligence, global south, human-computer interaction, ict4d, machine learning, location = San Jose, Costa Rica}, abstract = {A growing body of research focuses on the machine learning professionals in the Global North, but the perceptions and practices of the professionals in the Global South have remained understudied. For this work, we interviewed 15 AI/ML professionals from Bangladesh to gain insights into their motivations, perceptions, and practices when working with machine learning tools and technologies. Our findings reveal that the machine learning professionals in this region prefer labeled datasets for their work and have lack of knowledge regarding annotation, design, or representation bias. Their responses also indicate that resource constraints and systematic or organizational barriers limit their capacity to address environmental concerns as well as explainability and privacy issues. Our initial findings highlight the importance of a large-scale study with the machine learning professionals in the Global South.} }
@inproceedings{10.1145/3711542.3711585, title = {Machine Learning Approaches to the Exhaustive Auto-Identification of Japanese Clauses}, booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval}, pages = {235--241}, year = {2025}, isbn = {9798400717383}, doi = {10.1145/3711542.3711585}, url = {https://doi.org/10.1145/3711542.3711585}, author = {Zhong, Yong}, keywords = {Japanese clause, Random Forest, XGBoost, exhaustive auto-identification, machine learning, morphological information}, abstract = {Automatic clause identification is important for both Natural Language Processing and Second Language Writing Research. In order to explore the optimal model for exhaustive auto-identification of Japanese clauses, this paper trained six distinct machine learning models using the morphological information from the BCCWJ corpus and its clause boundary annotation dataset, BCCWJ-CBL, and evaluated and compared the prediction performance of each model. Results indicated that both the XGBoost and Random Forest models achieved high F1 scores of 0.982 for the lenient metric and 0.976 for the stringent metric in the evaluation, and the two can be considered as the optimal tools for exhaustive auto-identification of Japanese clauses currently available. Additionally, this paper also found that the five types of morphological information (active form, lemma pronunciation, macro-categorization of parts of speech (POS), original form, and meso-categorization of POS) of current words are the most important features influencing the prediction performance of these models.} }
@inproceedings{10.1145/3735358.3737771, title = {A Communication-Efficient Paradigm for Decentralized Machine Learning with Model Logits}, booktitle = {Proceedings of the 9th Asia-Pacific Workshop on Networking}, pages = {301--302}, year = {2025}, isbn = {9798400714016}, doi = {10.1145/3735358.3737771}, url = {https://doi.org/10.1145/3735358.3737771}, author = {Cai, Yiwen and Du, Haizhou} }
@inproceedings{10.1145/3626203.3670552, title = {NeedLR: Streamlining Point Cloud Annotation for Enhanced Machine Learning Integration}, booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing}, year = {2024}, isbn = {9798400704192}, doi = {10.1145/3626203.3670552}, url = {https://doi.org/10.1145/3626203.3670552}, author = {Stone, Gunner and Tavakkoli, Alireza}, keywords = {3D data processing, complex networks, machine learning, point cloud annotation, superpoint-graph, location = Providence, RI, USA}, abstract = {With the rising popularity of remote sensing across various scientific and engineering disciplines, the demand for efficient analysis of point cloud data has surged. However, the inherent complexity and volume of point cloud data pose considerable obstacles to human annotation efforts, which are an essential step within the machine learning pipeline for generating accurate training datasets. NeedLR emerges as a solution, offering a robust, user-friendly platform tailored for precise and streamlined point cloud annotation. By harnessing GPU-accelerated visualization, NeedLR facilitates interactive 3D manipulation of point clouds, granting users an intuitive means to dive into their data. Further optimized for efficient RAM usage and employing parallel computing strategies, NeedLR achieves great performance across varied computing environments. Its accessible interface broadens user engagement, rendering it a prime candidate for crowdsourced annotation initiatives and enhancing its utility in machine learning endeavors. This paper presents NeedLR, exploring its development, key features, and the user-centric philosophy that shapes its design. We highlight the potential for NeedLR’s role in enhancing current point cloud annotation techniques, merging operational efficiency with broad access to empower users across disciplines.} }
@inproceedings{10.1145/3613905.3651116, title = {Machine Learning Insights Into Eating Disorder Twitter Communities}, booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703317}, doi = {10.1145/3613905.3651116}, url = {https://doi.org/10.1145/3613905.3651116}, author = {Kao, Hsien-Te and Erickson, Isabel and Chu, Minh Duc Hoang and He, Zihao and Lerman, Kristina and Volkova, Svitlana}, keywords = {Discussion Analysis, Eating Disorders, Social Media, location = Honolulu, HI, USA}, abstract = {Eating disorders have serious impacts on young population physical, psychological, and social functioning. Health agencies are calling for new psycho-therapeutic intervention considerations that take into account online communities, which is not possible if therapists know little about eating disorders discussions online. In this paper, we leverage machine learning analytics to understand what the eating disorder communities are talking about over time and how they are talking about it. By analyzing local and global community discussions, we discovered complex group dynamics underpinning collective identities offering emotional support but also potentially perpetuating harmful behaviors. Our analysis of four local subcommunities and four global theme evolutions revealed prevalent subjects, perspectives, motivations, and linguistic patterns. We found tight-knit communities grounded in shared membership, goals, cultures, and practices. Community voices highlighting recovery journeys were limited. Our computational assessment of invisible online spaces aims to inform personalized interventions accounting for community forces in youth mental health.} }
@inproceedings{10.1145/3764924.3770889, title = {Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses}, booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Spatial Intelligence for Smart and Connected Communities}, pages = {1--4}, year = {2025}, isbn = {9798400721878}, doi = {10.1145/3764924.3770889}, url = {https://doi.org/10.1145/3764924.3770889}, author = {Shen, Bolin and Ozguven, Eren and Zhao, Yue and Wang, Guang and Xie, Yiqun and Dong, Yushun}, keywords = {machine learning, storm surge, economic losses, risk assessment, location = The Graduate Hotel Minneapolis, Minneapolis, MN, USA}, abstract = {Florida communities face recurring and severe hurricane impacts, leading to substantial and repeated economic losses, particularly in vulnerable coastal and low-income areas. While prior studies have examined individual drivers of hurricane-induced damage, few have developed a unified, community-relevant framework that integrates diverse factors to comprehensively assess sources of loss. Drawing on data that capture the unique environmental and socioeconomic conditions of Florida communities, we propose a modeling framework tailored to the Florida's disaster context. The framework categorizes contributing factors into three components: (1) hurricane characteristics, (2) water-related environmental conditions, and (3) socioeconomic attributes of affected areas. By integrating multi-source data and aggregating variables at the ZIP Code Tabulation Area (ZCTA) level, we use machine learning models to predict economic loss, with insurance claims serving as indicators of realized damage. Beyond accurate prediction, our approach systematically evaluates the relative importance of each factor category, providing actionable insights for disaster preparedness, equitable recovery, and adaptive urban planning in hurricane-exposed communities. Our code is available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction} }
@inproceedings{10.1145/3747227.3747279, title = {The Reflection of Traditional Architectural Features in Modern Residence using Machine Learning: A case study near ancient town in Shanghai}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {330--335}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747279}, url = {https://doi.org/10.1145/3747227.3747279}, author = {Fan, Yilun and Cai, Jun}, keywords = {Architectural styles, K-Means model, Machine learning, Quality of life, Satisfaction score}, abstract = {This study examines the role of the machine learning model in the clustering of groups having similarities in rating architectural styles. The survey approach was used to collect data from participants residing near Qibao Ancient Town in Shanghai, China. The collected data was preprocessed for extraction of significant architectural features. The extracted information was used to train and evaluate the K-Means model to cluster residents' satisfaction and identify their needs. Results show that moderate architectural similarity (0.4 - 0.8) was achieved by the machine learning model, while human-rated similarity (0.0 - 1.0) was obtained in this study. The K-Means clustering approach categorized participants into three clusters based on their satisfaction and their concerns on lighting and noise. This research study suggests redesigning fixtures with tactile finishes, ergonomic handles, and modular shelving. Furthermore, this work suggests installing LED lighting in cold regions and adding acoustic panels in high-traffic areas.} }
@inproceedings{10.1145/3644032.3644467, title = {Machine Learning-based Test Case Prioritization using Hyperparameter Optimization}, booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)}, pages = {125--135}, year = {2024}, isbn = {9798400705885}, doi = {10.1145/3644032.3644467}, url = {https://doi.org/10.1145/3644032.3644467}, author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta}, keywords = {hyperparameter optimization, test case prioritization, machine learning, continuous integration, location = Lisbon, Portugal}, abstract = {Continuous integration pipelines execute extensive automated test suites to validate new software builds. In this fast-paced development environment, delivering timely testing results to developers is critical to ensuring software quality. Test case prioritization (TCP) emerges as a pivotal solution, enabling the prioritization of fault-prone test cases for immediate attention. Recent advancements in machine learning have showcased promising results in TCP, offering the potential to revolutionize how we optimize testing workflows. Hyperparameter tuning plays a crucial role in enhancing the performance of ML models. However, there needs to be more work investigating the effects of hyperparameter tuning on TCP. Therefore, we explore how optimized hyperparameters influence the performance of various ML classifiers, focusing on the Average Percentage of Faults Detected (APFD) metric. Through empirical analysis of ten real-world, large-scale, diverse datasets, we conduct a grid search-based tuning with 885 hyperparameter combinations for four machine learning models. Our results provide model-specific insights and demonstrate an average 15\% improvement in model performance with hyperparameter tuning compared to default settings. We further explain how hyperparameter tuning improves precision (max = 1), recall (max = 0.9633), F1-score (max = 0.9662), and influences APFD value (max = 0.9835), indicating a direct connection between tuning and prioritization performance. Hence, this study underscores the importance of hyperparameter tuning in optimizing failure prediction models and their direct impact on prioritization performance.} }
@inproceedings{10.1145/3721145.3729514, title = {SmartNIC-GPU-CPU Heterogeneous System for Large Machine Learning Model with Software-Hardware Codesign}, booktitle = {Proceedings of the 39th ACM International Conference on Supercomputing}, pages = {837--852}, year = {2025}, isbn = {9798400715372}, doi = {10.1145/3721145.3729514}, url = {https://doi.org/10.1145/3721145.3729514}, author = {Guo, Anqi and Hao, Yuchen and Yao, Xiteng and Yang, Shining and Huang, Jianyu and Geng, Tony (Tong) and Herbordt, Martin}, keywords = {Heterogeneous System, SmartNIC, Machine learning}, abstract = {The rapid growth of large machine learning models, from billions to trillions of parameters, has led to powerful AI capabilities that increasingly impact everyday life. However, this expansion in model size has surpassed the capacity of GPU memory. As a result, GPU clusters—built by aggregating multiple GPUs—have scaled up significantly to accommodate these models. To address this scalability challenge, and to make large-model training more widely accessible, researchers have proposed heterogeneous systems. These systems leverage CPUs and secondary memory to offload storage and computation onto these devices, thereby reducing the total number of GPUs required for training. Despite their promise, such heterogeneous systems have so far faced challenges in achieving high efficiency and performance.In response, we address this problem with SmartNIC-GPU-CPU (SGC), a heterogeneous system, enhanced with SmartNICs, for training large machine learning models with software-hardware codesign. SGC increases system performance and efficiency while simultaneously reducing power consumption and overall costs.In SGC, SmartNICs serve as an intermediate layer that seamlessly connects the heterogeneous components. By implementing optimization techniques such as prefetching, buffering, and dynamic scheduling and control, SmartNICs streamline the data pipeline, minimizing idle times and overlapping communication with computation. In addition, system configuration software optimizes the system and model settings for maximal efficiency, given different system specifications. Experiments demonstrate that the SGC system achieves an improvement of over 1.6 in training throughput over the baseline for a 100B parameter model.} }
@inproceedings{10.1145/3746972.3747001, title = {Double Machine Learning - Enabled Analysis of How Digital Transformation Shapes ESG Performance}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {175--180}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747001}, url = {https://doi.org/10.1145/3746972.3747001}, author = {Wu, Yanting}, keywords = {Digital Transformation, Double Machine Learning, ESG Performance, Sustainable development}, abstract = {Digital transformation (DT), propelled by emerging technologies, signifies a strategic realignment of business operations, with the goal of enhancing organizational performance and competitive advantages. It involves the application of innovative digital technologies to comprehensively reshape business models and service delivery mechanisms. Key drivers of DT encompass big data, cloud computing, artificial intelligence, and block chain. In the context of global sustainability imperatives and China's dual carbon targets, this study empirically explores the mechanisms through which information technology - driven digital transformation improves corporate environmental, social, and governance (ESG) performance. Based on 11,800 firm - year observations of Chinese listed companies in heavy - polluting industries from 2010 to 2023, a double machine learning (DML) model is established, and tools such as Stata 19 are employed for data management, statistical analysis and programming. The findings demonstrate that digital transformation significantly enhances corporate ESG performance, with a more pronounced effect on state - owned enterprises. Replacing DML algorithms, including methods like lasso, gradient boosting, and neural network regression, shows that the research findings stay consistent across various model configurations. The present research stands out with its three pivotal contributions. Theoretically, it validates the positive impact of digital transformation on ESG performance for heavy - polluting listed companies in China, enriching the relevant theoretical system. Methodologically, the establishment of the DML model and the empirical analysis framework provide a new approach for similar research. Practically, it offers actionable strategies for emerging economies to integrate information technology into ESG governance, facilitating the construction of a sustainable development model characterized by “technology - enabled empowerment, market - oriented regulation, and environmental synergy,” and promoting the realization of China's “Dual Carbon” goals and broader sustainable development objectives.} }
@inproceedings{10.1145/3719384.3719386, title = {Towards Classification of Covariance Matrices via Bures-Wasserstein-Based Machine Learning}, booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference}, pages = {10--18}, year = {2025}, isbn = {9798400717925}, doi = {10.1145/3719384.3719386}, url = {https://doi.org/10.1145/3719384.3719386}, author = {Zirpoli, Michael and Yi, Yuyan and Lin, Shu-Chin and Ge, Linqiang and Zheng, Jingyi}, keywords = {Bures-Wasserstein distance, Fr\'echet Mean, Random Forest, Riemannian manifold, Support Vector Machine}, abstract = {Spatial-temporal data is a prevalent data type in biomedical domains, encompassing instances like multi-channel EEG and fMRI. In the analysis of such data, the connectivity matrix (e.g., functional connectivity derived from fMRI, covariance matrix derived from EEG) is widely extracted and analyzed. Rather than analyzing these matrices within the Euclidean space, this paper considers each matrix as a point situated on the manifold of positive semi-definite (PSD) matrices coupled with Bures-Wasserstein (BW) metric. Within this framework, two machine learning models based on the BW metric are proposed for the classification of PSD matrices on the manifold. Specifically, projection map techniques, based on the BW metric, have been introduced and integrated into machine learning models such as support vector machines and random forest. In comparison with Euclidean methods, our approach considers the geometry of the Riemannian manifold where PSD matrices reside. Moreover, compared with prevalent Affine-Invariant (AI) metrics, our framework does not require matrix regularization and is computationally efficient. To comprehensively evaluate the proposed methods, four fMRI datasets and three brain-computer interface datasets with varying dimensions and quantities have been utilized. The results demonstrate comparable and even superior performance of the proposed methods compared with Euclidean and AI-based approaches.} }
@inproceedings{10.1145/3477495.3531722, title = {Retrieval-Enhanced Machine Learning}, booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {2875--2886}, year = {2022}, isbn = {9781450387323}, doi = {10.1145/3477495.3531722}, url = {https://doi.org/10.1145/3477495.3531722}, author = {Zamani, Hamed and Diaz, Fernando and Dehghani, Mostafa and Metzler, Donald and Bendersky, Michael}, keywords = {knowledge grounding, memory augmentation, retrieval augmentation, location = Madrid, Spain}, abstract = {Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.} }
@inproceedings{10.1145/3605423.3605450, title = {Machine Learning on Insurance Premium Prediction}, booktitle = {Proceedings of the 2023 9th International Conference on Computer Technology Applications}, pages = {19--24}, year = {2023}, isbn = {9781450399579}, doi = {10.1145/3605423.3605450}, url = {https://doi.org/10.1145/3605423.3605450}, author = {Jesus, Rodrigo M. and Brito, Miguel A. and Duarte, Duarte N.}, keywords = {Insurance Premium Prediction, MLOPs, Machine Learning, location = Vienna, Austria}, abstract = {The insurance field is going through a phase of great transformation due to the growth of new technologies and techniques that are causing a change in the way data is handled and analyzed. The main perpetrator of this phenomenon is the introduction of Machine Learning (ML) in financial decision-making due to their efficiency and productivity. However, there is a new intervenient in the room, which will automate and support all steps of ML system development. Machine Learning Operations (MLOPs) will reduce technical friction, so that the model may move from an idea into production, in the shortest amount of time, and subsequently to market with the least possible risk. In this paper, a detailed review of the impacts of ML on insurance premium forecasting and the influence that MLOPs can have on forecasting outcomes is provided. Furthermore, a comprehensive summary is presented of crucial principles in the insurance industry, which are essential for comprehending the role that MLOPs will play in tailoring and individualizing insurance policies and premiums.} }
@inproceedings{10.1145/3747897.3747907, title = {Machine Learning-Based Approach for Android Spyware Detection An Analysis Using Logistic Regression}, booktitle = {Proceedings of the Sixth International Conference on Digital Age \&amp; Technological Advances for Sustainable Development}, pages = {55--59}, year = {2025}, isbn = {9798400713590}, doi = {10.1145/3747897.3747907}, url = {https://doi.org/10.1145/3747897.3747907}, author = {Fenjan, Ali and Md Desa, Dr. Jalil and Elaskari, Dr.Salah and Alsayafi, Eng. Athraa Saleh}, keywords = {Android spyware, Machine learning, Logistic Regression, Malware detection, Cybersecurity.}, abstract = {Android phones are fast becoming the targets of spyware, and the security and privacy consequences are severe. Signature-based methods are unable to deal with the malware dynamics, and therefore, machine learning (ML) techniques are employed. Logistic Regression is used in the identification of Android spyware in this paper with the help of static and dynamic analysis to improve classification accuracy. The model achieved 99.33\% classification accuracy with a precision of 97.50\% and recall of 98.62\%, effectively detecting spyware from ordinary programs. Results of the confusion matrix indicate minimal false positives and zero misclassification of genuine programs. Statistical measures like the Chi-Square and T-Test validate the model’s predictions and confirm its appropriateness. The study also explores issues such as adversarial evasion, computational efficiency, and privacy, suggesting optimizations for practical applications. Overall, the findings support ML-based Android spyware detection and offer scalable security solutions for the Android ecosystem.} }
@inproceedings{10.1145/3690771.3690777, title = {Investigating the Factors Affecting Risky Levels of Alcohol Consumption among Students Using Machine Learning Approach}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {7--13}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690777}, url = {https://doi.org/10.1145/3690771.3690777}, author = {Shanchary, Sara Fariha and Meraz, Md Naved and Ibne Hakim, Ayman and Faiyaz, Chowdhury Nafis and Shafayat Oshman, Muhammad}, keywords = {alcohol consumption, alcohol use disorder, family relationships, machine learning}, abstract = {Addressing the pressing issue of alcohol consumption among students is crucial for the well-being of young individuals and the community. Understanding and mitigating alcohol related challenges faced by young people is essential for their healthy development. This study utilizes machine learning models to analyze data from a Portuguese school, aiming to link students' alcohol consumption levels to personal and familial factors. The primary objective is to identify key factors associated with high alcohol consumption among students. After data preprocessing on their dataset, we employed various machine learning algorithms, including hyperparameter-optimized Decision Tree, Random Forest, Boosting, and Ensemble Learning. Our findings revealed that the decision tree algorithm performed well in predicting risky alcohol consumption for our target research question. Our selected feature subset showed strong positive correlation with the target variable, achieving an accuracy of 80.9 percent on the test set and 98.43 percent on the train set. Notably, this project breaks new ground by using explainable artificial intelligence to add reason to our prediction based on students' familial relationships, expanding upon previous research that also focused on demographic factors.} }
@inproceedings{10.1145/3704323.3704325, title = {Research on crime risk prediction model based on machine learning}, booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition}, pages = {361--365}, year = {2025}, isbn = {9798400717482}, doi = {10.1145/3704323.3704325}, url = {https://doi.org/10.1145/3704323.3704325}, author = {Hou, Xuehui and Rexiti, Kudelaiti and Aizezi, Yasen}, keywords = {Crime risk prediction, Naive Bayes, Random Forest, XGboost}, abstract = {Abstract: Crime prediction is an important method for public security departments to conduct crime early warning and investigation. According to the multidimensional characteristics of criminals, machine learning classification algorithm is used to establish a prediction model of whether a criminal commits a crime again, so as to quantitatively evaluate the possibility of the criminal commits a crime again, and provide a feasible scheme reference for preventing and reducing the recurrence of crime. Analysis of urban crime data in Boston, using Naive Bayes, Random Forest and XGboost algorithms to build a crime type prediction model, we can get the urban crime type tendency, help the public security department to predict the future crime, take effective measures to reduce the urban crime rate.} }
@inproceedings{10.1145/3675249.3675337, title = {Quantitative Investment Based on Fundamental Analysis Using Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology}, pages = {509--515}, year = {2024}, isbn = {9798400718267}, doi = {10.1145/3675249.3675337}, url = {https://doi.org/10.1145/3675249.3675337}, author = {Zhang, Zhiruo}, abstract = {Forecasting financial markets has always been a focus of investors and researchers. This study aims to build a reliable financial market prediction model through factor selection and machine learning model construction. Traditional technical and fundamental analysis are limited in their predictive results due to subjective judgment and information asymmetry. Therefore, more people are turning to intelligent quantitative investment to enhance prediction accuracy and stability using artificial intelligence technology. Fundamental quantitative investment combines quantitative and value investment methods, utilizing the relationship between stock fundamentals and excess returns to achieve modern value investing. This paper employs supervised machine learning methods to analyze the relationship between stock factors and returns, choosing regression methods to solve prediction problems. The paper introduces the application of factor selection and machine learning models in financial market forecasting. Factor selection involves selecting key factors related to stock returns from a large amount of fundamental data. Machine learning models, including traditional linear regression, ridge regression, and Lasso regression, utilize patterns and rules in historical data for prediction. By utilizing the sliding window method to divide the dataset and retain its time series features, a reliable financial market prediction model is established by selecting appropriate sliding window lengths and machine learning models. Empirical results and analysis show that the selected model performs well in metrics such as annualized returns, maximum drawdown rate, and annualized volatility, demonstrating certain advantages over the Shanghai and Shenzhen 300 Index.} }
@article{10.1145/3721977, title = {A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3721977}, url = {https://doi.org/10.1145/3721977}, author = {Casey, Beatrice and Santos, Joanna C. S. and Perry, George}, keywords = {Source code representation, machine learning for software security, systematic literature review}, abstract = {Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what is not there yet. This article presents a study of these existing machine learning based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall (e.g., AST and tokenizers are the representations with the highest count of papers, whereas graph-based representations is the category with the highest count of papers). We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines are the most popular model overall.} }
@inproceedings{10.5555/3615924.3623635, title = {First Workshop on Machine Learning Challenges in Cybersecurity}, booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering}, pages = {248--250}, year = {2023}, author = {Branco, Paula and Moniz, Nuno and Jourdan, Guy-Vincent}, keywords = {Cybersecurity, Machine Learning, Deep Learning, Imbalanced Data, Novelty Detection, location = Las Vegas, NV, USA}, abstract = {Cybersecurity events severely impact many individuals, infras-tructures, businesses, and institutions. Machine learning has been playing a key role in many aspects of our daily life and in particular as a cyber defense mechanism. However, the nature of cyber threats poses many unique challenges to machine learning models. The first workshop on Machine Learning Challenges in Cybersecurity held at the CASCON conference focused on the key challenges, recent developments, and open research issues of machine learning in cybersecurity. Two applications of deep learning to phishing and face verification systems are analyzed. Finally, an invited talk from Dr. Shirani covered a machine learning based solution for insider threat detection.} }
@inproceedings{10.1145/3706594.3728869, title = {Leveraging gem5 for Hardware Trojan Research: Simulation for Machine-Learning-Based Detection}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {9--16}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3728869}, url = {https://doi.org/10.1145/3706594.3728869}, author = {Palumbo, Alessandro and Salvador, Ruben}, keywords = {Hardware Trojans, Gem5 Simulator, Microprocessor Security, Machine Learning-Based Anomaly Detection}, abstract = {Hardware Trojans (HTs) consist of malicious modifications intentionally embedded in hardware designs, capable of bypassing security measures, leaking sensitive information, or disrupting system operations. This work proposes a methodology for introducing and simulating HTs on microprocessor-based systems within the gem5 simulator, focusing on RISC-V architectures. By leveraging the gem5 System Emulation (SE) mode, we generate comprehensive datasets encompassing both benign and attack scenarios, automating the collection of 866 features from Hardware Performance Counters (HPC) dumped by gem5 after every program run. The dataset consists of 10,000 samples, evenly split into 5,000 benign and 5,000 attacked instances on different runs across 8 benchmarks. Contrary to previous work using processors with a very limited number of HPCs, we demonstrate how simulation provides access to a much larger number of HPCs and can, hence, enable new research on effective detection mechanisms. In particular, we show preliminary results for one HT and improve previous work on detection mechanisms using Machine Learning (ML). Using a Random Forest (RF) classifier and taking inspiration from the PIC16F84-T100 HT from the Trust-Hub platform as a case study, we achieve 100\% attack detection. Our approach facilitates the security evaluation of microprocessors under HT attacks and provides a framework for experimenting with and analyzing ML-based detection strategies.} }
@inproceedings{10.1109/SCW63240.2024.00166, title = {Applying a Task-Based Approach to Distributed Machine Learning Workflows}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {1252--1261}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00166}, url = {https://doi.org/10.1109/SCW63240.2024.00166}, author = {V\'azquez-Novoa, Fernando and Lezzi, Daniele and Lordan, Francesc and Baghdadi, Fatemeh and Cirillo, Davide}, abstract = {The growing demands across various scientific fields have led to a significant shift in applications that consume data at the edge of the computing continuum. These applications require unified programming models for the composition of components and coordinating the execution of computational workloads, including training machine learning (ML) models on distributed resources. Personalized healthcare often leverages data generated from wearable devices used to train ML models, can be benefited from distributed computing approaches. Specifically, stroke care can be greatly benefited from distributed ML with modifiable risk factors that can be monitored using wearable devices. In this work, we present an implementation that leverages distributed techniques for large-scale ML workflows using electrocardiogram (ECG) recordings for atrial fibrillation (AF) classification. The application was evaluated using the PhysioNet database, showcasing the potential of distributed, ML in stroke care, opening the way for future creation of more advanced models embedded in edge devices.} }
@article{10.1145/3696430, title = {Leveraging Blockchain and Machine Learning to Promote Child Labor-Free Sustainable Development}, journal = {Distrib. Ledger Technol.}, volume = {4}, year = {2025}, doi = {10.1145/3696430}, url = {https://doi.org/10.1145/3696430}, author = {Musamih, Ahmad and Hassan, Abduraouf and Salah, Khaled and Jayaraman, Raja and Omar, Mohammed and Yaqoob, Ibrar}, keywords = {Child Labor, Social Change, Technological Innovation, Sustainable Development, Blockchain, Object Detection}, abstract = {Child labor has been on the rise in recent years, which necessitates improved identification and reporting mechanisms. Current labor management systems, which are often manual or article-based, lack traceability, audit, privacy, security, and trust features. This leads to challenges in detecting and reporting violations, particularly in large or remote areas. The persistence of this issue undermines the achievement of Sustainable Development Goals (SDGs) and highlights the important role of Corporate Social Responsibility (CSR) in addressing this challenge. Our article proposes a solution combining machine learning and blockchain to automate child labor detection and ensure traceable, auditable, private, and secure reporting. Utilizing Decentralized Proxy Re-Encryption (DPRE), Zero-Knowledge Proofs (ZKPs), and oracles on the Ethereum blockchain, with decentralized storage, our approach maintains privacy and transparency. We present a child labor detection model using Mask2Former and ResNet-18 Convolutional Neural Network (CNN) to achieve high accuracy and reliability. The model’s performance is evaluated using various metrics, achieving an accuracy rate of 89.45\%, a precision score of 0.906, and a recall score of 0.9332. Additionally, we assess smart contracts for cost-efficiency and security, and discuss the solution’s generalizability, challenges, and practical implications. We make the source code of our solution publicly available on GitHub.} }
@article{10.1145/3607870, title = {SMaLL: Software for Rapidly Instantiating Machine Learning Libraries}, journal = {ACM Trans. Embed. Comput. Syst.}, volume = {23}, year = {2024}, issn = {1539-9087}, doi = {10.1145/3607870}, url = {https://doi.org/10.1145/3607870}, author = {Sridhar, Upasana and Tukanov, Nicholai and Binder, Elliott and Low, Tze Meng and McMillan, Scott and Schatz, Martin D.}, keywords = {Mathematical software, machine learning libraries, high-performance, portability, embedded systems}, abstract = {Interest in deploying deep neural network (DNN) inference on edge devices has resulted in an explosion of the number and types of hardware platforms that machine learning (ML) libraries must support. High-level programming interfaces, such as TensorFlow, can be readily ported across different devices; however, maintaining performance when porting the low-level implementation is more nuanced. High-performance inference implementations require an effective mapping of the high-level interface to the target hardware platform. Commonly, this mapping may use optimizing compilers to generate code at compile time or high-performance vendor libraries that have been specialized to the target platform. Both approaches rely on expert knowledge across levels to produce an efficient mapping. This makes supporting new architectures difficult and time-consuming.In this work, we present a DNN library framework, SMaLL, that is easily extensible to new architectures. The framework uses a unified loop structure and shared, cache-friendly data format across all intermediate layers, eliminating the time and memory overheads incurred by data transformation between layers. Each layer is implemented by specifying its dimensions and a kernel, the key computing operation of that layer. The unified loop structure and kernel abstraction allows the reuse of code across layers and computing platforms. New architectures only require a few hundred lines in the kernel to be redesigned. To show the benefits of our approach, we have developed software that supports a range of layer types and computing platforms; this software is easily extensible for rapidly instantiating high-performance DNN libraries.An evaluation of the portability of our framework is shown by instantiating end-to-end networks from the MLPerf:tiny benchmark suite on five ARM platforms and one x86 platform (an AMD Zen 2). We also show that the end-to-end performance is comparable to or better than ML frameworks such as TensorFlow, TVM, and LibTorch.} }
@inproceedings{10.1145/3757749.3757765, title = {Machine Learning Value Prediction based on a Soccer Player's Performance on the Field}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {92--101}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757765}, url = {https://doi.org/10.1145/3757749.3757765}, author = {Chen, Yunzhe}, keywords = {Machine Learning, On-field Performance Data, Player's value prediction, Web Crawling}, abstract = {Football is the most popular sport worldwide. With the development of the global economy, it has evolved beyond a competitive activity into a rapidly expanding market. Especially for frequent market transfers, the value of players has become a hot topic of concern to everyone. This paper employs machine learning to analyze the huge amount of player performance data on the field to predict the value of players. The data in this paper comes from a professional football data website, T Football Website. It mainly crawls various data including offense, organization, defense, etc. of players in the five major leagues in the 23/24 season. Feature selection is carried out by analyzing the correlation of the features, and then the target variable is logarithmically transformed to reduce the scale and alleviate skewness and other preprocessing. In terms of the model, this paper uses three different models, XGBoost, GBDT, and RF, to predict the player's value. Model performance is evaluated by comparing the predicted value with the true value scatter plot, as well as indicators such as R², MAE, and RMSE. The player's value is evaluated through 54 player instances, and the cause of the error is analyzed. The results indicate that the three different models are all feasible for this prediction to a certain extent, but due to factors such as the international reputation of the players, the rise and fall of their values during the growth and decline periods, etc., the models have a certain degree of error.} }
@inproceedings{10.1145/3736733.3736740, title = {Humans, Machine Learning, and Language Models in Union: A Cognitive Study on Table Unionability}, booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics}, year = {2025}, isbn = {9798400719592}, doi = {10.1145/3736733.3736740}, url = {https://doi.org/10.1145/3736733.3736740}, author = {Marimuthu, Sreeram and Klimenkova, Nina and Shraga, Roee}, keywords = {human-in-the-loop, table unionability, data discovery, machine learning, meta-cognitive analysis, large language models, location = Intercontinental Berlin, Berlin, Germany}, abstract = {Data discovery and table unionability in particular became key tasks in modern Data Science. However, the human perspective for these tasks is still under-explored. Thus, this research investigates the human behavior in determining table unionability within data discovery. We have designed an experimental survey and conducted a comprehensive analysis, in which we assess human decision-making for table unionability. We use the observations from the analysis to develop a machine learning framework to boost the (raw) performance of humans. Furthermore, we perform a preliminary study on how LLM performance is compared to humans indicating that it is typically better to consider a combination of both. We believe that this work lays the foundations for developing future Human-in-the-Loop systems for efficient data discovery.} }
@inproceedings{10.1145/3641554.3701815, title = {PhysioML: A Web-Based Tool for Machine Learning Education with Real-Time Physiological Data}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1}, pages = {485--491}, year = {2025}, isbn = {9798400705311}, doi = {10.1145/3641554.3701815}, url = {https://doi.org/10.1145/3641554.3701815}, author = {Hern\'andez-Cuevas, Bryan Y. and Lewis, Myles and Junkins, Wesley and Crawford, Chris S. and Denham, Andre and Luo, Feiya}, keywords = {computer science education, electromyography (emg), machine learning, muscle computer interfaces, physiological computing, location = Pittsburgh, PA, USA}, abstract = {Artificial Intelligence and Machine Learning continue to increase in popularity. As a result, several new approaches to machine learning education have emerged in recent years. Many existing interactive techniques utilize text, image, and video data to engage students with machine learning. However, the use of physiological sensors for machine learning education activities is significantly unexplored. This paper presents findings from a study exploring students' experiences learning basic machine learning concepts while using physiological sensors to control an interactive game. In particular, the sensors measured electrical activity generated from students' arm muscles. Activities featuring physiological sensors produced similar outcomes when compared to exercises that leveraged image data. While students' machine learning self-efficacy increased in both conditions, students seemed more curious about machine learning after working with the physiological sensor. These results suggest that PhysioML may provide learning support similar to traditional ML education approaches while engaging students with novel interactive physiological sensors. We discuss these findings and reflect on ways physiological sensors may be used to augment traditional data types during classroom activities focused on machine learning.} }
@inproceedings{10.1145/3585088.3593929, title = {Developing Machine Learning Agency Among Youth: Investigating Youth Critical Use, Examination, and Production of Machine Learning Applications}, booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference}, pages = {781--784}, year = {2023}, isbn = {9798400701313}, doi = {10.1145/3585088.3593929}, url = {https://doi.org/10.1145/3585088.3593929}, author = {Adisa, Ibrahim Oluwajoba}, keywords = {agency, computational thinking, machine learning, youth, location = Chicago, IL, USA}, abstract = {Abstract. Young people are surrounded by machine learning (ML) devices and their lived experiences are increasingly shaped by the ML technologies that are ever-present in their lives. As innovations in machine learning technologies continue to shape society, it raises important implications for what young people learn, their career trajectories, and the required literacies they need to thrive in this changing occupational environment. Youth are particularly vulnerable to the impact of ML and very little has been done to empower them to critically engage in the discourse surrounding the next generation of technologies that have a marked potential to shape their lives for better or worse. My dissertation work seeks to develop youth autonomy and agency around ML by designing an intervention that supports youth critical use, examination, and production of ML applications in the context of promoting self-expression and social good. I will conduct a qualitative single case study research and collect multiple forms of data using interviews, story completions, digital artifacts, observations, and focus group discussions. These data sources will allow me to conduct an intensive analysis and investigation of how youth populations can be supported to develop the skills, practices and critical consciousness needed to effectively engage with machine learning technologies. Through my research, I also hope to advance the literature on how young people creatively collaborate with ML and use ML for self-expression.} }
@inproceedings{10.1145/3638584.3638678, title = {Review on Research of Automated Machine Learning}, booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence}, pages = {526--532}, year = {2024}, isbn = {9798400708688}, doi = {10.1145/3638584.3638678}, url = {https://doi.org/10.1145/3638584.3638678}, author = {Zhong, Yuyanzhen and Yang, Chuan and Su, Xinyi and Li, Biao and Huang, Xiaoping and Shuai, Yong}, keywords = {Automated Machine Learning, Automatic Processing, Lifelong Learning, Neural Architecture Search, location = Beijing, China}, abstract = {Automated Machine Learning (AutoML) can automatically discover high-performance models to build deep learning systems without human assistance, with the ultimate goal of reducing the complexity and entry barriers of building deep learning systems. Although AutoML has achieved a certain degree of automation through four important steps: data preparation, feature engineering, model generation, and model evaluation, there is still a significant gap compared to the ultimate ideal of achieving truly intelligent lifelong learning. Therefore, a deep understanding of AutoML can help drive the development of artificial intelligence. Firstly, we comprehensively reviewed the latest technologies and achievements involved in these four steps, then we introduced their shortcomings and challenges. Secnodly, a detailed introduction was given to the existing AutoML libraries and the theoretical and practical applications of AutoML. Finally, we summarized AutoML models and Proposed an outlook.} }
@inproceedings{10.1145/3724363.3729028, title = {AfriML: An Interactive and Culturally-Infused Tool for Teaching Machine Learning in Schools}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {23--29}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729028}, url = {https://doi.org/10.1145/3724363.3729028}, author = {Okafor, David Odafe and Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday}, keywords = {afriml, culturally relevant pedagogy, machine learning education, no-code tools, location = Nijmegen, Netherlands}, abstract = {This study developed and assessed the impact of a machine learning (ML) tool called AfriML, a culturally infused, no-code tool designed to engage high school students in learning ML concepts. Inspired by Google's Teachable Machine, AfriML integrates African cultural elements, such as image, accent, and language detectors, to enhance student engagement and relatability. The platform's effectiveness was evaluated through classroom trials involving 40 students and 4 instructors across four schools, employing a mixed-methods approach grounded in Design Science Research (DSR). Results demonstrated significant improvements in student motivation and understanding, highlighting the importance of culturally relevant content in education. Despite resource constraints and technological challenges, AfriML shows promise in making ML education more effective and inclusive. Future work should focus on enhancing language processing capabilities, developing a mobile version for broader accessibility, and extending its application to diverse educational contexts.} }
@inproceedings{10.1145/3588155.3588181, title = {NFT Appraisal Using Machine Learning}, booktitle = {Proceedings of the 2023 5th Asia Pacific Information Technology Conference}, pages = {160--166}, year = {2023}, isbn = {9781450399500}, doi = {10.1145/3588155.3588181}, url = {https://doi.org/10.1145/3588155.3588181}, author = {Dawod, Ahmed Dawod Mohammed and Munkhdalai, Lkhagvadorj and Park, Kwang Ho and Ryu, Keun Ho and Pham, Van Huy}, keywords = {Blockchain, CatBoost, ElasticNet, Lasso, LightGBM, Linear and Polynomial Regression, Non-Fungible Tokens, Random Forest, Ridge, SVM, TabNet, XGBoost, location = Ho Chi Minh City, Vietnam}, abstract = {Non-Fungible Tokens (NFTs) are digital assets based on a blockchain and those are characterized as unique cryptographic tokens and non-interchangeable. To date, research into the NFT marketplace has been relatively limited. As it is an emerging platform with many unique elements, The NFT market has been impacted due to recent fluctuations in crypto-asset markets more broadly. This current bear market cycle has shed light on concerns around the value of NFTs, profit-based motivation, and environmental sustainability. However, periods of volatility and cyclicality are to be expected with any nascent technology as it develops a product-market fit. consequently, the appraisal of real-price for NFT collections is essential for individual financial security and investment making. In this study, we evaluate the machine learning algorithms to appraise their real-price based on NFT item's characteristics, market event information, and their rarity score data acquired by retrieved from the biggest marketplace OpenSea. Furthermore, the procedures were applied to meet the objectives of this study we built prediction models based on various machine-learning algorithms ranging from Random Forest, XGBoost, SVM, Lasso, ElasticNet, Ridge, Linear Polynomial Regression, TabNet, CatBoost, and LightGBM models. From the results, LightGBM regression model outperformed the other by RMSE around 0.905. The best R2 is only found in this model, which has a value of 0.917.} }
@inproceedings{10.1145/3655497.3655498, title = {The Knowledge Training System Based on Machine Learning Technology}, booktitle = {Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence}, pages = {38--44}, year = {2024}, isbn = {9798400709302}, doi = {10.1145/3655497.3655498}, url = {https://doi.org/10.1145/3655497.3655498}, author = {Gong, Zhansheng and Zhai, Chenggong and Ding, Shunjie and Zhang, Heng and Li, Yan and Huang, Yang}, keywords = {AI big mode, Knowledge graph, knowledge training, Machine learning, location = Tokyo, Japan}, abstract = {With the rapid development of information technology, people's demand for knowledge acquisition and application is becoming increasingly strong. However, with the explosive growth of information, it is difficult for ordinary people to effectively access and process a large amount of information, so knowledge training systems have become an increasingly important field. This article focuses on exploring knowledge training systems based on machine learning technology. By introducing the basic situation of machine learning technology and the requirements of knowledge training systems for machine learning technology, a knowledge graph concept based on machine learning technology is proposed, and a knowledge training system and its evaluation system are constructed on this basis. With the arrival of the big data era and the further development of technology, knowledge training systems based on machine learning technology will continue to evolve, enabling people to obtain the required knowledge more effectively.} }
@inproceedings{10.1145/3728199.3728216, title = {Research on Aircraft Delivery Forecasting and Supply-demand Matching Optimization Based on Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {115--119}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728216}, url = {https://doi.org/10.1145/3728199.3728216}, author = {Zhang, Yuehuan}, keywords = {Aircraft delivery forecasting, Deep learning, Machine learning, Production scheduling, Supply-demand matching optimization, neural network}, abstract = {This paper proposes a joint optimization model based on deep learning for aircraft delivery forecasting and supply-demand matching. The model processes historical delivery data and market demand characteristics through DNN to predict aircraft delivery, and combines the optimization algorithm to tune production scheduling and resource allocation, thereby achieving accurate matching of supply and demand. Experimental simulation results show that the joint model based on deep learning is significantly better than traditional statistical methods in prediction accuracy. Specifically, the prediction error of the deep learning model is reduced by about 18\%, and compared with the traditional prediction method based on time series, it can better capture the complex nonlinear relationship of market demand. In addition, in terms of supply and demand matching optimization, the model can effectively reduce resource waste, optimize production scheduling, and reduce inventory costs by about 12\%. Compared with traditional optimization methods, the deep learning joint model performs better in convergence speed and optimization effect.} }
@inproceedings{10.1145/3696843.3696846, title = {SoK Paper: Security Concerns in Quantum Machine Learning as a Service}, booktitle = {Proceedings of the International Workshop on Hardware and Architectural Support for Security and Privacy 2024}, pages = {28--36}, year = {2024}, isbn = {9798400712210}, doi = {10.1145/3696843.3696846}, url = {https://doi.org/10.1145/3696843.3696846}, author = {Kundu, Satwik and Ghosh, Swaroop}, keywords = {Quantum machine learning, training, untrusted providers, security}, abstract = {Quantum machine learning (QML) is a category of algorithms that uses variational quantum circuits (VQCs) to solve machine learning tasks. Recent works have shown that QML models can effectively generalize from limited training data samples. This capability has led to an increased interest in deploying these models to address practical, real-world problems, resulting in the emergence of Quantum Machine Learning as a Service (QMLaaS). QMLaaS represents a hybrid model that utilizes both classical and quantum computing resources. Classical computers play a crucial role in this setup, handling initial pre-processing and subsequent post-processing of data to compensate for the current limitations of quantum hardware. Since this is a new area, very little work exists to paint the whole picture of QMLaaS in the context of known security threats in the domain of classical and quantum machine learning. This SoK paper is aimed to bridge this gap by outlining the complete QMLaaS workflow, which includes both the training and inference phases and highlighting security concerns involving untrusted classical and quantum providers. QML models contain several sensitive assets, such as the model architecture, training data, encoding techniques, and trained parameters. Unauthorized access to these components could compromise the model’s integrity and lead to intellectual property (IP) theft. We pinpoint the critical security issues that must be considered to pave the way for a secure QMLaaS deployment.} }
@inproceedings{10.1145/3677182.3677290, title = {Effective e-commerce price prediction with machine learning technologies}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {603--608}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677290}, url = {https://doi.org/10.1145/3677182.3677290}, author = {Jiang, Kai}, abstract = {The development of e-commerce plays a significant role in improving market efficiency and promoting the optimization of industrial structures. Machine learning technology has been widely used in the exploration and prediction of data in this field. In this study, we aim to investigate the performance of machine learning models for commodity price prediction problems. We used eight different machine learning models to predict commodity prices and discounted prices, and added Bayesian optimization and sentiment analysis to improve the performance of the model. Experimental results showed that RandomForest model had the best comprehensive and prediction performance. In addition, we also found that Bayesian optimization could effectively improve the performance of the models, and the sentiment analysis method did not bring a positive effect in the experiment of predicting discounted prices.} }
@inproceedings{10.1145/3698038.3698548, title = {FaPES: Enabling Efficient Elastic Scaling for Serverless Machine Learning Platforms}, booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing}, pages = {443--459}, year = {2024}, isbn = {9798400712869}, doi = {10.1145/3698038.3698548}, url = {https://doi.org/10.1145/3698038.3698548}, author = {Zhao, Xiaoyang and Yang, Siran and Wang, Jiamang and Diao, Lansong and Qu, Lin and Wu, Chuan}, keywords = {Cluster Scheduling, Distributed System, location = Redmond, WA, USA}, abstract = {Serverless computing platforms have become increasingly popular for running machine learning (ML) tasks due to their user-friendliness and decoupling from underlying infrastructure. However, auto-scaling to efficiently serve incoming requests still remains a challenge, especially for distributed ML training or inference jobs in a serverless GPU cluster. Distributed training and inference jobs are highly sensitive to resource configurations, and demand high model efficiency throughout their lifecycle. We propose FaPES, a FaaS-oriented Performance-aware Elastic Scaling system to enable efficient resource allocation in serverless platforms for ML jobs. FaPES enables flexible resource loaning between virtual clusters for running training and inference jobs. For running inference jobs, servers are reclaimed on demand with minimal preemption overhead to guarantee service level objective (SLO); for training jobs, optimal GPU allocation and model hyperparameters are jointly adapted based on an ML-based performance model and a resource usage prediction board, alleviating users from model tuning and resource specification. Evaluation on a 128-GPU testbed demonstrates up to 24.8\% job completion time reduction and 1.8 Goodput improvement, as compared to representative elastic scaling schemes.} }
@inproceedings{10.1145/3696687.3696698, title = {Analysis of vertical tank vertical deformation based on machine learning and point cloud data}, booktitle = {Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, pages = {60--63}, year = {2024}, isbn = {9798400709876}, doi = {10.1145/3696687.3696698}, url = {https://doi.org/10.1145/3696687.3696698}, author = {Liu, Haiyu and Cui, Lifu}, abstract = {As an important oil storage container, the vertical oil storage tank will be deformed due to the change of service age, wind load, pressure change, operation error and other reasons, which leaves a huge hidden danger for the instability or explosion caused by the storage tank collapse and oil leakage. Machine learning method to optimize the point cloud data processing process, the laboratory vibration table experiment tank model as the research object, the tank vertical deformation analysis, through 3 d laser scanning of point cloud data after downsampling, registration, extraction, slice, center fitting and reference point fitting, analyzed the deformation of the tank in the vertical direction (the maximum residual is about 2.21mm), prove that using machine learning method processing point cloud data analysis vertical tank in vertical deformation is fast and accurate.} }
@inproceedings{10.1145/3748825.3748912, title = {Factors Influencing Rural Migrants' Intentions for Urban Household Registration: An Interpretable Machine Learning Analysis}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {558--563}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748912}, url = {https://doi.org/10.1145/3748825.3748912}, author = {Rao, Jiawen}, keywords = {SHAP value, intentions of urban household registration, machine learning, rural migrants}, abstract = {Leveraging 2017 China Migrants Dynamic Survey (CMDS) data and machine learning algorithms (KNN, GBDT, XGBoost, LightGBM, BP neural networks) with SHAP interpretability, this study investigates determinants of rural migrants' urban household registration intentions. Findings reveal heightened registration intentions in proximity to hometowns and in a state of high psychological identification, but constrained by rural land tenure and housing affordability challenges. Notably, stable urban housing does not uniformly translate to registration transfer. Informed by these insights, policy recommendations emphasize achieving people-centered urbanization: (a) fostering localized urbanization; (b) strengthening socio-psychological integration; (c) establishing flexible land-rights mechanisms; and (d) enhancing housing security.} }
@article{10.14778/3750601.3750620, title = {Magnus: A Holistic Approach to Data Management for Large-Scale Machine Learning Workloads}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {4964--4977}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3750601.3750620}, url = {https://doi.org/10.14778/3750601.3750620}, author = {Song, Jun and Ding, Jingyi and Kandy, Irshad and Lin, Yanghao and Wei, Zhongjia and Zhou, Zilong and Peng, Zhiwei and Shan, Jixi and Mao, Hongyue and Huang, Xiuqi and Song, Xun and Chen, Cheng and Li, Yanjia and Yang, Tianhao and Jia, Wei and Dong, Xiaohong and Lei, Kang and Shi, Rui and Zhao, Pengwei and Chen, Wei}, abstract = {Machine learning (ML) has become a cornerstone of key applications at ByteDance. As model complexity and data volumes surge, data management for large-scale ML workloads faces substantial challenges, particularly with recent advances in large recommendation models (LRMs) and large multimodal models (LMMs). Traditional approaches exhibit limitations in storage efficiency, metadata scalability, update mechanisms, and integration with ML frameworks. To address these challenges, we propose Magnus, a holistic data management system built upon Apache Iceberg. Magnus integrates innovative optimizations across resource-efficient storage formats optimized for large wide tables and multimodal data, built-in support for vector and inverted indexes to accelerate data retrieval, scalable metadata planning with Git-like branching and tagging capabilities, and high-performance update/upsert based on lightweight merge-on-read (MOR) strategies. Additionally, Magnus provides native support and specialized enhancement for LRM and LMM training workloads. Experimental results demonstrate significant performance gains in real-world ML scenarios. Magnus has been deployed at ByteDance for over five years, enabling robust and efficient data infrastructure for large-scale ML workloads.} }
@inproceedings{10.1145/3580305.3599574, title = {Trustworthy Machine Learning: Robustness, Generalization, and Interpretability}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5827--5828}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599574}, url = {https://doi.org/10.1145/3580305.3599574}, author = {Wang, Jindong and Li, Haoliang and Wang, Haohan and Pan, Sinno Jialin and Xie, Xing}, keywords = {adversarial learning, interpretability, out-of-distribution generalization, trustworthy machine learning, location = Long Beach, CA, USA}, abstract = {Machine learning is becoming increasingly important in today's world. Beyond its powerful performances, there has been an emerging concern about the trustworthiness of machine learning, including but not limited to: robustness to malicious attacks, generalization to unseen datasets, and interpretability to explain its outputs. Such concerns are even more urgent in some safety-critical applications such as medical diagnosis and autonomous driving. Trustworthy machine learning (TrustML) aims to tackle these challenges from the perspectives of theory, algorithm, and applications. In this tutorial, we will give a comprehensive introduction to the recent advance of trustworthy machine learning in robustness, generalization, and interpretability. We will cover their problem formulation, related research, popular algorithms, and successful applications. Additionally, we will also introduce some potential challenges for future research. We do hope that this tutorial will not only serve as a platform to understand TrustML, but also raise the awareness of everyone for more trustworthy applications.} }
@inproceedings{10.1145/3562939.3565688, title = {Visualizing Machine Learning in 3D}, booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology}, year = {2022}, isbn = {9781450398893}, doi = {10.1145/3562939.3565688}, url = {https://doi.org/10.1145/3562939.3565688}, author = {Rivera, Diego}, keywords = {Interactive, Neural Networks, Transformers, location = Tsukuba, Japan}, abstract = {Understanding machine learning models can be difficult when the models at hand have many parts to them. Having a visual model can help aid in understanding how the model functions. A way to visualize these models is to use a 3D (three-dimensional) game development application. An application that will have an interactive element allowing the users to interact with the model (rotating and scaling it) and see changes at run-time. An interactive element will keep the users engaged, understanding, and seeing how a machine learning model looks and behaves. This paper describes the process of visualizing a machine learning model in a 3D application.} }
@inproceedings{10.1145/3658644.3690337, title = {Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {4480--4494}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690337}, url = {https://doi.org/10.1145/3658644.3690337}, author = {Bagdasarian, Eugene and Shmatikov, Vitaly}, keywords = {ML security, backdoors, hyperparameter search, location = Salt Lake City, UT, USA}, abstract = {Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a "backdoor" task (e.g., misclassify inputs with a certain feature) in addition to its main task. Recent research proposed many hypothetical backdoor attacks whose efficacy depends on the configuration and training hyperparameters of the target model. At the same time, state-of-the-art defenses require massive changes to the existing ML pipelines and protect only against some attacks.Given the variety of potential backdoor attacks, ML engineers who are not security experts have no way to measure how vulnerable their current training pipelines are, nor do they have a practical way to compare training configurations so as to pick the more resistant ones. Deploying a defense may not be a realistic option, either. It requires evaluating and choosing from among dozens of research papers, completely re-engineering the pipeline as required by the chosen defense, and then repeating the process if the defense disrupts normal model training (while providing theoretical protection against an unknown subset of hypothetical threats).In this paper, we aim to provide ML engineers with pragmatic tools to audit the backdoor resistance of their training pipelines and to compare different training configurations, to help choose the one that best balances accuracy and security.First, we propose a universal, attack-agnostic resistance metric based on the minimum number of training inputs that must be compromised before the model learns any backdoor.Second, we design, implement, and evaluate Mithridates, a multi-stage approach that integrates backdoor resistance into the training-configuration search. ML developers already rely on hyperparameter search to find configurations that maximize the model's accuracy. Mithridates extends this tool to also order configurations based on their backdoor resistance. We demonstrate that Mithridates discovers configurations whose resistance to multiple types of backdoor attacks increases by 3-5x with only a slight impact on accuracy. We also discuss extensions to AutoML and federated learning.} }
@inproceedings{10.1145/3671151.3671363, title = {Research on depression diagnosis based on the Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {1213--1218}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671363}, url = {https://doi.org/10.1145/3671151.3671363}, author = {Li, Ningyu and Wang, Guixiang}, abstract = {Depression is a common mental disorder that has serious effects on patients' physical and mental health. Diagnosis of depression in clinical practice mainly relies on the experience and professional knowledge of physicians, which results in inaccurate diagnosis, misdiagnosis, and missed diagnosis. In recent years, the development of machine learning technology has provided new possibilities for automated diagnosis of depression. This paper proposes a machine learning-based diagnosis model for depression and validates the model through experiments. The model uses the deep neural network gcForest (multi-granularity cascade forest) to classify patients with depression and normal individuals, and uses multiple physiological features as input, including electroencephalogram data, heart rate variability, skin conductance activity, etc. The experimental results show that the model can effectively distinguish patients with depression from normal individuals and has high accuracy, precision, recall, and F1 scores. The machine learning-based diagnosis model for depression proposed in this study has certain clinical application value, can provide doctors with diagnostic assistance tools, and is expected to provide more effective means for early warning and treatment of depression in the future.} }
@inproceedings{10.1145/3664647.3680665, title = {AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models}, booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia}, pages = {8586--8594}, year = {2024}, isbn = {9798400706868}, doi = {10.1145/3664647.3680665}, url = {https://doi.org/10.1145/3664647.3680665}, author = {Luo, Daqin and Feng, Chengjian and Nong, Yuxuan and Shen, Yiqing}, keywords = {automated machine learning, human-ai interaction, large language model, usability, user study, location = Melbourne VIC, Australia}, abstract = {Automated Machine Learning (AutoML) offers a promising approach to streamline the training of machine learning models. However, existing AutoML frameworks are often limited to unimodal scenarios and require extensive manual configuration. Recent advancements in Large Language Models (LLMs) have showcased their exceptional abilities in reasoning, interaction, and code generation, presenting an opportunity to develop a more automated and user-friendly framework. To this end, we introduce AutoM3L, an innovative Automated Multimodal Machine Learning framework that leverages LLMs as controllers to automatically construct multimodal training pipelines. AutoM3L comprehends data modalities and selects appropriate models based on user requirements, providing automation and interactivity. By eliminating the need for manual feature engineering and hyperparameter optimization, our framework simplifies user engagement and enables customization through directives, addressing the limitations of previous rule-based AutoML approaches. We evaluate the performance of AutoM3L on six diverse multimodal datasets spanning classification, regression, and retrieval tasks, as well as a comprehensive set of unimodal datasets. The results demonstrate that AutoM3L achieves competitive or superior performance compared to traditional rule-based AutoML methods. Furthermore, a user study highlights the user-friendliness and usability of our framework, compared to the rule-based AutoML methods.} }
@inproceedings{10.1145/3576781.3608735, title = {Towards Molecular Machine Learning for the IoBNT}, booktitle = {Proceedings of the 10th ACM International Conference on Nanoscale Computing and Communication}, pages = {168--169}, year = {2023}, isbn = {9798400700347}, doi = {10.1145/3576781.3608735}, url = {https://doi.org/10.1145/3576781.3608735}, author = {Angerbauer, Stefan and Enzenhofer, Franz and Pankratz, Tobias and Haselmayr, Werner}, keywords = {Internet of Bio-Nano Things, Molecular Machine Learning, Molecular communications, location = Coventry, United Kingdom}, abstract = {In this paper, we present a reaction-diffusion-based architecture, which can be used as basic building block for the implementation of machine learning algorithms at the nano-scale. We present the principle and a first mathematical model and validated the proposed approach through particle-based simulations.} }
@inproceedings{10.1145/3747227.3747255, title = {Intelligent monitoring of insult management based on machine learning and multi-source big data — Application innovation for digital economy}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {167--172}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747255}, url = {https://doi.org/10.1145/3747227.3747255}, author = {Lu, Ling and Xie, Ling and Luo, Renjie and Yuan, Dan and Peng, Ling and Lin, Haibo}, keywords = {Abusive Supervision, Intelligent Monitoring, Machine Learning, Multi-Source Big Data, Organizational Behavior Management}, abstract = {In the context of the digital economy, this study explores an intelligent monitoring framework for abusive supervision based on machine learning and multi-source big data. By integrating support vector machines (SVM) and random forests (RF), the research constructs a fusion model to enhance the identification and early warning of abusive behaviors within organizations. Through data collection from employee behavior records, internal feedback, and social media, the system achieves real-time analysis and high-accuracy risk prediction. Experimental results show that the integrated model outperforms single models in accuracy, precision, recall, and F1 score, especially under imbalanced data conditions. Furthermore, key variables such as job satisfaction, emotional scores, and behavioral frequency were identified as critical indicators for early detection. This study not only validates the application potential of data-driven methods in organizational behavior management but also provides enterprises with a practical intelligent tool to optimize leadership behaviors and improve employee experience.} }
@inproceedings{10.1145/3589335.3665841, title = {Leveraging Machine Learning Models for Trustworthy Prediction of Diabetes}, booktitle = {Companion Proceedings of the ACM Web Conference 2024}, pages = {1872--1875}, year = {2024}, isbn = {9798400701726}, doi = {10.1145/3589335.3665841}, url = {https://doi.org/10.1145/3589335.3665841}, author = {B, Aruna Devi and N, Karthik}, keywords = {anomalies, diabetes, imbalanced dataset, missing values, xai, location = Singapore, Singapore}, abstract = {A significant and widespread health issue among people of all ages is diabetes. Incorporating machine learning (ML) algorithms in clinical care can assist in the early detection of diabetes and preventing patients from experiencing significant health issues caused by the impact of diabetes. Furthermore, the most recent Explainable Artificial Intelligence (XAI) techniques may enable end users to understand and trust AI decisions. This research proposes an approach to detect diabetes by leveraging ML algorithms. The detection of diabetes allows for prompt medication, dietary, and lifestyle modifications, which enhance blood sugar regulation and lower the risk of complications from diabetes. Building a reliable machine learning model is influenced by several factors, including anomalies, data quality, and model selection. This work emphasizes building a trustworthy model to predict diabetes by overcoming the issues of missing data, anomalies, and appropriate model selection, along with an understanding of the results obtained by the model.} }
@inproceedings{10.1145/3650215.3650280, title = {Rethinking on Misleading of Machine Learning in Bank Risk Warning System}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {367--373}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650280}, url = {https://doi.org/10.1145/3650215.3650280}, author = {Xiao, Yan and He, Zehao and Liu, Yuyang}, abstract = {Machine learning is widely used in the bank risk warning system more and more popular. It has been able to provide early internal bank risk warning nowadays. However, its ability of external risk warning is limited and need a long way to go to meet the needs of bank risk warning system. This study shows that external bank risk warning information, especially the subtle information provided by the machine learning would mislead the bank risk warning system. Analyses indicated that machine learning relied heavily on the accuracy of data, typically provided predictive rather than deterministic results, overlooked or underestimated unexpected events during the learning process, and it might provide opposite predictive results when dealing with external subtle information. With the supports of external subtle information filtering criteria and bank simulation training center, machine learning could jointly improve its ability to process external subtle information and reduce the possibilities of misleading in bank risk warning system.} }
@inproceedings{10.1145/3615366.3622793, title = {Advanced Machine Learning for Runtime Data Generation}, booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing}, pages = {182--187}, year = {2023}, isbn = {9798400708442}, doi = {10.1145/3615366.3622793}, url = {https://doi.org/10.1145/3615366.3622793}, author = {Zamir, Bukhtawar and Campos, Jo\~ao R. and Vieira, Marco}, keywords = {Artificial Intelligence, Generative Models, Machine Learning, location = La Paz, Bolivia}, abstract = {Given the ubiquity of software in everyday critical tasks, ensuring its dependability is of utmost importance. Software faults, which can lead to errors and vulnerabilities, can significantly comprise the target system. Various techniques have been developed to improve the dependability of software-intensive systems, from fault avoidance to fault tolerance. Machine Learning (ML) techniques have been playing a vital role in improving the dependability of systems. Nonetheless, such techniques require significant amounts of data, which are not typically available. To overcome this, various techniques, such as fault injection or intrusion injection, have been proposed to generate realistic data. Still, they are computationally expensive and require considerable expertise. At the same time, a recent growing sub-field of ML is generative models. Generative models offer an innovative solution by creating synthetic data that closely resemble real-world samples. If such models could be used to generate realistic synthetic failure or intrusion data on demand, their value would be significant. Notwithstanding, the feasibility of such an approach has not yet been researched. Generative models have only mostly been used for sequential data (e.g., text or music) or data with high spatial dependency (e.g., images). On the other hand, dependability problems often have high dimensional tabular data, for which generative models are yet to excel, and for which it is also considerably more difficult to assess the representativeness of the generated data. This research will focus on determining the feasibility of using generative techniques to generate runtime data to support dependability research.} }
@inproceedings{10.1145/3658271.3658326, title = {Investigating Predicting Voluntary Resignation Program Participation with Machine Learning}, booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems}, year = {2024}, isbn = {9798400709968}, doi = {10.1145/3658271.3658326}, url = {https://doi.org/10.1145/3658271.3658326}, author = {Jorge, Ezequiel Mule and Barbieri, L\'ucio Tales and Escovedo, Tatiana and Kalinowski, Marcos}, keywords = {Algorithms, Machine Learning, Models, Prediction, Voluntary Resignation Programs, location = Juiz de Fora, Brazil}, abstract = {Context: The growing challenge in attracting employees to Voluntary Resignation Programs (VRP) lies in the need to balance the company’s cost control with the goal of increasing participation from the target audience. Problem: It is essential to ensure that the process occurs smoothly, reducing tension during the separation and fostering a more cooperative and responsible environment. Companies need to maximize attraction to VRP, minimize costs, and improve resource allocation. Solution: This article aims to construct a Machine Learning (ML) model to predict employee participation VRPs in an organizational context and to identify the key factors that influence employee participation in the program, identifying patterns and trends based on previous programs. IS Theory: This work is associated with the Theory of Computational Learning, which aims to understand the fundamental principles of learning and design better-automated methods. Method: This article constitutes a study of past data, aiming to identify patterns and develop trends related to employee participation through the utilization of ML algorithms. Summary of Results: The investigation into predicting VRP participation using ML revealed compelling correlations. The Bootstrap Aggregating with Logistic Regression model emerged as the most effective, demonstrating high F1-Score and Accuracy. Contributions and Impact in the IS area: The research significantly contributes to the IS field by showcasing ML’s application in predicting VRP participation, enriching our understanding of factors influencing employee decisions and highlights technology-driven solutions in workforce management. Insights from this investigation offer a valuable framework for future research, paving the way for predictive analytics integration in addressing complex HR challenges within the broader IS context.} }
@inproceedings{10.1145/3675417.3675484, title = {Analysis of Random Initialization Methods in Machine Learning}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {405--409}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675484}, url = {https://doi.org/10.1145/3675417.3675484}, author = {Wei, Yucheng and Li, Xiangdong and Lang, Fengyong and Wang, Yi and Ma, Teng}, abstract = {Machine learning is an important field in artificial intelligence. In machine learning, the process of randomly initializing model parameters during model training is very important because good random initialization can help the model converge to better solutions faster. The choice of initialization method depends on factors such as the structure of the model, the nature of the data, and the activation function used. In practice, people usually try different initialization methods and choose the best initialization strategy based on the performance of the model. Through analysis, it was found that in most cases, random initialization methods use normal distribution to initialize parameters, while uniform distribution is used to initialize parameters when each value has an equal probability.} }
@inproceedings{10.1145/3715020.3715058, title = {Redefined Classification of Hepatitis Variants through Arima, Signal Processing and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computational Biology and Bioinformatics}, pages = {136--143}, year = {2025}, isbn = {9798400709623}, doi = {10.1145/3715020.3715058}, url = {https://doi.org/10.1145/3715020.3715058}, author = {Shah, Vatsalkumar Vipulkumar and Fadia, Love and Hassanzadeh, Mohammad and Ahmadi, Majid and Wu, Jonathan}, keywords = {Genomic Data Analysis, Hepatitis Virus, Machine learning, ARIMA and Finite Impulse Response Filters}, abstract = {This study introduces a novel feature selection method using the Auto Regressive Moving Average model to classify four hepatitis virus types: Hepatitis B, C, D, and E. Firstly, DNA sequences are transformed from characters into numerical representations using Electron-Ion Interaction Potential coding. Then discrete sine transform is applied to extract features from the sequences. Our proposed feature selection technique, using the inverse integrated moving average (ARIMA) as a statistical filter, refines the features for optimal classification. We train two machine learning models Light Gradient Boosting Machine and Random Forest on the selected features. We compare the results of our approach with another 5 window-based finite impulse response filters (FIR) as they extract significant features by reducing noise and emphasizing key patterns. Our findings indicate that ARIMA, with an order of (1,1,5), achieves 98\% accuracy, while the Bartlett window-based filter yields 93.25\% accuracy. This approach highlights the potential of ARIMA for effective feature selection in the classification of hepatitis viruses using machine learning.} }
@inproceedings{10.1145/3747227.3747260, title = {Assessment of ship carbon emission reduction technology based on analytical hierarchy and machine learning models}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {197--204}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747260}, url = {https://doi.org/10.1145/3747227.3747260}, author = {Wang, Yutong and Tao, Wenmei and Liu, Yimeng}, keywords = {analytical hierarchy, carbon emission, comprehensive assessment, logistic regression, random forest, ship emission}, abstract = {The issue of ship carbon emissions is becoming increasingly prominent. Researchers have proposed various measures to reduce carbon emissions, achieving certain levels of reduction. However, each technique also has its own operational drawbacks. Currently, there is a lack of comprehensive evaluation methods to specifically compare the merits and drawbacks of different technologies. Therefore, this paper proposes the use of Random Forest and Logistic Regression models for prediction of carbon emission reduction technologies. Analytic Hierarchy Process (AHP) to quantitatively evaluate and analyze five typical ship carbon emission reduction technologies. Firstly, the AHP logic is applied to establish a comprehensive evaluation index system for ship carbon emission reduction technologies, encompassing three criteria layers of economy, environment and technology and eleven scheme layers. Secondly, by constructing a judgment matrix, the weight coefficients for each scheme layer are derived, and the overall weights for each criterion are obtained. Finally, three experts were invited to rate the carbon emission reduction technologies, resulting in a ranking of their merits and drawbacks as follows: ammonia fuel technology \&gt; carbon capture technology \&gt; wind-assisted propulsion technology \&gt; air layer drag reduction technology \&gt; hydrogen fuel technology. Random Forest model achieves better prediction accuracy compared to Logistic Regression model. The AHP quantitative research method applied in this paper can provide a valuable decision-making support for ship operators.} }
@inproceedings{10.1145/3625007.3632288, title = {pyStudio: An Open-Source Machine Learning Platform}, booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining}, pages = {436--440}, year = {2024}, isbn = {9798400704093}, doi = {10.1145/3625007.3632288}, url = {https://doi.org/10.1145/3625007.3632288}, author = {Gomicia-Murccia, Enrique and Souissi, Riad and AL-Qurishi, Muhammad and Bordel S\'anchez, Borja}, abstract = {Data analytics has emerged as a critical capability for businesses and organizations in the modern era. The abundance of data necessitates a deep understanding and the exploitation of its potential to gain insights into current and future scenarios.This paper introduces an integrated platform designed to streamline data acquisition, storage, management, processing, and visualization. The primary objective is to facilitate data analysis by offering a machine learning studio equipped with pre-built algorithms. Remarkably, this platform eliminates the need for coding, allowing users to effortlessly generate AI models. Furthermore [19], it provides a secure environment for sharing these models without compromising data privacy---a noteworthy contribution in the realm of federated learning (FL).The platform's significance lies in its ability to empower nontechnical users to perform advanced tasks without requiring specialized expertise.} }
@inproceedings{10.1145/3589334.3645520, title = {ModelGo: A Practical Tool for Machine Learning License Analysis}, booktitle = {Proceedings of the ACM Web Conference 2024}, pages = {1158--1169}, year = {2024}, isbn = {9798400701719}, doi = {10.1145/3589334.3645520}, url = {https://doi.org/10.1145/3589334.3645520}, author = {Duan, Moming and Li, Qinbin and He, Bingsheng}, keywords = {ai licensing, license analysis, model mining, location = Singapore, Singapore}, abstract = {Productionizing machine learning projects is inherently complex, involving a multitude of interconnected components that are assembled like LEGO blocks and evolve throughout development lifecycle. These components encompass software, databases, and models, each subject to various licenses governing their reuse and redistribution. However, existing license analysis approaches for Open Source Software (OSS) are not well-suited for this context. For instance, some projects are licensed without explicitly granting sublicensing rights, or the granted rights can be revoked, potentially exposing their derivatives to legal risks. Indeed, the analysis of licenses in machine learning projects grows significantly more intricate as it involves interactions among diverse types of licenses and licensed materials. To the best of our knowledge, no prior research has delved into the exploration of license conflicts within this domain. In this paper, we introduce ModelGo, a practical tool for auditing potential legal risks in machine learning projects to enhance compliance and fairness. With ModelGo, we present license assessment reports based on five use cases with diverse model-reusing scenarios, rendered by real-world machine learning components. Finally, we summarize the reasons behind license conflicts and provide guidelines for minimizing them. Our code is publicly available at https://github.com/Xtra-Computing/ModelGo.} }
@inbook{10.1145/3757749.3757776, title = {Machine Learning-XGBoost Analysis of Subjective Well-being Among Chronic Hepatitis B Patients}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {165--170}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757776}, author = {Li, Lala and Zhou, Jie}, abstract = {This study examined the relationships among objective social support, subjective social support, social participation, self-efficacy, and subjective well-being in chronic hepatitis B (CHB) patients using the XGBoost machine learning algorithm. Data were collected from 253 CHB patients. Using an optimal hyperparameter search, the XGBoost model achieved a classification accuracy of 98.04\%. The results indicated that objective support, subjective support, self-efficacy, and social participation significantly predicted subjective well-being. XGBoost also highlighted self-efficacy as the most crucial predictive factor. These findings emphasize targeted psychological interventions to enhance self-efficacy and social support among CHB patients.} }
@inproceedings{10.1145/3643651.3659899, title = {Modeling and Security Analysis of Attacks on Machine Learning Systems}, booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics}, pages = {1--2}, year = {2024}, isbn = {9798400705564}, doi = {10.1145/3643651.3659899}, url = {https://doi.org/10.1145/3643651.3659899}, author = {Singhal, Anoop}, keywords = {deep learning. security analysis, machine learning, location = Porto, Portugal}, abstract = {The past several years have witnessed rapidly increasing use of machine learning (ML) systems in multiple industry sectors. Since security analysis is one of the most essential parts of the real-world ML system protection practice, there is an urgent need to conduct systematic security analysis of ML systems. However, it is widely recognized that the existing security analysis approaches and techniques, which were developed to analyze enterprise (software) systems and networks, are no longer very suitable for analyzing ML systems. In this paper, we present a methodology for ML-system-specific security analysis.} }
@inproceedings{10.1145/3626232.3658637, title = {Machine Learning Techniques for Python Source Code Vulnerability Detection}, booktitle = {Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy}, pages = {151--153}, year = {2024}, isbn = {9798400704215}, doi = {10.1145/3626232.3658637}, url = {https://doi.org/10.1145/3626232.3658637}, author = {Farasat, Talaya and Posegga, Joachim}, keywords = {bilstm, python, software vulnerabilities, location = Porto, Portugal}, abstract = {Software vulnerabilities are a fundamental reason for the prevalence of cyber attacks and their identification is a crucial yet challenging problem in cyber security. In this paper, we apply and compare different machine learning algorithms for source code vulnerability detection specifically for Python programming language. Our experimental evaluation demonstrates that our Bidirectional Long Short-Term Memory (BiLSTM) model achieves a remarkable performance (average Accuracy = 98.6\%, average F-Score = 94.7\%, average Precision = 96.2\%, average Recall = 93.3\%, average ROC = 99.3\%), thereby, establishing a new benchmark for vulnerability detection in Python source code.} }
@article{10.1145/3680463, title = {An Empirical Study of Testing Machine Learning in the Wild}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3680463}, url = {https://doi.org/10.1145/3680463}, author = {Openja, Moses and Khomh, Foutse and Foundjem, Armstrong and Jiang, Zhen Ming (Jack) and Abidi, Mouna and Hassan, Ahmed E.}, keywords = {Machine learning, Deep learning, Software Testing, Machine learning workflow, Testing strategies, Testing methods, ML properties, Test types/Types of testing}, abstract = {Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies.Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow.Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems.Results: Our findings reveal several key insights: (1) The most common testing strategies, accounting for less than 40\%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. (2) A wide range of (17) ML properties are tested, out of which only 20\% to 30\% are frequently tested, including Consistency, Correctness, and Efficiency. (3) Bias and Fairness is more tested in Recommendation (6\%) and Computer Vision (CV) (3.9\%) systems, while Security and Privacy is tested in CV (2\%), Application Platforms (0.9\%), and NLP (0.5\%). (4) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing.Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.} }
@inproceedings{10.1109/WSESE66602.2025.00016, title = {Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?}, booktitle = {Proceedings of the 2025 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering}, pages = {56--63}, year = {2025}, isbn = {9798331502256}, doi = {10.1109/WSESE66602.2025.00016}, url = {https://doi.org/10.1109/WSESE66602.2025.00016}, author = {Costalonga, Marcelo and Napole\~ao, Bianca Minetto and Baldassarre, Maria Teresa and Felizardo, Katia Romero and Steinmacher, Igor and Kalinowski, Marcos}, keywords = {systematic review automation, selection of studies, machine learning, systematic literature review update, location = Ottawa, Ontario, Canada}, abstract = {[Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort. Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence. [Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates. [Method] We reproduce the study selection of an SLR update performed by three SE researchers. We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR. We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure. We also compared the performance of human-ML pairs with human-only pairs when selecting studies. [Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation. However, we found that such models can reduce the study selection effort by 33.9\% without loss of evidence (keeping a 100\% recall). Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result. [Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase.} }
@inbook{10.1145/3658617.3697772, title = {A Hybrid Machine Learning and Numeric Optimization Approach to Analog Circuit Deobfuscation}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {801--807}, year = {2025}, isbn = {9798400706356}, url = {https://doi.org/10.1145/3658617.3697772}, author = {Jain, Dipali Deepak and Zhao, Guangwei and Datta, Rajesh Kumar and Shamsi, Kaveh}, abstract = {Oracle-guided circuit deobfuscation (or learning) is the problem of disambiguating an obfuscated (partially hidden) circuit given black-box access to it. This has applications in various hardware security areas such as analyzing the security of circuit obfuscation defense schemes, side-channel analysis, reverse engineering, and hardware Trojan detection. Generic deobfuscation of analog circuits has received less attention than the digital counterpart with existing methods relying on manual expert work to extract closed-form equations from the circuit. In this work, we move towards a significantly more automated process by using a combination of machine learning and Newton-method-based analog circuit optimization. We showcase how this hybrid scheme is superior to either standalone approach in terms of runtime and accuracy on a set of analog circuits that include amplifiers, filters, and oscillators. We achieve \&gt;98\% average accuracy without any manual expert equation extraction in addition to demonstrating a superior resilience to process variation.} }
@inproceedings{10.1145/3759928.3759946, title = {Research on the Construction of Digital Art Graphics Classification and Retrieval Framework under Machine Learning Technology}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {101--106}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759946}, url = {https://doi.org/10.1145/3759928.3759946}, author = {Pan, Lina}, keywords = {digital art graphics, image classification, structure perception, style embedding}, abstract = {To improve the classification and retrieval accuracy of digital art graphics with complex style attributes, this study proposes a joint framework integrating style and structural embeddings. A multi-scale residual network with channel attention is employed for feature extraction, and dual-branch representations are constructed for stylistic semantics and geometric structure. A multitask classification model and a dual-tower cross-modal retrieval structure are designed. A two-level retrieval strategy—style-based indexing and structure-based matching—supports efficient similarity fusion, while contrastive learning enhances embedding alignment. Experimental results show that the framework surpasses existing methods in multi-label classification, embedding clustering, and cross-modal retrieval, particularly in distinguishing similar styles and supporting text-guided queries. These findings validate the effectiveness and scalability of the dual-branch embedding approach for complex art graphic semantics.} }
@inproceedings{10.1109/SCW63240.2024.00103, title = {Predicting Compute Node Unavailability in HPC: A Graph-Based Machine Learning Approach}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {737--740}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00103}, url = {https://doi.org/10.1109/SCW63240.2024.00103}, author = {Krumpak, Roy and Rozanec, Joze M. and Molan, Martin and Angelinelli, Matteo and Bartolini, Andrea}, keywords = {Anomalies Forecasting, Artificial Intelligence, Data Center, Graphs, HPC, Machine Learning, location = Atlanta, GA, USA}, abstract = {As high-performance computing (HPC) systems advance towards Exascale computing, their size and complexity increase, introducing new maintenance challenges. Modern HPC systems feature data monitoring infrastructures that provide insights into the system's state. This data can be leveraged to train machine learning models to anticipate anomalies that require compute nodes to undergo maintenance procedures. This paper presents a novel approach to predicting such anomalies by creating a graph per measurement that encodes current and past sensor readings and information related to the compute node sensors. The experiments were performed with data collected from Marconi 100, a tier-0 production supercomputer at CINECA in Bologna, Italy. Our results show that the machine learning model can accurately predict anomalies and surpass current State-Of-The-Art (SOTA) models regarding the quality of predictions and the time horizon considered to forecast them.} }
@inproceedings{10.1145/3647444.3652464, title = {Detection of Pneumonia using Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652464}, url = {https://doi.org/10.1145/3647444.3652464}, author = {Bhattarai, Pankaj and K a, Varun Kumar and Th, Balachander and Od, Rashmi}, keywords = {Deep Learning, Image Segmentation, Medical Imaging, Pneumonia, X-Ray Images, location = Jaipur, India}, abstract = {Pneumonia is a severe respiratory infection that can cause life-threatening complications if left untreated. Although an early and accurate diagnosis is essential for successful treatment, conventional techniques of diagnosis can be expensive and time-consuming. For the automated detection of pneumonia from medical images, deep learning algorithms and computer vision methods have recently been investigated. In this study, we suggest a system for automatically identifying pneumonia from chest X-ray images using deep learning algorithms. In order to categorize chest X-ray pictures as either pneumonia-positive or pneumonia-negative, we will create a convolutional neural network (CNN) model that will be trained on a dataset of chest X-ray images. Using different assessment metrics, such as accuracy, sensitivity, and specificity, we will assess the model's performance. Particularly in re-source-constrained settings with a shortage of skilled medical personnel, the suggested system has the potential to greatly improve the effectiveness and accuracy of pneumonia diagnosis. The system is a useful instrument for the medical community because of its capacity to provide early and accurate diagnosis, which may be able to save lives and enhance patient outcomes to a considerable degree.} }
@inproceedings{10.1145/3712256.3726462, title = {Machine Learning-Assisted Constraint Handling Under Variable Uncertainty for Preference-based Multi-Objective Optimization}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {508--516}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3726462}, url = {https://doi.org/10.1145/3712256.3726462}, author = {Yadav, Deepanshu and Ramu, Palaniappan and Deb, Kalyanmoy}, keywords = {evolutionary algorithms, multi-criteria decision-making, machine learning, reference point, reliability, uncertainty, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Evolutionary Multi-objective Optimization (EMO) algorithms are widely used to solve real-world multi-objective optimization problems, aiming to obtain a set of non-dominated solutions close to the Pareto front. However, most EMO methods assume deterministic decision variables, ignoring inherent uncertainties in engineering applications, which can lead to design failures, especially in reliability-based designs. Reliability-based Multi-objective Optimization (ReMOO) addresses this issue by incorporating variable uncertainty and probabilistic constraints to generate a Reliable Front. ReMOO operates using a bi-level framework: the outer level optimizes objective functions, while the inner level estimates reliability through computationally intensive methods, like Monte Carlo Simulation (MCS) or the Performance Measure Approach (PMA). Additionally, decision-makers (DMs) often select only a subset of reliable solutions, limiting computational efficiency. To overcome these challenges, this paper proposes a Machine Learning-assisted reliability-based Multi-Criteria Decision-Making (ML-ReMCDM) technique. ML models are trained on reliability-based constraints within the decision space before an EMO execution. In the inner loop, ML models predict probabilistic constraints and reliability indices, significantly reducing computational costs. Moreover, the outer loop computes only the DM-preferred segment of the reliable front, further enhancing efficiency. The ML-ReMCDM approach, implemented on several benchmark and real-world examples, demonstrates substantial improvements in computational efficiency as well as practical applicability.} }
@article{10.1145/3511543, title = {Steampunk Machine Learning: Victorian contrivances for modern data science}, journal = {Queue}, volume = {19}, pages = {5--17}, year = {2022}, issn = {1542-7730}, doi = {10.1145/3511543}, url = {https://doi.org/10.1145/3511543}, author = {Kelly, Terence}, abstract = {Fitting models to data is all the rage nowadays but has long been an essential skill of engineers. Veterans know that real-world systems foil textbook techniques by interleaving routine operating conditions with bouts of overload and failure; to be practical, a method must model the former without distortion by the latter. Surprisingly effective aid comes from an unlikely quarter: a simple and intuitive model-fitting approach that predates the Babbage Engine. The foundation of industrial-strength decision support and anomaly detection for production datacenters, this approach yields accurate yet intelligible models without hand-holding or fuss. It is easy to practice with modern analytics software and is widely applicable to computing systems and beyond.} }
@inproceedings{10.1145/3607822.3616410, title = {Machine Learning on Topological Constraint for Mismatching Removal}, booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction}, year = {2023}, isbn = {9798400702815}, doi = {10.1145/3607822.3616410}, url = {https://doi.org/10.1145/3607822.3616410}, author = {Shen, Chentao and He, Zaixing and Zhao, Xinyue and Qiu, Mengyu}, keywords = {Machine Learning, Mismatch Removal, Sampling, Topological Constraints, location = Sydney, NSW, Australia}, abstract = {Image feature matching is a critical task in human-computer interaction, such as driverless cars, collaborative medical robots, aiming to establish correspondence between two images inputted from human or computer. To ensure the accuracy of matches, removing mismatches is of utmost importance. In recent years, machine learning has emerged as a new perspective for achieving effective mismatch removal. However, current learning-based methods suffer from a lack of generalizability due to their heavy reliance on extensive image data for training. Consequently, handling cases with a high mismatch ratio becomes challenging. In this paper, a novel approach is proposed that incorporates the triangular topology constraint into machine learning, which we called LTM. By summarizing topology constraints around the matching points and integrating the idea of sampling, we successfully address the mismatch removal task. Notably, our method stands out by requiring few parameters as input, enabling us to train it using just several image pairs from four sets. Remarkably, our method achieves outstanding results on diverse datasets using various machine learning techniques compared with many existing methods.} }
@inproceedings{10.1145/3671151.3671158, title = {Progress of machine learning potentials for material atomic simulation}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {33--38}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671158}, url = {https://doi.org/10.1145/3671151.3671158}, author = {Zheng, Guikai and Zhu, Min and Xu, Zijian and Liu, Chao and Yin, Hao and Sun, Peng}, abstract = {How to simulate material phenomena of large time-scales and system with high accuracy at low computational cost is the driving force for the development of simulation techniques in computational materials science. The study of complex large-scale natural phenomena needs the support of fast and accurate computational methods. Since the data-driven machine learning potential has been proposed in 2007, it has attracted wide attention because of its advantages of high precision and high computational efficiency. In this review, we present the key construction process of machine learning potential and its application in materials science, discuss the current challenges, and point out future directions.} }
@article{10.1145/3695986, title = {A Survey of Machine Learning for Urban Decision Making: Applications in Planning, Transportation, and Healthcare}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3695986}, url = {https://doi.org/10.1145/3695986}, author = {Zheng, Yu and Hao, Qianyue and Wang, Jingwei and Gao, Changzheng and Chen, Jinwei and Jin, Depeng and Li, Yong}, keywords = {machine learning, urban planning, optimization, decision making}, abstract = {Developing smart cities is vital for ensuring sustainable development and improving human well-being. One critical aspect of building smart cities is designing intelligent methods to address various decision-making problems that arise in urban areas. As machine learning techniques continue to advance rapidly, a growing body of research has been focused on utilizing these methods to achieve intelligent urban decision-making. In this survey, we conduct a systematic literature review on the application of machine learning methods in urban decision-making, with a focus on planning, transportation, and healthcare. First, we provide a taxonomy based on typical applications of machine learning methods for urban decision-making. We then present background knowledge on these tasks and the machine learning techniques that have been adopted to solve them. Next, we examine the challenges and advantages of applying machine learning in urban decision-making, including issues related to urban complexity, urban heterogeneity, and computational cost. Afterward and primarily, we elaborate on the existing machine learning methods that aim at solving urban decision-making tasks in planning, transportation, and healthcare, highlighting their strengths and limitations. Finally, we discuss open problems and the future directions of applying machine learning to enable intelligent urban decision-making, such as developing foundation models and combining reinforcement learning algorithms with human feedback. We hope this survey can help researchers in related fields understand the recent progress made in existing works, and inspire novel applications of machine learning in smart cities.} }
@proceedings{10.1145/3757110, title = {CMNM '25: Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, year = {2025}, isbn = {9798400714344} }
@inproceedings{10.1145/3587716.3587732, title = {Ensemble Two Stage Machine Learning for Network Abnormal Detection}, booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing}, pages = {97--102}, year = {2023}, isbn = {9781450398411}, doi = {10.1145/3587716.3587732}, url = {https://doi.org/10.1145/3587716.3587732}, author = {Du, Runze and Li, Runzhi and Zhang, Zijiao}, keywords = {CNN-CatBoost, intrusion detection, machine learning, network traffic, location = Zhuhai, China}, abstract = {With the endless emergence of network security problems, it also brings the corresponding security threats to the society. IDS is an effective means to deal with network threats, but in the modern large-scale network environment, the traditional IDS has a high false positive rate. Researchers apply machine learning method to intrusion detection and get good results. In this work, based on the network traffic features, we proposed an ensemble two stage machine learning method CNN-CatBoost to detect network attacking behavior. To solve imbalance problem between normal and anomaly traffic, we construct a CNN model with multi-scale convolutional adopted to extract relations among traffic features for anomaly traffic recognition. Next for a multi-class classification problem, we use classic CatBoost model to identify different attacking types. In the experiments, datasets is from CICIDS2017. We deploy comparison experiments among proposed method and other methods. The experimental results show that CNN-CatBoost outperforms others in performance. Lastly, we give feature analysis by SHAP, understand how features influence on the model.} }
@inproceedings{10.1145/3728985.3728997, title = {Modified Particle Swarm Optimization and Machine Learning for Solving Handwritten English Characters}, booktitle = {Proceedings of the 2024 10th International Conference on Robotics and Artificial Intelligence}, pages = {45--50}, year = {2025}, isbn = {9798400717451}, doi = {10.1145/3728985.3728997}, url = {https://doi.org/10.1145/3728985.3728997}, author = {Ratanavilisagul, Chiabwoot}, keywords = {Handwritten English Characters, Machine Learning, Particle Swarm Optimization, Support Vector Machine}, abstract = {The Handwritten English Recognition Problem (HERP) involves interpreting handwritten English text from sources such as documents and answer sheets. This problem is particularly challenging and complex due to the uniqueness of each person's handwriting, along with several factors that influence the interpretation process. Recently, many researchers have proposed using the Histogram of Oriented Gradients (HOG) technique or the combination of Discrete Wavelet Transform and Discrete Cosine Transform (DWT-DCT) for feature extraction. Additionally, researchers have explored machine learning techniques to address this problem, with experimental results showing that machine learning can effectively solve it. Therefore, this research proposes using machine learning methods such as Support Vector Machine (SVM), Random Forest (RF), and Logistic Regression (LR), combined with HOG or DWT-DCT for feature extraction, to tackle this challenge. Experimental results from this research indicate that SVM achieves the best outcomes. Consequently, this research enhances SVM by using Particle Swarm Optimization (PSO) to optimize SVM parameters, improving model performance and achieving a higher recognition rate. Experiments were conducted on two datasets: the EMNIST dataset and a personalized handwriting dataset from 30 participants. The results demonstrate that the proposed method, which employs HOG for feature extraction and PSO-optimized SVM, achieves superior accuracy compared to other methods, underscoring its effectiveness in HERP.} }
@inproceedings{10.1145/3611643.3616352, title = {Can Machine Learning Pipelines Be Better Configured?}, booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages = {463--475}, year = {2023}, isbn = {9798400703270}, doi = {10.1145/3611643.3616352}, url = {https://doi.org/10.1145/3611643.3616352}, author = {Wang, Yibo and Wang, Ying and Zhang, Tingwei and Yu, Yue and Cheung, Shing-Chi and Yu, Hai and Zhu, Zhiliang}, keywords = {Empirical Study, Machine Learning Libraries, location = San Francisco, CA, USA}, abstract = {A Machine Learning (ML) pipeline configures the workflow of a learning task using the APIs provided by ML libraries. However, a pipeline’s performance can vary significantly across different configurations of ML library versions. Misconfigured pipelines can result in inferior performance, such as poor execution time and memory usage, numeric errors and even crashes. A pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries. We refer to such performance inconsistency as a pipeline configuration (PLC) issue. There is no prior systematic study on the pervasiveness, impact and root causes of PLC issues. A systematic understanding of these issues helps configure effective ML pipelines and identify misconfigured ones. In this paper, we conduct the first empirical study of PLC issues. To better dig into the problem, we propose Piecer, an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ML libraries and compares their performance inconsistencies. We apply Piecer to the 3,380 pipelines that can be deployed out of the 11,363 ML pipelines collected from multiple ML competitions at Kaggle platform. The empirical study results show that 1,092 (32.3} }
@article{10.1145/3631326, title = {Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey}, journal = {ACM J. Responsib. Comput.}, volume = {1}, year = {2024}, doi = {10.1145/3631326}, url = {https://doi.org/10.1145/3631326}, author = {Hort, Max and Chen, Zhenpeng and Zhang, Jie M. and Harman, Mark and Sarro, Federica}, keywords = {Fairness, bias mitigation, debiasing, fairness-aware machine learning, classification}, abstract = {This article provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics, and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.} }
@inproceedings{10.1145/3746972.3747010, title = {AI-Driven Marketing Strategy Optimization in the Digital Economy: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {239--242}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747010}, url = {https://doi.org/10.1145/3746972.3747010}, author = {Chen, Yuezhang and Liu, Jiacheng}, keywords = {Customer Segmentation, Marketing Strategy Optimization, Predictive Analytics, XGBoost}, abstract = {During digital transformation, AI-powered marketing plan optimization gives the organization a competitive edge. Machine learning models improve consumer targeting, campaign targeting, ROI, and campaign likelihood in this study. We tested these methods using two real-world datasets and supervised learning algorithms like Logistic Regression, Decision Trees, Random Forest, and XGBoost. The data was imputed, one-hot encoded, normalized, and feature-selected to improve model accuracy and relevance. With an accuracy of 90.3\% and an AUC-ROC score of 0.927, XGBoost surpasses all models in high-dimensional, difficult-to-characterize marketing data. The report also examines AI adoption's ethical issues, including data protection and model interpretation.} }
@article{10.1145/3722215, title = {I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3722215}, url = {https://doi.org/10.1145/3722215}, author = {Lewis, Noah and Bez, Jean Luca and Byna, Suren}, keywords = {I/O access pattern, HPC I/O, storage, machine learning}, abstract = {Growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference. This demand for speed has prompted the use of high performance computing (HPC) systems that excel in managing distributed workloads. Because data is the main fuel for AI applications, the performance of the storage and I/O subsystem of HPC systems is critical. In the past, HPC applications accessed large portions of data written by simulations or experiments or ingested data for visualizations or analysis tasks. ML workloads perform small reads spread across a large number of random files. This shift of I/O access patterns poses several challenges to modern parallel storage systems. In this article, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024. We define the scope of the survey, provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during offline data preparation, training, and inference, and explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature. Lastly, we seek to expose research gaps that could spawn further R\&amp;D.} }
@inproceedings{10.1145/3757749.3757766, title = {Research on the Prediction Model of Basketball Player Rehabilitation Efficiency Based on Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {102--106}, year = {2025}, isbn = {9798400713347}, doi = {10.1145/3757749.3757766}, url = {https://doi.org/10.1145/3757749.3757766}, author = {Yan, Sheng and Liu, Linjun}, keywords = {basketball, injury prediction, machine learning, rehabilitation efficiency, ridge regression}, abstract = {This paper proposes a prediction model of injury rehabilitation efficiency based on machine learning for the common sports injury problems in high-intensity basketball. The study uses a data set containing athlete biomechanical data, injury information and rehabilitation results, and establishes an effective rehabilitation efficiency prediction system by comparing the performance of multiple regression algorithms. Experimental analysis shows that the ridge regression model achieves the best effect in predicting rehabilitation efficiency, with a root mean square error (RMSE) of 0.157 and a mean absolute error (MAE) of 0.136. SHAP (SHapley Additive exPlanations) analysis further reveals the important influence of biomechanical factors such as knee angle, height, weight and jump height on rehabilitation efficiency. The results of this study can provide decision support for sports medicine experts to formulate personalized rehabilitation plans, which is helpful to improve the efficiency of injury rehabilitation and reduce the risk of injury recurrence.} }
@inproceedings{10.1145/3580305.3599571, title = {Socially Responsible Machine Learning: A Causal Perspective}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5819--5820}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599571}, url = {https://doi.org/10.1145/3580305.3599571}, author = {Moraffah, Raha and Karimi, Amir-Hossein and Raglin, Adrienne and Liu, Huan}, keywords = {causality, fairness, interpretability, responsible ml, robustness, location = Long Beach, CA, USA}, abstract = {The evergrowing reliance of humans and society on machine learning methods has raised concerns about their trustworthiness and liability. As a response to these concerns, Socially Responsible Machine Learning (SRML) aims at developing fair, transparent, and robust machine learning algorithms. However, traditional approaches to SRML do not incorporate human perspectives, and therefore are not sufficient to build long-lasting trust between machines and human being. Causality as the key to human intelligence plays a vital role in achieving socially responsible machine learning algorithms which are compatible with human notions. Bridging the gap between traditional SRML and causality, in this tutorial, we aim at providing a holistic overview of SRML through the lens of causality. In particular, we will focus on state-of-the-art techniques on causal socially responsible ML in terms of fairness, interpretability, and robustness. The objectives of this tutorial are as follows: (1) we provide a taxonomy of existing literature on causal socially responsible ML from fairness, interpretability, and robustness perspective; (2) we review the state-of-the-art techniques for each task; and (3) we elucidate open questions and future research directions. We believe this tutorial is beneficial to researchers and practitioners from the areas of data mining, machine learning, and social sciences.} }
@article{10.1145/3649841, title = {TorchQL: A Programming Framework for Integrity Constraints in Machine Learning}, journal = {Proc. ACM Program. Lang.}, volume = {8}, year = {2024}, doi = {10.1145/3649841}, url = {https://doi.org/10.1145/3649841}, author = {Naik, Aaditya and Stein, Adam and Wu, Yinjun and Naik, Mayur and Wong, Eric}, keywords = {Machine Learning, Integrity Constraints, Query Languages}, abstract = {Finding errors in machine learning applications requires a thorough exploration of their behavior over data. Existing approaches used by practitioners are often ad-hoc and lack the abstractions needed to scale this process. We present TorchQL, a programming framework to evaluate and improve the correctness of machine learning applications. TorchQL allows users to write queries to specify and check integrity constraints over machine learning models and datasets. It seamlessly integrates relational algebra with functional programming to allow for highly expressive queries using only eight intuitive operators. We evaluate TorchQL on diverse use-cases including finding critical temporal inconsistencies in objects detected across video frames in autonomous driving, finding data imputation errors in time-series medical records, finding data labeling errors in realworld images, and evaluating biases and constraining outputs of language models. Our experiments show that TorchQL enables up to 13x faster query executions than baselines like Pandas and MongoDB, and up to 40\% shorter queries than native Python. We also conduct a user study and find that TorchQL is natural enough for developers familiar with Python to specify complex integrity constraints.} }
@inproceedings{10.1145/3701100.3701121, title = {Machine learning methods for identifying destructive texts in the Kazakh language}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {99--103}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701121}, url = {https://doi.org/10.1145/3701100.3701121}, author = {Bolatbek, Milana and Sagynay, Moldir and Mussiraliyeva, Shynar and Zhastay, Yeltay}, keywords = {Bag of Words, NLTK, TF-IDF, bullying, cyber security, destructive messages, machine learning, national extremism, racism, text classification, text corpora, violent extremism, word2vec}, abstract = {In the digital age, society is faced with an increasing number of destructive messages on the Internet, including insults, racism, violent extremism and national extremism. This paper presents a machine learning-based approach to classify destructive texts in the Kazakh language, collected from social networks. During the study a text corpora has been accumulated on social networks: YouTube, Vkontakte, Telegram. The study uses advanced text vectorization methods, including Bag of Words, TF-IDF, and Word2Vec, to preprocess text data. Our findings demonstrate that Logistic Regression outperforms other models, achieving an accuracy of 86\%. These results contribute to the development of automated systems for identifying destructive content, promoting safer online communication."} }
@inproceedings{10.1145/3711129.3711303, title = {Research on Large-scale Ocean Date Analysis based on Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering}, pages = {1023--1028}, year = {2025}, isbn = {9798400710094}, doi = {10.1145/3711129.3711303}, url = {https://doi.org/10.1145/3711129.3711303}, author = {Zhou, Tong and Meng, Wenzheng and Yu, Dingfeng}, keywords = {Machine learning, Marine data analysis, Spatiotemporal convolutional neural networks, Variational autoencoders}, abstract = {The rapid development of electronic information and communication technology has led to the widespread use of a variety of marine monitoring technologies, including aerospace remote sensors, automatic buoys, multi-beam echo-sounders, and underwater detection equipment. This has resulted in a significant increase in the volume of marine big data. This work puts forth an innovative model that is based on the most recent machine learning techniques. In consideration of the distinctive attributes of high-dimensional, nonlinear, and strongly spatiotemporally correlated marine data, the model employs an enhanced deep neural network architecture. Initially, the Variational Autoencoder (VAE) was employed for the purpose of reducing the dimensionality of the data set and extracting the most pertinent features, thus enabling the effective handling of the noise and redundant information inherent to marine data. Subsequently, the spatiotemporal dependencies in the data were mined using a spatiotemporal convolutional neural network (ST-CNN) with a self-attention mechanism, enabling high-precision prediction and classification of ocean parameters. The model has been optimised to account for the distinctive characteristics of marine data, thereby enhancing its capacity to adapt to complex marine environments.} }
@inbook{10.5555/3716662.3716707, title = {A Conceptual Framework for Ethical Evaluation of Machine Learning Systems}, booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {534--546}, year = {2025}, author = {Gupta, Neha R. and Hullman, Jessica and Subramonyam, Hari}, abstract = {Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.} }
@article{10.1145/3772367, title = {Leveraging Machine Learning Models to Improve Smart Contract Security: A Survey of Vulnerabilities and Detection Methods}, journal = {ACM Comput. Surv.}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3772367}, url = {https://doi.org/10.1145/3772367}, author = {Alsunaidi, Shikah J. and Aljamaan, Hamoud and Hammoudeh, Mohammad}, keywords = {Blockchain, Ethereum, machine learning, security, smart contracts, smart contract vulnerabilities, software security}, abstract = {Smart Contracts (SCs), self-executing programs on blockchain platforms, are transforming industries such as banking, healthcare, and supply chains through automated, trustless transactions. However, their inherent vulnerabilities have led to severe financial and operational losses, with large-scale exploits causing substantial economic damage. Machine Learning (ML) has emerged as a promising approach for SC vulnerability detection, yet its effectiveness, adaptability, and generalizability remain insufficiently explored. This article comprehensively classifies current Ethereum SC vulnerabilities and attacks. It also surveys 108 ML-based detection methods, covering both traditional models and a structured taxonomy of advanced approaches such as GNN-based, LLM-based, contrastive learning, ensemble, hybrid, meta-learning, and transfer learning techniques. The strengths, limitations, and practical challenges of these methods are systematically analyzed, with particular attention to factors such as detection stages, classification problems, dataset characteristics, feature engineering, performance evaluation, generalizability, detection capability, model aging, and ethical and privacy implications. Additionally, existing datasets on SC vulnerabilities are reviewed and consolidated. By integrating these insights, this work provides actionable guidelines and a foundation for building secure, resilient, and trustworthy SC ecosystems.} }
@inproceedings{10.1145/3670474.3685947, title = {A Parallel Simulation Framework Incorporating Machine Learning-Based Hotspot Detection for Accelerated Power Grid Analysis}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685947}, url = {https://doi.org/10.1145/3670474.3685947}, author = {Jiang, Yangfan and Song, Jianfei and Yin, Xunzhao and Dong, Xiao and Sun, Songyu and Lin, Yibo and Jin, Zhou and Yang, Xiaoyu and Zhuo, Cheng}, keywords = {Power grid, hotspot, machine learning, parallel, location = Salt Lake City, UT, USA}, abstract = {Power grid analysis is essential for integrated circuit design, especially as the scale extends to billions of nodes, making precise calculations across the entire spatial domain prohibitively expensive. Domain decomposition methods (DDM) facilitate block-wise parallel analysis of power distribution networks (PDN) through segmentation, yet they still face bottlenecks in computational load balance during parallel processing. Considering the vastness of PDN structures and the emphasis on verifying areas prone to significant IR drop (i.e., hotspot) rather than those less affected, our research introduces a machine learning (ML)-based method for detecting PDN hotspots. This method focuses on accurately identifying areas within sub-blocks where the hotspot intensity is high. We then propose a novel parallel approximation computing approach that integrates hotspot detection, enabling approximations in regions not marked as hotspots. This strategy optimizes computational resources and effectively accelerates analysis. Our experimental results demonstrate that the hotspot detection model achieves an average accuracy of 94.39\%. The approximation strategy for non-hotspot sub-blocks results in 0.56-1.4\% mean relative error, while achieving an average computational speedup of 3.21 over the CK-TSO parallel solver and 2.21 over the conventional DDM-based solver.} }
@inproceedings{10.1145/3670474.3685961, title = {HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685961}, url = {https://doi.org/10.1145/3670474.3685961}, author = {Abi-Karam, Stefan and Sarkar, Rishov and Seigler, Allison and Lowe, Sean and Wei, Zhigang and Chen, Hanqiu and Rao, Nanditha and John, Lizy and Arora, Aman and Hao, Cong}, abstract = {Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present great challenges to FPGA and ML researchers. Existing datasets either cover only a subset of previously published benchmarks, provide no way to enumerate optimization design spaces, are limited to a specific vendor, or have no reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs to existing datasets, limiting wider adoption and sustainability of such datasets.In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture not only ensures broad coverage of data points via design space expansion but also supports interoperability with tools from multiple vendors. Users can contribute to each stage easily by submitting their own HLS designs or synthesis results via provided user APIs. The framework is also flexible, allowing extensions at every step via user APIs with custom frontends, synthesis tools, and scripts.To demonstrate the framework functionality, we include an initial set of built-in base designs from PolyBench, MachSuite, Rosetta, CHStone, Kastner et al.'s Parallel Programming for FPGAs, and curated kernels from existing open-source HLS designs. We report the statistical analyses and design space visualizations to demonstrate the completed end-to-end compilation flow, and to highlight the effectiveness of our design space expansion beyond the initial base dataset, which greatly contributes to dataset diversity and coverage.In addition to its evident application in ML, we showcase the versatility and multi-functionality of our framework through seven case studies:I) Building an ML model for post-implementation QoR predictionII) Using design space sampling in stage 1 to expand the design space covered from a small base set of HLS designs; III) Demonstrating the speedup from the fine-grained design parallelism backend; IV) Extending HLSFactory to target Intel's HLS flow across all stages; V) Adding and running new auxiliary designs using HLSFactory; VI) Integration of previously published HLS data in stage 3; VII) Using HLSFactory to perform HLS tool version regression benchmarking.Code available at https://github.com/sharc-lab/HLSFactory.} }
@inproceedings{10.1145/3673038.3673106, title = {Scheduling Machine Learning Compressible Inference Tasks with Limited Energy Budget}, booktitle = {Proceedings of the 53rd International Conference on Parallel Processing}, pages = {961--970}, year = {2024}, isbn = {9798400717932}, doi = {10.1145/3673038.3673106}, url = {https://doi.org/10.1145/3673038.3673106}, author = {Da Silva Barros, Tiago and Ferre, Davide and Giroire, Frederic and Aparicio-Pardo, Ramon and Perennes, Stephane}, keywords = {deadlines, energy budget, neural network compression, scheduling, location = Gotland, Sweden}, abstract = {Advancements in cloud computing have boosted Machine Learning as a Service (MLaaS), highlighting the challenge of scheduling tasks under latency and deadline constraints. Neural network compression offers the latency and energy consumption reduction in data centers, aligning with efforts to minimize cloud computing’s carbon footprint, despite some accuracy loss. This paper investigates the Deadline Scheduling with Compressible Tasks - Energy Aware (DSCT-EA) problem, which addresses the scheduling of compressible machine learning tasks on several machines, with different speeds and energy efficiencies, under an energy budget constraint. Solving DSCT-EA involves determining both the machine on which each task will be processed and its processing time, a problem that has been proven to be NP-Hard. We formulate DSCT-EA as a Mixed-Integer Programming (MIP) problem and also provide an approximation algorithm for solving it. The efficacy of our approach is demonstrated through extensive experimentation, revealing its superiority over traditional scheduling techniques. It allows to save up to 70\% of the energy budget of image classification tasks, while only losing 2\% of accuracy compared to when not using compression.} }
@inproceedings{10.1145/3706598.3713482, title = {Preventing Harmful Data Practices by using Participatory Input to Navigate the Machine Learning Multiverse}, booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, year = {2025}, isbn = {9798400713941}, doi = {10.1145/3706598.3713482}, url = {https://doi.org/10.1145/3706598.3713482}, author = {Simson, Jan and Draxler, Fiona and Mehr, Samuel and Kern, Christoph}, keywords = {Participatory Design, Machine Learning, Algorithmic Fairness, Multiverse Analysis, Citizen Science, Garden of Forking Paths}, abstract = {In light of inherent trade-offs regarding fairness, privacy, interpretability and performance, as well as normative questions, the machine learning (ML) pipeline needs to be made accessible for public input, critical reflection and engagement of diverse stakeholders.In this work, we introduce a participatory approach to gather input from the general public on the design of an ML pipeline. We show how people’s input can be used to navigate and constrain the multiverse of decisions during both model development and evaluation. We highlight that central design decisions should be democratized rather than “optimized” to acknowledge their critical impact on the system’s output downstream. We describe the iterative development of our approach and its exemplary implementation on a citizen science platform. Our results demonstrate how public participation can inform critical design decisions along the model-building pipeline and combat widespread lazy data practices.} }
@inproceedings{10.1145/3701100.3701144, title = {Research on a Bridge Health Monitoring System Based on Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {214--218}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701144}, url = {https://doi.org/10.1145/3701100.3701144}, author = {Zhang, DaYong}, keywords = {Bridge health monitoring, data analysis, system design, performance optimization, machine learning}, abstract = {This study designs and implements a bridge health monitoring system based on machine learning. The system adopts a four-layer architecture, including data acquisition, processing, analysis and decision-making, and user interaction layers. Core functions encompass data preprocessing, feature extraction, model training, health assessment, and early warning. It integrates algorithms such as Support Vector Machines, Random Forests, and Deep Neural Networks to enhance the accuracy of damage identification and status prediction. The development utilizes modern tools to improve efficiency. Test results show that the system is stable, responsive, and has strong processing capabilities. This system provides an intelligent solution for bridge health monitoring, improving the automation level and management efficiency of monitoring, and offers strong support for bridge safety management.} }
@inproceedings{10.1145/3674912.3674937, title = {Machine Learning Algorithms for Cyber Attack Detection And Classification}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {29--36}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674937}, url = {https://doi.org/10.1145/3674912.3674937}, author = {Note, Johan and Mullalli, Erind and CICO, BETIM}, abstract = {As a result of the accelerated development and expansion of technology in the present day, a new concern has emerged: cyberattacks. This has generated significant concern across various domains globally, leading to considerable disruption in networks and presenting PC users with a multitude of challenges. Presently, a multitude of organisations are striving to combat these types of cyber-attacks through the implementation of novel detection and subsequent destruction methods. The domain of machine learning enables computers to acquire knowledge and skills without requiring explicit programming. There is an abundance of implementation strategies for this technology. This study aims to demonstrate a diverse array of algorithms utilised in the defense against various cyber-attacks. This paper will examine various classification algorithms utilised to defend against diverse cyber-attacks, as well as the methods of defense against these attacks. The implementation, accuracy, and testing time of these algorithms will vary depending on the classification of the attack. This thesis will discuss various varieties of these algorithms.} }
@article{10.14778/3641204.3641209, title = {PilotScope: Steering Databases with Machine Learning Drivers}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {980--993}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3641204.3641209}, url = {https://doi.org/10.14778/3641204.3641209}, author = {Zhu, Rong and Weng, Lianggui and Wei, Wenqing and Wu, Di and Peng, Jiazhen and Wang, Yifan and Ding, Bolin and Lian, Defu and Zheng, Bolong and Zhou, Jingren}, abstract = {Learned databases, or AI4DB techniques, have rapidly developed in the last decade. Deploying machine learning (ML) and AI4DB algorithms into actual databases is the gold standard to examine their performance in practice. However, due to the complexity of database systems, the difference between ML and DB programming paradigms, and the diversity of ML models, the tasks of developing and deploying AI4DB algorithms into databases are prohibitively difficult. Most previous works focus on specific AI4DB algorithms and ML models whose deployment requires close cooperation between ML and DB developers and heavy engineering cost.In this paper, we design and implement PilotScope, an AI4DB middleware with a programming model that largely reduces such difficulties. With a novel abstraction of AI4DB algorithms for, e.g., knob tuning and query optimization, PilotScope consists of two classes of components, AI4DB drivers and DB interactors, with different programming paradigms and roles in AI4DB tasks. ML developers focus on designing and implementing AI4DB drivers, which are algorithmic workflows that collect statistics from databases, train ML models, make decisions and optimize databases using learned models. AI4DB drivers interact with databases via DB interactors (e.g., for collecting data and enforcing actions in databases). DB developers focus on implementing these interactors on one or more database engines, with the interaction details hindered from ML developers. PilotScope supports a variety of AI4DB tasks, and the implementation of an AI4DB algorithm on PilotScope can be deployed in different databases with only minimum modifications. PilotScope is effective in benchmarking these AI4DB algorithms in real-world scenarios. We hope that PilotScope could significantly accelerate iterating AI4DB research and make AI4DB techniques truly applicable in production.} }
@inproceedings{10.1145/3731806.3731834, title = {Enhancing Global Air Quality Classification Using Efficient Machine Learning Techniques}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {284--289}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731834}, url = {https://doi.org/10.1145/3731806.3731834}, author = {Putra Mulyana, Muhammad Haikel Nur Kamil and Hikmawati, Erna and Mandasari, Rizza Indah Mega}, keywords = {Air Quality Prediction, Decision Tree, Feature Selection, Information Gain, K-Nearest Neighbors}, abstract = {Air quality problems have become a global problem due to an important impact on human and environmental health. The goal of this research is to improve the accuracy of the air quality prediction using the method of learning with the machine. How to select the features that are used to receive data are used to identify the most involved parameters such as carbon monoxide (CO), sulfur dioxide (SO2), nitrogen dioxide (NO2), the temperature is similar to the industrial area and measurement. Efficiency, such as AUC and MCC K-Nearest, two, the main algorithm called neighbors (KNN) is used to show that the decision-making plan is the best results with AUC 0.9380 and CA at 0.8820 while KNN also. There is significant improvement after selecting this method of choosing this method not only But improving the analysis of efficiency data but still has more accurate predictions This research is a trend for further development, including the application of all the methods or the combination of additional parameters to solve more complex challenges in the future.} }
@inproceedings{10.1145/3686081.3686100, title = {Machine Learning-Based Market Segmentation and Consumer Behavior Prediction Models}, booktitle = {Proceedings of the International Conference on Decision Science \&amp; Management}, pages = {122--126}, year = {2024}, isbn = {9798400718151}, doi = {10.1145/3686081.3686100}, url = {https://doi.org/10.1145/3686081.3686100}, author = {Liu, Sheng and Yang, Shixun}, keywords = {Consumer behavior prediction, Feature engineering, K-means clustering, Machine learning, Random Forest}, abstract = {This study aims to explore the application of machine learning technology in market segmentation and consumer behavior prediction. By utilizing large-scale data sets from e-commerce platforms, this paper builds two models: a market segmentation model based on K-means clustering and a consumer behavior prediction model based on random forest. Data preprocessing includes data cleaning, feature engineering, and data standardization, aiming to optimize the quality of model inputs. The market segmentation model divides consumers into different market segments by analyzing their purchasing behavior, age, gender and other characteristics. Consumer behavior prediction models use users’ historical purchase data and personal characteristics to predict their future purchase behavior. Model evaluation is based on precision, recall and F1 scores, while cross-validation and parameter optimization techniques are used to improve the generalization ability of the model. The results show that the random forest model performs better than the K-means clustering model in predicting consumer behavior, proving the effectiveness of machine learning technology in precise market analysis and consumer behavior prediction. This research provides enterprises with a practical framework for using machine learning technology to conduct market analysis and consumer behavior prediction, helping enterprises to better understand market dynamics and consumer needs, thereby formulating more effective market strategies.} }
@inproceedings{10.1145/3759928.3759953, title = {Social-psychological Dual-dimensional Clustering Modeling: Heterogeneity Analysis of Smoking Cessation Success Based on Machine Learning}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {148--156}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759953}, url = {https://doi.org/10.1145/3759928.3759953}, author = {Wu, Yuting and Sun, Xiao}, keywords = {Smoking cessation intervention, cluster analysis, principal component analysis, random forest model, working population}, abstract = {Smoking poses a serious threat to public health worldwide, with significant differences in smoking behavior and quit success rates among different occupational groups. We propose an "Occupational Clustering - Dynamic Modeling" framework, which involves finely classifying occupations into seven major categories and analyzing them from both the "social-psychological" dimensions to construct a differentiated intervention model. From a methodological standpoint, we apply Principal Component Analysis (PCA) to perform dimensionality reduction and extract the most informative features. K-Means clustering is employed to reveal latent socio-psychological groupings within each occupational category, thereby enabling a stratified examination of populations. Random Forest classifier is built and assessed across multiple combinations of input variables: the configuration yielding the highest validation performance is chosen, and its feature‐importance metrics pinpoint the principal drivers of quitting success. To rectify the imbalance between cessation outcomes, the Borderline-SMOTE oversampling approach is used, which enhances the model's ability to detect and correctly classify minority‐class instances. We find significant differences in quit success rates and feature influences among different occupational groups, with the group model performing better in specific occupational groups and the Random Forest model accurately identifying key influencing factors. The use of grouping-based smoking cessation prediction models and feature importance analysis helps in formulating precise intervention strategies, improving quit success rates, advancing the field of smoking cessation interventions, and enhancing public health.} }
@inproceedings{10.1145/3637528.3671551, title = {Causal Machine Learning for Cost-Effective Allocation of Development Aid}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5283--5294}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671551}, url = {https://doi.org/10.1145/3637528.3671551}, author = {Kuzmanovic, Milan and Frauen, Dennis and Hatt, Tobias and Feuerriegel, Stefan}, keywords = {causal machine learning, development aid, heterogeneous treatment effects, medicine, treatment effect estimation, location = Barcelona, Spain}, abstract = {The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3\% (~50,000 cases) compared to the current allocation practice.} }
@inproceedings{10.1145/3689236.3689890, title = {Personalized Online Training Method for Power Dispatchers Based on Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {677--685}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3689890}, url = {https://doi.org/10.1145/3689236.3689890}, author = {Liu, Guiqing and Chen, Manqi and Zhang, Lei and Lei, Xiaoyu and Wang, Xunshi and Ling, Xinglong}, keywords = {Learning Path, Machine Learning, Online Training, Personalized Learning, Power Dispatcher}, abstract = {To fully utilize online course resources and optimize the job training for power dispatchers, this paper proposes a personalized learning path recommendation algorithm based on machine learning. The paper employs the Deep Q-learning algorithm to construct a recommendation model aimed at suggesting a series of knowledge points to learners, with the ultimate goal of improving learning efficiency. By analyzing personalized data such as the learner's profession, job requirements, and skill level, the process involves job and skill analysis, course module construction, and personalized learning path design. This approach achieves a more efficient and scientific personalized education model, helping learners to achieve their learning goals with minimal cost.} }
@inproceedings{10.1145/3674912.3674915, title = {Machine Learning Models for Advanced Air Quality Prediction}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {51--56}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674915}, url = {https://doi.org/10.1145/3674912.3674915}, author = {Sadriddin, Zuhra and Mekuria, Remudin Reshid and Gaso, Mekia Shigute}, keywords = {AQI, Data Analysis, Long Short-Term Memory (LSTM), Machine Learning, Random Forest Regression (RFR), location = Ruse, Bulgaria}, abstract = {Air quality stands as a pivotal factor influencing public health and well-being, shaping urban planning strategies, health management practices, and environmental policies. Having meticulously scrutinized PM2.5 concentration data from 42 monitoring stations across Bishkek, we have employed meteorological factors, including temperature and humidity from these stations to model air quality. We have thus implemented two distinct prediction methodologies namely, Random Forest Regression (RFR) and Long Short-Term Memory (LSTM). Our findings unequivocally favored RFR as the superior predictor for PM2.5 values, achieving accuracy levels of up to 90\%.} }
@inproceedings{10.1145/3718751.3718924, title = {Analysis of Chronic Disease Severity Using Machine Learning Approaches}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {1053--1059}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718924}, url = {https://doi.org/10.1145/3718751.3718924}, author = {Zhu, Yiwen}, keywords = {Chronic Disease Severity, Neural Networks, Random Forest, TfidfVectorizer}, abstract = {With the rising global prevalence of chronic diseases each year, there is a growing economic burden impacting social and economic development worldwide. In response, patient data pertaining to chronic illnesses were gathered and transformed into feature representations using the TfidfVectorizer. Neural Networks, Random Forest, GBDT, and SVM algorithms were then employed to predict the severity of these conditions accurately. Upon training and testing the models, it became evident that the Random Forest model outperformed the other models, showcasing superior predictive capabilities. Furthermore, by combining models through fusion techniques, there was potential for further enhancing predictive accuracy. This study has the potential to equip healthcare professionals with more precise and effective tools for analyzing chronic conditions, ultimately enhancing diagnostic efficiency and patient care.} }
@article{10.14778/3659437.3659441, title = {InferDB: In-Database Machine Learning Inference Using Indexes}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {1830--1842}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3659437.3659441}, url = {https://doi.org/10.14778/3659437.3659441}, author = {Salazar-D\'az, Ricardo and Glavic, Boris and Rabl, Tilmann}, abstract = {The performance of inference with machine learning (ML) models and its integration with analytical query processing have become critical bottlenecks for data analysis in many organizations. An ML inference pipeline typically consists of a preprocessing workflow followed by prediction with an ML model. Current approaches for in-database inference implement preprocessing operators and ML algorithms in the database either natively, by transpiling code to SQL, or by executing user-defined functions in guest languages such as Python. In this work, we present a radically different approach that approximates an end-to-end inference pipeline (preprocessing plus prediction) using a light-weight embedding that discretizes a carefully selected subset of the input features and an index that maps data points in the embedding space to aggregated predictions of an ML model. We replace a complex preprocessing workflow and model-based inference with a simple feature transformation and an index lookup. Our framework improves inference latency by several orders of magnitude while maintaining similar prediction accuracy compared to the pipeline it approximates.} }
@inproceedings{10.1145/3716554.3716579, title = {Accelerating Edge Intelligence: Challenges and Future Directions in Hardware-Driven Machine Learning}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {168--173}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716579}, url = {https://doi.org/10.1145/3716554.3716579}, author = {Batzolis, Eleftherios and Karampatzakis, Dimitris}, keywords = {Hardware Accelerators, Machine Learning, Artificial Intelligence, Embedded Systems, Neural Network Optimization}, abstract = {The joining of hardware speed-up for machine learning (ML) in small devices shows a very important step in technology growth. This ability to run complicated ML models on low-power systems opens up many uses, like Internet of Things (IoT) gadgets, robots and different edge computing tasks. This paper studies the present condition of hardware speed-up for ML in small devices, clarifies difficulties and looks at future paths for progress.} }
@inproceedings{10.1145/3591197.3591306, title = {Privacy-Preserving Distributed Machine Learning Made Faster}, booktitle = {Proceedings of the 2023 Secure and Trustworthy Deep Learning Systems Workshop}, year = {2023}, isbn = {9798400701818}, doi = {10.1145/3591197.3591306}, url = {https://doi.org/10.1145/3591197.3591306}, author = {Jiang, Zoe L. and Gu, Jiajing and Wang, Hongxiao and Wu, Yulin and Fang, Junbin and Yiu, Siu-Ming and Luo, Wenjian and Wang, Xuan}, keywords = {Privacy-preserving machine learning, distributed machine learning, multi-key fully homomorphic encryption, location = Melbourne, VIC, Australia}, abstract = {With the development of machine learning, it is difficult for a single server to process all the data. So machine learning tasks need to be spread across multiple servers, turning the centralized machine learning into a distributed one. Multi-key homomorphic encryption is one of the suitable candidates to solve the problem. However, the most recent result of the Multi-key homomorphic encryption scheme (MKTFHE) only supports the NAND gate. Although it is Turing complete, it requires efficient encapsulation of the NAND gate to further support mathematical calculation. This paper designs and implements a series of operations on positive and negative integers accurately. First, we design basic bootstrapped gates, the efficiency of which is times that the number of using NAND to build. Second, we construct practical k-bit complement mathematical operators based on our basic binary bootstrapped gates. The constructed created can perform addition, subtraction, multiplication, and division on both positive and negative integers. Finally, we demonstrated the generality of the designed operators by achieving a distributed privacy-preserving machine learning algorithm, i.e. linear regression with two different solutions. Experiments show that the consumption time of the operators built with our gate is about 50 ∼ 70\% shorter than built directly with NAND gate and the iteration time of linear regression with our gates is 66.7\% shorter than with NAND gate directly.} }
@article{10.1145/3757892.3757893, title = {Causal Machine Learning Approaches for Modelling Data Center Heat Recovery: A Physical Testbed Study}, journal = {SIGENERGY Energy Inform. Rev.}, volume = {5}, pages = {4--10}, year = {2025}, doi = {10.1145/3757892.3757893}, url = {https://doi.org/10.1145/3757892.3757893}, author = {Gonzalez, David Zapata and Meyer, Marcel and M\"uller, Oliver}, keywords = {data center operations, heat recovery, causal machine learning}, abstract = {Data centers (DCs) form the backbone of our growing digital economy, but their rising energy demands pose challenges to our environment. At the same time, reusing waste heat from DCs also represents an opportunity, for example, for more sustainable heating of residential buildings. Modeling and optimizing these coupled and dynamic systems of heat generation and reuse is complex. On the one hand, physical simulations can be used to model these systems, but they are time-consuming to develop and run. Machine learning (ML), on the other hand, allows efficient data-driven modeling, but conventional correlation-based approaches struggle with the prediction of interventions and out-of-distribution generalization. Recent advances in causal ML, which combine principles from causal inference with flexible ML methods, are a promising approach for more robust predictions. Due to their focus on modeling interventions and cause-and-effect relationships, it is difficult to evaluate causal ML approaches rigorously. To address this challenge, we built a testbed of a miniature DC with an integrated waste heat network, equipped with sensors and actuators. This testbed allows conducting controlled experiments and automatic collection of realistic data, which can then be used to benchmark conventional and causal ML methods. Our experimental results highlight the strengths and weaknesses of each modeling approach, providing valuable insights on how to appropriately apply different types of machine learning to optimize data center operations and enhance their sustainability.} }
@inproceedings{10.1145/3731715.3733275, title = {An Explainable Machine Learning Approach for Cognitive Load Detection in Virtual Reality Using Eye Tracking Data}, booktitle = {Proceedings of the 2025 International Conference on Multimedia Retrieval}, pages = {340--348}, year = {2025}, isbn = {9798400718779}, doi = {10.1145/3731715.3733275}, url = {https://doi.org/10.1145/3731715.3733275}, author = {Gao, Hong and Gao, Yapeng and Kasneci, Enkelejda}, keywords = {cognitive load, explainable ai, eye tracking, machine learning, virtual reality, location = Chicago, IL, USA}, abstract = {Accurate cognitive load (CL) detection during virtual reality (VR) locomotion is critical for enhancing user experience and improving interaction design. Traditional CL assessment methods, such as self-reports and physiological measures, face challenges in VR environments. Eye tracking has shown potential as a reliable indicator of CL across various human-computer interaction (HCI) tasks. It offers significant promise as a discriminative feature for predictive models in VR. This study explores the feasibility of detecting CL induced by VR locomotion using an explainable machine-learning approach along with eye-tracking techniques. A comparative user study employing a within-subjects design evaluated five unique gait-free locomotion techniques. Statistical analysis revealed distinct CL levels across these locomotion techniques. Several machine learning models were developed for CL detection using eye-tracking data, with the Light Gradient Boosting Machine (LightGBM) achieving the highest accuracy of 0.78. The SHAP approach was employed to analyze the importance of features to provide interpretability, offering insights into the machine learning model's decision-making process. Our findings highlight the potential of using eye-tracking-based machine learning techniques as a practical approach for cognitive load detection in VR, contributing to the growing research in multimedia analytics, human perception, and user intent within immersive environments. Additionally, our work demonstrates how eye-tracking data can be leveraged to improve user interactions and optimize immersive multimedia experiences based on cognitive load analysis.} }
@inproceedings{10.1145/3665689.3665699, title = {A Machine Learning Approach for Medical Imaging Evaluation}, booktitle = {Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing}, pages = {54--58}, year = {2024}, isbn = {9798400716645}, doi = {10.1145/3665689.3665699}, url = {https://doi.org/10.1145/3665689.3665699}, author = {Qiu, Wei and Shi, Dongxiao and Li, Qiang and Zheng, Yating and Shen, Xiaohui}, abstract = {Collaborating efficiently on medical imaging presents hurdles stemming from data privacy concerns and collaboration barriers. In response to these challenges, federated learning emerges as a decentralized machine learning method showing considerable promise. Its notable advantage lies in preserving data privacy robustly, enabling collaborative efforts without compromising sensitive medical information confidentiality. This study meticulously examines federated learning's application in medical imaging, thoroughly assessing its strengths and limitations. Our experimental results show that federated learning has potential in the field of medical images, achieving cooperative improvement in model performance while protecting data privacy.} }
@inproceedings{10.1145/3704137.3704138, title = {Heart Rate Arrhythmia Identification with Internet of Things and Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {1--7}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704138}, url = {https://doi.org/10.1145/3704137.3704138}, author = {Chze Xin, Loo and Yogarayan, Sumendra and Abdul Razak, Siti Fatimah and Azman, Afizan}, keywords = {Heart Rate Classification, Internet of Things, Machine Learning}, abstract = {Heart rate classification is a critical task in health monitoring and diagnosis, particularly facilitated by advancements in Internet of Things (IoT) and wearable technology. This study evaluates the performance of various machine learning models, with a specific focus on Support Vector Machine (SVM) with a linear kernel, in classifying heart rate data. The testing indicates that SVM with a linear kernel achieves a testing accuracy of 100\%, surpassing other models such as Random Forest (99.6\%) and Decision Tree (99.3\%). This exceptional performance is attributed to the linear separability of the heart rate data, where SVM with a linear kernel effectively identifies the optimal hyperplane for class separation. Additionally, SVM's linear kernel demonstrates robustness against noise, which is common in real-world heart rate data, thereby enhancing its reliability. The study also highlights the interpretability of linear models through feature weights, providing insights into the physiological factors influencing heart rate. This research bridges significant gaps in existing literature by demonstrating the accuracy and practical applicability of linear kernel SVMs in heart rate classification, especially in the context of IoT and wearable technologies. The findings suggest that simpler models should be considered before resorting to complex non-linear models, offering a balance of high performance, computational efficiency, and interpretability in medical applications.} }
@proceedings{10.1145/3674029, title = {ICMLT '24: Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, year = {2024}, isbn = {9798400716379} }
@inproceedings{10.1145/3767624.3767626, title = {Improving Crop Yield Prediction Accuracy: A Composite Machine Learning Model Specially Designed for Sub-frigid Regions}, booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence}, pages = {10--16}, year = {2025}, isbn = {9798400715907}, doi = {10.1145/3767624.3767626}, url = {https://doi.org/10.1145/3767624.3767626}, author = {Xu, Qingquan and Zhang, Xiao and Qiu, Tingyu}, keywords = {Crop yield prediction, composite model, machine learning, sub-frigid regions}, abstract = {This study introduces a novel approach for crop yield prediction in Heilongjiang, filling the gap in crop yield forecasting in sub-frigid countries. We developed a composite model, which ensured its applicability by double filtration and combined the strength of several machine learning models with hard vote and soft vote. The summary of our prediction results showed excellent predictive power of our model in general, and in the cases of four major crops, our model demonstrated superior performance over non-filtration machine learning models (ANN, LR, RF, SVR, RR), evidenced by RMSE. Our findings not only contribute to the improvement of yield forecasting in sub-frigid regions, but also solve the underlying instability of machine learning models to make it better applied in reality.} }
@article{10.1613/jair.1.14340, title = {Towards Green Automated Machine Learning: Status Quo and Future Directions}, journal = {J. Artif. Int. Res.}, volume = {77}, year = {2023}, issn = {1076-9757}, doi = {10.1613/jair.1.14340}, url = {https://doi.org/10.1613/jair.1.14340}, author = {Tornede, Tanja and Tornede, Alexander and Hanselle, Jonas and Mohr, Felix and Wever, Marcel and H\"ullermeier, Eyke}, abstract = {Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution — a machine learning pipeline — tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticized for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large-scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool w.r.t. their “greenness”, i.e., sustainability, are summarized. Finally, we elaborate on how to be transparent about the environmental footprint and what kind of research incentives could direct the community in a more sustainable AutoML research direction. As part of this, we propose a sustainability checklist to be attached to every AutoML paper featuring all core aspects of Green AutoML.} }
@inproceedings{10.1145/3700906.3700998, title = {Applying Machine Learning Technology for Weather Forecasting: A Case Study of the Logistic Regression Model}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {572--576}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700998}, url = {https://doi.org/10.1145/3700906.3700998}, author = {Shen, Kaiwei}, keywords = {Artificial intelligence, Data preprocessing, Logistic regression model, Weather forecasting}, abstract = {This study is focused on improving the dependability and precision of weather forecasting by employing the capabilities of Artificial Intelligence. Specifically, this study utilizes Logistic Regression and Machine Learning techniques to forecast weather, demonstrating the potential in optimizing weather-related activities and disaster management strategies. The study relies on comprehensive weather data observed over several years, sourced from Kaggle, and handles missing data and outliers during its pre-processing stages. The primary machine learning tool applied is Logistic Regression, followed by a stepwise feature selection to identify influential features for accurate weather prediction. The workflow also involves data collection, pre-processing, model building, training, and testing, with provisions for handling both numeric and categorical features along with imputations. The accuracy, precision, and recall of the prediction module are tested using appropriate statistical tools. The Logistic Regression model, upon implementation, demonstrated considerable accuracy, with an ability to predict rainy days and non-rainy days efficiently. An analytical approach was used to examine the model's sensitivity towards the removal of each feature, thereby ascertaining the relative importance of each. Critical predictors like 'Rainfall', 'Pressure9am', and 'WindGustSpeed' exhibited significant effects on the probability of rain. Overall, the use of Logistic Regression and Machine Learning techniques notably improved rain prediction, offering potential for further advancements in the field of weather forecasting.} }
@inproceedings{10.1145/3731806.3731829, title = {Leveraging Machine Learning Techniques to Obtain Data for Virtual Sensors}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {290--294}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731829}, url = {https://doi.org/10.1145/3731806.3731829}, author = {Zhao, Ge-Zhi and Tan, Yi-Fei and Abdul Karim, Hezerul and Cheeng, Tze-Hang and Chia, Ching-King}, keywords = {Internet of Things (IoT), data fusion, physical sensors, predictive model, virtual sensors}, abstract = {The Internet-of-Things (IoT) has revolutionised smart devices by enabling real-time monitoring through remote sensors. It is the most essential element particularly in smart sensing industrial applications such as environmental monitoring and industrial automation. These sensors provide crucial raw data to be analysed and accurate prediction of events of equipment breakdowns or preventive maintenance is required. However, if a physical sensor fails to function normally, virtual sensors can facilitate the missing data during downtime. Virtual sensors utilise predictive models to forecast the missing data, leveraging historical data and patterns from previously trained events to forecast sensor readings under the same conditions. In this research, the authors build a predictive model to generate data for a malfunctioned sensor by using actual data from other functional sensors. The hybrid setup between physical and virtual sensors will complement each other during operations to ensure fail-safe operation. In the research methodology, data from five sensors were analysed with predictive models of random forest. Data were trained on four of the sensors to predict the next day's readings of the fifth sensor. The experiment examined the impact of training various data durations (5, 10, and 15 days). The results revealed promising outcomes across all three training data sizes. Notably, the random forest regression model achieved better performance with larger training datasets, highlighting the impact of dataset size on model effectiveness.} }
@proceedings{10.1145/3748382, title = {FAIML '25: Proceedings of the 2025 4th International Conference on Frontiers of Artificial Intelligence and Machine Learning}, year = {2025}, isbn = {9798400713217} }
@inproceedings{10.1145/3638530.3658378, title = {Evolutionary Machine Learning for Interpretable and eXplainable AI}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1038--1068}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3658378}, url = {https://doi.org/10.1145/3638530.3658378}, author = {Siddique, Abubakar and Browne, Will N. and Urbanowicz, Ryan J.} }
@article{10.1145/3686973, title = {Articulation Work and Tinkering for Fairness in Machine Learning}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {8}, year = {2024}, doi = {10.1145/3686973}, url = {https://doi.org/10.1145/3686973}, author = {Fahimi, Miriam and Russo, Mayra and Scott, Kristen M. and Vidal, Maria-Esther and Berendt, Bettina and Kinder-Kurlanda, Katharina}, keywords = {articulation work, doability, fair machine learning, interview study}, abstract = {The field of fair AI aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory, and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI research to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research for them. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve.} }
@inproceedings{10.1145/3724154.3724317, title = {A Comparative Research on Stock Selection Strategy based on Machine Learning Models}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {997--1001}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724317}, url = {https://doi.org/10.1145/3724154.3724317}, author = {Jiang, Yulai}, keywords = {Machine learning, Quantitative Investment, Stock price forecasting}, abstract = {This paper aims to develop efficient prediction techniques using machine learning models. In this paper, We have extensive access to market trading and technical indicator data, and a sliding window method is used to better capture more time series information. To identify the precise forecasting approach, we employed multiple algorithms like Linear Regression, Random Forest and XGBoost, etc,. After the daily rotation stock selection models were constructed, we use the investing performance indicators such as annualized return, sharpe ratio, and maximum drawdown to evaluate and compare the efficiency of difference models.} }
@article{10.1145/3665795, title = {Self-Supervised Machine Learning Framework for Online Container Security Attack Detection}, journal = {ACM Trans. Auton. Adapt. Syst.}, volume = {19}, year = {2024}, issn = {1556-4665}, doi = {10.1145/3665795}, url = {https://doi.org/10.1145/3665795}, author = {Tunde-Onadele, Olufogorehan and Lin, Yuhang and Gu, Xiaohui and He, Jingzhu and Latapie, Hugo}, keywords = {Performance debugging, microservices, causal analysis}, abstract = {Container security has received much research attention recently. Previous work has proposed to apply various machine learning techniques to detect security attacks in containerized applications. On one hand, supervised machine learning schemes require sufficient labeled training data to achieve good attack detection accuracy. On the other hand, unsupervised machine learning methods are more practical by avoiding training data labeling requirements, but they often suffer from high false alarm rates. In this article, we present a generic self-supervised hybrid learning (SHIL) framework for achieving efficient online security attack detection in containerized systems. SHIL can effectively combine both unsupervised and supervised learning algorithms but does not require any manual data labeling. We have implemented a prototype of SHIL and conducted experiments over 46 real-world security attacks in 29 commonly used server applications. Our experimental results show that SHIL can reduce false alarms by 33\%–93\% compared to existing supervised, unsupervised, or semi-supervised machine learning schemes while achieving a higher or similar detection rate.} }
@inproceedings{10.1145/3759928.3759957, title = {In-depth Analysis, Model Optimization, and Interpretability of Heart Disease Risk Factors Using Machine Learning and Stacking Ensembles}, booktitle = {Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition}, pages = {177--185}, year = {2025}, isbn = {9798400715884}, doi = {10.1145/3759928.3759957}, url = {https://doi.org/10.1145/3759928.3759957}, author = {Lin, Jintian}, keywords = {Ensemble learning, Heart disease prediction, Machine learning, SMOTE, Tomek Links}, abstract = {Heart disease, as a major threat to human health, imposes critical significance on early prediction for clinical prevention and control. To enhance prediction accuracy, this study leverages the Heart Failure Prediction Dataset from Kaggle, addressing data imbalance via SMOTE combined with Tomek Links algorithm, normalizing dimensions and handling outliers through Z-score standardization and capping method. Logistic Regression, Random Forest, XGBoost, and Stacking ensemble learning models are constructed, with their prediction performances systematically compared. Key influencing factors including ST-segment slope, chest pain type, and exercise-induced angina are identified via Random Forest feature importance analysis. Using SHAP values to interpret model decision logic reveals a strong correlation between abnormal ST-segment slope and heart disease incidence. The Stacking model demonstrates optimal comprehensive performance, achieving an accuracy of 0.9057 and an AUC value of 0.9581, outperforming single models. This study provides efficient modeling tools and interpretable risk assessment basis for early heart disease prediction, facilitating the formulation of clinical precision prevention strategies and public health interventions.} }
@inbook{10.1145/3760023.3760053, title = {Comparison and Optimization of Social Media Text Sentiment Analysis Models Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Management Science and Computer Engineering}, pages = {179--184}, year = {2025}, isbn = {9798400715969}, url = {https://doi.org/10.1145/3760023.3760053}, author = {Zhu, Jiemin and Liang, Yan and Lv, Le}, abstract = {In this paper, a hybrid sentiment analysis framework combining lexical rules and machine learning is proposed, aiming to improve the performance of sentiment classification for English social media texts. The study first preprocesses the raw data through a multi-stage text cleaning process (including denoising, stemming extraction, and deactivated word filtering) and generates preliminary sentiment labels by calculating text sentiment scores with the VADER sentiment lexicon; further, the text is vectorized using the TF-IDF method to construct the feature space, and the passive-aggressive classifiers, logistic regression, random forests, support vector machines, and 14 models such as Simple Bayes for the classification task. In order to optimize the model performance, the study tunes the key hyperparameters by random search cross-validation, and finally compares the accuracy, precision, recall and F1 value of each model on the test set.The experimental results show that the optimized passive-aggressive classifier performs best in the emotion classification task, with an improvement of about 1.2\% in accuracy, 0.7\% in precision, 1.2\% in recall, 1.1\% in F1 score, and a significant enhancement in the recognition of neutral emotions. In addition, the study reveals the high-frequency sentiment expression patterns in social media texts through word frequency statistics and visualization, and finds that negative sentiments are mostly focused on topics such as service complaints and product defects.} }
@article{10.1145/3766539, title = {What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity}, journal = {ACM J. Responsib. Comput.}, year = {2025}, doi = {10.1145/3766539}, url = {https://doi.org/10.1145/3766539}, author = {Kong, Youjin}, keywords = {Machine Learning Fairness, Normative Foundation, Egalitarianism, Distributive Equality (DE), Equality of Opportunity (EOP), Relational Equality (RE), Allocative Harm, Representational Harm}, abstract = {Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable benefits and goods, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation, especially in the context of text- and image-generation models. Grounding ML fairness in egalitarianism—the view that equality is a fundamental moral and social ideal—requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong—that is, why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups—and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a novel multifaceted egalitarian framework for ML fairness that integrates both distributive and relational notions of equality. Drawing on critical social and political philosophy, including the work of Anderson, Young, and Fraser, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the entire ML pipeline.} }
@inproceedings{10.1145/3675888.3676144, title = {Automated Examination System using Machine Learning and Natural Language Processing}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {752--761}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676144}, url = {https://doi.org/10.1145/3675888.3676144}, author = {Bhadouria, Aman and Gupta, Pranav and Bindal, Parish and Madan, Kapil and Sonal, Sonal}, keywords = {Examination System, NLP, OpenSource, Question Generation, location = Noida, India}, abstract = {Traditional examination processes are burdened with inefficiencies and vulnerabilities that hinder the educational process. Manual tasks such as question formulation and answer grading consume significant time and resources, while security concerns regarding physical exam materials persist. In response, this paper proposes an automated examination system leveraging advanced technological tools such as machine learning and natural language processing. By addressing the shortcomings of traditional methods and enhancing efficiency and security, this system aims to revolutionize academic evaluation.} }
@inproceedings{10.1145/3641525.3663629, title = {Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines}, booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability}, pages = {101--109}, year = {2024}, isbn = {9798400705304}, doi = {10.1145/3641525.3663629}, url = {https://doi.org/10.1145/3641525.3663629}, author = {Bhaskar, Adhithya and Stodden, Victoria}, keywords = {Computational Reproducibility, CyberInfrastructure, Machine Learning, Open Code, Open Data, ReproScore, Reproducibility Policy, Reproscreener, location = Rennes, France}, abstract = {The increasing reliance on machine learning models in scientific research and day-to-day applications – and the near-opacity of their associated computational methods – creates a widely recognized need to enable others to verify results coming from Machine Learning Pipelines. In this work we use an empirical approach to build on efforts to define and deploy structured publication standards that allow machine learning research to be automatically assessed and verified, enabling greater reliability and trust in results. To automate the assessment of a set of publication standards for Machine Learning Pipelines we developed Reproscreener; a novel, open-source software tool (see https://reproscreener.org/). We benchmark Reproscreener’s automatic reproducibility assessment against a novel manually labeled “gold standard” dataset of machine learning arXiv preprints. Our empirical evaluation has a dual goal: to assess Reproscreener’s performance; and to uncover gaps and opportunities in current reproducibility standards. We develop reproducibility assessment metrics we called the Repo Metrics to provide a novel overall assessment of the re-executability potential of the Machine Learning Pipeline, called the ReproScore. We used two approaches to the automatic identification of reproducibility metrics, keywords and LLM tools, and found the reproducibility metric evaluation performance of Large Language Model (LLM) tools superior to keyword associations.} }
@inproceedings{10.1145/3653724.3653776, title = {Data feature analysis for blast furnace temperature prediction from machine learning perspective}, booktitle = {Proceedings of the International Conference on Mathematics and Machine Learning}, pages = {298--303}, year = {2024}, isbn = {9798400716973}, doi = {10.1145/3653724.3653776}, url = {https://doi.org/10.1145/3653724.3653776}, author = {Duan, Junyi}, abstract = {Modern blast furnace ironmaking technology mainly uses the thermal state of the furnace cylinder to reflect the furnace temperature conditions. However, due to the complexity of the blast furnace smelting process, it is very difficult to modelling and control this process effectively. Therefore, it is important to carry out research on blast furnace temperature prediction modelling in order to realize early warning of furnace health condition in production systems, where nowadays more and more attentions are paid to technologies of machine learning and deep learning. Taking the actual application scenario of a large iron and steel production enterprise as a case study, this paper focuses on the lack of robustness when training machine learning models, due to the noise in the original collected business data. The experimental results show that the application of feature engineering, including feature construction, key feature analysis, and feature ranking within different data analysis stages, is able to improve the quality of the collected raw business data, consequently it is helpful in solving practical engineering problems from machine learning perspective.} }
@article{10.1145/3725835, title = {Evaluating Gaze Event Detection Algorithms: Impacts on Machine Learning-based Classification and Psycholinguistic Statistical Modeling}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3725835}, url = {https://doi.org/10.1145/3725835}, author = {Reich, David R. and Prasse, Paul and J\"ager, Lena A.}, keywords = {I-DT, I-VT, eye movements, eye tracking, eye-tracking, machine learning, psycholinguistic analysis, psycholinguistic statistical modeling}, abstract = {Eye movements offer valuable, non-invasive insights into cognitive processes and are widely used in both psycholinguistic research and machine-learning applications, such as assessing reading comprehension and cognitive load. These applications typically rely on fixations and saccades detected through gaze event algorithms, which may be either proprietary or open-source. The impact of different gaze event detection algorithms on subsequent analysis is underexplored and often overlooked. This study investigates how two threshold-based algorithms, I-DT and I-VT, influence both machine-learning classification tasks and psycholinguistic statistical modeling. Using diverse datasets-including stationary, remote, and VR eye-tracking data across multiple sampling frequencies-our findings show significant differences in downstream performance. For ML tasks, I-DT generally outperforms I-VT, with I-VT being highly sensitive to threshold choices. In psycholinguistic analysis, results confirm established findings only when thresholds align with established fixation metrics, emphasizing the importance of appropriate threshold selection for meaningful analysis. Our code is publicly available: https://github.com/aeye-lab/eye-movement-preprocessing.} }
@inbook{10.1145/3730436.3730494, title = {Leveraging Machine Learning for Employee Resignation Prediction in HR Analytics}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {342--346}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730494}, author = {Fang, Xiang}, abstract = {A major issue in human resource management, employee attrition prediction offers insightful information for corporate decision-making. In this regard, conventional approaches including decision trees have only shown modest success. These techniques usually presume feature independence, though, and struggle to fit the relationships in the data. Using Graph Attention Networks (GAT), we present a novel method for estimating employee attrition that makes use of the linkages and similarities among employees to raise prediction performance. In this study, we represent each employee as a node in a graph, with their personal attributes (such as age, salary, job satisfaction, etc.) serving as node features. We build an adjacency matrix based on employee similarity computed with cosine similarity or Euclidean distance. We develop a GAT model using this graph structure that aggregates surrounding node features based on attention-based criteria therefore enabling the model to weigh the significance of various relationships between employees. The GAT-based model beats conventional logistic regression according to experimental data, therefore greatly enhancing the accuracy of employee attrition prediction. Improving prediction performance depends critically on the model's capacity to replicate dependencies between characteristics and include relational information from adjacent nodes. This paper shows the possibilities of GAT in employee attrition prediction and emphasizes its capacity to model complex interactions inside employee data, therefore providing a fresh approach for strategic workforce management and human resource analytics.} }
@inproceedings{10.1145/3712255.3726701, title = {Machine Learning and Genetic Algorithms: An Intricate Relationship for Locating Methane in Satellite Images}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {931--934}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726701}, url = {https://doi.org/10.1145/3712255.3726701}, author = {Wijata, Agata M. and Long\'ep\'e, Nicolas and Przewozniczek, Michal W. and Nalepa, Jakub}, keywords = {earth observation, hyperspectral image, methane detection, onboard machine learning, feature selection, genetic algorithm, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Detecting methane in remotely-sensed hyperspectral images is a crucial task in environmental monitoring. It aids in identifying methane leaks and emissions, which is essential for mitigating their impact on the climate change. We tackle this challenge and propose an end-to-end and flexible approach for detecting and locating methane plumes in satellite hyperspectral images. It couples classic machine learning with a genetic algorithm to identify the most important image features that contribute to methane localization. We extract features from superpixels—locally-coherent image areas—and then perform feature selection to identify the most discriminative ones, pruning unnecessary (or even noisy) features. Our experiments performed over the large-scale methane dataset (STAR-COP) indicated that feature subsets optimized using a genetic algorithm allow us to build effective methane detectors that outperform supervised models trained over full feature sets. For reproducibility, we refer to our GitHub repository: https://github.com/smile-research/GeneticAlgorithms4MethaneDetection.git.} }
@inproceedings{10.1145/3597638.3608421, title = {Reimagining Machine Learning's Role in Assistive Technology by Co-Designing Exergames with Children Using a Participatory Machine Learning Design Probe}, booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility}, year = {2023}, isbn = {9798400702204}, doi = {10.1145/3597638.3608421}, url = {https://doi.org/10.1145/3597638.3608421}, author = {Duval, Jared and Turmo Vidal, Laia and M\'arquez Segura, Elena and Li, Yinchu and Waern, Annika}, keywords = {Designing with Children, Games, Participatory Machine Learning, Physical Therapy, Play, Sensory Based Motor Disorder, location = New York, NY, USA}, abstract = {The paramount measure of success for a machine learning model has historically been predictive power and accuracy, but even a gold-standard accuracy benchmark fails when it inappropriately misrepresents a disabled or minority body. In this work, we reframe the role of machine learning as a provocation through a case study of participatory work co-creating exergames by employing machine learning and its training as a source of play and motivation rather than an accurate diagnostic tool for children with and without Sensory Based Motor Disorder. We created a design probe, Cirkus, that supports nearly any aminal locomotion exergame while collecting movement data for training a bespoke machine learning model. During 5 participatory workshops with a total of 30 children using Cirkus, we co-created a catalog of 17 exergames and a resulting machine-learning model. We discuss the potential implications of reframing machine learning’s role in Assistive Technology for values other than accuracy, share the challenges of using “messy” movement data from children with disabilities in an ever-changing co-creation context for training machine learning, and present broader implications of using machine learning in therapy games.} }
@inproceedings{10.1145/3709016.3737807, title = {A Context-Aware Framework for Dynamic NFT Updates Using a Machine Learning Driven Adaptive Smart Contract}, booktitle = {Proceedings of the 7th ACM International Symposium on Blockchain and Secure Critical Infrastructure}, year = {2025}, isbn = {9798400714122}, doi = {10.1145/3709016.3737807}, url = {https://doi.org/10.1145/3709016.3737807}, author = {Mabarani, Samukeliso and Rahman, Mohammad Saidur and Gondal, Iqbal and Bandara, H. M. N. Dilum}, keywords = {NFT, tokenization, context-aware updates, static attributes, dynamic attributes, machine learning, adaptive smart-contract, location = Meli\'a Hanoi, Hanoi, Vietnam}, abstract = {Updating dynamic attributes of Real-World Asset (RWA)-backed Non-Fungible Tokens (NFTs) presents challenges due to blockchain immutability. To address this, we propose a context-aware framework that integrates a machine learning (ML) model with an adaptive smart contract (ASC). The ML model intelligently determines when and which dynamic attributes require updates, while the ASC ensures security by locking static attributes and enabling automatic updates for dynamic ones. A specialized locking mechanism prevents unauthorized modifications, preserving NFT integrity. Experimental results validate the framework's effectiveness in securing static attributes, dynamically updating valid attributes, and preventing unauthorized transactions. Security evaluations further demonstrate robust access control, input validation, and data integrity, ensuring resilience against unauthorized modifications.} }
@inproceedings{10.1145/3717664.3717671, title = {Research on enterprise internal audit risk prediction model based on machine learning}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {37--40}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717671}, url = {https://doi.org/10.1145/3717664.3717671}, author = {Li, Na}, keywords = {Audit Risk Prediction, Machine Learning, Random Forest}, abstract = {This study constructs a machine learning-based internal audit risk prediction model to address the complex and dynamic financial risks faced by modern enterprises. By utilizing algorithms such as Random Forest, Support Vector Machine (SVM), and Logistic Regression, we analyze five years of internal financial data and perform feature selection, coupled with grid search and cross-validation to optimize model parameters. The experimental results indicate that the Random Forest model performs best in terms of accuracy, F1 score, and AUC value, effectively identifying high-risk transactions. The research conclusions provide an intelligent tool for enterprise audit risk management and offer insights for future model optimization and application research.} }
@inproceedings{10.1145/3715669.3726802, title = {Predicting Mental Demand of Teammates Using Eye Tracking Metrics: A Machine Learning Approach}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726802}, url = {https://doi.org/10.1145/3715669.3726802}, author = {Atweh, Jad and Riggs, Sara}, keywords = {Eye Tracking, Teams, Mental Demand, Complex Systems, ML}, abstract = {Analyzing eye tracking data using machine learning (ML) offers new insights into cognitive states such as mental workload. However, most research has focused on individuals rather than teams, where workload emerges from shared attention and coordination. Additionally, traditional workload assessment methods like the NASA Task Load Index (NASA-TLX) face limitations in aggregating subscales, raising concerns about their accuracy. This study explores the feasibility of using gaze-based ML models to classify mental demand in Unmanned Aerial Vehicle (UAV) command-and-control (C2) teams. Eye tracking and workload data were collected from four experimental studies involving UAV C2 tasks. Using eight ML classifiers, we evaluated whether gaze features could predict workload levels. k-Nearest Neighbors (kNN) achieved the highest accuracy (81\%) and precision (90\%), outperforming other models. These findings demonstrate the potential of real-time gaze-based workload monitoring of teams in high-stakes environments, paving the way for adaptive systems that support team performance and situation awareness.} }
@article{10.5555/3648699.3648824, title = {Statistical robustness of empirical risks in machine learning}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Guo, Shaoyan and Xu, Huifu and Zhang, Liwei}, keywords = {empirical risks, stability analysis, asymptotic qualitative statistical robustness, non-asymptotic quantitative statistical robustness, uniform consistency}, abstract = {This paper studies convergence of empirical risks in reproducing kernel Hilbert spaces (RKHS). A conventional assumption in the existing research is that empirical training data are generated by the unknown true probability distribution but this may not be satisfied in some practical circumstances. Consequently the existing convergence results may not provide a guarantee as to whether the empirical risks are reliable or not when the data are potentially corrupted (generated by a distribution perturbed from the true). In this paper, we fill out the gap from robust statistics perspective (Kr\"atschmer, Schied and Z\"ahle (2012); Kr\"atschmer, Schied and Z\"ahle (2014); Guo and Xu (2020)). First, we derive moderate sufficient conditions under which the expected risk changes stably (continuously) against small perturbation of the probability distributions of the underlying random variables and demonstrate how the cost function and kernel affect the stability. Second, we examine the difference between laws of the statistical estimators of the expected optimal loss based on pure data and contaminated data using Prokhorov metric and Kantorovich metric, and derive some asymptotic qualitative and non-asymptotic quantitative statistical robustness results. Third, we identify appropriate metrics under which the statistical estimators are uniformly asymptotically consistent. These results provide theoretical grounding for analysing asymptotic convergence and examining reliability of the statistical estimators in a number of regression models.} }
@inproceedings{10.1145/3644713.3644804, title = {Machine Learning for Malware Detection in Network Traffic}, booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems}, pages = {605--610}, year = {2024}, isbn = {9798400709036}, doi = {10.1145/3644713.3644804}, url = {https://doi.org/10.1145/3644713.3644804}, author = {Omopintemi, Ayorinde Henry and Ghafir, Ibrahim and Eltanani, Shadi and Kabir, Sohag and Lefoane, Moemedi}, keywords = {Intrusion Detection, K-Nearest Neighbor Algorithm, Machine learning, Malware Analysis, Malware Detection, Random Forest, The Adaboost Algorithm, location = Dubai, United Arab Emirates}, abstract = {Developing advanced and efficient malware detection systems is becoming significant in light of the growing threat landscape in cybersecurity. This work aims to tackle the enduring problem of identifying malware and protecting digital assets from cyber-attacks. Conventional methods frequently prove ineffective in adjusting to the ever-evolving field of harmful activity. As such, novel approaches that improve precision while simultaneously taking into account the ever-changing landscape of modern cybersecurity problems are needed. To address this problem this research focuses on the detection of malware in network traffic. This work proposes a machine-learning-based approach for malware detection, with particular attention to the Random Forest (RF), Support Vector Machine (SVM), and Adaboost algorithms. In this paper, the model’s performance was evaluated using an assessment matrix. Included the Accuracy (AC) for overall performance, Precision (PC) for positive predicted values, Recall Score (RS) for genuine positives, and the F1 Score (SC) for a balanced viewpoint. A performance comparison has been performed and the results reveal that the built model utilizing Adaboost has the best performance. The TPR for the three classifiers performs over 97\% and the FPR performs \&lt; 4\% for each of the classifiers. The created model in this paper has the potential to help organizations or experts anticipate and handle malware. The proposed model can be used to make forecasts and provide management solutions in the network’s everyday operational activities.} }
@inproceedings{10.1145/3689939.3695783, title = {The Quantum Imitation Game: Reverse Engineering of Quantum Machine Learning Models}, booktitle = {Proceedings of the 2024 Workshop on Attacks and Solutions in Hardware Security}, pages = {48--57}, year = {2024}, isbn = {9798400712357}, doi = {10.1145/3689939.3695783}, url = {https://doi.org/10.1145/3689939.3695783}, author = {Ghosh, Archisman and Ghosh, Swaroop}, keywords = {quantum machine learning, quantum security, reverse engineering, location = Salt Lake City, UT, USA}, abstract = {Quantum Machine Learning (QML) is an amalgamation of quantum computing paradigms with machine learning models, providing significant prospects for solving complex problems. However, with the expansion of numerous third-party vendors in the Noisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security of QML models is of prime importance, particularly against reverse engineering, which could expose sensitive parameters and proprietary algorithms embedded within the models. We assume the untrusted third-party quantum cloud provider is an adversary having white-box access to the transpiled version of the user-designed trained QML model during inference. Although the adversary can steal and use the model without any modification, reverse engineering (RE) to extract the pre-transpiled copy of the QML circuit will enable re-transpilation and usage of the model for various hardware with completely different native gate sets and even different qubit technology. The information about the parameters (e.g., number of parameters, their placements, and optimized values) can allow further training of the QML model if the adversary plans to alter the QML model to tamper with the watermark and/or embed their own watermark or refine the model for other purposes. In this first effort to investigate the RE of QML circuits, we examine quantum classifiers by comparing the training accuracy of original and reverse-engineered models across various sizes (i.e., number of qubits and number of parametric layers) of Quantum Neural Networks (QNNs). We note that multi-qubit classifiers can be reverse-engineered under specific conditions with a mean error of order 10^-2 in a reasonable time. We also propose adding dummy rotation gates in the QML model with fixed parameters to increase the RE overhead for defense. For instance, an addition of 2 dummy qubits and 2 layers increases the overhead by ~1.76 times for a classifier with 2 qubits and 3 layers with a performance overhead of less than 9\%. We note that RE is a very powerful attack model which warrants further efforts on defenses.} }
@inproceedings{10.1145/3747227.3747272, title = {An Empirical and Machine Learning Analysis of Emotional Experiences in College English Translation Courses with Online Platforms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {285--291}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747272}, url = {https://doi.org/10.1145/3747227.3747272}, author = {Tang, Jinfeng}, keywords = {College English Translation Course, English learning emotion, Machine Learning, Support Machine Vector, Yicat online translation platform, correlation analysis, difference analysis}, abstract = {Emotion experience in foreign language learning. Emotional experience directly influences students’ learning outcomes. College English translation is a high-level course. This course is highly complicated and professional. The students also have different emotional experiences. So this study takes Yicat online translation platform for teaching. Questionnaire surveys are carried out to collect data in this study. Non-English major students in a Wuhan university in China are the analysis targets. Anxiety, enjoyment and boredom are the targeted emotional factors for analysis. The study purpose is to figure out which emotional factors significantly influence the translation learning result. To ensure the study objective and novelty, support vector machine (SVM) method is applied in this study to analyze the data and get the main emotion factors that influence the translation performance. And it is to show the connections among the anxiety, the enjoyment and the boredom. From the final results, some valuable suggestions are provided for improving the teaching approach and making the translation courses more effective.} }
@inproceedings{10.1145/3539618.3591872, title = {On the "Rough Use" of Machine Learning Techniques}, booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {2}, year = {2023}, isbn = {9781450394086}, doi = {10.1145/3539618.3591872}, url = {https://doi.org/10.1145/3539618.3591872}, author = {Lin, Chih-Jen}, keywords = {machine learning, validation and prediction, location = Taipei, Taiwan}, abstract = {Machine learning is everywhere, but unfortunately, we are not experts of every method. Sometimes we "inappropriately'' use machine learning techniques. Examples include reporting training instead of test performance and comparing two methods without suitable hyper-parameter searches. However, the reality is that there are more sophisticated or more subtle examples, which we broadly call the "rough use'' of machine learning techniques. The setting may be roughly fine, but seriously speaking, is inappropriate. We briefly discuss two intriguing examples.- In the topic of graph representation learning, to evaluate the quality of the obtained representations, the multi-label problem of node classification is often considered. An unrealistic setting was used in almost the entire area by assuming that the number of labels of each test instance is known in the prediction stage. In practice, such ground truth information is rarely available. Details of this interesting story are in Lin et al. (2021).- In training deep neural networks, the optimization process often relies on the validation performance for termination or selecting the best epoch. Thus in many public repositories, training, validation, and test sets are explicitly provided. Many think this setting is standard in applying any machine learning technique. However, except that the test set should be completely independent, users can do whatever the best setting on all the available labeled data (i.e., training and validation sets combined). Through real stories, we show that many did not clearly see the relation between training, validation, and test sets.The rough use of machine learning methods is common and sometimes unavoidable. The reason is that nothing is called a perfect use of a machine learning method. Further, it is not easy to assess the seriousness of the situation. We argue that having high-quality and easy-to-use software is an important way to improve the practical use of machine learning techniques.} }
@inproceedings{10.1145/3675888.3676104, title = {Customer Opinion Analysis and Topic Extraction Using Machine Learning}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {502--507}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676104}, url = {https://doi.org/10.1145/3675888.3676104}, author = {Moza, Sidhant and Ashraf, Ehtesham and Agrawal, Himani and .. Vikash}, keywords = {Clustering, Deep Learning, Dimensionality Reduction, Feature Extraction, Machine Learning, NLP, location = Noida, India}, abstract = {In today’s competitive market landscape, understanding customer sentiments through online reviews is imperative for businesses. However, the unstructured nature of this data poses challenges for traditional supervised learning methods. This paper proposes a methodology integrating semi-supervised learning and clustering techniques to analyze software reviews. Through extensive preprocessing and feature extraction, including Word2Vec embeddings and keyword extraction, the approach achieves nuanced language understanding. Clustering algorithms like HDBSCAN partition the data into meaningful clusters, facilitating insightful classification. Hyperparameter tuning ensures optimal performance, with findings indicating the effectiveness of the approach in discerning product-specific opinions. Evaluation metrics confirm the quality of obtained clusters, revealing distinct sentiments towards various software products. This research contributes to enhancing customer understanding and decision-making in software development and marketing strategies.} }
@article{10.1145/3611383, title = {Machine Learning and Physics: A Survey of Integrated Models}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3611383}, url = {https://doi.org/10.1145/3611383}, author = {Seyyedi, Azra and Bohlouli, Mahdi and Oskoee, Seyedehsan Nedaaee}, keywords = {Machine learning-guided physics, physics-guided machine learning, modeling, neural networks, physics-based models, deep learning, machine learning}, abstract = {Predictive modeling of various systems around the world is extremely essential from the physics and engineering perspectives. The recognition of different systems and the capacity to predict their future behavior can lead to numerous significant applications. For the most part, physics is frequently used to model different systems. Using physical modeling can also very well help the resolution of complexity and achieve superior performance with the emerging field of novel artificial intelligence and the challenges associated with it. Physical modeling provides data and knowledge that offer a meaningful and complementary understanding about the system. So, by using enriched data and training phases, the overall general integrated model achieves enhanced accuracy. The effectiveness of hybrid physics-guided or machine learning-guided models has been validated by experimental results of diverse use cases. Increased accuracy, interpretability, and transparency are the results of such hybrid models. In this article, we provide a detailed overview of how machine learning and physics can be integrated into an interactive approach. Regarding this, we propose a classification of possible interactions between physical modeling and machine learning techniques. Our classification includes three types of approaches: (1)\&nbsp;physics-guided machine learning (2)\&nbsp;machine learning-guided physics, and (3)\&nbsp;mutually-guided physics and ML. We studied the models and specifications for each of these three approaches in-depth for this survey.} }
@inproceedings{10.1145/3677619.3678114, title = {Identifying Secondary School Students' Misconceptions about Machine Learning: An Interview Study}, booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research}, year = {2024}, isbn = {9798400710056}, doi = {10.1145/3677619.3678114}, url = {https://doi.org/10.1145/3677619.3678114}, author = {Marx, Erik and Witt, Clemens and Leonhardt, Thiemo}, keywords = {artificial intelligence, interview study, machine learning, mental models, misconceptions, qualitative research, students conceptions, location = Munich, Germany}, abstract = {Since students are familiar with machine learning (ML)-based applications in their everyday lives, they already construct mental models of how these systems work. This can result in misconceptions that influence the learning of correct ML concepts. Therefore, this study investigates the misconceptions students hold about the functionality of ML-based applications. To this end, we conducted semi-structured interviews with five students, focusing on their understanding of facial recognition and ChatGPT. The interviews were analyzed using an inductively developed code system and qualitative content analysis. This process identified six key misconceptions held by students: “Programmed Behavior,” “Exactness,” “Data Storage,” “Continuous Learning,” “User-trained Model,” and “Autonomous Data Acquisition”. These misconceptions include the notion that AI learns continuously during application, or that training data is saved and reused later. This paper presents the identified misconceptions and discusses their implication for the design and evaluation of effective learning activities in the context of ML.} }
@proceedings{10.1145/3701047, title = {CNML '24: Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, year = {2024}, isbn = {9798400711688} }
@inproceedings{10.1145/3701571.3703393, title = {Decoding Fatigue: Analyzing Offline Handwriting with Machine Learning to Detect Perceived Exhaustion}, booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia}, pages = {487--489}, year = {2024}, isbn = {9798400712838}, doi = {10.1145/3701571.3703393}, url = {https://doi.org/10.1145/3701571.3703393}, author = {Schoen, Dominik and Kosch, Thomas and Becker, Till and Antwi-Boasiako, Godfred and Jung, Merret and Chioca Vieira, Ana Laura and M\"uhlh\"auser, Max and M\"uller, Florian}, keywords = {Exertion, Fatigue, Machine Learning, Drawing, Exhaustion, Discomfort}, abstract = {The quality and readability of an individual’s handwriting and drawing can be influenced by various factors, including their level of physical exertion. This enables us to explore the quantification of exertion by observing an individual’s handwriting. To test this hypothesis, we collected data from 17 participants, building a database of handwriting and drawing samples and their corresponding Borg 10 exertion ratings at the time of drawing. In this paper, we investigate using machine learning techniques to estimate perceived exertion before, during, and after physical activity based on handwriting and drawings. We apply a regression model to compare different drawing tasks and demonstrate that perceived exertion can be predicted using simple line drawings. However, more complex sketches and handwriting demand further research. Our findings suggest that interactive systems could use handwriting and drawing to intervene when users experience excessive discomfort.} }
@inproceedings{10.1145/3641512.3686393, title = {In-Sensor Machine Learning: Radio Frequency Neural Networks for Wireless Sensing}, booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing}, pages = {261--270}, year = {2024}, isbn = {9798400705212}, doi = {10.1145/3641512.3686393}, url = {https://doi.org/10.1145/3641512.3686393}, author = {Tong, Jingyu and An, Zhenlin and Zhao, Xiaopeng and Liao, Sicong and Yang, Lei}, keywords = {in-sensor machine learning, physical neural network, location = Athens, Greece}, abstract = {Growing interest in wireless sensing, a cornerstone of the Artificial Intelligence of Things (AIoT), stems from its ability to gauge target states through nearby wireless signals. However, the escalating count of AIoT nodes escalates redundant data flow and exacerbates energy usage in AI cloud infrastructures. This amplifies the urgency for machine learning techniques that function in proximity to, or directly within, sensors. In light of this, we present the Radio-Frequency Neural Network (RFNN), a novel architecture that uses cost-effective transmissive intelligent surfaces to mimic the functions of a traditional neural network near (or in) sensors, transforming sensory nodes into intelligent terminals primed for machine learning. We first devised a unique training algorithm to mitigate the issues arising from unmodelable error-backward propagation; secondly, we incorporated contrastive learning to address the issue of blind labels stemming from environmental uncertainties. Our RFNN prototype, resonating at a 5 GHz WiFi bandwidth, has been honed across nine varied sensing tasks. The rigorous evaluation shows that it achieves a mean accuracy of 91.5\% while consuming only 67.2 μJ of energy. This positions RFNN as a match in inferencing prowess to its electronic neural network counterparts but with significantly diminished energy demands.} }
@inproceedings{10.1145/3658644.3691366, title = {Demo: FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {5075--5077}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3691366}, url = {https://doi.org/10.1145/3658644.3691366}, author = {Gu, Yuechun and He, Jiajie and Chen, Keke}, keywords = {differential privacy, membership inference attack, location = Salt Lake City, UT, USA}, abstract = {Data privacy has been a top concern in the AI era. Despite the recent development of differentially private learning methods, controlled data access remains a mainstream method for protecting data privacy in many industrial and research environments. In controlled data access, authorized model builders work in a restricted environment to access sensitive data, which can fully preserve data utility with reduced risk of data leak. However, unlike differential privacy, there is no quantitative measure for individual data contributors to tell their privacy risk before participating in a machine learning task. We developed the demo prototype FT-PrivacyScore to show that it's possible to efficiently and quantitatively estimate the privacy risk of participating in a model fine-tuning task. The demo source code will be available at https://github.com/RhincodonE/demo_privacy_scoring.} }
@inproceedings{10.1145/3624062.3626083, title = {Machine Learning Applied to Single-Molecule Activity Prediction}, booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {66--72}, year = {2023}, isbn = {9798400707858}, doi = {10.1145/3624062.3626083}, url = {https://doi.org/10.1145/3624062.3626083}, author = {Hood, Kendric and Guan, Qiang}, keywords = {chemistry, data sets, machine learning, neural networks, single-molecule, location = Denver, CO, USA}, abstract = {Catalytic processes are used in about 1/3 of US manufacturing, from the field of chemical engineering to renewable energy. Assessing the activity of single-molecules, or individual molecules, is necessary to the development of efficient catalysts. Their heterogeneity structure leads to particle-specific catalytic activity. Experimentation with single-molecules can be time consuming and difficult. We purpose a Machine learning (ML) model that allows chemical researchers to run shorter single-molecule experiments to obtain the same level of results. We use common and widely understood ML methods to reduce complexity and enable accessibility to the chemical engineering community. We reduce the experiment time by up to 83\%. Our evaluation shows that a small data set is sufficient to train an acceptable model. 300 experiments are needed, including the validation set. We use a well understood multilayer perceptron (MLP) model. We show that more complex models are not necessary and simpler methods are not sufficient.} }
@inproceedings{10.1145/3674029.3674036, title = {Comparative Study of Machine Learning Techniques for Inventory Classification Based on Multi-Criteria Decision-Making}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {36--40}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674036}, url = {https://doi.org/10.1145/3674029.3674036}, author = {Phruksaphanrat, Busaba}, keywords = {ABC, AHP, Inventory classification, Machine learning, Multi-attribute decision-making, location = Oslo, Norway}, abstract = {Various multicriteria inventory classification methods have been developed to overcome the limitations of conventional ABC analysis. Commonly used techniques include the analytic hierarchy process (AHP) and data envelopment analysis (DEA). However, these methods are mainly focused on classifying existing items in the inventory. Furthermore, both total inventory costs and the similarity of each group should be of concern. To address the challenge of assigning groups to new, unclassified items in the warehouse, this research proposes integrating machine learning (ML) techniques with multicriteria inventory classification. The combined approach considers both similarity and total costs, thereby improving the accuracy of inventory classification for both existing and new items based on the existing groups classified using the multicriteria approach. The result has shown that among ABC analysis, DEA, and AHP; AHP outperforms in the classification of the current inventory items of the case study factory based on the minimum total inventory cost and similarity index. To achieve the highest accuracy in inventory classification, firstly discriminant analysis (DA) and artificial neural network (ANN) were identified as the most suitable machine learning (ML) techniques to be integrated. After tuning some parameters, the best adjusted ANN model was found with the highest accuracy at 97.70\% of testing data and F1 at 100\%, 94.74\%, and 98.25\% for classes A, B, and C, respectively.} }
@inproceedings{10.1145/3589806.3600039, title = {Integrated Reproducibility with Self-describing Machine Learning Models}, booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability}, pages = {1--14}, year = {2023}, isbn = {9798400701764}, doi = {10.1145/3589806.3600039}, url = {https://doi.org/10.1145/3589806.3600039}, author = {Wonsil, Joseph and Sullivan, Jack and Seltzer, Margo and Pocock, Adam}, keywords = {Machine Learning, provenance, reproducibility, location = Santa Cruz, CA, USA}, abstract = {Researchers and data scientists frequently want to collaborate on machine learning models. However, in the presence of sharing and simultaneous experimentation, it is challenging both to determine if two models were trained identically and to reproduce precisely someone else’s training process. We demonstrate how provenance collection that is tightly integrated into a machine learning library facilitates reproducibility. We present MERIT, a reproducibility system that leverages a robust configuration system and extensive provenance collection to exactly reproduce models, given only a model object. We integrate MERIT with Tribuo, an open-source Java-based machine learning library. Key features of this integrated reproducibility framework include controlling for sources of non-determinism in a multi-threaded environment and exposing the training differences between two models in a human-readable form. Our system allows simple reproduction of deployed Tribuo models without any additional information, ensuring data science research is reproducible. Our framework is open-source and available under an Apache 2.0 license.} }
@inproceedings{10.1145/3705927.3705947, title = {Optimizing Medical Imaging: High-Performance Hardware for Image Processing, Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Digital Medicine and Image Processing}, pages = {112--116}, year = {2025}, isbn = {9798400709586}, doi = {10.1145/3705927.3705947}, url = {https://doi.org/10.1145/3705927.3705947}, author = {N S, Kalyan Chakravarthy and Balapanur, Mouli Chandra and Pandian, Arun Nambi and Pulikanti, Jyothi and D, Prasad and Syed Masood, Jafar Ali Ibrahim}, keywords = {GPUS, High-performance hardware, Machine learning and image processing, Medical imaging}, abstract = {The demand for high-performance hardware solutions for machine learning tasks is growing as medical imaging evolves. In this paper, we will focus on the latest hardware advanced technologies: GPUs, TPUs and FPGAs that can be used for image processing optimization in medical imaging such as brain tumor segmentation or chest X-ray classification. We test these devices on the BraTS and ChestX-ray14 datasets to show how running models quicker increases diagnostic performance. The study offers a comprehensive performance and energy efficiency evaluation of all the considered hardware solutions when training deep learning models for medical imaging, providing clinical guidelines to choose appropriate new-generation accelerators before engaging in expensive leasing contracts.} }
@inproceedings{10.1145/3718751.3718877, title = {Optimizing Vegetable Pricing and Replenishment Decisions Using Hybrid Machine Learning Models}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {770--774}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718877}, url = {https://doi.org/10.1145/3718751.3718877}, author = {Han, Yiyou and Huang, Ziheng and Xu, Yihong}, keywords = {ARIMA, Genetic Algorithm, Linear Regression, PSO, Replenishment}, abstract = {In the modern business environment, the automatic pricing and replenishment decision of vegetable commodities has become the core to ensure the smooth supply chain and the profit of merchants. This study combined ARIMA, particle swarm optimization (PSO), linear regression and genetic algorithm to propose and implement an automatic pricing and replenishment strategy based on a mixed machine learning model and a large number of historical sale data. The average (R^2) of different categories of vegetables was 0.992, indicating a high accuracy of the model. At the same time, it provided a pricing and purchase strategy for vegetables for merchants.} }
@inproceedings{10.1109/ICSE-Companion58688.2023.00065, title = {Towards Machine Learning Guided by Best Practices}, booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings}, pages = {240--244}, year = {2023}, isbn = {9798350322637}, doi = {10.1109/ICSE-Companion58688.2023.00065}, url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00065}, author = {Mojica-Hanke, Anamaria}, keywords = {machine learning, good practices, software engineering, location = Melbourne, Victoria, Australia}, abstract = {Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where multiple studies related to the use of Machine Learning in Software Engineering have been published in conferences and journals. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional software, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and focused on their domain (e.g., biomechanics), but for the best of our knowledge, there is no guideline focused on the SE community. This thesis aims to reduce the gap of not having clear guidelines in the SE community by using possible sources of practices such as question-and-answer communities and also previous research studies. As a result, we will present a set of practices with an SE perspective, for researchers and practitioners, including a tool for searching them.} }
@inproceedings{10.1145/3726854.3727297, title = {NetJIT: Bridging the Gap from Traffic Prediction to Preknowledge for Distributed Machine Learning}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, booktitle = {Abstracts of the 2025 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems}, volume = {9}, pages = {145--147}, year = {2025}, isbn = {9798400715938}, doi = {10.1145/3726854.3727297}, url = {https://doi.org/10.1145/3726854.3727297}, author = {Ai, Xin and Li, Zijian and Zhu, Yuanyi and Chen, Zixuan and Liu, Sen and Xu, Yang}, keywords = {JIT, distributed machine learning, just-in-time program analysis, network optimization, traffic prediction, location = Stony Brook, NY, USA, jit}, abstract = {Today's distributed machine learning (DML) introduces heavy traffic load, making the interconnection network one of the primary bottlenecks. To mitigate this bottleneck, existing state-of-the-art network optimization methods, such as traffic or topology engineering, are proposed to adapt to real-time traffic. However, current traffic measurement and prediction methods struggle to collect sufficiently fine-grained and accurate traffic patterns. This limitation impedes the ability of cutting-edge network optimization techniques to react agilely to the ever-changing traffic demands of DML jobs.This paper proposes NetJIT, a novel program-behavior-aware toolkit for accurately foreseeing the traffic pattern of DML. To the best of our knowledge, this is the first work proposing the use of just-in-time (JIT) program analysis for real-time traffic measurement. In DML applications, communication behavior is primarily determined by the previously computed results. NetJIT leverages this characteristic to anticipate communication details by tracing and analyzing the data relations in the computation process. This capability enables the deployment of optimization strategies in advance.We deploy NetJIT in real-world network optimization for traffic preknowledge. Evaluation with the self-built testbed prototype demonstrates that NetJIT can achieve up to about 97\% less error of detecting communication events compared with other methods. Simulations with real-world DML workloads further illustrate that NetJIT enables more precise network optimization, leading to approximately 50\% better network performance w.r.t the metrics including average iteration time, throughput, and average packet delay.} }
@inproceedings{10.1145/3647444.3647926, title = {Early diagnosis of Parkinson disease using Machine Learning Techniques}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647926}, url = {https://doi.org/10.1145/3647444.3647926}, author = {Srivastava, Atul and Rana, Harshita and Dixit, Prashant and Singh, Reecha}, keywords = {Gradient Boosting, K-NN, Machine Learning, Parkinson Disease Prediction, Random Forest, SVM, location = Jaipur, India}, abstract = {Approximately two in every thousand people suffer from Parkinson disease. The symptoms of this neurological disorder can be motor or non-motor. However, it is significantly difficult to determine the gravity and appropriate classification of the disease. The diagnosis of PD majorly depends on the clinical examination and neurological examinations. Recently, machine learning techniques have proved to be an alternate method to detect the disease in a very early stage of it. Machine learning techniques use motor symptoms (gait analysis, handwriting, etc.) and non-motor symptoms (voice characteristics) to classify the people suffering and not suffering from PD. This study evaluated classifiers such as K-Nearest Neighbours (K-NN), Random Forest (RF), Gradient Boosting, Support Vector Machine (SVM), Boosting, and Bagging.} }
@inproceedings{10.1145/3715020.3715044, title = {Predicting Protein Interactions with BteA in Bordetella pertussis Pathogenesis Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computational Biology and Bioinformatics}, pages = {42--47}, year = {2025}, isbn = {9798400709623}, doi = {10.1145/3715020.3715044}, url = {https://doi.org/10.1145/3715020.3715044}, author = {Koshiba, Toshiki and Sudo, Chisato and Ogawa, Toshinobu and Iuchi, Toshihiko and Martono, Niken Prasasti and Kuwae, Asaomi and Ohwada, Hayato}, keywords = {Protein-Protein Interactions (PPIs), Machine Leargning, BteA, Bordetella pertussis}, abstract = {This study employed machine learning techniques to identify protein interactions with BteA in Bordetella pertussis, aiming to enhance our understanding of whooping cough pathogenesis and discover potential targets for new drug. Whooping cough (pertussis), caused by B. pertussis, remains a significant global health challenge despite widespread vaccine availability. B. pertussis protein BteA, delivered into host cells through the type III secretion system, plays a crucial role in promoting host cell death and pathogenesis. We utilized machine learning to predict proteins that interact with BteA. First, the STRING database was used to learn protein interactions, and Biopython was employed to extract four biological features for each protein: isoelectric point, total number of positively and negatively charged amino acid residues, and GRAVY. Six machine learning models, including XGBoost, Random Forest, and LightGBM, were trained using these features. The top models were applied to predict proteins likely to interact with BteA. RpoB, RpID, BP2755 and RpsK were identified with high interaction probability. Feature importance analysis using Shapley values showed that isoelectric points and GRAVY contributed significantly to the prediction models. Our findings demonstrate the effectiveness of machine learning in predicting protein-protein interactions within B. pertussis, providing valuable insights into the interaction network of BteA. The identified proteins offer promising targets for new drug, potentially contributing to the development of new treatments for whooping cough.} }
@inproceedings{10.1145/3551901.3556484, title = {A Thermal Machine Learning Solver For Chip Simulation}, booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD}, pages = {111--117}, year = {2022}, isbn = {9781450394864}, doi = {10.1145/3551901.3556484}, url = {https://doi.org/10.1145/3551901.3556484}, author = {Ranade, Rishikesh and He, Haiyang and Pathak, Jay and Chang, Norman and Kumar, Akhilesh and Wen, Jimin}, keywords = {autoencoders, chip thermal simulation, machine learning, location = Virtual Event, China}, abstract = {Thermal analysis provides deeper insights into electronic chips' behavior under different temperature scenarios and enables faster design exploration. However, obtaining detailed and accurate thermal profile on chip is very time-consuming using FEM or CFD. Therefore, there is an urgent need for speeding up the on-chip thermal solution to address various system scenarios. In this paper, we propose a thermal machine-learning (ML) solver to speed-up thermal simulations of chips. The thermal ML-Solver is an extension of the recent novel approach, CoAEMLSim (Composable Autoencoder Machine Learning Simulator) with modifications to the solution algorithm to handle constant and distributed HTC. The proposed method is validated against commercial solvers, such as Ansys MAPDL, as well as a latest ML baseline, UNet, under different scenarios to demonstrate its enhanced accuracy, scalability, and generalizability.} }
@inproceedings{10.1145/3702468.3702478, title = {A Comparative Analysis of Machine Learning Techniques for Therapeutic Warfarin Dose Prediction}, booktitle = {Proceedings of the 2024 7th International Conference on Robot Systems and Applications}, pages = {59--63}, year = {2024}, isbn = {9798400717031}, doi = {10.1145/3702468.3702478}, url = {https://doi.org/10.1145/3702468.3702478}, author = {Khianchainat, Khatadet and Kanjanawattana, Sarunya}, keywords = {Machine learning, Warfarin, Grid search, Artificial intelligence, Dose Estimation}, abstract = {Warfarin is a crucial anticoagulant medication used for the prevention and treatment of thromboembolic disorders. This study aims to develop an effective machine learning model for predicting appropriate warfarin dosing for patients in Thailand, considering their Asian origin and based on Thailand medication process. Our dataset for this preliminary investigation was derived from IWPC. We conducted the necessary data preprocessing, feature selection, and balancing procedures to acquire the suitable dataset. Nine machine learning algorithms that have been refined include the following: Multilayer Perceptron Classifier, Support Vector Machines Classifier, K Nearest Neighbours, Extreme Gradient Boosting, Categorical Boosting Classifier, Gradient Boosting Tree, Random Forest, Decision Tree Classifier, and Logistic Regression. The experiment yielded the finding that, in comparison to the other algorithms, the Categorical Boosting Classifier exhibited the maximum accuracy (72\%).} }
@proceedings{10.1145/3690771, title = {ACMLC '24: Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, year = {2024}, isbn = {9798400710018} }
@inproceedings{10.1145/3652037.3663949, title = {Predicting Stress among Students via Psychometric Assessments and Machine Learning}, booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {662--669}, year = {2024}, isbn = {9798400717604}, doi = {10.1145/3652037.3663949}, url = {https://doi.org/10.1145/3652037.3663949}, author = {Ghosh, Sagnik and Tripathi, Kirti and Garg, Ankita and Singh, Dinesh and Prasad, Amit and Bhavsar, Arnav and Dutt, Varun}, keywords = {Artificial Neural Networks, Common Yoga Protocol, Mental Health, R-squared Score, RMSPE, Stress, location = Crete, Greece}, abstract = {Yoga is recommended as a method for managing stress among students. Despite its widespread acceptance, however, there exists a gap in research concerning the prediction of changes in psychological stress among students practicing yoga. The main objective of this study was to assess the immediate consequences of practicing yoga among students on their psychological stress levels and to predict changes in stress levels via machine learning. A total of 166 participants were recruited in this study and were randomly divided into two groups: intervention (N = 110) and control N =56). The intervention group engaged in regular, structured sessions of the common yoga protocol by the Indian Government thrice a week in 45 minutes sessions over a period of 6 weeks. The self-reported questionnaire such as the Depression, Anxiety, Stress Scales (DASS 21), was employed to assess the psychological stress change before and after the intervention. Additionally, machine learning (ML) algorithms such as Linear Regression (LR), Random Forest (RF), Support Vector Regression (SVR), and Artificial Neural Networks (ANN) were developed to predict changes in stress levels as a result of the intervention from data collected before the intervention. These models were evaluated on metrics such as R-squared and root mean square percent error (RMSPE), with the RF algorithm showing the lowest RMSPE of 1.23 units and R-squared of 0.71 by relying on top 7 features. This research not only affirms the positive effects of yoga on psychological health but also highlights the utility of machine learning in predicting stress changes, offering new perspectives on yoga's role in stress management.} }
@inproceedings{10.1145/3708036.3708152, title = {Research on Car Insurance Compensation Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {687--691}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708152}, url = {https://doi.org/10.1145/3708036.3708152}, author = {Han, Yichen}, keywords = {Car Insurance, Logistic Regression, Multilayer Perceptron, Random Forest}, abstract = {In the field of insurance claims, it is important to understand the impact of each variable on the amount of the claim. Accurate prediction of whether a car insurance claim will be paid out and the amount of the claim will be paid out is of great significance to increase the company's profit, optimize the premium pricing, reduce the financial risk of the insurance company, and enhance the competitiveness of the insurance company in the market. In this study, we analyze industry reports, market data and relevant literature to study the key factors affecting the amount of insurance claims, firstly, we construct the ‘Insurance Claims Prediction Model’ and select five variables that have the greatest impact on the amount of car insurance claims, and then we predict the amount of car insurance claims through the selected variables. By analyzing the intrinsic connection of the data, based on the random forest model, it predicts whether the insurance is paid or not, and selects the variables that affect the insurance compensation amount, and predicts whether the insurance is paid or not; in addition, in order to accurately predict the amount of the insurance compensation, based on the theory of neural network, it constructs a large-data neural network prediction model, and achieves good experimental results, and obtains a low error. This study conducted the preliminary research on the prediction of car insurance compensation amount, which provides an important reference value for the subsequent formulation of the premium amount of car insurance.} }
@inproceedings{10.1145/3718491.3718638, title = {A Comparative Study of Linear and Nonlinear Machine Learning Algorithms in Quantitative Investment}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {909--913}, year = {2025}, isbn = {9798400710865}, doi = {10.1145/3718491.3718638}, url = {https://doi.org/10.1145/3718491.3718638}, author = {Zheng, Zengji and Sun, Haocheng and Chen, Sihan and Cai, Jiayi}, keywords = {investment portfolio, machine learning, multi-factor modeling, quantitative investment}, abstract = {In quantitative investing, multi-factor models are widely used, but the growing number of factors presents challenges for traditional models in factor selection and model structure. Machine learning algorithms, however, offer advantages in feature selection and return prediction, enabling multi-factor strategies to handle more data and achieve better performance. Linear and nonlinear machine learning algorithms have distinct feature selection mechanisms, each with its strengths and weaknesses in return prediction.This paper compares two linear algorithms (Lasso and Elastic Net) with two nonlinear algorithms (Random Forest and Gradient Boosting Decision Tree) in terms of factor selection and investment performance. Using a sample of CSI 300 index constituents from January 2007 to November 2021, 244 factors across eight categories—including quality, value, and momentum—were studied. The results show that linear models excel at overcoming factor correlation and noise, while nonlinear models are better at capturing complex, long-term relationships between factors and returns. Linear models tend to favor momentum factors related to price movements, whereas nonlinear models emphasize value and quality factors based on company fundamentals.To leverage the strengths of both linear and nonlinear models, we applied Simple Average Integration and Stacking Integration, resulting in enhanced portfolio performance, including higher monthly returns and Sharpe ratios, compared to both traditional linear regression and the Shanghai-Shenzhen-300 Index. This integrated approach offers a more robust and reliable investment strategy.In conclusion, this study demonstrates the potential of machine learning algorithms in improving multi-factor quantitative investment strategies, providing valuable insights for future investment practices.} }
@article{10.1145/3665929, title = {IoT Video Delivery Optimization through Machine Learning-Based Frame Resolution Adjustment}, journal = {ACM Trans. Multimedia Comput. Commun. Appl.}, volume = {20}, year = {2024}, issn = {1551-6857}, doi = {10.1145/3665929}, url = {https://doi.org/10.1145/3665929}, author = {Bandung, Yoanes and Wicaksono, Mokhamad Arfan and Pribadi, Sean and Langi, Armein Z. R. and Tanjung, Dion}, keywords = {Internet of video things, machine learning, time series forecasting, throughput prediction, file size estimator}, abstract = {Providing acceptable video quality in the Internet of Things (IoT) implementation poses a significant challenge, mainly when the application is performed on low-cost and low-power devices. This research focuses on developing a frame resolution adjustment system that maintains the frame rate value of video delivery in wireless IoT environments with resource-constrained devices. Consistent frame rates prevent motion lag and data loss, improving user experience. The system works by predicting the upcoming throughput values using machine learning methods to adjust the sensing parameter, which is the resolution of the video frame to be captured by camera nodes. Hence, the proposed system is equipped with a file size estimator to estimate the size of the next video frame and then adjust the resolution in accordance with the throughput prediction. In this research, we conducted extensive experiments to evaluate the accuracy of the file size estimator and the throughput prediction. The experiment generated a dataset to evaluate throughput prediction and file size estimator model. The evaluation results for the file size estimator showed a mean absolute percentage error (MAPE) of 6.73\% in the experiment using 317 frames with video resolutions between 72p and 720p. Experiments were also conducted to compare several machine learning methods for predicting throughput values. Compared to long short-term memory (LSTM) and autoregressive integrated moving average (ARIMA), simple exponential smoothing (SES) outperforms the others with the lowest root mean squared error (RMSE) and mean absolute error (MAE) values. Building upon these findings, we implemented the frame resolution adjustment system using SES as the method for predicting the upcoming throughput values. Finally, we demonstrated that the proposed system can maintain the frame rate according to the threshold set by the system while the resolution is being maximized, thereby addressing the challenges of maintaining video quality in resource-constrained IoT environments.} }
@article{10.1109/TNET.2022.3190797, title = {Machine Learning Feature Based Job Scheduling for Distributed Machine Learning Clusters}, journal = {IEEE/ACM Trans. Netw.}, volume = {31}, pages = {58--73}, year = {2022}, issn = {1063-6692}, doi = {10.1109/TNET.2022.3190797}, url = {https://doi.org/10.1109/TNET.2022.3190797}, author = {Wang, Haoyu and Liu, Zetian and Shen, Haiying}, abstract = {With the rapid proliferation of Machine Learning (ML) and Deep learning (DL) applications running on modern platforms, it is crucial to satisfy application performance requirements such as meeting deadline and ensuring accuracy. To this end, researchers have proposed several job schedulers for ML clusters. However, none of the previously proposed schedulers consider ML model parallelism, though it has been proposed as an approach to increase the efficiency of running large-scale ML and DL jobs. Thus, in this paper, we propose an ML job Feature based job Scheduling system (MLFS) for ML clusters running both data parallelism and model parallelism ML jobs. MLFS first uses a heuristic scheduling method that considers an ML job’s spatial and temporal features to determine task priority for job queue ordering in order to improve job completion time (JCT) and accuracy performance. It uses the data from the heuristic scheduling method for training a deep reinforcement learning (RL) model. After the RL model is well trained, it then switches to the RL method to automatically make decisions on job scheduling. In addition, MLFS has a system load control method that selects tasks from overloaded servers to move to underloaded servers based on task priority, and also intelligently removes the tasks that generate little or no improvement on the desired accuracy performance when the system is overloaded to improve JCT and accuracy by job deadline. Furthermore, we propose Optimal ML iteration stopping method that determines the proper time to stop training ML model when this model reaches the minimum loss value. Our real experiments and large-scale simulation based on real trace show that MLFS reduces JCT by up to 53\% and makespan by up to 52\%, and improves accuracy by up to 64\% when compared with existing ML job schedulers. We also open sourced our code.} }
@inproceedings{10.1145/3597512.3597522, title = {MACAIF: Machine Learning Auditing for Clinical AI Fairness}, booktitle = {Proceedings of the First International Symposium on Trustworthy Autonomous Systems}, year = {2023}, isbn = {9798400707346}, doi = {10.1145/3597512.3597522}, url = {https://doi.org/10.1145/3597512.3597522}, author = {Barnard, Pepita and Bautista, John Robert and Krook, Joshua and Liu, Anqi and Men\'endez, H\'ector and Schmidt, Aurora and Sookoor, Tamim}, keywords = {Auditing, Dashboard, Doctor-centred, Healthcare, MLighter, Machine Learning, location = Edinburgh, United Kingdom}, abstract = {Artificial intelligence in the form of machine learning algorithms is driving the latest industrial revolution, leading to disruptive changes in the ways we communicate, interact, design, collect information, and express ourselves. While these changes offer new possibilities for our societies, they may also introduce biases that can lead to unfair decisions. This issue is particularly critical in the context of medical diagnosis, as bias can jeopardize patient treatment and health. To mitigate these biases, it is essential to such biases and involve all relevant stakeholders in the design of fair machine learning algorithms. In this context, the MACAIF project aims to develop user-centred interfaces that allow stakeholders, including doctors, to challenge the fairness of machine learning algorithms based on demographics, such as gender or race. Our project proposes a methodology to engage with stakeholders and incorporate their concerns during the design of a dashboard based on MLighter - an adversarial tool which is applied to identify fairness-related issues in machine learning models.} }
@inbook{10.1145/3718491.3718672, title = {Environmental Ecological Analysis Based on Machine Learning and Big Data--Evidence from China}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {1125--1130}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718672}, author = {Chao, Xiao and Zhang, Qiang}, abstract = {Based on machine learning and big data technology, this paper analyzes the environmental ecology of China. Firstly, this paper collected and analyzed a large number of eco-environmental data sets, and discussed the trend and reasons of eco-environmental changes in China from the perspective of precipitation and land cover. Then the article uses LGBM and SHAP models to evaluate the contribution of influencing factors to the ecological environment, and evaluates the effect of the model. The key indicators affecting the ecological environment are obtained and the corresponding analysis is given. It provides an empirical reference for the use of machine learning methods in geography.} }
@inproceedings{10.1145/3627673.3679925, title = {Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {3637--3641}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679925}, url = {https://doi.org/10.1145/3627673.3679925}, author = {Briscoe, Jarren and Gebremedhin, Assefaw}, keywords = {bias, bias metric, discrimination law, disparate impact, fairness, legal policy, objective fairness index, protected classes, location = Boise, ID, USA}, abstract = {Leveraging current legal standards, we define bias through the lens of marginal benefits and objective testing with the novel metric "Objective Fairness Index". This index combines the contextual nuances of objective testing with metric stability, providing a legally consistent and reliable measure. Utilizing the Objective Fairness Index, we provide fresh insights into sensitive machine learning applications, such as COMPAS (recidivism prediction), highlighting the metric's practical and theoretical significance. The Objective Fairness Index allows one to differentiate between discriminatory tests and systemic disparities.} }
@inproceedings{10.1145/3697467.3697640, title = {Identifying and predicting key factors of customer satisfaction based on machine learning}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {200--204}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697640}, url = {https://doi.org/10.1145/3697467.3697640}, author = {Li, Beiling and Lai, Yuxuan and Ling, Yangbo and Li, Ganxiang}, keywords = {AdaBoost, Customer satisfaction, Feature engineering, LightGBM, SVM, Stacking, XGBoost}, abstract = {With the advancement of mobile communication technology and the continuous development of network infrastructure, people increasingly enjoy the convenience brought by mobile communication and the internet. The user's network experience is also increasingly valued by mobile operators. To help further enhance the quality of network services and market operations, the current study focuses on customer satisfaction as an important indicator. Firstly, feature engineering is used to better anchor the key factors and improve the performance of the model. Secondly, various models such as Tree-based models and Support Vector Machine (SVM) model were used for data processing, and Bayesian parameter tuning methods for model optimization. Finally, a predictive model is established using the Stacking ensemble method to integrate the models. It turns out that the ensemble model has a better performance to predict customers’ satisfaction than any single model.} }
@inproceedings{10.1145/3690771.3690779, title = {A study of relationship between business performance and stock prices using machine learning techniques}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {14--19}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690779}, url = {https://doi.org/10.1145/3690771.3690779}, author = {Rukpanichsiri, Vorapat and Soonthornphisaj, Nuanwan}, keywords = {AdaBoost, Feature Importance, Gradient Boosting, Lasso regression, Neural Networks, Random Forest, Regressing Tree, Stock price}, abstract = {This research investigates the relationship between fundamental factors and stock prices. The study uses 50 stocks listed on the SET100 index in 2022. The fundamental factors of interest are earnings per share (EPS), price-to-book value (P/BV), net profit margin, dividend yield, return on assets (ROA), return on equity (ROE), and debt-to-equity ratio (D/E). The study uses quarterly data for 10 years, from 2012 to 2021. This results in 40 data points per stock. The relationship between fundamental factors and stock prices is investigated using machine learning methods, including Lasso, Regression Tree, Random Forest, AdaBoost, Xgboost, Gradient Boosting, and Neural Network. The relationship between fundamental factors and stock prices is evaluated using k-fold cross-validation. This method divides the data into training and testing sets. The performance of each model is measured using the R-squared value. The findings of the study suggest that some fundamental factors are significant predictors of stock prices. The strongest relationships are found in P/BV. The results also suggest that Neural Network methods can be used to identify the relationships between fundamental factors and stock prices.} }
@inproceedings{10.1145/3703935.3704116, title = {Research on Intention Recognition of Air Target Based on Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Artificial Intelligence and Pattern Recognition}, pages = {386--395}, year = {2025}, isbn = {9798400717178}, doi = {10.1145/3703935.3704116}, url = {https://doi.org/10.1145/3703935.3704116}, author = {Niu, QianRu and Ren, ShuangYin and Gao, Wei and Wang, ChunJiang}, keywords = {Air Target, Combat Intention, Deep Learning, Intention Recognition, Machine Learning}, abstract = {With the evolution and enhancement of air combat forces in modern warfare, accurate recognition of the combat intentions of air targets plays a vital role in battlefield situation assessment and command decision-making. This paper comprehensively sorts out and deeply analyzes the research on air target intention recognition through a systematic literature review method. The article first introduces the relevant concepts of intention recognition, summarizes the application of the two key elements of air target intention recognition, and explores its basic problems in the military field. Then, the research progress of traditional and machine learning-based intention recognition approaches is summarized in chronological order, outlining the main progress and shortcomings of current research, besides, the performance of various methods is compared. Since deep learning technology has shown great application potential in the field of intention recognition in complex battlefields, this paper constructs an intention recognition framework using deep learning methods. Finally, the future research direction of air target intention recognition is prospected, and targeted research suggestions are put forward.} }
@inproceedings{10.1145/3703323.3703749, title = {Road traffic accident severity prediction using causal inference and machine learning}, booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)}, pages = {292--300}, year = {2025}, isbn = {9798400711244}, doi = {10.1145/3703323.3703749}, url = {https://doi.org/10.1145/3703323.3703749}, author = {Srivastava, Nishtha and Gohil, Bhavesh N. and Ray, Suprio}, abstract = {The global rise in road traffic accidents presents substantial challenges across economic, societal, and public health domains, leading to millions of injuries and fatalities annually. Current studies on modeling and analyzing traffic accident frequency largely treat the issue as a classification task, primarily utilizing learning-based or ensemble methods. However, these approaches frequently neglect the intricate relationships among the multifaceted factors—such as road complexity, environmental conditions, driver behavior, and contextual elements—that contribute to traffic accidents and hazardous scenarios. We propose an approach that employs causal inference and causal Machine Learning (ML) techniques to predict accident severity and identify key causal factors. We evaluate our proposed approach with two datasets, from Ethiopia and UK. Given the inherent imbalance in these datasets, the Synthetic Minority Oversampling Technique (SMOTE) is utilized to achieve balanced data representation. Uplift modeling and causal inference methods are employed for severity prediction. Individual Treatment Effect (ITE) and Average Treatment Effect (ATE) are used to make interpretations of the predictions. Our research contributes to understanding and mitigating the impact of road traffic accidents through advanced causal analysis techniques, offering actionable insights for policymakers, urban planners, and public health officials globally.} }
@inproceedings{10.1145/3745812.3745818, title = {Sustainable Vertical Farming: Leveraging Machine Learning and IoT for Energy Efficiency and Productivity}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745818}, url = {https://doi.org/10.1145/3745812.3745818}, author = {Gupta, Shruti and Das, Arnab and Sinha, Sanjay Kumar}, keywords = {Vertical farming, energy efficiency, machine learning, sustainability, water management}, abstract = {Vertical farming as an idea has emerged as a helpful strategy to mitigate problems related to food production throughout the world, less land available, and climate changes. Through the use of controlled environment agriculture, vertical farming consists of growing food in layers and within the vicinity of consumers, hence decreasing the need for conventional agriculture. One of the precision framing efficient crop growth practices that easily integrates machine learning (ML) technologies, and the Internet of Things (IoT) is vertical framing. Various technologies are used, including robotics, machine learning, big data analytics, drones, and IoT sensors in gathering data on crop growth, environmental conditions, and weather patterns. For this reason, energy efficiency is regarded as a significant parameter that determines the economic sustainability of vertical farms if not their environmental impacts. This article explores energy efficiency within vertical farming systems highlighting how technology, better practices, and the use of clean energy sources help counter the high energy needs of vertical farms. The paper also looks into energy-efficient systems and designs such as LED lights which consume lower electric power and are designed to produce light like natural sunlight, energy-efficient HVAC systems which operate with less energy, systems for recycling water and nutrients and minimize usage of these resources.} }
@inproceedings{10.1145/3698062.3698088, title = {Exploring Machine Learning for Credit Card Fraud Detection from a Philippine Perspective}, booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)}, pages = {177--182}, year = {2024}, isbn = {9798400717086}, doi = {10.1145/3698062.3698088}, url = {https://doi.org/10.1145/3698062.3698088}, author = {Blancaflor, Eric and Asuncion, Keziah Dawn and Reyes, Hailie Jade and Verzosa, Michaela}, keywords = {Artificial Neural Network, Credit Card, Detection, Fraud Detection, Machine Learning, Philippines, Prevention, Support Vector Machines}, abstract = {This study examines how machine learning (ML) techniques are applied in the Philippine setting to identify credit card fraud. This research aims to provide insights into the effectiveness of ML techniques in fraud detection, focusing on customizing ML algorithms to the distinct patterns and dynamics of credit card fraud in the Philippines, considering the nation's unique economic, technological, and social milieu. The research assesses the efficacy of different machine learning (ML) models using available data on credit card fraud occurrences and suggests improving fraud detection systems in Philippine financial institutions through ML integration. It also examines the opportunities and difficulties of using ML-driven fraud detection techniques in the Philippine financial industry.} }
@article{10.1145/3771766, title = {Saga++: A Scalable Framework for Optimizing Data Cleaning Pipelines for Machine Learning Applications}, journal = {ACM Trans. Database Syst.}, year = {2025}, issn = {0362-5915}, doi = {10.1145/3771766}, url = {https://doi.org/10.1145/3771766}, author = {Siddiqi, Shafaq and Phani, Arnab and Kern, Roman and Boehm, Matthias}, keywords = {Data Cleaning for ML, Linear-algebra-based Primitives, Data Cleaning Pipelines, Evolutionary Algorithms, Hyper-parameter Tuning, Data- and Task-parallel Execution}, abstract = {In the exploratory data science lifecycle, data scientists often spent the majority of their time finding, integrating, validating and cleaning relevant datasets. Despite recent work on data validation, and numerous error detection and correction algorithms, in practice, data cleaning for ML remains largely a manual, unpleasant, and labor-intensive trial and error process, especially in large-scale, distributed computation settings. The target ML application—such as classification or regression models—can be used as a signal of valuable feedback though, for selecting effective data cleaning strategies. In this paper, we introduce Saga++, a framework for automatically generating the top-K most effective data cleaning pipelines. Saga++ adopts ideas from Auto-ML, feature selection, and hyper-parameter tuning. Our framework is extensible for user-provided constraints, new data cleaning primitives, and ML applications; automatically generates hybrid runtime plans of local and distributed operations; and performs pruning by interesting properties (e.g., monotonicity). Furthermore, we exploit guided sampling on the input dataset to enable enumeration on a smaller subset, reducing the time required to discover the top-K pipelines. As a post-processing step, we also perform pipeline pruning on the selected top-K pipelines, removing redundant and less effective cleaning primitives. Instead of full automation—which is rather unrealistic—Saga++ simplifies the mechanical aspects of data cleaning. Our experiments show that Saga++ yields robust accuracy improvements over state-of-the-art, and good scalability regarding increasing data sizes and number of evaluated pipelines.} }
@article{10.1145/3696354, title = {Federated In-Network Machine Learning for Privacy-Preserving IoT Traffic Analysis}, journal = {ACM Trans. Internet Technol.}, volume = {24}, year = {2024}, issn = {1533-5399}, doi = {10.1145/3696354}, url = {https://doi.org/10.1145/3696354}, author = {Zang, Mingyuan and Zheng, Changgang and Koziak, Tomasz and Zilberman, Noa and Dittmann, Lars}, keywords = {In-network computing, federated learning, security, internet of things, P4}, abstract = {The expanding use of Internet-of-Things (IoT) has driven machine learning (ML)-based traffic analysis. 5G networks’ standards, requiring low-latency communications for time-critical services, pose new challenges to traffic analysis. They necessitate fast analysis and response, preventing service disruption or security impact on network infrastructure. Distributed intelligence on IoT edge has been studied to analyze traffic, but introduces delays and raises privacy concerns. Federated learning can address privacy concerns, but does not meet latency requirements. In this article, we propose FLIP4: an efficient federated learning-based framework for in-network traffic analysis. Our solution introduces a lightweight federated tree-based model, offloaded and running within network devices. FLIP4 consumes less resources than previous solutions and reduces communication overheads, making it well-suited for IoT edge traffic analysis. It ensures prompt mitigation and minimal impact on services in the presence of false alerts using two approaches (metering and dropping), thereby balancing learning accuracy and privacy requirements.} }
@inproceedings{10.1145/3625343.3625348, title = {Enhancing Phishing URL Detection: A Comparative Study of Machine Learning Algorithms}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625348}, url = {https://doi.org/10.1145/3625343.3625348}, author = {Alsarhan, Ayoub and Igried, Bashar and Bani Saleem, Raad Mohammad and Alauthman, Mohammad and Aljaidi, Mohammad}, keywords = {Cybersecurity, Machine Learning Algorithms, Phishing Detection, Server-side Analysis, URL Analysis, location = Bangkok, Thailand}, abstract = {Phishing constitutes a significant threat in the digital world, often exploiting human vulnerabilities to illicitly obtain sensitive data such as personal credentials, financial details, and private information. Misusing this information results in substantial financial loss and personal harm to victims. This study introduces an innovative approach to mitigate the risk of phishing attacks by employing machine learning algorithms to detect phishing URLs. The proposed method applies a suite of algorithms, including j48, Na\"ve Bayes, JRip, and Decision Table, to a robust dataset of 11,430 URLs, each with 87 extracted features. The results reveal the considerable potential of machine learning in identifying phishing threats. Furthermore, the study explores a novel server-side analysis concept where the server scrutinizes links transmitted via emails or social communication platforms such as WhatsApp, Messenger, and Instagram. The application of phishing detection algorithms filters and prevents the delivery of phishing links, thus reducing the potential harm to users. This research is poised to significantly contribute to cybersecurity by enhancing phishing detection mechanisms' accuracy and efficiency.} }
@article{10.1145/3674849, title = {Empirical Architecture Comparison of Two-input Machine Learning Systems for Vision Tasks}, journal = {Form. Asp. Comput.}, volume = {36}, year = {2024}, issn = {0934-5043}, doi = {10.1145/3674849}, url = {https://doi.org/10.1145/3674849}, author = {Wakigami, Kazuya and Machida, Fumio and Phung-Duc, Tuan}, keywords = {Energy consumption, machine learning system, performance, reliability, simulation}, abstract = {As machine learning models have been deployed in many vision systems, including autonomous vehicles and robots, designing architectures for machine learning systems (MLSs) has emerged as a critical concern. Previous studies have shown that enhancing the reliability of MLS outputs can be achieved by comparing multiple inference results on distinct inputs. Nevertheless, the architectures facilitating multiple inferences incur non-negligible performance overhead and energy consumption that have been less investigated. This article delves into the trade-offs among reliability, performance, and energy efficiency of architectures for two-input MLSs through real experiments conducted on image classification and object detection tasks. Specifically, we scrutinize the comparison between parallel- and shared-type architectures of two-input MLSs for vision tasks. The experiments confirm that the shared-type architecture can achieve a shorter response time and smaller energy consumption by using a shared machine learning module for both image classification and object detection tasks. However, the parallel-type architecture can benefit the redundant machine learning modules for improving throughput and fault tolerance. Our empirical results also show the service time distributions of image classification and object detection tasks fit well with a log-normal distribution and a mixture of the Gaussian model, respectively.} }
@inproceedings{10.1145/3676581.3676583, title = {Smart Emergency Alerting System: A Machine Learning Approach}, booktitle = {Proceedings of the 2024 2nd International Conference on Communications, Computing and Artificial Intelligence}, pages = {8--15}, year = {2024}, isbn = {9798400716898}, doi = {10.1145/3676581.3676583}, url = {https://doi.org/10.1145/3676581.3676583}, author = {Alrowaily, Majed Abdullah}, keywords = {BLE, GPS, IoT, beacons, camera analysis, deep learning, metadata, location = Jeju, Republic of Korea}, abstract = {Abstract: Recently, Saudi Arabia has hosted significant sports and technology events. Saudi Arabia has also successfully secured the bid to host Expo 2030 and declared its intention to host the FIFA World Cup in 2034. These crowds pertain to the elderly and special needs groups, which constitute the most vulnerable category in general, and they usually require some form of assistance. This study proposes a framework that integrates beacon technology with deep learning and real-time camera feed analysis to help crowds with special needs request emergency assistance by training a prioritization model for a patient based on this labeled dataset of 6962 records that consists of demographic and clinical details. In this, the random forest gives a nearly perfect classification and is the most successful model compared to the logistic regression and the SVM models. Logistic regression and the SVM model were not good with the minority classes. Once again, the model has been used by the application of that integrated application, and the result will be better in giving more priority to the medical emergency request and making a sound response. Another disadvantage could be, for example, accuracy in GPS coordinates, illumination conditions, and crowd density in indoor situations.} }
@inproceedings{10.1145/3674029.3674054, title = {Degree of Difference in Clinical Data and Imaging Based on Machine Learning and Complex Network}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {153--157}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674054}, url = {https://doi.org/10.1145/3674029.3674054}, author = {Kong, Guanqing and Li, Xiuxu and Wu, Chuanfu and Zhang, Lanhua}, keywords = {Complex network, Data driven, Machine learning, Multimodal imaging, location = Oslo, Norway}, abstract = {Clinical patients should have corresponding clinical indicators to characterize the disease. Multimodal data and physiological indicators provide a basis for patient diagnosis and assessment, but small sample data pose statistical difficulties. In order to better support the clinical conclusions, from a data-driven perspective, using machine learning algorithms, we explored the support of physiological indicator data for multimodal data in the case of insufficient samples, and according to the results of the model, it is shown that the data-driven results can better support the final conclusions, and therefore, integrating the multimodal data and the clinical indicators can better provide the diagnosis and assessment conclusions for the clinical patients.} }
@inproceedings{10.1145/3735014.3735893, title = {Risk Prediction of Container Cargo Loss and Damage Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Big Data Mining and Information Processing}, pages = {168--172}, year = {2025}, isbn = {9798400710407}, doi = {10.1145/3735014.3735893}, url = {https://doi.org/10.1145/3735014.3735893}, author = {Ting, Nong}, keywords = {Container Cargo, Gradient Boosting Decision Tree, Loss and Damage, Machine Learning, Random Forest}, abstract = {With the rapid development of globalization, container cargo transportation occupies a core position in international trade, but the loss and damage of its goods have brought huge economic losses and reputation risks to both sides of the trade. To meet this challenge, this paper proposes a fusion model combining Random Forest (RF) and Gradient Boosting Decision Tree (GBDT) to predict the risk of loss and damage of container goods during transportation. Firstly, through collecting about 15,000 cargo transportation records of a company in the last three years, covering multi-dimensional data such as cargo attributes, transportation environment and whether it is lost or damaged, and cleaning and preprocessing the data. Subsequently, an integrated learning model based on RF and GBDT is constructed, and the model is trained and optimized by cross-validation and feature selection methods. The experimental results show that the fusion model is superior to the single RF and GBDT models in accuracy, precision, recall, and F1 score, with an accuracy of about 90\%, which shows high prediction accuracy and robustness. The research in this paper not only verifies the effectiveness of machine learning in the risk prediction of container cargo loss and damage but also puts forward a strategy to improve prediction accuracy through model fusion. In addition, according to the experimental results and the analysis of the importance of characteristics, this paper puts forward a series of measures to reduce the risk of goods transportation, such as strengthening the classification of goods and customizing protection measures, monitoring key environmental factors, optimizing transportation routes, etc., which provides scientific basis and practical suggestions for the risk management of goods transportation industry.} }
@inproceedings{10.1145/3697467.3697604, title = {Construction of a News Event Influence Assessment Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {76--79}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697604}, url = {https://doi.org/10.1145/3697467.3697604}, author = {Jiang, Junnan}, keywords = {Deep Learning, Multimodal Feature Fusion, News Event Influence}, abstract = {This study proposes a deep learning-based model for assessing the influence of news events by integrating multimodal features such as text, time series, and social networks to construct a comprehensive influence score metric. The model utilizes Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) to process feature data, and incorporates attention mechanisms to optimize feature weights. Experimental results demonstrate that the model performs excellently in short-term predictions, with a Mean Squared Error (MSE) of 0.022 and a Mean Absolute Error (MAE) of 0.015. However, the model's performance in long-term predictions is less satisfactory, with higher MSE and MAE, indicating that there is room for improvement in long-term prediction accuracy.} }
@inproceedings{10.1145/3594806.3596591, title = {Predicting Adverse Childhood Experiences via Machine Learning Ensembles}, booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {773--779}, year = {2023}, isbn = {9798400700699}, doi = {10.1145/3594806.3596591}, url = {https://doi.org/10.1145/3594806.3596591}, author = {K Rao, Akash and Y Trivedi, Gunjan and Bajpai, Anshika and Singh Chouhan, Gajraj and G Trivedi, Riri and Kumar, Anita and Dutt, Varun and Soundappan, Kathirvel and Ramani, Hemalatha}, keywords = {Adverse Childhood Experiences, Childhood trauma, Depression, Insomnia, Machine learning, Random Forest, Suicidal Behavior, location = Corfu, Greece}, abstract = {Adverse Childhood Experiences (ACEs) have been linked to negative health outcomes later in life, including depression, anxiety, insomnia, and suicidal behavior. Recent studies have explored machine learning methods to classify individuals based on their ACE scores and predict their mental health outcomes. However, an extensive prediction of ACE via novel machine-learning ensembles based on several measures is yet to be undertaken. In this study, we used machine learning algorithms to classify individuals into high and low ACE groups and predict their mental health outcomes using various measures, including the Major Depressive Inventory, Generalized Anxiety Disorder, Insomnia Severity Index, World Health Organization Well-Being Index (WHO-5), suicide behavior, irrational decisions, self-harm, ability to focus, and suicidal thoughts. The study results showed that novel machine learning ensemble algorithms like a support-vector-decision tree ensemble and a support-vector-decision tree-random forest ensemble could accurately classify individuals into high and low ACE groups and predict their mental health outcomes. The study highlights the potential of using machine learning methods to identify individuals at high risk for mental health issues and provide targeted interventions to prevent the long-term negative consequences of ACEs.} }
@article{10.1145/3655635, title = {Machine Learning in Computer Security is Difficult to Fix}, journal = {Commun. ACM}, volume = {67}, pages = {103}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3655635}, url = {https://doi.org/10.1145/3655635}, author = {Biggio, Battista} }
@inproceedings{10.1145/3697090.3699798, title = {Comparison of Machine Learning Algorithms for Detecting Software Aging in SQL Server}, booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing}, pages = {159--164}, year = {2024}, isbn = {9798400717406}, doi = {10.1145/3697090.3699798}, url = {https://doi.org/10.1145/3697090.3699798}, author = {Nascimento, Maria Gizele and Moura, Rafael Jos\'e and Machida, Fumio and Andrade, Ermeson}, keywords = {Software Aging Detection, Machine Learning, Memory Exhaustion.}, abstract = {Software aging is a phenomenon characterized by the progressive degradation of system performance, resulting from the accumulation of internal erros, such as memory leaks and resource exhaustion. Efficient detection of this process is essential to prevent critical failures in production environments. Although several studies use Machine Learning (ML) algorithms to detect software aging, systematic comparison between these algorithms is still limited, especially in terms of their ability to predict resource exhaustion. This paper aims to fill this gap by comparing ML algorithms for detecting software aging, focusing on RAM memory exhaustion as the main indicator. The analysis was conducted using a dataset on RAM memory usage in SQL Server, applying the algorithms K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF). The performance of the models was evaluated using the metrics Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and coefficient of determination (R2). Based on these indicators, it was possible to identify the most accurate algorithm and predict the time until memory exhaustion.} }
@inproceedings{10.1145/3706594.3726967, title = {HEEPstor: an Open-Hardware Co-design Framework for Quantized Machine Learning at the Edge}, booktitle = {Proceedings of the 22nd ACM International Conference on Computing Frontiers: Workshops and Special Sessions}, pages = {22--25}, year = {2025}, isbn = {9798400713934}, doi = {10.1145/3706594.3726967}, url = {https://doi.org/10.1145/3706594.3726967}, author = {Palacios, Pedro and Medina, Rafael and Ansaloni, Giovanni and Atienza, David}, keywords = {Machine learning framework, hardware-software co-design, open-hardware, RISC-V, edge AI.}, abstract = {Edge-AI applications necessitate the joint application of hardware acceleration and software optimization to meet energy and area constraints. However, the co-design of these systems is hindered by the lack of integration options for novel hardware prototypes offered by commonly employed Machine Learning frameworks. Bridging this gap, we present HEEPstor, an open-hardware co-design framework that enables the deployment of quantized, PyTorch-defined models on heterogeneous RISC-V systems interfacing custom accelerators. We demonstrate its application by executing quantized ML models for image classification on a X-HEEP platform integrating tailored systolic arrays, showcasing its flexibility for hardware-software explorations.} }
@inproceedings{10.1145/3735452.3735538, title = {Multi-level Machine Learning-Guided Autotuning for Efficient Code Generation on a Deep Learning Accelerator}, booktitle = {Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems}, pages = {134--145}, year = {2025}, isbn = {9798400719219}, doi = {10.1145/3735452.3735538}, url = {https://doi.org/10.1145/3735452.3735538}, author = {Cha, JooHyoung and Lee, Munyoung and Kwon, Jinse and Lee, Jemin and Kwon, Yongin}, keywords = {Auto-tuning, Deep learning accelerator, Hardware-aware optimization, Machine learning for systems, Performance prediction, location = Seoul, Republic of Korea}, abstract = {The growing complexity of deep learning models necessitates specialized hardware and software optimizations, particularly for deep learning accelerators. While machine learning-based autotuning methods have emerged as a promising solution to reduce manual effort, both template-based and template-free approaches suffer from prolonged tuning times due to the profiling of invalid configurations, which may result in runtime errors. To address this issue, we propose ML2Tuner, a multi-level machine learning-guided autotuning technique designed to improve efficiency and robustness. ML2Tuner introduces two key ideas: (1) a validity prediction model to filter out invalid configurations prior to profiling, and (2) an advanced performance prediction model that leverages hidden features extracted during the compilation process. Experimental results on an extended VTA accelerator demonstrate that ML2Tuner achieves equivalent performance improvements using only 12.3\% of the samples required by a TVM-like approach and reduces invalid profiling attempts by an average of 60.8\%, highlighting its potential to enhance autotuning performance by filtering out invalid configurations.} }
@inproceedings{10.1145/3586209.3591395, title = {Machine Learning-Based Jamming Detection and Classification in Wireless Networks}, booktitle = {Proceedings of the 2023 ACM Workshop on Wireless Security and Machine Learning}, pages = {39--44}, year = {2023}, isbn = {9798400701337}, doi = {10.1145/3586209.3591395}, url = {https://doi.org/10.1145/3586209.3591395}, author = {Testi, Enrico and Arcangeloni, Luca and Giorgetti, Andrea}, keywords = {internet of things, jammer classification, jamming detection, machine learning, privacy preservation, location = Guildford, United Kingdom}, abstract = {The development of novel tools to detect, classify and counteract the new generation of smart jammers in Internet of Things (IoT) is of paramount importance. Detection and classification have to be performed in a short time, with high reliability, and preserving the privacy of network users. In this work, we propose a novel machine learning (ML)-based jamming detection and classification algorithm which can be implemented in the network gateway (GW). The proposed method is based on energy detector (ED), the extraction of specific problem-tailored features, dimensionality reduction, and multi-class classification. Extensive numerical results have been carried out to evaluate the performance of detection and classification, varying the number of principal components selected through dimensionality reduction, the observation window length, the shadowing intensity, and the signal-to-jammer ratio (SJR). Our solution reaches remarkably high accuracy, i.e., up to 99\%, outperforming a state-of-the-art solution. That is a very promising result considering that the approach does not need to inspect the decoded information, thus preserving the privacy of the network users.} }
@inproceedings{10.1145/3767052.3767054, title = {Machine Learning Models for Bank Customer Churn Prediction: A Comparative Study of LightGBM, CatBoost, and XGBoost}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {6--16}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767054}, url = {https://doi.org/10.1145/3767052.3767054}, author = {Zhang, Hanyuan and Wang, Yankai and Li, Zexuan and Wang, Xiaoyin}, keywords = {bank customer churn prediction, exploratory data analysis, machine learning}, abstract = {This research is dedicated to forecasting bank customer churn by using machine learning methods. In the highly competitive landscape of the banking industry, retaining customers has emerged as a critical factor for a bank's success. Eleven distinct machine learning models were utilized in the analysis of a bank customer dataset. Prior to model implementation, comprehensive data preprocessing was conducted, encompassing the management of missing values, encoding of categorical variables, and detection of outliers. The models were then evaluated under various data split ratios and sampling methods, with recall serving as the primary evaluation metric.The study's findings indicate that LightGBM, CatBoost, and XGBoost demonstrate superior performance in predicting customer churn. Additionally, Gradient Boost, Ada Boost, and Bagging prove to be effective in handling imbalanced data. Several key predictors of customer churn, such as Total_Trans_Amt and Total Trans ct, were identified. These results offer valuable practical implications for banks in formulating customer retention strategies. However, it is necessary to recognize the research's limitations. There may be deficiencies in data representation. What's more, external factors that could influence customer churn have been excluded.} }
@inproceedings{10.1145/3726122.3726142, title = {The Impact of AI and Machine Learning on E commerce Personalization}, booktitle = {Proceedings of the 8th International Conference on Future Networks \&amp; Distributed Systems}, pages = {115--128}, year = {2025}, isbn = {9798400711701}, doi = {10.1145/3726122.3726142}, url = {https://doi.org/10.1145/3726122.3726142}, author = {Khamdamov, Shoh Jakhon and Shahbaz, Muhammad and Mamadiyarov, Zokir and Usmanov, Anvar and Xonturayev, Bobur and Rashidov, Sharofjon and Izzatillayev, Alisher}, abstract = {The rapid evolution of e-commerce has propelled personalization to the forefront of digital marketing strategies. This study investigates the transformative impact of Artificial Intelligence (AI) and Machine Learning (ML) on e-commerce personalization, examining their effects on customer engagement, sales performance, and long-term market dynamics. We conducted a comprehensive econometric analysis using panel data from diverse e-commerce platforms over multiple years. Employing difference-in-differences models and instrumental variable approaches, we isolated the specific impact of AI and ML-driven personalization on key performance metrics. Our research encompassed various product categories and market segments to assess heterogeneous effects across the e-commerce landscape. The implementation of AI and ML-driven personalization strategies led to statistically significant increases in conversion rates (10-15\%) and customer lifetime value (20-30\%). These insights have significant implications for e-commerce strategy, investment decisions, and regulatory frameworks in an increasingly AI-driven economy. This research contributes to the growing body of literature on AI economics, providing empirical evidence of its impact in e-commerce. It offers valuable guidance for practitioners in optimizing personalization strategies and informs policy discussions surrounding digital market regulation, data privacy, and technological competition.} }
@inproceedings{10.1145/3627673.3679218, title = {Introducing CausalBench: A Flexible Benchmark Framework for Causal Analysis and Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5220--5224}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679218}, url = {https://doi.org/10.1145/3627673.3679218}, author = {Kapkic, Ahmet and Mandal, Pratanu and Wan, Shu and Sheth, Paras and Gorantla, Abhinav and Choi, Yoonhyuk and Liu, Huan and Candan, K. Selcuk}, keywords = {benchmark, causality, dataset, machine learning, metric, model, location = Boise, ID, USA}, abstract = {While witnessing the exceptional success of machine learning (ML) technologies in many applications, users are starting to notice a critical shortcoming of ML: correlation is a poor substitute for causation. The conventional way to discover causal relationships is to use randomized controlled experiments (RCT); in many situations, however, these are impractical or sometimes unethical. Causal learning from observational data offers a promising alternative. While being relatively recent, causal learning aims to go far beyond conventional machine learning, yet several major challenges remain. Unfortunately, advances are hampered due to the lack of unified benchmark datasets, algorithms, metrics, and evaluation service interfaces for causal learning. In this paper, we introduce CausalBench, a transparent, fair, and easy-to-use evaluation platform, aiming to (a) enable the advancement of research in causal learning by facilitating scientific collaboration in novel algorithms, datasets, and metrics and (b) promote scientific objectivity, reproducibility, fairness, and awareness of bias in causal learning research. CausalBench provides services for benchmarking data, algorithms, models, and metrics, impacting the needs of a broad of scientific and engineering disciplines.} }
@inproceedings{10.1145/3675417.3675432, title = {Machine Learning in the Chinese Corporate Bond Market}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {84--90}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675432}, url = {https://doi.org/10.1145/3675417.3675432}, author = {Geng, Yiyang}, abstract = {Research on the factors influencing corporate bond yields has consistently been a focal point in the financial field. However, there is currently insufficient attention directed toward Chinese corporate bond yields, especially neglecting the consideration of the nonlinear and interactive relationships between variables and yields. This paper utilizes seven machine learning algorithms, including random forest and feed-forward neural networks, to compare and analyze the predictive effectiveness of each method on China's corporate bond yields. Additionally, it evaluates the importance of feature variables and variable combinations. The findings indicate that these methods demonstrate certain applicability in predicting Chinese corporate bond yields, revealing distinct nonlinear and interactive effects between these yields and the feature variables. Liquidity risk and downside risk emerge as the most critical predictive factors, highlighting the heightened sensitivity of market participants to both liquidity conditions and extreme risk scenarios. Notably, within the portfolio of liquidity risk, Amihud and ILLIQ, as well as CVaR10 and ES10 within the downside risk portfolio, along with the timing of bond issuance, emerge as the feature variables making the most significant predictive contributions.} }
@inproceedings{10.1145/3639478.3639797, title = {Enhancing Model-Driven Reverse Engineering Using Machine Learning}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {173--175}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3639797}, url = {https://doi.org/10.1145/3639478.3639797}, author = {Siala, Hanan Abdulwahab}, keywords = {application programs, model driven reverse engineering (MDRE), unified modeling language (UML), object constraint language (OCL), machine learning, large language models (LLMS), program comprehension, location = Lisbon, Portugal}, abstract = {Organizations often rely on large applications that are classified as legacy systems due to their dependence on outdated programming languages or platforms. To modernize these systems, it is necessary to understand their architecture, functionality, and business rules. Our research aims to define a novel model-driven reverse engineering (MDRE) approach to extract Unified Modeling Language (UML) and Object Constraint Language (OCL) representations from source code using Large Language Models (LLMs).} }
@inproceedings{10.1145/3723936.3723959, title = {Analysis of Momentum in Tennis Matches Based on Machine Learning Models}, booktitle = {Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis}, pages = {148--155}, year = {2025}, isbn = {9798400712234}, doi = {10.1145/3723936.3723959}, url = {https://doi.org/10.1145/3723936.3723959}, author = {Lin, Shengbo and Wu, Bingheng and Yuan, Feijie and Wang, Yuqi and Wen, Quan and Zhang, Puzhao}, keywords = {Binary Logistic Regression, LightGBM, Momentum, Tennis Matches, XGBoost}, abstract = {Momentum, as one of the critical factors affecting the outcomes of sports competitions, has long posed challenges in its quantification and mechanism of action. This study focuses on Wimbledon tennis matches, constructing and validating a multitask analytical framework to systematically explore momentum and its impact on match outcomes. Firstly, the study filters and normalizes predictive indicators affecting point wins and losses from Wimbledon match data. By combining binary logistic regression with various machine learning model optimizations, it designs an optimal model to capture scoring dynamics, enabling dynamic visual analysis of match processes. Secondly, momentum decomposition analysis is conducted based on data training results, and the potential influence of momentum on match outcomes is verified using the Kruskal-Wallis test (P = 0.27). The study further validates the model using data from the 2023 Wimbledon Men's Singles Final, achieving a predictive accuracy of 72\% through aggregate processing and the Hosmer-Lemeshow test. A LightGBM model is employed to visualize momentum transitions, while analysis of variance (ANOVA) identifies key factors affecting momentum changes. Finally, the model's performance and potential error sources are evaluated using PRC curves and Pearson analysis. Comparative analyses across different sports validate the model's generalizability and cross-discipline applicability. The findings provide novel insights into the quantification of momentum and its application in match prediction, demonstrating the model's extensive potential for dynamic performance analysis in sports.} }
@inproceedings{10.1145/3647444.3647923, title = {Analysis on Machine Learning Strategies for Carcinoma Detection Biomarker}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647923}, url = {https://doi.org/10.1145/3647444.3647923}, author = {Verma, Shairal and Verma, Prem Kumari and Singh, Nagendra Pratap}, keywords = {Biomarkers, and carcinoma, machine learning, location = Jaipur, India}, abstract = {Biomarkers are substances that are identifiable and can potentially be used to show if a disease is present or is progressing. Biomarkers have the potential to diagnose cancer, forecast its course, and direct decisions regarding treatment. It has enabled the discovery of genes, plasma metabolites, and miRNA biomarkers for various cancers. A potent method for finding and analyzing biomarkers in data sets is machine learning. A component of artificial intelligence, machine learning is still a crucial and significant step in the diagnosis of several illnesses in the human body. In this rapidly expanding research, an increasing number of medical professionals are depending on artificial intelligence to identify and diagnose illnesses inside the body. The only issue is that they are striving to improve both precision and accuracy. Thus, we have provided an overview of the technologies used in the case of biomarkers used to detect cancer in the body in this article. This article will provide an overview of the work completed so far and assist future researchers in discovering new avenues for exploration within the specific research areas they cover.} }
@inproceedings{10.1145/3712335.3712431, title = {Research on the application of machine learning in corporate public opinion monitoring and management}, booktitle = {Proceedings of the 3rd International Conference on Signal Processing, Computer Networks and Communications}, pages = {557--562}, year = {2025}, isbn = {9798400710834}, doi = {10.1145/3712335.3712431}, url = {https://doi.org/10.1145/3712335.3712431}, author = {Ma, Xiaohui}, keywords = {machine learning, natural language processing, public opinion monitoring, sentiment analysis}, abstract = {This article makes a point of the overall application of corporate public opinion monitoring and management technology based on machine learning. This work proves that by means of case analysis, the basic theories and methods of machine learning, it elaborates on how to use effective machine learning for large-scale public opinion data collection, processing, and analysis, and prediction. Considerable attention is paid to the way natural language processing techniques and sentiment analysis are applied in processing text data and how the models of machine learning, among other support vector machines, decision trees, and neural networks, bear upon implementation in real enterprise environments. Research results show that the level of efficiency and prediction accuracy will be greatly improved if the machine learning technique is applied during public opinion data processing, therefore effectively optimizing the strategy of public opinion response and management of the company. The effectiveness and application scenarios of each prediction model in public opinion analysis were verified through comparative analysis with multiple prediction models, which provided the scientific data support for the decision-making reference of the company.} }
@article{10.1145/3545574, title = {The Role of Machine Learning in Cybersecurity}, journal = {Digital Threats}, volume = {4}, year = {2023}, doi = {10.1145/3545574}, url = {https://doi.org/10.1145/3545574}, author = {Apruzzese, Giovanni and Laskov, Pavel and Montes de Oca, Edgardo and Mallouli, Wissam and Brdalo Rapa, Luis and Grammatopoulos, Athanasios Vasileios and Di Franco, Fabio}, keywords = {Cybersecurity, incident detection, machine learning, artificial intelligence}, abstract = {Machine Learning (ML) represents a pivotal technology for current and future information systems, and many domains already leverage the capabilities of ML. However, deployment of ML in cybersecurity is still at an early stage, revealing a significant discrepancy between research and practice. Such a discrepancy has its root cause in the current state of the art, which does not allow us to identify the role of ML in cybersecurity. The full potential of ML will never be unleashed unless its pros and cons are understood by a broad audience.This article is the first attempt to provide a holistic understanding of the role of ML in the entire cybersecurity domain—to any potential reader with an interest in this topic. We highlight the advantages of ML with respect to human-driven detection methods, as well as the additional tasks that can be addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic problems affecting real ML deployments in cybersecurity. Finally, we present how various stakeholders can contribute to future developments of ML in cybersecurity, which is essential for further progress in this field. Our contributions are complemented with two real case studies describing industrial applications of ML as defense against cyber-threats.} }
@inproceedings{10.1145/3713043.3728853, title = {"It’s Just a Machine that Predicts" - Demystifying Artificial Intelligence / Machine Learning with Teenagers}, booktitle = {Proceedings of the 24th Interaction Design and Children}, pages = {168--182}, year = {2025}, isbn = {9798400714733}, doi = {10.1145/3713043.3728853}, url = {https://doi.org/10.1145/3713043.3728853}, author = {Klemettil\"a, Pauli Aleksi and Sharma, Sumita and Mochiyama, Fumika and Iivari, Netta and Iwata, Megumi and Koivisto, Jussi}, keywords = {AI/ML Literacy, Youth Perceptions, Cross-cultural studies}, abstract = {Teenagers today face an expanding and unpredictable role of Artificial Intelligence (AI) in society, yet educational interventions are still catching up. AI literacy is crucial for teenagers (and others) to help them make informed decisions about their futures. We present our work on Artificial Intelligence / Machine Learning (AI/ML) literacy with 43 Japanese and 20 Finnish high school students, exploring their pre-existing perceptions of AI/ML and how those perceptions evolved after they trained image classification models. Our findings indicate that while a substantial number of teenagers still have limited or contradictory understanding of the topic, even short-term workshops can be effective in demystifying core AI/ML concepts when the activities are tailored to their interests. Furthermore, we find that cultural background and language may factor into how teenagers perceive AI. Our study contributes to growing research on AI/ML literacy by focusing on teenagers and including cross-cultural perspectives.} }
@inproceedings{10.1145/3690771.3690785, title = {Machine Learning Regression Model Development and Data Visualization of Road Accident in Urdaneta City, Pangasinan, Philippines}, booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing}, pages = {27--32}, year = {2025}, isbn = {9798400710018}, doi = {10.1145/3690771.3690785}, url = {https://doi.org/10.1145/3690771.3690785}, author = {Dorado, Danilo and Aviles, Joey}, keywords = {Data Visualization, Machine Learning, Model Development, Road Accident, Urdaneta City}, abstract = {Road accidents contribute significantly to annual fatalities. Various agencies engaged in road safety management are consistently striving for quality improvement to predict and mitigate accidents through the aid of information technology. As machine learning emerged, its data analysis, visualization, and prediction capabilities have proven to help policymakers. The main objective of this study is to develop a machine-learning model for predicting road accidents in Urdaneta City, Philippines. The dataset used consists of three years (2021–2023) of road accident records from the Emergency Medical Services (EMS). Various preprocessing methods are employed, and a grid search technique is implemented to identify optimal features for the model. Random Forest, XGBoost, AdaBoost, Decision Tree, and LGBM algorithms are developed to predict and visualize road accident occurrences. Upon development, it was found that XGBoost performs well among other models, acquiring a total Rsquared of 0.9996 in training and 0.9898 during testing. To validate the findings of the study, the model's interpretability was assessed using a William Plot. The SHAP and LIME methodologies offer valuable insights into how various variables affect the model's forecast of the monthly number of road accidents. The Permutation Feature Importance approach provides insight into the specific impact of each feature on the model's predictions, emphasizing the significant influence of time on accident frequency, particularly during peak hours or late-night accidents. The Box Plot offers valuable insights, with the median (Q2) indicated by a line showing a value of around 10. Leveraging this model can significantly reduce the likelihood of accidents in real-time, resulting in a safer and more intelligent transportation ecosystem.} }
@inproceedings{10.1145/3613904.3642628, title = {Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference}, booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703300}, doi = {10.1145/3613904.3642628}, url = {https://doi.org/10.1145/3613904.3642628}, author = {Hohman, Fred and Wang, Chaoqun and Lee, Jinmook and G\"ortler, Jochen and Moritz, Dominik and Bigham, Jeffrey P and Ren, Zhile and Foret, Cecile and Shan, Qi and Zhang, Xiaoyi}, keywords = {Efficient machine learning, interactive systems, model compression, on-device machine learning, visual analytics, location = Honolulu, HI, USA}, abstract = {On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria : a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.} }
@inproceedings{10.1145/3650400.3650563, title = {Exploring Heart Disease Prediction through Machine Learning Techniques}, booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering}, pages = {964--969}, year = {2024}, isbn = {9798400708305}, doi = {10.1145/3650400.3650563}, url = {https://doi.org/10.1145/3650400.3650563}, author = {Lin, Zhicong and Chen, Shujing and Chen, Jichang}, abstract = {Currently, heart disease stands as the most formidable threat to human life. The application of machine learning in scrutinizing data holds the promise of augmenting early detection and prevention strategies for this ailment. Within this study, a suite of six distinctive and classical machine learning models—Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbor, Support Vector Classifier, and Neural Network—are introduced and meticulously evaluated. These models leverage data gathered from heart patients across four distinct regions, contributing to a comprehensive assessment. The investigative process encompasses five pivotal stages: data collection, preprocessing, K-Means clustering, application of classification algorithms models for heart disease prediction, and rigorous evaluation. At the end of the study, a comprehensive summary of heart disease prediction was presented.} }
@inproceedings{10.1145/3745812.3745853, title = {Diagnosis of cardiac diseases using spectral features and machine learning}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745853}, url = {https://doi.org/10.1145/3745812.3745853}, author = {Tiwari, Shashi Kant and Sinha, Shweta and Sekhon, Karamjit Kaur and Singh, Ram Sewak}, keywords = {Congestive Heart Failure (CHF), Generalized Discriminant Analysis (GDA), Heart Rate Variability (HRV), coronary artery disease (CAD)}, abstract = {This study presents combination of spectral analysis method, generalized discriminant analysis (GDA), and the extreme learning machine (LELM) classifier algorithms which provide a robust method for prediction of cardiac diseases. Firstly, spectral approaches, has been applied for features extraction from heart rate variability (HRV) signal. The proposed GDA +ELM model was fitted with the features derived from approaches for the classification model of congestive heart disease and coronary artery disease. The standard database of HRV signal was collected from Physio net ATM's HRV source for training and validating predictive models for cardiac diseases.} }
@article{10.1145/3687267, title = {Understanding the performance of machine learning models from data- to patient-level}, journal = {J. Data and Information Quality}, volume = {16}, year = {2024}, issn = {1936-1955}, doi = {10.1145/3687267}, url = {https://doi.org/10.1145/3687267}, author = {Valeriano, Maria Gabriela and Matran-Fernandez, Ana and Kiffer, Carlos and Lorena, Ana Carolina}, keywords = {Machine learning, Instance Hardness, Data-centric, Healthcare}, abstract = {Machine Learning (ML) models have the potential to support decision-making in healthcare by grasping complex patterns within data. However, decisions in this domain are sensitive and require active involvement of domain specialists with deep knowledge of the data. To address this task, clinicians need to understand how predictions are generated so they can provide feedback for model refinement. There is usually a gap in the communication between data scientists and domain specialists that needs to be addressed. Specifically, many ML studies are only concerned with presenting average accuracies over an entire dataset, losing valuable insights that can be obtained at a more fine-grained patient-level analysis of classification performance. In this article, we present a case study aimed at explaining the factors that contribute to specific predictions for individual patients. Our approach takes a data-centric perspective, focusing on the structure of the data and its correlation with ML model performance. We utilize the concept of Instance Hardness, which measures the level of difficulty an instance poses in being correctly classified. By selecting the hardest and easiest to classify instances, we analyze and contrast the distributions of specific input features and extract meta-features to describe each instance. Furthermore, we individually examine certain instances, offering valuable insights into why they offer challenges for classification, enabling a better understanding of both the successes and failures of the ML models. This opens up the possibility for discussions between data scientists and domain specialists, supporting collaborative decision-making.} }
@article{10.1145/3582575, title = {Estimating Software Functional Size via Machine Learning}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {32}, year = {2023}, issn = {1049-331X}, doi = {10.1145/3582575}, url = {https://doi.org/10.1145/3582575}, author = {Lavazza, Luigi and Locoro, Angela and Liu, Geng and Meli, Roberto}, keywords = {Function Points, functional size measurement, NESMA Estimated, early size estimation, Function Point Analysis, High-level FPA, simple function points, SFP, SiFP, machine learning estimation, Neural Networks, Support Vector Regression, Random Forests}, abstract = {Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods\&nbsp;for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the “High-level FPA” method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression\&nbsp;, and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data.} }
@inproceedings{10.1145/3665320.3670994, title = {Implementing a Machine Learning Deformer for CG Crowds: Our Journey}, booktitle = {Proceedings of the 2024 Digital Production Symposium}, year = {2024}, isbn = {9798400706905}, doi = {10.1145/3665320.3670994}, url = {https://doi.org/10.1145/3665320.3670994}, author = {Arcelin, Bastien and S\'ebastien, Maraux and Chaverou, Nicolas}, keywords = {Animation, Crowds, Neural Networks, Rigging, location = Denver, CO, USA}, abstract = {CG crowds have become increasingly popular this last decade in the VFX and animation industry: formerly reserved to only a few high end studios and blockbusters, they are now widely used in TV shows or commercials. Yet, there is still one major limitation: in order to be ingested properly in crowd software, studio rigs have to comply with specific prerequisites, especially in terms of deformations. Usually only skinning, blend shapes and geometry caches are supported preventing close-up shots with facial performances on crowd characters. We envisioned two approaches to tackle this: either reverse engineer the hundreds of deformer nodes available in the major DCCs/plugins and incorporate them in our crowd package, or surf the machine learning wave to compress the deformations of a rig using a neural network architecture. Considering we could not commit 5+ man/years of development into this problem, and that we were excited to dip our toes in the machine learning pool, we went for the latter. From our first tests to a minimum viable product, we went through hopes and disappointments: we hit multiple pitfalls, took false shortcuts and dead ends before reaching our destination. With this paper, we hope to provide a valuable feedback by sharing the lessons we learnt from this experience.} }
@inproceedings{10.1145/3701100.3701104, title = {A Tennis Momentum Analysis Method Based on Gaussian Dynamics and Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology}, pages = {13--17}, year = {2025}, isbn = {9798400718120}, doi = {10.1145/3701100.3701104}, url = {https://doi.org/10.1145/3701100.3701104}, author = {Long, Zheng and Li, Na and Sun, Mengqing and Luo, Jiawen and Pan, Ting and Yin, Yujie and Yang, Xinhua}, keywords = {Gaussian dynamics model, Machine learning, Tennis match momentum analysis, prediction accuracy}, abstract = {Momentum analysis in tennis matches has predominantly focused on qualitative methods, lacking systematic quantitative approaches. This study proposes a novel method that integrates Gaussian dynamics models with machine learning techniques to quantitatively analyze and predict momentum changes in tennis matches. By selecting and defining key indicators such as serve success rate and scoring rate, we developed an algorithm to identify and predict momentum shifts. The proposed method employs ensemble learning techniques, specifically Stacking, to combine multiple machine learning models, enhancing prediction accuracy and execution efficiency. Experimental validation using data from major tennis tournaments, including the 2023 Wimbledon Championships, demonstrates the effectiveness of the proposed approach. The results show that the Stacking model outperforms individual models in accuracy and robustness, providing scientific decision support for coaches and players. This method has low computational requirements, is simple to implement, and considers a wide range of variables, making it highly efficient. Future research will focus on further optimizing the model and applying it to other sports and match types.} }
@inproceedings{10.1145/3605764.3623905, title = {Information Leakage from Data Updates in Machine Learning Models}, booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security}, pages = {35--41}, year = {2023}, isbn = {9798400702600}, doi = {10.1145/3605764.3623905}, url = {https://doi.org/10.1145/3605764.3623905}, author = {Hui, Tian and Farokhi, Farhad and Ohrimenko, Olga}, keywords = {attribute inference, data update, machine learning, privacy, location = Copenhagen, Denmark}, abstract = {In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the updated model. Moreover, we observe that data records with rare values are more vulnerable to attacks, which points to the disparate vulnerability of privacy attacks in the update setting. When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model. These observations point to vulnerability of machine learning models to attribute inference attacks in the update setting.} }
@inproceedings{10.1145/3638782.3638790, title = {Tackling Disinformation: Machine Learning Solutions for Fake News Detection}, booktitle = {Proceedings of the 2023 13th International Conference on Communication and Network Security}, pages = {46--51}, year = {2024}, isbn = {9798400707964}, doi = {10.1145/3638782.3638790}, url = {https://doi.org/10.1145/3638782.3638790}, author = {Sangi, Abdur Rashid and Nagaram, Jyothish and Sudulagunta, Akshara and Talari, Sai Sandeep and Malla, Vinay and Enduri, Muralikrishna and Anamalamudi, Satish}, keywords = {Fake news, decision tree, machine learning, medical issues, naive bayes, performance measure, political issues, location = Fuzhou, China}, abstract = {Fake news is termed as the news that spreads via the internet very fast which is not true i.e., false news. Since we are in a society of modern living culture, we will be attracted to the trend easily. So, taking this as an advantage some business traders make this news as their profit by clicking on that fake news. We can observe these types of issues in areas like Political issues, medical issues, Job rackets, etc. The tremendous increase in the spreading of fake news may result in less hope for real news. The main goal is to create a resilient and effective system with the ability to automatically differentiate between authentic and falsified news articles. We can find whether the news is fake or real through machine learning algorithms with greater methodology. We have selected and implemented a few datasets using machine learning algorithms like Decision Tree, Naive Bayes, SVM, Random Forest, Logistic Regression, and Passive aggressive classifier. Further, we come up with the algorithm which gives the highest performance measures. The results from the experiments exhibit encouraging performance metrics in identifying fake news, highlighting machine learning’s potential in countering misinformation. The outcomes imply that blending various feature types and advanced algorithms leads to better performance when contrasted with individual methods. We have applied these ML methods on two datasets and achieved accuracy of 99.69\% with SVM, 99.06\% with Logistic Regression and 99.64} }
@inproceedings{10.1145/3655497.3655515, title = {Machine Learning-Based Web Application for ADHD Detection in Children}, booktitle = {Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence}, pages = {92--98}, year = {2024}, isbn = {9798400709302}, doi = {10.1145/3655497.3655515}, url = {https://doi.org/10.1145/3655497.3655515}, author = {Porras, Diego Oscar Alexander and Mejia, Gerson Antonio and Casta\~neda, Pedro Segundo}, keywords = {ADHD detection, Child mental health, Computing methodologies, Machine learning, location = Tokyo, Japan}, abstract = {Attention deficit hyperactivity disorder (ADHD) represents a medical condition characterized by the presence of inattention, hyperactivity, and impulsivity, which affects the academic development of students globally. In Peru, it affects a proportion of the pediatric population ranging from 2\% to 12\%, with a prevalence of 12.1\% in South Lima, particularly in public schools. This research presents an online application with machine learning to improve the detection of ADHD in elementary school children. Several machine learning algorithms were reviewed and Random Forest was selected as the best-performing model with an accuracy of 96.08\%. The model uses 27 selected variables, optimizing data collection and training. The child answers the questionnaire within the app and psychologists can access the app to visualize the results, aiding in the early detection of ADHD. The experiment involved 189 participants, resulting in a high accuracy of the Random Forest model. This innovative solution can have a significant impact on the early identification of ADHD, benefiting children's health and educational process.} }
@inproceedings{10.1145/3583780.3614786, title = {Automatic and Precise Data Validation for Machine Learning}, booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, pages = {2198--2207}, year = {2023}, isbn = {9798400701245}, doi = {10.1145/3583780.3614786}, url = {https://doi.org/10.1145/3583780.3614786}, author = {Shankar, Shreya and Fawaz, Labib and Gyllstrom, Karl and Parameswaran, Aditya}, keywords = {data validation, machine learning, location = Birmingham, United Kingdom}, abstract = {Machine learning (ML) models in production pipelines are frequently retrained on the latest partitions of large, continually- growing datasets. Due to engineering bugs, partitions in such datasets almost always have some corrupted features; thus, it's critical to find data issues and block retraining before downstream ML accuracy decreases. However, current ML data validation methods are difficult to operationalize: they yield too many false positive alerts, require manual tuning, or are infeasible at scale. In this pa- per, we present an automatic, precise, and scalable data validation system for ML pipelines, employing a simple idea that we call a Partition Summarization (PS) approach to data validation: each timestamp-based partition of data is summarized with data quality metrics, and summaries are compared to detect corrupted partitions. We demonstrate how to adapt PS for any data validation method in a robust manner and evaluate several adaptations-which by themselves provide limited precision. Finally, we present gate, our data validation method that leverages these adaptations, giving a 2.1 average improvement in precision over the baseline from prior work on a case study within our large tech company.} }
@inproceedings{10.1145/3675249.3675313, title = {A machine learning-based diabetes risk prediction modeling study}, booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology}, pages = {363--369}, year = {2024}, isbn = {9798400718267}, doi = {10.1145/3675249.3675313}, url = {https://doi.org/10.1145/3675249.3675313}, author = {Ming, Jiexiu and Xu, Junyi and Zhang, Miaomiao and Li, Ningyu and Yan, Xu}, abstract = {Diabetes mellitus is a chronic metabolic disease, mainly characterized by insufficient insulin secretion or impaired insulin action in the body, resulting in elevated blood glucose. According to the World Health Organization (WHO), the number of diabetes patients worldwide has been on the rise in recent years, and has become an important public health problem worldwide today. In this paper, we used the Random Forest-based feature importance screening method to retain the variables with larger variable feature weights, performed Spearman correlation analysis, selected the top 10 operational variables with lower correlations, and used information entropy theory and correlation analysis to test the representativeness and independence of the main variables, and finally screened out the main variables as platelet volume distribution width, HDL cholesterol, and the proportion of white globules, platelet specific volume, platelet count, red blood cell count, lymphocyte \%, albumin, neutrophil \%, and leukocyte count. Blood glucose prediction models were established through data mining techniques, in this paper five machine learning were selected for prediction, namely Extreme Gradient Boosted Tree (XGBoost), Random Forest Regression, Support Vector Machine Regression SVR, LightGBM, Gradient Boosted Decision Tree (GBDT). The training set was put into each model for training, and the test set was inputted into the model to get the root mean squared error produced by the five models ( MSE), Mean Absolute Error (MAE), and Maximum Absolute Error (MAS), comparing the five models, in general, the Support Vector Machine regression SVR has the highest accuracy. To establish a support vector machine SVR blood sugar prediction model based on Bayesian optimization, the sample data are normalized, the parameters are initially corrected using Bayesian principles, and then the support vector machine estimation algorithm is selected to initialize the model, the parameters are inferred using the Bayesian evidence framework, and the optimal model is established after several iterations, and the support vector machine regression SVR trained using the optimal hyperparameters obtained from Bayesian optimization model has improved accuracy in all three evaluation metrics.} }
@inproceedings{10.1145/3589883.3589889, title = {HE-MAN – Homomorphically Encrypted MAchine learning with oNnx models}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {35--45}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589889}, url = {https://doi.org/10.1145/3589883.3589889}, author = {Nocker, Martin and Drexel, David and Rader, Michael and Montuoro, Alessio and Sch\"ottle, Pascal}, keywords = {Homomorphic Encryption, Machine Learning as a Service, Secure and Privacy-Preserving Machine Learning, location = Stockholm, Sweden}, abstract = {Machine learning (ML) algorithms are increasingly important for the success of products and services, especially considering the growing amount and availability of data. This also holds for areas handling sensitive data, e.g. applications processing medical data or facial images. However, people are reluctant to pass their personal sensitive data to a ML service provider. At the same time, service providers have a strong interest in protecting their intellectual property and therefore refrain from publicly sharing their ML model. Fully homomorphic encryption (FHE) is a promising technique to enable individuals using ML services without giving up privacy and protecting the ML model of service providers at the same time. Despite steady improvements, FHE is still hardly integrated in today’s ML applications. Reasons for that are, among others, that existing implementations either require the user to possess expertise in FHE, do not feature an easy ML framework integration, or have to approximate non-polynomial activations. We introduce HE-MAN, an open-source two-party machine learning toolset for privacy preserving inference with ONNX models and homomorphically encrypted data. Both the model and the input data do not have to be disclosed. HE-MAN abstracts cryptographic details away from the users, thus expertise in FHE is not required for either party. HE-MAN’s security relies on its underlying FHE schemes. For now, we integrate two different homomorphic encryption schemes, namely Concrete and TenSEAL. Compared to prior work, HE-MAN supports a broad range of ML models in ONNX format out of the box without sacrificing accuracy. We evaluate the performance of our implementation on different network architectures classifying handwritten digits and performing face recognition and report accuracy and latency of the homomorphically encrypted inference. Cryptographic parameters are automatically derived by the tools. We show that the accuracy of HE-MAN is on par with models using plaintext input while inference latency is several orders of magnitude higher compared to the plaintext case.} }
@inproceedings{10.1145/3626246.3654745, title = {PLUTUS: Understanding Data Distribution Tailoring for Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {528--531}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654745}, url = {https://doi.org/10.1145/3626246.3654745}, author = {Chang, Jiwon and Dionysio, Christina and Nargesian, Fatemeh and Boehm, Matthias}, keywords = {data acquisition, distribution tailoring, model debugging, location = Santiago AA, Chile}, abstract = {Existing data debugging tools allow users to trace model performance problems all the way to the data by efficiently identifying slices (conjunctions of features and values) for which a trained model performs significantly worse than the entire dataset. To ensure accurate and fair models, one solution is to acquire enough data for these slices. In addition to crowdsourcing, recent data acquisition techniques design cost-effective algorithms to obtain such data from a union of external sources such as data lakes and data markets. We demonstrate PLUTUS, a tool for human-in-the-loop and model-aware data acquisition pipeline, on SystemDS, as an open source ML system for the end-to-end data science lifecycle. In PLUTUS, a user can efficiently identify problematic slices, connect to external data sources, and acquire the right amount of data for these slices in a cost-effective manner.} }
@inbook{10.1145/3718491.3718545, title = {Machine Learning Based River Segmentation with GF-2 Satellite Imagery}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {325--328}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718545}, author = {Cui, Hanwen and Li, Cheng}, abstract = {Rivers constitute a crucial element of the natural environment, playing an indispensable role in ecosystem dynamics. Investigating river systems not only facilitates the sustainable management of water resources but also advances ecological preservation, alongside social and economic development. Considering escalating global challenges such as climate change, population pressures, and environmental degradation, acquiring a comprehensive understanding of river resources is imperative for addressing both current and future ecological dilemmas. This study employs high-spatial-resolution remote sensing imagery Gaofen-2 (GF-2) alongside two deep learning models: k-Nearest Neighbor (kNN) and Decision Tree (DT), to conduct river segmentation within the context of Zhuhai, a city characterized by abundant river resources. The research utilizes a 10-fold cross-validation approach and the grid search method to ascertain the optimal parameters for each model, aiming to enhance training accuracy. Comparative analysis indicates that the kNN model outperforms the DT model across various evaluation metrics and segmentation outcomes. The findings of this research contribute valuable theoretical insights and technical support for effective river management strategies, enabling governmental bodies to formulate more scientifically grounded and rational policies and measures for sustainable development.} }
@inproceedings{10.1145/3630106.3659043, title = {Understanding Disparities in Post Hoc Machine Learning Explanation}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {2374--2388}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3659043}, url = {https://doi.org/10.1145/3630106.3659043}, author = {Mhasawade, Vishwali and Rahman, Salman and Haskell-Craig, Zo\'e and Chunara, Rumi}, keywords = {explainability, fairness, post hoc explanation methods, location = Rio de Janeiro, Brazil}, abstract = {Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across “race” and “gender” as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.} }
@article{10.5555/3586589.3586678, title = {Machine learning on graphs: a model and comprehensive taxonomy}, journal = {J. Mach. Learn. Res.}, volume = {23}, year = {2022}, issn = {1532-4435}, author = {Chami, Ines and Abu-El-Haija, Sami and Perozzi, Bryan and R\'e, Christopher and Murphy, Kevin}, keywords = {network embedding, graph neural networks, geometric deep learning, manifold learning, relational learning}, abstract = {There has been a surge of recent interest in graph representation learning (GRL). GRL methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding, focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between network embedding, graph regularization and graph neural networks. We propose a comprehensive taxonomy of GRL methods, aiming to unify several disparate bodies of work. Specifically, we propose the GRAPHEDM framework, which generalizes popular algorithms for semi-supervised learning (e.g. GraphSage, GCN, GAT), and unsupervised learning (e.g. DeepWalk, node2vec) of graph representations into a single consistent approach. To illustrate the generality of GRAPHEDM, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.} }
@inproceedings{10.1145/3691573.3691582, title = {Analysis of Cybersickness through Biosignals: an approach with Symbolic Machine Learning}, booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality}, pages = {11--20}, year = {2024}, isbn = {9798400709791}, doi = {10.1145/3691573.3691582}, url = {https://doi.org/10.1145/3691573.3691582}, author = {Nunes da Silva, Wedrey and Porcino, Thiago Malheiros and Castanho, Carla Denise and Jacobi, Ricardo Pezzuol}, keywords = {Biosignals, Cybersickness, Decision Tree, HMD Devices, Random Forest., Symbolic Machine Learning, Virtual Reality, location = Manaus, Brazil}, abstract = {Cybersickness represents one of the main obstacles to the use of Virtual Reality, often triggered by the use of Head-mounted Display devices. The symptoms associated with cybersickness can vary among individuals and include nausea, dizziness, eye strain, and headache, which may persist for minutes or even hours after exposure to Virtual Reality. According to the literature, cybersickness has a considerable impact on physiological signals such as delta waves in the Electroencephalogram; Heart Rate and Heart Rate Variability, derived from the Electrocardiogram; Electrodermal Activity; and Electrogastrography, all of which show a significant correlation with this condition. In this study, we investigated the use of biosignals to identify the possible causes associated with cybersickness in Virtual Reality. Our main hypothesis is that the combination of quantitative and subjective assessments, combined with Symbolic Machine Learning techniques, is effective in creating a ranking of the main causative/indicative factors of this condition. The results of this study highlight significant contributions to the understanding of factors influencing cybersickness symptoms. Statistical analyses confirmed the relationship between physiological changes and cybersickness symptoms. By including biosignals in our model, we achieved a significant gain, with an AUC of 0.95. The rankings of the main factors, both for the model without and with the inclusion of biosignals, confirmed previous research described in the literature. To the best of our knowledge, this is the first work to employ Symbolic Machine Learning models combining data from user profiles, game, and biosignals to detect the causes of cybersickness and generate a ranking of the most relevant factors.} }
@inproceedings{10.1145/3723178.3723247, title = {Diabetes Prediction: A Comprehensive Study Integrating Deep Learning and Machine Learning Approaches}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {520--526}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723247}, url = {https://doi.org/10.1145/3723178.3723247}, author = {Z Waughfa, Mohammed and Adnan, Imranul Islam and Sultana, Syeda Samia and Mumu, Sumaiya Siddiqua}, keywords = {Diabetes prediction, machine learning, Pima Indian Diabetic dataset, classification, Random Forest, SVM, Decision Tree, MLP, KNN, Healthcare.}, abstract = {Diabetes, a prevalent chronic disease, affects a significant portion of the global population, with early detection and management being crucial for patient health. Despite the lack of a cure, advancements in machine learning (ML) and deep learning (DL) offer promising avenues for diabetes prediction. This study leverages the Pima Indian dataset to explore and compare the effectiveness of various ML and DL techniques, including Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), Random Forest, K-Nearest Neighbors (KNN), and Decision Tree. Our comprehensive evaluation, based on accuracy, precision, and recall, identified MLP as the most effective algorithm, achieving an accuracy rate of 85\%. This research addresses current challenges in diabetes prediction by highlighting the superior performance of MLP in early detection, thereby underscoring its potential in improving diabetes management. The findings contribute to the ongoing discourse in healthcare technology, advocating for the integration of advanced ML techniques to enhance predictive accuracy and patient outcomes.} }
@inproceedings{10.1145/3570361.3615740, title = {Runtime WCET Estimation Using Machine Learning}, booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking}, year = {2023}, isbn = {9781450399906}, doi = {10.1145/3570361.3615740}, url = {https://doi.org/10.1145/3570361.3615740}, author = {Yun, Sangwoon and Kang, Kyungtae}, keywords = {embedded systems, real-time systems, neural networks, location = Madrid, Spain}, abstract = {Accurate task execution time estimation is vital for efficient and dependable operation of safety-critical systems. However, modern automotive functions' complexity challenges conventional estimation methods. To address this, we propose a novel technique that combines execution time and job sequence data using a multi-layer perceptron (MLP) neural network. Leveraging MLP's capabilities, our approach achieves impressive 99.7\% prediction accuracy with a mere 38.33 μs latency. Integrating our technique into safety-critical systems optimizes resource allocation and scheduling, enhancing performance and reliability. Importantly, our method extends beyond automotive systems, finding potential in diverse safety-critical domains. By precisely estimating task execution time, we enhance operational efficiency and decision-making in complex systems.} }
@inproceedings{10.1145/3688671.3688757, title = {Machine Learning Methods for Emulating Personality Traits in a Gamified Environment}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688757}, url = {https://doi.org/10.1145/3688671.3688757}, author = {Liapis, Georgios and Vordou, Anna and Vlahavas, Ioannis}, keywords = {Machine Learning, OCEAN 5, Gamified Environment}, abstract = {Personality traits are regarded as a significant factor of competency for job candidates, for example, evaluating the capacity to work efficiently within a team. However, there is a gap in the traditional assessment system for these cases since they typically rely on self-answered questionnaires that are biased or easily exploitable. Artificial Intelligence techniques can fill this gap by generating objective data to define standard personality template profiles, utilizing trained Reinforcement Learning agents. In this paper, we propose a gamified framework that employs Machine Learning methods to emulate personality traits based on the players’ play styles, with the purpose of creating standard team profiles. The OCEAN Five personality model is used as a basis for this attempt, which characterizes personality as a synthesis of the five components: openness, conscientiousness, extraversion, agreeableness, and neuroticism. After generating gameplay data through self-play, we examine how various personality qualities, actions, and modes of communication impact the team performance of the agents, with respect to the different personality traits. Results indicate that the personality traits of the agents individually and as a team do impact their performance and efficiency. This can be used as a methodology for creating efficient individual bot agents or teams of agents in many game environments.} }
@inbook{10.1145/3718491.3718572, title = {Damage identification and evaluation model of blast-resistant structure based on machine learning}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {500--505}, year = {2025}, isbn = {9798400710865}, url = {https://doi.org/10.1145/3718491.3718572}, author = {Wang, Chongyu and Ding, Jianguo}, abstract = {Explosion-proof structure plays a vital role in protecting the safety of people and property. However, it is a complex and challenging task to identify and evaluate the damage caused by explosion to these structures. Traditional detection methods are time-consuming, laborious and ineffective in detecting hidden damage. In this paper, a damage identification and evaluation model of blast-resistant structure based on machine learning is proposed. The dynamic response data of the structure under the action of explosion is automatically analyzed by using the convolutional neural network (CNN) in deep learning, so as to quickly and accurately identify the location, degree and type of damage. CNN is selected because of its powerful feature extraction ability and efficient recognition ability of complex patterns. It can automatically learn and extract high-level features from original data, which simplifies the complicated process of manually designing features in traditional methods. The effectiveness of the proposed model is verified by the actual explosion experiment. The experimental results show that the CNN model maintains a high level of accuracy in identifying the three types of damage under different explosion intensities, especially in the medium-intensity explosion. In addition, CNN model shows remarkable advantages in damage identification and assessment tasks, and its accuracy reaches 93.87\%, far exceeding the methods based on vibration modal analysis and machine learning. This study not only improves the detection efficiency and reduces the risk of human misjudgment, but also provides a scientific basis for the safety management of anti-explosion structures and contributes a new technical means to improve the level of public safety.} }
@inproceedings{10.1145/3700838.3703661, title = {Postprandial Blood Glucose Level Prediction through combined Machine Learning, Meta-Learning and XAI}, booktitle = {Proceedings of the 26th International Conference on Distributed Computing and Networking}, pages = {284--286}, year = {2025}, isbn = {9798400710629}, doi = {10.1145/3700838.3703661}, url = {https://doi.org/10.1145/3700838.3703661}, author = {B, Aruna Devi and N, Karthik}, keywords = {Feature Engineering, Generalizability, Interpretability, Machine Learning, Meta-Learning, PPBG, T2DM, XAI}, abstract = {Postprandial blood glucose (PPBG) is the glucose level measured after a meal and it is one of the main factors for Type 2 Diabetes Mellitus (T2DM) management. This work aims to predict PPBG level of T2DM patients by addressing the problems of clinical interpretability, limited number of patient dataset and generalizability. The proposed work aims to build a machine learning (ML) model that integrates appropriate feature selection, meta-learning to improve the generalizability and Explainable Artificial Intelligence (XAI) for interpretability. The preliminary work employed Random Forest (RF) feature selection, Independent Component Analysis (ICA) feature extraction technique and followed by training of five ML models such as Support Vector Regression (SVR), Xtreme Gradient Boosting (XGB), Random Forest (RF), Convolutional Neural Network (CNN) and Multilayer Perceptron (MLP). RF produced low Root Mean Square Error (RMSE) of 30.91 followed by CNN and MLP.} }
@inproceedings{10.1145/3644815.3644972, title = {Software Design Decisions for Greener Machine Learning-based Systems}, booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI}, pages = {256--258}, year = {2024}, isbn = {9798400705915}, doi = {10.1145/3644815.3644972}, url = {https://doi.org/10.1145/3644815.3644972}, author = {del Rey, Santiago}, keywords = {green AI, energy efficiency, software engineering, sustainable computing, location = Lisbon, Portugal}, abstract = {The widespread integration of Machine Learning (ML) in software systems has brought forth unprecedented advancements, yet the surge in energy consumption raises ecological concerns. This research addresses the environmental impact of ML development, focusing on the energy implications of design decisions in ML-based systems. This thesis aims to offer insights into the energy consumption patterns influenced by deployment architecture and training environment. Different case studies on ML-based systems will be conducted to validate and demonstrate the implications of these design choices. The expected outcomes encompass actionable insights, validated through rigorous evaluations, and the development of an energy prediction tool for ML-based system development, to help in the decision-making process. This work contributes to the broader field of Green AI by addressing a critical gap and guiding the transition towards a more sustainable AI landscape.} }
@inproceedings{10.1145/3627106.3627175, title = {Secure Softmax/Sigmoid for Machine-learning Computation}, booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference}, pages = {463--476}, year = {2023}, isbn = {9798400708862}, doi = {10.1145/3627106.3627175}, url = {https://doi.org/10.1145/3627106.3627175}, author = {Zheng, Yu and Zhang, Qizhi and Chow, Sherman S. M. and Peng, Yuxiang and Tan, Sijun and Li, Lichun and Yin, Shan}, keywords = {Crypto, Machine Learning, Secure Computation, Sigmoid, Softmax, location = Austin, TX, USA}, abstract = {Softmax and sigmoid, composing exponential functions (ex) and division (1/x), are activation functions often required in training. Secure computation on non-linear, unbounded 1/x and ex is already challenging, let alone their composition. Prior works aim to compute softmax by its exact formula via iteration (CrypTen, NeurIPS\&nbsp;’21) or with ASM approximation\&nbsp;(Falcon, PoPETS\&nbsp;’21). They fall short in efficiency and/or accuracy. For sigmoid, existing solutions such as ABY2.0 (Usenix Security\&nbsp;’21) compute it via piecewise functions, incurring logarithmic communication rounds. We study a rarely-explored approach to secure computation using ordinary differential equations and Fourier series for numerical approximation of rational/trigonometric polynomials over composition rings. Our results include 1) the first constant-round protocol for softmax and 2) the first 1-round error-bounded protocol for approximating sigmoid. They reduce communication by and , respectively, shortening the private training process of state-of-the-art frameworks or platforms, namely, CryptGPU (S\&amp;P\&nbsp;’21), Piranha (Usenix Security\&nbsp;’22), and quantized training from MP-SPDZ (ICML\&nbsp;’22), while maintaining competitive accuracy.} }
@inproceedings{10.5555/3535850.3536121, title = {Manipulation of Machine Learning Algoirhtms}, booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems}, pages = {1833--1835}, year = {2022}, isbn = {9781450392136}, author = {Bishop, Nicholas}, keywords = {computational social choice, machine learning, location = Virtual Event, New Zealand}, abstract = {As data becomes increasingly available, individuals, organisations and companies are increasingly applying machine learning algorithms to make decisions. In many cases, those decisions have a direct effect on those who provided the data to the decision maker. In other words, data providers often have a vested interest in the decisions made based on the data provided. Therefore, decision makers should anticipate that data providers may alter or change the data they provide in order to achieve a preferential outcome. Such strategic behaviour is not adequately modelled by classical machine learning settings in the literature. As a result, new machine learning algorithms are required, which take into the account the incentives and capabilities of data providers when making decisions. This paper summarises a PhD project which attempts to address this problem in a number of contexts.} }
@inproceedings{10.1145/3615979.3656064, title = {Detecting Emergent Behavior in Complex Systems: A Machine Learning Approach}, booktitle = {Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation}, pages = {81--87}, year = {2024}, isbn = {9798400703638}, doi = {10.1145/3615979.3656064}, url = {https://doi.org/10.1145/3615979.3656064}, author = {Dahia, Simranjeet Singh and Szabo, Claudia}, keywords = {Complex systems, Emergent behavior, location = Atlanta, GA, USA}, abstract = {The live identification of emergent behavior in complex systems with little a-priori information is a challenging task and existing approaches are either applicable to a small subset of models or do not scale well. In contrast, post-mortem approaches that have a more in-depth understanding of the characteristics of emergent properties often struggle with analyzing a large amount of data to extract relationships between the variables, events, and entities whose interaction eventually leads to emergent behavior. Machine learning approaches have been promoted as potential replacements of existing approaches, due to their ability to analyze large amounts of data without a-priori knowledge of existing relationships. In this paper, we present a first step towards the use of supervised learning approaches to identify and predict emergent behavior. Our hybrid approach unifies live and post-mortem perspectives by relying on a visual inspection of the simulation run and the simulation data set to identify a set of features that are more likely to generate emergent behavior (post-mortem) which are then used by a machine learning module to predict emergent behavior (live). Our analysis shows the potential of such approaches but also highlights challenges and future avenues of research.} }
@article{10.1145/3729358, title = {RegTrieve: Reducing System-Level Regression Errors for Machine Learning Systems via Retrieval-Enhanced Ensemble}, journal = {Proc. ACM Softw. Eng.}, volume = {2}, year = {2025}, doi = {10.1145/3729358}, url = {https://doi.org/10.1145/3729358}, author = {Cao, Junming and Xiang, Xuwen and Cheng, Mingfei and Chen, Bihuan and Wang, Xinyan and Lu, You and Sha, Chaofeng and Xie, Xiaofei and Peng, Xin}, keywords = {Ensemble Model, Regression Error, Spoken QA System}, abstract = {Multiple machine learning (ML) models are often incorporated into real-world ML systems. However, updating an individual model in these ML systems frequently results in regression errors, where the new model performs worse than the old model for some inputs. While model-level regression errors have been widely studied, little is known about how regression errors propagate at system level. To address this gap, we propose RegTrieve, a novel retrieval-enhanced ensemble approach to reduce regression errors at both model and system level. Our evaluation across various model update scenarios shows that RegTrieve reduces system-level regression errors with almost no impact on system accuracy, outperforming all baselines by 20.43\% on average.} }
@article{10.5555/3722479.3722480, title = {10, 23, 81 --- Stacking up the LLM Risks: Applied Machine Learning Security}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {17--18}, year = {2024}, issn = {1937-4771}, author = {McGraw, Gary}, abstract = {I present the results of an architectural risk analysis (ARA) of large language models (LLMs), guided by an understanding of standard machine learning (ML) risks previously identified by BIML in 2020. After a brief level-set, I cover the top 10 LLM risks, then detail 23 black box LLM foundation model risks screaming out for regulation, finally providing a bird's eye view of all 81 LLM risks BIML identified. BIML's first work, published in January 2020 presented an in-depth ARA of a generic machine learning process model, identifying 78 risks. In this talk, I consider a more specific type of machine learning use case---large language models---and report the results of a detailed ARA of LLMs. This ARA serves two purposes: 1) it shows how our original BIML-78 can be adapted to a more particular ML use case, and 2) it provides a detailed accounting of LLM risks. At BIML, we are interested in "building security in" to ML systems from a security engineering perspective. Securing a modern LLM system (even if what's under scrutiny is only an application involving LLM technology) must involve diving into the engineering and design of the specific LLM system itself. This ARA is intended to make that kind of detailed work easier and more consistent by providing a baseline and a set of risks to consider.} }
@article{10.1145/3539783, title = {Software Engineering of Machine Learning Systems}, journal = {Commun. ACM}, volume = {66}, pages = {35--37}, year = {2023}, issn = {0001-0782}, doi = {10.1145/3539783}, url = {https://doi.org/10.1145/3539783}, author = {Isbell, Charles and Littman, Michael L. and Norvig, Peter}, abstract = {Seeking to make machine learning more dependable.} }
@inproceedings{10.1145/3675417.3675564, title = {Machine Learning-Based Crack Detection Methods in Ancient Buildings}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {885--890}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675564}, url = {https://doi.org/10.1145/3675417.3675564}, author = {Fang, Tianke and Hui, Zhenxing and P.Rey, William and Yang, Aihua and Liu, Bin and He, Yuanrong}, abstract = {This research paper introduces an innovative machine learning-based methodology for detecting cracks in ancient buildings, a vital aspect of architectural heritage conservation. Recognizing the limitations of conventional techniques in addressing the complex nature of historical structures, this study explores the integration of deep learning algorithms with enhanced image processing methods tailored for aged and varied materials typical of ancient architecture. The paper includes a comprehensive process involving meticulous data collection from diverse ancient structures, employing specialized preprocessing methods to handle the unique challenges of old surfaces. The development of a custom machine learning model, designed to adapt to the intricacies of historical construction, is thoroughly detailed. Experimental results demonstrate the model's superior performance in accurately identifying structural cracks compared to traditional methods. This research not only significantly contributes to the preservation of historical buildings but also exemplifies the versatile application of machine learning in the field of architectural conservation. The findings hold promise for revolutionizing the approach to maintaining and restoring our architectural heritage, ensuring longevity and integrity.} }
@inproceedings{10.1145/3677182.3677236, title = {Machine learning algorithm based airborne LiDAR point cloud classification method}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {301--305}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677236}, url = {https://doi.org/10.1145/3677182.3677236}, author = {Zhang, Yanwen and Wang, Xiaosong and Fu, Yuwen and Wang, Miao and Liu, Haoguang and Wang, Zhoujie}, abstract = {For decades, airborne LiDAR measurement technology has gradually matured and achieved high measurement results, obtaining detailed and accurate data. The airborne Lidar has a capability to capture high accuracy and high-resolution point clouds from the object surface efficiently. However, it is inefficient and labor-intensive to distinguish the information on land use by manual classification. Deep learning, as a part of machine learning research, plays a significant role in point cloud data classification, such as unstructured and chaotic data. At present, many scholars have proposed various algorithms in the processing of airborne LiDAR point cloud data. However, overall, there are relatively few literature reviews on radar point clouds. Therefore, this study introduces machine learning from three different stages of development, starting from the historical development of time. After that, a large number of application examples in point clouds classification are compared. The analysis results demonstrate that rule-based classification is tedious, but can help users understand internal structure, classic machine learning methods can achieve a higher classification accuracy and deep learning has broad prospects in airborne Lidar point clouds.} }
@inproceedings{10.1145/3627673.3680272, title = {Towards Making Effective Machine Learning Decisions Against Out-of-Distribution Data}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5479--5482}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3680272}, url = {https://doi.org/10.1145/3627673.3680272}, author = {Tamang, Lakpa D.}, keywords = {OOD detection, OOD generalization, out-of-distribution, location = Boise, ID, USA}, abstract = {Conventional machine learning systems operate on the assumption of independent and identical distribution (i.i.d), where both the training and test data share a similar sample space, and no distribution shift exists between them. However, this assumption does not hold in practical deployment scenarios, making it crucial to develop methodologies that address the non-trivial task of data distribution shift. In our research, we aim to address this problem by developing ML algorithms that explicitly achieve promising performance when subjected to various types of out-of-distribution (OOD) data. Specifically, we approach the problem by categorizing the data distribution shifts into two types: covariate shifts and semantic shifts, and proposing effective methodologies to tackle each type independently and conjointly while validating them with different types of datasets. We aim to propose ideas that are compatible with existing deep neural networks to perform detection and/or generalization of the test instances that are shifted in semantic and covariate space, respectively.} }
@inproceedings{10.1145/3640900.3640910, title = {Automatic CDT Scoring Using Machine Learning with Interpretable Feature}, booktitle = {Proceedings of the 2024 14th International Conference on Bioscience, Biochemistry and Bioinformatics}, pages = {55--59}, year = {2024}, isbn = {9798400716768}, doi = {10.1145/3640900.3640910}, url = {https://doi.org/10.1145/3640900.3640910}, author = {Chen, Bo-Lin and Hu, Kuan-Ting and Cheng, Kuo-Sheng and Chen, Chien-Yu}, abstract = {The Clock Drawing Test (CDT) is a widely used cognitive assessment tool in clinical practice. However, it requires a trained neuropsychologist to evaluate the drawings, and the scoring process may be subjective due to the experience of the neuropsychologist. In this paper, we propose a novel automatic CDT scoring method based on interpretable features using machine learning. First, we use image processing techniques to extract features associated with the scoring guideline. Then, we combine these features as an input vector for training the machine learning classifier. Our experimental results demonstrate that our method achieves an accuracy of 82\%, which is superior to that of deep learning methods.} }
@inproceedings{10.1145/3589883.3589884, title = {AI2: a novel explainable machine learning framework using an NLP interface}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {1--7}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589884}, url = {https://doi.org/10.1145/3589883.3589884}, author = {Dessureault, Jean-S\'ebastien and Massicotte, Daniel}, keywords = {NLP, accessibility, explainability, framework, machine learning, location = Stockholm, Sweden}, abstract = {This paper proposes a novel machine learning framework that encapsulates recent concerns of the data scientists community: accessibility and explainability. This framework, called AI2, proposes a natural language interface, making the framework accessible even to a non-expert. Traditionally, machine learning frameworks are accessible using a programming language. Python is one of the most common programming language for coding different machine learning methods. The AI2 framework, although made with Python scripts, is made to be accessed in a natural language, namely, English. Hence, the first contribution is about accessibility, allowing a non-data scientist to exploit a machine learning framework without knowing how to code. For decades, the data scientists community has known that one of the drawbacks in the machine learning field is the black-box problem. Data scientists have to create different methods to explain their results. The second contribution of this paper is to encapsulate the principle of explainability in the framework, systematically proposing not only the results but also the explanations of the results for every included machine learning algorithm.} }
@article{10.1613/jair.1.14238, title = {Generalizing Group Fairness in Machine Learning via Utilities}, journal = {J. Artif. Int. Res.}, volume = {78}, year = {2024}, issn = {1076-9757}, doi = {10.1613/jair.1.14238}, url = {https://doi.org/10.1613/jair.1.14238}, author = {Blandin, Jack and Kash, Ian A.}, abstract = {Group fairness definitions such as Demographic Parity and Equal Opportunity make assumptions about the underlying decision-problem that restrict them to classification problems. Prior work has translated these definitions to other machine learning environments, such as unsupervised learning and reinforcement learning, by implementing their closest mathematical equivalent. As a result, there are numerous bespoke interpretations of these definitions. This work aims to unify the shared aspects of each of these bespoke definitions, and to this end we provide a group fairness framework that generalizes beyond just classification problems. We leverage two fairness principles that enable this generalization. First, our framework measures outcomes in terms of utilities, rather than predictions, and does so for both the decision-maker and the individual. Second, our framework can consider counterfactual outcomes, rather than just observed outcomes, thus preventing loopholes where fairness criteria are satisfied through self-fulfilling prophecies. We provide concrete examples of how our utility fairness framework avoids these assumptions and thus naturally integrates with classification, clustering, and reinforcement learning fairness problems. We also show that many of the bespoke interpretations of Demographic Parity and Equal Opportunity fit nicely as special cases of our framework.} }
@inproceedings{10.1145/3658835.3658838, title = {Machine Learning-based Composition Analysis of Ancient Glass Objects}, booktitle = {Proceedings of the 2024 5th International Conference on Artificial Intelligence in Electronics Engineering}, pages = {9--19}, year = {2024}, isbn = {9798400716850}, doi = {10.1145/3658835.3658838}, url = {https://doi.org/10.1145/3658835.3658838}, author = {Li, Ying and Tang, Jierong and Rao, Junreng and Wang, Yuhan and Li, Le and Tan, Zhen and Xiao, Weidong}, keywords = {Ancient glass components, Decision trees, Fisher discriminant method, K-means clustering, Machine learning, location = Bangkok, Thailand}, abstract = {Ancient glass is one of the objects studied in archaeology, and the study of ancient Chinese glass has attracted much attention from scholars at home and abroad in recent years. Ancient glass is similar in appearance to exotic glass objects, but the chemical composition is different. Moreover, ancient glass is highly susceptible to weathered by the burial environment. During the weathered process, a large number of internal elements are exchanged with environmental elements, resulting in changes in their composition ratios, which affects scientists' correct judgement of their categories. This paper implements a variety of machine learning methods to analyze the chemical composition of ancient glass, predict the chemical composition content of ancient glass before weathered, correctly determine the type of ancient glass based on its chemical composition, and sub-classify ancient glass. The final results of this paper provide a scientific basis for archaeological research work and lay the foundation for subsequent studies on the origins, systems, manufacturing dates and preparation processes of ancient glass.} }
@inproceedings{10.1145/3584371.3612946, title = {Optimizing K-Mer Fingerprint Generation for Machine Learning}, booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics}, year = {2023}, isbn = {9798400701269}, doi = {10.1145/3584371.3612946}, url = {https://doi.org/10.1145/3584371.3612946}, author = {Kromer-Edwards, Cory}, keywords = {k-mer, cuda, fingerprint, hashmap, algorithm, extension, python, fasta, optimize, machine learning, bacteria, location = Houston, TX, USA}, abstract = {With the increasing availability of genomic data obtained through Whole-Genome Sequencing (WGS), Machine Learning (ML) algorithms are being used to analyze this data. However, processing large datasets or files poses challenges. One approach is to count K-Mers, which has been used in ML studies. However, larger K-Mer sizes may lead to decreased accuracy and training difficulties. Alternatively, combining multiple K-Mers of smaller sizes into fingerprints has shown promise in predicting species and antibiotic resistance. This study compares existing fingerprint generation techniques with a new algorithm called GPU K-Mer Fingerprinting (GKF), which utilizes a GPU for parallel processing. GKF demonstrates similar memory utilization compared to other approaches but achieves a speedup of 5,546X.} }
@inproceedings{10.1145/3698062.3698091, title = {IPL Cricket Fantasy Team Prediction for Dream11 using Machine Learning}, booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)}, pages = {196--202}, year = {2024}, isbn = {9798400717086}, doi = {10.1145/3698062.3698091}, url = {https://doi.org/10.1145/3698062.3698091}, author = {Raju, K Bhavish and M, Govindarajan and Thakur, Kritin and Kumar, Ishan and Paladugula, Lakshmi Snigdha and Rajapurohit, Prarthana and K S, Srinivas}, keywords = {CatBoost (CB), Cricket, Dream11, Fantasy Contests, Grid Search CV, \&nbsp, RandomSearch CV, Indian Premier League (IPL), Player prediction, Random forest Classifier (RF), eXtreme Gradient Boosting (XGB)}, abstract = {This study aims to pick the best fantasy cricket team for IPL matches (T20) to participate in fantasy contests on platforms like Dream11 and My11 Circle. This is done by predicting the top eleven players from both the participating teams for a particular match in line with the requirements. A dataset of ball-by-ball details of each IPL match from 2016 to 2023 was obtained from the Cricsheet website [10]. This data was then preprocessed into the required format followed by the addition of important columns such as bowler faced by each batsman and their bowling style. Most importantly we obtained individual batsman-bowler match-ups data in each IPL match along with a fielding dataset as well. We used several Machine Learning models which included regression models such as Random Forest (RF), Extreme Gradient Boosting (XGB) and a baseline Linear Regression Model for the batting predictions, and classification models such as Random Forest, Gradient Boosting, CatBoost (CB), etc for bowling and fielding predictions. Based on the predictions we used a standardized formula to calculate the Dream11 fantasy points scored by the batsman, bowlers and fielders. From the calculated fantasy points, the top 11 players are picked into the fantasy team while the players with the highest and second highest points are selected as captain and vice-captain for bonus points.\&nbsp; The model was able to predict at least 7 players correctly as top performers in a match accurately 85\% of the time.} }
@inproceedings{10.1145/3706468.3706482, title = {Towards Fair Assessments: A Machine Learning-based Approach for Detecting Cheating in Online Assessments}, booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference}, pages = {104--114}, year = {2025}, isbn = {9798400707018}, doi = {10.1145/3706468.3706482}, url = {https://doi.org/10.1145/3706468.3706482}, author = {Garg, Manika and Goel, Anita}, keywords = {Academic dishonesty, Cheating, Feature engineering, Integrity, Machine learning, Online education}, abstract = {Academic cheating poses a significant challenge to conducting fair online assessments. One common way is collusion, where students unethically share answers during the assessment. While several researchers proposed solutions, there is lack of clarity regarding the specific types they target among the different types of collusion. Researchers have used statistical techniques to analyze basic attributes collected by the platforms, for collusion detection. Only few works have used machine learning, considering two or three attributes only; the use of limited features leading to reduced accuracy and increased risk of false accusations.In this work, we focus on In-Parallel Collusion, where students simultaneously work together on an assessment. For data collection, a quiz tool is improvised to capture clickstream data at a finer level of granularity. We use feature engineering to derive seven features and create a machine learning model for collusion detection. The results show: 1) Random Forest exhibits the best accuracy (98.8\%), and 2) In contrast to less features as used in earlier works, the full feature set provides the best result; showing that considering multiple facets of similarity enhance the model accuracy. The findings provide platform designers and teachers with insights into optimizing quiz platforms and creating cheat-proof assessments.} }
@inproceedings{10.1145/3715669.3726844, title = {Predicting Children’s Reading Comprehension Through Eye Movements: Insights from Visual Search and Interpretable Machine Learning}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726844}, url = {https://doi.org/10.1145/3715669.3726844}, author = {Brasser, Jan and Tschirner, Chiara and Stegenwallner-Sch\"utz, Maja and Jakobi, Deborah Noemie and J\"ager, Lena A.}, keywords = {Eye Movements, Reading Comprehension, Visual Search, Reading Development, Beginning Readers, Machine Learning, Interpretability}, abstract = {Early identification of children at risk of reading difficulties is paramount for promoting educational success and equity, as earlier interventions are more effective. Traditional assessment methods of early reading abilities, however, are resource-intensive, and often require basic reading abilities. Predicting reading acquisition from non-reading tasks, particularly during the early stages of formal reading instruction, overcomes these limitations. In this paper, we investigate to what extent eye movements recorded during a visual search task that is hypothesized to correlate with reading ability allows to predict children’s reading comprehension scores at the time of recording as well as one year later. Using machine learning methods that allow for an evaluation of feature importance, namely Neural Additive Models and Random Forests, we explore what eye movement features obtained from a visual search task are predictive of reading comprehension, thus laying the groundwork for future research in early assessment systems based on eye movements.} }
@inproceedings{10.1145/3700906.3701007, title = {Design and Python Simulation of Cross-border E-commerce Logistics Pathways Based on Machine Learning Algorithms}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {627--633}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3701007}, url = {https://doi.org/10.1145/3700906.3701007}, author = {Liu, Jian and Lei, Fei N/A and Liu, Qi and Yuan, Huan}, keywords = {Cross-border e-commerce logistics path design, Machine learning algorithm, Python Simulation, Random Forest}, abstract = {In the era of globalization, cross-border e-commerce has emerged as a significant driver of international trade, necessitating efficient logistics pathways to enhance delivery speed and reduce costs. This study presents a comprehensive approach to designing and simulating cross-border e-commerce logistics pathways using machine learning algorithms. We first identify the key factors influencing logistics efficiency, including geographical, regulatory, and demand variability aspects. Utilizing a dataset comprising historical shipping data, we apply various machine learning techniques, such as decision trees, random forests, and neural networks, to model and predict optimal logistics routes. The proposed model integrates real-time data processing and predictive analytics to dynamically adapt to changing conditions and optimize the logistics pathways in real-time. A Python-based simulation framework is developed to visualize and test the effectiveness of the proposed logistics pathways under different scenarios. The results demonstrate significant improvements in delivery times and cost reductions when compared to traditional logistics strategies. This research not only contributes to the field of logistics and supply chain management but also provides practical insights for e-commerce businesses seeking to enhance their operational efficiency in the competitive cross-border market. Future work will focus on refining the model with additional variables and exploring the integration of blockchain technology to further enhance transparency and traceability in cross-border logistics operations.} }
@inproceedings{10.1145/3661167.3661268, title = {Insights Into Test Code Quality Prediction: Managing Machine Learning Techniques}, booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering}, pages = {2}, year = {2024}, isbn = {9798400717017}, doi = {10.1145/3661167.3661268}, url = {https://doi.org/10.1145/3661167.3661268}, author = {Pontillo, Valeria}, keywords = {Empirical Studies., Machine Learning, Test Code Quality Prediction, location = Salerno, Italy}, abstract = {Test cases represent the first line of defence against the introduction of software faults, especially when testing for regressions. They must be constantly maintained and updated as part of software components to keep them useful. With the help of testing frameworks, developers create test methods and run them periodically on their code. The entire team relies on the results from these tests to decide whether to merge a pull request or deploy the system. Unfortunately, tests are not immune to bugs or technical debts: indeed, they often suffer from issues that can preclude their effectiveness. Typical problems in test cases are called flaky tests and test smells. Over the last decades, the software engineering research community has been proposing a number of static and dynamic approaches to assist developers with the (semi-)automatic detection and removal of these problems. Despite this, most of these approaches rely on expensive dynamic steps and depend on tunable thresholds. These limitations have been partially targeted through machine learning solutions that could predict test quality issues using various features, like source code vocabulary or a mixture of static and dynamic metrics. In this tutorial, I will discuss our experience building prediction models to detect quality issues in test code. The tutorial will discuss the design choices to make in the context of test code quality prediction and the implications these choices have for the reliability of the resulting models.} }
@inproceedings{10.1145/3715669.3726785, title = {Learning Disorder Detection Using Eye Tracking: Are Large Language Models Better Than Machine Learning?}, booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications}, year = {2025}, isbn = {9798400714870}, doi = {10.1145/3715669.3726785}, url = {https://doi.org/10.1145/3715669.3726785}, author = {Nguyen, Quoc-Toan and Nguyen, Hy and Tang, Quang-Hieu and Truong, Tien and Pham, Van-Tuan and Le, Linh and Williams-King, David}, keywords = {Eye Tracking, LLMs, Dyslexia, Specific Learning Disorder, Human-centered computing}, abstract = {Early detection of learning disorders is essential for timely intervention, fostering improved academic performance and overall well-being. This research explores the potential of Large Language Models (LLMs) combined with cost-effective eye-tracking data for detecting dyslexia, one of the most prevalent learning disorders. The prominent LLMs included are DeepSeek-V3, Llama3.3-70B, GPT-4o, GPT-o3-mini, and GPT-o1-mini. Leveraging data from 70 participants across three distinct tasks, our findings reveal that LLMs can outperform traditional Machine Learning (ML) models. Notably, few-shot prompting significantly enhances accuracy, demonstrating the adaptability and efficiency of LLM-driven approaches. In summary, this study presents a novel approach to dyslexia detection by integrating eye-tracking data with LLMs. By outperforming specialised ML models, this scalable approach optimises resources and expands early detection, making dyslexia assessment more accessible. It enables timely support, enhancing academic performance and overall well-being for affected individuals.} }
@inproceedings{10.1145/3727993.3728063, title = {Research on a Machine Learning-Based Prediction Model for Tear Gas Smoke Dispersion}, booktitle = {Proceedings of the 2024 4th International Conference on Computational Modeling, Simulation and Data Analysis}, pages = {415--421}, year = {2025}, isbn = {9798400711831}, doi = {10.1145/3727993.3728063}, url = {https://doi.org/10.1145/3727993.3728063}, author = {He, Yipeng and Cui, Xiaoping and Tian, Ye and Zhang, Mengxing}, keywords = {Machine learning, Prediction model, Smoke diffusion, Tear gas}, abstract = {In this study, a machine learning-based prediction model for tear smoke dispersion was constructed, incorporating Random Forest, Gradient Boosting Tree and LSTM networks. Detailed meteorological, geographical and topographical data were collected, and data cleaning and feature engineering were performed. The model was found to outperform traditional models in terms of prediction accuracy, computational efficiency and adaptability, and to show excellent performance in different time scales, spatial scales and environmental conditions.} }
@inproceedings{10.1145/3576915.3616593, title = {DPMLBench: Holistic Evaluation of Differentially Private Machine Learning}, booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security}, pages = {2621--2635}, year = {2023}, isbn = {9798400700507}, doi = {10.1145/3576915.3616593}, url = {https://doi.org/10.1145/3576915.3616593}, author = {Wei, Chengkun and Zhao, Minghu and Zhang, Zhikun and Chen, Min and Meng, Wenlong and Liu, Bo and Fan, Yuan and Chen, Wenzhi}, keywords = {deep learning, differential privacy, privacy-preserving machine learning, location = Copenhagen, Denmark}, abstract = {Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning (ML) techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We first present a taxonomy of where improvements are located in the ML life cycle. Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms, over twelve algorithms, four model architectures, four datasets, two attacks, and various privacy budget configurations. We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation. According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense. We also explore some improvements that can maintain model utility and defend against MIAs more effectively. Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs. ML practitioners may benefit from these evaluations to select appropriate algorithms. To support our evaluation, we implement a modular re-usable software, DPMLBench,1. We open-source the tool in https://github.com/DmsKinson/DPMLBench which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners.} }
@inproceedings{10.1145/3640115.3640191, title = {Human Resources Analysis Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering}, pages = {470--477}, year = {2024}, isbn = {9798400708299}, doi = {10.1145/3640115.3640191}, url = {https://doi.org/10.1145/3640115.3640191}, author = {Guo, Ning}, keywords = {Data Analysis, Human Resources Management, Machine learning, Optimized Decision Support, PSO-PB Model, location = Changde, Hunan, China}, abstract = {This study is grounded in a human resource dataset, examining the correlation between employee performance and various factors, encompassing age, and educational background. The dataset comprises 13 dimensions, and meticulous data cleaning and processing were applied to rectify missing and incongruous data issues. Subsequent to the preparatory steps, data mining techniques were employed to scrutinize the influence of age and education on employee value. The paper then details the methodology for establishing and optimizing the model, utilizing the Particle Swarm Optimization (PSO) algorithm and BP00 neural network. Validation of the model's efficacy follows, with results indicating its robust capability to predict employee value accurately. These findings offer valuable insights for enhancing human resources management practices. The paper concludes by summarizing research outcomes and underscored the pivotal role of a judicious performance assessment system in human resources management. Future research directions are also delineated.} }
@article{10.1145/3656580, title = {Foundations \&amp; Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3656580}, url = {https://doi.org/10.1145/3656580}, author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe}, keywords = {Multimodal machine learning, representation learning, data heterogeneity, feature interactions, language and vision, multimedia}, abstract = {Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this article is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.} }
@proceedings{10.1145/3653724, title = {ICMML '23: Proceedings of the International Conference on Mathematics and Machine Learning}, year = {2023}, isbn = {9798400716973} }
@inproceedings{10.1145/3708635.3708653, title = {Comparing the Performance of a CDC Questionnaire with Machine Learning Models in Predicting Diabetes}, booktitle = {Proceedings of the 2024 13th International Conference on Software and Information Engineering}, pages = {119--124}, year = {2025}, isbn = {9798400717765}, doi = {10.1145/3708635.3708653}, url = {https://doi.org/10.1145/3708635.3708653}, author = {Alatawi, Yusuf and Kadhem, Hasan}, keywords = {Machine Learning, Diabetes, Classification, Nutrition and Dietary Habits, NHANES}, abstract = {This paper addresses the pressing issue of diabetes, a primary global health concern, by comparing machine learning models and a Centers for Disease Control and Prevention (CDC) questionnaire for diabetes diagnosis based on nutritional data. Utilizing the National Health and Nutrition Examination Survey (NHANES) dataset, the study highlights the limited of Machine Learning (ML) research focusing solely on nutritional features. The machine learning models, including K nearest neighbor (KNN) and support vector machine (SVM), are evaluated for recall, precision, and other performance metrics. Notably, KNN and SVM outperform the CDC questionnaire, obtaining F1 scores of more than 60\% compared to about 40\% for the questionnaire. In addition to that, they achieved recall scores of more than 80\%, while the questionnaire only managed a recall of about 30\%. These results suggest that machine learning models based on nutrition and diet could serve as more effective diabetes screening tools.} }
@article{10.5555/3648699.3648940, title = {Improved powered stochastic optimization algorithms for large-scale machine learning}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Yang, Zhuang}, keywords = {powerball function, stochastic optimization, variance reduction, adaptive learning rate, non-convex optimization}, abstract = {Stochastic optimization, especially stochastic gradient descent (SGD), is now the workhorse for the vast majority of problems in machine learning. Various strategies, e.g., control variates, adaptive learning rate, momentum technique, etc., have been developed to improve canonical SGD that is of a low convergence rate and the poor generalization in practice. Most of these strategies improve SGD that can be attributed to control the updating direction (e.g., gradient descent or gradient ascent direction), or manipulate the learning rate. Along these two lines, this work first develops and analyzes a novel type of improved powered stochastic gradient descent algorithms from the perspectives of variance reduction, where the updating direction was determined by the Powerball function. Additionally, to bridge the gap between powered stochastic optimization (PSO) and the learning rate, which is now still an open problem for PSO, we propose an adaptive mechanism of updating the learning rate that resorts the Barzilai-Borwein (BB) like scheme, not only for the proposed algorithm, but also for classical PSO algorithms. The theoretical properties of the resulting algorithms for non-convex optimization problems are technically analyzed. Empirical tests using various benchmark data sets indicate the efficiency and robustness of our proposed algorithms.} }
@article{10.5555/3648699.3649089, title = {A scalable and efficient iterative method for copying machine learning classifiers}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Statuto, Nahuel and Unceta, Irene and Nin, Jordi and Pujol, Oriol}, keywords = {sustainable AI, transfer learning, environmental adaptation, optimization, model enhancement}, abstract = {Differential replication through copying refers to the process of replicating the decision behavior of a machine learning model using another model that possesses enhanced features and attributes. This process is relevant when external constraints limit the performance of an industrial predictive system. Under such circumstances, copying enables the retention of original prediction capabilities while adapting to new demands. Previous research has focused on the single-pass implementation for copying. This paper introduces a novel sequential approach that significantly reduces the amount of computational resources needed to train or maintain a copy, leading to reduced maintenance costs for companies using machine learning models in production. The effectiveness of the sequential approach is demonstrated through experiments with synthetic and real-world datasets, showing significant reductions in time and resources, while maintaining or improving accuracy.} }
@inproceedings{10.1145/3514094.3539530, title = {Privacy Preserving Machine Learning Systems}, booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {898}, year = {2022}, isbn = {9781450392471}, doi = {10.1145/3514094.3539530}, url = {https://doi.org/10.1145/3514094.3539530}, author = {EL MESTARI, Soumia Zohra}, keywords = {differential privacy, homomorphic encryption, secure multi-party computation, location = Oxford, United Kingdom}, abstract = {Machine learning(ML) tools are among the promising data-driven techniques that can help solve many real-life problems. However these tools rely on the collection of large volumes of data, which raises many privacy concerns and more broadly trustworthiness concerns. Privacy Preserving technologies aim at solving the issue by integrating privacy enhancing technologies (PETs) within the machine learning pipelines.} }
@inproceedings{10.1145/3607947.3607967, title = {Prediction of Algae Growth: A Machine Learning Perspective}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {109--114}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3607967}, url = {https://doi.org/10.1145/3607947.3607967}, author = {Tiwary, Sanjeeb and Darshana, Subhashree and Mohanty, Debabrata and Dash, Adyasha and Rupsa, Potnuru and Barik, Rabindra K}, keywords = {Algae, Artificial Algae Algorithm, Artificial Neural Network, Blooms, Harmful Algal, Machine Learning, Support Vector, location = Noida, India}, abstract = {Algal blooms pose a significant threat to aquatic ecosystems and human health. To address this issue, this paper proposes a machine learning-based approach for predicting harmful algal blooms (HABs) by analyzing environmental features. Algae, as primary organic matter and oxygen producers, play a vital role in the biosphere. However, the exponential increase in algal growth worldwide poses significant challenges to economic development and long-term sustainability. The paper employs three popular machine learning algorithms: Artificial Neural Network (ANN), Gradient Boosting Decision Tree (GBDT), and Support Vector Machine (SVM) to predict algal blooms. The research utilizes real-time data from two locations: the Sassafras River in the United States Chesapeake Bay and Lake Okeechobee in Florida, USA. These locations have experienced frequent HABs due to factors like chemical runoff and nutrient-rich conditions. By analyzing the collected data, the paper identifies and selects the most important features to optimize the prediction models’ accuracy. Preliminary results demonstrate promising accuracy in predicting algal growth and identifying key characteristics associated with HABs. These findings contribute to a better understanding of algal blooms and pave the way for effective mitigation strategies to combat this global environmental challenge.} }
@inproceedings{10.1145/3708036.3708221, title = {"The Enterprise Performance Puzzle - Predictive Modeling Based on Interpretable Machine Learning"}, booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology}, pages = {1119--1124}, year = {2025}, isbn = {9798400709999}, doi = {10.1145/3708036.3708221}, url = {https://doi.org/10.1145/3708036.3708221}, author = {Yang, Nan}, keywords = {Business performance, Executive characteristics, Firm characteristics, SHAP algorithm, Tree model}, abstract = {Corporate performance is a mature and popular research topic, and this study focuses on the influence of internal factors on corporate performance, and constructs a systematic corporate performance prediction model through the tree model and SHAP algorithm in machine learning. Unlike previous studies that explored corporate performance in a single dimension, this study covers corporate characteristics and executive characteristics, and explores the contribution of internal factors in corporate performance prediction. We compare and analyze firm-level and executive-level characteristics to discover the importance and mode of their roles in predicting firm performance. The results show that firm-level characteristics have a more direct and significant impact than executive-level characteristics in predicting firm performance. Among them, total corporate assets play the most important role in predicting corporate performance, and this study not only overcomes the limitations of traditional measurement models, but also provides new perspectives and tools for corporate performance prediction and management practices.} }
@article{10.1109/TCBB.2024.3434340, title = {Machine Learning-Assisted High-Throughput Screening for Anti-MRSA Compounds}, journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics}, volume = {21}, pages = {1911--1921}, year = {2024}, issn = {1545-5963}, doi = {10.1109/TCBB.2024.3434340}, url = {https://doi.org/10.1109/TCBB.2024.3434340}, author = {Shehadeh, Fadi and Felix, LewisOscar and Kalligeros, Markos and Shehadeh, Adnan and Fuchs, Beth Burgwyn and Ausubel, Frederick M. and Sotiriadis, Paul P. and Mylonakis, Eleftherios}, abstract = {Background: Antimicrobial resistance is a major public health threat, and new agents are needed. Computational approaches have been proposed to reduce the cost and time needed for compound screening. Aims: A machine learning (ML) model was developed for the \&lt;italic\&gt;in silico\&lt;/italic\&gt; screening of low molecular weight molecules. Methods: We used the results of a high-throughput \&lt;italic\&gt;Caenorhabditis elegans\&lt;/italic\&gt; methicillin-resistant \&lt;italic\&gt;Staphylococcus aureus\&lt;/italic\&gt; (MRSA) liquid infection assay to develop ML models for compound prioritization and quality control. Results: The compound prioritization model achieved an AUC of 0.795 with a sensitivity of 81\% and a specificity of 70\%. When applied to a validation set of 22,768 compounds, the model identified 81\% of the active compounds identified by high-throughput screening (HTS) among only 30.6\% of the total 22,768 compounds, resulting in a 2.67-fold increase in hit rate. When we retrained the model on all the compounds of the HTS dataset, it further identified 45 discordant molecules classified as non-hits by the HTS, with 42/45 (93\%) having known antimicrobial activity. Conclusion: Our ML approach can be used to increase HTS efficiency by reducing the number of compounds that need to be physically screened and identifying potential missed hits, making HTS more accessible and reducing barriers to entry.} }
@inproceedings{10.1145/3671016.3674816, title = {CLUE: Customizing clustering techniques using machine learning for software modularization}, booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware}, pages = {189--198}, year = {2024}, isbn = {9798400707056}, doi = {10.1145/3671016.3674816}, url = {https://doi.org/10.1145/3671016.3674816}, author = {Meng, Fanyi and Wang, Ying and Chong, Chun Yong and Yu, Hai and Zhu, Zhiliang}, keywords = {Empirical study, Software architecture, Software clustering, Software modularization, location = Macau, China}, abstract = {Software clustering is often used as a remodularization and architecture recovery technique to help developers simplify software maintenance tasks and ease the burden of software comprehension. While the choice of clustering technique can significantly influence the outcomes of remodularization, it is noteworthy that existing works have yet to conduct an exhaustive exploration of the suitability of various clustering techniques for different software projects. Although many prior works introduce new clustering techniques, their validations often focus on specific domains, which may restrict the generalizability of their findings. In this paper, we conduct an empirical study aimed at understanding the impact of software features and architectural problems on the effectiveness of various software clustering techniques. Leveraging our empirical findings, we propose an approach, CLUE, which leverages Machine Learning (ML) models to customize a suitable software clustering technique for a given software. Our approach focuses on eight types of software clustering techniques and offers insights into their suitability based on features and architectural problems of software. This comprehensive analysis helps developers to select the suitable clustering technique that can achieve the best MoJoFM, c2ccvg, or TurboMQ value from the chosen pool of software clustering techniques for specific software. We evaluate CLUE by analyzing 100 open-source software projects. The experiment results demonstrate that CLUE achieves highly accurate clustering technique customization, with an accuracy exceeding 90\%.} }
@inproceedings{10.5555/3643142.3643363, title = {Transforming Discrete Event Models to Machine Learning Models}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {2662--2673}, year = {2024}, isbn = {9798350369663}, author = {Sarjoughian, Hessam S. and Fallah, Forouzan and Saeidi, Seyyedamirhossein and Yellig, Edward J.}, abstract = {Discrete event simulation, formalized as deductive modeling, has been shown to be effective for studying dynamical systems. Development of models, however, is challenging when numerous interacting components are involved and should operate under different conditions. Machine Learning (ML) holds the promise to help reduce the effort needed to develop models. Toward this goal, a collection of ML algorithms, including Automatic Relevance Determination is used. Parallel Discrete Event System Specification (PDEVS) models are developed for Single-stage and Two-stage cascade factories. Each model is simulated under different demand profiles. The simulated data sets are partitioned into subsets, each for one or more model components. The ML algorithms are applied to the data sets for generating models. The throughputs predicted by the ML models closely match those in the PDEVS simulated data. This study contributes to modeling by demonstrating the potential benefits and complications of utilizing ML for discrete-event systems.} }
@inproceedings{10.1145/3759275.3759290, title = {Data‐Driven Insights into Water Art: A Machine Learning Approach to the Power of Natural Spirit}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Systems and Design Innovation}, pages = {92--97}, year = {2025}, isbn = {9798400719554}, doi = {10.1145/3759275.3759290}, url = {https://doi.org/10.1145/3759275.3759290}, author = {Li, Yanhui}, keywords = {Audience Segmentation, Clustering, Contemporary Art, Cross-Cultural Communication, Machine Learning, Natural Spirit, Predictive Modeling, Visual Art, Water Art}, abstract = {In the field of visual arts, water is a special artistic element which symbolizes a profound understanding of nature. Through the examination of its expressive techniques, cultural significance, and close links to human emotions and philosophical ideas comprising painting, sculpture, installation art, photography, and digital media, this article explores the different types of water art and the power of its natural spirit. We employ machine learning methods to improve our traditional case studies and derive deeper, data-driven insights by utilizing a comprehensive dataset on audience viewpoints. First, based on emotional responses, ecological concern, and preferred art forms, unsupervised clustering reveals multiple audience archetypes, including aesthetics-driven and ecologically motivated. Next, classification models identify important demographic and ideological determinants by predicting people's likelihood to support or participate in water-themed events. By measuring hidden trends in viewer involvement, these computational methods work in combination to enhance our qualitative research and deepen our comprehension of water's function as a medium of natural spirit. In addition to providing insight into how water art promotes intercultural dialogue and environmental consciousness in a globalized setting, the results also highlight the usefulness of machine learning in enhancing empirical art studies.} }
@inproceedings{10.1145/3704137.3704142, title = {Understanding the relevance of parallelising machine learning algorithms using CUDA for sentiment analysis}, booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence}, pages = {28--38}, year = {2025}, isbn = {9798400718014}, doi = {10.1145/3704137.3704142}, url = {https://doi.org/10.1145/3704137.3704142}, author = {Chai, Dakun Mang and Moulitsas, Irene and Bisandu, Desmond Bala}, keywords = {CUDA, Machine Learning, Sentiment Analysis, Word Embedding}, abstract = {Sentiment classification is essential in natural language processing, leveraging machine learning algorithms to understand the sentiment expressed in textual data. Over the years, advancements in machine learning, particularly with Naive Bayes (NB) and Support Vector Machines (SVM), have tremendously improved sentiment classification. These models benefit from word embedding techniques such as Word2Vec and GloVe, which provide dense vector representations of words, capturing their semantic and syntactic relationships. This paper explores the parallelisation of NB and SVM models using CUDA on GPUs to enhance computational efficiency and performance. Despite the computational power offered by GPUs, the literature on parallelising machine learning methods, especially for sentiment classification, remains limited. Our work aims to fill this gap by comparing the performance of NB and SVM on CPU and GPU platforms, focusing on execution time and model accuracy. Our experiments demonstrate that NB outperforms SVM in execution time and overall efficiency, mainly when using GPU acceleration. The NB model consistently achieves higher accuracy, precision, and F1 scores with Word2Vec and GloVe embeddings. The results show the importance of leveraging GPU acceleration using varying numbers of threads per block for large-scale sentiment analysis and laying the foundation for parallelising sentiment classification tasks.} }
@proceedings{10.1145/3700906, title = {IPMLP '24: Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, year = {2024}, isbn = {9798400707032} }
@inproceedings{10.1145/3630106.3658955, title = {Machine learning data practices through a data curation lens: An evaluation framework}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, pages = {1055--1067}, year = {2024}, isbn = {9798400704505}, doi = {10.1145/3630106.3658955}, url = {https://doi.org/10.1145/3630106.3658955}, author = {Bhardwaj, Eshta and Gujral, Harshit and Wu, Siyi and Zogheib, Ciara and Maharaj, Tegan and Becker, Christoph}, keywords = {data practices, dataset creation, datasets, datasheets, documentation, evaluation, machine learning, rubric, location = Rio de Janeiro, Brazil}, abstract = {Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.} }
@inproceedings{10.1145/3733155.3734911, title = {Rent Price Prediction Using Machine Learning with Public Land Price, Geospatial, and Demographic Projection Data}, booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {439--445}, year = {2025}, isbn = {9798400714023}, doi = {10.1145/3733155.3734911}, url = {https://doi.org/10.1145/3733155.3734911}, author = {Sato, Goei and Nishiyama, Hiroyuki}, keywords = {Rent Prediction, Machine Learning, National Land Numerical Information}, abstract = {This study investigated the use of “National Land Numerical Information” provided by the Ministry of Land, Infrastructure, Transport and Tourism together with rental property data to improve rent price forecasting through machine learning. We integrated public land price data, geospatial information, and demographic projections (including future population by age group) with basic property attributes (for example, floor area, building age, and amenities). Experimental results using a LightGBM model showed that including land price data and population projections significantly enhanced predictive accuracy, reducing the root mean squared error (RMSE) from 14,319.50 (basic features only) to 13,072.48, a performance improvement of approximately 8.7\%. Feature importance and SHAP analyses revealed that well-known individual property factors (for example, floor area and building age) remained central to rent formation; however, macro-level indicators such as land price and future demographic composition also played critical roles, particularly in capturing mid- to long-term market dynamics. In contrast, including detailed facility information (for example, distances to schools or medical centers) did not yield substantial gains, suggesting that land prices might already encapsulate regional convenience. These findings highlighted a practical yet powerful approach to rent prediction that centered on easily updated public data sources. They also underscored the importance of model interpretability through methods such as SHAP for real-world applications in real estate technology and urban planning. Future research directions include tailoring models to local conditions, incorporating time-series data to capture ongoing changes, and enhancing explainability to support diverse stakeholders.} }
@article{10.1145/3630104, title = {A Comprehensive Survey on Automated Machine Learning for Recommendations}, journal = {ACM Trans. Recomm. Syst.}, volume = {2}, year = {2024}, doi = {10.1145/3630104}, url = {https://doi.org/10.1145/3630104}, author = {Chen, Bo and Zhao, Xiangyu and Wang, Yejing and Fan, Wenqi and Guo, Huifeng and Tang, Ruiming}, keywords = {Automated machine learning, recommender systems, neural networks}, abstract = {Deep recommender systems (DRS) are critical for current commercial online service providers, which address the issue of information overload by recommending items that are tailored to the user’s interests and preferences. They have unprecedented feature representations effectiveness and the capacity of modeling the non-linear relationships between users and items. Despite their advancements, DRS models, like other deep learning models, employ sophisticated neural network architectures and other vital components that are typically designed and tuned by human experts. This article will give a comprehensive summary of automated machine learning (AutoML) for developing DRS models. We first provide an overview of AutoML for DRS models and the related techniques. Then we discuss the state-of-the-art AutoML approaches that automate the feature selection, feature embeddings, feature interactions, and model training in DRS. We point out that the existing AutoML-based recommender systems are developing to a multi-component joint search with abstract search space and efficient search algorithm. Finally, we discuss appealing research directions and summarize the survey.} }
@article{10.5555/3722577.3722849, title = {Boundary constrained Gaussian processes for robust physics-informed machine learning of linear partial differential equations}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Dalton, David and Lazarus, Alan and Gao, Hao and Husmeier, Dirk}, keywords = {physics-informed machine learning, Gaussian processes, partial differential equations, boundary-value problems, inverse problems}, abstract = {We introduce a framework for designing boundary constrained Gaussian process (BCGP) priors for exact enforcement of linear boundary conditions, and apply it to the machine learning of (initial) boundary value problems involving linear partial differential equations (PDEs). In contrast to existing work, we illustrate how to design boundary constrained mean and kernel functions for all classes of boundary conditions typically used in PDE modelling, namely Dirichlet, Neumann, Robin and mixed conditions. Importantly, this is done in a manner which allows for both forward and inverse problems to be naturally accommodated. We prove that the BCGP kernel has a universal representational capacity under Dirichlet conditions, and establish a formal equivalence between BCGPs and boundary-constrained neural networks (BCNNs) of infinite width. Finally, extensive numerical experiments are performed involving several linear PDEs, the results of which demonstrate the effectiveness and robustness of BCGP inference in the presence of sparse, noisy data.} }
@inproceedings{10.1145/3625343.3625356, title = {Machine Learning based Intrusion Detection System for IoT Applications using Explainable AI}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625356}, url = {https://doi.org/10.1145/3625343.3625356}, author = {Mukhtar Bhatti, Muhammad Asim and Awais, Muhammad and Iqtidar, Aamna}, keywords = {Artificial Intelligence (AI), Decision Tree, Internet of things, Intrusion detection system (IDS), Machine learning (ML), Multilayer Perceptron (MLP), XGBoost classifier, location = Bangkok, Thailand}, abstract = {This research focuses on studying the classification performance of a Machine Learning-based Intrusion Detection System (IDS) using the UNSW-NB15 dataset. The effectiveness of three classifiers - Decision Tree, Multilayer Perceptron (MLP), and XGBoost - was analyzed to determine their accuracy in identifying attacks and normal network traffic. The experimental results revealed that Decision Tree achieved an accuracy of 96.5\%, MLP achieved an accuracy of 89.83\%, and XGBoost achieved an accuracy of 89.9\%. Additionally, the Explanability of the machine learning models was analyzed, highlighting the differences in interpretability among the classifiers. It was observed that Decision Tree provided better Explanability, but lower accuracy compared to MLP and XGBoost. Overall, this research contributes to our comprehension of the performance and Explanability of three different machine learning classifiers for intrusion detection. The findings can provide valuable insights for choosing suitable classifiers that align with the specific priorities and requirements of the IDS system.} }
@article{10.14778/3648160.3648172, title = {Optimizing Data Acquisition to Enhance Machine Learning Performance}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {1310--1323}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3648160.3648172}, url = {https://doi.org/10.14778/3648160.3648172}, author = {Wang, Tingting and Huang, Shixun and Bao, Zhifeng and Culpepper, J. Shane and Dedeoglu, Volkan and Arablouei, Reza}, abstract = {In this paper, we study how to acquire labeled data points from a large data pool to enrich a training set for enhancing supervised machine learning (ML) performance. The state-of-the-art solution is the clustering-based training set selection (CTS) algorithm, which initially clusters the data points in a data pool and subsequently selects new data points from clusters. The efficiency of CTS is constrained by its frequent retraining of the target ML model, and the effectiveness is limited by the selection criteria, which represent the state of data points within each cluster and impose a restriction of selecting only one cluster in each iteration. To overcome these limitations, we propose a new algorithm, called CTS with incremental estimation of adaptive score (IAS). IAS employs online learning, enabling incremental model updates by using new data, and eliminating the need to fully retrain the target model, and hence improves the efficiency. To enhance the effectiveness of IAS, we introduce adaptive score estimation, which serves as novel selection criteria to identify clusters and select new data points by balancing trade-offs between exploitation and exploration during data acquisition. To further enhance the effectiveness of IAS, we introduce a new adaptive mini-batch selection method that, in each iteration, selects data points from multiple clusters rather than a single cluster, hence eliminating the potential bias due to using only one cluster. By integrating this method into the IAS algorithm, we propose a novel algorithm termed IAS with adaptive mini-batch selection (IAS-AMS). Experimental results highlight the superior effectiveness of IAS-AMS, with IAS also outperforming other competing algorithms. In terms of efficiency, IAS takes the lead, while the efficiency of IAS-AMS is on par with that of the existing CTS algorithm.} }
@inproceedings{10.1145/3620678.3624790, title = {Is Machine Learning Necessary for Cloud Resource Usage Forecasting?}, booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing}, pages = {544--554}, year = {2023}, isbn = {9798400703874}, doi = {10.1145/3620678.3624790}, url = {https://doi.org/10.1145/3620678.3624790}, author = {Christofidi, Georgia and Papaioannou, Konstantinos and Doudali, Thaleia Dimitra}, keywords = {Cloud Computing, Data Persistence, Forecasting, Long Short Term Memory, Machine Learning, Persistent Forecast, Prediction, Resource Usage, location = Santa Cruz, CA, USA}, abstract = {Robust forecasts of future resource usage in cloud computing environments enable high efficiency in resource management solutions, such as autoscaling and overcommitment policies. Production-level systems use lightweight combinations of historical information to enable practical deployments. Recently, Machine Learning (ML) models, in particular Long Short Term Memory (LSTM) neural networks, have been proposed by various works, for their improved predictive capabilities. Following this trend, we train LSTM models and observe high levels of prediction accuracy, even on unseen data. Upon meticulous visual inspection of the results, we notice that although the predicted values seem highly accurate, they are nothing but versions of the original data shifted by one time step into the future. Yet, this clear shift seems to be enough to produce a robust forecast, because the values are highly correlated across time. We investigate time series data of various resource usage metrics (CPU, memory, network, disk I/O) across different cloud providers and levels, such as at the physical or virtual machine-level and at the application job-level. We observe that resource utilization displays very small variations in consecutive time steps. This insight can enable very simple solutions, such as data shifts, to be used for cloud resource forecasting and deliver highly accurate predictions. This is the reason why we ask whether complex machine learning models are even necessary to use. We envision that practical resource management systems need to first identify the extent to which simple solutions can be effective, and resort to using machine learning to the extent that enables its practical use.} }
@inproceedings{10.1145/3731806.3731818, title = {Prediction of Standard Minute Value Using Machine Learning in the Garment Industry}, booktitle = {Proceedings of the 2025 14th International Conference on Software and Computer Applications}, pages = {148--153}, year = {2025}, isbn = {9798400710124}, doi = {10.1145/3731806.3731818}, url = {https://doi.org/10.1145/3731806.3731818}, author = {Hemal, MD Tasadul Islam and Sin, Yew Keong and Ahmad Osman, Ahmad Farimin and Tan, Yi-fei}, keywords = {Garment industry, Mean square error (MSE), Regression, Squared correlation coefficient (R2), Standard minute value (SMV)}, abstract = {The garment industry is a critical component of the global economy, and it has been a major driver of economic growth. The industry faces various challenges, including labour practices, one of which involves the estimation of the standard minute value (SMV). The SMV, representing the time required for a qualified operator to complete a task under standard conditions with appropriate allowances, is often estimated primarily based on engineers' experience. Different individuals may predict the SMV differently. Advancements in technology are expected to standardize SMV prediction and make the production processes more efficient. By knowing SMV accurately in advance, the garment production processes can be improved, thereby reducing cost of producing clothes. In this research, data are collected from a ready-made garment (RMG) industry, with the aim to apply machine learning (ML) based regression models to predict SMV outcomes without depending on industrial engineers. Among the regression models, linear regression (LR), decision tree regression (DTR), and random forest regression (RFR) are chosen for predicting SMV. For the model performance evaluation, mean square error (MSE) and squared correlation coefficient (R2) are calculated. The testing results showed that MSE values fall within 0.004 to 0.006 and R2 range from 0.77 to 0.86, indicating that ML-based regression models are quite accurate in predicting SMV. In addition to providing an efficient method for predicting SMV, this research helps in reducing manpower requirements, enhancing productivity, and minimizing losses.} }
@article{10.1145/3561381, title = {Open-world Machine Learning: Applications, Challenges, and Opportunities}, journal = {ACM Comput. Surv.}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3561381}, url = {https://doi.org/10.1145/3561381}, author = {Parmar, Jitendra and Chouhan, Satyendra and Raychoudhury, Vaskar and Rathore, Santosh}, keywords = {Open-world Machine Learning, continual machine learning, incremental learning, open-world image and text classification}, abstract = {Traditional machine learning, mainly supervised learning, follows the assumptions of closed-world learning, i.e., for each testing class, a training class is available. However, such machine learning models fail to identify the classes, which were not available during training time. These classes can be referred to as unseen classes. Open-world Machine Learning (OWML) is a novel technique, which deals with unseen classes. Although OWML is around for a few years and many significant research works have been carried out in this domain, there is no comprehensive survey of the characteristics, applications, and impact of OWML on the major research areas. In this article, we aimed to capture the different dimensions of OWML with respect to other traditional machine learning models. We have thoroughly analyzed the existing literature and provided a novel taxonomy of OWML considering its two major application domains: Computer Vision and Natural Language Processing. We listed the available software packages and open datasets in OWML for future researchers. Finally, the article concludes with a set of research gaps, open challenges, and future directions.} }
@article{10.14778/3685800.3685859, title = {Demonstration of MaskSearch: Efficiently Querying Image Masks for Machine Learning Workflows}, journal = {Proc. VLDB Endow.}, volume = {17}, pages = {4297--4300}, year = {2024}, issn = {2150-8097}, doi = {10.14778/3685800.3685859}, url = {https://doi.org/10.14778/3685800.3685859}, author = {Wei, Lindsey Linxi and Yeung, Chung Yik Edward and Yu, Hongjian and Zhou, Jingchuan and He, Dong and Balazinska, Magdalena}, abstract = {We demonstrate MaskSearch, a system designed to accelerate queries over databases of image masks generated by machine learning models. MaskSearch formalizes and accelerates a new category of queries for retrieving images and their corresponding masks based on mask properties, which support various applications, from identifying spurious correlations learned by models to exploring discrepancies between model saliency and human attention. This demonstration makes the following contributions: (1) the introduction of MaskSearch's graphical user interface (GUI), which enables interactive exploration of image databases through mask properties, (2) hands-on opportunities for users to explore MaskSearch's capabilities and constraints within machine learning workflows, and (3) an opportunity for conference attendees to understand how MaskSearch accelerates queries over image masks.} }
@proceedings{10.1145/3670474, title = {MLCAD '24: Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998} }
@article{10.5555/3648699.3649030, title = {Fair data representation for machine learning at the pareto frontier}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Xu, Shizhou and Strohmer, Thomas}, keywords = {statistical parity, equalized odds, Wasserstein barycenter, Wasserstein geodesics, conditional expectation estimation}, abstract = {As machine learning powered decision-making becomes increasingly important in our daily lives, it is imperative to strive for fairness in the underlying data processing. We propose a pre-processing algorithm for fair data representation via which supervised learning results in estimations of the Pareto frontier between prediction error and statistical disparity. In particular, the present work applies the optimal affine transport to approach the post-processing Wasserstein barycenter characterization of the optimal fair L2-objective supervised learning via a pre-processing data deformation. Furthermore, we show that the Wasserstein geodesics from the conditional (on sensitive information) distributions of the learning outcome to their barycenter characterize the Pareto frontier between L2-loss and the average pairwise Wasserstein distance among sensitive groups on the learning outcome. Numerical simulations underscore the advantages: (1) the pre-processing step is compositive with arbitrary conditional expectation estimation supervised learning methods and unseen data; (2) the fair representation protects the sensitive information by limiting the inference capability of the remaining data with respect to the sensitive data; (3) the optimal affine maps are computationally efficient even for high-dimensional data.} }
@inproceedings{10.1145/3724154.3724270, title = {Research and Application of Human Resources Demand Forecasting Model Based on Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {696--701}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724270}, url = {https://doi.org/10.1145/3724154.3724270}, author = {Ma, Xuanke}, keywords = {Employee turnover prediction, Human resources management, Machine learning, Random forest model}, abstract = {This study demonstrates the potential of applying machine learning techniques in the field of human resource management by applying the random forest model to human resource demand forecasting, specifically employee turnover forecasting. The study used a dataset containing several features such as age, education level, department, gender and years of service to construct and train the model. The feature importance analysis shows that seniority is a key factor in prediction accuracy. The model achieved an accuracy of 85\% on the test dataset with an AUC-ROC value of 0.90, demonstrating its efficiency and accuracy in predicting employee turnover behaviour. In addition, the study further validated the reliability of the model through error analysis and performance evaluation. The results show that using the random forest model to predict employee turnover can provide strong data support for corporate human resource management and help companies better understand and predict changes in human resource needs.} }
@inproceedings{10.1145/3723178.3723278, title = {Prediction of IMDb Rating in Netflix Shows Using Machine Learning Algorithms}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {754--761}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723278}, url = {https://doi.org/10.1145/3723178.3723278}, author = {Fariha, Farzana and Irom Rushee, Kawser and Sharier Khan Arnop, Md. Fahim}, abstract = {Netflix is a streaming online service that contains TV shows, series and movies from various countries and categories. People of all ages can seamlessly watch their desired tv series or movies according to various genres. Regarding the prevailing circumstance, controversies arise while choosing the preferred show. This paper attempts to predict the rating of Netflix shows using Machine Learning Algorithms by relating Netflix movies with Internet Movie Database (IMDb) ratings. We performed research regarding the rating of Netflix which will portray the IMDb rating for every show. We have applied three algorithms in our database with a standard accuracy, which are Artificial Neural Network (ANN) = 0.97(97\%) Random Forest = 0.96(96\%) and Decision Tree =1.0(100\%). Among these three algorithms, the decision tree was the most accurate algorithm to predict the ratings. The purpose of our research is to assist people with watching movies and shows without any trouble. Moreover, people can easily choose according to their preferred categories. This research aims to improve the user experience of Netflix.} }
@inproceedings{10.1145/3628096.3628740, title = {An Interface Design Methodology for Serving Machine Learning Models}, booktitle = {Proceedings of the 4th African Human Computer Interaction Conference}, pages = {12--14}, year = {2024}, isbn = {9798400708879}, doi = {10.1145/3628096.3628740}, url = {https://doi.org/10.1145/3628096.3628740}, author = {Ogbuju, Emeka Emmanuel and Ihinkalu, Olalekan and Oladipo, Francisca}, keywords = {AI, Interface, ML, Machine Learning, Model, location = East London, South Africa}, abstract = {Interface design is an essential part of the machine learning (ML) modelling process, which enable users to interact effectively with developed models for solutions/decision making. This paper proposes an interface design methodology for serving ML models by taking into account specific user requirements of the model. The aim of the proposed methodology is to close the gap between the interaction of the ML developers and the end-users by providing user-friendly ML model interfaces.} }
@inproceedings{10.1145/3703323.3703698, title = {Imbalanced Multi-Class Research Article Classification using Sentence Transformers and Machine Learning Algorithms}, booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)}, pages = {309--310}, year = {2025}, isbn = {9798400711244}, doi = {10.1145/3703323.3703698}, url = {https://doi.org/10.1145/3703323.3703698}, author = {Gowhar, Saliq and Kempaiah, Praveen and Kamath S, Sowmya and Sugumaran, Vijayan}, keywords = {Document classification, Machine Learning, Sentence Transformers, Natural Language Processing}, abstract = {Categorizing scientific articles into specific research fields is a challenging problem, considering the volume and variety of published literature. However, existing classification systems often suffer from limitations regarding taxonomy or the models used for classification. This article explores approaches built on Sentence Transformer embeddings combined with Machine Learning algorithms to classify articles into 123 predefined classes, with the dataset being heavily imbalanced in nature. The effectiveness of Large Language Models (LLMs) for generating synthetic data is also experimented with, along with synonym augmentation and SMOTE. The best-performing model, the One vs Rest classifier trained on MP-Net sentence embeddings with SMOTE, achieved an accuracy of 77\%, and outperformed all the other models.} }
@inproceedings{10.1145/3615834.3615845, title = {Sensor-Based Detection of Food Hypersensitivity Using Machine Learning}, booktitle = {Proceedings of the 8th International Workshop on Sensor-Based Activity Recognition and Artificial Intelligence}, year = {2023}, isbn = {9798400708169}, doi = {10.1145/3615834.3615845}, url = {https://doi.org/10.1145/3615834.3615845}, author = {Jablonski, Lennart and Jensen, Torge and Ahlemann, Greta M. and Huang, Xinyu and Tetzlaff-Lelleck, Vivian V. and Piet, Artur and Schmelter, Franziska and Dinkler, Valerie S. and Sina, Christian and Grzegorzek, Marcin}, keywords = {adverse reaction to food, carbohydrate malassimilation, classification, explainable AI, feature engineering, food hypersensitivity, machine learning, precision nutrition, random forest, sensor-based, time series analysis, location = L\"ubeck, Germany}, abstract = {The recognition of physiological reactions with the help of machine learning methods already plays a major role in many research areas, but is still little represented in the field of food hypersensitivity recognition. The present work addresses the question of how food hypersensitivity can be detected by analysing sensor data with explainable machine learning algorithms. In a first step, the Empatica E4 wristband, a wearable device that can be easily integrated into everyday life, collects raw data on various physiological patterns, and algorithms are implemented to extract a variety of features from the raw data. Subsequently, machine learning methods are used to target this classification problem and examine how food hypersensitivity can be detected using these objectively measurable features. In a subject-independent setup, an accuracy of 91\% could be achieved, which provides a promising basis for a new non-invasive and objectively measurable method to detect food hypersensitivity.} }
@inproceedings{10.1145/3654823.3654870, title = {Machine Learning Based Early Rejection of Low Performance Cells in Li Ion Battery Production}, booktitle = {Proceedings of the 2024 3rd Asia Conference on Algorithms, Computing and Machine Learning}, pages = {251--256}, year = {2024}, isbn = {9798400716416}, doi = {10.1145/3654823.3654870}, url = {https://doi.org/10.1145/3654823.3654870}, author = {Xu, Xukuan and Moeckel, Michael J.}, keywords = {LIB cell production, Machine learning, flexible production, quality control, location = Shanghai, China}, abstract = {Lithium-ion battery cell production is conducted through a multistep production process which suffers from a notable scrap rate. Machine learning (ML) based process monitoring provides solutions to mitigate the impact of substantial scrap rates by repeated multifactorial quality predictions (virtual quality gates) along the process line. This enables an early rejection of battery cells which are unlikely to reach required specifications, avoids further waste of resources at later process steps and simplifies recycling of rejected cells. A hierarchical architecture is used to apply ML algorithms first for process-adapted feature extraction which is guided by a priori knowledge on typical production anomalies. In a second step, these features are correlated with end-of-line quality control data using explainable ML methods. The resulting predictions may lead to pass or fail of a battery cell, or -in the context of flexible production- may also trigger adjustments of later process steps to compensate for detected deficiencies. An example ML based quality control concept is illustrated for a pilot battery cell production line.} }
@inproceedings{10.1145/3746469.3746509, title = {Research on the Teaching System Combining Virtual Reality Technology and Machine Learning in Civil Engineering Education}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area Education Digitalization and Computer Science International Conference}, pages = {242--246}, year = {2025}, isbn = {9798400713811}, doi = {10.1145/3746469.3746509}, url = {https://doi.org/10.1145/3746469.3746509}, author = {Han, Luyue and Ren, Xiaoli and Zhang, Xiaolin and Zhou, Song and Wang, Yuxin}, keywords = {Civil Engineering, Intelligent Interaction, Machine Learning, Teaching System, Virtual Reality}, abstract = {The combination of virtual reality technology and machine learning technology brings new opportunities and challenges to civil engineering education. Based on the teaching practices of Chongqing University and Northeastern University, a teaching system combining virtual reality technology and machine learning algorithms is constructed. The system relies on 3D modeling and deep learning algorithms to achieve visualization and intelligent interaction of engineering entities. Experiments show that students can understand complex engineering concepts more intuitively, master design principles and participate in virtual construction processes with the help of this system. Compared with traditional teaching, the combined system significantly improves students' learning interest and practical ability. At the same time, it is found that the degree of virtual reality immersion and the accuracy of machine learning algorithms are key factors affecting teaching effectiveness.} }
@inproceedings{10.1145/3659677.3659738, title = {Machine Learning to Predict Law Graduates of Universit Thomas Sankara}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659738}, url = {https://doi.org/10.1145/3659677.3659738}, author = {Konate, Brahima and Ouedraogo, Tounwendyam Fr\'ed\'eric}, keywords = {Campusfaso, Machine Learning, Prediction, guidance, law, location = Meknes, AA, Morocco}, abstract = {Improving the academic success rate of students is of interest in public universities in Burkina Faso. The guidance of new student is an important step in this process. Predicting the success of students graduating in the Faculty of Law of Universite Thomas Sankara (UTS) aims ultimately to build an intelligent guidance process to help improve the success rate of students. This intelligent process will improve the algorithm currently used for the guidance of new students in public universities in Burkina Faso. For this first step of our research a literature review on similar problems allowed us to select 4 models, including logistic regression, KNN, decision tree and SVM. In this article, data from 2632 new students guided to UTS in 2018 by Campusfaso were used. The features focused the grades obtained in the high school diploma exam and on the demographic data of new students. The article describes our methodological approach and presents the results following the implementation of the 4 machine learning models used. The results show that the SVM model gives the best score in predicting success in obtaining a bachelor’s degree for a student guided to UTS by Campusfaso. Implementing this study could help to improve the success rate of new students in the faculty of law of UTS.} }
@inproceedings{10.1145/3748777.3748792, title = {ImPORTance - Machine Learning-Driven Analysis of Global Port Significance and Network Dynamics for Improved Operational Efficiency}, booktitle = {Proceedings of the 19th International Symposium on Spatial and Temporal Data}, pages = {66--75}, year = {2025}, doi = {10.1145/3748777.3748792}, url = {https://doi.org/10.1145/3748777.3748792}, author = {Carlini, Emanuele and Di Gangi, Domenico and de Lira, Vinicius Monteiro and Kavalionak, Hanna and Soares, Amilcar and Spadon, Gabriel}, keywords = {AIS, Ports Network, Port Centrality, Port Importance, Connectivity}, abstract = {Seaports play a crucial role in the global economy, and researchers have sought to understand their significance through various studies. In this paper, we aim to explore the common characteristics shared by important ports by analyzing the network of connections formed by vessel movement among them. To accomplish this task, we adopt a bottom-up network construction approach that combines three years’ worth of AIS (Automatic Identification System) data from around the world, constructing a Ports Network that represents the connections between different ports. Through this representation, we utilize machine learning to assess the relative significance of various port features. Our model examined such features and revealed that geographical characteristics and the port’s depth are indicators of a port’s importance to the Ports Network. Accordingly, this study employs a data-driven approach and utilizes machine learning to provide a comprehensive understanding of the factors contributing to the extent of ports. Our work aims to inform decision-making processes related to port development, resource allocation, and infrastructure planning within the industry.} }
@inproceedings{10.1145/3748825.3748936, title = {Constructing a hepatocellular carcinoma diagnosis model based on multiple machine learning algorithms and interpretable models}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {716--722}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748936}, url = {https://doi.org/10.1145/3748825.3748936}, author = {Chen, Junjie and Wen, Xianghua and Liu, Zhao and Hu, Huanjun}, keywords = {Diagnostic model, Liver cancer, Machine learning}, abstract = {Hepatocellular carcinoma is the most common type of primary liver cancer and ranks third in terms of cancer impact, making it one of the significant causes of cancer-related mortality. Identifying biomarkers and therapeutic targets associated with hepatocellular carcinoma is crucial. The effective screening and construction of diagnostic models to enhance the early diagnosis of hepatocellular carcinoma has been a major issue. To effectively address these challenges and improve the applicability of the models, our study performed differential expression analysis on sepsis-related data, constructed a protein-protein interaction network, and identified core genes associated with sepsis. We applied various machine learning algorithms to analyze the expression data of these key genes and selected effective models based on ROC curves and residual value metrics. Using the best model, we developed a diagnostic nomogram containing five genes with the highest importance scores, achieving an accuracy rate of up to 98.5\%, making it particularly suitable for predicting a risk rate of 60\%-95\% for hepatocellular carcinoma.} }
@inproceedings{10.1145/3631802.3631808, title = {Common Errors in Machine Learning Projects: A Second Look}, booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research}, year = {2024}, isbn = {9798400716539}, doi = {10.1145/3631802.3631808}, url = {https://doi.org/10.1145/3631802.3631808}, author = {Zimmermann, Renato Magela and Allin, Sonya and Zhang, Lisa}, keywords = {AI, Common Errors, Data science, Machine learning, Misconceptions, location = Koli, Finland}, abstract = {While machine learning (ML) has proved impactful in many disciplines, design decisions involved in building ML models are difficult for novices to make, and mistakes can cause harm. Prior work by Skripchuk et\&nbsp;al. [35] identified common errors made by ML students via qualitative analysis of open-ended ML assessments, but their sample was limited to a single institution, course, and assessment setting. Our work is an extended, conceptual replication of this work to understand the common errors made by machine learning students. We use a mixed-method approach to analyze errors in 30 final project reports in an undergraduate machine learning course. The final reports describe the model-building process for a classification task, where students build models on a complex data set with numerical, categorical, ordinal and text features. Our choice to analyze project reports (rather than code) allows us to uncover design errors via how students justify their methodology. Our project task is to achieve the best test accuracy on an unseen test set; thus, as a way to validate these common errors, we identify the association between these errors and the model’s test accuracy performance. Common errors we find include those consistent with Skripchuk et\&nbsp;al. [35], for example issues with data processing, hyperparameter tuning, and model selection. In addition, our focus on design error exposes other common errors, for example where students use certain kinds of features (e.g., bag of words representations) only with particular models (e.g., Naive Bayes). We call these latter types of errors model misconceptions, and such errors are associated with lower test accuracy. Some of these errors are also present in work by practitioners. Others reflect a difficulty by students to make correct connections between ML concepts and achieve the relational level of the SOLO taxonomy. We identify areas of opportunity to improve machine learning pedagogy, particularly related to data processing, data leakage, hyperparameters, nonsensical outputs, and disentangling data decisions from model decisions.} }
@inproceedings{10.1145/3637528.3671472, title = {Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6627--6632}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671472}, url = {https://doi.org/10.1145/3637528.3671472}, author = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Zheng, Chenguang and Chen, Hongzhi and Cheng, James}, keywords = {gnn, graph, graph neural network, structure, subgraph, system, vertex, location = Barcelona, Spain}, abstract = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems.} }
@inproceedings{10.1145/3650215.3650326, title = {Research on Financial Fraud Identification Model Based on Privacy-Preserving Machine Learning}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {628--632}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650326}, url = {https://doi.org/10.1145/3650215.3650326}, author = {Jiao, Li and Li, Hui}, abstract = {Financial fraud will not only mislead investors and reduce investor confidence, but also disrupt market order and hinder the orderly development of the capital market. In order to solve the problem of financial fraud, based on the data mining process, a process framework of the financial fraud identification method based on machine learning is proposed, which includes three steps: sample and feature selection, data preparation, and model construction and analysis. Based on the process framework of the financial fraud identification method based on machine learning, GBDT methods are used to build models respectively, and the model effect is evaluated based on indicators such as recall rate. Due to the particularity of financial reporting fraud, the model should aim to identify fraud samples as much as possible, so the recall rate is a major evaluation index in this article. The recall rate of the GBDT model is better when based on fewer indicators. Therefore, after comprehensively considering factors such as data acquisition costs, this article recommends using the GBDT model to identify financial fraud.} }
@inproceedings{10.1145/3639478.3643069, title = {Interpretable Software Maintenance and Support Effort Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {288--289}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3643069}, url = {https://doi.org/10.1145/3639478.3643069}, author = {Haldar, Susmita and Capretz, Luiz Fernando}, keywords = {maintenance and support effort prediction, explainable machine learning models, model agnostic interpretation, location = Lisbon, Portugal}, abstract = {Software maintenance and support efforts consume a significant amount of the software project budget to operate the software system in its expected quality. Manually estimating the total hours required for this phase can be very time-consuming, and often differs from the actual cost that is incurred. The automation of these estimation processes can be implemented with the aid of machine learning algorithms. The maintenance and support effort prediction models need to be explainable so that project managers can understand which features contributed to the model outcome. This study contributes to the development of the maintenance and support effort prediction model using various tree-based regression machine-learning techniques from cross-company project information. The developed models were explained using the state-of-the-art model agnostic technique SHapley Additive Explanations (SHAP) to understand the significance of features from the developed model. This study concluded that staff size, application size, and number of defects are major contributors to the maintenance and support effort prediction models.} }
@inproceedings{10.1145/3628797.3628930, title = {A Machine Learning-Based Anomaly Packets Detection for Smart Home}, booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology}, pages = {816--823}, year = {2023}, isbn = {9798400708916}, doi = {10.1145/3628797.3628930}, url = {https://doi.org/10.1145/3628797.3628930}, author = {Nguyen, Thanh Binh and Nguyen, Duc Dang Khoi and Le Nguyen, Binh Nguyen and Le, Tan}, keywords = {Anomaly Detection, Cybersecurity, IoT-23 Dataset, Machine Learning, Smart Homes, location = Ho Chi Minh, Vietnam}, abstract = {The advent of smart homes has revolutionized residential living, integrating advanced technologies and intelligent devices to create secure, comfortable, and efficient environments. However, this integration of diverse smart devices has brought significant cybersecurity challenges. Detecting and analyzing abnormal network packets have become paramount, signifying potential intrusions, malicious activities, or system errors and ensuring the security and stability of smart home systems. Machine learning techniques, such as Decision Trees, Support Vector Machines (SVM), Convolutional Neural Networks (CNN), K-Nearest Neighbors (KNN), Recurrent Neural Networks (RNN), and Random Forests, have shown promise in addressing these challenges. However, most research has concentrated on anomaly detection rather than malicious activity in smart homes. The vast datasets collected from various scenarios pose methodological and algorithmic challenges for applying machine learning techniques. To fill these research gaps, our study introduces traditional machine-learning methods for detecting abnormal network packets in smart homes using the IoT-23 dataset. It involves preprocessing the dataset, extracting relevant features, and training various machine learning models. The correlation matrix helps validate the feature selection of the best models based on performance metrics like precision, F1-score, recall, accuracy ratio, training score, and training time cost. Additionally, the study classifies 12 types of malicious malware across different machine learning models, considering performance within the context of smart home devices. This study implements real-time anomaly detection on the Raspberry Pi using packet captures and Zeek flowmeter methods. The findings contribute insights into models suitable for smart home security. In addition, our research enhances the understanding and application of machine learning methods for bolstering security in smart homes.} }
@inproceedings{10.1145/3627106.3627188, title = {PAVUDI: Patch-based Vulnerability Discovery using Machine Learning}, booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference}, pages = {704--717}, year = {2023}, isbn = {9798400708862}, doi = {10.1145/3627106.3627188}, url = {https://doi.org/10.1145/3627106.3627188}, author = {Ganz, Tom and Imgrund, Erik and H\"arterich, Martin and Rieck, Konrad}, keywords = {Patches, Program Representations, Vulnerability Discovery, location = Austin, TX, USA}, abstract = {Machine learning has been increasingly adopted for automatic security vulnerability discovery in research and industry. The ability to automatically identify and prioritize bugs in patches is crucial to organizations seeking to defend against potential threats. Previous works, however only consider bug discovery on statement, function or file level. How one would apply them to patches in realistic scenarios remains unclear. This paper presents a novel deep learning-based approach leveraging an interprocedural patch graph representation and graph neural networks to analyze software patches for identifying and locating potential security vulnerabilities. We modify current state-of-the-art learning-based static analyzers to be applicable to patches and show that our patch-based vulnerability discovery method, a context and flow-sensitive learning-based model, has a more than increased detection performance, is twice as robust against concept drift after model deployment and is particularly better suited for analyzing large patches. In comparison, other methods already lose their efficiency when a patch touches more than five methods.} }
@inproceedings{10.1145/3716554.3716617, title = {Optimizing Sample Size in Machine Learning: Balancing Complexity and Generalization}, booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics}, pages = {413--418}, year = {2025}, isbn = {9798400713170}, doi = {10.1145/3716554.3716617}, url = {https://doi.org/10.1145/3716554.3716617}, author = {Karapiperis, Dimitrios and Feretzakis, Georgios and Kalles, Dimitris and Verykios, Vassilios}, abstract = {This paper introduces a new direction in estimating an optimal sample size for a binary classification problem using two theoretical frameworks, namely Rademacher complexity and the PAC-Bayesian generalization bound. First, the Rademacher complexity provides a measure of the capacity of a hypothesis class, characterizing its risk of overfitting, while the second-the PAC-Bayesian generalization bound-provides a probabilistic upper bound on the generalization error from a Bayesian viewpoint. We unify these two bounds to get a tighter generalization error bound that will enable a closer estimate of the optimal sample size in achieving model performance that one can count on. The experimental results showed that the combination of both bounds allows a more thorough understanding of the generalization performance of the classifiers.} }
@inproceedings{10.1145/3695080.3695141, title = {A Method for Wal-Mart Sales Forecasting Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Cloud Computing and Big Data}, pages = {350--355}, year = {2024}, isbn = {9798400710223}, doi = {10.1145/3695080.3695141}, url = {https://doi.org/10.1145/3695080.3695141}, author = {Xu, Ruiheng}, abstract = {With the widespread use of big data science, there is a growing need for data-driven decision-making in the retail industry. As one of the largest retailers in the world, Walmart's sales forecasting through machine learning methods is crucial to its business operations, which directly affects inventory management, supply chain optimization, and promotional strategy development and helps the company to save huge amounts of costs. In this paper, a prediction method combining multiple machine-learning techniques is proposed using historical sales data. The experimental results verify the method's effectiveness and significantly improve the accuracy of sales forecasting, which can provide an important reference for Walmart's inventory management and supply chain optimization.} }
@inproceedings{10.1145/3723936.3723949, title = {Application of machine learning in motion capture and technical movement diagnosis of football players}, booktitle = {Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis}, pages = {79--85}, year = {2025}, isbn = {9798400712234}, doi = {10.1145/3723936.3723949}, url = {https://doi.org/10.1145/3723936.3723949}, author = {Yang, Jie and Hu, Juanjuan}, keywords = {Football motion capture, deep learning model, machine learning, sports data analysis, technical motion diagnosis}, abstract = {This study combines motion capture systems and machine learning methods to explore the construction of high-precision capture and diagnosis models and their applications in football. Motion capture technology relies on optical sensors and inertial sensors to obtain multi-dimensional motion data of athletes. Through data preprocessing and feature extraction, machine learning algorithms classify and analyze movements. This paper designs a technical motion diagnosis model based on deep learning to identify detail deviations in football shooting movements and optimize the training process. Experimental verification shows that the model is superior to traditional methods in terms of data capture accuracy and diagnostic accuracy, and can effectively improve the standardization and expressiveness of football players' movements. The study summarizes the advantages of machine learning in technical motion capture and diagnosis, proposes future improvement directions, and provides intelligent solutions for football training and technical analysis.} }
@inbook{10.1145/3724504.3724517, title = {Extract Information to Improve Teaching Effectiveness Using Machine Learning Methods}, booktitle = {Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence}, pages = {73--77}, year = {2025}, isbn = {9798400711732}, url = {https://doi.org/10.1145/3724504.3724517}, author = {Feng, Changli and Wei, Haiyan and Li, Xin and Qiao, Sai}, abstract = {In traditional curriculum evaluations, educators often suggest improvements based on their impressions of teaching effectiveness and student achievement. However, this approach is not based on empirical evidence and can lead to subjective and potentially biased initiatives. To address this problem, our research introduces a data-driven approach to unearthing teaching reform strategies using the Random Forest classifier. Under this framework, students provide a comprehensive self-assessment score, as well as ratings in eight different areas. These scores are then used to calculate the achievement of each course objective, as outlined in the syllabus, which helps to identify areas that need enhancement and suggest targeted interventions. In addition, a threshold-based approach is used to classify the students' comprehensive assessment scores. These data, together with the self-assessment scores, make up the dataset of the Random Forest algorithm, which produces key features that influence classification. By examining these key characteristics, we can identify key teaching areas that need attention in future curricula and develop practical measures to be integrate into subsequent curricula.} }
@inproceedings{10.1145/3580305.3599206, title = {The Second Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5859--5860}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599206}, url = {https://doi.org/10.1145/3580305.3599206}, author = {Goldenberg, Dmitri and Ross, Chana and Meir Lador, Shir and Cheong, Lin Lee and Xu, Panpan and Sokolova, Elena and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit and Weil Modlinger, Amit and Potdar, Saloni}, keywords = {data science management, machine learning management, ml product development, location = Long Beach, CA, USA}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Second KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@inproceedings{10.1145/3650215.3650398, title = {Analysis of Machine Learning Methods for Water Quality Evaluation of Penaeus Vannamei}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {1032--1041}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650398}, url = {https://doi.org/10.1145/3650215.3650398}, author = {Peng, Xiaohong and Li, Zixin and Ma, Zebin and Zhang, Ying}, abstract = {In global aquaculture, Penaeus vannamei stands out due to its immense economic importance. Water quality, being pivotal for its successful cultivation, demands precise evaluation techniques. This research undertook a meticulous systematic review and, leveraging data mining, crafted a rich dataset of 50,000 water quality samples pertinent to Penaeus vannamei. Diving deep into machine learning, we assessed four key algorithms: decision tree, Bayesian, k-nearest neighbor, and support vector machine, each tailored for intricate multi-feature multi-classification challenges. Of these, the Gaussian Parsimonious Bayes-based model was distinguished by its superior accuracy and efficiency. This study successfully applied machine learning techniques to develop a reliable and efficient water quality evaluation model for Penaeus vannamei farming, offering a scientific tool for the aquaculture industry and facilitating more efficient, scientifically informed farming management. This research contributes an innovative scientific approach and theoretical foundation for the sustainable growth of the aquaculture industry.} }
@inproceedings{10.1145/3708359.3712105, title = {Robust Relatable Explanations of Machine Learning with Disentangled Cue-specific Saliency}, booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces}, pages = {1203--1231}, year = {2025}, isbn = {9798400713064}, doi = {10.1145/3708359.3712105}, url = {https://doi.org/10.1145/3708359.3712105}, author = {Abichandani, Harshavardhan Sunil and Zhang, Wencan and Lim, Brian Y}, keywords = {Explainable AI, misleading explanations, robust machine learning, vocal emotion}, abstract = {Concept-based explanations help users understand the relation between model predictions and meaningful cues. However, under noisy real-world conditions, data perturbations lead to distorted and deviated explanations. We hypothesize that these corruptions affect specific cues rather than all, so disentangling them may help reduce model dependency on degraded cues. For the application of explaining emotional speech recognition, we propose RobustRexNet to explain with disentangled and discretized saliency maps for separate speech cues (e.g., loudness, pitch) to improve robustness against noise. Modeling evaluations show that RobustRexNet improved both model performance and explanation faithfulness in noisy and privacy-preserving distortions. User studies further indicate that the robust explanations aligned better with human intuition and improved user emotion labeling under noise. This work contributes toward robust explainable AI to improve user trust under real-world conditions.} }
@inproceedings{10.1145/3607888.3608962, title = {Special Session: Machine Learning for Embedded System Design}, booktitle = {Proceedings of the 2023 International Conference on Hardware/Software Codesign and System Synthesis}, pages = {28--37}, year = {2024}, isbn = {9798400702891}, doi = {10.1145/3607888.3608962}, url = {https://doi.org/10.1145/3607888.3608962}, author = {Alcorta Lozano, Erika Susana and Gerstlauer, Andreas and Deng, Chenhui and Sun, Qi and Zhang, Zhiru and Xu, Ceyu and Wills, Lisa Wu and Sanchez Lopera, Daniela and Ecker, Wolfgang and Garg, Siddharth and Hu, Jiang}, keywords = {machine learning, embedded system design, location = Hamburg, Germany}, abstract = {Embedded systems are becoming increasingly complex, which has led to a productivity crisis in their design and verification. Although conventional design automation coupled with IP and platform reuse techniques have led to leaps in design productivity improvement, they face fundamental limits given that most design optimization and verification problems remain NP-hard and that reuse of pre-designed IP blocks and platforms inherently limits flexibility and optimality. At the same time, machine learning (ML) has recently made unprecedented advances and created phenomenal impact in various computing applications. In particular, application of ML techniques as a way to extract knowledge and learn from existing design, optimization and verification data has recently seen a lot of excitement and promise at lower physical and integrated circuit levels of abstraction. Using ML has the potential to similarly close the complexity gap in embedded system design, but corresponding ML-based approaches for embedded system optimization and verification at higher levels of abstraction are still at their infancy.This paper presents the current state of the art, along with opportunities and open challenges, in the application of ML methods for embedded system design and optimization. We discuss design and optimization at different levels of abstraction ranging from system-level modeling and optimization and high-level synthesis to RTL and micro-architecture design, bringing together perspectives from different communities in both academia and industry.} }
@inproceedings{10.1145/3748825.3748931, title = {Construction of a Diagnostic Model for Sepsis Based on Multiple Machine Learning Algorithms and Bioinformatics}, booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence}, pages = {684--689}, year = {2025}, isbn = {9798400714337}, doi = {10.1145/3748825.3748931}, url = {https://doi.org/10.1145/3748825.3748931}, author = {Chen, Junjie and Liu, Zhao and Hu, Huanjun}, keywords = {Bioinformatics, Machine learning, Nomogram, Sepsis}, abstract = {Sepsis is a severe clinical condition with an increasing demand for the identification of biomarkers and therapeutic targets associated with the disease. The current challenge in research lies in how to effectively extract key genes closely related to sepsis from complex biological data and to construct diagnostic models that improve early diagnosis and treatment outcomes. To address this, this study performed differential expression analysis on sepsis-related data, constructed a protein-protein interaction network, and identified core genes associated with sepsis. We applied various machine learning algorithms to analyze the expression data of these key genes and selected effective models based on ROC curves and residual value metrics. Using the best model, we developed a diagnostic nomogram containing five genes with the highest importance scores, which was validated in an external dataset, achieving an accuracy rate of up to 97.3\%. The individual prediction accuracy of the model genes IL10RB, FCER1G, FARS2, FHIT, and CCDC88C was lower than that of the overall diagnostic model, reflecting the effectiveness of our model. These findings provide new perspectives for improving the early identification and treatment of sepsis and hold significant implications for enhancing patient prognosis.} }
@inproceedings{10.1145/3637528.3672497, title = {Machine Learning for Clinical Management: From the Lab to the Hospital}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4736}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3672497}, url = {https://doi.org/10.1145/3637528.3672497}, author = {Gavald\`a, Ricard}, keywords = {chronic disease, fairness in health, healthcare analytics, healthcare resources, machine learning, patient safety, location = Barcelona, Spain}, abstract = {Population aging, increasing social demands, and rising costs of treatments are stressing healthcare systems to the point of risking the sustainability of universal and accessible healthcare. A hope in this dismal panorama is that there are large inefficiencies, and so opportunities for getting more from the same resources. To name a few, avoidable hospitalizations, unnecessary medication and tests, and lack of coordination among healthcare agents are estimated to cost several hundred billion euros per year in the EU. Technology can be useful for locating and reducing these inefficiencies, and within technology, the full exploitation of the data that the system collects to record its activity. In this talk, I will review the case for activity data analytics in healthcare, with two main considerations: 1) The need to include information about resources and costs in the models, in addition to clinical knowledge and patient outcomes, and 2) the need to use mostly data that healthcare organizations already collect and is not locked and distributed in silos. Fortunately, data collected for administrative and billing purposes, even though imperfect, partial, and low resolution, can be used to improve efficiency and safety, as well as fairness and equity.I will focus on the work carried out at Amalfi Analytics, a spin-off of my research group at UPC in Barcelona. On the one hand, we have addressed predictive management in hospitals, from influx to the emergency room to availability of surgical areas, beds, and staff. Anticipating activity, needs, and resource availability lets managers improve critical KPIs, e.g. waiting times, but also reduce staff stress, which leads to fewer medical errors and accidents. On the other hand, we have developed a patient cohort analyzer, based mostly on a recent clustering algorithm, that gives experts a fresh view of their patient population and lets them refine protocols and identify high-risk patient groups. This tool has also been used to support territorial planning and resource allocation.These problems have been extensively addressed in the past, but actual penetration of solutions in hospitals is smaller than one could expect. For example, one can find hundreds of papers on predicting influx to emergency rooms or bed demands, but many of them conclude after producing an AUC figure, and even fewer describe a working system that can be exported from the hospital where they were developed to others at an affordable cost. I will describe the approach taken at Amalfi so that hospitals can have such a solution up and running in a few days of work for their IT departments, in what I think is an interesting combination of software engineering and automatic Machine Learning.} }
@inproceedings{10.1145/3691620.3695532, title = {Constructing Surrogate Models in Machine Learning Using Combinatorial Testing and Active Learning}, booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering}, pages = {1645--1654}, year = {2024}, isbn = {9798400712487}, doi = {10.1145/3691620.3695532}, url = {https://doi.org/10.1145/3691620.3695532}, author = {Shree, Sunny and Khadka, Krishna and Lei, Yu and Kacker, Raghu N. and Kuhn, D. Richard}, keywords = {machine learning, surrogate model, proxy model, model extraction attack, combinatorial testing, feature interactions, test case generation, location = Sacramento, CA, USA}, abstract = {Machine learning (ML)-based models are often black box, making it challenging to understand and interpret their decision-making processes. Surrogate models are constructed to approximate the behavior of a target model and are an essential tool for analyzing black-box models. The construction of a surrogate model typically includes querying the target model with carefully selected data points and using the responses from the target model to infer information about its structure and parameters.In this paper, we propose an approach to surrogate model construction using combinatorial testing and active learning, aiming to efficiently capture the essential interactions between features that drive the target model's predictions. Our approach first leverages t-way testing to generate data points that capture all the t-way feature interactions. We then use an iterative process to isolate the essential feature interactions, i.e., those that can determine a model prediction. In the iterative process, we remove nonessential feature interactions, generate additional data points to contain the remaining interactions, and employ active learning techniques to select a subset of the data points to update the surrogate model. This process is continued until we construct a surrogate model that closely mirrors the target model's behavior. We evaluate our approach on 4 public datasets and 12 ML models and compare the results with the state-of-the-art (SOTA) approaches. Our experimental results show that our approach can perform in most cases better than the SOTA approaches in terms of accuracy and efficiency.} }
@inproceedings{10.1145/3701625.3701695, title = {Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding}, booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality}, pages = {703--705}, year = {2024}, isbn = {9798400717772}, doi = {10.1145/3701625.3701695}, url = {https://doi.org/10.1145/3701625.3701695}, author = {Cabral, Raphael and Kalinowski, Marcos}, keywords = {SOLID Design Principles, Machine Learning, Code Understanding}, abstract = {[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects. These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms. However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn’t adhere to software design best practices. [Goal] To better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding. [Method] We conducted a controlled experiment with three independent trials involving 100 data scientists. We restructured real industrial ML code that did not use SOLID principles. Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles. Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding. [Results] The dissertation results provide statistically significant evidence that adopting the SOLID design principles can improve code understanding within ML projects. [Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the quality and maintainability of ML code.} }
@inproceedings{10.1145/3723178.3723239, title = {Advancements in Machine Learning for Adaptive Intrusion Detection: A Comprehensive Review}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {460--465}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723239}, url = {https://doi.org/10.1145/3723178.3723239}, author = {Mahmud, Mirza Asif and Hasan, Khandaker Tabin and Karim, Razuan}, keywords = {Intrusion Detection Systems, Cyber Security, Adaptive Defense Mechanisms, Resilient Cyber Defense, Emerging Threats, Feature Selection Methods, Ensemble Learning, Cyber Threat Detection, Data Preprocessing, Model Evaluation, Hybrid Approaches, Classifier Subset Evaluation, Correlation-Based Feature Selection, Security Analytics.}, abstract = {The stability and adaptability of the intrusion detection systems (IDS) are of prime importance in the current scenario of continuous combat between cyber attackers and defenders in cyber-crime activity. Those dedicated rule-based systems have made considerable advancements in detecting established patterns from attacks but severely lag behind when it comes to detection of emerging and evolving threats. Thus, this review article attempts to fill this research gap by synthesizing the recent methodologies and input from different studies regarding adapting intrusion detection today using machine learning techniques. The idea is to take into consideration the effectiveness of various ML algorithms along with feature selection methods and use of ensemble learning techniques for improving the IDS capability. The proposed methodology includes important procedures such as data loading and preprocessing, feature selection, exploratory data analysis, training and evaluating the model, applying a Bayesian Gaussian Mixture Model, performing threshold comparisons, and analyzing outcomes. Observed results from training six models, including Random Forest, MLP, LSTM, Logistic Regression, KNN, and Decision Tree, are reported. Among the six models trained, the Random Forest model emerged the best with an accuracy of 95.10\%, an F1 score of 95.10\%, precision of 95.11\%, and recall of 95.10\% while accruing a loss of 1.77. The models were trained and evaluated on the UNSW-NB15 dataset, a well-known network intrusion detection dataset. For each model, performance metrics such as accuracy, F1 score, precision, recall, and loss are reported. In addition, average Receiver Operating Characteristic Area under Curve analysis, optimal threshold analysis, and performance metric comparisons at various thresholds are included. The findings highlight the effectiveness of ML methods for adaptive intrusion detection and the importance of threshold selection for the optimal detection performance.} }
@inproceedings{10.1145/3639233.3639250, title = {Explainable Machine Learning Models for Swahili News Classification}, booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval}, pages = {12--18}, year = {2024}, isbn = {9798400709227}, doi = {10.1145/3639233.3639250}, url = {https://doi.org/10.1145/3639233.3639250}, author = {Murindanyi, Sudi and Yiiki, Brian Afedra and Katumba, Andrew and Nakatumba-Nabende, Joyce}, keywords = {Explainability., Interpretability, location = Seoul, Republic of Korea}, abstract = {Although Swahili is considered a well-resourced language, challenges persist in utilizing it for Natural Language Processing (NLP) tasks, primarily due to the limited data availability required for these systems. For instance, obtaining sufficient Swahili news data for classification remains a significant obstacle. This paper addresses the problem of accurate Swahili news classification by leveraging classical machine learning (ML) models and deep neural networks (DNN). Our proposed method involves data acquisition, Exploratory Data Analysis (EDA), and employing modelling techniques using classical ML models, such as Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes, Random Forest, Gradient Boosting, Hard Voting, and Bagging, as well as DNN models including Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (Bi-LSTM), and CNN-Bi-LSTM + Attention. The models were evaluated using Area Under the Curve (AUC) metrics and accuracy. Our results demonstrate commendable performance for classical ML classifiers and DNN models, with accuracies above 75\%. Notably, the CNN-Bi-LSTM + Attention model achieved an impressive AUC score of 97\%. Additionally, explainability using LIME (Local Interpretable Model-agnostic Explanations) provided valuable insights into model decisions. This research contributes to Swahili natural language processing and lays the foundation for further explorations into transformer-based models for improved classification.} }
@inproceedings{10.1145/3729605.3729694, title = {Research on Personal Credit Risk Assessment Model Based on Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Big Data and Informatization Education}, pages = {512--516}, year = {2025}, isbn = {9798400714405}, doi = {10.1145/3729605.3729694}, url = {https://doi.org/10.1145/3729605.3729694}, author = {Zhang, Jiaoxia}, keywords = {AUC, Credit Evaluation, Logistic, Random Forest, Smote Algorithm}, abstract = {As the personal credit business continues to grow and expand, credit assessment has become a key task in risk management for the entire credit industry. In order to reduce the number of non-performing loan cases and their probability of occurrence, financial institutions need to fully understand the creditworthiness of their customers before lending, assess the quality of their customers, and reduce non-performing loans by reducing the potential risk borne by the financial institutions. In this paper, based on default data, the SMOTE algorithm was first applied to solve the problem of unbalanced classification samples, and then logistic regression, random forest and SVM algorithms were selected to build the credit evaluation model, and the logistic regression model was selected as the optimal model using the AUC value as the judgment criterion.} }
@inproceedings{10.1145/3747227.3747266, title = {A Study on Optimizing the Integration of Police Work into Community Security Governance in Inner Mongolia with Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {241--247}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747266}, url = {https://doi.org/10.1145/3747227.3747266}, author = {Zhu, Guangqin and Zhao, Shuhui}, keywords = {community policing, community security governance, machine learning algorithms, multi-center governance, occupational burnout}, abstract = {Inner Mongolia police have adopted new models like "police grids + community grids" and "community police + grid members + N" to integrate into community governance, achieving certain results. However, issues such as insufficient community police force allocation, inadequate resources for community police work, and a shortage of participating entities in multi - center governance still exist. This study constructs a "data - algorithm - application" closed - loop system using machine learning algorithms, specifically the K-Means clustering algorithm. By preprocessing data, training and verifying the model, and analyzing various relationships like those between community security satisfaction and participation activities, and community policing input and performance, the research aims to optimize the integration of police and civilians in community security governance. The system was piloted in communities in Inner Mongolia to verify the model's effectiveness, shorten the response time to security incidents, and form a replicable "Inner Mongolia smart policing" framework for border areas across the country.} }
@inproceedings{10.1145/3607947.3608045, title = {Review of Machine Learning Techniques for Crop Recommendation}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {443--449}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608045}, url = {https://doi.org/10.1145/3607947.3608045}, author = {Kansal, Liza and Pandey, Anoushka and Shukla, Sanidhya Madhav and Dhaliwal, Parneeta}, keywords = {Agriculture, Crop Prediction, Machine Learning, Predictive Analytics, location = Noida, India}, abstract = {Agriculture is an important sector in India, and about 58\% of the Indian population depends on it. This is why it is paramount it remains profitable and provides a high yield. One of the problems that lead to reduced productivity is the selection of the wrong crop. For maximum productivity, every crop needs specific environmental conditions like soil quality, water, etc. In our work, we have used various Machine learning techniques and based on their comparative analysis adopted the best model to predict the most suitable crop for a particular soil sample based on parameters like Nitrogen, Potassium, Phosphorus, ph. level, rainfall, temperature, and humidity. The dataset is pre-processed and optimized using pre-processing techniques. We have reviewed existing algorithms such as Decision Trees, Naive Bayes, Support Vector Machine (SVM), K Nearest Neighbor (KNN), and Random Forest to predict the most suitable crop and found Naive Bayes Classifier to be the best model, based on performance metrics of precision, recall, accuracy and F1 score.} }
@inproceedings{10.1145/3627673.3680021, title = {GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {4514--4521}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3680021}, url = {https://doi.org/10.1145/3627673.3680021}, author = {Gupta, Vipul and Chen, Xin and Huang, Ruoyun and Meng, Fanlong and Chen, Jianjun and Yan, Yujun}, keywords = {billion-node graphs, distributed graph learning, node embedding, location = Boise, ID, USA}, abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools for supervised machine learning over graph-structured data, while sampling-based node representation learning is widely utilized in unsupervised learning. However, scalability remains a major challenge in both supervised and unsupervised learning for large graphs (e.g., those with over 1 billion nodes). The scalability bottleneck largely stems from the mini-batch sampling phase in GNNs and the random walk sampling phase in unsupervised methods. These processes often require storing features or embeddings in memory. In the context of distributed training, they require frequent, inefficient random access to data stored across different workers. Such repeated inter-worker communication for each mini-batch leads to high communication overhead and computational inefficiency.We propose GraphScale, a unified framework for both supervised and unsupervised learning to store and process large graph data distributedly. The key insight in our design is the separation of workers who store data and those who perform the training. This separation allows us to decouple computing and storage in graph training, thus effectively building a pipeline where data fetching and data computation can overlap asynchronously. Our experiments show that GraphScale outperforms state-of-the-art methods for distributed training of both GNNs and node embeddings. We evaluate GraphScale both on public and proprietary graph datasets and observe a reduction of at least 40\% in end-to-end training times compared to popular distributed frameworks, without any loss in performance. While most existing methods don't support billion-node graphs for training node embeddings, GraphScale is currently deployed in production at TikTok enabling efficient learning over such large graphs.} }
@inproceedings{10.1145/3708394.3708433, title = {Research on Learning Outcomes Prediction in Mechanical Engineering Courses based on Machine Learning}, booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education}, pages = {225--229}, year = {2025}, isbn = {9798400710650}, doi = {10.1145/3708394.3708433}, url = {https://doi.org/10.1145/3708394.3708433}, author = {Zhu, Zhongming and Wu, Jingyi and Wang, Mingxiang}, keywords = {Attention Mechanism, Deep Learning Hybrid Model, Educational Data Mining, Learning Outcomes Prediction, Machine Learning}, abstract = {The prediction of learning outcomes is becoming an increasingly crucial aspect of educational quality assurance and the provision of targeted student support. Conventional assessment techniques frequently prove inadequate for identifying students who are at risk of failing to learn in a timely manner. This makes it challenging to intervene in a timely manner. This work proposes an innovative predictive model based on the combination of machine learning and deep learning, which aims to accurately predict the learning outcomes in mechanical engineering courses by analyzing students' multi-dimensional data. Initially, classical principal component analysis (PCA) is employed for data preprocessing purposes. This enables the extraction of key features, reduction of data dimensions, and identification of the core factors influencing academic performance. Subsequently, the preprocessed features are fed into a deep learning model based on a long short-term memory network (LSTM) architecture. LSTM is designed to process time series data, thereby enabling the capture of dynamic changes in students' learning behaviors, including performance trends over time and the time-dependence of learning activities. Furthermore, the model incorporates an attention mechanism to dynamically assign weights to different features, thereby facilitating more accurate identification of the factors with the greatest impact on learning outcomes. The experimental results demonstrate that the hybrid model exhibits superior performance in terms of accuracy and generalization ability when compared to a single machine learning algorithm and traditional statistical methods.} }
@inproceedings{10.1145/3718751.3718839, title = {Binary Classification of Algae and Non-algae Microorganisms: A Comparison of Machine Learning Models}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {552--557}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718839}, url = {https://doi.org/10.1145/3718751.3718839}, author = {Xu, Jing}, keywords = {Decision tree, Logistic regression, Machine learning, Microorganisms classification, Random forest, Support vector machine}, abstract = {Microorganisms are the most numerous group of organisms in the world and play an indispensable role on earth. They affect soil fertility, water cleanliness, atmospheric composition, and biological health. Among them, algae microorganisms have unique ecological functions and biological characteristics compared with other microbes. For this reason, the distinction between algae and non-algae microbes is essential to protect the stability of ecosystems. The purpose of this work is to investigate a 30527 x 25-field microbial dataset using various machine learning models to tackle the binary classification problem of algae and non-algae microorganisms. To address this research question, the performance of decision trees, random forests, logistic regression, and support vector machine models in machine learning is assessed in terms of accuracy, F1 scores, and AUC values. With the maximum accuracy of 0.829, the findings demonstrate how effectively the machine learning decision tree performs in the analysis of this dataset. For classification analysis, the outcomes can be used to other datasets that are similar.} }
@inproceedings{10.1145/3677182.3677256, title = {SmartAuth: A Behavioral Biometric Authentication System based on Machine Learning}, booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security}, pages = {415--421}, year = {2024}, isbn = {9798400709784}, doi = {10.1145/3677182.3677256}, url = {https://doi.org/10.1145/3677182.3677256}, author = {Mi, Rongxin}, abstract = {Smartphone is an essential part of people's lives, which are often used to store highly sensitive and private information. The information leakage will cause vital security risks for smartphone users. User authentication is a key technology to guarantee smartphone security. Compared with traditional password authentication and face authentication, behavioral bio- metric authentication can keep authenticating the user's identity during use and does not require the user's collaboration in the authentication process. This paper proposes a behavioral bio- metric authentication scheme based on machine learning named SmartAuth, which is a mobile application designed to protect private data on the smartphone by combining touchscreen-based authentication and motion-based authentication. The paper de- scribes the design and implementation of SmartAuth on Android, designs three groups of experiments to verify the performance of the software, and discusses the impact of different machine learning algorithms on the performance.} }
@inproceedings{10.1145/3677525.3678633, title = {JoinDetect: A Data-Led Machine Learning Approach to Detection of Coinjoin Transactions}, booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good}, pages = {5--13}, year = {2024}, isbn = {9798400710940}, doi = {10.1145/3677525.3678633}, url = {https://doi.org/10.1145/3677525.3678633}, author = {O'Meara, John William and Taneja, Mohit and Nicholls, Jack and Kothale, Nitish and Flinter, Steve and Jurcut, Anca Delia}, keywords = {Anti-Money Laundering, Blockchain Technology, CoinJoin, Cryptocurrency, Forensic Analysis, Machine Learning, Privacy Enhancing Technologies, Social Good, location = Bremen, Germany}, abstract = {CoinJoin is a privacy-enhancing technique used in cryptocurrency transactions, designed to obfuscate the ownership of funds by amalgamating multiple inputs, thereby complicating the task of linking identities to addresses. While traditional methods for detecting such transactions have relied on rule-based and heuristic approaches, these often fall short in addressing the complexities of advanced CoinJoin techniques. Recent advancements have shown that machine learning methods significantly outperform these traditional approaches in identifying these types of transactions. This paper introduces a robust data engineering and machine learning framework to effectively distinguish CoinJoin transactions from standard ones. Our approach tackles class imbalance, and accounts for varying frequency distributions of target class transactions. We detail a two-part process: initially demonstrating the approach’s utility in identifying classic Chaumian CoinJoin transactions, then reapplying the methodology for model retraining to adapt to concept drift, enabling the detection of newer, more complex CoinJoin variants. Leveraging ground truth labelled data and proprietary blockchain analytics, our model adeptly navigates the evolving challenges of privacy technologies. Beyond mere identification, this process enhances blockchain forensic applications, improving blockchain address clustering and the efficiency of attribution tracing. This not only streamlines forensic investigations, but also supports regulatory compliance initiatives, ensuring that the application of blockchain technology aligns with broader societal values without compromising user privacy. By integrating blockchain technology in this manner, we contribute to a framework where digital finance meets social good, providing tools that ensure both innovation and integrity in blockchain applications.} }
@inproceedings{10.1145/3655038.3665943, title = {Rethinking Erasure-Coding Libraries in the Age of Optimized Machine Learning}, booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems}, pages = {23--30}, year = {2024}, isbn = {9798400706301}, doi = {10.1145/3655038.3665943}, url = {https://doi.org/10.1145/3655038.3665943}, author = {Hu, Jiyu and Kosaian, Jack and Rashmi, K. V.}, keywords = {erasure coding, machine learning, redundancy, location = Santa Clara, CA, USA}, abstract = {Erasure codes are critical tools for building fault-tolerant and resource-efficient storage systems. However developing and maintaining optimized erasure-coding libraries are challenging. We make the case that the growth of fast machine-learning (ML) libraries may serve as a lifeboat for easing the development of current and future optimized erasure-coding libraries: fast erasure-coding libraries for various hardware platforms can be easily implemented by using existing optimized ML libraries. We show that the computation structure of many erasure codes mirrors that common to matrix multiplication, which is heavily optimized in ML libraries. Due to this similarity, one can implement erasure codes using ML libraries in few lines of code and with little knowledge of erasure codes, while immediately adopting the many optimizations within these libraries, without requiring expertise in high-performance programming. We develop prototypes of our proposed approach using an existing ML library. Our prototypes are up to 1.75 faster than state-of-the-art custom erasure-coding libraries.} }
@article{10.1145/3575637.3575644, title = {Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications}, journal = {SIGKDD Explor. Newsl.}, volume = {24}, pages = {32--47}, year = {2022}, issn = {1931-0145}, doi = {10.1145/3575637.3575644}, url = {https://doi.org/10.1145/3575637.3575644}, author = {Fu, Xingbo and Zhang, Binchi and Dong, Yushun and Chen, Chen and Li, Jundong}, abstract = {Graph machine learning has gained great attention in both academia and industry recently. Most of the graph machine learning models, such as Graph Neural Networks (GNNs), are trained over massive graph data. However, in many realworld scenarios, such as hospitalization prediction in healthcare systems, the graph data is usually stored at multiple data owners and cannot be directly accessed by any other parties due to privacy concerns and regulation restrictions. Federated Graph Machine Learning (FGML) is a promising solution to tackle this challenge by training graph machine learning models in a federated manner. In this survey, we conduct a comprehensive review of the literature in FGML. Specifically, we first provide a new taxonomy to divide the existing problems in FGML into two settings, namely, FL with structured data and structured FL. Then, we review the mainstream techniques in each setting and elaborate on how they address the challenges under FGML. In addition, we summarize the real-world applications of FGML from different domains and introduce open graph datasets and platforms adopted in FGML. Finally, we present several limitations in the existing studies with promising research directions in this field.} }
@article{10.1145/3565271, title = {Machine Learning Optimization of Quantum Circuit Layouts}, journal = {ACM Transactions on Quantum Computing}, volume = {4}, year = {2023}, doi = {10.1145/3565271}, url = {https://doi.org/10.1145/3565271}, author = {Paler, Alexandru and Sasu, Lucian and Florea, Adrian-Catalin and Andonie, Razvan}, keywords = {Machine learning, quantum circuits, optimization}, abstract = {The quantum circuit layout (QCL) problem involves mapping out a quantum circuit such that the constraints of the device are satisfied. We introduce a quantum circuit mapping heuristic, QXX, and its machine learning version, QXX-MLP. The latter automatically infers the optimal QXX parameter values such that the laid out circuit has a reduced depth. In order to speed up circuit compilation, before laying the circuits out, we use a Gaussian function to estimate the depth of the compiled circuits. This Gaussian also informs the compiler about the circuit region that influences most the resulting circuit’s depth. We present empiric evidence for the feasibility of learning the layout method using approximation. QXX and QXX-MLP open the path to feasible large-scale QCL methods.} }
@article{10.1145/3717063, title = {Informing the Design of Individualized Self-Management Regimens from the Human, Data, and Machine Learning Perspectives}, journal = {ACM Trans. Comput.-Hum. Interact.}, volume = {32}, year = {2025}, issn = {1073-0516}, doi = {10.1145/3717063}, url = {https://doi.org/10.1145/3717063}, author = {Pichon, Adrienne and Urteaga, I\~nigo and Mamykina, Lena and Elhadad, No\'emie}, keywords = {reinforcement learning, self-management, chronic illness}, abstract = {Intelligent systems for self-management can help patients and improve quality of life. However, designing AI-based systems is challenging because designers need to account not only for user needs, but also for capabilities and practical constraints of underlying algorithms. We propose and implement a human-centered AI framework to align human and technological requirements and constraints that can guide design of intelligent systems for personal health. We use concepts from a machine learning technique, reinforcement learning, to elicit user needs, through directed content analysis of user interviews, and uncover practical data constraints, through analysis of “in the wild” user engagement logs from a self-monitoring app. We gather and triangulate human-machine-data requirements for a self-management tool for individuals with endometriosis—a poorly understood, complex chronic condition with no reliable treatment. We present recommendations for developing a system that aligns with needs, capabilities, and constraints from human user, data, and machine learning perspectives.} }
@inproceedings{10.1145/3595360.3595855, title = {Transactional Python for Durable Machine Learning: Vision, Challenges, and Feasibility}, booktitle = {Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning}, year = {2023}, isbn = {9798400702044}, doi = {10.1145/3595360.3595855}, url = {https://doi.org/10.1145/3595360.3595855}, author = {Chockchowwat, Supawit and Li, Zhaoheng and Park, Yongjoo}, abstract = {In machine learning (ML), Python serves as a convenient abstraction for working with key libraries such as PyTorch, scikit-learn, and others. Unlike DBMS, however, Python applications may lose important data, such as trained models and extracted features, due to machine failures or human errors, leading to a waste of time and resources. Specifically, they lack four essential properties that could make ML more reliable and user-friendly---durability, atomicity, replicability, and time-versioning (DART).This paper presents our vision of Transactional Python that provides DART without any code modifications to user programs or the Python kernel, by non-intrusively monitoring application states at the object level and determining a minimal amount of information sufficient to reconstruct a whole application. Our evaluation of a proof-of-concept implementation with public PyTorch and scikit-learn applications shows that DART can be offered with overheads ranging 1.5\%--15.6\%.} }
@inproceedings{10.1145/3640310.3674092, title = {Towards Runtime Monitoring for Responsible Machine Learning using Model-driven Engineering}, booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems}, pages = {195--202}, year = {2024}, isbn = {9798400705045}, doi = {10.1145/3640310.3674092}, url = {https://doi.org/10.1145/3640310.3674092}, author = {Naveed, Hira and Grundy, John and Arora, Chetan and Khalajzadeh, Hourieh and Haggag, Omar}, keywords = {Human-centric requirements, Machine learning components, Model-driven engineering, Responsible ML, Runtime monitoring, location = Linz, Austria}, abstract = {Machine learning (ML) components are used heavily in many current software systems, but developing them responsibly in practice remains challenging. 'Responsible ML' refers to developing, deploying and maintaining ML-based systems that adhere to human-centric requirements, such as fairness, privacy, transparency, safety, accessibility, and human values. Meeting these requirements is essential for maintaining public trust and ensuring the success of ML-based systems. However, as changes are likely in production environments and requirements often evolve, design-time quality assurance practices are insufficient to ensure such systems' responsible behavior. Runtime monitoring approaches for ML-based systems can potentially offer valuable solutions to address this problem. Many currently available ML monitoring solutions overlook human-centric requirements due to a lack of awareness and tool support, the complexity of monitoring human-centric requirements, and the effort required to develop and manage monitors for changing requirements. We believe that many of these challenges can be addressed by model-driven engineering. In this new ideas paper, we present an initial meta-model, model-driven approach, and proof of concept prototype for runtime monitoring of human-centric requirements violations, thereby ensuring responsible ML behavior. We discuss our prototype, current limitations and propose some directions for future work.} }
@inproceedings{10.1145/3745238.3745386, title = {A-Share Quantitative Investment Strategy for Multiple Objectives Driven by Machine Learning and Deep Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {945--950}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745386}, url = {https://doi.org/10.1145/3745238.3745386}, author = {Li, Xiang and Hong, Ruixin}, keywords = {Investment portfolio, Machine learning, Quantization method}, abstract = {This article uses the real market data of A-shares, selects price data, financial data and technical indicators, uses a variety of machine learning models to predict future returns, and updates the model monthly. At the same time, in order to consider different economic goals, a yield maximization strategy is set, and a risk-return equilibrium strategy based on the modified Markowitz model is set. It attempts to use stocks in the stock pool to track the market index, control risks, and set adjustable mean-variance parameters. This article aims to use advanced computer technology and quantitative methods to provide more reference value for investment groups with different needs and risk preferences.} }
@inproceedings{10.1145/3696348.3696878, title = {MLTCP: A Distributed Technique to Approximate Centralized Flow Scheduling For Machine Learning}, booktitle = {Proceedings of the 23rd ACM Workshop on Hot Topics in Networks}, pages = {167--176}, year = {2024}, isbn = {9798400712722}, doi = {10.1145/3696348.3696878}, url = {https://doi.org/10.1145/3696348.3696878}, author = {Rajasekaran, Sudarsanan and Narang, Sanjoli and Zabreyko, Anton A. and Ghobadi, Manya}, keywords = {Congestion control, DNN training, Datacenters for ML, Networks for ML, Resource allocation, Transport layer, location = Irvine, CA, USA}, abstract = {This paper argues that congestion control protocols in machine learning datacenters sit at a sweet spot between centralized and distributed flow scheduling solutions. We present MLTCP, a technique to augment today's congestion control algorithms to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. We demonstrate that MLTCP uses a gradient descent trend with a step taken at every training (or fine-tuning) iteration towards reducing network congestion among competing jobs.} }
@article{10.1145/3660801, title = {MirrorFair: Fixing Fairness Bugs in Machine Learning Software via Counterfactual Predictions}, journal = {Proc. ACM Softw. Eng.}, volume = {1}, year = {2024}, doi = {10.1145/3660801}, url = {https://doi.org/10.1145/3660801}, author = {Xiao, Ying and Zhang, Jie M. and Liu, Yepang and Mousavi, Mohammad Reza and Liu, Sicen and Xue, Dingyuan}, keywords = {Bias Mitigation, Fairness Bugs, Machine Learning, Software Discrimination}, abstract = {With the increasing utilization of Machine Learning (ML) software in critical domains such as employee hiring, college admission, and credit evaluation, ensuring fairness in the decision-making processes of underlying models has emerged as a paramount ethical concern. Nonetheless, existing methods for rectifying fairness issues can hardly strike a consistent trade-off between performance and fairness across diverse tasks and algorithms. Informed by the principles of counterfactual inference, this paper introduces MirrorFair, an innovative adaptive ensemble approach designed to mitigate fairness concerns. MirrorFair initially constructs a counterfactual dataset derived from the original data, training two distinct models—one on the original dataset and the other on the counterfactual dataset. Subsequently, MirrorFair adaptively combines these model predictions to generate fairer final decisions. We conduct an extensive evaluation of MirrorFair and compare it with 15 existing methods across a diverse range of decision-making scenarios. Our findings reveal that MirrorFair outperforms all the baselines in every measurement (i.e., fairness improvement, performance preservation, and trade-off metrics). Specifically, in 93\% of cases, MirrorFair surpasses the fairness and performance trade-off baseline proposed by the benchmarking tool Fairea, whereas the state-of-the-art method achieves this in only 88\% of cases. Furthermore, MirrorFair consistently demonstrates its superiority across various tasks and algorithms, ranking first in balancing model performance and fairness in 83\% of scenarios. To foster replicability and future research, we have made our code, data, and results openly accessible to the research community.} }
@inproceedings{10.1145/3747357.3747382, title = {A method combining finite element simulation and machine learning for performing lower extremity firearm injury diagnosis}, booktitle = {Proceedings of the 2025 International Symposium on Bioinformatics and Computational Biology}, pages = {156--162}, year = {2025}, isbn = {9798400714368}, doi = {10.1145/3747357.3747382}, url = {https://doi.org/10.1145/3747357.3747382}, author = {Mao, Yucheng and Zhang, Linxuan}, keywords = {FEA, Firearm Injury, Lower Limbs, Machine Learning, Wound Ballistics}, abstract = {As global conflicts escalate, addressing potential human injuries on the battlefield becomes increasingly critical. However, ethical considerations preclude the use of human experimentation in injury research. Fortunately, advancements in computer simulation, particularly through numerical methods and machine learning algorithms, offer promising avenues for understanding mechanisms of injury without resorting to unethical practices. In this study, we explore the integration of the finite element analysis method with machine learning algorithms to expedite the diagnosis of human lower extremity firearm injuries. Initially,we will establish a finite element model of the human lower limb bones based on CT images and validated material properties of the fibula and tibia. Subsequently, we will simulate various combat scenarios by altering different loading conditions. By scripting parameterized simulations, we will generate a large dataset of simulation results. Upon obtaining finite element results for each simulated case, we employ supervised machine learning algorithms to classify wound characteristics, including entrance velocity, projectile type, and angle of incidence. Our findings demonstrate high diagnostic accuracies, with k nearest neighbor (KNN), multilayer perceptron(MLP) ,support vector machine (SVM) and random forest classifiers achieving accuracies of 85.5\%, 86.5\%, 86.5\% and 86\%, respectively.} }
@inproceedings{10.1145/3607947.3608032, title = {Sales Analysis and Forecasting using Machine Learning Approach}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {375--377}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608032}, url = {https://doi.org/10.1145/3607947.3608032}, author = {Rajpoot, Dharmveer Singh and Mittal, Bhavey and Dudani, Harshit and Singhal, Ujjwal}, abstract = {In this paper, we examine how machine learning models are used in sales predictive analytics. This paper's main objective is to examine the key methods and case studies of using machine learning to sales forecasting. It has been thought about how machine learning generalization will affect things. When there is only a little quantity of historical data available for a certain sales time series, such as when a new store or product is released, this impact can be utilized to make sales predictions. Regression ensembles of single models have been built using a regressor technique. The findings demonstrate that we may enhance the performance of predictive models for sales forecasting by utilizing linear regression and XG boost regressor.} }
@inbook{10.1145/3729706.3729749, title = {State-owned Capital and Green M\&amp;A: A Machine Learning Analysis of Private Firms}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {272--277}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729749}, author = {Xu, Haili}, abstract = {This study examines the impact of state-owned capital participation on private enterprises’ green mergers and acquisitions (M\&amp;A) in China, using the data on green M\&amp;A events of Chinese listed organization between 2010 and 2023. An empirical analysis is conducted to investigate the moderating role of media attention in this relationship. Machine learning model such as Random Forest (RF) and Logistic Regression (LR) are used to validate the results. The results show that state-owned capital participation in private enterprises significantly facilitates green M\&amp;A by improving internal and external supervision and providing resource synergies. RF received (85.7\%) accuracy followed by LR with (83.1\&amp;) in predicting the success of green M\&amp;A success. Results also reveal that the media attention plays a positive and moderating role in the relationship between state-owned capital participation and green M\&amp;A of private enterprises. This study contributes to a deeper understanding of the impact of state-owned capital participation on private enterprises’ green M\&amp;A, empowering high-quality economic development and supporting the achievement of the national “dual carbon” goals.} }
@inproceedings{10.1145/3711896.3736851, title = {Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms and Hyperparameters on Shared Latent Space}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {1025--1036}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3736851}, url = {https://doi.org/10.1145/3711896.3736851}, author = {Ishikawa, Kazuki and Ozaki, Ryota and Kanzaki, Yohei and Takeuchi, Ichiro and Karasuyama, Masayuki}, keywords = {automl, bayesian optimization, gaussian process, location = Toronto ON, Canada}, abstract = {Selecting the optimal combination of a machine learning (ML) algorithm and its hyper-parameters is crucial for the development of high-performance ML systems. However, since the combination of ML algorithms and hyper-parameters is enormous, the exhaustive validation requires a significant amount of time. Many existing studies use Bayesian optimization (BO) for accelerating the search. On the other hand, a significant difficulty is that, in general, there exists a different hyper-parameter space for each one of candidate ML algorithms. BO-based approaches typically build a surrogate model independently for each hyper-parameter space, by which sufficient observations are required for all candidate ML algorithms. In this study, our proposed method embeds different hyper-parameter spaces into a shared latent space, in which a surrogate multi-task model for BO is estimated. This approach can share information of observations from different ML algorithms by which efficient optimization is expected with a smaller number of total observations. We further propose the pre-training of the latent space embedding with an adversarial regularization, and a ranking model for selecting an effective pre-trained embedding for a given target dataset. Our empirical study demonstrates effectiveness of the proposed method through datasets from OpenML.} }
@inproceedings{10.1145/3616855.3635725, title = {The 5th International Workshop on Machine Learning on Graphs (MLoG)}, booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining}, pages = {1210--1211}, year = {2024}, isbn = {9798400703713}, doi = {10.1145/3616855.3635725}, url = {https://doi.org/10.1145/3616855.3635725}, author = {Derr, Tyler and Ma, Yao and Ding, Kaize and Zhao, Tong and Ahmed, Nesreen K.}, abstract = {Graphs, which encode pairwise relations between entities, are a kind of universal data structure for a lot of real-world data, including social networks, transportation networks, and chemical molecules. Many important applications on these data can be treated as computational tasks on graphs. Recently, machine learning techniques are widely developed and utilized to effectively tame graphs for discovering actionable patterns and harnessing them for advancing various graph-related computational tasks. Huge success has been achieved and numerous real-world applications have benefited from it. However, since in today's world, we are generating and gathering data in a much faster and more diverse way, real-world graphs are becoming increasingly large-scale and complex. More dedicated efforts are needed to propose more advanced machine learning techniques and properly deploy them for real-world applications in a scalable way. Thus, we organize The 5th International Workshop on Machine Learning on Graphs (MLoG) (https://mlog-workshop.github.io/wsdm24.html), held in conjunction with the 17th ACM Conference on Web Search and Data Mining (WSDM), which provides a venue to gather academia researchers and industry researchers/practitioners to present the recent progress on machine learning on graphs.} }
@article{10.1145/3708497, title = {Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3708497}, url = {https://doi.org/10.1145/3708497}, author = {Bayram, Firas and Ahmed, Bestoun S.}, keywords = {Artificial intelligence, machine learning, Trustworthy AI, robustness, MLOps systems, DataOps, ModelOps, model performance}, abstract = {Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.} }
@inproceedings{10.1145/3697355.3697359, title = {Enhancing Dynamic Hand Gesture Recognition through Optimized Feature Selection using Double Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Big Data and Internet of Things}, pages = {19--25}, year = {2024}, isbn = {9798400717529}, doi = {10.1145/3697355.3697359}, url = {https://doi.org/10.1145/3697355.3697359}, author = {Yan, Keyue and Lam, Chi Fai and Fong, Simon and Marques, Jo\~ao Alexandre Lobo and Song, Qun and Qin, Huafeng}, keywords = {Causal Effect, Double Machine Learning, Feature Selection, Hand Gesture Recognition, Leap Motion Controller}, abstract = {Causal machine learning combines causal inference and machine learning to understand and utilize causal relationships in data. While traditional machine learning focuses on missions of prediction and pattern recognition, causal machine learning goes a step further by revealing causal relationships between variables. In this research, we employ the double machine learning method to identify variables in the gesture recognition problem where independent variables have causal relationships with the final gesture. These variables are then selected for further classification and analysis. By comparing this approach with traditional feature selection methods, we find that the variables selected using double machine learning are more useful for classification and yield excellent results across different machine learning classification models. This new double machine learning based approach provides a valuable reference for researchers during the feature selection stage.} }
@inproceedings{10.1145/3626246.3654680, title = {Applications and Computation of the Shapley Value in Databases and Machine Learning}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {630--635}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3654680}, url = {https://doi.org/10.1145/3626246.3654680}, author = {Luo, Xuan and Pei, Jian}, keywords = {Shapley value, cooperative game theory, data market, databases, machine learning, location = Santiago AA, Chile}, abstract = {Recently, the Shapley value, a concept rooted in cooperative game theory, has found more and more applications in databases and machine learning. Due to its combinatoric nature, the computation of the Shapley value is #P-hard. To address this challenge, numerous studies are actively engaged in developing efficient computation methods or exploring alternative solutions in specific application contexts. Applications of the Shapley value in databases and machine learning as well as fast computation or approximation of the Shapley value in those applications are becoming a new research frontier in the database community. This tutorial presents a comprehensive and systematic overview of Shapley value applications and computation within both database and machine learning domains. We survey the existing methods from a unique perspective that diverges from the current literature. Unlike most reviews, which mainly focus on applications, our approach focuses on the underlying algorithmic mechanisms and application specific assumptions in these methods. This approach allows us to highlight the similarities and differences among the various Shapley value applications and computation techniques more effectively. Our tutorial categorizes these methods based on their intrinsic processes, cutting across different applications. The tutorial begins with an introduction to the Shapley value and its diverse applications in databases and machine learning. Subsequently, it delves into the computational challenges of the Shapley value, presents cutting-edge solutions for its efficient computation, and explores alternative solutions.} }
@inproceedings{10.1145/3688671.3688744, title = {Inverse design of Hexagonal Moir\'e Materials: Machine Learning for tunable pore properties}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688744}, url = {https://doi.org/10.1145/3688671.3688744}, author = {Papia, Efi-Maria and Kondi, Alex and Constantoudis, Vassilios}, keywords = {porous materials, Moir\'e patterns, inverse design, machine learning, classification}, abstract = {Moir\'e patterns, emerging from the overlay of periodic structures, produce distinctive interference effects. Hexagonal Moir\'e patterns formed by superimposing hexagonal lattices with rotational misalignment or varying lattice constants are particularly notable. These patterns have unveiled extraordinary electronic properties in two-dimensional materials like graphene, significantly impacting fields such as quantum computing and nanoelectronics, while also holding promise in materials engineering for filtration purposes. This study employs machine learning to achieve inverse design of hexagonal Moir\'e lattices, predicting lattice configurations for specific pore characteristics. Using computational modeling, datasets of simulated lattices are generated, and pore size distribution data are extracted. Neural networks are trained on these datasets to predict the baseline mesh width while also classifying the number of meshes required for the desired pore size distribution. The results, evaluated through accuracy metrics and ROC curves, are compared to well-known classifiers, highlighting the approach’s potential to revolutionize materials design with tunable properties and advance materials science.} }
@inproceedings{10.1145/3646547.3689656, title = {Poster: Toward Achieving Inter-protocol Friendliness through Estimation of BBR Behaviors using Machine Learning}, booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference}, pages = {739--740}, year = {2024}, isbn = {9798400705922}, doi = {10.1145/3646547.3689656}, url = {https://doi.org/10.1145/3646547.3689656}, author = {Utsumi, Satoshi and Ishikawa, Junya and Ohba, Fuya and Zabir, Salahuddin Muhammad Salim and Atiquzzaman, Mohammed}, keywords = {bbr, congestion control algorithm, machine learning, location = Madrid, Spain}, abstract = {In this paper, we present a new mechanism for estimating the data sending rates of BBR congestion control algorithm using machine learning. Results show that machine learning can estimate the data sending rates of BBR with high accuracy.} }
@inproceedings{10.1145/3674912.3674950, title = {Localization in cellular network using machine learning and fingerprint methods}, booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024}, pages = {87--94}, year = {2024}, isbn = {9798400716843}, doi = {10.1145/3674912.3674950}, url = {https://doi.org/10.1145/3674912.3674950}, author = {Pham, Ngoc Hung and Nguyen, Huy Dinh and Nguyen, Dinh Thuan and La, The Vinh and Ta, Hai Tung and Hoang, Van Hiep}, keywords = {Cellular Network, Localization, Location-based services, Received Signal Strength, Signal Fingerprint, location = Ruse, Bulgaria}, abstract = {Location-based services (LBS) are widely used in many applications for daily life. The localization of mobile devices has attracted a great deal of research interest with many different techniques to provide precision and high effective positioning capabilities for users. Global Position System (GPS) has good results of location information and is popularly used in many cases. However, there is lack of some abilities compared with the localization techniques based on cellular network. Cellular network is the most used mobile technology for providing communication between user devices themselves, between user device and network operators, as well as enabling the localization ability of user device using cellular network information. In this study, we proposed a solution for collecting and processing a dataset of cell information that consists of cell identification, cell location, and the received signal strength indicator by using a mobile device, then evaluating the localization methods of the mobile device from this dataset. We conducted experiments for localization methods including Centroid, Weighted centroid, Linear Regression, Support Vector Regression, Multilayer Perceptron, and Fingerprint. We analyze and compare the accuracy of these localization methods from the experimental results. Centroid methods are simple and able to provide acceptable accuracy, machine learning methods provide quite good accuracy comparing with Centroid methods, and Fingerprint method provides the best accuracy among investigated localization methods.} }
@inproceedings{10.1145/3534678.3542902, title = {Machine Learning for Materials Science (MLMS)}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4902--4903}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542902}, url = {https://doi.org/10.1145/3534678.3542902}, author = {Sardeshmukh, Avadhut and Reddy, Sreedhar and Gautham, B P. and Agrawal, Ankit}, keywords = {machine learning, materials informatics, materials science, microstructure informatics, location = Washington DC, USA}, abstract = {Artificial intelligence and machine learning are being increasingly used in scientific domains such as computational fluid dynamics and chemistry. Particularly notable is a recently renewed interest in solving partial differential equations using machine learning models, especially deep neural networks, as partial differential equations arise in many scientific problems of interest. Within materials science literature, there has been a surge in publications on AI-enabled materials discovery, in the last five years. Despite this, the interaction between machine learning researchers and materials scientists (especially, scientists working on structural materials, their microstructures, textures and so on) has been very sparse. On the other hand, AI/ML techniques can clearly be integrated into materials design frameworks (e.g., MGI efforts) to support accelerated materials development, novel simulation methodologies and advanced data analytics. Hence there is an immediate need for exchange of ideas and collaborations between machine learning and materials science communities. We believe a workshop dedicated to this theme would be well- suited to foster such collaborations. The aim of this workshop is to bring together the computer science and materials science communities and foster deeper collaborations between the two to accelerate the adoption of AI/ML in materials science. We hope and envision this workshop to facilitate in building a community of researchers in this interdisciplinar area in the years ahead.} }
@article{10.1145/3715154, title = {Leveraging Incremental Machine Learning for Reconfigurable Systems Modeling under Dynamic Workloads}, journal = {ACM Trans. Reconfigurable Technol. Syst.}, volume = {18}, year = {2025}, issn = {1936-7406}, doi = {10.1145/3715154}, url = {https://doi.org/10.1145/3715154}, author = {Encinas, Juan and Rodr\'guez, Alfonso and Otero, Andr\'es}, keywords = {Multi-Accelerator Systems, Reconfigurable Computing, Dynamic Workloads, Incremental Machine Learning, System Modeling}, abstract = {Dynamic workload orchestration is one of the main concerns when working with heterogeneous computing infrastructures in the edge-cloud continuum. In this context, FPGA-based computing nodes can take advantage of their improved flexibility, performance, and energy efficiency provided that they use proper resource management strategies. In this regard, many state-of-the-art systems rely on proactive power management techniques and task scheduling decisions, which in turn require deep knowledge about the applications to be accelerated and the actual response of the target reconfigurable fabrics when executing them. While acquiring this knowledge at design time was more or less feasible in the past, with applications mostly being static task graphs that did not change at run time, the highly dynamic nature of current workloads in the edge-cloud continuum, where tasks can be deployed on any node and at any time, has removed this possibility. As a result, being able to derive such information at run time to make informed decisions has become a must. This article presents an infrastructure to build incremental ML models that can be used to obtain run-time power consumption and performance estimations in FPGA-based reconfigurable multi-accelerator systems operating under dynamic workloads. The proposed infrastructure features a novel stop-and-restart resource-aware mechanism to monitor and control the model training and evaluation stages during normal system operation, enabling low-overhead updates in the models to account for either unexpected acceleration requests (i.e., tasks not considered previously by the models) or model drift (e.g., fabric degradation). Experimental results show that the proposed approach induces a maximal additional error of 3.66\% compared to a continuous training alternative. Furthermore, the proposed approach incurs only a 4.49\% execution time overhead, compared to the 20.91\% overhead induced by the continuous training alternative. The proposed modeling strategy enables innovative scheduling approaches in reconfigurable systems. This is exemplified by the conflict-aware scheduler introduced in this work, which achieves up to a 1.35 times speedup in executing the experimental workload. Additionally, the proposed approach demonstrates superior adaptability compared to other methods in the literature, particularly in response to significant changes in workload and to mitigate the effects of model overfitting. The portability of the proposed modeling methodology and monitoring infrastructure is also shown through their application to both Zynq-7000 and Zynq UltraScale+ devices.} }
@inproceedings{10.1145/3712255.3726776, title = {An Evolutionary Approach to Interpretable Machine Learning for ICU Length of Stay Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {291--294}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3726776}, url = {https://doi.org/10.1145/3712255.3726776}, author = {Kar, Reshma and Ilangovan, Eslin Kiran and Bizel, Gulhan and Patra, Braja Gopal}, keywords = {interpretable machine learning, multiple gaussian model, ICU length of stay prediction, MIMIC-IV dataset, gravitational search algorithm, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Neural networks have repeatedly demonstrated the ability to accurately model complex nonlinear relationships. However, the black-box nature of such models has led to concerns about bias and trustworthiness especially in settings like healthcare where it impacts multiple aspects of patient care including prognosis and resource-allocation. Existing model-agnostic methods to enhance neural network interpretability only offer an external perspective on the working of neural networks. To solve this issue, we designed an Evolutionary Multiple Gaussian Model that replaces the activation function of a perceptron with an aggregate of multiple Gaussian functions. The parameters of this model were evaluated using the gravitational search algorithm with the objective of minimizing a function of the mean absolute percentage error (MAPE) and mean squared error between the predicted and actual values. We performed multiple experiments on the Medical Information Mart for Intensive Care-IV dataset to predict the patient length of stay in Intensive Care Units. Results indicate that the proposed algorithm outperforms it's popular counterparts in terms of MAPE while offering better interpretability.} }
@inproceedings{10.1145/3698038.3698519, title = {Vista: Machine Learning based Database Performance Troubleshooting Framework in Amazon RDS}, booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing}, pages = {83--98}, year = {2024}, isbn = {9798400712869}, doi = {10.1145/3698038.3698519}, url = {https://doi.org/10.1145/3698038.3698519}, author = {Singh, Vikramank and Song, Zhao and Narayanaswamy, Balakrishnan Murali and Vaidya, Kapil Eknath and Kraska, Tim}, keywords = {Cloud Databases, ML for Systems, Performance Troubleshooting, location = Redmond, WA, USA}, abstract = {Database performance troubleshooting is a complex multi-step process that broadly involves three key stages- (a) Detection: determining what's wrong and when; (b) Root Cause Analysis (RCA): reasoning about why is the performance poor; (c) Resolution: identifying a fix. A plethora of techniques exist to address each of these problems, but they hardly work in real-world at scale. First, real-world customer workloads are noisy, non-stationary and quasi-periodic in nature rendering traditional detectors ineffective. Second, real-world production databases execute a highly diverse set of queries that skew the database statistics into long-tail distributions causing traditional RCA methods to fail. Third, these databases typically execute millions of such diverse queries every minute rendering traditional methods inefficient when deployed at scale.In this paper we describe Vista, a machine learning based performance troubleshooting framework for databases, and dive-deep into how it addresses the 3 real-world problems outlined above. Vista deploys a deep auto-regressive model trained on a large and diverse Amazon Relational Database Service (RDS) fleet with custom skip connections and periodicity alignment features to model long range and varying periodicity in customer workloads, and detects performance bottlenecks in the form of outliers. Furthermore, it efficiently filters only a top few dominating SQL queries from millions in a problematic workload, and uses a robust causal inference framework to identify the culprit queries and their statistics leading to a low false-positive and false-negative rate. Currently, Vista runs on hundreds of thousands of RDS databases, analyzes millions of workloads every day bringing down the troubleshooting time for RDS customers from hours to seconds. At the end, we also describe several challenges and learnings from implementing and deploying Vista at Amazon scale.} }
@inproceedings{10.1145/3718751.3718820, title = {A Machine Learning Approach to Detecting Financial Anomalies in Large Global Companies}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {433--438}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718820}, url = {https://doi.org/10.1145/3718751.3718820}, author = {Ji, Jiexin}, keywords = {Financial Anomaly Detection, Machine Learning, Isolation Forest, Market Capitalization, P/E Ratio, Financial Metrics Analysis}, abstract = {Financial analysis is essential for assessing the health and performance of large global companies, particularly in identifying anomalies that may signal overvaluation, undervaluation, or operational inefficiencies. With the growing complexity of financial markets and the volume of data, traditional analysis methods are often insufficient for detecting hidden patterns or irregularities. In the present work, the Isolation Forest algorithm is utilized to analyzing financial metrics from the world's largest companies. The dataset includes key metrics that reflect both the market valuation and operational performance, such as Market Capitalization, Revenue, Earnings, and the Price-to-Earnings (P/E) Ratio. After data preprocessing, the Isolation Forest algorithm was applied to detect outliers in the dataset, revealing companies with discrepancies between market expectations and actual financial performance. As the research results reveal, the proposed machine learning solution performs well in identifying financial anomalies and thereby provide a statistical basis for investors and analysts in data-driven decision-making.} }
@inproceedings{10.1145/3649476.3658775, title = {IR drop Prediction Based on Machine Learning and Pattern Reduction}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024}, pages = {516--519}, year = {2024}, isbn = {9798400706059}, doi = {10.1145/3649476.3658775}, url = {https://doi.org/10.1145/3649476.3658775}, author = {Chang, Yong-Fong and Chen, Yung-Chih and Cheng, Yu-Chen and Lin, Shu-Hong and Lin, Che-Hsu and Chen, Chun-Yuan and Chen, Yu-Hsuan and Lee, Yu-Che and Lin, Jia-Wei and Pao, Hsun-Wei and Chang, Shih-Chieh and Li, Yi-Ting and Wang, Chun-Yao}, keywords = {Dynamic IR drop analysis, IR drop prediction, pattern reduction, location = Clearwater, FL, USA}, abstract = {With the advances in semiconductor technology, the sizes of transistors are getting smaller, which has led to an increasingly severe impact of IR drop. Consequently, this trend has amplified the significance of IR drop analysis within the realm of chip design. However, analyzing IR drop is resource-intensive and time-consuming, since numerous simulation patterns are required to verify the power integrity of circuits. Additionally, with every engineering change order (ECO) step, a reevaluation is necessary. In this paper, we propose a machine learning-based method to predict IR drop levels and present an algorithm for reducing simulation patterns, which could reduce the time and computing resources required for IR drop analysis within the ECO flow. Experimental results show that our approach can reduce the number of patterns by approximately 50\%, thereby decreasing the analysis time while maintaining accuracy.} }
@inproceedings{10.1145/3426826.3426837, title = {Machine Learning in Tourism}, booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence}, pages = {53--57}, year = {2020}, isbn = {9781450388344}, doi = {10.1145/3426826.3426837}, url = {https://doi.org/10.1145/3426826.3426837}, author = {Afsahhosseini, Fatemehalsadat and Al-Mulla, Yaseen}, keywords = {Demand Forecasting, Machine Learning, Recommender System, Sentiment Analysis, Tourism, location = Hangzhou, China}, abstract = {Machine Learning is a subset of Artificial Intelligence, which is a process of learning from different types of data to make accurate predictions. Data in tourism is various such as Statistics, Photos, Maps, and Texts. Also, each tourism cycle has different stages: Pre, During, and After Trip. In this paper application of machine learning in tourism related data and trip stages are introduced in detailed.} }
@inproceedings{10.1145/3671151.3671249, title = {Applications of Machine Learning in Recognizing Chinese Calligrapher's Handwriting Styles}, booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications}, pages = {548--552}, year = {2024}, isbn = {9798400718106}, doi = {10.1145/3671151.3671249}, url = {https://doi.org/10.1145/3671151.3671249}, author = {Hu, Xiangkun and Wu, Fei}, keywords = {CNN, Calligraphic Stroke Features, Chinese Calligraphy Recognition, Deep Learning, location = Wuhan, China}, abstract = {This study explores the potential of machine learning (ML) techniques in recognizing the handwriting styles of Chinese calligraphers. By analyzing a dataset comprising diverse samples of calligraphy from renowned artists, we implement and compare various ML models, including Convolutional Neural Networks (CNNs) and Support Vector Machines (SVMs), to identify unique style markers[1]. Our findings demonstrate the efficacy of CNNs in capturing intricate patterns, yielding a promising accuracy rate in style classification. This research not only contributes to the digitization and preservation of cultural heritage but also offers insights into the fusion of technology and art.} }
@inproceedings{10.1145/3639592.3639594, title = {Conventional Machine Learning Approach for Waste Classification}, booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference}, pages = {7--12}, year = {2024}, isbn = {9798400716225}, doi = {10.1145/3639592.3639594}, url = {https://doi.org/10.1145/3639592.3639594}, author = {Jangsamsi, Kharittha}, keywords = {Fourier descriptors, feature extraction, histogram of oriented gradients, local binary pattern, waste classification, location = Kyoto, Japan}, abstract = {Waste management is a complex and challenging process, especially waste classification to sort waste by categories. The paper aims to overcome these challenges by proposing a waste classification approach that uses various feature extraction algorithms along with a support vector machine (SVM). The purpose is to identify the most effective feature for building a classification model, even with a low number of samples and high intra-class variance. SVM was used for classification while Fourier descriptors (FDs), histogram of oriented gradients (HOG), and local binary pattern (LBP) were used for feature extraction. The dataset used in this paper was obtained from Kaggle.com and Google.com with different types of vision problems. The experimental results showed that classification with LBP feature extraction achieves the highest accuracy. This accuracy is higher than the experiments with other feature extractions.} }
@inproceedings{10.1145/3715931.3715955, title = {Classification of Depression and Anxiety with Machine Learning Applying Random Forest Models}, booktitle = {Proceedings of the 2024 5th International Conference on Intelligent Medicine and Health}, pages = {128--132}, year = {2025}, isbn = {9798400709616}, doi = {10.1145/3715931.3715955}, url = {https://doi.org/10.1145/3715931.3715955}, author = {Alvarez Espezua, Camila Britany and Cruz de la Cruz, Jose Emmanuel and Apaza Davila, Fabiana Alexandra and Cruz de la Cruz, Trinidad Dorotea and Huaquipaco Encinas, Saul and Mamani Machaca, Wilson Antony}, keywords = {Anxiety, Classification, Depression, Random Forest}, abstract = {Anxiety and depression have a strong correlation; although they often differ in their symptoms, both disorders frequently coexist in individuals, leading to a significant impairment of their daily functioning. Anxiety is among the most common conditions that, over time, can develop into a depressive disorder, or vice versa. The simultaneous presence of these disorders contributes to a range of symptoms, including mood disturbances, anhedonia, sleep problems, persistent fear, suicidal thoughts or ideation, distress, irritability, and constant worry, which result in substantial suffering for affected individuals. Recognizing this complex interplay, the present research applied Machine Learning models, particularly Random Forest, which is renowned for its robustness in classification tasks and its ability to identify both linear and non-linear relationships. The study involved the use of 17 predictor variables to train two models, each focused on predicting different outcomes: “Depression Severity” and “Anxiety Severity.” By leveraging the strengths of Random Forest, the research achieved highly accurate classifications. Specifically, the model for “Depression Severity” attained an accuracy of 99.35\% and an F1 Score of 99.34\%, while the model for “Anxiety Severity” reached an accuracy of 98.04\% and an F1 Score of 98.05\%. These results underscore the effectiveness and precision of the Random Forest algorithm in accurately identifying the severity of these mental health conditions.} }
@inproceedings{10.1145/3715335.3735485, title = {Machine Learning Fairness in House Price Prediction: A Case Study of America’s Expanding Metropolises}, booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies}, pages = {473--480}, year = {2025}, isbn = {9798400714849}, doi = {10.1145/3715335.3735485}, url = {https://doi.org/10.1145/3715335.3735485}, author = {Almajed, Abdalwahab and Tabar, Maryam and Najafirad, Peyman}, keywords = {Machine Learning for Sustainable Societies, Machine Learning Fairness, House Price Prediction}, abstract = {As a basic human need, housing plays a key role in enhancing health, well-being, and educational outcome in society, and the housing market is a major factor for promoting quality of life and ensuring social equity. To improve the housing conditions, there has been extensive research on building Machine Learning (ML)-driven house price prediction solutions to accurately forecast the future conditions, and help inform actions and policies in the field. In spite of their success in developing high-accuracy models, there is a gap in our understanding of the extent to which various ML-driven house price prediction approaches show ethnic and/or racial bias, which in turn is essential for the responsible use of ML, and ensuring that the ML-driven solutions do not exacerbate inequity. To fill this gap, this paper develops several ML models from a combination of structural and neighborhood-level attributes, and conducts comprehensive assessments on the fairness of ML models under various definitions of privileged groups. As a result, it finds that the ML-driven house price prediction models show various levels of bias towards protected attributes (i.e., race and ethnicity in this study). Then, it investigates the performance of different bias mitigation solutions, and the experimental results show their various levels of effectiveness on different ML-driven methods. However, in general, the in-processing bias mitigation approach tends to be more effective than the pre-processing one in this problem domain. Our code is available at https://github.com/wahab1412/housing_fairness.} }
@article{10.1145/3654663, title = {Machine Learning in Metaverse Security: Current Solutions and Future Challenges}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3654663}, url = {https://doi.org/10.1145/3654663}, author = {Otoum, Yazan and Gottimukkala, Navya and Kumar, Neeraj and Nayak, Amiya}, keywords = {Metaverse Security, Digital Twin, Machine Learning, Extended Reality, Generative AI, Blockchain}, abstract = {The Metaverse, positioned as the next frontier of the Internet, has the ambition to forge a virtual shared realm characterized by immersion, hyper-spatiotemporal dynamics, and self-sustainability. Recent technological strides in AI, Extended Reality, 6G, and blockchain propel the Metaverse closer to realization, gradually transforming it from science fiction into an imminent reality. Nevertheless, the extensive deployment of the Metaverse faces substantial obstacles, primarily stemming from its potential to infringe on privacy and be susceptible to security breaches, whether inherent in its underlying technologies or arising from the evolving digital landscape. Metaverse security provisioning is poised to confront various foundational challenges owing to its distinctive attributes, encompassing immersive realism, hyper-spatiotemporally, sustainability, and heterogeneity. This article undertakes a comprehensive study of the security and privacy challenges facing the Metaverse, leveraging machine learning models for this purpose. In particular, our focus centers on an innovative distributed Metaverse architecture characterized by interactions across 3D worlds. Subsequently, we conduct a thorough review of the existing cutting-edge measures designed for Metaverse systems while also delving into the discourse surrounding security and privacy threats. As we contemplate the future of Metaverse systems, we outline directions for open research pursuits in this evolving landscape.} }
@inbook{10.1145/3729706.3729773, title = {Analyzing Phase-Specific Drivers of Digital Transformation in SMEs: A Machine Learning-Based Approach}, booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy}, pages = {425--430}, year = {2025}, isbn = {9798400712715}, url = {https://doi.org/10.1145/3729706.3729773}, author = {Zhang, Fan and Liu, Chongyu and Meng, Haiting and Du, Gangqiang and Wang, Xiangyu and Zhou, Yanling}, abstract = {This study explores the evolving drivers of digital transformation (DT) in small and medium-sized enterprises (SMEs) by analyzing data from two phases: the shift from non-digitalization to initial digitalization, and from initial to advanced digitalization. Using a combination of Logit, linear regression and machine learning techniques, specifically XGBoost and SHAP value analysis, this research identifies key incentives in each phase. The external factors, such as the business environment and resource availability, are crucial in the early phase of digitalization. The internal factors, including digital organizational design and digital literacy are assuming an increasingly critical role in the second phase of DT. The findings offer insights into the challenges SMEs face during digital transformation journey, and underscore the importance of organizational restructuring and capacity-building for advancing digitalization.} }
@inproceedings{10.1145/3637528.3671603, title = {GraphStorm: All-in-one Graph Machine Learning Framework for Industry Applications}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6356--6367}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671603}, url = {https://doi.org/10.1145/3637528.3671603}, author = {Zheng, Da and Song, Xiang and Zhu, Qi and Zhang, Jian and Vasiloudis, Theodore and Ma, Runjie and Zhang, Houyu and Wang, Zichen and Adeshina, Soji and Nisa, Israt and Mottini, Alejandro and Cui, Qingjun and Rangwala, Huzefa and Zeng, Belinda and Faloutsos, Christos and Karypis, George}, keywords = {graph machine learning, industry scale, location = Barcelona, Spain}, abstract = {Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a \&lt;u\&gt;dozen\&lt;/u\&gt; \&lt;u\&gt;billion-scale\&lt;/u\&gt; industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.} }
@inproceedings{10.1145/3610978.3640598, title = {Design Principles for Building Robust Human-Robot Interaction Machine Learning Models}, booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction}, pages = {247--251}, year = {2024}, isbn = {9798400703232}, doi = {10.1145/3610978.3640598}, url = {https://doi.org/10.1145/3610978.3640598}, author = {Bhagat Smith, Josh and Mallampati, Vivek and Baskaran, Prakash and Giolando, Mark-Robin and Adams, Julie A.}, keywords = {design principles, human-robot interaction, machine learning, location = Boulder, CO, USA}, abstract = {Effective collaboration between humans and robots hinges on the robot's ability to comprehend its human teammate. This collaboration demands the development of machine learning models that bridge the gap between human physiological signals and their mental states. However, the challenge lies in developing generalizable machine learning models using data collected in controlled experimental conditions. This manuscript proposes a set of principles for designing human subject evaluations, emphasizing the crucial balance between experimental control and ecological validity while also balancing fundamental machine learning trade-offs.} }
@inproceedings{10.1145/3708360.3708389, title = {A machine learning based Anti-fraud approach for life insurance company : A Case Study in a life insurance company}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {179--185}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708389}, url = {https://doi.org/10.1145/3708360.3708389}, author = {Jiang, Yuwen and Feng, Jiangang and Jiang, Chengxuan}, keywords = {Life Insurance, Logistic Regression, Machine Learning, Policy Management}, abstract = {In China, when life insurance companies tried to develop agent channels, they often applied aggressive incentive policies to encourage agents to sell more products and enlarge sales team. Agents can easily utilize improper incentive policies to forge false insurance policies and defraud the company of incentive fees. This article proposed a “false policy” detection approach based on machine learning algorithm, which can effectively detect the suspected arbitrage policies of agents by combining the information of policyholder and insured, agent information and sales department information of newly underwritten policies. Firstly, the data of policy holders, policy features, agents and business unit were integrates as an analytical flat table, than Logistic Regression was applied the predict the probability of being a “false policy”. This approach has been applied to one domestic life insurance company, it has showed perfect performance and saved considerable losses for the company.} }
@inproceedings{10.1145/3607947.3608023, title = {Heart Failure Prediction Using Different Machine Learning Algorithms}, booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing}, pages = {352--360}, year = {2023}, isbn = {9798400700224}, doi = {10.1145/3607947.3608023}, url = {https://doi.org/10.1145/3607947.3608023}, author = {Aggarwal, Ananya and Gupta, Samarth and Varshney, Vanshita and Jaiswal, Shruti}, keywords = {Accuracy, Cardiovascular disease, Machine Learning Algorithms, UCI dataset, location = Noida, India}, abstract = {A doctor can benefit greatly from an early diagnosis of an illness and lower their patient's mortality risk. Machine learning algorithms play a significant role in the initial stages of illness detection, which would help in providing effective treatment for patients. We are using the dataset from the University of California, Irvine (UCI) repository to train and test our model. We will be using a total of twelve machine-learning algorithms. To improve their performance, Hyperparameter tuning of each algorithm would be done. For the comparative analysis of all algorithms, we are using accuracy on the testing set as the performance measure. For simulation, Jupyter Notebook 6.5.2 is used.} }
@inproceedings{10.1145/3688671.3688740, title = {Machine Learning Applications in Nanotechnology Manufacturing: From Etching Accuracy to Deposition Prediction}, booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence}, year = {2024}, isbn = {9798400709821}, doi = {10.1145/3688671.3688740}, url = {https://doi.org/10.1145/3688671.3688740}, author = {Kondi, Alex and Papia, Efi-Maria and Constantoudis, Vassilios}, keywords = {Line Edge Roughness (LER), nanofabrication, deposition, pattern transfer, machine learning, transfer learning, regression}, abstract = {The integration of machine learning (ML) within the realm of nanomanufacturing processes, specifically through the applications of thickness prediction during deposition and pattern fidelity during etching, may present transformative potential. This paper discusses two ML-based methodologies that enhance the precision and efficiency of these critical processes. In the context of etching, we revisit the 3D geometrical modeling of etch-induced Line Edge Roughness (LER) transfer from photoresist lines to the substrate, refining previous models to account for realistic three-dimensional surface topographies. Meanwhile, in the deposition segment, we employ a deep neural network (DNN) to predict the thickness of films deposited on rough substrates using binarized top-down Scanning Electron Microscopy (SEM) images. The outcomes demonstrate how ML-based methodologies not only predict but also potentially control the nanofabrication parameters, leading to improved manufacturing outcomes.} }
@inproceedings{10.1145/3555041.3589682, title = {Proactively Screening Machine Learning Pipelines with ARGUSEYES}, booktitle = {Companion of the 2023 International Conference on Management of Data}, pages = {91--94}, year = {2023}, isbn = {9781450395076}, doi = {10.1145/3555041.3589682}, url = {https://doi.org/10.1145/3555041.3589682}, author = {Schelter, Sebastian and Grafberger, Stefan and Guha, Shubha and Karlas, Bojan and Zhang, Ce}, keywords = {data validation, machine learning pipelines, provenance tracking, location = Seattle, WA, USA}, abstract = {Software systems that learn from data with machine learning (ML) are ubiquitous. ML pipelines in these applications often suffer from a variety of data-related issues, such as data leakage, label errors or fairness violations, which require reasoning about complex dependencies between their inputs and outputs. These issues are usually only detected in hindsight after deployment, after they caused harm in production. We demonstrate ArgusEyes, a system which enables data scientists to proactively screen their ML pipelines for data-related issues as part of continuous integration. ArgusEyes instruments, executes and screens ML pipelines for declaratively specified pipeline issues, and analyzes data artifacts and their provenance to catch potential problems early before deployment to production. We demonstrate our system for three scenarios: detecting mislabeled images in a computer vision pipeline, spotting data leakage in a price prediction pipeline, and addressing fairness violations in a credit scoring pipeline.} }
@proceedings{10.1145/3698263, title = {MLPR '24: Proceedings of the 2024 2nd International Conference on Machine Learning and Pattern Recognition}, year = {2024}, isbn = {9798400710001} }
@inproceedings{10.1145/3706890.3707039, title = {Machine Learning-based Algorithm for Screening Drug Candidates in Breast Cancer Treatment}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {868--874}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3707039}, url = {https://doi.org/10.1145/3706890.3707039}, author = {Zhou, Yuxuan and Ye, Yuhan and Fan, Caiyun}, keywords = {Breast cancer drug discovery, heuristic algorithms, machine learning, multi-objective optimization}, abstract = {Breast cancer is one of the most prevalent malignant tumors globally. This study aims to develop and optimize a model for predicting biological activity and estimating pharmacokinetic (ADMET) properties using machine learning algorithms. The goal is to screen potential drug candidates for breast cancer. Based on this model, we propose a molecular screening model for breast cancer drug molecules utilizing a greedy-genetic-LGB algorithm to enhance the efficiency of drug molecule screening.First, two rounds of screening of potential drug molecules using LightGBM and the Permutation importance algorithm were performed to identify the 20 most relevant molecular descriptors for biological activity.Secondly, the PSO-LGB algorithm was proposed. Based on this model, regression prediction models for compound bioactivity and classification prediction models for pharmacokinetic properties were established and compared with popular industry machine models, demonstrating the superiority of the proposed models.Finally, the greedy-genetic-LGB algorithm was proposed to transform the drug molecule screening problem into a multi-objective optimization problem. The objective function is the prediction result of the two models, and the optimization objective is the descriptor value of the drug molecule, based on the PSO-LGB compound bioactivity and PSO-LGB pharmacokinetic property prediction models. To verify the superiority of the proposed algorithm, experiments were conducted on the public dataset of breast cancer drug molecule screening provided by DrugBank. A set of optimal molecule values was obtained, resulting in an excellent active pIC50 value (8.7792) with this molecule combination along with good ADMET performance.In summary, the machine learning approach proposed in this study can effectively predict the biological activity and ADMET properties of compounds and improve the speed and effectiveness of screening potential breast cancer drug molecules, as well as potentially facilitate their clinical studies.} }
@article{10.1145/3568429, title = {Improving Storage Systems Using Machine Learning}, journal = {ACM Trans. Storage}, volume = {19}, year = {2023}, issn = {1553-3077}, doi = {10.1145/3568429}, url = {https://doi.org/10.1145/3568429}, author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Burford, Andrew and McNeill, Michael and Arkhangelskiy, Michael and Zadok, Erez}, keywords = {Operating systems, storage systems, Machine Learning, storage performance optimization}, abstract = {Operating systems include many heuristic algorithms designed to improve overall storage performance and throughput. Because such heuristics cannot work well for all conditions and workloads, system designers resorted to exposing numerous tunable parameters to users—thus burdening users with continually optimizing their own storage systems and applications. Storage systems are usually responsible for most latency in I/O-heavy applications, so even a small latency improvement can be significant. Machine learning (ML) techniques promise to learn patterns, generalize from them, and enable optimal solutions that adapt to changing workloads. We propose that ML solutions become a first-class component in OSs and replace manual heuristics to optimize storage systems dynamically. In this article, we describe our proposed ML architecture, called KML. We developed a prototype KML architecture and applied it to two case studies: optimizing readahead and NFS read-size values. Our experiments show that KML consumes less than 4 KB of dynamic kernel memory, has a CPU overhead smaller than 0.2\%, and yet can learn patterns and improve I/O throughput by as much as 2.3 and 15 for two case studies—even for complex, never-seen-before, concurrently running mixed workloads on different storage devices.} }
@proceedings{10.1145/3696687, title = {MLPRAE '24: Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering}, year = {2024}, isbn = {9798400709876} }
@inproceedings{10.1145/3732801.3732805, title = {Design and Application of a University Student Behavior Analysis Model Based on Machine Learning Technology}, booktitle = {Proceedings of the 2025 2nd International Conference on Informatics Education and Computer Technology Applications}, pages = {17--20}, year = {2025}, isbn = {9798400712432}, doi = {10.1145/3732801.3732805}, url = {https://doi.org/10.1145/3732801.3732805}, author = {Li, Xiangyun}, keywords = {Learning Outcome Prediction, Machine Learning, Personalized Learning Path, Student Behavior Analysis}, abstract = {With the acceleration of the process of information technology in education, the use of machine learning methods for the study of student learning behaviour has increasingly become an effective method. On this basis, this paper proposes a method for analysing college students' behaviour based on machine learning. On this basis, this paper proposes an approach based on the combination of mathematical modelling and mathematical modelling. This project intends to accurately identify students‘ learning patterns and needs through deep mining of multiple sources of information such as students’ online behaviour, homework, classroom behaviour, etc., so as to achieve personalized education and precise intervention.} }
@inproceedings{10.1145/3662739.3669983, title = {Research on Transient Stability of Power Systems Based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications}, pages = {383--391}, year = {2024}, isbn = {9798400718144}, doi = {10.1145/3662739.3669983}, url = {https://doi.org/10.1145/3662739.3669983}, author = {Luan, Jing and Yang, Yawen}, keywords = {Deep learning, Machine learning, Power system, Transient stability, location = Ningbo, China}, abstract = {The safe and stable operation of electric power system is an important foundation and support for the stable development of modern society. With the continuous expansion of the scale of the power system, the access of various new energy, new loads and the wide application of power electronic devices have increased the uncertainty and complexity of the network operation, making it face a severe test. The power system is often disturbed, when a variety of short circuit, broken line or switch without fault trip and other large disturbances, the need to study the transient stability of the system. This paper is based on the machine learning view and the transient stability evaluation process, the deep learning model is used to learn a more abstract representation of the data feature rules. On the basis of constructing the system feature quantity data, the stacking auto encoder and support vector machine algorithm are combined to learn and train the sample set and test the accuracy of the model. Considering the stacking automatic encoder belongs to the most basic deep learning model, further introduce deeper complex convolution neural network as a power system transient stability assessment model, and at the same time the support vector machine algorithm introduced output layer of discriminant mechanism, through the example verified the effectiveness of the two models, improve the performance of the transient stability assessment.} }
@article{10.1145/3711095, title = {Understanding Interaction with Machine Learning through a Thematic Analysis Coding Assistant: A User Study}, journal = {Proc. ACM Hum.-Comput. Interact.}, volume = {9}, year = {2025}, doi = {10.1145/3711095}, url = {https://doi.org/10.1145/3711095}, author = {Milana, Federico and Costanza, Enrico and Musolesi, Mirco and Ayobi, Amid}, keywords = {interactive machine learning, thematic analysis}, abstract = {Interactive Machine Learning (IML) enables users, including non-experts in ML, to iteratively train and improve ML models. However, limited research has been reported on how non-experts interact with these systems. Focusing on thematic analysis as a practical application, we report on a user study where 20 participants interacted with TACA, a functioning IML tool. Thematic analysis involves individual interpretation of ambiguous data, hence it is suited for and can benefit from the iterative customization of models supported by IML. Through a combination of interaction logs and semi-structured interviews, our findings revealed that, by using TACA, participants critically reflected on their analysis, gained new thematic insights, and adapted their interpretative stance. We also document misconceptions of ML concepts, positivist views, and personal blame for poor model performance. We then discuss how applications could be designed to improve the understanding of IML concepts and foster reflexive work practices.} }
@inproceedings{10.1145/3733155.3734916, title = {Classification of mild cognitive impairment using machine learning with dynamic functional connectivity from resting-state functional MRI}, booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments}, pages = {458--467}, year = {2025}, isbn = {9798400714023}, doi = {10.1145/3733155.3734916}, url = {https://doi.org/10.1145/3733155.3734916}, author = {Minami, Ryosuke and Hatano, Ryo and Nishiyama, Hiroyuki}, keywords = {rs-fMRI, machine learning, MCI, window-based FC analysis}, abstract = {The early diagnosis of mild cognitive impairment (MCI) is crucial for effective treatment. Resting-state functional magnetic resonance imaging (rs-fMRI) combined with machine learning has shown promise for the diagnosis of MCI. However, because rs-fMRI data tend to include substantial noise and the limited amount of available rs-fMRI data especially for MCI, it is important to develop a robust model to counter the effects of noise and data imbalance. Therefore, we propose a preprocessing method and classify preprocessed rs-fMRI data into cognitively normal and MCI groups using a machine learning model. Specifically, during preprocessing, we perform principal component analysis, window-based functional connectivity analysis, and feature selection based on hypothesis testing for differences. The highest classification performance from the fivefold cross-validation was an accuracy of 0.847, recall of 0.670, precision of 0.635, and F1 score of 0.633.} }
@inproceedings{10.1145/3709026.3709102, title = {Poor Posture Detection Based on Behavioral and Environmental Data Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence}, pages = {1--7}, year = {2025}, isbn = {9798400718182}, doi = {10.1145/3709026.3709102}, url = {https://doi.org/10.1145/3709026.3709102}, author = {Gu, Feng and Li, Jianwei and Deng, Shunrong and Yang, Qin}, keywords = {Behavioral and environmental factors, Poor posture, Prevention and intervention, Random forests}, abstract = {Poor spinal posture is a common problem among primary school students, which may lead to long-term health issues. It is essential to detect poor posture and identify the key behavioral and environmental factors for its preventions and intervention strategies, such as the sitting time, the backpack weight, the physical activities, and the sleep habits. We develop a machine learning method using random forests to predict the incorrect posture occurrences based on the collected behavioral and environmental data of primary school students and identify important influential factors. The data collected from 782 primary school students include six categories of behavioral factors and five categories of environmental factors. Experimental results show that the prediction accuracy reaches 75.16\%, and some of the most important factors for poor posture include carrying backpacks on two shoulders, the mattress firmness, backpack weights, and the excessive use of the electronic devices, which are able to facilitate the early detection and prevention of scoliosis for school children and provide guidelines and insights for families and schools to develop intervention strategies, such as using the right backpack, cutting down on the screen time, increasing physical activities, and making the sleep surface more supportive.} }
@inproceedings{10.1145/3718677.3718710, title = {Perioperative Immune-Inflammatory Analysis Method for Colorectal Cancer Based on Explainable Machine Learning}, booktitle = {Proceedings of the 2024 3rd International Conference on Public Health and Data Science}, pages = {204--208}, year = {2025}, isbn = {9798400711671}, doi = {10.1145/3718677.3718710}, url = {https://doi.org/10.1145/3718677.3718710}, author = {Tian, Naiyuan}, keywords = {Colorectal Cancer, Game Theory, Interpretability, Machine Learning, Perioperative Immune Inflammatory Responses}, abstract = {The single-controlled experimental method plays an important role in medical research, while it has a lot of limitations. With the rapid development of artificial intelligence, machine learning, and game theory are playing increasingly important roles. The research is about what factors infect the immune inflammatory response\&nbsp;of colorectal cancer surgery. We propose an innovative\&nbsp;model to predict immunity inflammatory response, and the model has more accuracy and more interpretability. Ensemble learning improves accuracy and reduces errors. The shap value explains which features are important and how the features explain the result. Many factors are considered, and the result shows that the anesthesia method\&nbsp;has a significant effect. The time of the surgery has no significant effect on the immune inflammatory response, which means that the surgical schedule can be more flexible.} }
@article{10.1145/3678168, title = {Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2024}, issn = {1049-331X}, doi = {10.1145/3678168}, url = {https://doi.org/10.1145/3678168}, author = {Li, Hao and Rajbahadur, Gopi Krishnan and Bezemer, Cor-Paul}, keywords = {Software engineering for machine learning, software quality, deep learning, binding, TensorFlow, PyTorch}, abstract = {Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework’s functionality using a programming language different from the framework’s default language (usually Python). In this article, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.} }
@inproceedings{10.1145/3681777.3698469, title = {Comparing Associations Of Chronic Health Outcomes with SDoH Indices Using Machine Learning}, booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Spatial Computing for Epidemiology}, pages = {9--18}, year = {2024}, isbn = {9798400711534}, doi = {10.1145/3681777.3698469}, url = {https://doi.org/10.1145/3681777.3698469}, author = {Gupta, Vandana and Gokhale, Swapna}, keywords = {chronic health outcomes, social vulnerability index, social deprivation index, machine learning, random forests, geography, racial minority, location = Atlanta, GA, USA}, abstract = {Chronic health outcomes require ongoing medical attention and have a significant impact on a person's quality of life. It is widely accepted that social determinants of health (SDoH) shape the onset and management of chronic health outcomes. Among the many composite indices that assess SDoH, there is no consensus on which index best explains these associations between health outcomes and social determinants. Furthermore, chronic outcomes may be modulated by place or geography both through cultural, social, and political forces and spatial correlations. The novelty of this paper lies in building a machine learning (ML) methodology to compare the strengths of SDoH indices in explaining the associations between chronic health outcomes and social determinants while adjusting for geography. The methodology is illustrated by studying the relative strengths of the Social Vulnerability Index (SVI) and Social Deprivation Index (SDI) in explaining age-adjusted prevalence rates of 12 chronic health outcomes obtained from the CDC PLACES project. Results suggest that the SVI is more strongly associated with all 12 chronic health outcomes, however, the increase in the strength of SVI over SDI varies across the health outcomes. For each outcome, importance scores of all SVI measures are then normalized according to its four sub themes, while introducing geography/place as the fifth sub theme. Comparing the relative importance of these five sub themes leads to a grouping of the outcomes into three clusters depending on whether geography/place, racial minority status, or socio-economic measures shows the greatest impact. The emergence of geography as a dominant sub theme alongside conventional social determinants underscores the value of our approach in providing the capability to consider the modulating effect of geography on understanding the relationships between social determinants and health.} }
@article{10.1145/3730577, title = {Less is More: Feature Engineering for Fairness and Performance of Machine Learning Software}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3730577}, url = {https://doi.org/10.1145/3730577}, author = {Meng, Linghan and Li, Yanhui and Chen, Lin and Ma, Mingliang and Zhou, Yuming and Xu, Baowen}, keywords = {Fairness, ML Software, Feature Engineering, Performance}, abstract = {Machine learning (ML) software employs statistical algorithms to perform high-stake tasks in our daily lives, whose results are usually discriminatory due to protected features (e.g., gender), i.e., one part (called privileged, e.g., male) may be more likely to obtain beneficial decisions than the other part (called unprivileged, e.g., female). In alleviating the unfairness, developers have obtained widely-held beliefs about the trade-off between performance and fairness for ML software. Surprisingly, recent research on feature engineering suggests that enlarging the feature set is the perfect way to kill two birds with one stone, i.e., achieving both higher performance and fairness. However, the experiments used in the prior study did not remove the effect of protected features, which have been suggested to be excluded in both industrial applications and academic studies. As a result, the study did not fully explore the trade-off between performance and fairness.In this paper, we first conduct an empirical study to replicate this prior study after excluding the protected features and observe that there is still a trade-off between performance and fairness with enlarging the features, i.e., more features are not perfect, which would lead to higher performance and lower fairness. Due to more features causing more collection and preprocessing budgets, we aim to search for an effective alternative. Inspired by the “less is more” principle, we propose a novel feature ranking method, Hybrid-importance and Early-validation based Feature Ranking (HEFR), to find an efficient subset to replace the full feature set with comparable performance and fairness. Our method, HEFR, employs hybrid feature importances to combine performance and fairness and conducts early validation to check the effectiveness of hybrid importances. We conduct experiments on seven datasets and three classifiers to evaluate our method with five baselines. The results have shown that (a) HEFR is efficient for ML software feature engineering: applying HEFR to choose about 10\% of features would construct ML software with better or comparable performance and fairness, and (b) HEFR is actionable with small dataset sizes: applying HEFR with only 10\% data size would still help choose the proper feature subset.} }
@inproceedings{10.1145/3672608.3707815, title = {Quantitative Assessment of Explainability in Machine Learning Models : A Study on the OULA Dataset}, booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing}, pages = {101--103}, year = {2025}, isbn = {9798400706295}, doi = {10.1145/3672608.3707815}, url = {https://doi.org/10.1145/3672608.3707815}, author = {Gunasekara, Sachini and Saarela, Mirka}, keywords = {fidelity, stability, ANN, feature importance, LIME, location = Catania International Airport, Catania, Italy}, abstract = {Many studies on AI in education compare model performance and fairness, but few focus on explainability. To address this gap, we evaluate two machine learning models—Artificial Neural Network (ANN) and Decision Tree (DT)—focusing on performance and explainability in predicting student performance using the OULA dataset. The DT, being inherently explainable, struggles with complex data relationships and misclassification, while ANN, although more accurate and stable, lacks transparency. Using the LIME method, the ANN outperforms the DT in accuracy and stability, but enhancing the interpretability of ANN models remains a key challenge for future research.} }
@inproceedings{10.1145/3716895.3717016, title = {Urban Waterlogging Risk Assessment Based on Multivariate Data and Machine Learning}, booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering}, pages = {680--687}, year = {2025}, isbn = {9798400718007}, doi = {10.1145/3716895.3717016}, url = {https://doi.org/10.1145/3716895.3717016}, author = {Zang, Feng and Fu, Jianyu and Chen, Quan}, keywords = {Machine learning, Multivariate data, Risk assessment, Urban waterlogging}, abstract = {With the acceleration of climate change and urbanization, urban waterlogging has become an increasingly severe issue, posing a threat to residents' safety. This study focuses on Nanjing City, selecting 12 risk indicators and integrating diverse data such as Weibo and remote sensing. Eight traditional and deep learning models were compared, with the best-performing Random Forest model selected for waterlogging risk assessment. The results show that the waterlogging risk is higher in the central areas of Nanjing, where vulnerability indicators such as POI density and population density, as well as probability indicators like surface impermeability and topography, significantly impact the risk. This study provides practical strategies for urban managers to prevent and mitigate waterlogging.} }
@inproceedings{10.1145/3640543.3645170, title = {Comparing Teaching Strategies of a Machine Learning-based Prosthetic Arm}, booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces}, pages = {715--730}, year = {2024}, isbn = {9798400705083}, doi = {10.1145/3640543.3645170}, url = {https://doi.org/10.1145/3640543.3645170}, author = {Sungeelee, Vaynee and Jarrass\'e, Nathana\"el and Sanchez, T\'eo and Caramiaux, Baptiste}, keywords = {Interactive Machine Teaching, Machine Learning, Mental model, Myoelectric prosthesis, Training curriculum, location = Greenville, SC, USA}, abstract = {Pattern-recognition-based arm prostheses rely on recognizing muscle activation to trigger movements. The effectiveness of this approach depends not only on the performance of the machine learner but also on the user’s understanding of its recognition capabilities, allowing them to adapt and work around recognition failures. We investigate how different model training strategies to select gesture classes and record respective muscle contractions impact model accuracy and user comprehension. We report on a lab experiment where participants performed hand gestures to train a classifier under three conditions: (1) the system cues gesture classes randomly (control), (2) the user selects gesture classes (teacher-led), (3) the system queries gesture classes based on their separability (learner-led). After training, we compare the models’ accuracy and test participants’ predictive understanding of the prosthesis’ behavior. We found that teacher-led and learner-led strategies yield faster and greater performance increases, respectively. Combining two evaluation methods, we found that participants developed a more accurate mental model when the system queried the least separable gesture class (learner-led). Our results conclude that, in the context of machine learning-based myoelectric prosthesis control, guiding the user to focus on class separability during training can improve recognition performances and support users’ mental models about the system’s behavior. We discuss our results in light of several research fields : myoelectric prosthesis control, motor learning, human-robot interaction, and interactive machine teaching.} }
@inproceedings{10.1145/3626641.3627146, title = {Exploring Machine Learning Techniques for Male Infertility Prediction: A Review}, booktitle = {Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology}, pages = {235--240}, year = {2023}, isbn = {9798400708503}, doi = {10.1145/3626641.3627146}, url = {https://doi.org/10.1145/3626641.3627146}, author = {Shofiyah, Shofiyah and Mahmudy, Wayan Firdaus}, keywords = {Literature Review, Machine Learning, Male Infertility, location = Badung, Bali, Indonesia}, abstract = {Infertility, also known as sterility in both men and women, is a global health problem that affects the quality of life of couples who want to have children. In recent decades, technological developments in medicine and computer science have inspired the exploration of machine learning techniques to support early prediction of male infertility. In this paper, the authors present a comprehensive review of the various machine learning techniques that have been applied to male infertility prediction. The authors begin by outlining the background of male infertility and the complexity of its clinical diagnosis. We then detail the advantages of machine learning techniques in processing and analyzing complex health data, as well as their potential to provide new insights into the causative factors of male infertility. Through in-depth analysis, we identify several machine learning approaches commonly used in the literature, such as regression and classification. We also review a series of recent studies that applied these techniques in diagnosing male infertility. In addition, the authors highlight the challenges faced by researchers in using machine learning for infertility prediction, including the lack of high-quality data and the interpretability of complex models. Nevertheless, many studies have shown positive results in using machine learning techniques to contribute to the development of decision support systems in this field. To support the quality of research, future research that can be done by conducting a comprehensive review of various techniques in deep learning in detecting infertility diseases, especially in men.} }
@inproceedings{10.1145/3696673.3723080, title = {Using Machine Learning for Air Quality Prediction in Alabama: An Environmental Justice Case Study}, booktitle = {Proceedings of the 2025 ACM Southeast Conference}, pages = {251--256}, year = {2025}, isbn = {9798400712777}, doi = {10.1145/3696673.3723080}, url = {https://doi.org/10.1145/3696673.3723080}, author = {Maskey, Arnav and Shinde, Rajat}, keywords = {neural networks, machine learning datasets, air quality, environmental justice, location = Southeast Missouri State University, Cape Girardeau, MO, USA}, abstract = {Environmental justice encompasses the fair treatment of all people regardless of race, color, national origin, education level, or income, in the context of environmental impacts. This paper investigates the application of machine learning (ML) to address environmental justice issues arising from air quality disparities in Jefferson County, Alabama, particularly around Birmingham. The study makes several key contributions including curation of ML-ready datasets from environmental sensor data for air quality analysis; fusion of domain specific datasets for analyzing the effects of air quality degradation across demographics; utilization of novel advanced ML approaches using a Convolutional Neural Network and Long Short-Term Memory (CNN-LSTM) architecture to identify, predict, and analyze areas with significant environmental injustice; and projection for air quality degradation with demographics to assist in decision making and future policy making. This research demonstrates the utility of ML in rapidly identifying environmental justice hotspots and offers predictive capabilities for timely mitigation measures. Additionally, this study provides a pathway for extending the methodology to similar issues globally. The work also offers social value by providing policymakers with actionable insights for mitigating air quality disparities and empowering marginalized communities with access to precise environmental predictions.} }
@inproceedings{10.1145/3603165.3607418, title = {Auxiliary Diagnosing Coronary Stenosis based on Machine Learning}, booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023}, pages = {98--99}, year = {2023}, isbn = {9798400702334}, doi = {10.1145/3603165.3607418}, url = {https://doi.org/10.1145/3603165.3607418}, author = {Zhu, Weijun and Liu, Yang}, keywords = {Classification, Coronary stenosis, Machine learning, location = Wuhan, China}, abstract = {How to accurately classify and diagnose whether an individual has Coronary Stenosis (CS) without invasive physical examination? This problem has not been solved satisfactorily. To this end, the four Machine Learning (ML) algorithms, i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and Random Forest (RF) are employed in this paper. First, eleven features including basic information of an individual, symptoms and results of routine physical examination are selected, as well as one label is specified, indicating whether an individual suffers from different severity of coronary artery stenosis or not. On the basis of it, a set containing one thousand samples is constructed. Second, each of these four ML algorithms learns from the sample set to obtain the corresponding optimal classified results, respectively. The experimental results show that: RF performs better than other three algorithms, and it classifies whether an individual has CS with an accuracy of 95.7\%.} }
@inproceedings{10.1145/3712256.3726461, title = {Rule-based Machine Learning: Separating Rule and Rule-Set Pareto-Optimization for Interpretable Noise-Agnostic Modeling}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {407--415}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3726461}, url = {https://doi.org/10.1145/3712256.3726461}, author = {Lipschutz-Villa, Gabriel and Bandhey, Harsh and Yin, Ruonan and Kamoun, Malek and Urbanowicz, Ryan}, keywords = {rule-based machine learning, learning classifier systems, supervised learning, interpretability, multi-objective optimization, location = NH Malaga Hotel, Malaga, Spain}, abstract = {Rule-based machine learning (RBML) algorithms, e.g. learning classifier systems (LCSs), can capture complex relationships while yielding more interpretable models than most other machine learning algorithms. Traditional LCSs rely on a single fitness function for both rule and/or rule-set optimization. However, ideal rule vs. rule-set discovery often requires distinct and multiple objectives. Recently, hybrid-LCSs were proposed that explicitly separated the task of rule vs. rule-set discovery but relied on distinct single-objective or weighted multi-objective fitness functions. This study introduces a newly developed Heuristic Evolutionary Rule Optimization System (HEROS) that combines previous LCS innovations aimed at tackling noisy, larger-scale, classification tasks, while adopting separation of rule vs. rule-set evolution. Uniquely, HEROS employs a custom Pareto-front-based multi-objective fitness function (for rule discovery) and NSGA-II-style multi-objective optimization (for rule-set discovery) to solve both clean and noisy-signal classification problems agnostically. Rule discovery is driven by rule-accuracy and instance coverage objectives, while rule-set discovery is driven by prediction accuracy and rule-set size objectives. Using diverse simulated benchmark datasets, i.e. noisy (GAMETES) and clean (MUX), we demonstrate proof-of-principle that HEROS can directly discover accurate, highly-compact, interpretable, and ideal solutions when compared to the established 'ExSTraCS' RBML algorithm, without objective weightings or adjusting hyperparameters.} }
@inproceedings{10.1145/3696271.3696285, title = {Machine Learning-Driven Optimization of Livestock Management: Classification of Cattle Behaviors for Enhanced Monitoring Efficiency}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {85--91}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696285}, url = {https://doi.org/10.1145/3696271.3696285}, author = {Zhao, Zhuqing and Shehada, Halah and Ha, Dong and Dos Reis, Barbara and White, Robin and Shin, Sook}, keywords = {HGBDT, RF, RFE, SVM}, abstract = {Monitoring cattle health in remote and expansive pastures poses significant challenges that necessitate automated, continuous, and real-time behavior monitoring. This paper investigates the effectiveness and reliability sensor-based cattle behavior classification for such monitoring, emphasizing the impact of intelligent feature selection in enhancing classification performance. To achieve this, we developed Wireless Sensor Nodes (WSN) affixed to individual cattle, enabling the capture of 3-axis acceleration data from five cows across varying seasons, spanning from summer to winter. Initially, we extracted a comprehensive set of 52 features, representing a broad spectrum of cow behaviors alongside statistical attributes. To enhance computational efficiency, we employed the Recursive Feature Elimination (RFE) method to distill 30 critical features by discarding redundant or less significant ones. Subsequently, these optimized features were utilized to train four machine learning (ML) models: Support Vector Machine (SVM), k-Nearest Neighbors (k-NN), Random Forest (RF), and Histogram-based Gradient Boosted Decision Trees (HGBDT). Notably, the HGBDT model demonstrated superior performance, achieving remarkable F1-scores of 99.01\% for 'grazing', 98.74\% for 'ruminating', 89.62\% for 'lying', 84.06\% for 'standing', and 91.87\% for 'walking'. These findings underscore the potential of our approach to serve as a robust framework for precision livestock farming, offering valuable insights into enhancing cattle health monitoring in remote environments.} }
@inbook{10.1145/3730436.3730465, title = {Anomaly Detection in Human Activity Logs Using Wearable Inertial Sensors and Machine Learning Techniques}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence}, pages = {176--180}, year = {2025}, isbn = {9798400713637}, url = {https://doi.org/10.1145/3730436.3730465}, author = {Ahmed, Muhammad Raisuddin and Srimal, Woshan and Aseeri, Mohammed A and bin Marhaban, Mohammad Hamiruce and Alabdullah, Ahmed A and Kaiser, M Shamim}, abstract = {The wearable inertial sensors enhanced by real-time monitoring employed to monitor and improve human activities relating to health, safety, and well-being. This paper provides a framework for detecting anomalies in activity data, based on records and collected values from accelerometers and gyroscopes with the help of machine learning. By jointly considering advanced signal processing, feature extraction, and Random Forest classification technique optimized with SMO, it solves acute noise, computational costs, and contextual ambiguity issues. The extracted features, like mean and max values, are capable of capturing static and dynamic patterns quite efficiently. The proposed system has shown good improvements in the detection of anomalies in critical events, such as falls and abnormal target behaviour. The framework increases the prospects of safety and improvement in living standards and could evolve into the mainstream market with reliable monitoring systems.} }
@inproceedings{10.1145/3745676.3745695, title = {Malicious Website Detection Optimization in Enterprise Cybersecurity Management: A Machine Learning-Based Decision Support Approach}, booktitle = {Proceedings of the 2025 2nd International Conference on Innovation Management and Information System}, pages = {121--127}, year = {2025}, isbn = {9798400715150}, doi = {10.1145/3745676.3745695}, url = {https://doi.org/10.1145/3745676.3745695}, author = {Zhao, Yiyi and Chen, Yingying and Zhang, Runnan}, keywords = {Enterprise cybersecurity management, Machine learning, Malicious website detection, Strategy optimization}, abstract = {The increasing proliferation of malicious web platforms presents critical risks to organizational cybersecurity, necessitating the creation of advanced detection systems that adapt to evolving cyber threats. Conventional identification approaches frequently prove inadequate against sophisticated attack patterns. This paper proposes an innovative website threat analysis framework combining structured feature selection with t-SNE visualization in data-centric methodologies. Through targeted feature optimization, we improve model transparency and processing effectiveness while minimizing noise in complex datasets. The analytical framework evaluates five widely-adopted classification algorithms, XGBoost, KNN, Naive Bayes, Random Forest, and SVM, to assess detection capabilities. Validation tests using the Malicious/Benign Websites dataset reveal that Random Forest and XGBoost achieved superior performance in identifying malicious domains, maintaining high accuracy and balanced F1-scores. These outcomes highlight the value of embedding adaptive machine learning solutions into corporate security infrastructures to optimize threat recognition, operational responsiveness, and system resource allocation. The study offers practical guidance for enhancing cybersecurity budgeting decisions and developing preemptive protection frameworks within enterprise risk management contexts.} }
@article{10.1145/3653319, title = {ForestEyes: Citizen Scientists and Machine Learning-Assisting Rainforest Conservation}, journal = {Commun. ACM}, volume = {67}, pages = {95--96}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3653319}, url = {https://doi.org/10.1145/3653319}, author = {Fazenda, \'Alvaro L. and Faria, Fabio A.} }
@article{10.1145/3475167, title = {Declarative machine learning systems}, journal = {Commun. ACM}, volume = {65}, pages = {42--49}, year = {2021}, issn = {0001-0782}, doi = {10.1145/3475167}, url = {https://doi.org/10.1145/3475167}, author = {Molino, Piero and R\'e, Christopher}, abstract = {The future of machine learning will depend on it being in the hands of the rest of us.} }
@inproceedings{10.1145/3700906.3700995, title = {A Methodology for Assessing Carbon Emission Indicators Based on Incorporates Machine Learning and Multi-modal data}, booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition}, pages = {556--560}, year = {2024}, isbn = {9798400707032}, doi = {10.1145/3700906.3700995}, url = {https://doi.org/10.1145/3700906.3700995}, author = {Zhang, Shuang and Liu, Jia and Ma, Rui and Sha, Jiangbo and Kang, Wenni and Qu, Fanghao and Kang, Yibin}, keywords = {BP neural network, Carbon emission costs, Carbon emission indicators, Fusion machine learning, Linear relationship, Multi-modal data}, abstract = {Since the status of carbon emission indicators involves many factors, there is often a problem of large errors when evaluating them. For this reason, this paper proposes a study on a carbon emission indicator evaluation method that integrates machine learning and multi-modal data. After comprehensively analyzing the influencing factors of the value of carbon emission rights from the four perspectives of macroeconomics, energy prices, climate change, policy, and the international carbon market, we integrated multi-modal influencing factor data, calculated the corresponding carbon emission costs, and combined them as the input characteristic parameter of the BP neural network, through forward propagation and backward propagation, the linear relationship between it and the value of carbon emission rights is determined, and the corresponding model is constructed to realize the value evaluation of carbon emission rights. In the test results, the fit between the carbon emission rights value assessment results and the actual transaction price has always remained relatively stable, and the specific error has always been within 0.70 yuan/ton, with the maximum error being only 0.66 yuan/ton. Compared with the control group, it has obvious advantages in assessment accuracy and effectiveness respectively.} }
@inproceedings{10.1145/3640429.3640437, title = {Design Thinking Using Qualitative Data Analysis and Machine Learning}, booktitle = {Proceedings of the 2023 13th International Conference on Information Communication and Management}, pages = {40--47}, year = {2024}, isbn = {9798400708114}, doi = {10.1145/3640429.3640437}, url = {https://doi.org/10.1145/3640429.3640437}, author = {Hanan, Moussa and Galal, Galal-Edeen H.}, keywords = {Agile Development, Design Thinking, Machine Learning, Persona creation, Qualitative Data Analysis, location = Cairo, Egypt}, abstract = {Design Thinking is a human-centered approach that allows continuous feedback by the user through Empathizing, Defining, Testing, Ideating and Prototyping. It mainly focuses on user needs, aspirations, wishes, concerns and frustrations in attempting to solve their problems. The Persona Creation approach follows the process of collecting data from multiple sources including social media platforms or the traditional methods including interviews of different users to cover the different types of behaviors, interactions and goals, questionnaires, or surveys. Condensing gathered data using qualitative data analysis renders assessable domain models that can be shared among and modified by stakeholders, so as to agree on user needs and issues. After agreeing on useful needs and user issues, they are used to generate Personas that represent the different types of users of a specific software product. When both approaches Design Thinking and Persona Creation are incorporated during Agile software development, this would lead to the creation of a successful software product. Successful software products are ones that cover all the needs as mentioned by the product user, also known as user perspectives.A user perspective refers to the perception of a given user and how they would use the final product. Those are\&nbsp;the people who would interact with the software product created and, therefore, the people for whom\&nbsp;the software is designed. For this reason, if an application, a website or a functionality that does not meet the final user's needs, this would ultimately result in a failure for the business. Inducing pain points/insights (what is needed by users) should not left totally to the skills of the analyst with little guidance. A systematic and more guidance is needed in this situation. Agile Software Development lack a coherent and explicit technique or open architecture [1] that can accommodate changes mandated by experiments on the ground. In addition, there has to be a method for objectively evaluating resultant prototypes/releases/deliverables at the end of each sprint in a way that can effectively guide path adjustments.Therefore, in this research, we make use of Design Thinking with software products. Through creating a framework that includes Design Thinking as an elicitation technique. We propose a framework composed of two phases: The first phase is the use of a robust qualitative data analysis method, to achieve models that are rich, and at the same time concise and traceable to their origins. We propose the use of the Grounded Theory method in the analysis and integration of the qualitative data that can characterize user needs, pain points and system requirements, in addition to second layer requirements that are often hard to spot. Second layer requirements are those requirements that are not immediately visible or perceivable by the end-user of a system, or those working with or observing him or her, such as systems and requirements analysts. The source of data for generating grounded theoretical formulations include interviews (of whatever type), observations, online chatter, and documents relating to the immediate and wider contexts of the need phenomenon under study.The second part of our proposed framework is applying Machine Learning on the data resulting from the first phase so that we are able to automate the Persona creation using Machine learning. Automatic Persona creation via machine learning is used to represent potential users, as an attempt to enhance the requirements of software products since they will necessarily include user perspectives.} }
@inproceedings{10.1145/3703847.3703853, title = {Advancing Disease Diagnosis and Biomarker Discovery: The Role of Machine Learning in ncRNA Analysis}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {29--35}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703853}, url = {https://doi.org/10.1145/3703847.3703853}, author = {Wang, Dan and Jin, Xin and Li, Shanshan}, keywords = {biomarker, diagnosis, machine learning, ncRNAs}, abstract = {Non-coding RNAs (ncRNAs) are critical regulators in diverse biological processes and pathological mechanisms. Their prospective utility as biomarkers for early disease detection and prognostic evaluation has attracted substantial scholarly interest. However, the complexity and volume of ncRNA data pose challenges in biomarker discovery. Recent advancements in machine learning have provided powerful tools to address these challenges, offering novel approaches for the analysis of ncRNA data. This review explores the integration of machine learning techniques in ncRNA research, focusing on miRNA, circRNA, and lncRNA. We discuss the biological functions of these ncRNAs, traditional methods for their analysis, and how machine learning enhances the discovery of disease biomarkers. Through a detailed examination of case studies, we highlight successful applications of machine learning in identifying ncRNA biomarkers. Additionally, we compare the effectiveness of various machine learning methods across different ncRNA types and address current challenges and future directions in the field. Our review underscores the transformative potential of machine learning in advancing ncRNA biomarker discovery, ultimately contributing to improved diagnostic and therapeutic strategies.} }
@inproceedings{10.1145/3697467.3697644, title = {A System for the Prediction of Fire Pump Failure Based on Internet of Things and Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 4th International Conference on Internet of Things and Machine Learning}, pages = {216--221}, year = {2024}, isbn = {9798400710353}, doi = {10.1145/3697467.3697644}, url = {https://doi.org/10.1145/3697467.3697644}, author = {Yu, Chenguang and Li, Shengli and Liu, Yanqian}, keywords = {Internet of things, STM32, fire pump failure prediction system, machine learning}, abstract = {In order to facilitate the early detection of potential failure in fire pumps, a fire pump failure prediction system has been designed which incorporates Internet of Things (IoT) and machine learning techniques. The system employs an STM32F103RCT6 microcontroller to acquire data from temperature, pressure and water flow sensors. The data is then transmitted to the Internet of Things (IoT) cloud platform via a Wi-Fi module, allowing for real-time monitoring of the fire pump environment. The data from the IoT cloud platform is also subjected to analysis and learning through the application of machine learning algorithms. Three machine learning algorithms, namely k-nearest neighbour, logistic regression and extreme gradient boosting, were employed for the purposes of modelling, training and prediction. It was determined that the accuracy, Kappa coefficient and AUC value of the XGBoost algorithm were superior to those of the other algorithms, and that it demonstrated an excellent capacity for predicting the occurrence of fire pump failure. The system is capable of meeting the real-time monitoring of fire pumps, as well as fault prediction, and of enhancing the reliability of fire pumps.} }
@inproceedings{10.1145/3550082.3564214, title = {Time-Dependent Machine Learning for Volumetric Simulation}, booktitle = {SIGGRAPH Asia 2022 Posters}, year = {2022}, isbn = {9781450394628}, doi = {10.1145/3550082.3564214}, url = {https://doi.org/10.1145/3550082.3564214}, author = {Giraud-Carrier, Samuel and Holladay, Seth and Egbert, Parris}, keywords = {machine learning, ode networks, volumetric simulation, location = Daegu, Republic of Korea}, abstract = {We explore the application of a time-dependent machine learning framework to art direction of volumetric simulations. We show the benefit of the time dependency inherent to the ODE-net model when used in conjunction with simulation sequences. Unlike other machine learning methods which maintain a uniform timestep constraint during evaluation, the ODE-net framework is able to generate results for arbitrary time samples. We demonstrate how this non-uniform time step evaluation can be leveraged for use in artistic direction tasks. We specifically apply the model to the retiming of volumetric simulations to showcase the ability of the machine learning method to properly predict arbitrary time steps. We show that with minimal training data, the model is able to generalize over several simulation sequences with similar parameters.} }
@inproceedings{10.5555/3635637.3662903, title = {Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework}, booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems}, pages = {525--533}, year = {2024}, isbn = {9798400704864}, author = {Esmaeili, Ahmad and Ghorrati, Zahra and Matson, Eric T.}, keywords = {collaborative learning, distributed learning, edge computing, holonic learning, location = Auckland, New Zealand}, abstract = {Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements HoloAvg, a special variant of HoL that employs weighted averaging for model aggregation across all holons. The convergence of the proposed method is validated through experiments on both identically and independently distributed (IID) and Non-IID settings of the standard MNIST dataset. Furthermore, the performance behaviors of HoL are investigated under various holarchical designs and data distribution scenarios. The presented results affirm HoL's prowess in delivering competitive performance particularly, in the context of the Non-IID data distribution.} }
@inproceedings{10.1145/3647444.3647916, title = {Exploring Relatonship between music and mood through machine learning technique}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647916}, url = {https://doi.org/10.1145/3647444.3647916}, author = {Gujar, Shital Shankar and Reha, Ali Yawar}, keywords = {Machine Learning, Mood Classification, Music, Neural Network, location = Jaipur, India}, abstract = {The importance of music in our lives is acknowledged, as it can help us feel happy and put a smile on our faces. Studies have shown that music therapy can improve mental health, and the link between mood and music is a crucial area of research. Machine learning is becoming more prevalent in the analysis of large datasets, and it can provide valuable insights. The goal of this study is to analyze the relationship between mood and music by using machine learning techniques. The findings of this research will be used to develop recommendations for individuals seeking therapeutic music. The paper uses machine learning methods to analyze the link between mood and music. It shows how this connection can be utilized to enhance mental health. The study analyzed a large dataset of music tracks and its associated moods. The results of the analysis revealed that certain musical elements, such as key, tempo, and mode, were associated with specific mood. The findings of this study provide valuable insight into the link between music and mood. They can also help develop recommendations for individuals who seek therapeutic music.} }
@inbook{10.1109/ICSE55347.2025.00122, title = {Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software}, booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}, pages = {743--755}, year = {2025}, isbn = {9798331505691}, url = {https://doi.org/10.1109/ICSE55347.2025.00122}, author = {Chen, Zhenpeng and Li, Xinyue and Zhang, Jie M. and Sarro, Federica and Liu, Yang}, abstract = {Intersectional fairness is a critical requirement for Machine Learning (ML) software, demanding fairness across subgroups defined by multiple protected attributes. This paper introduces FairHOME, a novel ensemble approach using higher order mutation of inputs to enhance intersectional fairness of ML software during the inference phase. Inspired by social science theories highlighting the benefits of diversity, FairHOME generates mutants representing diverse subgroups for each input instance, thus broadening the array of perspectives to foster a fairer decision-making process. Unlike conventional ensemble methods that combine predictions made by different models, FairHOME combines predictions for the original input and its mutants, all generated by the same ML model, to reach a final decision. Notably, FairHOME is even applicable to deployed ML software as it bypasses the need for training new models. We extensively evaluate FairHOME against seven state-of-the-art fairness improvement methods across 24 decision-making tasks using widely adopted metrics. FairHOME consistently outperforms existing methods across all metrics considered. On average, it enhances intersectional fairness by 47.5\%, surpassing the currently best-performing method by 9.6 percentage points.} }
@inproceedings{10.1145/3555776.3578591, title = {SOTERIA: Preserving Privacy in Distributed Machine Learning}, booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing}, pages = {135--142}, year = {2023}, isbn = {9781450395175}, doi = {10.1145/3555776.3578591}, url = {https://doi.org/10.1145/3555776.3578591}, author = {Brito, Cl\'audia and Ferreira, Pedro and Portela, Bernardo and Oliveira, Rui and Paulo, Jo\~ao}, keywords = {apache spark, machine learning, Intel SGX, privacy-preserving, location = Tallinn, Estonia}, abstract = {We propose Soteria, a system for distributed privacy-preserving Machine Learning (ML) that leverages Trusted Execution Environments (e.g. Intel SGX) to run code in isolated containers (enclaves). Unlike previous work, where all ML-related computation is performed at trusted enclaves, we introduce a hybrid scheme, combining computation done inside and outside these enclaves. The conducted experimental evaluation validates that our approach reduces the runtime of ML algorithms by up to 41\%, when compared to previous related work. Our protocol is accompanied by a security proof, as well as a discussion regarding resilience against a wide spectrum of ML attacks.} }
@inproceedings{10.1145/3639592.3639617, title = {IoT Based Accident Prevention System using Machine Learning techniques}, booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference}, pages = {179--188}, year = {2024}, isbn = {9798400716225}, doi = {10.1145/3639592.3639617}, url = {https://doi.org/10.1145/3639592.3639617}, author = {Alnashwan, Raghad and Mashaabi, Malak and Alotaibi, Areej and Qudaih, Hala and Albraheem, Lamya}, abstract = {The likelihood of car accidents increases during extreme weather conditions, such as fog, winds, snow, rain, etc. While it may not be possible to prevent all such accidents, their incidence can be reduced by taking proper measures. Therefore, an intelligent accident-avoidance system is necessary to predict the severity of accidents based on weather and road conditions. This research paper suggests three machine learning (ML) methods for an Internet of Things (IoT)-based accident severity prediction system. The methods are Random Forest, LightGBM, and XGBoost.The aim is to predict the severity of car accidents based on various weather features using a machine learning model. However, considering the previous work, we observed that the size of datasets is frequently minimal, and some of the research discusses the influence of the weather on the number of accidents. Therefore, we used the Countrywide Traffic Accident Dataset, which covers 2.8 million vehicle accidents in the United States from 2016 to 2021. In conclusion, our methodology appears to be efficient in predicting the severity of car accidents. Among the three methods, LightGBM achieved the highest prediction accuracy (72\%), precision (70\%), recall (70\%), F1-scores (70\%), and area curve (AUC) (0.86) of the receiver operating characteristic (ROC) curve.} }
@inproceedings{10.1145/3715275.3732046, title = {Measuring Machine Learning Harms from Stereotypes Requires Understanding Who Is Harmed by Which Errors in What Ways}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {746--762}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732046}, url = {https://doi.org/10.1145/3715275.3732046}, author = {Wang, Angelina and Bai, Xuechunzi and Barocas, Solon and Blodgett, Su Lin}, keywords = {stereotypes, machine learning fairness, harms from biases}, abstract = {Despite a proliferation of research on the ways that machine learning models can propagate harmful stereotypes, very little of this work is grounded in the psychological experiences of people exposed to such stereotypes. We use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use surveys to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experiential harm, while having minimal impact on participants’ cognitive beliefs, attitudes, or behaviors. This experiential harm impacts participants who are women more than those who are men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conclude by proposing a more nuanced perspective on the harms of machine learning errors—one that depends on who is experiencing what harm and why.} }
@inproceedings{10.1145/3745238.3745503, title = {Research on Business-finance Integration, Digital Transformation and Corporate Performance based on Dual Machine Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1698--1702}, year = {2025}, isbn = {9798400712791}, doi = {10.1145/3745238.3745503}, url = {https://doi.org/10.1145/3745238.3745503}, author = {Zheng, Qidong and Zhang, Xin and Ding, Mingyi}, keywords = {Business-finance integration, Causal inference, Digital transformation, Dual machine learning, Enterprise performance}, abstract = {With the rapid development of digital economy, business-financial integration and digital transformation have become an important means for enterprises to improve operational efficiency and enhance competitiveness. Based on the pairwise machine learning method, this paper explores the impact mechanism of industry-finance integration and digital transformation on enterprise performance, analyzes the mediating role of industry-finance integration in digital transformation through causal inference framework, big data analysis and machine learning algorithms, and further explores its long-term effect on enterprise performance. The results of the study show that business-financial integration can effectively promote the implementation of digital transformation, thereby improving enterprise financial performance and market competitiveness.} }
@inproceedings{10.1145/3675888.3676040, title = {PQTD-TM: Integrating Information Retrieval with Machine Learning for effective Placement Topic Discovery}, booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing}, pages = {122--127}, year = {2024}, isbn = {9798400709722}, doi = {10.1145/3675888.3676040}, url = {https://doi.org/10.1145/3675888.3676040}, author = {Sardana, Neetu and Kumar, Prabudh and Jain, Nehal and Goyal, Kritank Rishi and Arora, Anuja and Varshney, Deepika}, keywords = {Campus Placement, Information Retrieval, Machine Learning, TFIDF, location = Noida, India}, abstract = {Technical interviews, particularly coding rounds, are pivotal in shaping career trajectories, especially for roles in top-tier companies like Amazon, Google, and Microsoft. Understanding the patterns and nuances of coding problems and their associated tags is essential for effective interview preparation. To aid candidates in excelling in these assessments, we analyzed coding problems encountered in such interviews. We compile a comprehensive dataset of coding problems and their associated tags, sourced from real technical interviews, ensuring reliability and relevance. We propose a topic prediction approach for placement related questions, PQTD-TM (Placement question topic discovery using TF IDF and Machine Learning). The proposed technique integrates information retrieval technique (TF-IDF) with six machine learning classifiers for prediction. We evaluated the predictive performance of various machine learning models, including Logistic Regression, Decision Trees, Naive Bayes, KNN, One-vs-Rest Classifier, and Random Forest. Random forest attained the best prediction accuracy of 93\%} }
@article{10.1145/3617380, title = {Multi-objective Feature Attribution Explanation For Explainable Machine Learning}, journal = {ACM Trans. Evol. Learn. Optim.}, volume = {4}, year = {2024}, doi = {10.1145/3617380}, url = {https://doi.org/10.1145/3617380}, author = {Wang, Ziming and Huang, Changwu and Li, Yun and Yao, Xin}, keywords = {Explainable machine learning, feature attribution explanations, multi-objective learning, multi-objective evolutionary algorithms}, abstract = {The feature attribution-based explanation (FAE) methods, which indicate how much each input feature contributes to the model’s output for a given data point, are one of the most popular categories of explainable machine learning techniques. Although various metrics have been proposed to evaluate the explanation quality, no single metric could capture different aspects of the explanations. Different conclusions might be drawn using different metrics. Moreover, during the processes of generating explanations, existing FAE methods either do not consider any evaluation metric or only consider the faithfulness of the explanation, failing to consider multiple metrics simultaneously. To address this issue, we formulate the problem of creating FAE explainable models as a multi-objective learning problem that considers multiple explanation quality metrics simultaneously. We first reveal conflicts between various explanation quality metrics, including faithfulness, sensitivity, and complexity. Then, we define the considered multi-objective explanation problem and propose a multi-objective feature attribution explanation (MOFAE) framework to address this newly defined problem. Subsequently, we instantiate the framework by simultaneously considering the explanation’s faithfulness, sensitivity, and complexity. Experimental results comparing with six state-of-the-art FAE methods on eight datasets demonstrate that our method can optimize multiple conflicting metrics simultaneously and can provide explanations with higher faithfulness, lower sensitivity, and lower complexity than the compared methods. Moreover, the results have shown that our method has better diversity, i.e., it provides various explanations that achieve different tradeoffs between multiple conflicting explanation quality metrics. Therefore, it can provide tailored explanations to different stakeholders based on their specific requirements.} }
@inproceedings{10.1145/3638584.3638667, title = {Scaling Machine Learning with a Ring-based Distributed Framework}, booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence}, pages = {23--32}, year = {2024}, isbn = {9798400708688}, doi = {10.1145/3638584.3638667}, url = {https://doi.org/10.1145/3638584.3638667}, author = {Zhao, Kankan and Leng, Youfang and Zhang, Hui}, keywords = {Artificial Intelligence, Machine Learning, Parameter Server, Ring-based Distributed Framework, location = Beijing, China}, abstract = {In centralized distributed machine learning systems, communication overhead between servers and computing nodes has always been an important issue affecting the training efficiency. Although existing research has proposed various measures to reduce communication overhead between nodes in parameter server frameworks, the communication pressure and overhead inherited from centralized architectures are still significant. To address the above issue, this paper proposes a ring-based parameter server framework that is distinct from node division and model training mechanism in the standard p/s framework. The ring-based architecture cancels the global model stored on the server side, and each computing node stores a local copy of the model. During model training, computing nodes can asynchronously train local models based on local or remote training data. After all nodes finish learning, the ensemble learning method can predict test data based on all local models. To avoid the negative impact of remote data reading on model training efficiency, a producer-consumer data reading strategy is proposed. This strategy can reduce data reading overhead in a pipeline manner. To make rational use of the input and output bandwidths of all nodes, a circular data scheduling mechanism is proposed. At any given time, this mechanism ensures each node has at most one input stream and one output stream, thereby dispersing communication pressure. The experimental results show that the proposed distributed architecture achieves significantly better performance (1.7\%-2.1\% RMSE) than the state-of-the-art baselines and also achieves a 2.2x-3.4x speedup when reaching a comparable RMSE performance.} }
@inproceedings{10.1145/3637528.3671461, title = {Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6470--6479}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671461}, url = {https://doi.org/10.1145/3637528.3671461}, author = {Derezi\'nski, Micha and Mahoney, Michael W.}, keywords = {matrix computations, optimization, randomization, statistics, location = Barcelona, Spain}, abstract = {Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.} }
@article{10.1145/3610536, title = {Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview}, journal = {ACM Trans. Evol. Learn. Optim.}, volume = {3}, year = {2023}, doi = {10.1145/3610536}, url = {https://doi.org/10.1145/3610536}, author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merch\'an, Eduardo C. and Branke, Juergen and Bischl, Bernd}, keywords = {Multi-objective hyperparameter optimization, neural architecture search, Bayesian optimization}, abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.} }
@inproceedings{10.1145/3587716.3587731, title = {Baileys: An Efficient Distributed Machine Learning Framework by Dynamic Grouping}, booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing}, pages = {92--96}, year = {2023}, isbn = {9781450398411}, doi = {10.1145/3587716.3587731}, url = {https://doi.org/10.1145/3587716.3587731}, author = {Ni, Chengdong and Du, Haizhou}, keywords = {Distributed machine learning, Dynamic grouping, Multi-server architecture, System heterogeneity, location = Zhuhai, China}, abstract = {Many machine-learning applications rely on distributed machine learning (DML) systems to train models from massive datasets using massive computing resources (e.g., GPUs and TPUs). However, given a DML system in most applications, its parameter synchronization mechanism is fixed and independent from the types and amounts of resources available to the system. In this paper, we argue that given an application, the synchronization mechanism in its DML system should be co-designed with the available resources in the heterogeneous cluster. To this end, we design an efficient parameter synchronization framework called Baileys. First, Baileys retrieves resource information from the heterogeneous clusters and uses such information to partition heterogeneous workers into multiple groups dynamically. Second, Baileys develops efficient group-based parameter synchronization mechanisms to converge model parameters quickly and accurately. We implement a prototype of Baileys and demonstrate its efficiency and efficacy through experiments. Results show that Baileys can reduce block time up to 33.2\% and decrease the test error up to 39.7\%.} }
@inproceedings{10.1145/3603287.3656163, title = {An Augmented Machine Learning-Based Course Enrollment Recommender System}, booktitle = {Proceedings of the 2024 ACM Southeast Conference}, pages = {319--320}, year = {2024}, isbn = {9798400702372}, doi = {10.1145/3603287.3656163}, url = {https://doi.org/10.1145/3603287.3656163}, author = {Zhu, Lizi and Perchyk, Oleg and Wang, Xiwei}, keywords = {Collaborative Filtering, Contextual Information, Machine Learning, Matrix Factorization, Students Demographics, location = Marietta, GA, USA}, abstract = {Higher education has been undergoing a transformation in many aspects such as course reorganization and technology adoption. Many universities keep updating their curriculum to account for changes. This, however, poses a great challenge to both students and advisors. This paper proposes a new approach to course recommender system that takes into consideration the contextual information such as students demographics and courses description.} }
@inproceedings{10.1145/3704522.3704553, title = {Short Paper: Predicting and Analyzing EV Energy Consumption in Bangladesh : A Machine Learning Approach}, booktitle = {Proceedings of the 11th International Conference on Networking, Systems, and Security}, pages = {222--227}, year = {2025}, isbn = {9798400711589}, doi = {10.1145/3704522.3704553}, url = {https://doi.org/10.1145/3704522.3704553}, author = {Haque, F.M. Mahmudul and Tabassum, Humayra and Amin, Md Fazal and Jony, Md Nazrul Islam and Dey, Shamik and Quaium, Adnan}, keywords = {Machine learning, Energy, EV charging, EV, Electric Power, Bangladesh}, abstract = {The increasing adoption of EVs in Bangladesh poses challenges to the existing power grid [15]. This study develops a machine learning model to accurately predict EV energy consumption at charging stations [23]. using California City data as a proxy [8]. Evaluating algorithms like Random Forest, Gradient Boosting Regression (GBR), and Decision Tree [3] [7], Gradient Boosting outperformed with an R² of 0.9999965 and an MAE of 0.0071. Beyond predictions, this research optimizes energy management, assesses grid impacts, and supports sustainable infrastructure development, offering crucial insights for integrating EVs into Bangladesh’s transportation sector.} }
@inproceedings{10.1145/3611643.3617845, title = {Detecting Overfitting of Machine Learning Techniques for Automatic Vulnerability Detection}, booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages = {2189--2191}, year = {2023}, isbn = {9798400703270}, doi = {10.1145/3611643.3617845}, url = {https://doi.org/10.1145/3611643.3617845}, author = {Risse, Niklas}, keywords = {automatic vulnerability detection, large language models, machine learning, semantic preserving transformations, location = San Francisco, CA, USA}, abstract = {Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function f, models trained by machine learning techniques can decide if f contains a security flaw with up to 70\% accuracy. But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model’s accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels. In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose a cross validation algorithm, where a semantic preserving transformation is applied during the amplification of either the training set or the testing set. Using 11 transformations and 3 ML techniques, we find that the improved robustness only applies to the specific transformations used during training data amplification. In other words, the robustified models still rely on unrelated features for predicting the vulnerabilities in the testing data.} }
@inproceedings{10.1145/3626246.3655014, title = {Eighth Workshop on Data Management for End-to-End Machine Learning (DEEM)}, booktitle = {Companion of the 2024 International Conference on Management of Data}, pages = {651--652}, year = {2024}, isbn = {9798400704222}, doi = {10.1145/3626246.3655014}, url = {https://doi.org/10.1145/3626246.3655014}, author = {Hulsebos, Madelon and Interlandi, Matteo and Shankar, Shreya}, keywords = {data management, machine learning, systems, location = Santiago AA, Chile}, abstract = {The DEEM'24 workshop (Data Management for End-to-End Machine Learning) is held on Sunday June 9th, in conjunction with SIGMOD/PODS 2024. DEEM brings together researchers and practitioners at the intersection of applied machine learning, data management and systems research, with the goal to discuss the arising data management issues in ML application scenarios. The workshop solicits regular research papers (8 pages) describing preliminary and ongoing research results, including industrial experience reports of end-to-end ML deployments, related to DEEM topics. In addition, DEEM 2023 has a category for short papers (4 pages) as a forum for sharing interesting use cases, problems, datasets, benchmarks, visionary ideas, system designs, preliminary results, and descriptions of system components and tools related to end-to-end ML pipelines. This year, the workshop received 16 high-quality submissions on diverse topics relevant to DEEM, of which 8 regular papers and 8 short papers.} }
@inproceedings{10.1145/3676536.3697114, title = {Co-Designing NVM-based Systems for Machine Learning and In-memory Search Applications}, booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design}, year = {2025}, isbn = {9798400710773}, doi = {10.1145/3676536.3697114}, url = {https://doi.org/10.1145/3676536.3697114}, author = {Henkel, J\"org and Siddhu, Lokesh and Nassar, Hassan and Bauer, Lars and Chen, Jian-Jia and Hakert, Christian and Seidl, Tristan and Chen, Kuan Hsun and Hu, Xiaobo Sharon and Li, Mengyuan and Yang, Chia-Lin and Wei, Ming-Liang}, abstract = {With the rapid development of the Internet of Things, machine learning applications on edge devices with limited resources face challenges due to large data scales and irregular memory access patterns. Non-volatile memory (NVM) technologies provide promising solutions by offering larger capacity, low leakage power, and data persistence. In this paper, we discuss the potential of NVM technology in enhancing machine learning applications by improving energy efficiency and reducing latency through in-memory computation and different NVM write modes. The insights from this analysis provide valuable guidance to device researchers and system architects working to develop highperformance systems for machine learning and accelerators in large-scale search applications using NVMs.} }
@inbook{10.1145/3745238.3745498, title = {Analysis and Prediction of Cycling Behavior's Effect on Sleep Quality by Means of Machine Learning}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1655--1668}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745498}, author = {Huang, Jinqiang}, abstract = {Sleep quality is quite important for health nowadays, influencing how the body works and how it feels mentally. Physical activity can be an alternative without medication to make sleep better. But various kinds of exercise have unique effects on how sleep is helped. Cycling is common as moderate aerobic activity having special body and mind effects giving a big influence on sleep. We examined how cycling frequency, intensity, and duration affect sleep duration, trouble falling asleep, and how content one feels using surveys mixed with machine learning, developing many prediction models. Results are positive for cycling improving sleep factors, with random forest model being best for predicting cycling's sleep effect. Strong data for health and policy is provided by this study, showing why cycling matters for healthy life now.} }
@inproceedings{10.1145/3724154.3724282, title = {Machine Learning-XGBoost Analysis of Re-employment Intention of Full-time Mothers}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {771--774}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724282}, url = {https://doi.org/10.1145/3724154.3724282}, author = {Wu, Junbi and Zhou, Jie}, keywords = {Artificial intelligence algorithm, Re-employment intention, XGBoost}, abstract = {This study focused on the effects of social support from strong and weak relationships on the re-employment intention of full-time mothers after the implementation of the three-child birth policy in China. This study received 358 valid questionnaires, and utilized the XGBoost machine learning algorithm to create model. After conducting an optimal search for the network parameters, we achieved an accuracy of 97.22\% on the test set. Furthermore, XGBoost can effectively rank the importance of independent variables, facilitating timely and effective prediction and adjustment of re-employment intention based on significant factors.} }
@inproceedings{10.1145/3701625.3701696, title = {Identifying Concerns When Specifying Machine Learning-Enabled Systems: A Perspective-Based Approach}, booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality}, pages = {673--675}, year = {2024}, isbn = {9798400717772}, doi = {10.1145/3701625.3701696}, url = {https://doi.org/10.1145/3701625.3701696}, author = {Villamizar, Hugo and Kalinowski, Marcos}, keywords = {Requirements Specification, Machine Learning, Software Quality}, abstract = {Engineering successful machine learning (ML)-enabled systems poses various challenges from both a theoretical and a practical side. Among those challenges are how to effectively address unrealistic expectations of ML capabilities from customers, managers and even other team members, and how to connect business value to engineering and data science activities composed by interdisciplinary teams. In this thesis, we studied the state of the practice and literature of requirements engineering (RE) for ML to propose\&nbsp;PerSpecML, a perspective-based approach for specifying ML-enabled systems that helps practitioners identify which attributes, including ML and non-ML components, are important to contribute to the overall system’s quality. The approach involves analyzing 60 concerns related to 28 tasks that practitioners typically face in ML projects, grouping them into five perspectives: system objectives, user experience, infrastructure, model, and data. The conception of \&nbsp;PerSpecML involved a series of validations conducted in different contexts: (i) in academia, (ii) with industry representatives, and (iii) in two real industrial case studies. As a result of the diverse validations and continuous improvements,\&nbsp;PerSpecML showed a positive impact to the specification of ML-enabled systems, particularly helping to specify key quality components that would have been otherwise missed without using\&nbsp;PerSpecML.} }
@inproceedings{10.1145/3718751.3718892, title = {Stock Return Prediction Using Hybrid Machine Learning Method Based on Optiver Company Data}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {866--870}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718892}, url = {https://doi.org/10.1145/3718751.3718892}, author = {Yang, Jiahua and Guo, Yutao and Meng, Lingdi and Zhu, Yi and Geng, Weiyi}, keywords = {Hybrid Machine Learning, Quantitative Finance, Stock Return Prediction}, abstract = {In this paper, we present a novel approach for stock return prediction based on the optiver company stock data using a hybrid machine learning method. In this project, we utilize the advanced models like LightGBM, CatBoost, neural networks and we engineered a powerful ensemble technique to improve the quality of prediction and strengthen the accuracy and robustness. We leverage the best of each and complement their shortcomings by developing a hybrid model to improve performance. Extensive experimental results show that the hybrid method we proposed is superior to both the conventional method and all individual methods. More specifically, the hybrid model with a mean absolute error (MAE) of 5.466 is lower than the MAE for the models on their own: XGBoost (MAE: 5.602), support vector machine (MAE: 5.910), LightGBM (MAE: 5.512), CatBoost (MAE: 5.566). These outcomes highlight the successfulness of the Hybrid Method as it accurately predicts market trends by capturing complex relationships within stock return data. These contributions are important for the domain of financial machine learning because they provide a simple yet powerful approach for improving the performance of stock return forecasting based on ensemble learning methods.} }
@inproceedings{10.1145/3529399.3529400, title = {Machine Learning in Textual Criticism: An examination of the performance of supervised machine learning algorithms in reconstructing the text of the Greek New Testament}, booktitle = {Proceedings of the 2022 7th International Conference on Machine Learning Technologies}, pages = {1--5}, year = {2022}, isbn = {9781450395748}, doi = {10.1145/3529399.3529400}, url = {https://doi.org/10.1145/3529399.3529400}, author = {Jones, Mason and Romano, Francesco and Mohd, Abidalrahman}, keywords = {Koine Greek, New Testament, Textual criticism, location = Rome, Italy}, abstract = {New Testament textual criticism is a field of research that seeks to confidently establish the text of the New Testament by comparing thousands of manuscripts, finding the variants between these manuscripts, and making informed decisions as to the original text based on the internal and external features of those manuscripts. By examining such features as date of composition, textual family, and variant length, scholars are able to determine the correct reading of a text with a high degree of confidence. The use of computing in this field has been documented since at least the 1970’s, but they have not been applied to the task of textual criticism itself. Rather, computers have been used primarily to classify manuscripts and determine their relationships to each other. Our research in this paper takes a new approach by applying machine learning algorithms to the task of textual criticism. After testing multiple supervised learning algorithms, our research finds that support vector machines and decision trees outperform the other tested and achieve 98.8\% accuracy.} }
@inproceedings{10.1145/3603166.3632535, title = {Machine Learning Inference on Serverless Platforms Using Model Decomposition}, booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing}, year = {2024}, isbn = {9798400702341}, doi = {10.1145/3603166.3632535}, url = {https://doi.org/10.1145/3603166.3632535}, author = {Gallego, Adrien and Odyurt, Uraz and Cheng, Yi and Wang, Yuandou and Zhao, Zhiming}, keywords = {serverless computing, machine learning, model decomposition, inference, location = Taormina (Messina), Italy}, abstract = {Serverless offers a scalable and cost-effective service model for users to run applications without focusing on underlying infrastructure or physical servers. While the Serverless architecture is not designed to address the unique challenges posed by resource-intensive workloads, e.g., Machine Learning (ML) tasks, it is highly scalable. Due to the limitations of Serverless function deployment and resource provisioning, the combination of ML and Serverless is a complex undertaking. We tackle this problem through decomposition of large ML models into smaller sub-models, referred to as slices. We set up ML inference tasks using these slices as a Serverless workflow, i.e., sequence of functions. Our experimental evaluations are performed on the Serverless offering by AWS for demonstration purposes, considering an open-source format for ML model representation, Open Neural Network Exchange. Achieved results portray that our decomposition method enables the execution of ML inference tasks on Serverless, regardless of the model size, benefiting from the high scalability of this architecture while lowering the strain on computing resources, such as required run-time memory.} }
@inproceedings{10.1145/3724154.3724354, title = {Research on Financial Risk Management and Investment Strategies Based on Bayesian Machine Learning Models}, booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management}, pages = {1228--1234}, year = {2025}, isbn = {9798400711862}, doi = {10.1145/3724154.3724354}, url = {https://doi.org/10.1145/3724154.3724354}, author = {Liu, Chang}, keywords = {Bayesian Risk Model, Machine Learning, Python, Quantitative Finance, Risk Allocation}, abstract = {Traditional quantitative investment tactics are facing limitations by the growing of data and the increasing complexity of financial markets. Machine learning (ML) has been a transformative approach in quantitative finance. This paper explores ML-based strategies, focusing on adding more risk considerations into the quantitative stock selection strategy, and back tested and compared, and reached the relevant conclusions, to provide some help for the increasing market uncertainty faced by investors.} }
@inproceedings{10.1145/3486001.3486248, title = {Machine Learning for Databases}, booktitle = {Proceedings of the First International Conference on AI-ML Systems}, year = {2021}, isbn = {9781450385947}, doi = {10.1145/3486001.3486248}, url = {https://doi.org/10.1145/3486001.3486248}, author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei}, abstract = {Machine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously learning the optimization strategy from historical data or explorations. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including (i) NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), (ii) regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and (iii) prediction problems (e.g., transaction scheduling, trend prediction). We review existing machine learning based techniques to address these problems and provide research challenges.} }
@inproceedings{10.1145/3656019.3676952, title = {FriendlyFoe: Adversarial Machine Learning as a Practical Architectural Defense against Side Channel Attacks}, booktitle = {Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques}, pages = {338--350}, year = {2024}, isbn = {9798400706318}, doi = {10.1145/3656019.3676952}, url = {https://doi.org/10.1145/3656019.3676952}, author = {Nam, Hyoungwook and Pothukuchi, Raghavendra Pradyumna and Li, Bo and Kim, Nam Sung and Torrellas, Josep}, keywords = {Hardware security, Machine learning, Side-channel analysis, location = Long Beach, CA, USA}, abstract = {Machine learning (ML)-based side channel attacks have become prominent threats to computer security. These attacks are often powerful, as ML models easily find patterns in signals. To address this problem, this paper proposes dynamically applying Adversarial Machine Learning (AML) to obfuscate side channels. The rationale is that it has been shown that intelligently injecting an adversarial perturbation can confuse ML classifiers. We call this approach FriendlyFoe and the neural network we introduce to perturb signals FriendlyFoe Defender. FriendlyFoe is a practical, effective, and general architectural technique to obfuscate signals. We show a workflow to design Defenders with low overhead and information leakage, and to customize them for different environments. Defenders are transferable, i.e., they thwart attacker classifiers that are different from those used to train the Defenders. They also resist adaptive attacks, where attackers train using the obfuscated signals collected while the Defender is active. Finally, the approach is general enough to be applicable to different environments. We demonstrate FriendlyFoe against two side channel attacks: one based on memory contention and one on system power. The first example uses a hardware Defender with ns-level response time that, for the same level of security as a Pad-to-Constant scheme, has 27\% and 64\% lower performance overhead for single- and multi-threaded workloads, respectively. The second example uses a software Defender with ms-level response time that reduces leakage by 3.7 over a state-of-the-art scheme while reducing the energy overhead by 22.5\%.} }
@inproceedings{10.1145/3640115.3640189, title = {Research on Network Security Technology Based on Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering}, pages = {455--461}, year = {2024}, isbn = {9798400708299}, doi = {10.1145/3640115.3640189}, url = {https://doi.org/10.1145/3640115.3640189}, author = {Han, Fei}, keywords = {Machine learning, Network intrusion, Network security, Random Forest, location = Changde, Hunan, China}, abstract = {In recent years, with the popularization of the Internet, the network has penetrated into all aspects of individuals and society. Internet technology has made people's daily lives more intelligent and convenient, but the accompanying network security issues cannot be ignored. Network vulnerabilities and illegal intrusions can affect internet security, and even affect the construction of national infrastructure, critical defense systems, and even military systems. This article selects 50000 sets of data from the KDD99 cybersecurity dataset provided by the National Security Administration of the United States. Each sample in the dataset includes 41 feature attributes and 1 decision attribute. We constructed a model based on the random forest algorithm, using 30\% of the total sample data as the test set and the remaining 70\% of the data as the training set. Then, 100 sets of data were taken for model training. After the training was completed, the detection model successfully predicted 98 intrusions out of 100 detections, and the final score of the test set was 0.982, basically achieving the effect of intrusion detection. A machine learning algorithm model is provided for detecting network intrusions.} }
@inproceedings{10.1145/3654522.3654577, title = {ChainSniper: A Machine Learning Approach for Auditing Cross-Chain Smart Contracts}, booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology}, pages = {223--230}, year = {2024}, isbn = {9798400716713}, doi = {10.1145/3654522.3654577}, url = {https://doi.org/10.1145/3654522.3654577}, author = {Tran, Tuan-Dung and Vo, Kiet Anh and Phan, Duy The and Tan, Cam Nguyen and Pham, Van-Hau}, keywords = {Cross-chain, Machine Learning, Smart Contract, Vulnerability, location = Ho Chi Minh City, Vietnam}, abstract = {Smart contracts are autonomous programs stored on blockchain networks that self-execute agreed terms in a transparent and accurate manner. Within cross-chain platforms, smart contracts facilitate interaction and exchange of data between diverse blockchains. However, the presence of vulnerabilities in smart contracts renders them susceptible to exploitation, jeopardizing security. Considerable research has focused on identifying and detecting such vulnerabilities, though existing approaches have yet to achieve comprehensive coverage. This paper presents ChainSniper, a sidechain-based framework integrating machine learning to automatically appraise vulnerabilities in cross-chain smart contracts. A comprehensive dataset, denoted "CrossChainSentinel", was compiled comprising 300 manually labeled code snippets. This dataset was leveraged to train machine learning models discerning vulnerable versus secure smart contracts. Experimental findings demonstrate the viability of machine learning methodologies for enhancing smart contract auditing within decentralized applications spanning multiple networks. Notable detection precision was achieved, substantiating ChainSniper’s potential to strengthen security analysis through an automated and expansive evaluation of smart contract code.} }
@inproceedings{10.1145/3677404.3677410, title = {Optimization of Beer Yeast Centrifuge Separation Based on Machine Learning Algorithms}, booktitle = {Proceedings of the 2024 International Academic Conference on Edge Computing, Parallel and Distributed Computing}, pages = {35--40}, year = {2024}, isbn = {9798400718168}, doi = {10.1145/3677404.3677410}, url = {https://doi.org/10.1145/3677404.3677410}, author = {Li, Huabin and Cui, Yunqian}, abstract = {Beer yeast is one of the indispensable microorganisms in the beer brewing process, and its centrifugal separation is a crucial step in the brewing industry. The traditional centrifugal separation method relies on experience and trial and error, and has problems of low efficiency and resource waste. With the development of machine learning algorithms, utilizing their advantages to optimize the centrifugal separation process has become possible. This study aims to optimize the centrifugal separation process of beer yeast using machine learning algorithms. This article proposes a centrifugal separation optimization method based on machine learning algorithms. Firstly, this article collected a large amount of experimental data, including key parameters such as yeast concentration, centrifugation speed, and centrifugation time. Then, this article used these data to train a machine learning model to predict the optimal centrifugal conditions. Compared with traditional methods, the results of this article indicate that machine learning algorithms can significantly improve the efficiency and quality of beer yeast centrifugal separation. This study provides a reference for optimizing methods in the brewing industry, with the potential to reduce production costs and improve product quality. The experimental results showed that the separation efficiency before optimization was only 0.80, but after optimization, it was increased to 0.92, and the yeast purity was also increased from 0.80 to 0.88.} }
@inproceedings{10.1145/3671127.3699688, title = {Improving Cyber-Physical Building Energy System via Large-Scale Machine Learning Evaluation}, booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages = {262--263}, year = {2024}, isbn = {9798400707063}, doi = {10.1145/3671127.3699688}, url = {https://doi.org/10.1145/3671127.3699688}, author = {Deng, Yang}, keywords = {ML evaluation, automation, cyber-physical, smart building, location = Hangzhou, China}, abstract = {Machine learning (ML) is playing a crucial role in almost every business sector. In the building automation sector, the Industries (e.g., Siemens, Honeywell) depend on AI to introduce new services, e.g., ML-based HVAC control. Despite recent advances in this area and many ML-based solutions have been developed, however, the building practitioner argued that published ML research is sometimes not driving sufficient real-world impact and is hard to deploy. This triggered us to promote ML deployment through evaluation instead of developing new ML models. In our current work, we propose a behavior testing methodology to systematically evaluate the ML building energy forecasting model; and two data augmentation solutions to solve the data shortage problem while testing ML models; and we developed a visualization tool contains these research outputs, which is user-friendly to the practitioners.} }
@inproceedings{10.1145/3644116.3644174, title = {Study on Machine Learning based Heart Disease Prediction Model}, booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science}, pages = {346--352}, year = {2024}, isbn = {9798400708138}, doi = {10.1145/3644116.3644174}, url = {https://doi.org/10.1145/3644116.3644174}, author = {Zhang, Shihan}, abstract = {In this study, this paper examined and compared the performance of two different machine learning models (logistic regression and random forest) in predicting the presence of heart disease using a dataset with 13 characteristics. While the logistic regression model provided reasonable performance with an accuracy of 82.47\%, the random forest model significantly outperformed logistic regression, achieving an impressive 99.03\% accuracy. In all models, features such as "cp" "thal" and "oldpeak" have been consistently recognized as important predictors of heart disease. These findings highlight the potential of advanced machine learning techniques, in particular random forest, to improve predictive accuracy in health risk assessments. The study also highlights the importance of considering various machine learning models to determine the most efficient way to predict specific data sets and tasks. Future research should focus on validating these models with different, larger datasets and exploring the integration of these models into clinical decision support systems.} }
@inproceedings{10.1145/3503161.3548549, title = {CurML: A Curriculum Machine Learning Library}, booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, pages = {7359--7363}, year = {2022}, isbn = {9781450392037}, doi = {10.1145/3503161.3548549}, url = {https://doi.org/10.1145/3503161.3548549}, author = {Zhou, Yuwei and Chen, Hong and Pan, Zirui and Yan, Chuanhao and Lin, Fanqi and Wang, Xin and Zhu, Wenwu}, keywords = {curriculum learning, machine learning, training strategy, location = Lisboa, Portugal}, abstract = {Curriculum learning (CL) is a machine learning paradigm gradually learning from easy to hard, which is inspired by human curricula. As an easy-to-use and general training strategy, CL has been widely applied to various multimedia tasks covering images, texts, audios, videos, etc. The effectiveness of CL has recently facilitated an increasing number of new CL algorithms. However, there has been no open-source library for curriculum learning, making it hard to reproduce, evaluate and compare the numerous CL algorithms on fair benchmarks and settings. To ease and promote future research on CL, we develop CurML, the first \&lt;u\&gt;Cur\&lt;/u\&gt;riculum \&lt;u\&gt;M\&lt;/u\&gt;achine \&lt;u\&gt;L\&lt;/u\&gt; earning library to integrate existing CL algorithms into a unified framework. It is convenient to use and flexible to customize by calling the provided five APIs, which are designed for easily plugging into a general training process and conducting the data-oriented, model-oriented and loss-oriented curricula. Furthermore, we present empirical results obtained by CurML to demonstrate the advantages of our library. The code is available online at https://github.com/THUMNLab/CurML.} }
@inproceedings{10.1145/3669947.3669963, title = {Multi-Label Emotion Classification of Online Learners' Reviews Using Machine Learning}, booktitle = {Proceedings of the 2024 5th International Conference on Education Development and Studies}, pages = {59--64}, year = {2024}, isbn = {9798400718083}, doi = {10.1145/3669947.3669963}, url = {https://doi.org/10.1145/3669947.3669963}, author = {Makhoukhi, Hajar and Roubi, Sarra}, keywords = {Emotions Recognition, Machine Learning, Multi-label Classification, Online Learning, location = Cambridge, United Kingdom}, abstract = {Text- based emotion recognition is one of research areas widely developed in applied computing, but it is highly limited when dealing with online learners. In this study, we evaluate the performances of 13 multi-label classification machine learning-based methods for automatic recognizing of online learners’ emotions, 12 of them are problem transformation methods and 1 is an adaptation algorithm method. The experiments are carried out using a dataset of online learners’ reviews sourced from Coursera and manually multi-labeled with the emotions: Enjoyment, Excitement, Satisfaction, Frustration, Boredom, and Confusion. Our best results in term of Hamming Loss and Micro-averaged F1 Score are obtained using Random Forest classifier and classifier chains approach, while the best Macro-averaged F1 Score was obtained using Decision Tree classifier and binary relevance approach.} }
@inproceedings{10.1145/3647444.3652479, title = {Machine Learning approach for Diabetes Prediction using Pima Dataset}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3652479}, url = {https://doi.org/10.1145/3647444.3652479}, author = {Moon, Pradnya Sumit and Bainalwar, Prachi A. and Borkar, Shubhangi M. and Shambharkar, Shubhangii S.}, abstract = {The rising cases of diabetes globally have called for effective prediction and early detection techniques. This study explores the use of machine learning methods to identify this condition in the PIMA diabetes dataset. The research is focused on analyzing the algorithms K-Nearest Neighbors, Logistic Regression, Decision Trees, Random Forest, and XGBoost. The objective of this study is to analyze the performance of the various algorithms used in predicting diabetes. The data collected from PIMA, which consists of diagnostic and clinical measurements, is the primary source of the study. All of the algorithms are then tested and trained on this dataset to improve their precision, F1-Score, recall, and accuracy. Moreover, cross-validation and hyperparameter tuning techniques are utilized to enhance the performance of the algorithms. The findings of this study provide valuable information on the algorithms' effectiveness when it comes to identifying diabetes. Among the tested algorithms, XGBoost performed well and consistently achieved high precision, F1-Score, recall, and accuracy. It has been concluded that this algorithm is the most suitable for identifying diabetes in the PIMA dataset. Different levels of performance were exhibited by the different algorithms. This provided a comprehensive analysis of their weaknesses and strengths in predicting diabetes. The findings of this study highlight the significance of utilizing machine learning methods in predicting diabetes. It also identified XGBoost as the best performer among the evaluated systems. The findings prove valuable in helping develop effective tools for early detection of diabetes, ultimately leading to better healthcare outcomes for those at risk.} }
@inproceedings{10.1145/3660395.3660455, title = {Research on 3D Animation Simulation Based on Machine Learning}, booktitle = {Proceedings of the 2023 3rd Guangdong-Hong Kong-Macao Greater Bay Area Artificial Intelligence and Big Data Forum}, pages = {352--356}, year = {2024}, isbn = {9798400716362}, doi = {10.1145/3660395.3660455}, url = {https://doi.org/10.1145/3660395.3660455}, author = {Huang, Xinran and Li, Panpan and Tan, Yupeng and Zhang, Yining and Deng, Mengyuan}, abstract = {The provided document is a research paper on 3D animation simulation based on machine learning, focusing on the application of machine learning in three-dimensional human animation technology. The paper discusses the importance of virtual reality technology based on computer three-dimensional animation in various fields such as film production, game development, sports simulation, and simulated training. The authors emphasize the significance of three-dimensional human animation technology in portraying subtle changes in human movement and simulating human emotions contained in movement. The paper also explores the application of machine learning methods in processing human body movement data and synthesizing new realistic movement sequences. Various machine learning theories and methods, including regression, function approximation, dimension reduction, classification, clustering, and decision-making, are discussed in relation to their application in three-dimensional human animation technology. The document also reviews the research status of human body movement synthesis technology at home and abroad, highlighting the contributions of various researchers in the field. The authors conclude that machine learning methods have unique advantages in processing human body movement data and can significantly improve the utilization rate of movement capture samples, thereby reducing the cost and time cycle of three-dimensional human animation production..} }
@inproceedings{10.1145/3592149.3592157, title = {O-RAN with Machine Learning in ns-3}, booktitle = {Proceedings of the 2023 Workshop on Ns-3}, pages = {60--68}, year = {2023}, isbn = {9798400707476}, doi = {10.1145/3592149.3592157}, url = {https://doi.org/10.1145/3592149.3592157}, author = {Garey, Wesley and Ropitault, Tanguy and Rouil, Richard and Black, Evan and Gao, Weichao}, keywords = {LTE, Machine Learning, Mobile Networks, Modeling and Simulation, O-RAN, ONNX, ns-3, location = Arlington, VA, USA}, abstract = {The Open Radio Access Network (O-RAN) Alliance is an industry-led standardization effort, with the main objective of evolving the Radio Access Network (RAN) to be open, intelligent, interoperable, and autonomous to support the ever growing need of improved performance and flexibility in mobile networks. This paper introduces an extension to Network Simulator 3 (ns-3) which mimics the behavior and components of the O-RAN Alliance’s O-RAN architecture. In this paper, we will describe the O-RAN architecture, our model in ns-3, and a Long Term Evolution (LTE) case study that utilizes Machine Learning (ML) and its integration with ns-3. At the end of this paper, the reader will have a general understanding of O-RAN and the capabilities of our fully simulated contribution so it can be leveraged to design and evaluate O-RAN-based solutions.} }
@inproceedings{10.1109/SCW63240.2024.00117, title = {Framework for Integrating Machine Learning Methods for Path-Aware Source Routing}, booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis}, pages = {829--838}, year = {2025}, isbn = {9798350355543}, doi = {10.1109/SCW63240.2024.00117}, url = {https://doi.org/10.1109/SCW63240.2024.00117}, author = {Al-Najjar, Anees and Paraiso, Domingos and Kiran, Mariam and Dominicini, Cristina and Borges, Everson and Guimaraes, Rafael and Martinello, Magnos and Newman, Harvey}, keywords = {congestion minimization, machine learning, network optimization, segment routing, traffic engineering, location = Atlanta, GA, USA}, abstract = {Since the advent of software-defined networking (SDN), Traffic Engineering (TE) has been highlighted as one of the key applications that can be achieved through software-controlled protocols (e.g. PCEP and MPLS). Being one of the most complex challenges in networking, TE problems involve difficult decisions such as allocating flows, either via splitting them among multiple paths or by using a reservation system, to minimize congestion. However, creating an optimized solution is cumbersome and difficult as traffic patterns vary and change with network scale, capacity, and demand. AI methods can help alleviate this by finding optimized TE solutions for the best network performance. SDN-based TE tools such as Teal, Hecate and more, use classification techniques or deep reinforcement learning to find optimal network TE solutions that are demonstrated in simulation. Routing control conducted via source routing tools, e.g., PolKA, can help dynamically divert network flows. In this paper, we propose a novel framework that leverages Hecate to practically demonstrate TE on a real network, collaborating with PolKA, a source routing tool. With real-time traffic statistics, Hecate uses this data to compute optimal paths that are then communicated to PolKA to allocate flows. Several contributions are made to show a practical implementation of how this framework is tested using an emulated ecosystem mimicking a real P4 testbed scenario. This work proves valuable for truly engineered self-driving networks helping translate theory to practice.} }
@article{10.1145/3605153, title = {Offloading Machine Learning to Programmable Data Planes: A Systematic Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3605153}, url = {https://doi.org/10.1145/3605153}, author = {Parizotto, Ricardo and Coelho, Bruno Loureiro and Nunes, Diego Cardoso and Haque, Israat and Schaeffer-Filho, Alberto}, keywords = {In-network computing, Programmable NICs, programmable switches, offloading, ML training, ML inference}, abstract = {The demand for machine learning (ML) has increased significantly in recent decades, enabling several applications, such as speech recognition, computer vision, and recommendation engines. As applications become more sophisticated, the models trained become more complex while also increasing the amount of data used for training. Several domain-specific techniques can be helpful to scale machine learning to large amounts of data and more complex models. Among the methods employed, of particular interest is offloading machine learning functionality to the network infrastructure, which is enabled by the use of emerging programmable data plane hardware, such as SmartNICs and programmable switches. As such, offloading machine learning to programmable network hardware has attracted considerable attention from the research community in the last few years. This survey presents a study of programmable data planes applied to machine learning, also highlighting how in-network computing is helping to speed up machine learning applications. In this article, we provide various concepts and propose a taxonomy to classify existing research. Next, we systematically review the literature that offloads machine learning functionality to programmable data plane devices, classifying it based on our proposed taxonomy. Finally, we discuss open challenges in the field and suggest directions for future research.} }
@proceedings{10.1145/3686490, title = {SPML '24: Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning}, year = {2024}, isbn = {9798400717192} }
@article{10.1145/3709742, title = {Sequoia: An Accessible and Extensible Framework for Privacy-Preserving Machine Learning over Distributed Data}, journal = {Proc. ACM Manag. Data}, volume = {3}, year = {2025}, doi = {10.1145/3709742}, url = {https://doi.org/10.1145/3709742}, author = {Xu, Kaiqiang and Chai, Di and Zhang, Junxue and Lai, Fan and Chen, Kai}, keywords = {compiler-executor architecture, distributed data, privacy-preserving machine learning (PPML), secure computation, training throughput}, abstract = {Privacy-preserving machine learning (PPML) algorithms use secure computation protocols to allow multiple data parties to collaboratively train machine learning (ML) models while maintaining their data confidentiality. However, current PPML frameworks couple secure protocols with ML models in PPML algorithm implementations, making it challenging for non-experts to develop and optimize PPML applications, limiting their accessibility and performance.We propose Sequoia, a novel PPML framework that decouples ML models and secure protocols to optimize the development and execution of PPML applications across data parties. Sequoia offers JAX-compatible APIs for users to program their ML models, while using a compiler-executor architecture to automatically apply PPML algorithms and system optimizations for model execution over distributed data. The compiler in Sequoia incorporates cross-party PPML processes into user-defined ML models by transparently adding computation, encryption, and communication steps with extensible policies, and the executor efficiently schedules code execution across multiple data parties, considering data dependencies and device heterogeneity.Compared to existing PPML frameworks, Sequoia requires 64\%-92\% fewer lines of code for users to implement the same PPML algorithms, and achieves 88\% speedup of training throughput in horizontal PPML.} }
@inproceedings{10.1145/3643786.3648022, title = {Data vs. Model Machine Learning Fairness Testing: An Empirical Study}, booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning}, pages = {1--8}, year = {2024}, isbn = {9798400705748}, doi = {10.1145/3643786.3648022}, url = {https://doi.org/10.1145/3643786.3648022}, author = {Shome, Arumoy and Cruz, Luis and Van Deursen, Arie}, keywords = {SE4ML, ML fairness testing, empirical software engineering, data-centric AI, location = Lisbon, Portugal}, abstract = {Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a "cheap" and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.} }
@article{10.1145/3744749, title = {Localization of Data Compromised by Hardware Attacks in Machine Learning Enabled Cyber-Physical Edge Devices}, journal = {ACM Trans. Cyber-Phys. Syst.}, volume = {9}, year = {2025}, issn = {2378-962X}, doi = {10.1145/3744749}, url = {https://doi.org/10.1145/3744749}, author = {Edara, Pravineeth and Banerjee, Sanmitra and Joardar, Biresh Kumar}, keywords = {Cyber-physical systems, Fault-injection attacks, Diagnosis, Convolutional Neural Networks, Rowhammer}, abstract = {Hardware attacks present a new and easy way for malicious actors to compromise model parameters in machine learning (ML) enabled cyber-physical systems (CPS). This can have severe consequences for many safety-critical cyber-physical applications such as power systems, self-driving cars, healthcare, security, and so on. Prior works have proposed several pre-emptive mitigation approaches for hardware attacks that can be adopted. However, adversarial attacks can bypass existing pre-emptive attack detection methods. Existing defense setups offer no further protection once the detection is bypassed. The attacker can then cause damage without getting noticed easily. In this work, we propose a new diagnosis method to search for compromised weights in real-time even when detection is bypassed considering fault-injection attacks. The proposed methodology provides an additional level of protection, which can rapidly identify and localize more than 99\% of affected weights in ML models, even when thousands of model parameters are affected simultaneously, with low power, performance, and area (PPA) overheads. In addition, we also propose a method to ensure that the CPS remains functional, even when undergoing attack diagnosis.} }
@inproceedings{10.1145/3746709.3746795, title = {Efficient Cross-Rack Data Updates Based on Intelligent Dynamic Data Layout Using Machine-learning}, booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications}, pages = {500--505}, year = {2025}, isbn = {9798400713163}, doi = {10.1145/3746709.3746795}, url = {https://doi.org/10.1145/3746709.3746795}, author = {Song, Zengyuan and Wei, Bing and Song, Yao and Wu, Yi}, keywords = {Big data storage, Dynamic stripe distribution, Erasure-coded data update, Inter-rack traffic, Machine-learning}, abstract = {In large-scale storage systems, erasure-coded data updates often cause performance issues due to high inter-rack traffic. This paper introduces CRUD, an efficient method to reduce inter-rack traffic. CRUD uses combinatorial mathematics to create a pairwise balanced design (PBD), distributing stripes across racks to exploit data update locality. It applies a machine-learning model to classify data blocks into repeated and non-repeated updates. CRUD considers update categories, strategies, block numbers, and rack involvement to optimize stripe distribution, dynamically adjusting it to further reduce traffic. Experiments show CRUD cuts inter-rack traffic by up to 42.31\% and saves 31.95\% in update time compared to the best current method.} }
@inproceedings{10.1145/3727993.3728015, title = {A Multi-Modal Machine Learning Framework for Wind Power Forecasting}, booktitle = {Proceedings of the 2024 4th International Conference on Computational Modeling, Simulation and Data Analysis}, pages = {129--134}, year = {2025}, isbn = {9798400711831}, doi = {10.1145/3727993.3728015}, url = {https://doi.org/10.1145/3727993.3728015}, author = {Yang, Zexiang and Yin, Han and Wen, Yongjia}, keywords = {Conv-BiLSTM, Feature dimensionality reduction, LightGBM SIM, New quality productive power}, abstract = {The report of the 2024 National People's Congress and the National People's Congress clearly pointed out that the primary task of high-quality development should be grasped, relating with the new quality productive forces. Meanwhile. wind power, as the new quality productive forces of renewable energy, has significantly played the vital role in the global power supply. So it's important for us to explore the ways and likelihoods about how to accurately predict wind power. Nevertheless, on the one hand,it's numerous features involved in wind power that possibly create the dimensions collapse and waste the unnecessary computational resource. On the other hand, Most of the features which involve with too manyt invalid information are easily to lead to the overfittting model. Thus, we made a analysis on a wind power dateset with 140,000 samples and 76 features. The steps are as follows:①In the first place, it's initial dimensionality reduction that we applied LightGBM to analyse the importance of wind power features. Based on the result, we remove the features with low importance.②Subsequently, it's secondary dimensionality reduction we combined MIC and VIF to construct the SIM matrix so as to measure the linear and nonlinear relationships of each feature.③Consequently, Based on the former dimensionality reduction results, we combined convolution and bidirectional long short-term memory network to build the Conv-BiLSTM model is utilized to forecast the wind power. It shows that the method can effectively achieve a high feature dimensionality reduction rate while achieving more efficient power prediction.} }
@inproceedings{10.1145/3639478.3643531, title = {Improving Fairness in Machine Learning Software via Counterfactual Fairness Thinking}, booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings}, pages = {420--421}, year = {2024}, isbn = {9798400705021}, doi = {10.1145/3639478.3643531}, url = {https://doi.org/10.1145/3639478.3643531}, author = {Yin, Zhipeng and Wang, Zichong and Zhang, Wenbin}, abstract = {Machine Learning (ML) software is increasingly influencing decisions that impact individuals' lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social subgroups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works enhance software fairness, a non-functional software property, at the cost of software performance. To this end, we proposed a novel framework, namely Group Equality Counterfactual Fairness (GECF), which aims to mitigate sensitive attribute bias and labeling bias using counterfactual fairness while reducing the resulting performance loss based on ensemble learning. Experimental results on 6 real-world datasets show the superiority of our proposed framework from different aspects.} }
@article{10.1145/3652918, title = {Unveiling Patterns of the Earth through Machine Learning and Geospatial Analysis}, journal = {XRDS}, volume = {30}, pages = {32--33}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3652918}, url = {https://doi.org/10.1145/3652918}, author = {Li, Jiayi and Klemmer, Konstantin}, abstract = {Konstantin Klemmer is a researcher at Microsoft Research New England, where he works on the representation of geospatial phenomena in machine learning methods.} }
@inproceedings{10.1145/3603287.3651186, title = {Benchmarking Machine Learning Techniques for Bankruptcy Prediction under Benign and Adversarial Behaviors}, booktitle = {Proceedings of the 2024 ACM Southeast Conference}, pages = {259--265}, year = {2024}, isbn = {9798400702372}, doi = {10.1145/3603287.3651186}, url = {https://doi.org/10.1145/3603287.3651186}, author = {Yin, Xing and Le, Thai}, keywords = {10K Financial Report, Adversarial Machine Learning, Data Balancing Techniques, Datasets, Machine Learning, Neural Networks, location = Marietta, GA, USA}, abstract = {This research uses machine learning methods to perform company bankruptcy prediction. 10K Financial reports are audited by public auditing firms and submitted by companies to the Security Exchange Commission (SEC). 10K Financial reports are the appropriate and reasonable financial statements including financial variables (numbers) and texts to evaluate a company's financial status yearly. However, people who don't have a business background would find it difficult to read and understand the 10K Financial reports. Therefore, it is useful to have machine learning methods to analyze the financial status or predict bankruptcy on 10K Financial reports datasets. This research uses different time frames 10K Financial reports datasets and compares several machine learning models to classify and predict bankruptcy companies. Based on different dataset's experimental results, this research concluded that the Random Forest model or XGBoost model generates a better performance. The outcomes of comparing adversarial model performance showed that MLP (Multi-layer Perceptron) + FGSM (Fast Gradient Sign Method) generate better performance, dropping the prediction performance in F1 score from 81.63\% to 79.01\%.} }
@inproceedings{10.1145/3615366.3615375, title = {Evaluation of Machine Learning for Intrusion Detection in Microservice Applications}, booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing}, pages = {126--135}, year = {2023}, isbn = {9798400708442}, doi = {10.1145/3615366.3615375}, url = {https://doi.org/10.1145/3615366.3615375}, author = {Araujo, Iury and Antunes, Nuno and Vieira, Marco}, keywords = {Intrusion Detection, Machine Learning, Microservices, System Calls, location = La Paz, Bolivia}, abstract = {Microservices have thrived recently as an approach for service design, development, and delivery. It provides several benefits to the systems as an architecture, such as faster delivery, improved scalability, and greater autonomy. Although microservice architectures are popular, security characteristics of these architectures impair the deployment of security, such as sizable attack surface, network complexity, heterogeneity, and others. For years, intrusion detection has been a practical security approach for many applications. Recently, machine learning provided improved functionality for intrusion detection systems with exciting results in overall tests. This paper presents the evaluation of machine learning techniques for intrusion detection in a microservice scenario. System call data was collected from containers simulating microservice applications; these containers were submitted to attacks that exploited different vulnerabilities. The data was used to train and test machine learning techniques, and the test results provided us with exciting possibilities for this approach. Some of the tested attacks were very well detected by the techniques, while some were not, attesting that machine-learning-based intrusion detection is usable in this environment. However, to enhance detection, it is required to improve data processing and representation for this type of scenario.} }
@inproceedings{10.1145/3706890.3706901, title = {Intelligent Diagnosis and Progression Analysis of Alzheimer's Disease Using Machine Learning}, booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science}, pages = {66--72}, year = {2025}, isbn = {9798400717826}, doi = {10.1145/3706890.3706901}, url = {https://doi.org/10.1145/3706890.3706901}, author = {Yu, Kexin and Luo, Xin}, keywords = {Alzheimer's disease, K-means++ clustering, fixed effect model, logistic regression}, abstract = {AD is a progressive neurodegenerative disease with insidious onset. The disease is usually progressive in the elderly, with a gradual loss of independent living skills and death from complications 10 to 20 years after the onset of the disease. However, due to the limited cognition of patients and their families for such diseases, most patients are often diagnosed to the middle and late stages of the disease, missing the best early intervention time. Therefore, timely and accurate identification of AD and its early stages is critical for improving treatment outcomes. In this paper, we first employs four machine learning algorithms: logistic regression, decision trees, random forests, and SVM to build different intelligent diagnostic models. The results show that the logistic regression model has the best performance, with an F1 score of 0.96. Our study also used the method of K-Means ++ clustering and hierarchical clustering to further subdivide MCI into three categories: SMC, EMCI and LMCI. Finally, we use fixed effects models and the least squares method to reveal the temporal progression patterns of various disease categories.} }
@inproceedings{10.1145/3477495.3531678, title = {Table Enrichment System for Machine Learning}, booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {3267--3271}, year = {2022}, isbn = {9781450387323}, doi = {10.1145/3477495.3531678}, url = {https://doi.org/10.1145/3477495.3531678}, author = {Dong, Yuyang and Oyamada, Masafumi}, keywords = {machine learning, table augmentation, table discovery, table enrichment, location = Madrid, Spain}, abstract = {Data scientists are constantly facing the problem of how to improve prediction accuracy with insufficient tabular data. We propose a table enrichment system that enriches a query table by adding external attributes (columns) from data lakes and improves the accuracy of machine learning predictive models. Our system has four stages, join row search, task-related table selection, row and column alignment, and feature selection and evaluation, to efficiently create an enriched table for a given query table and a specified machine learning task. We demonstrate our system with a web UI to show the use cases of table enrichment.} }
@inproceedings{10.1145/3632410.3633290, title = {Unveiling Graph Structures for Machine Learning: Learning, Structuring, and Coarsening}, booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)}, pages = {484--488}, year = {2024}, isbn = {9798400716348}, doi = {10.1145/3632410.3633290}, url = {https://doi.org/10.1145/3632410.3633290}, author = {Kumar, Manoj and Kumar, Sandeep}, keywords = {Graph learning, graph coarsening, node and classification, structured graph learning, location = Bangalore, India}, abstract = {Graph structure is an important element in the realm of machine learning tasks. This tutorial centers on the art of deriving graph representations from data. It unfolds through three pivotal themes: Each theme is dissected thoroughly, combining theory with practical application. The tutorial demonstrates how these graph representations drive various applications, including clustering, node and graph classification, and edge prediction. In essence, this tutorial arms participants with the tools to unleash the potential of graph structures in the realm of machine learning.} }
@inproceedings{10.1145/3522664.3528620, title = {Code smells for machine learning applications}, booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI}, pages = {217--228}, year = {2022}, isbn = {9781450392754}, doi = {10.1145/3522664.3528620}, url = {https://doi.org/10.1145/3522664.3528620}, author = {Zhang, Haiyin and Cruz, Lu\'s and van Deursen, Arie}, keywords = {anti-pattern, code quality, code smell, machine learning, technical debt, location = Pittsburgh, Pennsylvania}, abstract = {The popularity of machine learning has wildly expanded in recent years. Machine learning techniques have been heatedly studied in academia and applied in the industry to create business value. However, there is a lack of guidelines for code quality in machine learning applications. In particular, code smells have rarely been studied in this domain. Although machine learning code is usually integrated as a small part of an overarching system, it usually plays an important role in its core functionality. Hence ensuring code quality is quintessential to avoid issues in the long run. This paper proposes and identifies a list of 22 machine learning-specific code smells collected from various sources, including papers, grey literature, GitHub commits, and Stack Overflow posts. We pinpoint each smell with a description of its context, potential issues in the long run, and proposed solutions. In addition, we link them to their respective pipeline stage and the evidence from both academic and grey literature. The code smell catalog helps data scientists and developers produce and maintain high-quality machine learning application code.} }
@inproceedings{10.1145/3603719.3603726, title = {Accelerating Machine Learning Queries with Linear Algebra Query Processing}, booktitle = {Proceedings of the 35th International Conference on Scientific and Statistical Database Management}, year = {2023}, isbn = {9798400707469}, doi = {10.1145/3603719.3603726}, url = {https://doi.org/10.1145/3603719.3603726}, author = {Sun, Wenbo and Katsifodimos, Asterios and Hai, Rihan}, keywords = {database, machine learning, operator fusion, query optimization, location = Los Angeles, CA, USA}, abstract = {The rapid growth of large-scale machine learning (ML) models has led numerous commercial companies to utilize ML models for generating predictive results to help business decision-making. As two primary components in traditional predictive pipelines, data processing, and model predictions often operate in separate execution environments, leading to redundant engineering and computations. Additionally, the diverging mathematical foundations of data processing and machine learning hinder cross-optimizations by combining these two components, thereby overlooking potential opportunities to expedite predictive pipelines. In this paper, we propose an operator fusing method based on GPU-accelerated linear algebraic evaluation of relational queries. Our method leverages linear algebra computation properties to merge operators in machine learning predictions and data processing, significantly accelerating predictive pipelines by up to 317x. We perform a complexity analysis to deliver quantitative insights into the advantages of operator fusion, considering various data and model dimensions. Furthermore, we extensively evaluate matrix multiplication query processing utilizing the widely-used Star Schema Benchmark. Through comprehensive evaluations, we demonstrate the effectiveness and potential of our approach in improving the efficiency of data processing and machine learning workloads on modern hardware.} }
@inproceedings{10.1145/3669754.3669781, title = {Leukaemia Detection and Classification of its Sub-types using Machine Learning}, booktitle = {Proceedings of the 2024 10th International Conference on Computing and Artificial Intelligence}, pages = {180--183}, year = {2024}, isbn = {9798400717055}, doi = {10.1145/3669754.3669781}, url = {https://doi.org/10.1145/3669754.3669781}, author = {V, Harsha and P R, Chandan and C, Niharika and Siddesh Loni, Ankit and U, Purushotham}, keywords = {Additional Key Words and Phrases: Machine Learning, Leukaemia Diagnosis, Vision Transformers, location = Bali Island, Indonesia}, abstract = {Leukemia, a group of blood cancers originating from abnormal white blood cells as show in Fig 1, poses a significant threat to global health. Early detection and accurate classification of leukemia subtypes are crucial for effective treatment planning and patient management. This research focuses on developing a comprehensive approach for leukemia detection and subtype classification using advanced machine learning techniques. The objectives of this research are to efficiently detect leukemia and classify its subtypes using machine learning techniques. The successful implementation of this research project has the potential to significantly improve the efficiency of leukemia diagnosis by providing a reliable, automated system for early detection and accurate classification of leukemia subtypes. This, in turn, can contribute to more timely and targeted treatment strategies, ultimately improving patient outcomes in the battle against leukemia.} }
@inproceedings{10.1145/3686625.3686630, title = {An Effectual Image based Authentication Scheme for Mobile Device using Machine Learning}, booktitle = {Proceedings of the 2024 6th International Electronics Communication Conference}, pages = {24--30}, year = {2024}, isbn = {9798400717598}, doi = {10.1145/3686625.3686630}, url = {https://doi.org/10.1145/3686625.3686630}, author = {Kumar, Kota Lokesh and Ray, Sangram and Das, Priyanka}, keywords = {Image, Machine Learning, Mobile Authentication, Privacy, Security, location = Fukuoka, Japan}, abstract = {In today's digital world, the integration of mobile device authentication has revolutionized the way we access services and applications. But, due to the growing amount of sensitive data that is stored in mobile devices, a strong authentication system is essential to protect user privacy and prevent unauthorized access. To address this aspect the incorporation of password, biometric, etc., to authentication scheme has increased owing to its significant features such as - high security, fast verification, etc. However, there are several weaknesses of using these mechanisms as it cannot be reset or reissued effortlessly. Therefore, to overcome these disadvantages a novel effectual Image based authentication scheme for mobile device using Machine Learning is proposed. Further, the performance efficiency of the proposed scheme is evaluated in terms of processing time. Thus, our scheme is comparable efficient than other existing password and/or biometric based schemes.} }
@inproceedings{10.1145/3691620.3695258, title = {MLOLET - Machine Learning Optimized Load and Endurance Testing: An industrial experience report}, booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering}, pages = {1956--1966}, year = {2024}, isbn = {9798400712487}, doi = {10.1145/3691620.3695258}, url = {https://doi.org/10.1145/3691620.3695258}, author = {Vitui, Arthur and Chen, Tse-Hsun}, abstract = {Load testing is essential for ensuring the performance and stability of modern large-scale systems, which must handle vast numbers of concurrent requests. Traditional load tests, often requiring extensive execution times, are costly and impractical within the short release cycles typical of contemporary software development. In this paper, we present our experience deploying MLOLET, a machine learning optimized load testing framework, at Ericsson. MLOLET addresses key challenges in load testing by determining early stop points for tests and forecasting throughput and response time trends in production environments. By training a time-series model on key performance indicators (KPIs) collected from load tests, MLOLET enables early detection of abnormal system behavior and provides accurate performance forecasting. This capability allows load test engineers to make informed decisions on resource allocation, enhancing both testing efficiency and system reliability. We document the design of MLOLET, its application in industrial settings, and the feedback received from its implementation, highlighting its impact on improving load testing processes and operational performance.} }
@inproceedings{10.1145/3663529.3663831, title = {Costs and Benefits of Machine Learning Software Defect Prediction: Industrial Case Study}, booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering}, pages = {92--103}, year = {2024}, isbn = {9798400706585}, doi = {10.1145/3663529.3663831}, url = {https://doi.org/10.1145/3663529.3663831}, author = {Stradowski, Szymon and Madeyski, Lech}, keywords = {case study, cost-benefit analysis, industry, machine learning, software defect prediction, software testing, location = Porto de Galinhas, Brazil}, abstract = {Context: Our research is set in the industrial context of Nokia 5G and the introduction of Machine Learning Software Defect Prediction (ML SDP) to the existing quality assurance process within the company. Objective: We aim to support or undermine the profitability of the proposed ML SDP solution designed to complement the system-level black-box testing at Nokia, as cost-effectiveness is the main success criterion for further feasibility studies leading to a potential commercial introduction. Method: To evaluate the expected cost-effectiveness, we utilize one of the available cost models for software defect prediction formulated by previous studies on the subject. Second, we calculate the standard Return on Investment (ROI) and Benefit-Cost Ratio (BCR) financial ratios to demonstrate the profitability of the developed approach based on real-world, business-driven examples. Third, we build an MS Excel-based tool to automate the evaluation of similar scenarios that other researchers and practitioners can use. Results: We considered different periods of operation and varying efficiency of predictions, depending on which of the two proposed scenarios were selected (lightweight or advanced). Performed ROI and BCR calculations have shown that the implemented ML SDP can have a positive monetary impact and be cost-effective in both scenarios. Conclusions: The cost of adopting new technology is rarely analyzed and discussed in the existing scientific literature, while it is vital for many software companies worldwide. Accordingly, we bridge emerging technology (machine learning software defect prediction) with a software engineering domain (5G system-level testing) and business considerations (cost efficiency) in an industrial environment of one of the leaders in 5G wireless technology.} }
@proceedings{10.1145/3661725, title = {CMLDS '24: Proceedings of the International Conference on Computing, Machine Learning and Data Science}, year = {2024}, isbn = {9798400716393} }
@inproceedings{10.1145/3613904.3642855, title = {Machine Learning Processes As Sources of Ambiguity: Insights from AI Art}, booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}, year = {2024}, isbn = {9798400703300}, doi = {10.1145/3613904.3642855}, url = {https://doi.org/10.1145/3613904.3642855}, author = {Sivertsen, Christian and Salimbeni, Guido and Lvlie, Anders Sundnes and Benford, Steven David and Zhu, Jichen}, keywords = {ambiguity, art, artificial intelligence, computer vision, generative art, machine learning, location = Honolulu, HI, USA}, abstract = {Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success. This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work. Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis. Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes. Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details. Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one.} }
@article{10.1145/3699711, title = {A Systematic Literature Review on Automated Software Vulnerability Detection Using Machine Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3699711}, url = {https://doi.org/10.1145/3699711}, author = {Shiri Harzevili, Nima and Boaye Belle, Alvine and Wang, Junjie and Wang, Song and Jiang, Zhen Ming (Jack) and Nagappan, Nachiappan}, keywords = {Source code, software security, software vulnerability detection, software bug detection, machine learning, deep learning}, abstract = {In recent years, numerous Machine Learning (ML) models, including Deep Learning (DL) and classic ML models, have been developed to detect software vulnerabilities. However, there is a notable lack of comprehensive and systematic surveys that summarize, classify, and analyze the applications of these ML models in software vulnerability detection. This absence may lead to critical research areas being overlooked or under-represented, resulting in a skewed understanding of the current state of the art in software vulnerability detection. To close this gap, we propose a comprehensive and systematic literature review that characterizes the different properties of ML-based software vulnerability detection systems using six major Research Questions (RQs).Using a custom web scraper, our systematic approach involves extracting a set of studies from four widely used online digital libraries: ACM Digital Library, IEEE Xplore, ScienceDirect, and Google Scholar. We manually analyzed the extracted studies to filter out irrelevant work unrelated to software vulnerability detection, followed by creating taxonomies and addressing RQs. Our analysis indicates a significant upward trend in applying ML techniques for software vulnerability detection over the past few years, with many studies published in recent years. Prominent conference venues include the International Conference on Software Engineering (ICSE), the International Symposium on Software Reliability Engineering (ISSRE), the Mining Software Repositories (MSR) conference, and the ACM International Conference on the Foundations of Software Engineering (FSE), whereas Information and Software Technology (IST), Computers \&amp; Security (C\&amp;S), and Journal of Systems and Software (JSS) are the leading journal venues.Our results reveal that 39.1\% of the subject studies use hybrid sources, whereas 37.6\% of the subject studies utilize benchmark data for software vulnerability detection. Code-based data are the most commonly used data type among subject studies, with source code being the predominant subtype. Graph-based and token-based input representations are the most popular techniques, accounting for 57.2\% and 24.6\% of the subject studies, respectively. Among the input embedding techniques, graph embedding and token vector embedding are the most frequently used techniques, accounting for 32.6\% and 29.7\% of the subject studies. Additionally, 88.4\% of the subject studies use DL models, with recurrent neural networks and graph neural networks being the most popular subcategories, whereas only 7.2\% use classic ML models. Among the vulnerability types covered by the subject studies, CWE-119, CWE-20, and CWE-190 are the most frequent ones. In terms of tools used for software vulnerability detection, Keras with TensorFlow backend and PyTorch libraries are the most frequently used model-building tools, accounting for 42 studies for each. In addition, Joern is the most popular tool used for code representation, accounting for 24 studies.Finally, we summarize the challenges and future directions in the context of software vulnerability detection, providing valuable insights for researchers and practitioners in the field.} }
@inproceedings{10.1145/3580305.3599223, title = {KDD Workshop on Machine Learning in Finance}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5863--5864}, year = {2023}, isbn = {9798400701030}, doi = {10.1145/3580305.3599223}, url = {https://doi.org/10.1145/3580305.3599223}, author = {Akoglu, Leman and Chawla, Nitesh and Kumar, Senthil and Nagrecha, Saurabh and Das, Mahashweta and Naware, Vidyut M. and Faruquie, Tanveer}, keywords = {finance, graph mining, machine learning, nlp, location = Long Beach, CA, USA}, abstract = {The finance industry is constantly faced with an ever evolving set of challenges including credit card fraud, identity theft, network intrusion, money laundering, human trafficking, and illegal sales of firearms. There is also the newly emerging threat of fake news in financial media that can lead to distortions in trading strategies and investment decisions. In addition, traditional problems such as customer analytics, forecasting, and recommendations take on a unique flavor when applied to financial data. A number of new ideas are emerging to tackle all these problems including self-supervised learning methods, deep learning algorithms, network/graph based solutions as well as linguistic approaches. These methods must often be able to work in real-time and be able handle large volumes of data. The purpose of this workshop is to bring together researchers and practitioners to discuss both the problems faced by the financial industry and potential solutions. We plan to invite regular papers, positional papers and extended abstracts of work in progress. We will also encourage short papers from financial industry practitioners that introduce domain specific problems and challenges to academic researchers.} }
@inproceedings{10.1145/3629296.3629368, title = {A Systematic Analysis of Machine Learning Studies in Education}, booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers}, pages = {451--455}, year = {2024}, isbn = {9798400709111}, doi = {10.1145/3629296.3629368}, url = {https://doi.org/10.1145/3629296.3629368}, author = {Pektas, Sule Tasl}, keywords = {Bibliometric Analysis, Education, Keyword Co-occurrence Network, Machine Learning, location = Barcelona, Spain}, abstract = {Machine learning has been transforming education and changing learning, teaching, and administration processes. However, studies analyzing the existing body of work and emerging research foci are lacking. To fill in the re-search gap, this paper presents a bibliometric analysis of articles on machine learning in education that were indexed by Web of Science Core Citation In-dices from 1979 to 2023. The study investigates publication patterns (articles per year and journals) and key research areas. A keyword co-occurrence analysis was conducted to identify the clusters of keywords which often co-exist in articles. The analysis revealed six clusters which correspond to the main research themes: profiling and prediction, assessment, intelligent tutoring systems, MOOCs, natural language processing, and prediction in distance learning. It is discussed that the newly emerging and rapidly developing research area focuses merely on applications of the technology, while ethical, pedagogical, socio-cultural, and administrative is-sues regarding machine learning in education need further attention.} }
@inproceedings{10.1145/3583740.3626617, title = {SODA: Protecting Proprietary Information in On-Device Machine Learning Models}, booktitle = {Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing}, pages = {121--132}, year = {2024}, isbn = {9798400701238}, doi = {10.1145/3583740.3626617}, url = {https://doi.org/10.1145/3583740.3626617}, author = {Atrey, Akanksha and Sinha, Ritwik and Mitra, Saayan and Shenoy, Prashant}, keywords = {on-device, machine learning, proprietary information, privacy, location = Wilmington, DE, USA}, abstract = {The growth of low-end hardware has led to a proliferation of machine learning-based services in edge applications. These applications gather contextual information about users and provide some services, such as personalized offers, through a machine learning (ML) model. A growing practice has been to deploy such ML models on the user's device to reduce latency, maintain user privacy, and minimize continuous reliance on a centralized source. However, deploying ML models on the user's edge device can leak proprietary information about the service provider. In this work, we investigate on-device ML models that are used to provide mobile services and demonstrate how simple attacks can leak proprietary information of the service provider. We show that different adversaries can easily exploit such models to maximize their profit and accomplish content theft. Motivated by the need to thwart such attacks, we present an end-to-end framework, SODA, for deploying and serving on edge devices while defending against adversarial usage. Our results demonstrate that SODA can detect adversarial usage with 89\% accuracy in less than 50 queries with minimal impact on service performance, latency, and storage.} }
@article{10.1145/3652579, title = {Machine Learning-Based Kernel Selector for SpMV Optimization in Graph Analysis}, journal = {ACM Trans. Parallel Comput.}, volume = {11}, year = {2024}, issn = {2329-4949}, doi = {10.1145/3652579}, url = {https://doi.org/10.1145/3652579}, author = {Xiao, Guoqing and Zhou, Tao and Chen, Yuedan and Hu, Yikun and Li, Kenli}, keywords = {Adaptive framework, GPU computing, graph analysis, machine learning, SpMV}, abstract = {Sparse Matrix and Vector multiplication (SpMV) is one of the core algorithms in various large-scale scientific computing and real-world applications. With the rapid development of AI and big data, the input vector in SpMV becomes sparse in many application fields. Especially in some graph analysis calculations, the sparsity of the input vector will change with the running of the program, and the non-zero element distribution of the adjacency matrix of some graph data has the power law property, leading to serious load imbalance, which requires additional optimization means. Therefore, the optimal SpMV kernel may be different, and a single SpMV kernel can no longer meet the acceleration requirements. In this article, we propose a decision tree-based adaptive SpMV framework, named DTSpMV, that can automatically select appropriate SpMV kernels according to different input data in iterations of graph computation. Based on the analysis of computing patterns, bit-array compression algorithms, and serial and parallel algorithms, we encapsulate nine SpMV kernels within the framework. We explore machine learning-based kernel selectors in terms of both accuracy and runtime overhead. Experimental results on NVIDIA Tesla T4 GPU show that our adaptive framework achieves the arithmetic average performance improvement of 152 compared to the SpMV kernel in cuSPARSE.} }
@inproceedings{10.1145/3643915.3644085, title = {Explanation-driven Self-adaptation using Model-agnostic Interpretable Machine Learning}, booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems}, pages = {189--199}, year = {2024}, isbn = {9798400705854}, doi = {10.1145/3643915.3644085}, url = {https://doi.org/10.1145/3643915.3644085}, author = {Negri, Francesco Renato and Nicolosi, Niccolo and Camilli, Matteo and Mirandola, Raffaela}, keywords = {explainable self-adaptation, model-agnostic explanations, interpretable machine learning, location = Lisbon, AA, Portugal}, abstract = {Self-adaptive systems increasingly rely on black-box predictive models (e.g., Neural Networks) to make decisions and steer adaptations. The lack of transparency of these models makes it hard to explain adaptation decisions and their possible effects on the surrounding environment. Furthermore, adaptation decisions in this context are typically the outcome of expensive optimization processes. The complexity arises from the inability to directly observe or comprehend the internal mechanisms of the black-box predictive models, which requires employing iterative methods to explore a possibly large search space and optimize according to many goals. Here, balancing the trade-off between effectiveness and cost becomes a crucial challenge. In this paper, we propose explanation-driven self-adaptation, a novel approach that embeds model-agnostic interpretable machine learning techniques into the feedback loop to enhance the transparency of the predictive models and gain insights that help drive adaptation decisions effectively by significantly reducing the cost of planning them. Our empirical evaluation demonstrates the cost-effectiveness of our approach using two evaluation subjects in the robotics domain.} }
@inproceedings{10.1145/3511808.3557501, title = {Fairness of Machine Learning in Search Engines}, booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management}, pages = {5132--5135}, year = {2022}, isbn = {9781450392365}, doi = {10.1145/3511808.3557501}, url = {https://doi.org/10.1145/3511808.3557501}, author = {Fang, Yi and Liu, Hongfu and Tao, Zhiqiang and Yurochkin, Mikhail}, keywords = {fairness, machine learning, search engines, location = Atlanta, GA, USA}, abstract = {Fairness has gained increasing importance in a variety of AI and machine learning contexts. As one of the most ubiquitous applications of machine learning, search engines mediate much of the information experiences of members of society. Consequently, understanding and mitigating potential algorithmic unfairness in search have become crucial for both users and systems. In this tutorial, we will introduce the fundamentals of fairness in machine learning, for both supervised learning such as classification and ranking, and unsupervised learning such as clustering. We will then present the existing work on fairness in search engines, including the fairness definitions, evaluation metrics, and taxonomies of methodologies. This tutorial will help orient information retrieval researchers to algorithmic fairness, provide an introduction to the growing literature on this topic, and gathering researchers and practitioners interested in this research direction.} }
@inproceedings{10.1145/3675417.3675455, title = {Revenue Prediction Research of Vegetable Superstores Based on Machine Learning}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {235--239}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675455}, url = {https://doi.org/10.1145/3675417.3675455}, author = {Zhang, Yuqing and Wang, Chunping}, abstract = {Due to the instability of vegetable commodity pricing and replenishment decisions, it‘s difficult to accurately predict their superstore revenues. In this paper, a machine learning-based revenue prediction method is proposed for vegetable superstores. Firstly, based on the historical unit price and wholesale price data, an XGBoost sales volume prediction model is established to obtain various types of sales volume in the future. Secondly, Multilayer Perceptron Model is applied to gain a time series prediction of the wholesale price. Finally, according to the constraints, a particle swarm algorithm optimized objective planning model is presented to optimize the key parameters and achieve the maximum superstore revenue. The simulation results show that the proposed machine learning prediction method can get better prediction results of superstore revenue.} }
@inproceedings{10.1145/3717664.3717687, title = {Exploring the Impact of Digital Transformation on Bank Credit Risk through Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Economic Data Analytics and Artificial Intelligence}, pages = {132--136}, year = {2025}, isbn = {9798400713255}, doi = {10.1145/3717664.3717687}, url = {https://doi.org/10.1145/3717664.3717687}, author = {Lu, Xunuo and Tu, Binghao and Yu, Zengyi}, keywords = {Bank Digital Transformation, Credit Risk, Decision Tree, Machine Learning}, abstract = {In the current economic environment, digital transformation has become a new trend of innovation and development of commercial banks, while its impact on bank credit risk cannot be ignored. This paper takes 59043 bank data points in 2011-2021 as the research sample, preprocesses the data based on the K-means clustering method with pruning and 5-fold cross-validation which is used resulting in 96.2\% model accuracy, and finally examines the impact of banks' digital transformation on credit risk using decision tree analysis, . The study finds that (1) digital transformation has a more significant negative effect on bank credit risk. (2) In the decomposition of the degree of digital transformation into three indicators, namely, "strategic digitalization", "business digitalization" and "management digitalization", business digitalization has the most significant impact on it. (3) There is a heterogeneous effect of banks' digital transformation on credit risk, and the dampening effect on credit risk is more pronounced for urban commercial banks, local state-owned banks and listed banks than for rural commercial banks, non-local state-owned banks and non-listed banks. This paper not only provides the basis for related research, but also provides the basis for the study of the machine tool. This paper not only provides machine learning research methods for related research, but also has important significance for the government to guide the development of fintech, promote the digital transformation of banks, and improve the national financial risk management ability.} }
@article{10.1145/3702646, title = {Exploring Dataset Bias and Scaling Techniques in Multi-Source Gait Biomechanics: An Explainable Machine Learning Approach}, journal = {ACM Trans. Intell. Syst. Technol.}, volume = {16}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3702646}, url = {https://doi.org/10.1145/3702646}, author = {Fleischmann, Sophie and Dietz, Simon and Shanbhag, Julian and Wuensch, Annika and Nitschke, Marlies and Miehling, J\"org and Wartzack, Sandro and Leyendecker, Sigrid and Eskofier, Bjoern M. and Koelewijn, Anne D.}, keywords = {datasets, dataset combination, neural networks, explainable AI, scaling, biomechanics, motion capture, machine learning, LRP}, abstract = {Machine learning has become increasingly important in biomechanics. It allows to unveil hidden patterns from large and complex data, which leads to a more comprehensive understanding of biomechanical processes and deeper insights into human movement. However, machine learning models are often trained on a single dataset with a limited number of participants, which negatively affects their robustness and generalizability. Combining data from multiple existing sources provides an opportunity to overcome these limitations without spending more time on recruiting participants and recording new data. It is furthermore an opportunity for researchers who lack the financial requirements or laboratory equipment to conduct expensive motion capture studies themselves. At the same time, subtle interlaboratory differences can be problematic in an analysis due to the bias that they introduce. In our study, we investigated differences in motion capture datasets in the context of machine learning, for which we combined overground walking trials from four existing studies. Specifically, our goal was to examine whether a machine learning model was able to predict the original data source based on marker and GRF trajectories of single strides and how different scaling methods and pooling procedures affected the outcome. Layer-wise relevance propagation was applied to understand which factors were influential to distinguish the original data sources. We found that the model could predict the original data source with a very high accuracy (up to (gt) 99\%), which decreased by about 15 percentage points when we scaled every dataset individually prior to pooling. However, none of the proposed scaling methods could fully remove the dataset bias. Layer-wise relevance propagation revealed that there was not only one single factor that differed between all datasets. Instead, every dataset had its unique characteristics that were picked up by the model. These variables differed between the scaling and pooling approaches but were mostly consistent between trials belonging to the same dataset. Our results show that motion capture data is sensitive even to small deviations in marker placement and experimental setup and that small inter-group differences should not be overinterpreted during data analysis, especially when the data was collected in different labs. Furthermore, we recommend scaling datasets individually prior to pooling them which led to the lowest accuracy. We want to raise awareness that differences in datasets always exist and are recognizable by machine learning models. Researchers should thus think about how these differences might affect their results when combining data from different studies.} }
@inproceedings{10.1145/3747227.3747270, title = {A Machine Learning-Based Design Case Study on the Influencing Factors of Classroom Silence Among Chinese University Students: The “Reconciliation Classroom” Conceptual Healing Space}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {268--275}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747270}, url = {https://doi.org/10.1145/3747227.3747270}, author = {Liu, Qiman and Xie, Wanying and Guo, Nuoya and Shen, Wenxin and Yang, Yan}, keywords = {Chinese university students, Classroom Silence, Healing Space, Machine Learning, Quantitative Research}, abstract = {Classroom interaction among students is an externalization and expression of knowledge, playing a crucial role in facilitating learning. However, university students in China tend to remain silent in class, rarely initiating or responding to questions. This study investigates the phenomenon of classroom silence among university students by employing quantitative statistical methods to explore the influence of classroom-related factors on students’ level of classroom engagement. A total of 449 questionnaires were distributed and collected, and after verifying the reliability and validity of the questionnaire, data cleaning, and the exclusion of outliers, 357 valid samples were obtained for multiple regression and machine learning analyses. Results show that the multiple regression model performs better in predicting the independent variables and the random forest (RF) model outperforms in predicting the most influencing factor on the classroom interactive activity level. Performance accuracy achieved by the multiple regression model is 100\% compared to the RF model with 80\% accuracy. The findings indicate that (1) students’ personality traits exhibit a significant positive correlation with students' classroom engagement; (2) students' abilities have a notable positive impact on their participation in classroom interaction; and (3) teachers’ teaching styles and response benefits demonstrate a weak positive correlation with classroom engagement. Based on the implementation of machine learning models, a new idea emerges to tackle students’ silence in classrooms by proposing and implementing a predictive modeling approach to identify students at risk in learning in universities and colleges.} }
@inbook{10.1145/3746027.3756875, title = {PySimPace v2.0: An Easy-to-Use Simulation Tool with Machine Learning Pipelines for Realistic MRI Motion Artifact Generation}, booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia}, pages = {13656--13659}, year = {2025}, isbn = {9798400720352}, url = {https://doi.org/10.1145/3746027.3756875}, author = {Kumar, Snehil and Vaughan, Neil and Fu, Zeyu and Wilson, Heather}, abstract = {Motion artifacts in structural and functional magnetic resonance imaging (MRI) pose a significant challenge for both clinical use and machine learning (ML)-based image analysis. Existing ML approaches for artifact correction require paired clean and corrupted datasets, which are difficult to acquire. We present py-simpace, an open-source, pip-installable MRI motion artifact simulation toolkit with native ML integration. py-simpace supports structural MRI and functional MRI (fMRI) simulation, offering configurable k-space and image-space motion, ghosting, Gibbs ringing, and physiological noise. It provides an end-to-end pipeline with a ready-to-use PyTorch Dataset interface for ML training. We describe the design of py-simpace v2.0, compare it with existing tools, and demonstrate its utility for robust artifact correction model development.} }
@inproceedings{10.1145/3640310.3674087, title = {Enhancing Automata Learning with Statistical Machine Learning: A Network Security Case Study}, booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems}, pages = {172--182}, year = {2024}, isbn = {9798400705045}, doi = {10.1145/3640310.3674087}, url = {https://doi.org/10.1145/3640310.3674087}, author = {Ayoughi, Negin and Nejati, Shiva and Sabetzadeh, Mehrdad and Saavedra, Patricio}, keywords = {Decision trees, Denial of Service (DoS) attacks, Intrusion detection, Model checking, Query checking, State-machine learning, location = Linz, Austria}, abstract = {Intrusion detection systems are crucial for network security. Verification of these systems is complicated by various factors, including the heterogeneity of network platforms and the continuously changing landscape of cyber threats. In this paper, we use automata learning to derive state machines from network-traffic data with the objective of supporting behavioural verification of intrusion detection systems. The most innovative aspect of our work is addressing the inability to directly apply existing automata learning techniques to network-traffic data due to the numeric nature of such data. Specifically, we use interpretable machine learning (ML) to partition numeric ranges into intervals that strongly correlate with a system's decisions regarding intrusion detection. These intervals are subsequently used to abstract numeric ranges before automata learning. We apply our ML-enhanced automata learning approach to a commercial network intrusion detection system developed by our industry partner, RabbitRun Technologies. Our approach results in an average 67.5\% reduction in the number of states and transitions of the learned state machines, while achieving an average 28\% improvement in accuracy compared to using expertise-based numeric data abstraction. Furthermore, the resulting state machines help practitioners in verifying system-level security requirements and exploring previously unknown system behaviours through model checking and temporal query checking. We make our implementation and experimental data available online.} }
@inproceedings{10.1145/3704323.3704367, title = {Comparison of Peruvian coffee varieties applying near infrared spectroscopy (NIRS) and machine learning}, booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition}, pages = {240--246}, year = {2025}, isbn = {9798400717482}, doi = {10.1145/3704323.3704367}, url = {https://doi.org/10.1145/3704323.3704367}, author = {Ovalle, Christian and Santillan Aching, Omar and Temoche Lopez, Alfredo and Bojorquez Segura, Jorge Alfredo}, keywords = {Coffee, MAE, Machine Learning, Near Infrared Spectroscopy, PLS, RMSE, Random Forests, SVM}, abstract = {In the coffee industry, the traditional cupping process, based on subjective judgments, has been superseded by implementing more objective techniques. Near-infrared spectroscopy (NIR) and Machine Learning (ML) algorithms were used to compare Peruvian coffee varieties in this context. Data preparation was critical, addressing outlier detection and management and partitioning into training and test sets. The evaluation focused on four models: PLS, SVM, and Random Forests. The results revealed that Random Forests stood out with exceptional performance, achieving an average accuracy of 96\%. This key metric was complemented by RMSE (Root Mean Square Error), MAE (Mean Absolute Error), and overall accuracy. The comparison of these metrics allowed us not only to evaluate the predictive ability of the models but also to understand the magnitude and nature of the prediction errors. Implementing these models provided a rapid and non-destructive evaluation of the quality of coffee varieties, overcoming the limitations of the traditional cupping approach. This study supports the feasibility of integrating NIR with ML as an effective tool for comparing Peruvian coffee varieties, as well as providing insight into the objective evaluation of coffee quality, opening new perspectives in the coffee industry.} }
@inproceedings{10.1145/3576915.3623076, title = {SalsaPicante: A Machine Learning Attack on LWE with Binary Secrets}, booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security}, pages = {2606--2620}, year = {2023}, isbn = {9798400700507}, doi = {10.1145/3576915.3623076}, url = {https://doi.org/10.1145/3576915.3623076}, author = {Li, Cathy Yuanchen and Sot\'akov\'a, Jana and Wenger, Emily and Malhou, Mohamed and Garcelon, Evrard and Charton, Francois and Lauter, Kristin}, keywords = {cryptanalysis, machine learning, post-quantum cryptography, location = Copenhagen, Denmark}, abstract = {Learning with Errors (LWE) is a hard math problem underpinning many proposed post-quantum cryptographic (PQC) systems. The only PQC Key Exchange Mechanism (KEM) standardized by NIST [13] is based on module LWE [2], and current publicly available PQ Homomorphic Encryption (HE) libraries are based on ring LWE. The security of LWE-based PQ cryptosystems is critical, but certain implementation choices could weaken them. One such choice is sparse binary secrets, desirable for PQ HE schemes for efficiency reasons. Prior work SALSA[51] demonstrated a machine learning-based attack on LWE with sparse binary secrets in small dimensions (n ≤ = 128) and low Hamming weights (h ≤ = 4). However, this attack assumes access to millions of eavesdropped LWE samples and fails at higher Hamming weights or dimensions.We present PICANTE, an enhanced machine learning attack on LWE with sparse binary secrets, which recovers secrets in much larger dimensions (up to n=350) and with larger Hamming weights (roughly n/10, and up to h=60 for n=350). We achieve this dramatic improvement via a novel preprocessing step, which allows us to generate training data from a linear number of eavesdropped LWE samples (4n) and changes the distribution of the data to improve transformer training. We also improve the secret recovery methods of SALSA and introduce a novel cross-attention recovery mechanism allowing us to read off the secret directly from the trained models. While PICANTE does not threaten NIST's proposed LWE standards, it demonstrates significant improvement over SALSA and could scale further, highlighting the need for future investigation into machine learning attacks on LWE with sparse binary secrets.} }
@article{10.1145/3648682, title = {Self-adapting Machine Learning-based Systems via a Probabilistic Model Checking Framework}, journal = {ACM Trans. Auton. Adapt. Syst.}, volume = {19}, year = {2024}, issn = {1556-4665}, doi = {10.1145/3648682}, url = {https://doi.org/10.1145/3648682}, author = {Casimiro, Maria and Soares, Diogo and Garlan, David and Rodrigues, Lu\'s and Romano, Paolo}, keywords = {Self-adaptation, machine learning, model retrain, fraud detection system}, abstract = {This article focuses on the problem of optimizing the system utility of Machine Learning (ML)-based systems in the presence of ML mispredictions. This is achieved via the use of self-adaptive systems and through the execution of adaptation tactics, such as model retraining, which operate at the level of individual ML components.To address this problem, we propose a probabilistic modeling framework that reasons about the cost/benefit tradeoffs associated with adapting ML components. The key idea of the proposed approach is to decouple the problems of estimating (1) the expected performance improvement after adaptation and (2) the impact of ML adaptation on overall system utility.We apply the proposed framework to engineer a self-adaptive ML-based fraud detection system, which we evaluate using a publicly available, real fraud detection dataset. We initially consider a scenario in which information on the model’s quality is immediately available. Next, we relax this assumption by integrating (and extending) state-of-the-art techniques for estimating the model’s quality in the proposed framework. We show that by predicting the system utility stemming from retraining an ML component, the probabilistic model checker can generate adaptation strategies that are significantly closer to the optimal, as compared against baselines such as periodic or reactive retraining.} }
@inproceedings{10.1145/3605098.3635983, title = {GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption}, booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing}, pages = {953--962}, year = {2024}, isbn = {9798400702433}, doi = {10.1145/3605098.3635983}, url = {https://doi.org/10.1145/3605098.3635983}, author = {Frimpong, Eugene and Nguyen, Khoa and Budzys, Mindaugas and Khan, Tanveer and Michalas, Antonis}, keywords = {hybrid homomorphic encryption, machine learning as a service, privacy-preserving machine learning, location = Avila, Spain}, abstract = {Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model. We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data. Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data. Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach. The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices.} }
@inproceedings{10.1145/3638529.3654154, title = {Survival-LCS: A Rule-Based Machine Learning Approach to Survival Analysis}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {431--439}, year = {2024}, isbn = {9798400704949}, doi = {10.1145/3638529.3654154}, url = {https://doi.org/10.1145/3638529.3654154}, author = {Woodward, Alexa and Bandhey, Harsh and Moore, Jason H. and Urbanowicz, Ryan J.}, keywords = {learning classifier systems, rule-based machine learning, survival analysis, genetic algorithm, location = Melbourne, VIC, Australia}, abstract = {Survival analysis is a critical aspect of modeling time-to-event data in fields such as epidemiology, engineering, and econometrics. Traditional survival methods rely heavily on assumptions and are limited in their application to real-world datasets. To overcome these challenges, we introduce the survival learning classifier system (Survival-LCS) as a more flexible approach. Survival-LCS extends the capabilities of ExSTraCS, a rule-based machine learning algorithm optimized for biomedical applications, to handle survival (time-to-event) data. In addition to accounting for right-censored observations, Survival-LCS handles multiple feature types and missing data, and makes no assumptions about baseline hazard or survival distributions.As proof of concept, we evaluated the Survival-LCS on simulated genetic survival datasets of increasing complexity derived from the GAMETES software. The four genetic models included univariate, epistatic, additive, and heterogeneous models, simulated across a range of censoring proportions, minor allele frequencies, and number of features. The results of this sensitivity analysis demonstrated the ability of Survival-LCS to identify complex patterns of association in survival data. Using the integrated Brier score as the key performance metric, Survival-LCS demonstrated reliable survival time and distribution predictions, potentially useful for clinical applications such as informing self-controls in clinical trials.} }
@article{10.14778/3749646.3749700, title = {Time-Series Clustering: A Comprehensive Study of Data Mining, Machine Learning, and Deep Learning Methods}, journal = {Proc. VLDB Endow.}, volume = {18}, pages = {4380--4395}, year = {2025}, issn = {2150-8097}, doi = {10.14778/3749646.3749700}, url = {https://doi.org/10.14778/3749646.3749700}, author = {Paparrizos, John and Bogireddy, Sai Prasanna Teja Reddy}, abstract = {Time-series clustering is a key task in time series analysis, enabling unsupervised data exploration and often serving as a subroutine for other tasks. Despite decades of active cross-disciplinary research, benchmarking of time-series clustering methods has received limited attention. Existing studies have (i) excluded popular methods and entire method classes; (ii) used a narrow range of distance measures; (iii) evaluated only a few datasets; (iv) lacked statistical validation; (v) had poor reproducibility; or (vi) relied on questionable evaluation setups. The rise of deep learning—especially foundation models claiming broad generalization—further emphasizes the need for comprehensive evaluation, as their role in time-series clustering remains largely untested. To address these gaps, we evaluate 84 time-series clustering methods across 10 method classes from data mining, machine learning, and deep learning. Our analysis spans 128 time-series datasets and uses rigorous statistical methods. Within a fair comparison framework, we (i) identify the top-performing method in each class; (ii) highlight previously overlooked, high-performing classes; (iii) challenge assumptions about elastic distance measures; (iv) refute the claimed superiority of deep learning methods, including foundation models; (v) expose reproducibility issues; (vi) analyze performance variation across dataset properties; and (vii) assess scalability. Our findings reveal an illusion of progress: no method significantly outperforms the decade-old k-Shape method. Still, we highlight a deep learning-based approach with notable promise. Our results provide a strong benchmark for advancing time-series clustering, and we have open-sourced our work to support future research.} }
@article{10.1145/3648608, title = {Resilient Machine Learning: Advancement, Barriers, and Opportunities in the Nuclear Industry}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3648608}, url = {https://doi.org/10.1145/3648608}, author = {Khadka, Anita and Sthapit, Saurav and Epiphaniou, Gregory and Maple, Carsten}, keywords = {Resilient machine learning, nuclear industry, adversaries, defences, resilience, robustness, survey}, abstract = {The widespread adoption and success of Machine Learning (ML) technologies depend on thorough testing of the resilience and robustness to adversarial attacks. The testing should focus on both the model and the data. It is necessary to build robust and resilient systems to withstand disruptions and remain functional despite the action of adversaries, specifically in the security-sensitive Nuclear Industry (NI), where consequences can be fatal in terms of both human lives and assets. We analyse ML-based research works that have investigated adversaries and defence strategies in the NI. We then present the progress in the adoption of ML techniques, identify use cases where adversaries can threaten the ML-enabled systems, and finally identify the progress on building Resilient Machine Learning (rML) systems entirely focusing on the NI domain.} }
@inproceedings{10.1145/3534678.3542914, title = {Workshop on Applied Machine Learning Management}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4874--4875}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542914}, url = {https://doi.org/10.1145/3534678.3542914}, author = {Goldenberg, Dmitri and Sokolova, Elena and Meir Lador, Shir and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit}, keywords = {data science management, machine learning management, ml product development, location = Washington DC, USA}, abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.} }
@article{10.1145/3544013, title = {A Survey on Dynamic Fuzzy Machine Learning}, journal = {ACM Comput. Surv.}, volume = {55}, year = {2022}, issn = {0360-0300}, doi = {10.1145/3544013}, url = {https://doi.org/10.1145/3544013}, author = {Liu, Li and Li, Fanzhang}, keywords = {dynamic fuzzy machine learning, dynamic fuzzy logic, Dynamic fuzzy sets}, abstract = {Dynamic fuzzy characteristics are ubiquitous in a lot of scientific and engineering problems. Specifically, the physical systems and learning processes in machine learning are dynamic and fuzzy in general. This fact has driven researchers to integrate dynamic elements into fuzzy theory and proposed dynamic fuzzy sets and dynamic fuzzy logic. Based on these pioneering theoretical works and various theories for uncertain datasets, an innovative machine learning paradigm that is referred to as dynamic fuzzy machine learning (DFML) was proposed in the early 2000s. DFML extends existing fuzzy machine learning paradigms to deal with dynamic fuzzy problems in machine learning activities. This article provides an insightful overview of DFML by surveying the field from basics to advances in five aspects: (1) the theoretical basics; (2) the system and the learning model; (3) typical DFML methods and categorization of the methods; (4) the open challenges; and (5) the research frontiers. As the first survey addressing the topic, this article intends to help more researchers better understand the basics and state-of-the-art in this field, find the most appropriate tools for a particular application, and identify possible directions for future research.} }
@inproceedings{10.1145/3650215.3650382, title = {Automatic Protein Sequences Classification Using Machine Learning Methods based on N-Gram Model}, booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application}, pages = {936--940}, year = {2024}, isbn = {9798400709449}, doi = {10.1145/3650215.3650382}, url = {https://doi.org/10.1145/3650215.3650382}, author = {Zou, Chengen}, abstract = {Protein sequence classification is a crucial task in the field of bioinformatics as it helps to reveal various types of properties of proteins. Machine learning and deep learning algorithms have great application value in the problem of protein classification prediction. In this study, we propose a novel approach for extracting sequence features based on N-Gram model. We then utilize this approach to train and evaluate machine learning and deep learning algorithms on the same dataset. The experimental results show that the Random Forest method based on N-Gram features significantly outperforms other types of algorithmic models on this dataset, with a further increase in accuracy.} }
@inproceedings{10.5555/3566055.3566095, title = {Workshop: Machine Learning in Software Quality}, booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering}, pages = {249--250}, year = {2022}, author = {Azim, Akramul and Smith, Kevin}, abstract = {The workshop focuses on talks related to advancing the software quality paradigm using machine learning (ML) and/or artificial intelligence (AI). Software testing is an essential part of develop-ment that can be further advanced with the help of automation and effectively leveraging historical information. Continuous inte-gration environments enable large-scale software testing and the large volume of data generated in the process promotes the use of data analytics and ML techniques. Moreover, the historical infor-mation is useful to improve the next version of the software from models to code, which are essential components of software engi-neering. The workshop aims to cover the topics on software testing using ML, software testing in continuous integration environments, test case prioritization using ML, software quality assurance and requirements engineering using AI/ML.} }
@inproceedings{10.1145/3703847.3703885, title = {Research on Wearable Sensors Data Collection and Health Assessment based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices}, pages = {227--231}, year = {2024}, isbn = {9798400709746}, doi = {10.1145/3703847.3703885}, url = {https://doi.org/10.1145/3703847.3703885}, author = {Chen, Runhua and Liu, Qigang and Chen, Zinuo}, keywords = {Health Assessment, Machine Learning, Self-attention Mechanism, Wearable Sensors}, abstract = {With the rapid development of economy and culture, more and more people are starting to use wearable devices for health monitoring. However, as the health monitoring industry is still in its infancy, there are many problems in the professional knowledge, skill reserves, equipment use and logistics support of users. These issues lead to suboptimal health monitoring and can pose a threat to the health and safety of users. In this work, the health assessment of wearable sensor data acquisition and machine learning studied the discrimination method of integrating multi-source sensor signals and expert experience, and designed a two-stage anomaly recognition scheme. In the first stage, the discrimination rules containing expert experience were used to complete the preliminary discrimination of anomalies, and in the research of expert knowledge base, the representation and use of expert experience in wearable systems were discussed, and the construction method of anomaly discrimination knowledge base based on problems was designed and implemented. The Transformer model is used to process multi-source sensor signals, and its self-attention mechanism and timing expression characteristics are fully utilized, which greatly reduces the false negative rate of abnormal states. Experimental analysis shows that the proposed method significantly improves the accuracy and reliability of health assessment.} }
@inproceedings{10.1145/3626252.3630759, title = {A Fast and Accurate Machine Learning Autograder for the Breakout Assignment}, booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1}, pages = {736--742}, year = {2024}, isbn = {9798400704239}, doi = {10.1145/3626252.3630759}, url = {https://doi.org/10.1145/3626252.3630759}, author = {Liu, Evan Zheran and Yuan, David and Ahmed, Ahmed and Cornwall, Elyse and Woodrow, Juliette and Burns, Kaylee and Nie, Allen and Brunskill, Emma and Piech, Chris and Finn, Chelsea}, keywords = {autograder, cs1, feedback, grading support, graphics, machine learning, location = Portland, OR, USA}, abstract = {In this paper, we detail the successful deployment of a machine learning autograder that significantly decreases the grading labor required in the Breakout computer science assignment. This assignment - which tasks students with programming a game consisting of a controllable paddle and a ball that bounces off the paddle to break bricks - is popular for engaging students with introductory computer science concepts, but creates a large grading burden. Due to the game's interactive nature, grading defies traditional unit tests and instead typically requires 8+ minutes of manually playing each student's game to search for bugs. This amounts to 45+ hours of grading in a standard course offering and prevents further widespread adoption of the assignment. Our autograder alleviates this burden by playing each student's game with a reinforcement learning agent and providing videos of discovered bugs to instructors. In an A/B test with manual grading, we find that our human-in-the-loop AI autograder reduces grading time by 44\%, while slightly improving grading accuracy by 6\%, ultimately saving roughly 30 hours over our deployment in two offerings of the assignment. Our results further suggest the practicality of grading other interactive assignments (e.g., other games or building websites) via similar machine learning techniques. Live demo at https://ezliu.github.io/breakoutgrader.} }
@inproceedings{10.1145/3725988.3725994, title = {Applications and Comparative Analysis of Machine Learning and Statistical Learning Signal Processing in Music Genre Classification}, booktitle = {Proceedings of the 2025 9th International Conference on Digital Signal Processing}, pages = {15--26}, year = {2025}, isbn = {9798400710469}, doi = {10.1145/3725988.3725994}, url = {https://doi.org/10.1145/3725988.3725994}, author = {Wu, Yue and Cheng, Zhihan and Zhou, Kaige and Chen, Baixuan and Tong, Guanchao}, keywords = {Audio Signal Processing, Deep Learning, MFCC, Machine Learning, STFT}, abstract = {Music genre classification is a crucial task in audio signal processing and machine learning, widely applied in music recommendation systems, streaming platforms, and digital libraries. This study presents a comprehensive framework integrating advanced preprocessing techniques, feature extraction, and a comparative analysis of traditional machine learning models and deep learning architectures. The preprocessing pipeline employs the Short-Time Fourier Transform (STFT) to extract time-frequency domain information, complemented by the computation of Mel-Frequency Cepstral Coefficients (MFCC) and Mel spectrograms as essential features. Data augmentation and normalization are utilized to enhance the robustness and generalization of the models. The framework evaluates traditional machine learning methods, including Support Vector Machines (SVM), Naive Bayes, Random Forest, and XGBoost, alongside deep learning architectures such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRU). The performance of these models is evaluated using the GTZAN Music Genre Dataset, a widely used benchmark in the field of music genre classification, which provides a rich and diverse set of music genres for testing.\&nbsp;Experimental results assessed using metrics like accuracy, F1 score, and confusion matrix, demonstrate that deep learning models excel at leveraging time-frequency features, while traditional models perform effectively with smaller datasets. By combining Fourier Transform-based preprocessing with robust modeling strategies, this research offers a systematic approach to improving the music genre classification performance and provides valuable insights for advancements in music signal processing.} }
@inproceedings{10.1145/3583668.3600025, title = {From Distributed Algorithms to Machine Learning and Back}, booktitle = {Proceedings of the 2023 ACM Symposium on Principles of Distributed Computing}, pages = {1}, year = {2023}, isbn = {9798400701214}, doi = {10.1145/3583668.3600025}, url = {https://doi.org/10.1145/3583668.3600025}, author = {Wattenhofer, Roger}, keywords = {networks algorithms, distributed computing, graph neural networks, location = Orlando, FL, USA}, abstract = {In the realm of computer science, it may seem that distributed computing and machine learning exist on opposite ends of the spectrum. However, there are many connections between the two domains, both in theory and practice.Recently, machine learning research has become excited about graphs. And when machine learning meets graphs, researchers familiar with distributed algorithms may experience a sense of d\'ej\`a vu, as many classic distributed computing paradigms are being rediscovered. It feels a bit like "machine learning + graphs = distributed algorithms." In my talk, I am going to introduce some key concepts in graph machine learning such as underreaching and oversquashing. These concepts have been known in the distributed computing community as local and congest, respectively.In the main part of the talk, I am going to present some recent breakthroughs in this exciting intersection of fields. Finally, I will also present some intriguing open problems.} }
@inproceedings{10.1145/3641555.3705269, title = {Three-stage Learning with Portable Online Hands-on Labware for Quantum-based Machine Learning Development}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1619--1620}, year = {2025}, isbn = {9798400705328}, doi = {10.1145/3641555.3705269}, url = {https://doi.org/10.1145/3641555.3705269}, author = {Shi, Yong and Lo, Dan and Chi, Hongmei and Polisetty, Andrew and Suo, Kun and Nguyen, Tu}, abstract = {Quantum-based Machine Learning (QML) combines quantum computing (QC) with machine learning (ML), which can be applied in various sectors, and there is a high demand for QML professionals. However, QML is not yet in many schools' curricula. We design labware for the basic concepts of QC, ML, and QML and their applications in science and engineering fields in Google Colab, applying a three-stage learning strategy for efficient and effective student learning.} }
@inproceedings{10.1145/3638530.3664166, title = {Towards Evolutionary-based Automated Machine Learning for Small Molecule Pharmacokinetic Prediction}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1544--1553}, year = {2024}, isbn = {9798400704956}, doi = {10.1145/3638530.3664166}, url = {https://doi.org/10.1145/3638530.3664166}, author = {de S\'a, Alex G. C. and Ascher, David B.}, keywords = {AutoML, bio(chem)informatics, grammar-based genetic programming, small molecules, pharmacokinetics, location = Melbourne, VIC, Australia}, abstract = {Machine learning (ML) is revolutionising drug discovery by expediting the prediction of small molecule properties essential for developing new drugs. These properties - including absorption, distribution, metabolism and excretion (ADME) - are crucial in the early stages of drug development since they provide an understanding of the course of the drug in the organism, i.e., the drug's pharmacokinetics. However, existing methods lack personalisation and rely on manually crafted ML algorithms or pipelines, which can introduce inefficiencies and biases into the process. To address these challenges, we propose a novel evolutionary-based automated ML method (AutoML) specifically designed for predicting small molecule properties, with a particular focus on pharmacokinetics. Leveraging the advantages of grammar-based genetic programming, our AutoML method streamlines the process by automatically selecting algorithms and designing predictive pipelines tailored to the particular characteristics of input molecular data. Results demonstrate AutoML's effectiveness in selecting diverse ML algorithms, resulting in comparable or even improved predictive performances compared to conventional approaches. By offering personalised ML-driven pipelines, our method promises to enhance small molecule research in drug discovery, providing researchers with a valuable tool for accelerating the development of novel therapeutic drugs.} }
@inproceedings{10.1145/3765325.3765354, title = {Exploration of Task-Oriented Teaching Reform and Practice in Geo-Information Science Based on Machine Learning}, booktitle = {Proceedings of the 2025 3rd International Conference on Educational Knowledge and Informatization}, pages = {161--166}, year = {2025}, isbn = {9798400715846}, doi = {10.1145/3765325.3765354}, url = {https://doi.org/10.1145/3765325.3765354}, author = {Xu, Lina and Liu, Siyu and Zhang, Yuxiang and Chen, Tao and Chen, Lixia}, keywords = {Geo-Information Science and Technology, Machine Learning Assessment, Process-Oriented Teaching, Project-Based Instruction, Task-Driven Learning, XGBoost}, abstract = {In response to evolving educational demands, the Geo-Information Science and Technology curriculum faces growing complexity in ensuring teaching effectiveness. This study proposes a task-driven teaching model emphasizing process-oriented learning, integrating multimedia tools, mind mapping, project-based instruction, and collaborative tasks to foster innovation. To evaluate its impact, we employed supervised machine learning algorithms—including Logistic Regression, Random Forest, and XGBoost—to analyze assessment data encompassing peer evaluations, presentation quality, and report originality. XGBoost outperformed other models in predictive accuracy and interpretability, highlighting key factors influencing student satisfaction and learning outcomes. Results reveal a strong correlation between learning effectiveness and engagement metrics, supporting the value of diversified, student-centered assessment. Our findings underscore the potential of combining pedagogical reform with data-driven analysis to enhance instructional quality and cultivate talent aligned with modern educational goals.} }
@inproceedings{10.1145/3743642.3743649, title = {Invited Paper: Rethinking Benchmarks for Parallel Machine Learning Techniques: Integrating Qualitative and Quantitative Evaluation Metrics}, booktitle = {Proceedings of the 7th Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems}, pages = {1--10}, year = {2025}, isbn = {9798400720062}, doi = {10.1145/3743642.3743649}, url = {https://doi.org/10.1145/3743642.3743649}, author = {Bahbouh, Abdulfatah and Ahmad, Ishfaq}, keywords = {Benchmarking, Parallel Machine Learning, Scalability, Sustainability, location = Huatulco, Mexico}, abstract = {Growing model complexity and data volumes in Machine Learning (ML), especially Deep Learning (DL), necessitate parallel processing for efficient, scalable computation. Benchmarking is critical for evaluating parallel ML techniques. Current methodologies predominantly emphasize quantitative metrics like throughput and accuracy, exemplified by MLPerf and HPC-AI500. Such quantitative focus neglects key qualitative factors-portability, robustness, deployment complexity, and usefulness-essential for practical parallel ML. This paper argues for a paradigm shift in benchmarking, advocating for a holistic evaluation framework that inherently integrates qualitative assessments alongside traditional quantitative measures. To this end, we introduce NNoPP (Neural Network on-top-of Parallel Processing), a structured evaluation approach designed to complement existing benchmarks. NNoPP proposes criteria---novelty, portability, performance, scalability, complexity, usefulness, and sustainability---methodically defined for real-world ML solution viability, sustainability, and impact. Beyond performance numbers, these metrics promote sustainable solutions: computationally efficient, practically deployable, broadly applicable, and robust across diverse contexts. Through illustrative case studies across key DL model families, we demonstrate the application of NNoPP and its capacity to provide nuanced insights beyond conventional benchmark rankings. Future directions for parallel ML benchmarking emphasize integrated metrics, dynamic workloads, domain-specific relevance, and collaborative community evolution. Ultimately, this paper emphasizes a re-envisioned holistic benchmarking approach that fosters the adoption of parallel ML solutions that are both quantitatively superior and qualitatively robust, useful, and sustainably impactful for continuous advancement.} }
@article{10.5555/3606402.3606413, title = {Practical Machine Learning for Liberal Arts Undergraduates}, journal = {J. Comput. Sci. Coll.}, volume = {38}, pages = {69--79}, year = {2023}, issn = {1937-4771}, author = {Sherman, Mark and Hogan, Alyssa and O'Sullivan, Jamison and Schumacher, Samantha}, abstract = {Liberal arts education provides students with many interdisciplinary problem-solving skills, but utilizing high-level computational tools like machine learning (ML) remains largely inaccessible without a significant background in computer science. We present a course to bridge that gap: a full-semester undergraduate course that teaches the concepts of ML and deep learning with emphasis on real-world application and ethical considerations. The course is low-code, exploring concepts and applications through a collection of visual tools. Early outcomes show that after this course students are equipped to learn code-based systems, feel empowered to understand and identify misunderstandings in popular AI discourse, and can design ML-based solutions for data-oriented problems in their fields.} }
@inproceedings{10.1145/3718491.3718639, title = {Classification Prediction of ADMET Properties for Anti-Pancreatic Cancer Drug Candidates Using Machine Learning Approaches}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {914--919}, year = {2025}, isbn = {9798400710865}, doi = {10.1145/3718491.3718639}, url = {https://doi.org/10.1145/3718491.3718639}, author = {Wu, Chao and Huang, Zongxiang and Feng, Yanghui and Wang, Yukun and Xie, Zongsheng and Chen, Jinsheng}, keywords = {Pancreatic cancer, classification prediction, ensemble learning model, machine learning}, abstract = {Utilizing machine learning algorithms, this study aimed to predict the pharmacokinetic properties (ADMET properties) of anti-pancreatic cancer drug candidates, focusing on five key indicators: Absorption, Distribution, Metabolism, Excretion, and Toxicity. Based on 729 molecular descriptors derived from 1,974 compounds, classification prediction models were developed for intestinal epithelial cell permeability (Caco-2), cytochrome P450 3A4 subtype (CYP3A4) inhibition, cardiac safety (hERG), oral bioavailability (HOB), and micronucleus (MN) test. Various algorithms, including decision trees, discriminant analysis, support vector machines (SVMs), K-nearest neighbors, and ensemble learning, were applied. The Quadratic SVM algorithm emerged as the best performer in predicting Caco-2, CYP3A4, and hERG, achieving accuracies of 91.6\%, 95.0\%, and 90.5\%, respectively. In contrast, the Boosted Tree model within ensemble learning showed superior performance in predicting HOB and MN, with accuracies of 90.5\% and 96.1\%. These findings demonstrate that machine learning algorithms significantly enhance the efficiency and success rate of drug development in the context of anti-pancreatic cancer candidates.} }
@inproceedings{10.1145/3736426.3736474, title = {Data-Driven Supplier Management Framework: Integrating Machine Learning Models into Planning and Assessment Strategies}, booktitle = {Proceedings of the 2025 International Conference on Digital Management and Information Technology}, pages = {304--309}, year = {2025}, isbn = {9798400714238}, doi = {10.1145/3736426.3736474}, url = {https://doi.org/10.1145/3736426.3736474}, author = {Lin, Nuo and Wang, Shiyin and Gao, Yaguxun}, keywords = {Algorithms, Civil Aircraft, Machine Learning Models, Plan Control}, abstract = {Civil aircraft programmes are distinguished by their considerable scale and duration. They also involve synergies with numerous suppliers. By establishing an effective evaluation mechanism for supplier programme management, the main manufacturer can regularly audit and monitor the quality of the supplier's work and the progress of the development to identify and correct problems in the supplier's project development process in good time.This paper presents in-depth applications of computer technology, such as machine learning-based prediction models and optimisation algorithms, to further improve the efficiency and accuracy of supplier planning control. In addition, this paper proposes a practical and efficient appraisal strategy for supplier program control, aiming to improve suppliers' program execution capability and project delivery quality.By establishing three rating levels, A, B and C, in the programme evaluation, the rating level directly affects the allocation of research and development funds to stimulate supplier enthusiasm and initiative.} }
@inproceedings{10.1145/3723178.3723179, title = {Machine Learning-Enhanced Cross-Tier Security and Anomaly Detection in Wireless Body Area Networks}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {1--8}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723179}, url = {https://doi.org/10.1145/3723178.3723179}, author = {Shamshuzzoha, Md and Islam, Md. Motaharul}, keywords = {Machine Learning, Cross-Tier Security, Wireless Body area Networks, secured Healthcare, Anomaly Detection}, abstract = {Nowadays, in the dynamic landscape of e-health, the relentless pace of information technology advances brings both transformative possibilities and heightened security concerns, particularly within WBANs. At the forefront of this challenge is the critical imperative to fortify security measures across healthcare Internet of Things environments. This research addresses this pressing issue by championing a pioneering three-layered defense system, thoughtfully integrating the potency of machine learning techniques. The paramount emphasis is on achieving cross-tier security, strategically targeting the device, Hub, and Cloud layers. The groundbreaking SenseGuard anomaly detection system, fueled by sophisticated machine learning algorithms, not only elevates security at the Device layer but strategically reinforces patient wellness and the entire WBAN network. Simultaneously, an innovative intrusion detection algorithm fortifies the Hub layer, while adaptive machine learning models in the Cloud layer ensure a comprehensive defense. This approach provides an effective solution to the security challenges inherent in healthcare IoT as a remarkable achievement in advancing the state-of-the-art, placing cross-tier security at the forefront of WBAN innovation.} }
@inproceedings{10.1145/3669721.3674555, title = {Data-Driven Modeling of Miniature Hall Thrusters: A Machine Learning Approach}, booktitle = {Proceedings of the 2024 3rd International Symposium on Intelligent Unmanned Systems and Artificial Intelligence}, pages = {216--220}, year = {2024}, isbn = {9798400710025}, doi = {10.1145/3669721.3674555}, url = {https://doi.org/10.1145/3669721.3674555}, author = {Zine el abidine, Hebboul and Tang, Hai-Bin and Wang, Zixiang}, abstract = {New data-driven and physics-based modeling approaches have been applied synergistically to advance the design of sub-kilowatt Hall thrusters. An extensive literature database was compiled and cleaned using a custom program to impute missing values resulting from patents and confidentiality restrictions. Generative adversarial networks (GANs) augmented the limited dataset, covering various thruster geometries and operating conditions. A correlation analysis identified the most influential performance parameters, which served as inputs to a surrogate artificial neural network (ANN) model for rapid design prediction. The ANN predictions were closely aligned with conventional linear regression models, thus validating the approach. Numerical simulations of the ANN-generated designs demonstrated accurate performance projections, highlighting the potential of these new machine-learning techniques in the design of electric propulsion systems. This framework synergized data resources, machine learning models, and high-fidelity simulations to realize next-generation, low-power Hall thrusters.} }
@inproceedings{10.1145/3644713.3644838, title = {Machine Learning Algorithms to Detect Illicit Accounts on Ethereum Blockchain}, booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems}, pages = {747--752}, year = {2024}, isbn = {9798400709036}, doi = {10.1145/3644713.3644838}, url = {https://doi.org/10.1145/3644713.3644838}, author = {Obi-Okoli, Chibuzo and Jogunola, Olamide and Adebisi, Bamidele and Hammoudeh, Mohammad}, keywords = {Ethereum blockchain, anomaly detection, blockchain security, illicit activities, machine learning, location = Dubai, United Arab Emirates}, abstract = {The rapid growth and psudonomity inherent in blockchain technology such as in Bitcoin and Ethereum has marred its original intent to reduce dependant on centralised system, but created an avenue for illicit activities, including fraud, phishing, scams, etc. This undermines the reputation of blockchain network, giving rise to the need to identify these illicit activities within the blockchain network. This current work tackles this crucial problem by investigating and implementing six machine learning algorithms with a particular emphasis on striking a balance between accuracy, precision and recall. The novelty of the work lies in the utilising of the synthetic minority over-sampling technique to handle data imbalance. Thus, increasing the accuracy of the light gradient boosting machine classifier to 98.4\%. The outcome of this work holds great potential for enhancing the security and credibility of blockchain ecosystems paving the way for a more secure and dependable digital future in the age of decentralised and trustless systems.} }
@inproceedings{10.1145/3589883.3589886, title = {Energy-aware Tiny Machine Learning for Sensor-based Hand-washing Recognition}, booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies}, pages = {15--22}, year = {2023}, isbn = {9781450398329}, doi = {10.1145/3589883.3589886}, url = {https://doi.org/10.1145/3589883.3589886}, author = {Lattanzi, Emanuele and Calisti, Lorenzo}, keywords = {Energy aware, Human Activity Recognition, Long Short-Term Memory networks, Support Vector Machine, Tiny Machine Learning, location = Stockholm, Sweden}, abstract = {Tiny wearable devices are nowadays one of the most popular and used devices in everyday life. At the same time, machine learning techniques have reached a level of maturity such that they can be used in the most varied fields. The union of these two technologies represents a valuable opportunity for the development of pervasive computing applications. On the other hand, pushing the machine learning inference on a wearable device can lead to nontrivial issues. In fact, devices with small size and low-energy availability, like those dedicated to wearable platforms, pose strict computational, memory, and power requirements which result in challenging issues to be addressed by designers. The main purpose of this study is to empirically explore the trade-off between energy consumption and classification accuracy of a machine learning-based hand-washing recognition task deployed on a real wearable device. Through extensive experimental results, obtained on a public human activity recognition dataset, we demonstrated that given an identical level of classification performance, a classic SVM classifier can save about 40\% of energy with respect to a more complex LSTM network. Moreover, reducing the LSTM complexity, by lowering the number of its internal unit, can make the LSTM network energy cost-effective (with a savings of about 30\%) at the cost of a reduction in accuracy of only 2\%.} }
@inbook{10.1145/3745238.3745497, title = {The Application of Big Data Screening and Machine Learning in Developing Investment Strategies for the Stock Market}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {1650--1654}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745497}, author = {Zou, Chengyu}, abstract = {Substantial investors often rely on valuation and financial indices to forecast stock prices, but no method guarantees profits, as even financially strong companies can see their stock prices decline. In this case, more research should be done to look for some specific characteristics of the rising of stock price and verify of what degree can these characteristics help forecasting the changes of stock price. This study will shift out the relatively valuable data with the help of a system function which represent the difficulty to predict a result for polytomized variable. For continuous variable, using reduced classification method or calculating the variance of data to evaluate its value. Moreover, this study will explain how to evaluate machine learning models with cost matrix and how to improve the accuracy of machine learning models with bagging method and Adaboost method. With the theories introduced in this study, investors can shift out more valuable message from applying the difficulty of prediction function or calculating variance of continuous variable data and update their invest strategies more efficiently and earn more profit with the advanced machine learning models.} }
@inproceedings{10.1145/3647444.3647842, title = {Transforming Healthcare through Machine Learning: A Revolution in Patient Care}, booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence}, year = {2024}, isbn = {9798400709418}, doi = {10.1145/3647444.3647842}, url = {https://doi.org/10.1145/3647444.3647842}, author = {Gupta, Divya and Kaur, Jaspreet and Kaur, Simarjeet}, keywords = {Additional Key Words and Phrases: Machine learning (ML), healthcare, Artificial Intelligence (AI), location = Jaipur, India}, abstract = {Machine learning (ML) ushered in a new era in healthcare by providing unprecedented opportunities to improve patient health, improve clinical decision making, and improve patient outcomes. This article provides an overview of the far-reaching implications of machine learning in healthcare, highlighting its different applications and the challenges associated with it. In this study, we explore how machine learning (ML) can be used to improve healthcare, including personalised treatment planning, disease prediction, medication development, and medical picture analysis. The ability of ML algorithms to spot subtle patterns, forecast illness outbreaks, and enable early intervention has been proved by studying a large diversity of healthcare data. This study also explores the difficulties and moral dilemmas associated with ML integration into healthcare systems, highlighting the significance of data protection, bias reduction, and legal compliance. We also look at the possible advantages and drawbacks of these technologies in environments with limited resources, highlighting the necessity for flexible and accessible solutions.} }
@inbook{10.1145/3757749.3757838, title = {Research on Landscape Architecture Space Optimization Based on Machine Learning: Taking Tourist Behavior Analysis as an Example}, booktitle = {Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology}, pages = {535--539}, year = {2025}, isbn = {9798400713347}, url = {https://doi.org/10.1145/3757749.3757838}, author = {Wang, Yanrong}, abstract = {This study presents a data-driven framework integrating machine learning (ML) techniques to analyze tourist behavior and optimize spatial configurations in landscape architecture. By leveraging GPS trajectory data, IoT sensor networks, and GIS spatial analytics, we develop models to cluster visitor groups, predict activity preferences, and forecast crowd dynamics. Key methodologies include DBSCAN for activity zone identification, Random Forest for spatial preference modeling, and LSTM networks for congestion forecasting. A case study of Qingfeng Park in Shanghai demonstrates the framework's application, yielding a 32\% reduction in peak-hour congestion and a 28\% improvement in facility utilization. Mathematical formulations for spatial interaction modeling and model evaluation metrics are explicitly detailed, providing a replicable framework for evidence-based landscape design.} }
@inproceedings{10.1145/3660853.3660929, title = {Secure Data Transmission in IoT Networks using Machine Learning-based Encryption Techniques}, booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference}, pages = {285--291}, year = {2024}, isbn = {9798400716928}, doi = {10.1145/3660853.3660929}, url = {https://doi.org/10.1145/3660853.3660929}, author = {Thamer, Khudhair Abed and Ahmed, Saadaldeen Rashid and Almashhadany, Mohammed Thakir Mahmood and Abdulqader, Sarah G. and Abduladheem, Wameedh and Algburi, Sameer}, keywords = {Encryption Techniques, Internet of Things (IoT), Machine Learning, data transmission, security protocols, location = undefinedstanbul, Turkiye}, abstract = {The penetration of Internet of Things (IoT) appliances by day leads to the escalation of security protocols in data transmission as the topic of the highest priority level. While traditional admission procedures grow on the stream of innovative internet linked technology, they lack countering the fluctuation caused by the innovation. This study explicitly focuses on the area of ML encryption algorithms, believed to be renovators of the essential principles of IoT security. The ability of machine learning to make humans ready to constantly update the algorithms doing programming and harness the wisdom given by real-time data examination engenders a masterpiece (tapestry) of solutions that can reject the continuous evolution of cybersecurity menaces among IoT networks. An exploration experience is endured in deep, to the hearts, experiments, which then disclose the courses that networks in transit, encrypted ones, follow across the full collection of state-of-the art machine learning algorithms. Tender as the artificial neural network may be, it perseveres and finally overshadows the naysayers near the conclusion of the scene when it replaces the flawed analyzing method. These measurements of validation toll like a sacrament, quivering with the resounding acclaim that testifies the latent power that lurks at the center of those approaches. Like an angelic cry of the arena, this remark stimulates and uplifts IoT ecosystem security within sacred limits. The waves that arise from these findings expand and assault activities played by many, which serves as a reminder of the essential role played by blending the most modern machine learning technologies with tailored solutions unique to the demands of IoT devices. This traversal through the historical chapters of the study imputes onto the realm of IoT security a mantle with an increased drive for expansion that is intended at further creating a new story that will determine the future route for this vital component.CCS CONCEPTS • Security and privacy∼ Systems security∼ Network security∼Software and application security} }
@inproceedings{10.1145/3686592.3686600, title = {Comparison of Machine Learning Methods for Binary Classification of Multicollinearity Data}, booktitle = {Proceedings of the 2024 7th International Conference on Mathematics and Statistics}, pages = {44--49}, year = {2024}, isbn = {9798400707223}, doi = {10.1145/3686592.3686600}, url = {https://doi.org/10.1145/3686592.3686600}, author = {Araveeporn, Autcha and Wanitjirattikal, Puntipa}, keywords = {Backpropagation Neural Network, Na\"ve Bayes, Random Forest, Support Vector Machine}, abstract = {This study examines the effectiveness of binary classification performance in multicollinearity. Four machine learning methods, namely backpropagation neural network, Na\"ve Bayes, support vector machine, and random forest, are compared in terms of their efficiency in handling multicollinear data. The evaluation of binary classification performance efficiency considers multicollinearity in independent variables, considering both a constant correlation model and the Toeplitz correlation. Correlation coefficients of 0.1 and 0.9 are explored in the analysis. The independent variables in this study are simulated from a multivariate normal distribution with 10, 20, 30, and 40 variables, respectively. The dependent variable is constructed using the logit function with sample sizes of 100 and 200. The simulation and data analysis are performed using the R Studio program and repeated 1,000 times for each scenario. The findings of this research reveal that the backpropagation neural network and Na\"ve Bayes methods exhibit superior performance in determining the mean accuracy percentage under constant correlation. On the other hand, the backpropagation neural network and support vector machine are the most effective methods in determining the mean accuracy percentage when dealing with multicollinearity in the form of Toeplitz correlation.} }
@article{10.1145/3579363, title = {Directive Explanations for Actionable Explainability in Machine Learning Applications}, journal = {ACM Trans. Interact. Intell. Syst.}, volume = {13}, year = {2023}, issn = {2160-6455}, doi = {10.1145/3579363}, url = {https://doi.org/10.1145/3579363}, author = {Singh, Ronal and Miller, Tim and Lyons, Henrietta and Sonenberg, Liz and Velloso, Eduardo and Vetere, Frank and Howe, Piers and Dourish, Paul}, keywords = {counterfactual explanations, directive explanations, Explainable AI}, abstract = {In this article, we show that explanations of decisions made by machine learning systems can be improved by not only explaining why a decision was made but also explaining how an individual could obtain their desired outcome. We formally define the concept of directive explanations (those that offer specific actions an individual could take to achieve their desired outcome), introduce two forms of directive explanations (directive-specific and directive-generic), and describe how these can be generated computationally. We investigate people’s preference for and perception toward directive explanations through two online studies, one quantitative and the other qualitative, each covering two domains (the credit scoring domain and the employee satisfaction domain). We find a significant preference for both forms of directive explanations compared to non-directive counterfactual explanations. However, we also find that preferences are affected by many aspects, including individual preferences and social factors. We conclude that deciding what type of explanation to provide requires information about the recipients and other contextual information. This reinforces the need for a human-centered and context-specific approach to explainable AI.} }
@inproceedings{10.1145/3526073.3527584, title = {Operationalizing machine learning models: a systematic literature review}, booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI}, pages = {1--8}, year = {2023}, isbn = {9781450393195}, doi = {10.1145/3526073.3527584}, url = {https://doi.org/10.1145/3526073.3527584}, author = {Kolltveit, Ask Berstad and Li, Jingyue}, keywords = {systematic literature review, operationalization, machine learning, deployment, MLOps, location = Pittsburgh, Pennsylvania}, abstract = {Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments.} }
@inproceedings{10.1145/3664646.3664769, title = {Leveraging Machine Learning for Optimal Object-Relational Database Mapping in Software Systems}, booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software}, pages = {94--102}, year = {2024}, isbn = {9798400706851}, doi = {10.1145/3664646.3664769}, url = {https://doi.org/10.1145/3664646.3664769}, author = {Azizian, Sasan and Rastegari, Elham and Bagheri, Hamid}, keywords = {Dynamic Analysis, Machine Learning, ORM Mapping, Relational logic, Specification-driven Synthesis, Static Analysis, Tradespace Analysis, location = Porto de Galinhas, Brazil}, abstract = {Modern software systems, developed using object-oriented programming languages (OOPL), often rely on relational databases (RDB) for persistent storage, leading to the object-relational impedance mismatch problem (IMP). Although Object-Relational Mapping (ORM) tools like Hibernate and Django provide a layer of indirection, designing efficient application-specific data mappings remains challenging and error-prone. The selection of mapping strategies significantly influences data storage and retrieval performance, necessitating a thorough understanding of paradigms and systematic tradeoff exploration. The state-of-the-art systematic design tradeoff space exploration faces scalability issues, especially in large systems. This paper introduces a novel methodology, dubbed Leant, for learning-based analysis of tradeoffs, leveraging machine learning to derive domain knowledge autonomously, thus aiding the effective mapping of object models to relational schemas. Our preliminary results indicate a reduction in time and cost overheads associated with developing (Pareto-) optimal object-relational database schemas, showcasing Leant's potential in addressing the challenges of object-relational impedance mismatch and advancing object-relational mapping optimization and database design.} }
@proceedings{10.1145/3696271, title = {MLMI '24: Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, year = {2024}, isbn = {9798400717833} }
@inproceedings{10.1145/3644116.3644176, title = {Application of Machine Learning Algorithms in Predicting Hepatitis C}, booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science}, pages = {359--365}, year = {2024}, isbn = {9798400708138}, doi = {10.1145/3644116.3644176}, url = {https://doi.org/10.1145/3644116.3644176}, author = {Wang, Yunchuan and Yin, Baohua and Zhu, Qiang}, abstract = {Hepatitis C, caused by the hepatitis C virus (HCV) infection, is a disease that can progress from initial asymptomatic stages to chronic infection if left untreated, potentially leading to cirrhosis and liver cancer. The diagnosis of hepatitis C requires at least two different types of tests: serological tests and molecular tests. These testing methods impose a financial burden on patients and contribute to patient attrition. The objective of this study is to predict this disease using various machine learning techniques based on common blood test data, in order to achieve early diagnosis and treatment for patients. In this study, we integrated features from literature and original data, applying six machine learning algorithms (logistic regression (LR), support vector machine (SVM), K-nearest neighbors (KNN), decision tree (DT), random forest (RF), adaptive boosting (AdaBoost)) to forecast hepatitis C. The performance of these techniques was compared using metrics such as accuracy, precision, recall, F1-score, receiver operating characteristics (ROC), and the area under the curve (AUC) to identify suitable methods for this disease. Results from the UCI dataset indicate that AdaBoost achieved the highest accuracy (97.8\%) and AUC (0.994), making it an effective and cost-efficient method for predicting hepatitis C.} }
@inproceedings{10.1145/3625343.3625344, title = {Practical and Efficient Secure Aggregation for Privacy-Preserving Machine Learning}, booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics}, year = {2023}, isbn = {9798400708312}, doi = {10.1145/3625343.3625344}, url = {https://doi.org/10.1145/3625343.3625344}, author = {Zhang, Yuqi and Li, Xiangyang and Luo, Qingcai and Wang, Yang and Shen, Yanzhao}, keywords = {Secure Multi-party Computation, Secure Aggregation, Information Security, Homomorphic Encryption, Federal Learning, location = Bangkok, Thailand}, abstract = {In recent years, Federal Learning has received much attention becourse it can train models by updating gradients without contacting users’ true data. However, adversaries also can track users’ privacy from the shared gradient. In this paper, we aim to solve three major issues during the process of federated learning: 1) how to protect users’ privacy during training; 2) How to verify the correctness of the aggregation results returned from the server; 3) How to reduce communication costs while ensuring training security. So we propose a verifiable aggregation scheme that can effectively verify the results of server aggregation. Specifically, we follow the classic double mask aggregation scheme, and use Paillier homomorphic encryption algorithm to implement the message authentication code with additive homomorphic property. Users can compare their local codes with server’s aggregation results to verify the correctness of the aggregation results and improve model’s accuracy. In our framework, we adopt a Top-k gradient selection scheme to reduce models’ communication and computing overhead. Experimental results indicate that our training framework is feasible and efficient.} }
@inproceedings{10.1145/3712335.3712411, title = {Prediction of mechanical properties of Q345qE steel welded joints based on machine learning}, booktitle = {Proceedings of the 3rd International Conference on Signal Processing, Computer Networks and Communications}, pages = {432--438}, year = {2025}, isbn = {9798400710834}, doi = {10.1145/3712335.3712411}, url = {https://doi.org/10.1145/3712335.3712411}, author = {Liu, Chengqing and Hou, Xiaolong and Liu, Lijun and Qiao, Zhu and Qin, Zhiqi and Wang, Shengbao}, keywords = {BP neural network, Mechanical properties, Prediction model, Welding wire composition}, abstract = {The mechanical properties of welded joints are the key indicators to evaluate the welding quality, which are mainly subject to the rationality of the welding process and the technical level of the welders. Traditionally, enterprises screen out the optimal welding scheme by implementing a large number of welding procedure qualification tests, but this method is costly and inefficient. In this study, the prediction model of tensile strength and yield strength of Q345 qE steel welded joints was established based on the machine learning algorithm of BP neural network. The results show that the prediction accuracy of the model is high, the R2 value is more than 0.96, and the relative error is controlled within (pm ) 3MPa (tensile strength) and ( pm ) 5MPa (yield strength). This prediction method is expected to reduce the dependence on laboratory tensile tests, reduce experimental costs, and has significant application value for actual production.} }
@inproceedings{10.1145/3658271.3658341, title = {Professionals' Perceptions of the Interaction between User Experience and Machine Learning}, booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems}, year = {2024}, isbn = {9798400709968}, doi = {10.1145/3658271.3658341}, url = {https://doi.org/10.1145/3658271.3658341}, author = {Costa, Ricardo Luiz Hentges and Soares, Tales Schifelbein and Lunardi, Gabriel Machado and Valle, Pedro Henrique Dias and Silva, Williamson}, keywords = {Machine Learning, Qualitative Analysis, User Experience, location = Juiz de Fora, Brazil}, abstract = {Context: Users connect significantly with Information Systems (ISs) daily. The software industry is increasingly dedicated to meeting User Experience (UX) needs. Despite this, there is little evidence that UX experts use techniques to automate processes, such as employing Machine Learning (ML) techniques. The convergence of UX and ML is gaining relevance for its potential to enhance UX quality. Problem: This work arises from the need to understand perceptions, challenges, and opportunities in the intersection of UX and ML. The central issue is to gain insights enriching the understanding of ML’s transformative role in UX research, identifying integration advantages and challenges. Method: Semi-structured interviews were conducted with six professionals experienced in the intersection of UX and ML. A qualitative analysis collected detailed perceptions, generating relevant categories to understand professionals’ views on UX and ML integration. IS Theory: This work was conceived under the auspices of the community of practice theory, aiming to help community members interact to share ideas and thoughts and expand their knowledge. Results Summary: The interview analysis identified nine main categories, covering ML professionals’ difficulties in integrating UX into positive perceptions about ML adoption, providing a comprehensive view of respondents’ opinions. Contributions and Impact on the IS Area: The results aim to significantly contribute to the ISs community, offering valuable insights into professionals’ perceived advantages, disadvantages, and challenges in UX and ML convergence. The study enhances understanding of ML’s role in UX, supporting future practices and related research and positively impacting systems’ development and improvement.} }
@proceedings{10.1145/3630050, title = {SAFE '23: Proceedings of the 2023 on Explainable and Safety Bounded, Fidelitous, Machine Learning for Networking}, year = {2023}, isbn = {9798400704499}, abstract = {It is with great pleasure that we welcome you to the 2023 ACM CoNEXT Workshop on 'Explainable and Safety Bounded, Fidelitous, Machine Learning for Networking' - SAFE'23. We are excited to be hosting the first edition of this workshop, and it brings us pleasure to see the growing interest and enthusiasm surrounding the convergence of machine learning and networking. Machine learning offers promising solutions for network optimization, security, and management. Control and decision-making algorithms are critical for the operation of networks, hence we believe that the solutions should be safety bounded and interpretable. Understanding the decisions and behaviors of machine learning models is crucial for optimizing network performance, enhancing security, and ensuring reliable network operations. This is a very crucial topic which needs to be addressed, as network operators, managers or administrators are reluctant to use ML based solutions which are black box in nature. The production networks have a critical and sensitive nature, where outages or performance degradations can be very costly.} }
@inproceedings{10.1145/3606042.3616460, title = {AI/Machine Learning for Internet of Dependable and Controllable Things}, booktitle = {Proceedings of the 2023 Workshop on Advanced Multimedia Computing for Smart Manufacturing and Engineering}, pages = {1--2}, year = {2023}, isbn = {9798400702730}, doi = {10.1145/3606042.3616460}, url = {https://doi.org/10.1145/3606042.3616460}, author = {Song, Houbing Herbert}, keywords = {artificial intelligence, internet of things, machine learning, location = Ottawa ON, Canada}, abstract = {The Internet of Things (IoT) has the potential to enable a variety of applications and services. However, it also presents grand challenges in security, safety, and privacy. Therefore, there is a need for moving from IoT to Internet of Dependable Things, which is defined as Internet of Things which is designed, built, deployed and operated in a highly trustworthy manner, and Internet of Controllable Things, which is defined as Internet of Things which is operated in a highly controllable manner. A massive resurgence of artificial intelligence (AI) and machine learning (ML) presents tremendous opportunities for Internet of Dependable and Controllable Things and as well as significant challenges to Internet of Dependable and Controllable Things. In this lecture, I will present the state of the art by reviewing and classifying the existing literature, evaluate the opportunities and challenges, and identify trends by evaluating what needs to be done to enable AI/Machine Learning for Internet of Dependable and Controllable Things.} }
@inproceedings{10.1145/3565287.3617639, title = {Mitigating Racial Biases for Machine Learning Based Skin Cancer Detection}, booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing}, pages = {556--561}, year = {2023}, isbn = {9781450399265}, doi = {10.1145/3565287.3617639}, url = {https://doi.org/10.1145/3565287.3617639}, author = {Abhari, Julian and Ashok, Ashwin}, keywords = {generative adversarial networks, app, computer vision, bias, artificial intelligence, skin tones, racial, domain adaptation, machine learning, skin cancer, location = Washington, DC, USA}, abstract = {Machine learning (ML) based skin cancer detection tools are an example of a transformative medical technology that could potentially democratize early detection for skin cancer cases for everyone. However, due to the dependency of datasets for training, ML based skin cancer detection always suffers from a systemic racial bias. Racial communities and ethnicity not well represented within the training datasets will not be able to use these tools, leading to health disparities being amplified. Based on empirical observations we posit that skin cancer training data is biased as it's dataset represents mostly communities of lighter skin tones, despite skin cancer being far more lethal for people of color. In this paper we use domain adaptation techniques by employing CycleGANs to mitigate racial biases existing within state of the art machine learning based skin cancer detection tools by adapting minority images to appear as the majority. Using our domain adaptation techniques to augment our minority datasets, we are able to improve the accuracy, precision, recall, and F1 score of typical image classification machine learning models for skin cancer classification from the biased 50\% accuracy rate to a 79\% accuracy rate when testing on minority skin tone images. We evaluate and demonstrate a proof-of-concept smartphone application.} }
@article{10.1145/3688086, title = {Harnessing Machine Learning and Generative AI: A New Era in Online Tutoring Systems}, journal = {XRDS}, volume = {31}, pages = {40--45}, year = {2024}, issn = {1528-4972}, doi = {10.1145/3688086}, url = {https://doi.org/10.1145/3688086}, author = {Schmucker, Robin}, abstract = {Discover how the convergence of machine learning and generative AI is revolutionizing online tutoring, enabling systems that evolve to become better teachers--continuously refining their instructional methods based on student data and feedback.} }
@inproceedings{10.1145/3689236.3691500, title = {Multimodal Machine Learning Based Object Recognition Techniques for Engineering Applications}, booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering}, pages = {724--730}, year = {2024}, isbn = {9798400718137}, doi = {10.1145/3689236.3691500}, url = {https://doi.org/10.1145/3689236.3691500}, author = {Zeng, An and Qian, Ying and Zhou, Qing and Xie, Honghui}, keywords = {Corner Detection, Deep Learning, Graph Convolutional Neural Network, Multimodal}, abstract = {Object recognition technology has important applications in the industrial field, but there are limitations in relying on single modal information. To address this problem, the study proposes a multimodal object recognition system that combines tactile sensors and visual information, and processes tactile data through graph convolutional neural networks to improve recognition accuracy. The system utilizes multi-channel impulse map convolutional layer, attention layer and fully connected layer for tactile data analysis, and introduces Harris corner point detection algorithm, orientation gradient histogram and optical flow field orientation histogram to enhance visual feature extraction. The experimental results show that the recognition accuracy of the multimodal neural network system on different datasets reaches 95.85\%, which is better than the performance of traditional KNN and DTW algorithms. The research system demonstrates high accuracy and stability in error analysis and physical localization experiments, for example, the error is only 0.1 cm in distance measurement experiments, and the attitude angle error is within 3 degrees. By combining haptic and visual information, the system realizes a more comprehensive and precise recognition of objects, which provides strong support for industrial automation and intelligent manufacturing. In summary, the effectiveness and advantages of multimodal fusion technology in object recognition tasks are remarkable and have a wide range of application potential.} }
@inproceedings{10.1145/3700058.3700062, title = {Identification of Stock Violations in Chinese Listed Companies: Based on Machine Learning Algorithms}, booktitle = {Proceedings of the International Conference on Digital Economy, Blockchain and Artificial Intelligence}, pages = {27--33}, year = {2024}, isbn = {9798400710261}, doi = {10.1145/3700058.3700062}, url = {https://doi.org/10.1145/3700058.3700062}, author = {Ye, Jiasheng}, keywords = {A-share market, ESG, XGBoost, machine learning}, abstract = {In recent years, frequent violations by publicly listed companies in China have eroded market trust. This study employs traditional financial and non-financial metrics, emerging ESG metrics, and composite indicators constructed from merger data to predict violations using these models. We identify the most effective data indicators and model algorithms through rigorous comparative analysis. The findings reveal that ESG rating metrics, along with conventional financial and non-financial metrics, provide robust predictive accuracy for regulatory breaches. Moreover, the composite metrics outperform individual sets. The XGBoost algorithm performs best among machine learning model algorithms, particularly trained by composite indicators.} }
@inproceedings{10.1145/3745533.3745632, title = {Reliability Analysis of Seismic Foundation Bearing Capacity via Machine Learning and Monte Carlo Simulation}, booktitle = {Proceedings of the 2025 5th International Conference on Applied Mathematics, Modelling and Intelligent Computing}, pages = {614--620}, year = {2025}, isbn = {9798400713873}, doi = {10.1145/3745533.3745632}, url = {https://doi.org/10.1145/3745533.3745632}, author = {Wang, Naixin and Wang, Yujie and Sun, Ping}, keywords = {Finite Element Limit Analysis (FELA), Monte Carlo Simulation (MCS), Multi-Layer Perceptron (MLP), Reliability analysis, Seismic foundation bearing capacity}, abstract = {The assessment of seismic foundation bearing capacity is crucial for ensuring the stability and safety of buildings, hydraulic structures, and bridges. Traditional safety factor methods are limited in capturing parameter uncertainties and quantifying failure probabilities. To address this issue, This study develops an efficient reliability assessment framework by integrating FELA with a Multi-Layer Perceptron (MLP) model. The FELA method is employed to compute seismic foundation bearing capacity and generate a large-scale dataset encompassing diverse parameter combinations. This dataset is used to train the MLP model, which establishes a highly accurate mapping between input parameters and ultimate bearing capacity. Monte Carlo Simulation (MCS) is incorporated to quantify failure probabilities and reliability indices under static and seismic conditions, significantly improving computational efficiency. The analysis determines a reliability index threshold of β = 2.58 for seismic conditions, providing a probabilistic reference for foundation safety evaluation. An engineering case study validates the effectiveness and practical applicability of the proposed approach. The combination of FELA and machine learning offers a robust and accurate method for seismic foundation reliability assessment, advancing the optimization of seismic foundation design.} }
@inproceedings{10.1145/3658617.3697766, title = {Machine Learning-Based Real-Time Detection of Power Analysis Attacks Using Supply Voltage Comparisons}, booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference}, pages = {440--446}, year = {2025}, isbn = {9798400706356}, doi = {10.1145/3658617.3697766}, url = {https://doi.org/10.1145/3658617.3697766}, author = {Wang, Nan and Liu, Ruichao and Shan, Yufeng and Zhu, Yu and Chen, Song}, keywords = {power analysis attack, real-time detection, power grid, voltage comparator, support vector machine, location = Tokyo, Japan}, abstract = {Modern power analysis attacks (PAAs) pose significant threats to hardware security, and reliably securing integrated systems against advanced PAAs has become a significant design target in integrated circuits. However, the detection accuracy of most countermeasures to PAAs significantly decreases when power side-channel information is mixed with voltage noise. In this paper, a real-time PAA detection technique is proposed to achieve high detection accuracy even with large voltage noise. The voltage drops of certain power grid (PG) nodes caused by PAA are evaluated by a number of voltage comparisons between PG nodes, which compensate for the effects of voltage noise. These voltage comparison results are analyzed by machine learning algorithms, and a linear support vector machine (SVM) model is selected as the PAA detection model. The PAAs on an IBM benchmarked microprocessor are applied to evaluate the detection accuracy, and our proposed PAA detection method achieves 93.11\% accuracy in detecting a resistance of 1 Ω with noise equal to 20\% of Vdd. Furthermore, the power and area overheads (evaluated using a 65-nm CMOS process) of this method are reduced by 68\% and 75\%, respectively, compared to those of the existing machine learning-based PAA detection techniques.} }
@inproceedings{10.1145/3718491.3718655, title = {Demand Analysis and Prediction of shared Bicycle Based on Machine Learning}, booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum}, pages = {1014--1021}, year = {2025}, isbn = {9798400710865}, doi = {10.1145/3718491.3718655}, url = {https://doi.org/10.1145/3718491.3718655}, author = {Sun, Yibo and Ma, Jiaqi and Zhang, Ruizhe and Lyu, Qiongshuai}, keywords = {Bike demand prediction, Ensemble learning, LightGBM, Linear regression, XGBoost}, abstract = {As urban traffic continues to grow at a swift pace and road congestion intensifies, shared bicycles have increasingly caught the public's attention. Utilizing shared bicycles has emerged as a mode of transportation that is not only convenient and eco-friendly but also conducive to health. In this study, we employed two ensemble learning approaches—stacking and weighted averaging—to forecast the demand for urban bike-sharing services. These predictions were based on various optimized combinations of XGBoost, linear regression, and LightGBM models. Specifically, the stacking learning approach integrates models like XGBoost, linear regression, and LightGBM in diverse configurations. Meanwhile, the weighted averaging learning approach allocates distinct weights to models such as XGBoost, linear regression, and LightGBM. For the experimental phase, we utilized a bicycle rental dataset sourced from Washington D.C., USA, covering the period from 2011 to 2012. The evaluation metrics utilized were RMSLE, RMSE, and MAE. Our findings indicate that the ensemble model comprising XGBoost, linear regression, and LightGBM (referred to as XGB-LGB-LR) outperforms models employing alternative strategies across a range of metrics.} }
@inproceedings{10.1145/3659677.3659707, title = {Active Metadata and Machine Learning based Framework for Enhancing Big Data Quality}, booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security}, year = {2024}, isbn = {9798400709296}, doi = {10.1145/3659677.3659707}, url = {https://doi.org/10.1145/3659677.3659707}, author = {Elouataoui, Widad and El Mendili, Saida and Gahi, Youssef}, keywords = {Active metadata, Anomaly correction, Anomaly detection, Big data quality, Machine learning, location = Meknes, AA, Morocco}, abstract = {The advent of big data has ushered in a new era of opportunities across industries, facilitating transformative insights and operational enhancements. However, the inherent challenges of big data, including its voluminous nature, rapid generation pace, and heterogeneous sources, pose serious issues to data quality. Inaccuracies, incompleteness, and inconsistency undermine the integrity and reliability of analytics, necessitating robust solutions for quality assurance. Despite the recognition of these challenges, existing solutions often lack comprehensive and adaptable mechanisms to ensure data quality throughout its lifecycle. To address this gap, this paper proposes a novel framework using active metadata, enriched with machine learning capabilities, to enhance big data quality effectively and intelligently. The framework comprises five key steps, starting with metadata acquisition to provide foundational insights into data characteristics. Subsequent phases involve advanced preprocessing techniques, machine learning-based anomaly detection using acquired metadata, and correction of anomalies using predictive models within appropriate metadata neighborhoods. Based on active metadata and machine learning, the framework automatically identifies and rectifies discrepancies, thereby improving overall data reliability and usability. Experimental validation of the proposed framework using a large dataset demonstrates its efficacy in correcting quality anomalies.} }
@inproceedings{10.1145/3577193.3593710, title = {CMLCompiler: A Unified Compiler for Classical Machine Learning}, booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing}, pages = {63--74}, year = {2023}, isbn = {9798400700569}, doi = {10.1145/3577193.3593710}, url = {https://doi.org/10.1145/3577193.3593710}, author = {Wen, Xu and Gao, Wanling and Li, Anzheng and Wang, Lei and Jiang, Zihan and Zhan, Jianfeng}, keywords = {compiler, deep learning, classical machine learning, location = Orlando, FL, USA}, abstract = {Classical machine learning (CML) occupies nearly half of machine learning pipelines in production applications. Unfortunately, it fails to utilize the state-of-the-practice devices fully and performs poorly. Without a unified framework, the hybrid deployments of deep learning (DL) and CML also suffer from severe performance and portability issues. This paper presents the design of a unified compiler, called CMLCompiler, for CML inference. We propose two unified abstractions: operator representations and extended computational graphs. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to DL compilers or frameworks. We implement CMLCompiler on TVM. The evaluation shows CMLCompiler's portability and superior performance. It achieves up to 4.38 speedup on CPU, 3.31 speedup on GPU, and 5.09 speedup on IoT devices, compared to the state-of-the-art solutions --- scikit-learn, intel sklearn, and hummingbird. Our performance of CML and DL mixed pipelines achieves up to 3.04x speedup compared with cross-framework implementations. The project documents and source code are available at https://www.computercouncil.org/cmlcompiler.} }
@inproceedings{10.1145/3756580.3756612, title = {Nonlinear Mediation of Parental Phubbing on Child Loneliness: SHAP Interpretability and Double Machine Learning Insights}, booktitle = {Proceedings of the 2025 6th International Conference on Education, Knowledge and Information Management}, pages = {196--201}, year = {2025}, isbn = {9798400715624}, doi = {10.1145/3756580.3756612}, url = {https://doi.org/10.1145/3756580.3756612}, author = {Hao, Jiabao and Li, Huijie}, abstract = {Objective Grounded in social-ecological theory and parent-child system theory, this study investigates the mechanism by which parental phubbing (excessive smartphone use during parent-child interactions) influences children's loneliness, with a focus on the chained mediation effects of parent-child cohesion and children's social anxiety. Existing research predominantly employs parallel mediation models, overlooking the sequential transmission between parent-child interactions and social development, while traditional regression analyses fail to capture nonlinear relationships in behavioral data. Methods Utilizing scales for parental phubbing, children's loneliness, parent-child cohesion, and social anxiety, we conducted a questionnaire survey with 300 fourth- to sixth-grade students from a primary school in Shanxi Province. Predictive models were constructed using Gradient Boosted Regression Trees (GBRT) and a double machine learning framework, with variable contributions analyzed via SHAP (Shapley Additive Explanations) values, and chained pathways validated through mediation random forests. Results Parental phubbing exhibited a direct effect on loneliness (β = 0.29, 95\% CI: 0.21–0.37). The chained mediation effects of parent-child cohesion (β = −0.24) and social anxiety (β = 0.18) were significant (total indirect effect = 0.041), with a nonlinear threshold effect: when parent-child cohesion scores fell below 3.2, the mediation effect intensity increased by 1.7-fold. Conclusion This study reveals a nonlinear dose-response relationship between parental smartphone use and children's psychological distress, providing quantitative evidence for targeted interventions.} }
@inproceedings{10.1145/3563766.3564115, title = {Congestion control in machine learning clusters}, booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks}, pages = {235--242}, year = {2022}, isbn = {9781450398992}, doi = {10.1145/3563766.3564115}, url = {https://doi.org/10.1145/3563766.3564115}, author = {Rajasekaran, Sudarsanan and Ghobadi, Manya and Kumar, Gautam and Akella, Aditya}, keywords = {transport layer, resource allocation, networks for ML, datacenters for ML, congestion control, DNN training, location = Austin, Texas}, abstract = {This paper argues that fair-sharing, the holy grail of congestion control algorithms for decades, is not necessarily a desirable property in Machine Learning (ML) training clusters. We demonstrate that for a specific combination of jobs, introducing unfairness improves the training time for all competing jobs. We call this specific combination of jobs compatible and define the compatibility criterion using a novel geometric abstraction. Our abstraction rolls time around a circle and rotates the communication phases of jobs to identify fully compatible jobs. Using this abstraction, we demonstrate up to 1.3 improvement in the average training iteration time of popular ML models. We advocate that resource management algorithms should take job compatibility on network links into account. We then propose three directions to ameliorate the impact of network congestion in ML training clusters: (i) an adaptively unfair congestion control scheme, (ii) priority queues on switches, and (iii) precise flow scheduling.} }
@inproceedings{10.1145/3723178.3723280, title = {Skin Cancer Detection: Leveraging Hybrid Deep Learning Models and Traditional Machine Learning Classifiers}, booktitle = {Proceedings of the 3rd International Conference on Computing Advancements}, pages = {770--778}, year = {2025}, isbn = {9798400713828}, doi = {10.1145/3723178.3723280}, url = {https://doi.org/10.1145/3723178.3723280}, author = {Nusrat Prome, Jannatun and Sultana, Fariha and Anika, Saraf}, keywords = {Deep learning, K-Nearest Neighbors, Machine Learning, MobileNet, Random Forest, ResNet-50, ShuffleNet, Skin Cancer Detection, Support Vector Machine, VGG16, and DenseNet-201}, abstract = {Skin cancer is a significant worldwide health concern, necessitating efficient observation and treatment methods. We explore the potential of deep neural network techniques to automate skin cancer identification, addressing challenges like classification complexity and dataset scarcity. The research makes use of two publicly accessible datasets of skin cancer photographs, featuring 3311 and 5000 photos, respectively. Our approach combines established deep learning models like ResNet-50, VGG16, MobileNet, ShuffleNet and DenseNet-201 with novel hybrid structures to leverage their complementary strengths. The methodology initiated with data pre-processing, encompassing steps such as data splitting, normalization, reshaping, and encoding. Augmentation techniques were subsequently applied to augment the dataset volume. Following pre-processing, traditional deep learning models were independently evaluated from scratch. Subsequently, these models were re-evaluated in conjunction with machine learning classifiers. Hybrid models were then constructed from the individual models and subjected to assessment. Additionally, hybrid models paired with Support Vector Machine (SVM) classifiers were assessed. Throughout the experimentation, the Adam optimizer consistently demonstrated effective performance. Notably, SVM classifiers exhibited superior performance among the classifiers measured. Accuracy-Loss plots, Confusion Matrix, and Receiver Operating Characteristic (ROC) curves were used for a comprehensive analysis. The suggested hybrid model of VGG16 and ResNet-50 with SVM classifier outperformed the conventional DL models and ML classifiers during the anatomizing process with consistent and dependable performance. Accordingly, the suggested model demonstrated accuracy on the corresponding datasets of 90.64\% and 93.61\%, indicating its capability as a tool aimed at skin cancer early detection} }
@inproceedings{10.1145/3756580.3756620, title = {An Empirical Analysis of Factors Influencing the Effectiveness of Legal Education in Chinese Universities Using Machine Learning}, booktitle = {Proceedings of the 2025 6th International Conference on Education, Knowledge and Information Management}, pages = {245--249}, year = {2025}, isbn = {9798400715624}, doi = {10.1145/3756580.3756620}, url = {https://doi.org/10.1145/3756580.3756620}, author = {Yang, Danming and Lv, Tong and Wu, Miaoxian and Wu, Jinhe and Xie, Xinjiu and Wu, Ruiqing}, keywords = {Legal education in universities, SHAP interpretation, XGBoost, learning attitude, legal cognition, machine learning}, abstract = {Legal education is essential for improving students’ legal awareness, yet its effectiveness often falls short, and the key influencing factors remain understudied using quantitative methods. This study uses survey data from university students and combines machine learning with traditional statistical approaches to identify the drivers of legal education outcomes. A questionnaire was designed to assess legal cognition and learning attitudes. Using XGBoost and SHAP values, the model revealed that active participation, teacher-student interaction, and real-life content relevance were the strongest predictors of legal cognition. In contrast, background factors such as being a law major or taking legal courses had minimal impact. These findings were validated through PCA, multiple linear regression, logistic regression, and t-tests. Logistic models struggled with class imbalance, resulting in low AUC (∼0.6), which limits their predictive utility. The results underscore that student engagement and positive learning attitudes are more important than formal background in shaping legal understanding. This study provides empirical evidence to guide improvements in legal education and suggests future research directions, including behavioral data integration, class imbalance solutions, and deep learning applications.} }
@inproceedings{10.1145/3687311.3687398, title = {Study on Exam Paper Generation Methods and Experiments based on Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology}, pages = {483--488}, year = {2024}, isbn = {9798400709920}, doi = {10.1145/3687311.3687398}, url = {https://doi.org/10.1145/3687311.3687398}, author = {Liu, Mingyang}, abstract = {This study aims to address issues in traditional composing a test paper, including subjectivity, uncertainty, and inconsistency in quality, through machine learning techniques. We developed a new method to enhance efficiency and rationality. Our analysis showed the machine learning-based method outperforms manual assembly in knowledge point coverage, test question similarity, and diversity. However, manual assembly excels in differentiation. Our contributions include a reliable evaluation model and a test paper creation system, which were experimentally validated. We aim to further optimize our model and explore effective combinations of machine learning and manual assembly for improved educational support.} }
@inproceedings{10.1145/3600160.3605004, title = {User Acceptance Criteria for Privacy Preserving Machine Learning Techniques}, booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security}, year = {2023}, isbn = {9798400707728}, doi = {10.1145/3600160.3605004}, url = {https://doi.org/10.1145/3600160.3605004}, author = {L\"obner, Sascha and Pape, Sebastian and Bracamonte, Vanessa}, keywords = {Privacy Preserving Machine Learning, Privacy-by-design, User Acceptance, location = Benevento, Italy}, abstract = {Users are confronted with a variety of different machine learning applications in many domains. To make this possible especially for applications relying on sensitive data, companies and developers are implementing Privacy Preserving Machine Learning (PPML) techniques what is already a challenge in itself. This study provides the first step for answering the question how to include the user’s preferences for a PPML technique into the privacy by design process, when developing a new application. The goal is to support developers and AI service providers when choosing a PPML technique that best reflects the users’ preferences. Based on discussions with privacy and PPML experts, we derived a framework that maps the characteristics of PPML to user acceptance criteria.} }
@inproceedings{10.1145/3746972.3746983, title = {Modeling and Forecasting Global GDP Trends Using PCA and Machine Learning: Evidence from 1960 to 2020}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {60--65}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3746983}, url = {https://doi.org/10.1145/3746972.3746983}, author = {Wu, Junxi}, keywords = {China, GDP, Machine Learning, PPP, Principal Component Analysis}, abstract = {This paper analyzes the GDP (PPP) data of more than 200 countries from 1960 to 2020, and explores the economic growth trend and its influencing factors. This study reveals the structural patterns behind GDP data and predicts future economic trends through methods such as data cleaning, PCA reduction and machine learning. The results show that the linear regression model has the best prediction effect on GDP, and its R² is close to 1, indicating that it effectively captures the main trends in the data. The article also analyzed the impact of the COVID-19 pandemic on China's GDP, pointing out that China's GDP grew by 8.1\% in 2021, with GDP (purchasing power parity) increasing by 8.1\%, while in 2022, GDP growth slowed down to 3.0\%, with GDP (purchasing power parity) increasing by 4.9\%. The research emphasizes the significance of GDP forecasting for policy-making and economic analysis, pointing out that GDP forecasting can assist the government in optimizing resource allocation, formulating scientific and reasonable economic policies, and providing references for enterprise risk hedging, supply chain optimization, and market access strategies.} }
@inproceedings{10.1145/3703935.3703979, title = {Research on Printing Icon Defect Detection Based on Variation Model and Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Artificial Intelligence and Pattern Recognition}, pages = {615--622}, year = {2025}, isbn = {9798400717178}, doi = {10.1145/3703935.3703979}, url = {https://doi.org/10.1145/3703935.3703979}, author = {Liu, Jiawei and Li, Yanjin and Qi, Wenjing and Li, Yule and Liu, Jihong}, keywords = {defect detection, machine vision, support vector machine, variation model}, abstract = {A printing icon defect detection method based on variation model and machine learning is proposed to address the shortcomings of defect detection methods for glass bottle printing on industrial production lines. On the basis of using the standard variation model for bottle icon detection, subjective measurement standards are introduced from two perspectives to construct a locally variable variation model, and support vector machine in machine learning is combined to address the false detection problem in the detection process. The experimental results show that compared with existing methods, the research method proposed in this paper can more accurately detect defective bottle icons, and is more subjective and efficient in practical applications.} }
@inproceedings{10.1145/3546918.3546921, title = {Machine-Learning-Based Self-Optimizing Compiler Heuristics✱}, booktitle = {Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes}, pages = {98--111}, year = {2022}, isbn = {9781450396967}, doi = {10.1145/3546918.3546921}, url = {https://doi.org/10.1145/3546918.3546921}, author = {Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M\"ossenb\"ock, Hanspeter}, keywords = {Dynamic Compilation, Heuristics, Loop Peeling, Machine Learning, Neural Networks, Optimization, Performance, location = Brussels, Belgium}, abstract = {Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30\% for several benchmarks and only few slowdowns of up to 7\%.} }
@inproceedings{10.1145/3756423.3756451, title = {Research on the Application of Machine Learning Algorithms in Marketing Data Analysis under the Background of Artificial Intelligence}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing}, pages = {179--184}, year = {2025}, isbn = {9798400714351}, doi = {10.1145/3756423.3756451}, url = {https://doi.org/10.1145/3756423.3756451}, author = {Li, Baohong}, keywords = {Customer, Machine learning, Marketing data}, abstract = {In the digital age, the marketing industry is undergoing profound changes. With the rapid development of Internet technology, consumers' behavioral patterns and shopping habits have undergone tremendous changes. They make decisions based on a vast amount of information, which makes it difficult for traditional marketing methods to precisely reach target customers. Machine learning algorithms, as an important branch in the field of artificial intelligence, possess powerful capabilities in data processing and analysis. It can automatically learn patterns and rules from a large amount of data, thereby achieving the prediction and classification of unknown data. In marketing data analysis, machine learning algorithms can help enterprises discover the potential demands of consumers, accurately identify target customer groups, optimize marketing strategies, and enhance marketing effectiveness and return on investment. This study aims to deeply analyze the application of machine learning algorithms in marketing data analysis and compare the performance and applicable scenarios of different algorithms. Through the analysis of actual cases and empirical research on data, the advantages and potential of machine learning algorithms in improving marketing effectiveness are revealed, providing theoretical support and practical guidance for enterprises to rationally select and apply machine learning algorithms in marketing decisions.} }
@inproceedings{10.1145/3461702.3462611, title = {Quantum Fair Machine Learning}, booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {843--853}, year = {2021}, isbn = {9781450384735}, doi = {10.1145/3461702.3462611}, url = {https://doi.org/10.1145/3461702.3462611}, author = {Perrier, Elija}, keywords = {quantum, machine, learning, fair, location = Virtual Event, USA}, abstract = {In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.} }
@inproceedings{10.1145/3718751.3718847, title = {Momentum on Sports Performance: Machine Learning and Multi-Aspect Analysis}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {602--611}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718847}, url = {https://doi.org/10.1145/3718751.3718847}, author = {Li, Junxiao and Tan, Xin and Yi, Lanfang and Ma, Qianting}, keywords = {IEW-TOPSIS, Momentum, Multilayer Perceptro, correlation coefficient}, abstract = {This study examines the multidimensional influence of momentum on competitive sports performance through a holistic analytical lens. Commencing with the application of the Information Entropy Weight Technique for Order Preference by Similarity to Ideal Solution (IEW-TOPSIS), it meticulously measures momentum by integrating psychological components, scoring trends, and skill indicators. This refined measurement approach enables an exhaustive exploration of athletes' variable performance throughout competitive engagements. Expanding on this groundwork, a Multilayer Perceptron (MLP) model is employed to forecast player performance with notable precision, reinforcing its efficacy as a predictive instrument. The research extends to strategy formulation, leveraging opponents' historical data, athletes' self-reflections, and pivotal momentum transitions to inform tactical strategies, thereby augmenting decision-making acumen. Model generalizability is demonstrated through successful application to additional tennis matches, albeit suggesting calibration with specific match category data for enhanced accuracy. Sensitivity analyses confirm the robustness of the IEW-TOPSIS and MLP models across varied parameters and data distributions, positioning them as valuable tools for coaching and athlete enhancement in diverse competitive scenarios.} }
@inproceedings{10.1145/3646548.3676546, title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning}, booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference}, pages = {82--87}, year = {2024}, isbn = {9798400705939}, doi = {10.1145/3646548.3676546}, url = {https://doi.org/10.1145/3646548.3676546}, author = {G\"uthing, Lukas and Pett, Tobias and Schaefer, Ina}, keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines, location = Dommeldange, Luxembourg}, abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.} }
@inproceedings{10.1145/3534678.3542636, title = {Automated Machine Learning \&amp; Tuning with FLAML}, booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {4828--4829}, year = {2022}, isbn = {9781450393850}, doi = {10.1145/3534678.3542636}, url = {https://doi.org/10.1145/3534678.3542636}, author = {Wang, Chi and Wu, Qingyun and Liu, Xueqing and Quintanilla, Luis}, keywords = {automl, tutorial, location = Washington DC, USA}, abstract = {In this tutorial, we will provide an in-depth and hands-on tutorial on Automated Machine Learning \&amp; Tuning with a fast python library FLAML. We will start with an overview of the AutoML problem and the FLAML library. In the first half of the tutorial, we will then give a hands-on tutorial on how to use FLAML to automate typical machine learning tasks in an end-to-end manner with different customization options and how to perform general tuning tasks on user-defined functions. In the second half of the tutorial, we will introduce several advanced functionalities of the library. For example, zero-shot AutoML, fair AutoML, and online AutoML. We will close the tutorial with several open problems, and challenges learned from AutoML practice.} }
@inproceedings{10.1145/3665601.3669849, title = {On Integrating the Data-Science and Machine-Learning Pipelines for Responsible AI}, booktitle = {Proceedings of the Conference on Governance, Understanding and Integration of Data for Effective and Responsible AI}, pages = {50--53}, year = {2024}, isbn = {9798400706943}, doi = {10.1145/3665601.3669849}, url = {https://doi.org/10.1145/3665601.3669849}, author = {Esmaelizadeh, Armin and Rorseth, Joel and Yu, Andy and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw and Taghva, Kazem}, keywords = {Data Science, Explainable AI, Machine Learning Model Diagnostics, location = Santiago, AA, Chile}, abstract = {Herein, we advocate for the integration of the pipelines for data science (e.g., extraction, cleaning, and exploration) and machine learning (e.g., training data collection, feature selection, model selection, and parameter tuning), toward responsible and trustworthy artificial intelligence. We argue that the metadata generated by the machine-learning pipeline, which includes model outputs and model accuracy scores, is best managed and analyzed using data-science tools, thereby obtaining actionable insights into model performance, interpretability, and bias. We illustrate via two examples from our recent work as proof of concept: data summarization for model performance diagnostics; and input and output exploration to understand retrieval-augmented language models.} }
@inproceedings{10.1145/3675417.3675475, title = {Sailboat price prediction based on statistical and machine learning methods}, booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {349--354}, year = {2024}, isbn = {9798400717147}, doi = {10.1145/3675417.3675475}, url = {https://doi.org/10.1145/3675417.3675475}, author = {Xia, Shengjie}, abstract = {Pricing of sailboats is a topic that many extreme sports enthusiasts like to pay attention to. For the sailboat market, price prediction is crucial and is a key factor in making decisions such as marketing strategies, cost calculation, and profit calculation. By using regression models and machine learning, companies can better grasp market risks, improve product pricing accuracy, and increase their profitability. In addition, this method is very valuable in various industries such as e-commerce, retail, finance, and real estate. This paper aims to establish the relationship between sailboat prices and various factors using typical models based on statistical methods and machine learning, and explore the regional and sailboat type effects on prices, particularly the regional effects of specific areas such as Hong Kong (SAR). By comparing the accuracy of each model, we can obtain some insights into the above issues.} }
@article{10.1145/3759242, title = {Load Balancing in the Internet of Vehicles: A Comprehensive Review of SDN and Machine Learning Approaches}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3759242}, url = {https://doi.org/10.1145/3759242}, author = {Marwein, Phibadeity S. and Kandar, Debdatta}, keywords = {WSN-LB, IoT-LB, UAV-LB, IoV-LB, SDN-based LB, ML-based LB, mmWave, THz}, abstract = {Efficient load balancing (LB) is crucial for optimizing network performance in Wireless Sensor Networks (WSN), the Internet of Things (IoT), and Unmanned Aerial Vehicles (UAV), as well as the emerging Internet of Vehicles (IoV). In this article, we study various LB techniques across these domains, including Software-Defined Networking (SDN) and Machine Learning (ML)-based approaches. SDN enables centralized control and real-time adaptability, while ML enhances decision-making through predictive analytics. Given the limited research on IoV, we leverage insights from WSN, IoT, and UAVs to propose an innovative technique that integrates SDN with ML for intelligent, adaptive LB in IoV. This approach promises to optimize network performance, reduce latency, and improve fault tolerance, offering a new research direction in vehicular networks.} }
@article{WOS:000358218600040, title = {Machine learning: Trends, perspectives, and prospects}, journal = {SCIENCE}, volume = {349}, pages = {255-260}, year = {2015}, issn = {0036-8075}, doi = {10.1126/science.aaa8415}, author = {Jordan, M. I. and Mitchell, T. M.}, abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.} }
@article{WOS:000439850800044, title = {Machine learning for molecular and materials science}, journal = {NATURE}, volume = {559}, pages = {547-555}, year = {2018}, issn = {0028-0836}, doi = {10.1038/s41586-018-0337-2}, author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron}, abstract = {Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.} }
@article{WOS:001172157700001, title = {A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Human Action Recognition in Machine and Robotic Vision}, journal = {TECHNOLOGIES}, volume = {12}, year = {2024}, doi = {10.3390/technologies12020015}, author = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and Fragulis, George F.}, abstract = {Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications.} }
@article{WOS:001296591100001, title = {Pushing the frontiers in climate modelling and analysis with machine learning}, journal = {NATURE CLIMATE CHANGE}, year = {2024}, issn = {1758-678X}, doi = {10.1038/s41558-024-02095-y}, author = {Eyring, Veronika and Collins, William D. and Gentine, Pierre and Barnes, Elizabeth A. and Barreiro, Marcelo and Beucler, Tom and Bocquet, Marc and Bretherton, Christopher S. and Christensen, Hannah M. and Dagon, Katherine and Gagne, David John and Hall, David and Hammerling, Dorit and Hoyer, Stephan and Iglesias-Suarez, Fernando and Lopez-Gomez, Ignacio and Mcgraw, Marie C. and Meehl, Gerald A. and Molina, Maria J. and Monteleoni, Claire and Mueller, Juliane and Pritchard, Michael S. and Rolnick, David and Runge, Jakob and Stier, Philip and Watt-Meyer, Oliver and Weigel, Katja and Yu, Rose and Zanna, Laure}, abstract = {Climate modelling and analysis are facing new demands to enhance projections and climate information. Here we argue that now is the time to push the frontiers of machine learning beyond state-of-the-art approaches, not only by developing machine-learning-based Earth system models with greater fidelity, but also by providing new capabilities through emulators for extreme event projections with large ensembles, enhanced detection and attribution methods for extreme events, and advanced climate model analysis and benchmarking. Utilizing this potential requires key machine learning challenges to be addressed, in particular generalization, uncertainty quantification, explainable artificial intelligence and causality. This interdisciplinary effort requires bringing together machine learning and climate scientists, while also leveraging the private sector, to accelerate progress towards actionable climate science. Machine learning methods allow for advances in many aspects of climate research. In this Perspective, the authors give an overview of recent progress and remaining challenges to harvest the full potential of machine learning methods.} }
@article{WOS:001203939100001, title = {Machine learning for battery systems applications: Progress, challenges, and opportunities}, journal = {JOURNAL OF POWER SOURCES}, volume = {601}, year = {2024}, issn = {0378-7753}, doi = {10.1016/j.jpowsour.2024.234272}, author = {Nozarijouybari, Zahra and Fathy, Hosam K.}, abstract = {Machine learning has emerged as a transformative force throughout the entire engineering life cycle of electrochemical batteries. Its applications encompass a wide array of critical domains, including material discovery, model development, quality control during manufacturing, real-time monitoring, state estimation, optimization of charge cycles, fault detection, and life cycle management. Machine learning excels in its ability to identify and capture complex behavioral trends in batteries, which may be challenging to model using more traditional methods. The goal of this survey paper is to synthesize the rich existing literature on battery machine learning into a structured perspective on the successes, challenges, and prospects within this research domain. This critical examination highlights several key insights. Firstly, the selection of data sets, features, and algorithms significantly influences the success of machine learning applications, yet it remains an open research area with vast potential. Secondly, data set richness and size are both pivotal for the efficacy of machine learning algorithms, suggesting a potential for active machine learning techniques in the battery systems domain. Lastly, the field of machine learning in battery systems has extensive room for growth, moving beyond its current focus on specific applications like state of charge (SOC) and state of health (SOH) estimation, offering ample opportunities for innovation and expansion.} }
@article{WOS:001507295200001, title = {A Review of Machine Learning Algorithms for Biomedical Applications}, journal = {ANNALS OF BIOMEDICAL ENGINEERING}, volume = {52}, pages = {1159-1183}, year = {2024}, issn = {0090-6964}, doi = {10.1007/s10439-024-03459-3}, author = {Binson, V. A. and Thomas, Sania and Subramoniam, M. and Arun, J. and Naveen, S. and Madhu, S.}, abstract = {As the amount and complexity of biomedical data continue to increase, machine learning methods are becoming a popular tool in creating prediction models for the underlying biomedical processes. Although all machine learning methods aim to fit models to data, the methodologies used can vary greatly and may seem daunting at first. A comprehensive review of various machine learning algorithms per biomedical applications is presented. The key concepts of machine learning are supervised and unsupervised learning, feature selection, and evaluation metrics. Technical insights on the major machine learning methods such as decision trees, random forests, support vector machines, and k-nearest neighbors are analyzed. Next, the dimensionality reduction methods like principal component analysis and t-distributed stochastic neighbor embedding methods, and their applications in biomedical data analysis were reviewed. Moreover, in biomedical applications predominantly feedforward neural networks, convolutional neural networks, and recurrent neural networks are utilized. In addition, the identification of emerging directions in machine learning methodology will serve as a useful reference for individuals involved in biomedical research, clinical practice, and related professions who are interested in understanding and applying machine learning algorithms in their research or practice.} }
@article{WOS:001064661300018, title = {Machine learning applications in stroke medicine: advancements, challenges, and future prospectives}, journal = {NEURAL REGENERATION RESEARCH}, volume = {19}, pages = {769-773}, year = {2024}, issn = {1673-5374}, doi = {10.4103/1673-5374.382228}, author = {Daidone, Mario and Ferrantelli, Sergio and Tuttolomondo, Antonino}, abstract = {Stroke is a leading cause of disability and mortality worldwide, necessitating the development of advanced technologies to improve its diagnosis, treatment, and patient outcomes. In recent years, machine learning techniques have emerged as promising tools in stroke medicine, enabling efficient analysis of large-scale datasets and facilitating personalized and precision medicine approaches. This abstract provides a comprehensive overview of machine learning's applications, challenges, and future directions in stroke medicine. Recently introduced machine learning algorithms have been extensively employed in all the fields of stroke medicine. Machine learning models have demonstrated remarkable accuracy in imaging analysis, diagnosing stroke subtypes, risk stratifications, guiding medical treatment, and predicting patient prognosis. Despite the tremendous potential of machine learning in stroke medicine, several challenges must be addressed. These include the need for standardized and interoperable data collection, robust model validation and generalization, and the ethical considerations surrounding privacy and bias. In addition, integrating machine learning models into clinical workflows and establishing regulatory frameworks are critical for ensuring their widespread adoption and impact in routine stroke care. Machine learning promises to revolutionize stroke medicine by enabling precise diagnosis, tailored treatment selection, and improved prognostication. Continued research and collaboration among clinicians, researchers, and technologists are essential for overcoming challenges and realizing the full potential of machine learning in stroke care, ultimately leading to enhanced patient outcomes and quality of life. This review aims to summarize all the current implications of machine learning in stroke diagnosis, treatment, and prognostic evaluation. At the same time, another purpose of this paper is to explore all the future perspectives these techniques can provide in combating this disabling disease.} }
@article{WOS:001219213300001, title = {Machine learning in construction and demolition waste management: Progress, challenges, and future directions}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {162}, year = {2024}, issn = {0926-5805}, doi = {10.1016/j.autcon.2024.105380}, author = {Gao, Yu and Wang, Jiayuan and Xu, Xiaoxiao}, abstract = {The application of machine learning contributes to intelligent and efficient management of construction and demolition waste, leading to a reduction in waste generation and an increased emphasis on recycling. This research conducts a comprehensive analysis of 98 journals related to the application of machine learning in construction waste management from 2012 to 2023 to identify current hot topics and emerging trends. The results reveal that machine learning is applied in four main areas and 15 subfields, specifically focusing on construction and demolition waste generation, on-site handling, transportation, and disposal. Various models, such as artificial neural networks, deep learning, convolutional neural networks, and support vector machines, demonstrate their effectiveness in different processes of construction and demolition waste management. The findings of this research will aid researchers in gaining a comprehensive understanding of the current state and future directions of machine learning in construction waste management.} }
@article{WOS:001137399100001, title = {Machine Learning to Solve Vehicle Routing Problems: A Survey}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {25}, pages = {4754-4772}, year = {2024}, issn = {1524-9050}, doi = {10.1109/TITS.2023.3334976}, author = {Bogyrbayeva, Aigerim and Meraliyev, Meraryslan and Mustakhov, Taukekhan and Dauletbayev, Bissenbay}, abstract = {This paper provides a systematic overview of machine learning methods applied to solve NP-hard Vehicle Routing Problems (VRPs). Recently, there has been great interest from both the machine learning and operations research communities in solving VRPs either through pure learning methods or by combining them with traditional handcrafted heuristics. We present a taxonomy of studies on learning paradigms, solution structures, underlying models, and algorithms. Detailed results of state-of-the-art methods are presented, demonstrating their competitiveness with traditional approaches. The survey highlights the advantages of the machine learning-based models that aim to exploit the symmetry of VRP solutions. The paper outlines future research directions to incorporate learning-based solutions to address the challenges of modern transportation systems.} }
@article{WOS:001310661500002, title = {Cardiovascular disease diagnosis: a holistic approach using the integration of machine learning and deep learning models}, journal = {EUROPEAN JOURNAL OF MEDICAL RESEARCH}, volume = {29}, year = {2024}, issn = {0949-2321}, doi = {10.1186/s40001-024-02044-7}, author = {Sadr, Hossein and Salari, Arsalan and Ashoobi, Mohammad Taghi and Nazari, Mojdeh}, abstract = {BackgroundThe incidence and mortality rates of cardiovascular disease worldwide are a major concern in the healthcare industry. Precise prediction of cardiovascular disease is essential, and the use of machine learning and deep learning can aid in decision-making and enhance predictive abilities.ObjectiveThe goal of this paper is to introduce a model for precise cardiovascular disease prediction by combining machine learning and deep learning.MethodTwo public heart disease classification datasets with 70,000 and 1190 records besides a locally collected dataset with 600 records were used in our experiments. Then, a model which makes use of both machine learning and deep learning was proposed in this paper. The proposed model employed CNN and LSTM, as the representatives of deep learning models, besides KNN and XGB, as the representatives of machine learning models. As each classifier defined the output classes, majority voting was then used as an ensemble learner to predict the final output class.ResultThe proposed model obtained the highest classification performance based on all evaluation metrics on all datasets, demonstrating its suitability and reliability in forecasting the probability of cardiovascular disease.} }
@article{WOS:001263598700003, title = {Fuzzy Machine Learning: A Comprehensive Framework and Systematic Review}, journal = {IEEE TRANSACTIONS ON FUZZY SYSTEMS}, volume = {32}, pages = {3861-3878}, year = {2024}, issn = {1063-6706}, doi = {10.1109/TFUZZ.2024.3387429}, author = {Lu, Jie and Ma, Guangzhi and Zhang, Guangquan}, abstract = {Machine learning draws its power from various disciplines, including computer science, cognitive science, and statistics. Although machine learning has achieved great advancements in both theory and practice, its methods have some limitations when dealing with complex situations and highly uncertain environments. Insufficient data, imprecise observations, and ambiguous information/relationships can all confound traditional machine learning systems. To address these problems, researchers have integrated machine learning from different aspects and fuzzy techniques, including fuzzy sets, fuzzy systems, fuzzy logic, fuzzy measures, fuzzy relations, and so on. This article presents a systematic review of fuzzy machine learning, from theory, approach to application, with the overall objective of providing an overview of recent achievements in the field of fuzzy machine learning. To this end, the concepts and frameworks discussed are divided into five categories: 1) fuzzy classical machine learning; 2) fuzzy transfer learning; 3) fuzzy data stream learning; 4) fuzzy reinforcement learning; and 5) fuzzy recommender systems. The literature presented should provide researchers with a solid understanding of the current progress in fuzzy machine learning research and its applications.} }
@article{WOS:001166743400001, title = {Machine learning assisted biosensing technology: An emerging powerful tool for improving the intelligence of food safety detection}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {8}, year = {2024}, doi = {10.1016/j.crfs.2024.100679}, author = {Zhou, Zixuan and Tian, Daoming and Yang, Yingao and Cui, Han and Li, Yanchun and Ren, Shuyue and Han, Tie and Gao, Zhixian}, abstract = {Recently, the application of biosensors in food safety assessment has gained considerable research attention. Nevertheless, the evaluation of biosensors' sensitivity, accuracy, and efficiency is still ongoing. The advent of machine learning has enhanced the application of biosensors in food security assessment, yielding improved results. Machine learning has been preliminarily applied in combination with different biosensors in food safety assessment, with positive results. This review offers a comprehensive summary of the diverse machine learning methods employed in biosensors for food safety. Initially, the primary machine learning methods were outlined, and the integrated application of biosensors and machine learning in food safety was thoroughly examined. Lastly, the challenges and limitations of machine learning and biosensors in the realm of food safety were underscored, and potential solutions were explored. The review's findings demonstrated that algorithms grounded in machine learning can aid in the early detection of food safety issues. Furthermore, preliminary research suggests that biosensors could be optimized through machine learning for real-time, multifaceted analyses of food safety variables and their interactions. The potential of machine learning and biosensors in realtime monitoring of food quality has been discussed.} }
@article{WOS:001155473600001, title = {Expanding the Horizons of Machine Learning in Nanomaterials to Chiral Nanostructures}, journal = {ADVANCED MATERIALS}, volume = {36}, year = {2024}, issn = {0935-9648}, doi = {10.1002/adma.202308912}, author = {Kuznetsova, Vera and Coogan, Aine and Botov, Dmitry and Gromova, Yulia and Ushakova, Elena V. and Gun'ko, Yurii K.}, abstract = {Machine learning holds significant research potential in the field of nanotechnology, enabling nanomaterial structure and property predictions, facilitating materials design and discovery, and reducing the need for time-consuming and labor-intensive experiments and simulations. In contrast to their achiral counterparts, the application of machine learning for chiral nanomaterials is still in its infancy, with a limited number of publications to date. This is despite the great potential of machine learning to advance the development of new sustainable chiral materials with high values of optical activity, circularly polarized luminescence, and enantioselectivity, as well as for the analysis of structural chirality by electron microscopy. In this review, an analysis of machine learning methods used for studying achiral nanomaterials is provided, subsequently offering guidance on adapting and extending this work to chiral nanomaterials. An overview of chiral nanomaterials within the framework of synthesis-structure-property-application relationships is presented and insights on how to leverage machine learning for the study of these highly complex relationships are provided. Some key recent publications are reviewed and discussed on the application of machine learning for chiral nanomaterials. Finally, the review captures the key achievements, ongoing challenges, and the prospective outlook for this very important research field. This review analyzes machine learning methods for studying achiral nanomaterials and offers guidance for adapting and extending this work to chiral nanomaterials. An overview of chiral nanomaterials in the context of synthesis-structure-property-application relationships is presented, offering insights on leveraging machine learning for these complex relationships. Key achievements, challenges, and outlook for machine learning in chiral nanomaterials research are discussed. image} }
@article{WOS:001156411300001, title = {The role of hyperparameters in machine learning models and how to tune them}, journal = {POLITICAL SCIENCE RESEARCH AND METHODS}, volume = {12}, pages = {841-848}, year = {2024}, issn = {2049-8470}, doi = {10.1017/psrm.2023.61}, author = {Arnold, Christian and Biedebach, Luka and Kuepfer, Andreas and Neunhoeffer, Marcel}, abstract = {Hyperparameters critically influence how well machine learning models perform on unseen, out-of-sample data. Systematically comparing the performance of different hyperparameter settings will often go a long way in building confidence about a model's performance. However, analyzing 64 machine learning related manuscripts published in three leading political science journals (APSR, PA, and PSRM) between 2016 and 2021, we find that only 13 publications (20.31 percent) report the hyperparameters and also how they tuned them in either the paper or the appendix. We illustrate the dangers of cursory attention to model and tuning transparency in comparing machine learning models' capability to predict electoral violence from tweets. The tuning of hyperparameters and their documentation should become a standard component of robustness checks for machine learning models.} }
@article{WOS:001335131500001, title = {Avoiding common machine learning pitfalls}, journal = {PATTERNS}, volume = {5}, year = {2024}, issn = {2666-3899}, doi = {10.1016/j.patter.2024.101046}, author = {Lones, Michael A.}, abstract = {Mistakes in machine learning practice are commonplace and can result in loss of confidence in the findings and products of machine learning. This tutorial outlines common mistakes that occur when using machine learning and what can be done to avoid them. While it should be accessible to anyone with a basic understanding of machine learning techniques, it focuses on issues that are of particular concern within academic research, such as the need to make rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.} }
@article{WOS:001230185600011, title = {In-Network Machine Learning Using Programmable Network Devices: A Survey}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {26}, pages = {1171-1200}, year = {2024}, doi = {10.1109/COMST.2023.3344351}, author = {Zheng, Changgang and Hong, Xinpeng and Ding, Damu and Vargaftik, Shay and Ben-Itzhak, Yaniv and Zilberman, Noa}, abstract = {Machine learning is widely used to solve networking challenges, ranging from traffic classification and anomaly detection to network configuration. However, machine learning also requires significant processing and often increases the load on both networks and servers. The introduction of in-network computing, enabled by programmable network devices, has allowed to run applications within the network, providing higher throughput and lower latency. Soon after, in-network machine learning solutions started to emerge, enabling machine learning functionality within the network itself. This survey introduces the concept of in-network machine learning and provides a comprehensive taxonomy. The survey provides an introduction to the technology and explains the different types of machine learning solutions built upon programmable network devices. It explores the different types of machine learning models implemented within the network, and discusses related challenges and solutions. In-network machine learning can significantly benefit cloud computing and next-generation networks, and this survey concludes with a discussion of future trends.} }
@article{WOS:001359921300001, title = {A roadmap to fault diagnosis of industrial machines via machine learning: A brief review}, journal = {MEASUREMENT}, volume = {242}, year = {2025}, issn = {0263-2241}, doi = {10.1016/j.measurement.2024.116216}, author = {Vashishtha, Govind and Chauhan, Sumika and Sehri, Mert and Zimroz, Radoslaw and Dumond, Patrick and Kumar, Rajesh and Gupta, Munish Kumar}, abstract = {In fault diagnosis, machine learning theories are gaining popularity as they proved to be an efficient tool that not only reduces human effort but also identifies the health conditions of the machines automatically. In this work, an attempt has been made to systematically review the progress of machine learning theories in fault diagnosis from scratch to future perspectives. Initially, artificial intelligence came into the picture which started to weaken the human effort whose efficiency relies on feature extraction which depends on expert knowledge. The introduction of deep learning theories has reformed the fault diagnosis process by realising the artificial aid, encouraging end-to-end encryption in the diagnostic procedure. The deep learning theories have also filled the gap between the large amount of monitoring data and the health conditions of industrial machines. The future of deep learning theories i.e. transfer learning which uses the knowledge of one domain to another related domain during fault diagnosis has been reviewed. In last, the research trends of the machine learning theories have been briefly discussed along with their challenges in fault diagnostics.} }
@article{WOS:001334283900001, title = {A comprehensive review of quantum machine learning: from NISQ to fault tolerance}, journal = {REPORTS ON PROGRESS IN PHYSICS}, volume = {87}, year = {2024}, issn = {0034-4885}, doi = {10.1088/1361-6633/ad7f69}, author = {Wang, Yunfei and Liu, Junyu}, abstract = {Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.} }
@article{WOS:001166777200003, title = {Machine learning in cartography}, journal = {CARTOGRAPHY AND GEOGRAPHIC INFORMATION SCIENCE}, volume = {51}, pages = {1-19}, year = {2024}, issn = {1523-0406}, doi = {10.1080/15230406.2023.2295948}, author = {Harrie, Lars and Touya, Guillaume and Oucheikh, Rachid and Ai, Tinghua and Courtial, Azelle and Richter, Kai-Florian}, abstract = {Machine learning is increasingly used as a computing paradigm in cartographic research. In this extended editorial, we provide some background of the papers in the CaGIS special issue Machine Learning in Cartography with a special focus on pattern recognition in maps, cartographic generalization, style transfer, and map labeling. In addition, the paper includes a discussion about map encodings for machine learning applications and the possible need for explicit cartographic knowledge and procedural modeling in cartographic machine learning models.} }
@article{WOS:001331425700001, title = {Nanofluid heat transfer and machine learning: Insightful review of machine learning for nanofluid heat transfer enhancement in porous media and heat exchangers as sustainable and renewable energy solutions}, journal = {RESULTS IN ENGINEERING}, volume = {24}, year = {2024}, issn = {2590-1230}, doi = {10.1016/j.rineng.2024.103002}, author = {Riyadi, Tri W. B. and Herawan, Safarudin G. and Tirta, Andy and Ee, Yit Jing and Hananto, April Lia and Paristiawan, Permana A. and Yusuf, Abdulfatah Abdu and Venu, Harish and Irianto and Veza, Ibham}, abstract = {Nanofluid, coupled with machine learning, is at the forefront of cutting-edge research in sustainable and renewable energy sector. This review paper examines the latest developments in the intersection of nanofluid and machine learning for heat transfer enhancement. This hybrid nanofluid-machine learning review investigates nanofluid heat transfer enhancement leveraged by machine learning both in porous media as well as heat exchangers. Several studies in porous media nanofluid transport utilize advanced methodologies that integrate machine learning and computational techniques. Machine learning and computational methods are employed to tackle complex thermodynamics, transport processes, and heat transfer challenges in complex multiphysics systems. An interesting hybrid nanofluid-machine learning application involves applying a machine learning method such as Support Vector Machine (SVM) to forecast movement of hybrid nanofluid flows across porous surfaces. Such hybrid nanofluid-machine learning technique involves utilising training data obtained from computational fluid dynamics (CFD) to decrease computational time and expenses. Machine learning offers a more efficient and cost-effective modelling for nanofluid heat transfer enhancement. Techniques such as scanning electron microscopy (SEM) along with X-ray diffraction (XRD) are also often used for assessing the forms as well as nanocomposites configurations in heat exchangers while studying nanofluids. The importance of machine learning models, especially artificial neural networks (ANNs) and genetic algorithms, is evident in their ability to predict and optimize thermal performance of nanofluid application for nanofluid heat transfer enhancement. Furthermore, integrating nanofluids into various heat exchanger designs has demonstrated significant enhancements in efficiency, decreased energy usage, and total cost reduction. These achievements align with the research goal in sustainable and renewable energy, highlighting the critical role of nanofluid-enhanced heat exchange systems in tackling current difficulties related to energy efficiency and sustainability. Overall, combining nanofluids with machine learning shows promising advancements, providing a route toward creating more efficient and eco-friendly heat exchange systems.} }
@article{WOS:001225931500001, title = {Research trends in deep learning and machine learning for cloud computing security}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {57}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-10776-5}, author = {Alzoubi, Yehia Ibrahim and Mishra, Alok and Topcu, Ahmet Ercan}, abstract = {Deep learning and machine learning show effectiveness in identifying and addressing cloud security threats. Despite the large number of articles published in this field, there remains a dearth of comprehensive reviews that synthesize the techniques, trends, and challenges of using deep learning and machine learning for cloud computing security. Accordingly, this paper aims to provide the most updated statistics on the development and research in cloud computing security utilizing deep learning and machine learning. Up to the middle of December 2023, 4051 publications were identified after we searched the Scopus database. This paper highlights key trend solutions for cloud computing security utilizing machine learning and deep learning, such as anomaly detection, security automation, and emerging technology's role. However, challenges such as data privacy, scalability, and explainability, among others, are also identified as challenges of using machine learning and deep learning for cloud security. The findings of this paper reveal that deep learning and machine learning for cloud computing security are emerging research areas. Future research directions may include addressing these challenges when utilizing machine learning and deep learning for cloud security. Additionally, exploring the development of algorithms and techniques that comply with relevant laws and regulations is essential for effective implementation in this domain.} }
@article{WOS:001230996600003, title = {ddml: Double/debiased machine learning in Stata}, journal = {STATA JOURNAL}, volume = {24}, pages = {3-45}, year = {2024}, issn = {1536-867X}, doi = {10.1177/1536867X241233641}, author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E. and Wiemann, Thomas}, abstract = {In this article, we introduce a package, ddml, for double/debiased machine learning in Stata. Estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in Stata. We recommend using double/debiased machine learning in combination with stacking estimation, which combines multiple machine learners into a final predictor. We provide Monte Carlo evidence to support our recommendation.} }
@article{WOS:001167870100001, title = {DoubleML: An Object-Oriented Implementation of Double Machine Learning in R}, journal = {JOURNAL OF STATISTICAL SOFTWARE}, volume = {108}, year = {2024}, issn = {1548-7660}, doi = {10.18637/jss.v108.i03}, author = {Bach, Philipp and Kurz, Malte S. and Chernozhukov, Victor and Spindler, Martin and Klaassen, Sven}, abstract = {The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consists of three key ingredients: Neyman orthogonality, high -quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object -oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods.} }
@article{WOS:001206759600001, title = {Active Machine Learning for Chemical Engineers: A Bright Future Lies Ahead!}, journal = {ENGINEERING}, volume = {27}, pages = {23-30}, year = {2023}, issn = {2095-8099}, doi = {10.1016/j.eng.2023.02.019}, author = {Ureel, Yannick and Dobbelaere, Maarten R. and Ouyang, Yi and De Ras, Kevin and Sabbe, Maarten K. and Marin, Guy B. and Van Geem, Kevin M.}, abstract = {By combining machine learning with the design of experiments, thereby achieving so-called active machine learning, more efficient and cheaper research can be conducted. Machine learning algorithms are more flexible and are better than traditional design of experiment algorithms at investigating processes spanning all length scales of chemical engineering. While active machine learning algorithms are maturing, their applications are falling behind. In this article, three types of challenges presented by active machine learning-namely, convincing the experimental researcher, the flexibility of data creation, and the robustness of active machine learning algorithms-are identified, and ways to overcome them are discussed. A bright future lies ahead for active machine learning in chemical engineering, thanks to increasing automation and more efficient algorithms that can drive novel discoveries. (c) 2023 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:001139747600005, title = {Advancements in machine learning for material design and process optimization in the field of additive manufacturing}, journal = {CHINA FOUNDRY}, volume = {21}, pages = {101-115}, year = {2024}, issn = {1672-6421}, doi = {10.1007/s41230-024-3145-3}, author = {Zhou, Hao-ran and Yang, Hao and Li, Huai-qian and Ma, Ying-chun and Yu, Sen and Shi, Jian and Cheng, Jing-chang and Gao, Peng and Yu, Bo and Miao, Zhi-quan and Wei, Yan-peng}, abstract = {Additive manufacturing technology is highly regarded due to its advantages, such as high precision and the ability to address complex geometric challenges. However, the development of additive manufacturing process is constrained by issues like unclear fundamental principles, complex experimental cycles, and high costs. Machine learning, as a novel artificial intelligence technology, has the potential to deeply engage in the development of additive manufacturing process, assisting engineers in learning and developing new techniques. This paper provides a comprehensive overview of the research and applications of machine learning in the field of additive manufacturing, particularly in model design and process development. Firstly, it introduces the background and significance of machine learning-assisted design in additive manufacturing process. It then further delves into the application of machine learning in additive manufacturing, focusing on model design and process guidance. Finally, it concludes by summarizing and forecasting the development trends of machine learning technology in the field of additive manufacturing.} }
@article{WOS:000611065800001, title = {Explainable AI: A Review of Machine Learning Interpretability Methods}, journal = {ENTROPY}, volume = {23}, year = {2021}, doi = {10.3390/e23010018}, author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris}, abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.} }
@article{WOS:000895445500045, title = {Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {35}, pages = {614-633}, year = {2023}, issn = {1041-4347}, doi = {10.1109/TKDE.2021.3079836}, author = {von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Michal and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis}, abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.} }
@article{WOS:000957059700001, title = {Small data machine learning in materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {9}, year = {2023}, doi = {10.1038/s41524-023-01000-z}, author = {Xu, Pengcheng and Ji, Xiaobo and Li, Minjie and Lu, Wencong}, abstract = {This review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by small data. Then, the workflow of materials machine learning has been introduced. Next, the methods of dealing with small data were introduced, including data extraction from publications, materials database construction, high-throughput computations and experiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level; active learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machine learning in materials science were proposed.} }
@article{WOS:001171070000005, title = {Machine Learning and Health Science Research: Tutorial}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {26}, year = {2024}, issn = {1438-8871}, doi = {10.2196/50890}, author = {Cho, Hunyong and She, Jane and De Marchi, Daniel and El-Zaatari, Helal and Barnes, Edward L. and Kahkoska, Anna R. and Kosorok, Michael R. and Virkud, Arti V.}, abstract = {Machine learning (ML) has seen impressive growth in health science research due to its capacity for handling complex data to perform a range of tasks, including unsupervised learning, supervised learning, and reinforcement learning. To aid health science researchers in understanding the strengths and limitations of ML and to facilitate its integration into their studies, we present here a guideline for integrating ML into an analysis through a structured framework, covering steps from framing a research question to study design and analysis techniques for specialized data types.} }
@article{WOS:000893245700006, title = {Challenges in Deploying Machine Learning: A Survey of Case Studies}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3533378}, author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.}, abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.} }
@article{WOS:001028153700001, title = {Interpretable machine learning for building energy management: A state-of-the-art review}, journal = {ADVANCES IN APPLIED ENERGY}, volume = {9}, year = {2023}, issn = {2666-7924}, doi = {10.1016/j.adapen.2023.100123}, author = {Chen, Zhe and Xiao, Fu and Guo, Fangzhou and Yan, Jinyue}, abstract = {Machine learning has been widely adopted for improving building energy efficiency and flexibility in the past decade owing to the ever-increasing availability of massive building operational data. However, it is challenging for end-users to understand and trust machine learning models because of their black-box nature. To this end, the interpretability of machine learning models has attracted increasing attention in recent studies because it helps users understand the decisions made by these models. This article reviews previous studies that adopted interpretable machine learning techniques for building energy management to analyze how model interpretability is improved. First, the studies are categorized according to the application stages of interpretable machine learning techniques: ante-hoc and post-hoc approaches. Then, the studies are analyzed in detail according to specific techniques with critical comparisons. Through the review, we find that the broad application of interpretable machine learning in building energy management faces the following significant challenges: (1) different terminologies are used to describe model interpretability which could cause confusion, (2) performance of interpretable ML in different tasks is difficult to compare, and (3) current prevalent techniques such as SHAP and LIME can only provide limited interpretability. Finally, we discuss the future R \\& D needs for improving the interpretability of black-box models that could be significant to accelerate the application of machine learning for building energy management.} }
@article{WOS:001194621500001, title = {Machine learning with a reject option: a survey}, journal = {MACHINE LEARNING}, volume = {113}, pages = {3073-3110}, year = {2024}, issn = {0885-6125}, doi = {10.1007/s10994-024-06534-x}, author = {Hendrickx, Kilian and Perini, Lorenzo and van der Plas, Dries and Meert, Wannes and Davis, Jesse}, abstract = {Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.} }
@article{WOS:000298103200003, title = {Scikit-learn: Machine Learning in Python}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {12}, pages = {2825-2830}, year = {2011}, issn = {1532-4435}, author = {Pedregosa, Fabian and Varoquaux, Gaeel and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard}, abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.} }
@article{WOS:001370097700001, title = {The rise of scientific machine learning: a perspective on combining mechanistic modelling with machine learning for systems biology}, journal = {FRONTIERS IN SYSTEMS BIOLOGY}, volume = {4}, year = {2024}, doi = {10.3389/fsysb.2024.1407994}, author = {Noordijk, Ben and Gomez, Monica L. Garcia and ten Tusscher, Kirsten H. W. J. and de Ridder, Dick and van Dijk, Aalt D. J. and Smith, Robert W.}, abstract = {Both machine learning and mechanistic modelling approaches have been used independently with great success in systems biology. Machine learning excels in deriving statistical relationships and quantitative prediction from data, while mechanistic modelling is a powerful approach to capture knowledge and infer causal mechanisms underpinning biological phenomena. Importantly, the strengths of one are the weaknesses of the other, which suggests that substantial gains can be made by combining machine learning with mechanistic modelling, a field referred to as Scientific Machine Learning (SciML). In this review we discuss recent advances in combining these two approaches for systems biology, and point out future avenues for its application in the biological sciences.} }
@article{WOS:000987798300001, title = {A Comprehensive Review on Machine Learning in Healthcare Industry: Classification, Restrictions, Opportunities and Challenges}, journal = {SENSORS}, volume = {23}, year = {2023}, doi = {10.3390/s23094178}, author = {An, Qi and Rahman, Saifur and Zhou, Jingwen and Kang, James Jin}, abstract = {Recently, various sophisticated methods, including machine learning and artificial intelligence, have been employed to examine health-related data. Medical professionals are acquiring enhanced diagnostic and treatment abilities by utilizing machine learning applications in the healthcare domain. Medical data have been used by many researchers to detect diseases and identify patterns. In the current literature, there are very few studies that address machine learning algorithms to improve healthcare data accuracy and efficiency. We examined the effectiveness of machine learning algorithms in improving time series healthcare metrics for heart rate data transmission (accuracy and efficiency). In this paper, we reviewed several machine learning algorithms in healthcare applications. After a comprehensive overview and investigation of supervised and unsupervised machine learning algorithms, we also demonstrated time series tasks based on past values (along with reviewing their feasibility for both small and large datasets).} }
@article{WOS:001086797900003, title = {Machine Learning-Assisted Low-Dimensional Electrocatalysts Design for Hydrogen Evolution Reaction}, journal = {NANO-MICRO LETTERS}, volume = {15}, year = {2023}, issn = {2311-6706}, doi = {10.1007/s40820-023-01192-5}, author = {Li, Jin and Wu, Naiteng and Zhang, Jian and Wu, Hong-Hui and Pan, Kunming and Wang, Yingxue and Liu, Guilong and Liu, Xianming and Yao, Zhenpeng and Zhang, Qiaobao}, abstract = {Efficient electrocatalysts are crucial for hydrogen generation from electrolyzing water. Nevertheless, the conventional ``trial and error'' method for producing advanced electrocatalysts is not only cost-ineffective but also time-consuming and labor-intensive. Fortunately, the advancement of machine learning brings new opportunities for electrocatalysts discovery and design. By analyzing experimental and theoretical data, machine learning can effectively predict their hydrogen evolution reaction (HER) performance. This review summarizes recent developments in machine learning for low-dimensional electrocatalysts, including zero-dimension nanoparticles and nanoclusters, one-dimensional nanotubes and nanowires, two-dimensional nanosheets, as well as other electrocatalysts. In particular, the effects of descriptors and algorithms on screening low-dimensional electrocatalysts and investigating their HER performance are highlighted. Finally, the future directions and perspectives for machine learning in electrocatalysis are discussed, emphasizing the potential for machine learning to accelerate electrocatalyst discovery, optimize their performance, and provide new insights into electrocatalytic mechanisms. Overall, this work offers an in-depth understanding of the current state of machine learning in electrocatalysis and its potential for future research. [GRAPHICS]} }
@article{WOS:001102175100001, title = {Machine learning for microbiologists}, journal = {NATURE REVIEWS MICROBIOLOGY}, volume = {22}, pages = {191-205}, year = {2024}, issn = {1740-1526}, doi = {10.1038/s41579-023-00984-1}, author = {Asnicar, Francesco and Thomas, Andrew Maltez and Passerini, Andrea and Waldron, Levi and Segata, Nicola}, abstract = {Machine learning is increasingly important in microbiology where it is used for tasks such as predicting antibiotic resistance and associating human microbiome features with complex host diseases. The applications in microbiology are quickly expanding and the machine learning tools frequently used in basic and clinical research range from classification and regression to clustering and dimensionality reduction. In this Review, we examine the main machine learning concepts, tasks and applications that are relevant for experimental and clinical microbiologists. We provide the minimal toolbox for a microbiologist to be able to understand, interpret and use machine learning in their experimental and translational activities. In this Review, Segata, Waldron and colleagues discuss important key concepts of machine learning that are relevant to microbiologists and provide them with a set of tools essential to apply machine learning in microbiology research.} }
@article{WOS:000906478900001, title = {A comprehensive review of machine learning-based methods in landslide susceptibility mapping}, journal = {GEOLOGICAL JOURNAL}, volume = {58}, pages = {2283-2301}, year = {2023}, issn = {0072-1050}, doi = {10.1002/gj.4666}, author = {Liu, Songlin and Wang, Luqi and Zhang, Wengang and He, Yuwei and Pijush, Samui}, abstract = {Landslide susceptibility mapping (LSM) has been widely used as an important reference for development and construction planning to mitigate the potential social-eco impact caused by landslides. Originally, most of those maps were generated by the judgements of experts, which is time-consuming and laborious, and whose accuracy is difficult to be quantified because of the subjective effects. With the development of machine learning algorithms and the methods of data collection, big data and artificial intelligence have now been popularized in this field, significantly improving mapping accuracy and efficiency. Various machine learning-based methods, mainly including conventional machine learning, deep learning, and transfer learning have been applied and compared in LSM in different areas by previous researchers. Nevertheless, none of them can be effective in all cases. Although deep learning-based methods were proven more accurate than conventional machine learning-based methods in most data-rich situations, the latter is sometimes more popularly used in LSM, as there is not that much data in this field to train a deep learning network perfectly. In a more rigorous situation when there is very limited data, transfer learning-based approaches are applied by several researchers, which have contributed to improve the workability and the accuracy of LSM in data-limited areas. Such technical explosion has promoted the application of landslide susceptibility maps, thus contributing to mitigating the social-eco impact associated with landslides. This paper comprehensively reviews the whole process of generating landslide susceptibility maps based on machine learning methods, introduces and compares the commonly used machine learning methods, and discusses the topics for future research.} }
@article{WOS:001438192600001, title = {Machine learning and remote sensing integration for leveraging urban sustainability: A review and framework}, journal = {SUSTAINABLE CITIES AND SOCIETY}, volume = {96}, year = {2023}, issn = {2210-6707}, doi = {10.1016/j.scs.2023.104653}, author = {Li, Fei and Yigitcanlar, Tan and Nepal, Madhav and Nguyen, Kien and Dur, Fatih}, abstract = {Climate change and rapid urbanisation exacerbated multiple urban issues threatening urban sustainability. Numerous studies integrated machine learning and remote sensing to monitor urban issues and develop mitigation strategies for sustainability. However, few studies comparatively analysed joint applications of machine learning and remote sensing for urban issues and sustainability. This paper presents a systematic review and formulates a framework integrating machine learning and remote sensing in urban studies. The literature analysis reveals: Most studies occurred in Asia, Europe, and North America, driven by technical and ethical factors, highlighting responsible approaches for data-scarce regions; Reviewed studies prioritised physical spatial aspects over socioeconomic factors, requiring multi-source data for comprehensive analysis; Conventional satellite, aerial images, and Lidar data are prevalent due to affordability, quality, and accessibility; Although supervised machine learning dominates, unsupervised methods and algorithm selection paradigms require exploration; Integration offers accurate results and thorough analysis in image processing and analytics, while image acquisition and decision-making necessitate human supervision. This paper provides a comprehensive review and an integrative framework for machine learning and remote sensing, enriching insights into their potential for urban studies and spatial analytics. The study informs urban planning and policymaking by promoting efficient management via enhanced machine learning and remote sensing integration, and bolstering data-driven decision-making.} }
@article{WOS:001165437500001, title = {Scope of machine learning in materials research-A review}, journal = {APPLIED SURFACE SCIENCE ADVANCES}, volume = {18}, year = {2023}, issn = {2666-5239}, doi = {10.1016/j.apsadv.2023.100523}, author = {Mobarak, Md Hosne and Mimona, Mariam Akter and Islam, Md. Aminul and Hossain, Nayem and Zohura, Fatema Tuz and Imtiaz, Ibnul and Rimon, Md Israfil Hossain}, abstract = {This comprehensive review investigates the multifaceted applications of machine learning in materials research across six key dimensions, redefining the field's boundaries. It explains various knowledge acquisition mechanisms starting with supervised, unsupervised, reinforcement, and deep learning techniques. These techniques are transformative tools for transforming unactionable data into insightful actions. Moving on to the materials synthesis, the review emphasizes the profound influence of machine learning, as demonstrated by predictive models that speed up material selection, structure-property relationships that reveal crucial connections, and data-driven discovery that fosters innovation. Machine learning reshapes our comprehension and manipulation of materials by accelerating discovery and enabling tailored design through property prediction models and structure-property relationships. Machine learning extends its influence to image processing, improving object detection, classification, and segmentation precision and enabling methods like image generation, revolutionizing the potential of image processing in materials research. The most recent developments show how machine learning can have a transformative impact at the atomic level by enabling precise property prediction and intricate data extraction, representing significant advancements in material understanding and innovation. The review highlights how machine learning has the potential to revolutionize materials research by accelerating discovery, improving performance, and stimulating innovation. It does so while acknowledging obstacles like poor data quality and complicated algorithms. Machine learning offers a wide range of exciting prospects for scientific investigation and technological advancement, positioning it as a powerful force for influencing the future of materials research.} }
@article{WOS:000638010100001, title = {Machine learning and deep learning}, journal = {ELECTRONIC MARKETS}, volume = {31}, pages = {685-695}, year = {2021}, issn = {1019-6781}, doi = {10.1007/s12525-021-00475-2}, author = {Janiesch, Christian and Zschech, Patrick and Heinrich, Kai}, abstract = {Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.} }
@article{WOS:000977155100001, title = {Advancements and Challenges in Machine Learning: A Comprehensive Review of Models, Libraries, Applications, and Algorithms}, journal = {ELECTRONICS}, volume = {12}, year = {2023}, issn = {2079-9292}, doi = {10.3390/electronics12081789}, author = {Tufail, Shahid and Riggs, Hugo and Tariq, Mohd and Sarwat, Arif I.}, abstract = {In the current world of the Internet of Things, cyberspace, mobile devices, businesses, social media platforms, healthcare systems, etc., there is a lot of data online today. Machine learning (ML) is something we need to understand to do smart analyses of these data and make smart, automated applications that use them. There are many different kinds of machine learning algorithms. The most well-known ones are supervised, unsupervised, semi-supervised, and reinforcement learning. This article goes over all the different kinds of machine-learning problems and the machine-learning algorithms that are used to solve them. The main thing this study adds is a better understanding of the theory behind many machine learning methods and how they can be used in the real world, such as in energy, healthcare, finance, autonomous driving, e-commerce, and many more fields. This article is meant to be a go-to resource for academic researchers, data scientists, and machine learning engineers when it comes to making decisions about a wide range of data and methods to start extracting information from the data and figuring out what kind of machine learning algorithm will work best for their problem and what results they can expect. Additionally, this article presents the major challenges in building machine learning models and explores the research gaps in this area. In this article, we also provided a brief overview of data protection laws and their provisions in different countries.} }
@article{WOS:000939178900001, title = {Quantum Machine Learning: A Review and Case Studies}, journal = {ENTROPY}, volume = {25}, year = {2023}, doi = {10.3390/e25020287}, author = {Zeguendry, Amine and Jarir, Zahi and Quafafou, Mohamed}, abstract = {Despite its undeniable success, classical machine learning remains a resource-intensive process. Practical computational efforts for training state-of-the-art models can now only be handled by high speed computer hardware. As this trend is expected to continue, it should come as no surprise that an increasing number of machine learning researchers are investigating the possible advantages of quantum computing. The scientific literature on Quantum Machine Learning is now enormous, and a review of its current state that can be comprehended without a physics background is necessary. The objective of this study is to present a review of Quantum Machine Learning from the perspective of conventional techniques. Departing from giving a research path from fundamental quantum theory through Quantum Machine Learning algorithms from a computer scientist's perspective, we discuss a set of basic algorithms for Quantum Machine Learning, which are the fundamental components for Quantum Machine Learning algorithms. We implement the Quanvolutional Neural Networks (QNNs) on a quantum computer to recognize handwritten digits, and compare its performance to that of its classical counterpart, the Convolutional Neural Networks (CNNs). Additionally, we implement the QSVM on the breast cancer dataset and compare it to the classical SVM. Finally, we implement the Variational Quantum Classifier (VQC) and many classical classifiers on the Iris dataset to compare their accuracies.} }
@article{WOS:001131493500001, title = {Machine Learning Applications in Agriculture: Current Trends, Challenges, and Future Perspectives}, journal = {AGRONOMY-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/agronomy13122976}, author = {Araujo, Sara Oleiro and Peres, Ricardo Silva and Ramalho, Jose Cochicho and Lidon, Fernando and Barata, Jose}, abstract = {Progress in agricultural productivity and sustainability hinges on strategic investments in technological research. Evolving technologies such as the Internet of Things, sensors, robotics, Artificial Intelligence, Machine Learning, Big Data, and Cloud Computing are propelling the agricultural sector towards the transformative Agriculture 4.0 paradigm. The present systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to explore the usage of Machine Learning in agriculture. The study investigates the foremost applications of Machine Learning, including crop, water, soil, and animal management, revealing its important role in revolutionising traditional agricultural practices. Furthermore, it assesses the substantial impacts and outcomes of Machine Learning adoption and highlights some challenges associated with its integration in agricultural systems. This review not only provides valuable insights into the current landscape of Machine Learning applications in agriculture, but it also outlines promising directions for future research and innovation in this rapidly evolving field.} }
@article{WOS:000991648200011, title = {Integrated Sensing and Communication for 6G: Ten Key Machine Learning Roles}, journal = {IEEE COMMUNICATIONS MAGAZINE}, volume = {61}, pages = {113-119}, year = {2023}, issn = {0163-6804}, doi = {10.1109/MCOM.006.2200480}, author = {Demirhan, Umut and Alkhateeb, Ahmed}, abstract = {Integrating sensing and communication is a defining theme for future wireless systems. This is motivated by the promising performance gains, especially as they assist each other, and by the better utilization of the wireless and hardware resources. Realizing these gains in practice, however, is subject to several challenges where leveraging machine learning can provide a potential solution. This article focuses on ten key machine learning roles for joint sensing and communication, sensing-aided communication, and communication-aided sensing systems, explains why and how machine learning can be utilized, and highlights important directions for future research. The article also presents real-world results for some of these machine learning roles based on the large-scale real-world dataset DeepSense 6G, which could be adopted in investigating a wide range of integrated sensing and communication problems.} }
@article{WOS:000972340500002, title = {Advances in machine learning- and artificial intelligence-assisted material design of steels}, journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS}, volume = {30}, pages = {1003-1024}, year = {2023}, issn = {1674-4799}, doi = {10.1007/s12613-022-2595-0}, author = {Pan, Guangfei and Wang, Feiyang and Shang, Chunlei and Wu, Honghui and Wu, Guilin and Gao, Junheng and Wang, Shuize and Gao, Zhijun and Zhou, Xiaoye and Mao, Xinping}, abstract = {With the rapid development of artificial intelligence technology and increasing material data, machine learning- and artificial intelligence-assisted design of high-performance steel materials is becoming a mainstream paradigm in materials science. Machine learning methods, based on an interdisciplinary discipline between computer science, statistics and material science, are good at discovering correlations between numerous data points. Compared with the traditional physical modeling method in material science, the main advantage of machine learning is that it overcomes the complex physical mechanisms of the material itself and provides a new perspective for the research and development of novel materials. This review starts with data preprocessing and the introduction of different machine learning models, including algorithm selection and model evaluation. Then, some successful cases of applying machine learning methods in the field of steel research are reviewed based on the main theme of optimizing composition, structure, processing, and performance. The application of machine learning methods to the performance-oriented inverse design of material composition and detection of steel defects is also reviewed. Finally, the applicability and limitations of machine learning in the material field are summarized, and future directions and prospects are discussed.} }
@article{WOS:000935967900001, title = {Quantum machine learning: from physics to software engineering}, journal = {ADVANCES IN PHYSICS-X}, volume = {8}, year = {2023}, issn = {2374-6149}, doi = {10.1080/23746149.2023.2165452}, author = {Melnikov, Alexey and Kordzanganeh, Mohammad and Alodjants, Alexander and Lee, Ray-Kuang}, abstract = {Quantum machine learning is a rapidly growing field at the intersection of quantum technology and artificial intelligence. This review provides a two-fold overview of several key approaches that can offer advancements in both the development of quantum technologies and the power of artificial intelligence. Among these approaches are quantum-enhanced algorithms, which apply quantum software engineering to classical information processing to improve keystone machine learning solutions. In this context, we explore the capability of hybrid quantum-classical neural networks to improve model generalization and increase accuracy while reducing computational resources. We also illustrate how machine learning can be used both to mitigate the effects of errors on presently available noisy intermediate-scale quantum devices, and to understand quantum advantage via an automatic study of quantum walk processes on graphs. In addition, we review how quantum hardware can be enhanced by applying machine learning to fundamental and applied physics problems as well as quantum tomography and photonics. We aim to demonstrate how concepts in physics can be translated into practical engineering of machine learning solutions using quantum software.} }
@article{WOS:001193522300001, title = {Design and analysis of quantum machine learning: a survey}, journal = {CONNECTION SCIENCE}, volume = {36}, year = {2024}, issn = {0954-0091}, doi = {10.1080/09540091.2024.2312121}, author = {Chen, Linshu and Li, Tao and Chen, Yuxiang and Chen, Xiaoyan and Wozniak, Marcin and Xiong, Neal and Liang, Wei}, abstract = {Machine learning has demonstrated tremendous potential in solving real-world problems. However, with the exponential growth of data amount and the increase of model complexity, the processing efficiency of machine learning declines rapidly. Meanwhile, the emergence of quantum computing has given rise to quantum machine learning, which relies on superposition and entanglement, exhibiting exponential optimisation compared to traditional machine learning. Therefore, in the paper, we survey the basic concepts, algorithms, applications and challenges of quantum machine learning. Concretely, we first review the basic concepts of quantum computing including qubit, quantum gates, quantum entanglement, etc.. Secondly, we in-depth discuss 5 quantum machine learning algorithms of quantum support vector machine, quantum neural network, quantum k-nearest neighbour, quantum principal component analysis and quantum k-Means algorithm. Thirdly, we conduct discussions on the applications of quantum machine learning in image recognition, drug efficacy prediction and cybersecurity. Finally, we summarise the challenges of quantum machine learning consisting of algorithm design, hardware limitations, data encoding, quantum landscapes, noise and decoherence.} }
@article{WOS:001359559300001, title = {Quantum machine learning: Classifications, challenges, and solutions}, journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION}, volume = {42}, year = {2024}, issn = {2467-964X}, doi = {10.1016/j.jii.2024.100736}, author = {Lu, Wei and Lu, Yang and Li, Jin and Sigov, Alexander and Ratkin, Leonid and Ivanov, Leonid A.}, abstract = {Recently, research at the intersection of quantum mechanics and machine learning has gained attention. This interdisciplinary field aims to tackle the computational efficiency of machine learning by leveraging quantum computing and to derive novel machine learning algorithms inspired by quantum principles. Despite substantial progress in quantum science research, several challenges persist, including the preservation of quantum coherence, mitigation of environmental constraints, advancing quantum computer development, and formulating comprehensive quantum machine learning algorithms. To date, a comprehensive theoretical framework for quantum machine learning is lacking, with much of the research still in the exploratory and experimental stages. This study conducts a thorough survey on quantum machine learning, with the aim of classifying quantum machine learning algorithms while addressing the existing challenges and potential solutions in this emerging field.} }
@article{WOS:001061188500001, title = {Application of Machine Learning in Material Synthesis and Property Prediction}, journal = {MATERIALS}, volume = {16}, year = {2023}, doi = {10.3390/ma16175977}, author = {Huang, Guannan and Guo, Yani and Chen, Ye and Nie, Zhengwei}, abstract = {Material innovation plays a very important role in technological progress and industrial development. Traditional experimental exploration and numerical simulation often require considerable time and resources. A new approach is urgently needed to accelerate the discovery and exploration of new materials. Machine learning can greatly reduce computational costs, shorten the development cycle, and improve computational accuracy. It has become one of the most promising research approaches in the process of novel material screening and material property prediction. In recent years, machine learning has been widely used in many fields of research, such as superconductivity, thermoelectrics, photovoltaics, catalysis, and high-entropy alloys. In this review, the basic principles of machine learning are briefly outlined. Several commonly used algorithms in machine learning models and their primary applications are then introduced. The research progress of machine learning in predicting material properties and guiding material synthesis is discussed. Finally, a future outlook on machine learning in the materials science field is presented.} }
@article{WOS:000978351900001, title = {Synthesis optimization and adsorption modeling of biochar for pollutant removal via machine learning}, journal = {BIOCHAR}, volume = {5}, year = {2023}, issn = {2524-7972}, doi = {10.1007/s42773-023-00225-x}, author = {Zhang, Wentao and Chen, Ronghua and Li, Jie and Huang, Tianyin and Wu, Bingdang and Ma, Jun and Wen, Qingqi and Tan, Jie and Huang, Wenguang}, abstract = {Due to large specific surface area, abundant functional groups and low cost, biochar is widely used for pollutant removal. The adsorption performance of biochar is related to biochar synthesis and adsorption parameters. But the influence factor is numerous, the traditional experimental enumeration is powerless. In recent years, machine learning has been gradually employed for biochar, but there is no comprehensive review on the whole process regulation of biochar adsorbents, covering synthesis optimization and adsorption modeling. This review article systematically summarized the application of machine learning in biochar adsorbents from the perspective of all-round regulation for the first time, including the synthesis optimization and adsorption modeling of biochar adsorbents. Firstly, the overview of machine learning was introduced. Then, the latest advances of machine learning in biochar synthesis for pollutant removal were summarized, including prediction of biochar yield and physicochemical properties, optimal synthetic conditions and economic cost. And the application of machine learning in pollutant adsorption by biochar was reviewed, covering prediction of adsorption efficiency, optimization of experimental conditions and revelation of adsorption mechanism. General guidelines for the application of machine learning in whole-process optimization of biochar from synthesis to adsorption were presented. Finally, the existing problems and future perspectives of machine learning for biochar adsorbents were put forward. We hope that this review can promote the integration of machine learning and biochar, and thus light up the industrialization of biochar.} }
@article{WOS:001073553500001, title = {Forecasting Stock Market Prices Using Machine Learning and Deep Learning Models: A Systematic Review, Performance Analysis and Discussion of Implications}, journal = {INTERNATIONAL JOURNAL OF FINANCIAL STUDIES}, volume = {11}, year = {2023}, issn = {2227-7072}, doi = {10.3390/ijfs11030094}, author = {Sonkavde, Gaurang and Dharrao, Deepak Sudhakar and Bongale, Anupkumar M. and Deokate, Sarika T. and Doreswamy, Deepak and Bhat, Subraya Krishna}, abstract = {The financial sector has greatly impacted the monetary well-being of consumers, traders, and financial institutions. In the current era, artificial intelligence is redefining the limits of the financial markets based on state-of-the-art machine learning and deep learning algorithms. There is extensive use of these techniques in financial instrument price prediction, market trend analysis, establishing investment opportunities, portfolio optimization, etc. Investors and traders are using machine learning and deep learning models for forecasting financial instrument movements. With the widespread adoption of AI in finance, it is imperative to summarize the recent machine learning and deep learning models, which motivated us to present this comprehensive review of the practical applications of machine learning in the financial industry. This article examines algorithms such as supervised and unsupervised machine learning algorithms, ensemble algorithms, time series analysis algorithms, and deep learning algorithms for stock price prediction and solving classification problems. The contributions of this review article are as follows: (a) it provides a description of machine learning and deep learning models used in the financial sector; (b) it provides a generic framework for stock price prediction and classification; and (c) it implements an ensemble model-''Random Forest + XG-Boost + LSTM''-for forecasting TAINIWALCHM and AGROPHOS stock prices and performs a comparative analysis with popular machine learning and deep learning models.} }
@article{WOS:001041998200001, title = {Machine learning in marine ecology: an overview of techniques and applications}, journal = {ICES JOURNAL OF MARINE SCIENCE}, volume = {80}, pages = {1829-1853}, year = {2023}, issn = {1054-3139}, doi = {10.1093/icesjms/fsad100}, author = {Rubbens, Peter and Brodie, Stephanie and Cordier, Tristan and Destro Barcellos, Diogo and Devos, Paul and Fernandes-Salvador, Jose A. and Fincham, I, Jennifer and Gomes, Alessandra and Handegard, Nils Olav and Howell, Kerry and Jamet, Cedric and Kartveit, Kyrre Heldal and Moustahfid, Hassan and Parcerisas, Clea and Politikos, Dimitris and Sauzede, Raphaelle and Sokolova, Maria and Uusitalo, Laura and Van den Bulcke, Laure and van Helmond, Aloysius T. M. and Watson, Jordan T. and Welch, Heather and Beltran-Perez, Oscar and Chaffron, Samuel and Greenberg, David S. and Kuehn, Bernhard and Kiko, Rainer and Lo, Madiop and Lopes, Rubens M. and Moeller, Klas Ove and Michaels, William and Pala, Ahmet and Romagnan, Jean-Baptiste and Schuchert, Pia and Seydi, Vahid and Villasante, Sebastian and Malde, Ketil and Irisson, Jean-Olivier}, abstract = {Machine learning covers a large set of algorithms that can be trained to identify patterns in data. Thanks to the increase in the amount of data and computing power available, it has become pervasive across scientific disciplines. We first highlight why machine learning is needed in marine ecology. Then we provide a quick primer on machine learning techniques and vocabulary. We built a database of \\& SIM;1000 publications that implement such techniques to analyse marine ecology data. For various data types (images, optical spectra, acoustics, omics, geolocations, biogeochemical profiles, and satellite imagery), we present a historical perspective on applications that proved influential, can serve as templates for new work, or represent the diversity of approaches. Then, we illustrate how machine learning can be used to better understand ecological systems, by combining various sources of marine data. Through this coverage of the literature, we demonstrate an increase in the proportion of marine ecology studies that use machine learning, the pervasiveness of images as a data source, the dominance of machine learning for classification-type problems, and a shift towards deep learning for all data types. This overview is meant to guide researchers who wish to apply machine learning methods to their marine datasets.} }
@article{WOS:000935501400001, title = {A survey of machine learning in additive manufacturing technologies}, journal = {INTERNATIONAL JOURNAL OF COMPUTER INTEGRATED MANUFACTURING}, volume = {36}, pages = {1258-1280}, year = {2023}, issn = {0951-192X}, doi = {10.1080/0951192X.2023.2177740}, author = {Jiang, Jingchao}, abstract = {Thirty years into its development, additive manufacturing has become a mainstream manufacturing process. Additive manufacturing fabricates products by adding materials layer-by-layer directly based on a 3D model. It is able to manufacture complex parts and allows more freedom of design optimization compared with traditional manufacturing techniques. Machine learning is now a hot technology that has been used in medical diagnosis, image processing, prediction, classification, learning association, regression, etc. Currently, focuses are increasingly given to using machine learning in the manufacturing industry, including additive manufacturing. Due to the rapid development of machine learning in additive manufacturing, a special issue `Machine Learning in Additive Manufacturing' in International Journal of Computer Integrated Manufacturing is organized. This paper gives a comprehensive understanding of the current status of machine learning enhanced additive manufacturing technologies for this special issue. Discussions and future perspectives are also provided.} }
@article{WOS:001020157200009, title = {Machine learning in agriculture: a review of crop management applications}, journal = {MULTIMEDIA TOOLS AND APPLICATIONS}, volume = {83}, pages = {12875-12915}, year = {2024}, issn = {1380-7501}, doi = {10.1007/s11042-023-16105-2}, author = {Attri, Ishana and Awasthi, Lalit Kumar and Sharma, Teek Parval}, abstract = {Machine learning has created new opportunities for data-intensive study in interdisciplinary domains as a result of the advancement of big data technologies and high-performance computers. Search engines, email spam filters, websites that offer personalized recommendations, banking software that alerts users to suspicious activity, and a plethora of smartphone apps that perform tasks like voice recognition, image recognition, and natural language processing are just a few examples of the online and offline services that have incorporated machine learning in recent years. One of the most crucial areas where machine learning applications still has to be investigated is agriculture, which directly affects people's well-being. In this article, a literature review on machine learning algorithms used in agriculture is presented. The proposed paper deal with various crop management applications which are categorised into five parts i.e., Weed and pest detection, Plant disease detection, Stress detection in plants, Smart farms or automation in farms and the last one is Crop yield estimation and prediction. The articles' filtering and categorization show how machine learning may improve agriculture. This article examines machine learning breakthroughs in agriculture. This paper's findings show that by using novel machine learning approaches, models may achieve improved accuracy and shorter inference time for real-world applications.} }
@article{WOS:001008445700001, title = {Machine learning in biohydrogen production: a review}, journal = {BIOFUEL RESEARCH JOURNAL-BRJ}, volume = {10}, pages = {1844-1858}, year = {2023}, issn = {2292-8782}, doi = {10.18331/BRJ2023.10.2.4}, author = {Alagumalai, Avinash and Devarajan, Balaji and Song, Hua and Wongwises, Somchai and Ledesma-Amaro, Rodrigo and Mahian, Omid and Sheremet, Mikhail and Lichtfouse, Eric}, abstract = {Biohydrogen is emerging as a promising carbon-neutral and sustainable energy carrier with high energy yield to replace conventional fossil fuels. However, biohydrogen commercial uptake is mainly hindered by the supply side. As a result, various operating parameters must be optimized to realize biohydrogen commercial uptake on a large-scale. Recently, machine learning algorithms have demonstrated the ability to handle large amounts of data while requiring less in-depth knowledge of the system and being capable of adapting to evolving circumstances. This review critically reviews the role of machine learning in categorizing and predicting data related to biohydrogen production. The accuracy and potential of different machine learning algorithms are reported. Also, the practical implications of machine learning models to realize biohydrogen uptake by the transportation sector are discussed. The review indicates that machine learning algorithms can successfully model non-linear and complex interactions between operational and performance parameters in biohydrogen production. Additionally, machine learning algorithms can help researchers identify the most efficient methods for producing biohydrogen, leading to a more sustainable and cost-effective energy source. (c) 2023 BRTeam. All rights reserved.} }
@article{WOS:000695380500001, title = {A guide to machine learning for biologists}, journal = {NATURE REVIEWS MOLECULAR CELL BIOLOGY}, volume = {23}, pages = {40-55}, year = {2022}, issn = {1471-0072}, doi = {10.1038/s41580-021-00407-0}, author = {Greener, Joe G. and Kandathil, Shaun M. and Moffat, Lewis and Jones, David T.}, abstract = {Machine learning is becoming a widely used tool for the analysis of biological data. However, for experimentalists, proper use of machine learning methods can be challenging. This Review provides an overview of machine learning techniques and provides guidance on their applications in biology. The expanding scale and inherent complexity of biological data have encouraged a growing use of machine learning in biology to build informative and predictive models of the underlying biological processes. All machine learning techniques fit models to data; however, the specific methods are quite varied and can at first glance seem bewildering. In this Review, we aim to provide readers with a gentle introduction to a few key machine learning techniques, including the most recently developed and widely used techniques involving deep neural networks. We describe how different techniques may be suited to specific types of biological data, and also discuss some best practices and points to consider when one is embarking on experiments involving machine learning. Some emerging directions in machine learning methodology are also discussed.} }
@article{WOS:001117835000001, title = {Review of machine learning and deep learning models for toxicity prediction}, journal = {EXPERIMENTAL BIOLOGY AND MEDICINE}, volume = {248}, pages = {1952-1973}, year = {2023}, issn = {1535-3702}, doi = {10.1177/15353702231209421}, author = {Guo, Wenjing and Liu, Jie and Dong, Fan and Song, Meng and Li, Zoe and Khan, Md Kamrul Hasan and Patterson, Tucker A. and Hong, Huixiao}, abstract = {The ever-increasing number of chemicals has raised public concerns due to their adverse effects on human health and the environment. To protect public health and the environment, it is critical to assess the toxicity of these chemicals. Traditional in vitro and in vivo toxicity assays are complicated, costly, and time-consuming and may face ethical issues. These constraints raise the need for alternative methods for assessing the toxicity of chemicals. Recently, due to the advancement of machine learning algorithms and the increase in computational power, many toxicity prediction models have been developed using various machine learning and deep learning algorithms such as support vector machine, random forest, k-nearest neighbors, ensemble learning, and deep neural network. This review summarizes the machine learning- and deep learning-based toxicity prediction models developed in recent years. Support vector machine and random forest are the most popular machine learning algorithms, and hepatotoxicity, cardiotoxicity, and carcinogenicity are the frequently modeled toxicity endpoints in predictive toxicology. It is known that datasets impact model performance. The quality of datasets used in the development of toxicity prediction models using machine learning and deep learning is vital to the performance of the developed models. The different toxicity assignments for the same chemicals among different datasets of the same type of toxicity have been observed, indicating benchmarking datasets is needed for developing reliable toxicity prediction models using machine learning and deep learning algorithms. This review provides insights into current machine learning models in predictive toxicology, which are expected to promote the development and application of toxicity prediction models in the future.} }
@article{WOS:000922087600001, title = {Quantum machine learning in medical image analysis: A survey}, journal = {NEUROCOMPUTING}, volume = {525}, pages = {42-53}, year = {2023}, issn = {0925-2312}, doi = {10.1016/j.neucom.2023.01.049}, author = {Wei, Lin and Liu, Haowen and Xu, Jing and Shi, Lei and Shan, Zheng and Zhao, Bo and Gao, Yufei}, abstract = {With the outstanding superposition and entanglement properties of quantum computing, quantum machine learning has attracted widespread attention in many fields, such as medical image analysis, password cracking, and pattern recognition. Although classical machine learning is widely used and has shown great potential in medical image analysis, the bottlenecks of insufficient labeled data and low processing efficiency still exist. To overcome these challenges, massive studies combined quantum computing with machine learning to explore more advanced algorithms, which have achieved distin-guished improvements in parameter optimization, execution efficiency, and the reduction of error rates. Quantum machine learning provides new insights for the intersectional research of quantum technology and medical image analysis and contributes to the future development of medical image analysis. This review delivers an overview of the definition and taxonomy of quantum machine learning, as well as summarizes various quantum machine learning methods and their applications in medical image analy-sis over the past decade.(c) 2023 Elsevier B.V. All rights reserved.} }
@article{WOS:000977859400001, title = {Weakly supervised machine learning}, journal = {CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY}, volume = {8}, pages = {549-580}, year = {2023}, issn = {2468-6557}, doi = {10.1049/cit2.12216}, author = {Ren, Zeyu and Wang, Shuihua and Zhang, Yudong}, abstract = {Supervised learning aims to build a function or model that seeks as many mappings as possible between the training data and outputs, where each training data will predict as a label to match its corresponding ground-truth value. Although supervised learning has achieved great success in many tasks, sufficient data supervision for labels is not accessible in many domains because accurate data labelling is costly and laborious, particularly in medical image analysis. The cost of the dataset with ground-truth labels is much higher than in other domains. Therefore, it is noteworthy to focus on weakly supervised learning for medical image analysis, as it is more applicable for practical applications. In this review, the authors give an overview of the latest process of weakly supervised learning in medical image analysis, including incomplete, inexact, and inaccurate supervision, and introduce the related works on different applications for medical image analysis. Related concepts are illustrated to help readers get an overview ranging from supervised to unsupervised learning within the scope of machine learning. Furthermore, the challenges and future works of weakly supervised learning in medical image analysis are discussed.} }
@article{WOS:001162814300001, title = {Robust machine learning models: linear and nonlinear}, journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS}, volume = {20}, pages = {1043-1050}, year = {2025}, issn = {2364-415X}, doi = {10.1007/s41060-024-00512-1}, author = {Giudici, Paolo and Raffinetti, Emanuela and Riani, Marco}, abstract = {Artificial Intelligence relies on the application of machine learning models which, while reaching high predictive accuracy, lack explainability and robustness. This is a problem in regulated industries, as authorities aimed at monitoring the risks arising from the application of Artificial Intelligence methods may not validate them. No measurement methodologies are yet available to jointly assess accuracy, explainability and robustness of machine learning models. We propose a methodology which fills the gap, extending the Forward Search approach, employed in robust statistical learning, to machine learning models. Doing so, we will be able to evaluate, by means of interpretable statistical tests, whether a specific Artificial Intelligence application is accurate, explainable and robust, through a unified methodology. We apply our proposal to the context of Bitcoin price prediction, comparing a linear regression model against a nonlinear neural network model.} }
@article{WOS:001191466700001, title = {Compact Data Learning for Machine Learning Classifications}, journal = {AXIOMS}, volume = {13}, year = {2024}, doi = {10.3390/axioms13030137}, author = {Kim, Song-Kyoo (Amang)}, abstract = {This paper targets the area of optimizing machine learning (ML) training data by constructing compact data. The methods of optimizing ML training have improved and become a part of artificial intelligence (AI) system development. Compact data learning (CDL) is an alternative practical framework to optimize a classification system by reducing the size of the training dataset. CDL originated from compact data design, which provides the best assets without handling complex big data. CDL is a dedicated framework for improving the speed of the machine learning training phase without affecting the accuracy of the system. The performance of an ML-based arrhythmia detection system and its variants with CDL maintained the same statistical accuracy. ML training with CDL could be maximized by applying an 85\\% reduced input dataset, which indicated that a trained ML system could have the same statistical accuracy by only using 15\\% of the original training dataset.} }
@article{WOS:001074642900001, title = {Beyond generalization: a theory of robustness in machine learning}, journal = {SYNTHESE}, volume = {202}, year = {2023}, issn = {0039-7857}, doi = {10.1007/s11229-023-04334-9}, author = {Freiesleben, Timo and Grote, Thomas}, abstract = {The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept.} }
@article{WOS:001015789300001, title = {Machine Learning Algorithms and Fundamentals as Emerging Safety Tools in Preservation of Fruits and Vegetables: A Review}, journal = {PROCESSES}, volume = {11}, year = {2023}, doi = {10.3390/pr11061720}, author = {Pandey, Vinay Kumar and Srivastava, Shivangi and Dash, Kshirod Kumar and Singh, Rahul and Mukarram, Shaikh Ayaz and Kovacs, Bela and Harsanyi, Endre}, abstract = {Machine learning assists with food process optimization techniques by developing a model to predict the optimal solution for given input data. Machine learning includes unsupervised and supervised learning, data pre-processing, feature engineering, model selection, assessment, and optimization methods. Various problems with food processing optimization could be resolved using these techniques. Machine learning is increasingly being used in the food industry to improve production efficiency, reduce waste, and create personalized customer experiences. Machine learning may be used to improve ingredient utilization and save costs, automate operations such as packing and labeling, and even forecast consumer preferences to develop personalized products. Machine learning is also being used to identify food safety hazards before they reach the consumer, such as contaminants or spoiled food. The usage of machine learning in the food sector is predicted to rise in the near future as more businesses understand the potential of this technology to enhance customer experience and boost productivity. Machine learning may be utilized to enhance nano-technological operations and fruit and vegetable preservation. Machine learning algorithms may find trends regarding various factors that impact the quality of the product being preserved by examining data from prior tests. Furthermore, machine learning may be utilized to determine optimal parameter combinations that result in maximal produce preservation. The review discusses the relevance of machine learning in ready-to-eat foods and its use as a safety tool for preservation were highlighted. The application of machine learning in agriculture, food packaging, food processing, and food safety is reviewed. The working principle and methodology, as well as the principles of machine learning, were discussed.} }
@article{WOS:000875839900013, title = {Explainable Heart Disease Prediction Using Ensemble-Quantum Machine Learning Approach}, journal = {INTELLIGENT AUTOMATION AND SOFT COMPUTING}, volume = {36}, pages = {761-779}, year = {2023}, issn = {1079-8587}, doi = {10.32604/iasc.2023.032262}, author = {Abdulsalam, Ghada and Meshoul, Souham and Shaiba, Hadil}, abstract = {Nowadays, quantum machine learning is attracting great interest in a wide range of fields due to its potential superior performance and capabilities. The massive increase in computational capacity and speed of quantum computers can lead to a quantum leap in the healthcare field. Heart disease seriously threa-tens human health since it is the leading cause of death worldwide. Quantum machine learning methods can propose effective solutions to predict heart disease and aid in early diagnosis. In this study, an ensemble machine learning model based on quantum machine learning classifiers is proposed to predict the risk of heart disease. The proposed model is a bagging ensemble learning model where a quantum support vector classifier was used as a base classifier. Further-more, in order to make the model's outcomes more explainable, the importance of every single feature in the prediction is computed and visualized using SHapley Additive exPlanations (SHAP) framework. In the experimental study, other stand-alone quantum classifiers, namely, Quantum Support Vector Classifier (QSVC), Quantum Neural Network (QNN), and Variational Quantum Classifier (VQC) are applied and compared with classical machine learning classifiers such as Sup-port Vector Machine (SVM), and Artificial Neural Network (ANN). The experi-mental results on the Cleveland dataset reveal the superiority of QSVC compared to the others, which explains its use in the proposed bagging model. The Bagging-QSVC model outperforms all aforementioned classifiers with an accuracy of 90.16\\% while showing great competitiveness compared to some state-of-the-art models using the same dataset. The results of the study indicate that quantum machine learning classifiers perform better than classical machine learning classi-fiers in predicting heart disease. In addition, the study reveals that the bagging ensemble learning technique is effective in improving the prediction accuracy of quantum classifiers.} }
@article{WOS:000579808700028, title = {On hyperparameter optimization of machine learning algorithms: Theory and practice}, journal = {NEUROCOMPUTING}, volume = {415}, pages = {295-316}, year = {2020}, issn = {0925-2312}, doi = {10.1016/j.neucom.2020.07.061}, author = {Yang, Li and Shami, Abdallah}, abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyperparameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-theart optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000517855800037, title = {Applications of machine learning to machine fault diagnosis: A review and roadmap}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {138}, year = {2020}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2019.106587}, author = {Lei, Yaguo and Yang, Bin and Jiang, Xinwei and Jia, Feng and Li, Naipeng and Nandi, Asoke K.}, abstract = {Intelligent fault diagnosis (IFD) refers to applications of machine learning theories to machine fault diagnosis. This is a promising way to release the contribution from human labor and automatically recognize the health states of machines, thus it has attracted much attention in the last two or three decades. Although IFD has achieved a considerable number of successes, a review still leaves a blank space to systematically cover the development of IFD from the cradle to the bloom, and rarely provides potential guidelines for the future development. To bridge the gap, this article presents a review and roadmap to systematically cover the development of IFD following the progress of machine learning theories and offer a future perspective. In the past, traditional machine learning theories began to weak the contribution of human labor and brought the era of artificial intelligence to machine fault diagnosis. Over the recent years, the advent of deep learning theories has reformed IFD in further releasing the artificial assistance since the 2010s, which encourages to construct an end-to-end diagnosis procedure. It means to directly bridge the relationship between the increasingly-grown monitoring data and the health states of machines. In the future, transfer learning theories attempt to use the diagnosis knowledge from one or multiple diagnosis tasks to other related ones, which prospectively overcomes the obstacles in applications of IFD to engineering scenarios. Finally, the roadmap of IFD is pictured to show potential research trends when combined with the challenges in this field. (C) 2019 Elsevier Ltd. All rights reserved.} }
@article{WOS:000952240100002, title = {A review of deep learning and machine learning techniques for hydrological inflow forecasting}, journal = {ENVIRONMENT DEVELOPMENT AND SUSTAINABILITY}, volume = {25}, pages = {12189-12216}, year = {2023}, issn = {1387-585X}, doi = {10.1007/s10668-023-03131-1}, author = {Latif, Sarmad Dashti and Ahmed, Ali Najah}, abstract = {Conventional machine learning models have been widely used for reservoir inflow and rainfall prediction. Nowadays, researchers focus on a new computing architecture in the area of AI, namely, deep learning for hydrological forecasting parameters. This review paper tends to broadcast more of the intriguing interest in reservoir inflow prediction utilizing deep learning and machine learning algorithms. The AI models utilized for different hydrology sectors, as well as the most prevalent machine learning techniques, will be explored in this thorough study, which divides AI techniques into two primary categories: deep learning and machine learning. In this study, we look at the long short-term memory deep learning method as well as three traditional machine learning algorithms: support vector machine, random forest, and boosted regression tree. Under each part, a summary of the findings is provided. For convenience of reference, some of the benefits and drawbacks discovered through literature reviews have been listed. Finally, future recommendations and overall conclusions based on research findings are given. This review focuses on papers from high-impact factor periodicals published over a 4 years period beginning in 2018 onwards.} }
@article{WOS:000917312100001, title = {Navigating with chemometrics and machine learning in chemistry}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {9089-9114}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-023-10391-w}, author = {Joshi, Payal B.}, abstract = {Chemometrics and machine learning are artificial intelligence-based methods stirring a transformative change in chemistry. Organic synthesis, drug discovery and analytical techniques are incorporating machine learning techniques at an accelerated pace. However, machine-assisted chemistry faces challenges while solving critical problems in chemistry due to complex relationships in data sets. Even with increasing publishing volumes on machine learning, its application in areas of chemistry is not a straightforward endeavour. A particular concern in applying machine learning in chemistry is data availability and reproducibility. The present review article discusses the various chemometric methods, expert systems, and machine learning techniques developed for solving problems of organic synthesis and drug discovery with selected examples. Further, a concise discussion on chemometrics and ML deployed in analytical techniques such as, spectroscopy, microscopy and chromatography are presented. Finally, the review reflects the challenges, opportunities and future perspectives on machine learning and automation in chemistry. The review concludes by pondering on some tough questions on applying machine learning and their possibility of navigation in the different terrains of chemistry.} }
@article{WOS:000978338900001, title = {A Review of Macroscopic Carbon Emission Prediction Model Based on Machine Learning}, journal = {SUSTAINABILITY}, volume = {15}, year = {2023}, doi = {10.3390/su15086876}, author = {Zhao, Yuhong and Liu, Ruirui and Liu, Zhansheng and Liu, Liang and Wang, Jingjing and Liu, Wenxiang}, abstract = {Under the background of global warming and the energy crisis, the Chinese government has set the goal of carbon peaking and carbon neutralization. With the rapid development of machine learning, some advanced machine learning algorithms have also been applied to the control and prediction of carbon emissions due to their high efficiency and accuracy. In this paper, the current situation of machine learning applied to carbon emission prediction is studied in detail by means of paper retrieval. It was found that machine learning has become a hot topic in the field of carbon emission prediction models, and the main carbon emission prediction models are mainly based on back propagation neural networks, support vector machines, long short-term memory neural networks, random forests and extreme learning machines. By describing the characteristics of these five types of carbon emission prediction models and conducting a comparative analysis, we determined the applicable characteristics of each model, and based on this, future research ideas for carbon emission prediction models based on machine learning are proposed.} }
@article{WOS:001089938100001, title = {Machine Learning: Models, Challenges, and Research Directions}, journal = {FUTURE INTERNET}, volume = {15}, year = {2023}, issn = {1999-5903}, doi = {10.3390/fi15100332}, author = {Khoei, Tala Talaei and Kaabouch, Naima}, abstract = {Machine learning techniques have emerged as a transformative force, revolutionizing various application domains, particularly cybersecurity. The development of optimal machine learning applications requires the integration of multiple processes, such as data pre-processing, model selection, and parameter optimization. While existing surveys have shed light on these techniques, they have mainly focused on specific application domains. A notable gap that exists in current studies is the lack of a comprehensive overview of machine learning architecture and its essential phases in the cybersecurity field. To address this gap, this survey provides a holistic review of current studies in machine learning, covering techniques applicable to any domain. Models are classified into four categories: supervised, semi-supervised, unsupervised, and reinforcement learning. Each of these categories and their models are described. In addition, the survey discusses the current progress related to data pre-processing and hyperparameter tuning techniques. Moreover, this survey identifies and reviews the research gaps and key challenges that the cybersecurity field faces. By analyzing these gaps, we propose some promising research directions for the future. Ultimately, this survey aims to serve as a valuable resource for researchers interested in learning about machine learning, providing them with insights to foster innovation and progress across diverse application domains.} }
@article{WOS:001136730400009, title = {Interpretability of Machine Learning: Recent Advances and Future Prospects}, journal = {IEEE MULTIMEDIA}, volume = {30}, pages = {105-118}, year = {2023}, issn = {1070-986X}, doi = {10.1109/MMUL.2023.3272513}, author = {Gao, Lei and Guan, Ling}, abstract = {The proliferation of machine learning (ML) has drawn unprecedented interest in the study of various multimedia contents such as text, image, audio, and video, among others. Consequently, understanding and learning ML-based representations have taken center stage in knowledge discovery in intelligent multimedia research and applications. Nevertheless, the black-box nature of contemporary ML, especially in deep neural networks, has posed a primary challenge for ML-based representation learning. To address this black-box problem, studies on the interpretability of ML have attracted tremendous interest in recent years. This article presents a survey on recent advances in and future prospects for the interpretability of ML, with several application examples pertinent to multimedia computing, including text-image cross-modal representation learning, face recognition, and the recognition of objects. It is evidently shown that the study of the interpretability of ML promises an important research direction, one that is worth further investment in.} }
@article{WOS:001043801800001, title = {Machine learning in architecture}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {154}, year = {2023}, issn = {0926-5805}, doi = {10.1016/j.autcon.2023.105012}, author = {Topuz, Beyza and Alp, Nese Cakici}, abstract = {This paper explores the utilisation of machine learning in architecture, focusing on the addressed problems and commonly employed programming languages, software, platforms, libraries, and algorithms. Eight major academic search and publishing platforms were systematically reviewed, covering the period from 2007 to 2022, resulting in the selection of 60 relevant articles from a pool of 175. The articles were categorised based on their thematic focus, primarily centring around Computer-Aided Design (CAD), Computer-Aided Engineering (CAE), and Computer-Aided Manufacturing (CAM). By evaluating the current state of machine learning in architecture, this study provides valuable insights into its usage and identifies potential areas for future research. This paper contributes to a comprehensive understanding of the evolving landscape of machine learning in the field by investigating subfields within architecture and the specific tools used to tackle architectural challenges.} }
@article{WOS:001061075500001, title = {Ground truth tracings (GTT): On the epistemic limits of machine learning}, journal = {BIG DATA \\& SOCIETY}, volume = {10}, year = {2023}, issn = {2053-9517}, doi = {10.1177/20539517221146122}, author = {Kang, Edward B.}, abstract = {There is a gap in existing critical scholarship that engages with the ways in which current ``machine listening'' or voice analytics/biometric systems intersect with the technical specificities of machine learning. This article examines the sociotechnical assemblage of machine learning techniques, practices, and cultures that underlie these technologies. After engaging with various practitioners working in companies that develop machine listening systems, ranging from CEOs, machine learning engineers, data scientists, and business analysts, among others, I bring attention to the centrality of ``learnability'' as a malleable conceptual framework that bends according to various ``ground-truthing'' practices in formalizing certain listening-based prediction tasks for machine learning. In response, I introduce a process I call Ground Truth Tracings to examine the various ontological translations that occur in training a machine to ``learn to listen.'' Ultimately, by further examining this notion of learnability through the aperture of power, I take insights acquired through my fieldwork in the machine listening industry and propose a strategically reductive heuristic through which the epistemological and ethical soundness of machine learning, writ large, can be contemplated.} }
@article{WOS:001026855700002, title = {Practical advantage of quantum machine learning in ghost imaging}, journal = {COMMUNICATIONS PHYSICS}, volume = {6}, year = {2023}, issn = {2399-3650}, doi = {10.1038/s42005-023-01290-1}, author = {Xiao, Tailong and Zhai, Xinliang and Wu, Xiaoyan and Fan, Jianping and Zeng, Guihua}, abstract = {Quantum computation can provide practical applications where quantum algorithms can outperform classical ones. The authors focus on experimental ghost imaging using a mixture of classical and quantum machine learning (simulated on a classical computer) to process the experimental data and reconstruct/classify the image, finding that quantum machine learning techniques allow for a better reconstruction compared to standard methods. Demonstrating the practical advantage of quantum computation remains a long-standing challenge whereas quantum machine learning becomes a promising application that can be resorted to. In this work, we investigate the practical advantage of quantum machine learning in ghost imaging by overcoming the limitations of classical methods in blind object identification and imaging. We propose two hybrid quantum-classical machine learning algorithms and a physical-inspired patch strategy to allow distributed quantum learning with parallel variational circuits. In light of the algorithm, we conduct experiments for imaging-free object identification and blind ghost imaging under different physical sampling rates. We further quantitatively analyze the advantage through the lens of information geometry and generalization capability. The numerical results showcase that quantum machine learning can restore high-quality images but classical machine learning fails. The advantage of identification rate are up to 10\\% via fair comparison with the classical machine learning methods. Our work explores a physics-related application capable of practical quantum advantage, which highlights the prospect of quantum computation in the machine learning field.} }
@article{WOS:000626358900001, title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods}, journal = {MACHINE LEARNING}, volume = {110}, pages = {457-506}, year = {2021}, issn = {0885-6125}, doi = {10.1007/s10994-021-05946-3}, author = {Huellermeier, Eyke and Waegeman, Willem}, abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.} }
@article{WOS:000842577300001, title = {Human-in-the-loop machine learning: a state of the art}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {3005-3054}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-022-10246-w}, author = {Mosqueira-Rey, Eduardo and Hernandez-Pereira, Elena and Alonso-Rios, David and Bobes-Bascaran, Jose and Fernandez-Leal, Angel}, abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.} }
@article{WOS:000742179000001, title = {Machine Learning Testing: Survey, Landscapes and Horizons}, journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING}, volume = {48}, pages = {1-36}, year = {2022}, issn = {0098-5589}, doi = {10.1109/TSE.2019.2962027}, author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang}, abstract = {This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.} }
@article{WOS:000743249300001, title = {Interpretable machine learning: Fundamental principles and 10 grand challenges}, journal = {STATISTICS SURVEYS}, volume = {16}, pages = {1-85}, year = {2022}, issn = {1935-7516}, doi = {10.1214/21-SS133}, author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi}, abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the ``Rashomon set'' of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.} }
@article{WOS:001099973100001, title = {Interpretable machine learning assessment}, journal = {NEUROCOMPUTING}, volume = {561}, year = {2023}, issn = {0925-2312}, doi = {10.1016/j.neucom.2023.126891}, author = {Han, Henry and Wu, Yi and Wang, Jiacun and Han, Ashley}, abstract = {With the surge of machine learning in AI and data science, there remains an urgent need to not only compare the performance of different methods across diverse datasets but also to analyze machine learning behaviors with sensitivity using an explainable approach. In this study, we introduce a uniquely designed diagnostic index: dindex to tackle this challenge. This tool integrates classification effectiveness from multiple dimensions, delivering a transparent and comprehensive assessment that transcends the limitations of traditional evaluation methods in classification. We propose two innovative concepts: breakeven states and imbalanced points in this study. Integrated with the d-index, these concepts afford a more profound understanding of the learning behaviors across different machine learning models compared to the existing classification metrics. Significantly, the d-index excels as a powerful tool, identifying learning singularity problems (LSPs) that remain elusive to most current machine learning models and imbalanced learning techniques. Furthermore, leveraging the d-index, we unravel the mechanisms behind imbalanced point generation in binary and multiclass classification. We also put forth a novel technique: identifying a priori informative kernels to optimize support vector machine learning, ensuring outstanding d-index values with the fewest necessary support vectors. Moreover, we address a seldomdiscussed state of overfitting in deep learning, where overfitting occurs despite the training and testing loss curves exhibiting favorable trends throughout the epochs. To the best of our knowledge, this work represents a pioneering stride in the realm of explainable machine learning assessments and will inspire further studies in this area.} }
@article{WOS:000952857700002, title = {Machine Learning Security in Industry: A Quantitative Survey}, journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}, volume = {18}, pages = {1749-1762}, year = {2023}, issn = {1556-6013}, doi = {10.1109/TIFS.2023.3251842}, author = {Grosse, Kathrin and Bieringer, Lukas and Besold, Tarek R. and Biggio, Battista and Krombholz, Katharina}, abstract = {Despite the large body of academic work on machine learning security, little is known about the occurrence of attacks on machine learning systems in the wild. In this paper, we report on a quantitative study with 139 industrial practitioners. We analyze attack occurrence and concern and evaluate statistical hypotheses on factors influencing threat perception and exposure. Our results shed light on real-world attacks on deployed machine learning. On the organizational level, while we find no predictors for threat exposure in our sample, the amount of implement defenses depends on exposure to threats or expected likelihood to become a target. We also provide a detailed analysis of practitioners' replies on the relevance of individual machine learning attacks, unveiling complex concerns like unreliable decision making, business information leakage, and bias introduction into models. Finally, we find that on the individual level, prior knowledge about machine learning security influences threat perception. Our work paves the way for more research about adversarial machine learning in practice, but yields also insights for regulation and auditing.} }
@article{WOS:001070983900001, title = {A survey on multimodal bidirectional machine learning translation of image and natural language processing}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {235}, year = {2024}, issn = {0957-4174}, doi = {10.1016/j.eswa.2023.121168}, author = {Nam, Wongyung and Jang, Beakcheol}, abstract = {Advances in multimodal machine learning help artificial intelligence to resemble human intellect more closely, which perceives the world from multiple modalities. We surveyed state-of-the-art research on the modalities of bidirectional machine learning translation of image and natural language processing (NLP), which address a considerable proportion of human life. Recently, with the advances in deep learning model architectures and learning methods in the fields of image and NLP, considerable progress has been made in multimodal machine learning translations that can be built by integrating image and NLP. Our goal is to explore and summarize state-of-the-art research on multimodal machine learning translation and present a taxonomy for the multimodal bidirectional machine learning translation of image and NLP. Furthermore, we reviewed the evaluation metrics and compared state-of-the-art approaches that influences this field. We believe that this survey will become a cornerstone of future research by discussing the challenges in multimodal machine learning translation and direction of future research based on understanding state-of-the-art research in the field.} }
@article{WOS:000888210300020, title = {Challenges and opportunities in quantum machine learning}, journal = {NATURE COMPUTATIONAL SCIENCE}, volume = {2}, pages = {567-576}, year = {2022}, doi = {10.1038/s43588-022-00311-3}, author = {Cerezo, M. and Verdon, Guillaume and Huang, Hsin-Yuan and Cincio, Lukasz and Coles, Patrick J.}, abstract = {At the intersection of machine learning and quantum computing, quantum machine learning has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry and high-energy physics. Nevertheless, challenges remain regarding the trainability of quantum machine learning models. Here we review current methods and applications for quantum machine learning. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with quantum machine learning.} }
@article{WOS:001339794500006, title = {A review of the application of machine learning in water quality evaluation}, journal = {ECO-ENVIRONMENT \\& HEALTH}, volume = {1}, pages = {107-116}, year = {2022}, doi = {10.1016/j.eehl.2022.06.001}, author = {Zhu, Mengyuan and Wang, Jiawei and Yang, Xiao and Zhang, Yu and Zhang, Linyu and Ren, Hongqiang and Wu, Bing and Ye, Lin}, abstract = {With the rapid increase in the volume of data on the aquatic environment, machine learning has become an important tool for data analysis, classification, and prediction. Unlike traditional models used in water-related research, data-driven models based on machine learning can efficiently solve more complex nonlinear problems. In water environment research, models and conclusions derived from machine learning have been applied to the construction, monitoring, simulation, evaluation, and optimization of various water treatment and management systems. Additionally, machine learning can provide solutions for water pollution control, water quality improvement, and watershed ecosystem security management. In this review, we describe the cases in which machine learning algorithms have been applied to evaluate the water quality in different water environments, such as surface water, groundwater, drinking water, sewage, and seawater. Furthermore, we propose possible future applications of machine learning approaches to water environments.} }
@article{WOS:000833418600004, title = {A survey of human-in-the-loop for machine learning}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {135}, pages = {364-381}, year = {2022}, issn = {0167-739X}, doi = {10.1016/j.future.2022.05.014}, author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang}, abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-the-loop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions. Keywords: Human-in-the-loop Machine learning Deep learning Data processing Computer vision Natural language processing (C) 2022 Elsevier B.V. All rights reserved.} }
@article{WOS:000849857300001, title = {Machine Learning for Electrocatalyst and Photocatalyst Design and Discovery}, journal = {CHEMICAL REVIEWS}, volume = {122}, pages = {13478-13515}, year = {2022}, issn = {0009-2665}, doi = {10.1021/acs.chemrev.2c00061}, author = {Mai, Haoxin and Le, Tu C. and Chen, Dehong and Winkler, David A. and Caruso, Rachel A.}, abstract = {Electrocatalysts and photocatalysts are key to a sustainable future, generating clean fuels, reducing the impact of global warming, and providing solutions to environmental pollution. Improved processes for catalyst design and a better understanding of electro/ photocatalytic processes are essential for improving catalyst effectiveness. Recent advances in data science and artificial intelligence have great potential to accelerate electrocatalysis and photocatalysis research, particularly the rapid exploration of large materials chemistry spaces through machine learning. Here a comprehensive introduction to, and critical review of, machine learning techniques used in electrocatalysis and photocatalysis research are provided. Sources of electro/photocatalyst data and current approaches to representing these materials by mathematical features are described, the most commonly used machine learning methods summarized, and the quality and utility of electro/photocatalyst models evaluated. Illustrations of how machine learning models are applied to novel electro/ photocatalyst discovery and used to elucidate electrocatalytic or photocatalytic reaction mechanisms are provided. The review offers a guide for materials scientists on the selection of machine learning methods for electrocatalysis and photocatalysis research. The application of machine learning to catalysis science represents a paradigm shift in the way advanced, next-generation catalysts will be designed and synthesized.} }
@article{WOS:000770194100002, title = {Interpretable machine learning for knowledge generation in heterogeneous catalysis}, journal = {NATURE CATALYSIS}, volume = {5}, pages = {175-184}, year = {2022}, issn = {2520-1158}, doi = {10.1038/s41929-022-00744-z}, author = {Esterhuizen, Jacques A. and Goldsmith, Bryan R. and Linic, Suljo}, abstract = {Most applications of machine learning in heterogeneous catalysis thus far have used black-box models to predict computable physical properties (descriptors), such as adsorption or formation energies, that can be related to catalytic performance (that is, activity or stability). Extracting meaningful physical insights from these black-box models has proved challenging, as the internal logic of these black-box models is not readily interpretable due to their high degree of complexity. Interpretable machine learning methods that merge the predictive capacity of black-box models with the physical interpretability of physics-based models offer an alternative to black-box models. In this Perspective, we discuss the various interpretable machine learning methods available to catalysis researchers, highlight the potential of interpretable machine learning to accelerate hypothesis formation and knowledge generation, and outline critical challenges and opportunities for interpretable machine learning in heterogeneous catalysis.} }
@article{WOS:000899553000001, title = {Predictive models for concrete properties using machine learning and deep learning approaches: A review}, journal = {JOURNAL OF BUILDING ENGINEERING}, volume = {63}, year = {2023}, doi = {10.1016/j.jobe.2022.105444}, author = {Moein, Mohammad Mohtasham and Saradar, Ashkan and Rahmati, Komeil and Mousavinejad, Seyed Hosein Ghasemzadeh and Bristow, James and Aramali, Vartenie and Karakouzian, Moses}, abstract = {Concrete is one of the most widely used materials in various civil engineering applications. Its global production rate is increasing to meet demand. Mechanical properties of concrete are among important parameters in designing and evaluating its performance. Over the past few decades, machine learning has been used to model real-world problems. Machine learning, as a branch of artificial intelligence, is gaining popularity in many scientific fields such as robotics, statistics, bioinformatics, computer science, and construction materials. Machine learning has many advantages over statistical and experimental models, such as optimal accuracy, highperformance speed, responsiveness in complex environments, and economic cost-effectiveness. Recently, more researchers are looking into deep learning, which is a group of machine learning algorithms, as a powerful method in matters of diagnosis and classification. Hence, this paper provides a review of successful ML and DL model applications to predict concrete mechanical properties. Several modeling algorithms were reviewed highlighting their applications, performance, current knowledge gaps, and suggestions for future research. This paper will assist construction material engineers and researchers in selecting suitable and accurate techniques that fit their applications.} }
@article{WOS:000410555900032, title = {Quantum machine learning}, journal = {NATURE}, volume = {549}, pages = {195-202}, year = {2017}, issn = {0028-0836}, doi = {10.1038/nature23474}, author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth}, abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.} }
@article{WOS:001150877600003, title = {An Overview of Machine Learning for Asset Management}, journal = {JOURNAL OF PORTFOLIO MANAGEMENT}, volume = {49}, pages = {31-63}, year = {2023}, issn = {0095-4918}, author = {Lee, Yongjae and Thompson, John R. J. and Kim, Jang Ho and Kim, Woo Chang and Fabozzi, Francesco A.}, abstract = {Machine learning has been widely used in the asset management industry to improve operations and make data-driven decisions. This article provides an overview of machine learning for asset management by presenting various machine learning models in the context of their applications, including general classification and regression, time-series forecasting, natural language processing, dimension reduction, reinforcement learning, data generation, recommendation, and clustering. Additionally, it highlights the challenges of implementing machine learning in asset management, such as data quality and quantity, interpretability, and fairness.} }
@article{WOS:000917611300001, title = {Precision Machine Learning}, journal = {ENTROPY}, volume = {25}, year = {2023}, doi = {10.3390/e25010175}, author = {Michaud, Eric J. J. and Liu, Ziming and Tegmark, Max}, abstract = {We explore unique considerations involved in fitting machine learning (ML) models to data with very high precision, as is often required for science applications. We empirically compare various function approximation methods and study how they scale with increasing parameters and data. We find that neural networks (NNs) can often outperform classical approximation methods on high-dimensional examples, by (we hypothesize) auto-discovering and exploiting modular structures therein. However, neural networks trained with common optimizers are less powerful for low-dimensional cases, which motivates us to study the unique properties of neural network loss landscapes and the corresponding optimization challenges that arise in the high precision regime. To address the optimization issue in low dimensions, we develop training tricks which enable us to train neural networks to extremely low loss, close to the limits allowed by numerical precision.} }
@article{WOS:000998914800001, title = {Applied machine learning in hematopathology}, journal = {INTERNATIONAL JOURNAL OF LABORATORY HEMATOLOGY}, volume = {45}, pages = {87-94}, year = {2023}, issn = {1751-5521}, doi = {10.1111/ijlh.14110}, author = {Dehkharghanian, Taher and Mu, Youqing and Tizhoosh, Hamid R. and Campbell, Clinton J. V.}, abstract = {An increasing number of machine learning applications are being developed and applied to digital pathology, including hematopathology. The goal of these modern computerized tools is often to support diagnostic workflows by extracting and summarizing information from multiple data sources, including digital images of human tissue. Hematopathology is inherently multimodal and can serve as an ideal case study for machine learning applications. However, hematopathology also poses unique challenges compared to other pathology subspecialities when applying machine learning approaches. By modeling the pathologist workflow and thinking process, machine learning algorithms may be designed to address practical and tangible problems in hematopathology. In this article, we discuss the current trends in machine learning in hematopathology. We review currently available machine learning enabled medical devices supporting hematopathology workflows. We then explore current machine learning research trends of the field with a focus on bone marrow cytology and histopathology, and how adoption of new machine learning tools may be enabled through the transition to digital pathology.} }
@article{WOS:001011556200001, title = {Machine Learning Classification Model Comparison}, journal = {SOCIO-ECONOMIC PLANNING SCIENCES}, volume = {87}, year = {2023}, issn = {0038-0121}, doi = {10.1016/j.seps.2023.101560}, author = {Giudici, Paolo and Gramegna, Alex and Raffinetti, Emanuela}, abstract = {Machine learning models are boosting Artificial Intelligence applications in many domains, such as automotive, finance and health care. This is mainly due to their advantage, in terms of predictive accuracy, with respect to classic statistical models. However, machine learning models are much less explainable: less transparent, less interpretable. This paper proposes to improve machine learning models, by proposing a model selection methodology, based on Lorenz Zonoids, which allows to compare them in terms of predictive accuracy significant gains, leading to a selected model which maintains accuracy while improving explainability. We illustrate our proposal by means of simulated datasets and of a real credit scoring problem. The analysis of the former shows that the proposal improves alternative methods, based on the AUROC. The analysis of the latter shows that the proposal leads to models made up of two/three relevant variables that measure the profitability and the financial leverage of the companies asking for credit.} }
@article{WOS:001036662400001, title = {Machine learning in cardiology: Clinical application and basic research}, journal = {JOURNAL OF CARDIOLOGY}, volume = {82}, pages = {128-133}, year = {2023}, issn = {0914-5087}, doi = {10.1016/j.jjcc.2023.04.020}, author = {Komuro, Jin and Kusumoto, Dai and Hashimoto, Hisayuki and Yuasa, Shinsuke}, abstract = {Machine learning is a subfield of artificial intelligence. The quality and versatility of machine learning have been rapidly improving and playing a critical role in many aspects of social life. This trend is also observed in the med-ical field. Generally, there are three main types of machine learning: supervised, unsupervised, and reinforcement learning. Each type of learning is adequately selected for the purpose and type of data. In the field of medicine, various types of information are collected and used, and research using machine learning is becoming increas-ingly relevant. Many clinical studies are conducted using electronic health and medical records, including in the cardiovascular area. Machine learning has also been applied in basic research. Machine learning has been widely used for several types of data analysis, such as clustering of microarray analysis and RNA sequence anal-ysis. Machine learning is essential for genome and multi-omics analyses. This review summarizes the recent ad-vancements in the use of machine learning in clinical applications and basic cardiovascular research.\\& COPY; 2023 Japanese College of Cardiology. Published by Elsevier Ltd. All rights reserved.} }
@article{WOS:001133095300012, title = {pystacked: Stacking generalization and machine learning in Stata}, journal = {STATA JOURNAL}, volume = {23}, pages = {909-931}, year = {2023}, issn = {1536-867X}, doi = {10.1177/1536867X231212426}, author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E.}, abstract = {The pystacked command implements stacked generalization (Wolpert, 1992, Neural Networks 5: 241-259) for regression and binary classification via Python's scikit-learn. Stacking combines multiple supervised machine learners-the ``base'' or ``level-0'' learners-into one learner. The currently supported base learners include regularized regression, random forest, gradient boosted trees, support vector machines, and feed-forward neural nets (multilayer perceptron). pystacked can also be used as a ``regular'' machine learning program to fit one base learner and thus provides an easy-to-use application programming interface for scikit-learn's machine learning algorithms.} }
@article{WOS:001001420400001, title = {An enhanced Runge Kutta boosted machine learning framework for medical diagnosis}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {160}, year = {2023}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2023.106949}, author = {Qiao, Zenglin and Li, Lynn and Zhao, Xinchao and Liu, Lei and Zhang, Qian and Hechmi, Shili and Atri, Mohamed and Li, Xiaohua}, abstract = {With the development and maturity of machine learning methods, medical diagnosis aided with machine learning methods has become a popular method to assist doctors in diagnosing and treating patients. However, machine learning methods are greatly affected by their hyperparameters, for instance, the kernel parameter in kernel extreme learning machine (KELM) and the learning rate in residual neural networks (ResNet). If the hyperparameters are appropriately set, the performance of the classifier can be significantly improved. To boost the performance of the machine learning methods, this paper proposes to improve the Runge Kutta optimizer (RUN) to adaptively adjust the hyperparameters of the machine learning methods for medical diagnosis pur -poses. Although RUN has a solid mathematical theoretical foundation, there are still some performance defects when dealing with complex optimization problems. To remedy these defects, this paper proposes a new enhanced RUN method with a grey wolf mechanism and an orthogonal learning mechanism called GORUN. The superior performance of the GORUN was validated against other well-established optimizers on IEEE CEC 2017 bench-mark functions. Then, the proposed GORUN is employed to optimize the machine learning models, including the KELM and ResNet, to construct robust models for medical diagnosis. The performance of the proposed machine learning framework was validated on several medical data sets, and the experimental results have demonstrated its superiority.} }
@article{WOS:000985911200001, title = {Ensemble machine learning methods in screening electronic health records: A scoping review}, journal = {DIGITAL HEALTH}, volume = {9}, year = {2023}, issn = {2055-2076}, doi = {10.1177/20552076231173225}, author = {Stevens, Christophe A. T. and Lyons, Alexander R. M. and Dharmayat, I, Kanika and Mahani, Alireza and Ray, Kausik K. and Vallejo-Vaz, Antonio J. and Sharabiani, Mansour T. A.}, abstract = {BackgroundElectronic health records provide the opportunity to identify undiagnosed individuals likely to have a given disease using machine learning techniques, and who could then benefit from more medical screening and case finding, reducing the number needed to screen with convenience and healthcare cost savings. Ensemble machine learning models combining multiple prediction estimates into one are often said to provide better predictive performances than non-ensemble models. Yet, to our knowledge, no literature review summarises the use and performances of different types of ensemble machine learning models in the context of medical pre-screening. MethodWe aimed to conduct a scoping review of the literature reporting the derivation of ensemble machine learning models for screening of electronic health records. We searched EMBASE and MEDLINE databases across all years applying a formal search strategy using terms related to medical screening, electronic health records and machine learning. Data were collected, analysed, and reported in accordance with the PRISMA scoping review guideline. ResultsA total of 3355 articles were retrieved, of which 145 articles met our inclusion criteria and were included in this study. Ensemble machine learning models were increasingly employed across several medical specialties and often outperformed non-ensemble approaches. Ensemble machine learning models with complex combination strategies and heterogeneous classifiers often outperformed other types of ensemble machine learning models but were also less used. Ensemble machine learning models methodologies, processing steps and data sources were often not clearly described. ConclusionsOur work highlights the importance of deriving and comparing the performances of different types of ensemble machine learning models when screening electronic health records and underscores the need for more comprehensive reporting of machine learning methodologies employed in clinical research.} }
@article{WOS:001124950300001, title = {Bilevel optimization for automated machine learning: a new perspective on framework and algorithm}, journal = {NATIONAL SCIENCE REVIEW}, volume = {11}, year = {2023}, issn = {2095-5138}, doi = {10.1093/nsr/nwad292}, author = {Liu, Risheng and Lin, Zhouchen}, abstract = {Formulating the methodology of machine learning by bilevel optimization techniques provides a new perspective to understand and solve automated machine learning problems.} }
@article{WOS:001015283800001, title = {Low-Code Machine Learning Platforms: A Fastlane to Digitalization}, journal = {INFORMATICS-BASEL}, volume = {10}, year = {2023}, doi = {10.3390/informatics10020050}, author = {Raghavendran, Krishna Raj and Elragal, Ahmed}, abstract = {In the context of developing machine learning models, until and unless we have the required data engineering and machine learning development competencies as well as the time to train and test different machine learning models and tune their hyperparameters, it is worth trying out the automatic machine learning features provided by several cloud-based and cloud-agnostic platforms. This paper explores the possibility of generating automatic machine learning models with low-code experience. We developed criteria to compare different machine learning platforms for generating automatic machine learning models and presenting their results. Thereafter, lessons learned by developing automatic machine learning models from a sample dataset across four different machine learning platforms were elucidated. We also interviewed machine learning experts to conceptualize their domain-specific problems that automatic machine learning platforms can address. Results showed that automatic machine learning platforms can provide a fast track for organizations seeking the digitalization of their businesses. Automatic machine learning platforms help produce results, especially for time-constrained projects where resources are lacking. The contribution of this paper is in the form of a lab experiment in which we demonstrate how low-code platforms can provide a viable option to many business cases and, henceforth, provide a lane that is faster than the usual hiring and training of already scarce data scientists and to analytics projects that suffer from overruns.} }
@article{WOS:001096009300001, title = {Machine Learning in Gamification and Gamification in Machine Learning: A Systematic Literature Mapping}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app132011427}, author = {Swacha, Jakub and Gracel, Michal}, abstract = {Albeit in different ways, both machine learning and gamification have transfigured the user experience of information systems. Although both are hot research topics, so far, little attention has been paid to how these two technologies converge with each other. This relation is not obvious as while it is feasible to enhance gamification with machine learning, it is also feasible to support machine learning with gamification; moreover, there are applications in which machine learning and gamification are combined yet not directly connected. In this study, we aim to shed light on the use of both machine learning in gamification and gamification in machine learning, as well as the related topics of using gamification in machine learning education and machine learning in gamification research. By performing a systematic literature mapping, we not only identify prior works addressing these respective themes, but also analyze how their popularity evolved in time, investigate the areas of application reported by prior works, used machine learning techniques and software tools, as well as the character of research contribution and the character of evaluation results for works that presented them.} }
@article{WOS:001001117700011, title = {Introducing Machine Learning in Auditing Courses}, journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING}, volume = {20}, pages = {195-211}, year = {2023}, issn = {1554-1908}, doi = {10.2308/JETA-2022-017}, author = {Huang, Feiqi and Wang, Yunsen}, abstract = {The advances in machine learning have gained close attention from audit practitioners and standard setters. However, fewer than half of accounting programs teach predictive analysis, including machine learning. To develop students' knowledge and skills of machine learning in auditing applications, this study introduces machine learning to the accounting curriculum and presents a novel hands-on approach for teaching machine learning in auditing courses. The objective is to provide students who have no statistics background and programming skills with the basic knowledge of machine learning and hands-on exercises for predicting auditing tasks. In addition to instruction manuals, this study demonstrates an implementation of machine learning exercises in a graduate-level course.} }
@article{WOS:000760318100006, title = {Stable learning establishes some common ground between causal inference and machine learning}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {4}, pages = {110-115}, year = {2022}, doi = {10.1038/s42256-022-00445-z}, author = {Cui, Peng and Athey, Susan}, abstract = {Causal inference has recently attracted substantial attention in the machine learning and artificial intelligence community. It is usually positioned as a distinct strand of research that can broaden the scope of machine learning from predictive modelling to intervention and decision-making. In this Perspective, however, we argue that ideas from causality can also be used to improve the stronghold of machine learning, predictive modelling, if predictive stability, explainability and fairness are important. With the aim of bridging the gap between the tradition of precise modelling in causal inference and black-box approaches from machine learning, stable learning is proposed and developed as a source of common ground. This Perspective clarifies a source of risk for machine learning models and discusses the benefits of bringing causality into learning. We identify the fundamental problems addressed by stable learning, as well as the latest progress from both causal inference and learning perspectives, and we discuss relationships with explainability and fairness problems. Machine learning performs well at predictive modelling based on statistical correlations, but for high-stakes applications, more robust, explainable and fair approaches are required. Cui and Athey discuss the benefits of bringing causal inference into machine learning, presenting a stable learning approach.} }
@article{WOS:000819703300001, title = {Machine Learning for Organic Photovoltaic Polymers: A Minireview}, journal = {CHINESE JOURNAL OF POLYMER SCIENCE}, volume = {40}, pages = {870-876}, year = {2022}, issn = {0256-7679}, doi = {10.1007/s10118-022-2782-5}, author = {Mahmood, Asif and Irfan, Ahmad and Wang, Jin-Liang}, abstract = {Machine learning is a powerful tool that can provide a way to revolutionize the material science. Its use for the designing and screening of materials for polymer solar cells is also increasing. Search of efficient polymeric materials for solar cells is really difficult task. Researchers have synthesized and fabricated so many materials. Sorting the results and get feedback for further research requires an innovative approach. In this minireview, we provides brief introduction of machine learning. The importance of machine learning is also mentioned, and the application of machine learning for polymeric material design is discussed. The key challenges that are hindering the wide spread use of machine are discussed. Suggestions are also given to improve the use of data science. The predictions using machine learning maybe not highly accurate but it definitely better than no prediction at all.} }
@article{WOS:000760354200006, title = {Machine learning for multi-omics data integration in cancer}, journal = {ISCIENCE}, volume = {25}, year = {2022}, doi = {10.1016/j.isci.2022.103798}, author = {Cai, Zhaoxiang and Poulos, Rebecca C. and Liu, Jia and Zhong, Qing}, abstract = {Multi-omics data analysis is an important aspect of cancer molecular biology studies and has led to ground-breaking discoveries. Many efforts have been made to develop machine learning methods that automatically integrate omics data. Here, we review machine learning tools categorized as either general-purpose or task-specific, covering both supervised and unsupervised learning for integrative analysis of multi-omics data. We benchmark the performance of five machine learning approaches using data from the Cancer Cell Line Encyclopedia, reporting accuracy on cancer type classification and mean absolute error on drug response prediction, and evaluating runtime efficiency. This review provides recommendations to researchers regarding suitable machine learning method selection for their specific applications. It should also promote the development of novel machine learning methodologies for data integration, which will be essential for drug discovery, clinical trial design, and personalized treatments.} }
@article{WOS:001059183900006, title = {Machine learning for combustion}, journal = {ENERGY AND AI}, volume = {7}, year = {2022}, issn = {2666-5468}, doi = {10.1016/j.egyai.2021.100128}, author = {Zhou, Lei and Song, Yuntong and Ji, Weiqi and Wei, Haiqiao}, abstract = {Combustion science is an interdisciplinary study that involves nonlinear physical and chemical phenomena in time and length scales, including complex chemical reactions and fluid flows. Combustion widely supplies energy for powering vehicles, heating houses, generating electricity, cooking food, etc. The key to study combustion is to improve the combustion efficiency with minimum emission of pollutants. Machine learning facilitates datadriven techniques for handling large amounts of combustion data, either obtained through experiments or simulations under multiple spatiotemporal scales, thereby finding the hidden patterns underlying these data and promoting combustion research. This work presents an overview of studies on the applications of machine learning in combustion science fields over the past several decades. We introduce the fundamentals of machine learning and its usage in aiding chemical reactions, combustion modeling, combustion measurement, engine performance prediction and optimization, and fuel design. The opportunities and limitations of using machine learning in combustion studies are also discussed. This paper aims to provide readers with a portrait of what and how machine learning can be used in combustion research and to inspire researchers in their ongoing studies. Machine learning techniques are rapidly advancing in this era of big data, and there is high potential for exploring the combination between machine learning and combustion research and achieving remarkable results.} }
@article{WOS:000804596500003, title = {A comparative study of different machine learning methods for reservoir landslide displacement prediction}, journal = {ENGINEERING GEOLOGY}, volume = {298}, year = {2022}, issn = {0013-7952}, doi = {10.1016/j.enggeo.2022.106544}, author = {Wang, Yankun and Tang, Huiming and Huang, Jinsong and Wen, Tao and Ma, Junwei and Zhang, Junrong}, abstract = {ABSTR A C T This paper compares the performance of five popular machine learning methods, namely, particle swarm opti-mization-extreme learning machine (PSO-ELM), particle swarm optimization-kernel extreme learning machine (PSO-KELM), particle swarm optimization-support vector machine (PSO-SVM), particle swarm opti-mization-least squares support vector machine (PSO-LSSVM), and long short-term memory neural network (LSTM), in the prediction of reservoir landslide displacement. The Baishuihe, Shuping, and Baijiabao landslides in the Three Gorges reservoir area of China were used for case studies. Cumulative displacement was decom-posed into trend displacement and periodic displacement by the Hodrick-Prescott filter. The double exponential smoothing method and the five machine learning methods were used to predict the trend and periodic displacement, respectively. The five machine learning methods are compared in three aspects: highest single prediction accuracy, mean prediction accuracy, and prediction stability. The results show that no method per -formed the best for all three aspects in the three landslide cases. LSTM and PSO-ELM achieved better single prediction accuracy, but worse mean prediction accuracy and stability. PSO-KELM, PSO-LSSVM, and PSO-SVM always yielded consistent predictions with slight variations. On the whole, PSO-KELM and PSO-LSSVM are recommended for their superior mean prediction accuracy and prediction stability.} }
@article{WOS:000812536000080, title = {Data Poisoning Attacks on Federated Machine Learning}, journal = {IEEE INTERNET OF THINGS JOURNAL}, volume = {9}, pages = {11365-11375}, year = {2022}, issn = {2327-4662}, doi = {10.1109/JIOT.2021.3128646}, author = {Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Lyu, Lingjuan and Liu, Ji}, abstract = {Federated machine learning which enables resource-constrained node devices (e.g., Internet of Things (IoT) devices and smartphones) to establish a knowledge-shared model while keeping the raw data local, could provide privacy preservation, and economic benefit by designing an effective communication protocol. However, this communication protocol can be adopted by attackers to launch data poisoning attacks for different nodes, which has been shown as a big threat to most machine learning models. Therefore, we in this article intend to study the model vulnerability of federated machine learning, and even on IoT systems. To be specific, we here attempt to attacking a popular federated multitask learning framework, which uses a general multitask learning framework to handle statistical challenges in the federated learning setting. The problem of calculating optimal poisoning attacks on federated multitask learning is formulated as a bilevel program, which is adaptive to the arbitrary selection of target nodes and source attacking nodes. We then propose a novel systems-aware optimization method, called as attack on federated learning (AT(2)FL), to efficiently derive the implicit gradients for poisoned data, and further attain optimal attack strategies in the federated machine learning. This is an earlier work, to our knowledge, that explores attacking federated machine learning via data poisoning. Finally, experiments on several real-world data sets demonstrate that when the attackers directly poison the target nodes or indirectly poison the related nodes via using the communication protocol, the federated multitask learning model is sensitive to both poisoning attacks.} }
@article{WOS:000851470400007, title = {SUBSTITUTING HUMAN DECISION-MAKING WITH MACHINE LEARNING: IMPLICATIONS FOR ORGANIZATIONAL LEARNING}, journal = {ACADEMY OF MANAGEMENT REVIEW}, volume = {47}, pages = {448-465}, year = {2022}, issn = {0363-7425}, doi = {10.5465/amr.2019.0470}, author = {Balasubramanian, Natarajan and Ye, Yang and Xu, Mingtao}, abstract = {The richness of organizational learning relies on the ability of humans to develop diverse patterns of action by actively engaging with their environments and applying substantive rationality. The substitution of human decision-making with machine learning has the potential to alter this richness of organizational learning. Though machine learning is significantly faster and seemingly unconstrained by human cognitive limitations and inflexibility, it is not true sentient learning and relies on formal statistical analysis for decision-making. We propose that the distinct differences between human learning and machine learning risk decreasing the within-organizational diversity in organizational routines and the extent of causal, contextual, and general knowledge associated with routines. We theorize that these changes may affect organizational learning by exacerbating the myopia of learning, and highlight some important contingencies that may mute or amplify the risk of such myopia.} }
@article{WOS:000863168300001, title = {Naive automated machine learning}, journal = {MACHINE LEARNING}, volume = {112}, pages = {1131-1170}, year = {2023}, issn = {0885-6125}, doi = {10.1007/s10994-022-06200-0}, author = {Mohr, Felix and Wever, Marcel}, abstract = {An essential task of automated machine learning (AutoML) is the problem of automatically finding the pipeline with the best generalization performance on a given dataset. This problem has been addressed with sophisticated black-box optimization techniques such as Bayesian optimization, grammar-based genetic algorithms, and tree search algorithms. Most of the current approaches are motivated by the assumption that optimizing the components of a pipeline in isolation may yield sub-optimal results. We present Naive AutoML, an approach that precisely realizes such an in-isolation optimization of the different components of a pre-defined pipeline scheme. The returned pipeline is obtained by just taking the best algorithm of each slot. The isolated optimization leads to substantially reduced search spaces, and, surprisingly, this approach yields comparable and sometimes even better performance than current state-of-the-art optimizers.} }
@article{WOS:000838252900001, title = {Open-environment machine learning}, journal = {NATIONAL SCIENCE REVIEW}, volume = {9}, year = {2022}, issn = {2095-5138}, doi = {10.1093/nsr/nwac123}, author = {Zhou, Zhi-Hua}, abstract = {Conventional machine learning studies generally assume close-environment scenarios where important factors of the learning process hold invariant. With the great success of machine learning, nowadays, more and more practical tasks, particularly those involving open-environment scenarios where important factors are subject to change, called open-environment machine learning in this article, are present to the community. Evidently, it is a grand challenge for machine learning turning from close environment to open environment. It becomes even more challenging since, in various big data tasks, data are usually accumulated with time, like streams, while it is hard to train the machine learning model after collecting all data as in conventional studies. This article briefly introduces some advances in this line of research, focusing on techniques concerning emerging new classes, decremental/incremental features, changing data distributions and varied learning objectives, and discusses some theoretical issues. This article briefly introduces Open Environment Machine Learning, where important factors of the machine learning process are subject to change, as occurring in many practical tasks.} }
@article{WOS:000852243600006, title = {Cognitive Workload Recognition Using EEG Signals and Machine Learning: A Review}, journal = {IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS}, volume = {14}, pages = {799-818}, year = {2022}, issn = {2379-8920}, doi = {10.1109/TCDS.2021.3090217}, author = {Zhou, Yueying and Huang, Shuo and Xu, Ziming and Wang, Pengpai and Wu, Xia and Zhang, Daoqiang}, abstract = {Machine learning and its subfield deep learning techniques provide opportunities for the development of operator mental state monitoring, especially for cognitive workload recognition using electroencephalogram (EEG) signals. Although a variety of machine learning methods have been proposed for recognizing cognitive workload via EEG recently, there does not yet exist a review that covers in-depth the application of machine learning methods. To alleviate this gap, in this article, we survey cognitive workload and machine learning literature to identify the approaches and highlight the primary advances. To be specific, we first introduce the concepts of cognitive workload and machine learning. Then, we discuss the steps of classical machine learning for cognitive workload recognition from the following aspects, i.e., EEG data preprocessing, feature extraction and selection, classification method, and evaluation methods. Further, we review the commonly used deep learning methods for this domain. Finally, we expound on the open problem and future outlooks.} }
@article{WOS:000816020800006, title = {Understanding from Machine Learning Models}, journal = {BRITISH JOURNAL FOR THE PHILOSOPHY OF SCIENCE}, volume = {73}, pages = {109-133}, year = {2022}, issn = {0007-0882}, doi = {10.1093/bjps/axz035}, author = {Sullivan, Emily}, abstract = {Simple idealized models seem to provide more understanding than opaque, complex, and hyper-realistic models. However, an increasing number of scientists are going in the opposite direction by utilizing opaque machine learning models to make predictions and draw inferences, suggesting that scientists are opting for models that have less potential for understanding. Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided? In this article, using the case of deep neural networks, I argue that it is not the complexity or black box nature of a model that limits how much understanding the model provides. Instead, it is a lack of scientific and empirical evidence supporting the link that connects a model to the target phenomenon that primarily prohibits understanding.} }
@article{WOS:000819852500009, title = {Machine learning models and over-fitting considerations}, journal = {WORLD JOURNAL OF GASTROENTEROLOGY}, volume = {28}, pages = {605-607}, year = {2022}, issn = {1007-9327}, doi = {10.3748/wjg.v28.i5.605}, author = {Charilaou, Paris and Battat, Robert}, abstract = {Machine learning models may outperform traditional statistical regression algorithms for predicting clinical outcomes. Proper validation of building such models and tuning their underlying algorithms is necessary to avoid over-fitting and poor generalizability, which smaller datasets can be more prone to. In an effort to educate readers interested in artificial intelligence and model-building based on machine-learning algorithms, we outline important details on cross-validation techniques that can enhance the performance and generalizability of such models.} }
@article{WOS:000602863200001, title = {Machine learning for combinatorial optimization: A methodological tour d'horizon}, journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH}, volume = {290}, pages = {405-421}, year = {2021}, issn = {0377-2217}, doi = {10.1016/j.ejor.2020.07.063}, author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine}, abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000868715700001, title = {A Review on Machine Learning Styles in Computer Vision-Techniques and Future Directions}, journal = {IEEE ACCESS}, volume = {10}, pages = {107293-107329}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3209825}, author = {Mahadevkar, V, Supriya and Khemani, Bharti and Patil, Shruti and Kotecha, Ketan and Vora, Deepali R. and Abraham, Ajith and Gabralla, Lubna Abdelkareim}, abstract = {Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.} }
@article{WOS:000794033200007, title = {Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods}, journal = {JOURNAL OF BUSINESS RESEARCH}, volume = {144}, pages = {93-106}, year = {2022}, issn = {0148-2963}, doi = {10.1016/j.jbusres.2022.01.076}, author = {van Giffen, Benjamin and Herhausen, Dennis and Fahse, Tobias}, abstract = {Over the last decade, the importance of machine learning increased dramatically in business and marketing. However, when machine learning is used for decision-making, bias rooted in unrepresentative datasets, inade-quate models, weak algorithm designs, or human stereotypes can lead to low performance and unfair decisions, resulting in financial, social, and reputational losses. This paper offers a systematic, interdisciplinary literature review of machine learning biases as well as methods to avoid and mitigate these biases. We identified eight distinct machine learning biases, summarized these biases in the cross-industry standard process for data mining to account for all phases of machine learning projects, and outline twenty-four mitigation methods. We further contextualize these biases in a real-world case study and illustrate adequate mitigation strategies. These insights synthesize the literature on machine learning biases in a concise manner and point to the importance of human judgment for machine learning algorithms.} }
@article{WOS:000913331400001, title = {Machine learning accelerates the materials discovery}, journal = {MATERIALS TODAY COMMUNICATIONS}, volume = {33}, year = {2022}, doi = {10.1016/j.mtcomm.2022.104900}, author = {Fang, Jiheng and Xie, Ming and He, Xingqun and Zhang, Jiming and Hu, Jieqiong and Chen, Yongtai and Yang, Youcai and Jin, Qinglin}, abstract = {As the big data generated by the development of modern experiments and computing technology becomes more and more accessible, the material design method based on machine learning (ML) has opened a new paradigm for materials science research. With its ability to automatically solve complex tasks, machine learning is being used as a new method to help discover the relevance of materials, understand materials' properties, and accelerate the discovery of materials. This paper first introduces the general process of machine learning in materials science. Secondly, the applications of machine learning in material properties prediction, classification and identification, auxiliary micro-scale characterization, phase transformation research and phase diagram construction, process optimization, service behavior evaluation, accelerating the development of computational simulation technology, multi-objective optimization and inverse design of materials are reviewed. Finally, we discuss the main challenges and possible solutions in machine learning, and predict the potential research directions.} }
@article{WOS:000737778100001, title = {Applying machine learning to study fluid mechanics}, journal = {ACTA MECHANICA SINICA}, volume = {37}, pages = {1718-1726}, year = {2021}, issn = {0567-7718}, doi = {10.1007/s10409-021-01143-6}, author = {Brunton, Steven L.}, abstract = {This paper provides a short overview of how to use machine learning to build data-driven models in fluid mechanics. The process of machine learning is broken down into five stages: (1) formulating a problem to model, (2) collecting and curating training data to inform the model, (3) choosing an architecture with which to represent the model, (4) designing a loss function to assess the performance of the model, and (5) selecting and implementing an optimization algorithm to train the model. At each stage, we discuss how prior physical knowledge may be embedding into the process, with specific examples from the field of fluid mechanics.} }
@article{WOS:000761920900004, title = {Machine learning in the quantum realm: The state-of-the-art, challenges, and future vision}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {194}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.116512}, author = {Houssein, Essam H. and Abohashima, Zainab and Elhoseny, Mohamed and Mohamed, Waleed M.}, abstract = {Machine learning has become a ubiquitous and effective technique for data processing and classification. Furthermore, due to the superiority and progress of quantum computing in many areas (e.g., cryptography, machine learning, healthcare), a combination of classical machine learning and quantum information processing has established a new field, called, quantum machine learning. One of the most frequently used applications of quantum computing is machine learning. This paper aims to present a comprehensive review of state-of-the-art advances in quantum machine learning. Besides, this paper outlines recent works on different architectures of quantum deep learning, and illustrates classification tasks in the quantum domain as well as encoding methods and quantum subroutines. Furthermore, this paper examines how the concept of quantum computing enhances classical machine learning. Two methods for improving the performance of classical machine learning are presented. Finally, this work provides a general review of challenges and the future vision of quantum machine learning.} }
@article{WOS:001156335200001, title = {Machine learning in energy storage materials}, journal = {INTERDISCIPLINARY MATERIALS}, volume = {1}, pages = {175-195}, year = {2022}, issn = {2767-4401}, doi = {10.1002/idm2.12020}, author = {Shen, Zhong-Hui and Liu, Han-Xing and Shen, Yang and Hu, Jia-Mian and Chen, Long-Qing and Nan, Ce-Wen}, abstract = {With its extremely strong capability of data analysis, machine learning has shown versatile potential in the revolution of the materials research paradigm. Here, taking dielectric capacitors and lithium-ion batteries as two representative examples, we review substantial advances of machine learning in the research and development of energy storage materials. First, a thorough discussion of the machine learning framework in materials science is presented. Then, we summarize the applications of machine learning from three aspects, including discovering and designing novel materials, enriching theoretical simulations, and assisting experimentation and characterization. Finally, a brief outlook is highlighted to spark more insights on the innovative implementation of machine learning in materials science.} }
@article{WOS:000789003800004, title = {A Survey on Large-Scale Machine Learning}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {34}, pages = {2574-2594}, year = {2022}, issn = {1041-4347}, doi = {10.1109/TKDE.2020.3015777}, author = {Wang, Meng and Fu, Weijie and He, Xiangnan and Hao, Shijie and Wu, Xindong}, abstract = {Machine learning can provide deep insights into data, allowing machines to make high-quality predictions and having been widely used in real-world applications, such as text mining, visual classification, and recommender systems. However, most sophisticated machine learning approaches suffer from huge time costs when operating on large-scale data. This issue calls for the need of Large-scale Machine Learning (LML), which aims to learn patterns from big data with comparable performance efficiently. In this paper, we offer a systematic survey on existing LML methods to provide a blueprint for the future developments of this area. We first divide these LML methods according to the ways of improving the scalability: 1) model simplification on computational complexities, 2) optimization approximation on computational efficiency, and 3) computation parallelism on computational capabilities. Then we categorize the methods in each perspective according to their targeted scenarios and introduce representative methods in line with intrinsic strategies. Lastly, we analyze their limitations and discuss potential directions as well as open issues that are promising to address in the future.} }
@article{WOS:000781177700001, title = {Scientific machine learning benchmarks}, journal = {NATURE REVIEWS PHYSICS}, volume = {4}, pages = {413-420}, year = {2022}, doi = {10.1038/s42254-022-00441-7}, author = {Thiyagalingam, Jeyan and Shankar, Mallikarjun and Fox, Geoffrey and Hey, Tony}, abstract = {Finding the most appropriate machine learning algorithm for the analysis of any given scientific dataset is currently challenging, but new machine learning benchmarks for science are being developed to help. Deep learning has transformed the use of machine learning technologies for the analysis of large experimental datasets. In science, such datasets are typically generated by large-scale experimental facilities, and machine learning focuses on the identification of patterns, trends and anomalies to extract meaningful scientific insights from the data. In upcoming experimental facilities, such as the Extreme Photonics Application Centre (EPAC) in the UK or the international Square Kilometre Array (SKA), the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis. However, at present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is a challenge due to the potential applicability of many different machine learning frameworks, computer architectures and machine learning models. Historically, for modelling and simulation on high-performance computing systems, these issues have been addressed through benchmarking computer applications, algorithms and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to open, curated scientific datasets is a new challenge for both scientists and computer scientists. Here, we introduce the concept of machine learning benchmarks for science and review existing approaches. As an example, we describe the SciMLBench suite of scientific machine learning benchmarks.} }
@article{WOS:000754182500001, title = {New Opportunity: Machine Learning for Polymer Materials Design and Discovery}, journal = {ADVANCED THEORY AND SIMULATIONS}, volume = {5}, year = {2022}, doi = {10.1002/adts.202100565}, author = {Xu, Pengcheng and Chen, Huimin and Li, Minjie and Lu, Wencong}, abstract = {Under the guidance of the material genome initiative (MGI), the use of data-driven methods to discover new materials has become an innovation of materials science. The polymer materials have been one of the most important parts in materials science for the excellent physical and chemical properties as well as corresponding complex structures. Machine learning, as the core of data-driven methods, has taken an important place in polymer materials design and discovery. In this review, the authors have introduced the applications of machine learning in the design and discovery of polymer materials. The development tendency of published papers about machine learning in polymer materials, the commonly used algorithms, the polymer descriptors, the workflow of machine learning in polymer materials, and recent progresses of machine learning in materials are summarized. Then, the detail of how to use machine learning to assist design and discovery of polymer materials is fully discussed combined with two cases. Finally, the opportunities and challenges on the future development prospects of machine learning in the field of polymer materials are proposed.} }
@article{WOS:000884152000002, title = {Unsupervised machine learning methods and emerging applications in healthcare}, journal = {KNEE SURGERY SPORTS TRAUMATOLOGY ARTHROSCOPY}, volume = {31}, pages = {376-381}, year = {2023}, issn = {0942-2056}, doi = {10.1007/s00167-022-07233-7}, author = {Eckhardt, Christina M. and Madjarova, Sophia J. and Williams, Riley J. and Ollivier, Mattheu and Karlsson, Jon and Pareek, Ayoosh and Nwachukwu, Benedict U.}, abstract = {Unsupervised machine learning methods are important analytical tools that can facilitate the analysis and interpretation of high-dimensional data. Unsupervised machine learning methods identify latent patterns and hidden structures in high-dimensional data and can help simplify complex datasets. This article provides an overview of key unsupervised machine learning techniques including K-means clustering, hierarchical clustering, principal component analysis, and factor analysis. With a deeper understanding of these analytical tools, unsupervised machine learning methods can be incorporated into health sciences research to identify novel risk factors, improve prevention strategies, and facilitate delivery of personalized therapies and targeted patient care.} }
@article{WOS:000479252200001, title = {Recent advances and applications of machine learning in solid-state materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, doi = {10.1038/s41524-019-0221-0}, author = {Schmidt, Jonathan and Marques, Mario R. G. and Botti, Silvana and Marques, Miguel A. L.}, abstract = {One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure-property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.} }
@article{WOS:000761186500001, title = {Machine Learning and Deep Learning Approaches for CyberSecurity: A Review}, journal = {IEEE ACCESS}, volume = {10}, pages = {19572-19585}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3151248}, author = {Halbouni, Asmaa and Gunawan, Teddy Surya and Habaebi, Mohamed Hadi and Halbouni, Murad and Kartiwi, Mira and Ahmad, Robiah}, abstract = {The rapid evolution and growth of the internet through the last decades led to more concern about cyber-attacks that are continuously increasing and changing. As a result, an effective intrusion detection system was required to protect data, and the discovery of artificial intelligence's sub-fields, machine learning, and deep learning, was one of the most successful ways to address this problem. This paper reviewed intrusion detection systems and discussed what types of learning algorithms machine learning and deep learning are using to protect data from malicious behavior. It discusses recent machine learning and deep learning work with various network implementations, applications, algorithms, learning approaches, and datasets to develop an operational intrusion detection system.} }
@article{WOS:000870821400025, title = {Technology readiness levels for machine learning systems}, journal = {NATURE COMMUNICATIONS}, volume = {13}, year = {2022}, doi = {10.1038/s41467-022-33128-9}, author = {Lavin, Alexander and Gilligan-Lee, Ciaran M. and Visnjic, Alessya and Ganju, Siddha and Newman, Dava and Ganguly, Sujoy and Lange, Danny and Baydin, Atilim Gunes and Sharma, Amit and Gibson, Adam and Zheng, Stephan and Xing, Eric P. and Mattmann, Chris and Parr, James and Gal, Yarin}, abstract = {The development of machine learning systems has to ensure their robustness and reliability. The authors introduce a framework that defines a principled process of machine learning system formation, from research to production, for various domains and data scenarios. The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. Lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, with mission critical measures and robustness throughout the process. Drawing on experience in both spacecraft engineering and machine learning (research through product across domain areas), we've developed a proven systems engineering approach for machine learning and artificial intelligence: the Machine Learning Technology Readiness Levels framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for machine learning workflows, including key distinctions from traditional software engineering, and a lingua franca for people across teams and organizations to work collaboratively on machine learning and artificial intelligence technologies. Here we describe the framework and elucidate with use-cases from physics research to computer vision apps to medical diagnostics.} }
@article{WOS:000765501200001, title = {Application of machine learning for advanced material prediction and design}, journal = {ECOMAT}, volume = {4}, year = {2022}, doi = {10.1002/eom2.12194}, author = {Chan, Cheuk Hei and Sun, Mingzi and Huang, Bolong}, abstract = {In material science, traditional experimental and computational approaches require investing enormous time and resources, and the experimental conditions limit the experiments. Sometimes, traditional approaches may not yield satisfactory results for the desired purpose. Therefore, it is essential to develop a new approach to accelerate experimental progress and avoid unnecessary wasting of time and resources. As a data-driven method, machine learning provides reliable and accurate performance to solve problems in material science. This review first outlines the fundamental information of machine learning. It continues with the research concerning the prediction of various properties of materials by machine learning. Then it discusses the methods for the discovery of new materials and the prediction of their structural information. Finally, we summarize other applications of machine learning in material science. This review will be beneficial for future application of machine learning in more material science research.} }
@article{WOS:000674857200001, title = {Principles and Practice of Explainable Machine Learning}, journal = {FRONTIERS IN BIG DATA}, volume = {4}, year = {2021}, doi = {10.3389/fdata.2021.688969}, author = {Belle, Vaishak and Papantonis, Ioannis}, abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods-machine learning (ML) and pattern recognition models in particular-so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.} }
@article{WOS:000599821400007, title = {Machine learning applications for building structural design and performance assessment: State-of-the-art review}, journal = {JOURNAL OF BUILDING ENGINEERING}, volume = {33}, year = {2021}, doi = {10.1016/j.jobe.2020.101816}, author = {Sun, Han and Burton, Henry V. and Huang, Honglan}, abstract = {Machine learning models have been shown to be useful for predicting and assessing structural performance, identifying structural condition and informing preemptive and recovery decisions by extracting patterns from data collected via various sources and media. This paper presents a review of the historical development and recent advances in the application of machine learning to the area of building structural design and performance assessment. To this end, an overview of machine learning theory and the most relevant algorithms is provided with the goal of identifying problems suitable for machine learning and the appropriate models to use. The machine learning applications in building structural design and performance assessment are then reviewed in four main categories: (1) predicting structural response and performance, (2) interpreting experimental data and formulating models to predict component-level structural properties, (3) information retrieval using images and written text and (4) recognizing patterns in structural health monitoring data. The challenges of bringing machine learning into structural engineering practice are identified, and future research opportunities are discussed.} }
@article{WOS:000649545300034, title = {A review of machine learning in building load prediction}, journal = {APPLIED ENERGY}, volume = {285}, year = {2021}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2021.116452}, author = {Zhang, Liang and Wen, Jin and Li, Yanfei and Chen, Jianli and Ye, Yunyang and Fu, Yangyang and Livingood, William}, abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.} }
@article{WOS:000606751200009, title = {Comparative analysis of image classification algorithms based on traditional machine learning and deep learning}, journal = {PATTERN RECOGNITION LETTERS}, volume = {141}, pages = {61-67}, year = {2021}, issn = {0167-8655}, doi = {10.1016/j.patrec.2020.07.042}, author = {Wang, Pin and Fan, En and Wang, Peng}, abstract = {Image classification is a hot research topic in today's society and an important direction in the field of image processing research. SVM is a very powerful classification model in machine learning. CNN is a type of feedforward neural network that includes convolution calculation and has a deep structure. It is one of the representative algorithms of deep learning. Taking SVM and CNN as examples, this paper compares and analyzes the traditional machine learning and deep learning image classification algorithms. This study found that when using a large sample mnist dataset, the accuracy of SVM is 0.88 and the accuracy of CNN is 0.98; when using a small sample COREL1000 dataset, the accuracy of SVM is 0.86 and the accuracy of CNN is 0.83. The experimental results in this paper show that traditional machine learning has a better solution effect on small sample data sets, and deep learning framework has higher recognition accuracy on large sample data sets. (C) 2020 Published by Elsevier B.V.} }
@article{WOS:000663421300008, title = {Using machine learning approaches for multi-omics data analysis: A review}, journal = {BIOTECHNOLOGY ADVANCES}, volume = {49}, year = {2021}, issn = {0734-9750}, doi = {10.1016/j.biotechadv.2021.107739}, author = {Reel, Parminder S. and Reel, Smarti and Pearson, Ewan and Trucco, Emanuele and Jefferson, Emily}, abstract = {With the development of modern high-throughput omic measurement platforms, it has become essential for biomedical studies to undertake an integrative (combined) approach to fully utilise these data to gain insights into biological systems. Data from various omics sources such as genetics, proteomics, and metabolomics can be integrated to unravel the intricate working of systems biology using machine learning-based predictive algorithms. Machine learning methods offer novel techniques to integrate and analyse the various omics data enabling the discovery of new biomarkers. These biomarkers have the potential to help in accurate disease prediction, patient stratification and delivery of precision medicine. This review paper explores different integrative machine learning methods which have been used to provide an in-depth understanding of biological systems during normal physiological functioning and in the presence of a disease. It provides insight and recommendations for interdisciplinary professionals who envisage employing machine learning skills in multi-omics studies.} }
@article{WOS:000675035000001, title = {Machine learning for alloys}, journal = {NATURE REVIEWS MATERIALS}, volume = {6}, pages = {730-755}, year = {2021}, issn = {2058-8437}, doi = {10.1038/s41578-021-00340-w}, author = {Hart, Gus L. W. and Mueller, Tim and Toher, Cormac and Curtarolo, Stefano}, abstract = {Alloy modelling has a history of machine-learning-like approaches, preceding the tide of data-science-inspired work. The dawn of computational databases has made the integration of analysis, prediction and discovery the key theme in accelerated alloy research. Advances in machine-learning methods and enhanced data generation have created a fertile ground for computational materials science. Pairing machine learning and alloys has proven to be particularly instrumental in pushing progress in a wide variety of materials, including metallic glasses, high-entropy alloys, shape-memory alloys, magnets, superalloys, catalysts and structural materials. This Review examines the present state of machine-learning-driven alloy research, discusses the approaches and applications in the field and summarizes theoretical predictions and experimental validations. We foresee that the partnership between machine learning and alloys will lead to the design of new and improved systems. Machine learning is enabling a metallurgical renaissance. This Review discusses recent progress in representations, descriptors and interatomic potentials, overviewing metallic glasses, high-entropy alloys, superalloys and shape-memory alloys, magnets and catalysts, and the prediction of mechanical and thermal properties.} }
@article{WOS:000626617900002, title = {A Survey on Data Collection for Machine Learning: A Big Data-AI Integration Perspective}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {33}, pages = {1328-1347}, year = {2021}, issn = {1041-4347}, doi = {10.1109/TKDE.2019.2946162}, author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong}, abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.} }
@article{WOS:000658723000005, title = {Power of data in quantum machine learning}, journal = {NATURE COMMUNICATIONS}, volume = {12}, year = {2021}, doi = {10.1038/s41467-021-22539-9}, author = {Huang, Hsin-Yuan and Broughton, Michael and Mohseni, Masoud and Babbush, Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.}, abstract = {The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits. Expectations for quantum machine learning are high, but there is currently a lack of rigorous results on which scenarios would actually exhibit a quantum advantage. Here, the authors show how to tell, for a given dataset, whether a quantum model would give any prediction advantage over a classical one.} }
@article{WOS:000644444900006, title = {When Machine Learning Meets Privacy: A Survey and Outlook}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3436755}, author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai}, abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.} }
@article{WOS:000656549000001, title = {Best practices in machine learning for chemistry comment}, journal = {NATURE CHEMISTRY}, volume = {13}, pages = {505-508}, year = {2021}, issn = {1755-4330}, doi = {10.1038/s41557-021-00716-z}, author = {Artrith, Nongnuch and Butler, Keith T. and Coudert, Francois-Xavier and Han, Seungwu and Isayev, Olexandr and Jain, Anubhav and Walsh, Aron}, abstract = {Statistical tools based on machine learning are becoming integrated into chemistry research workflows. We discuss the elements necessary to train reliable, repeatable and reproducible models, and recommend a set of guidelines for machine learning reports.} }
@article{WOS:000628819200010, title = {Application of supervised machine learning paradigms in the prediction of petroleum reservoir properties: Comparative analysis of ANN and SVM models}, journal = {JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING}, volume = {200}, year = {2021}, issn = {0920-4105}, doi = {10.1016/j.petrol.2020.108182}, author = {Otchere, Daniel Asante and Ganat, Tarek Omar Arbi and Gholami, Raoof and Ridha, Syahrir}, abstract = {The advent of Artificial Intelligence (AI) in the petroleum industry has seen an increase in its use in exploration, development, production, reservoir engineering and management planning to accelerate decision making, reduce cost and time. Supervised machine learning has gained much popularity in establishing a relationship between complex non-linear datasets. This type of machine learning algorithm has showcased its superiority over petroleum engineering regression techniques in terms of prediction errors for high dimensional data, computational power and memory. This review focuses on the most widely used machine learning algorithm employed in the petroleum industry, the Artificial Neural Network (ANN) with different shallow models used in reservoir characterisation. The Support Vector Machine (SVM) and Relevant Vector Machine (RVM) has over the years emerged as competitive algorithms where in most cases based on this review it outperformed the ANN. This makes it preferable than the ANN when there are limited data sets. Finally, hybridisation of multiple algorithms methodologies also showed improved performance over singularly applied algorithms offering a pathway in improving reservoir characterisation based on supervised machine learning as future scope of work.} }
@article{WOS:000611850000003, title = {Machine learning for high performance organic solar cells: current scenario and future prospects}, journal = {ENERGY \\& ENVIRONMENTAL SCIENCE}, volume = {14}, pages = {90-105}, year = {2021}, issn = {1754-5692}, doi = {10.1039/d0ee02838j}, author = {Mahmood, Asif and Wang, Jin-Liang}, abstract = {Machine learning (ML) is a field of computer science that uses algorithms and techniques for automating solutions to complex problems that are hard to program using conventional programming methods. Owing to the chemical versatility of organic building blocks, a large number of organic semi-conductors have been used for organic solar cells. Selecting a suitable organic semi-conductor is like searching for a needle in a haystack. Data-driven science, the fourth paradigm of science, has the potential to guide experimentalists to discover and develop new high-performance materials. The last decade has seen impressive progress in materials informatics and data science; however, data-driven molecular design of organic solar cell materials is still challenging. The data-analysis capability of machine learning methods is well known. This review is written about the use of machine learning methods for organic solar cell research. In this review, we have outlined the basics of machine learning and common procedures for applying machine learning. A brief introduction on different classes of machine learning algorithms as well as related software and tools is provided. Then, the current research status of machine learning in organic solar cells is reviewed. We have discussed the challenges in anticipating the data driven material design, such as the complexity metric of organic solar cells, diversity of chemical structures and necessary programming ability. We have also proposed some suggestions that can enhance the usefulness of machine learning for organic solar cell research enterprises.} }
@article{WOS:000799950300002, title = {Machine learning in subsurface geothermal energy: Two decades in review}, journal = {GEOTHERMICS}, volume = {102}, year = {2022}, issn = {0375-6505}, doi = {10.1016/j.geothermics.2022.102401}, author = {Okoroafor, Esuru Rita and Smith, Connor M. and Ochie, Karen Ifeoma and Nwosu, Chinedu Joseph and Gudmundsdottir, Halldora and Aljubran, Mohammad (Jabs)}, abstract = {This paper reviews the trends in applying machine learning to subsurface geothermal resource development. The review is focused on the machine learning applications over the past two decades (from 2002 to 2021) to determine which machine learning algorithms are being used. In addition, the review seeks to determine what types of problems are being addressed with machine learning and how machine learning is aiding decisionmaking and problem-solving for subsurface aspects of the geothermal industry. The study shows that there has been a steady increase in the application of machine learning in the geothermal industry over the past 20 years, with an exponential increase in machine learning applications from 2018 to 2021. Several research areas associated with geothermal resource development were reviewed, including exploration, drilling, reservoir characterization, seismicity, petrophysics, reservoir engineering, and production and injection engineering. The study reveals that the field of reservoir characterization had the most significant applications of machine learning in the geothermal industry. Though machine learning has been applied across all the geothermal research areas we investigated, this study shows that there are still opportunities to improve and expand the adoption of machine learning in exploration, drilling, and seismicity. The main challenges that would need to be addressed are ensuring researchers have access to data, curating the data to be suitable for machine learning, and training geothermal industry students and professionals on artificial intelligence related to the energy sector.} }
@article{WOS:000930523100002, title = {Shear strength prediction of reinforced concrete beams using machine learning}, journal = {STRUCTURES}, volume = {47}, pages = {1196-1211}, year = {2023}, issn = {2352-0124}, doi = {10.1016/j.istruc.2022.11.140}, author = {Sandeep, M. S. and Tiprak, Koravith and Kaewunruen, Sakdirat and Pheinsusom, Phoonsak and Pansuk, Withit}, abstract = {Recent years have witnessed a surge in the application of machine learning techniques for solving hard to solve structural engineering problems. The application of machine learning can replace the use of empirical and semiempirical prediction models currently used in practice with highly accurate models. This paper provides a detailed discussion on the basic terminologies and concepts of commonly used machine learning algorithms for solving structural engineering problems. To provide confidence to use this method and show the potential of machine learning in accurately predicting the results of complex civil engineering problems, a comprehensive literature review on the application of machine learning in shear strength prediction is also presented. The literature review covers the application of different machine learning algorithms in predicting the shear strength of conventional concrete beams, steel fibre reinforced concrete beams, beams reinforced with FRP bars as well as high strength concrete beams. Major observations, challenges and future scope in this field are also discussed in detail. This article will be a valuable resource for individuals who are unfamiliar with machine learning yet aspire to learn more about it.} }
@article{WOS:000880854200004, title = {Using machine learning in photovoltaics to create smarter and cleaner energy generation systems: A comprehensive review}, journal = {JOURNAL OF CLEANER PRODUCTION}, volume = {364}, year = {2022}, issn = {0959-6526}, doi = {10.1016/j.jclepro.2022.132701}, author = {Sohani, Ali and Sayyaadi, Hoseyn and Cornaro, Cristina and Shahverdian, Mohammad Hassan and Pierro, Marco and Moser, David and Karimi, Nader and Doranehgard, Mohammad Hossein and Li, Larry K. B.}, abstract = {Photovoltaic (PV) technologies are expected to play an increasingly important role in future energy production. In parallel, machine learning has gained prominence because of a combination of factors such as advances in computational hardware, data collection and storage, and data-driven algorithms. Against this backdrop, we provide a comprehensive review of machine learning techniques applied to PV systems. First, conventional methods for modeling PV systems are introduced from both electrical and thermal perspectives. Then, the application of machine learning to the analysis of PV systems is discussed. We focus on reviewing the use of machine learning algorithms to predict performance and detect faults, and on discussing how machine learning can help humanity to achieve a cleaner environment in the worldwide drive towards carbon neutrality. This review also discusses the challenges to and future directions of using machine learning to analyze PV systems. A key conclusion is that the use of machine learning to analyze PV systems is still in its infancy, with many small-scale PV technologies, such as building integrated photovoltaic thermal systems (BIPV/T), not yet benefiting fully in terms of system efficiency and economic viability. The wider application of machine learning to PV systems could therefore forge a shorter path towards sustainable energy production.} }
@article{WOS:000803594800001, title = {Machine Learning and Application in Terahertz Technology: A Review on Achievements and Future Challenges}, journal = {IEEE ACCESS}, volume = {10}, pages = {53761-53776}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3174595}, author = {Jiang, Yuying and Li, Guangming and Ge, Hongyi and Wang, Faye and Li, Li and Chen, Xinyu and Lu, Ming and Zhang, Yuan}, abstract = {Terahertz (THz) radiation (0.1 similar to 10 THz) shows great potential in agricultural products detection, biomedical, and security inspection in recent years. Machine learning methods are widely used to support the user demand of higher efficiency and high prediction accuracy. The technological and key challenges of machine learning methods are for THz spectroscopy and image data preprocessing, reconstruction algorithms, and qualitative and quantitative analysis. In this paper, an exhaustive review of recent related works of THz detection and imaging techniques and machine learning methods are presented. The application of machine learning methods combined with THz technology in quality inspection of agricultural products, biomedical, security inspection, and materials science are highlighted. Challenges of machine learning methods for these applications are addressed. The development trend and future perspectives of THz technology are also discussed.} }
@article{WOS:000797748400001, title = {Machine learning and density functional theory}, journal = {NATURE REVIEWS PHYSICS}, volume = {4}, pages = {357-358}, year = {2022}, doi = {10.1038/s42254-022-00470-2}, author = {Pederson, Ryan and Kalita, Bhupalee and Burke, Kieron}, abstract = {Over the past decade machine learning has made significant advances in approximating density functionals, but whether this signals the end of human-designed functionals remains to be seen. Ryan Pederson, Bhupalee Kalita and Kieron Burke discuss the rise of machine learning for functional design.} }
@article{WOS:001466774000001, title = {Cybersecurity Threats and Their Mitigation Approaches Using Machine Learning-A Review}, journal = {JOURNAL OF CYBERSECURITY AND PRIVACY}, volume = {2}, pages = {527-555}, year = {2022}, doi = {10.3390/jcp2030027}, author = {Ahsan, Mostofa and Nygard, Kendall E. and Gomes, Rahul and Chowdhury, Md Minhaz and Rifat, Nafiz and Connolly, Jayden F.}, abstract = {Machine learning is of rising importance in cybersecurity. The primary objective of applying machine learning in cybersecurity is to make the process of malware detection more actionable, scalable and effective than traditional approaches, which require human intervention. The cybersecurity domain involves machine learning challenges that require efficient methodical and theoretical handling. Several machine learning and statistical methods, such as deep learning, support vector machines and Bayesian classification, among others, have proven effective in mitigating cyber-attacks. The detection of hidden trends and insights from network data and building of a corresponding data-driven machine learning model to prevent these attacks is vital to design intelligent security systems. In this survey, the focus is on the machine learning techniques that have been implemented on cybersecurity data to make these systems secure. Existing cybersecurity threats and how machine learning techniques have been used to mitigate these threats have been discussed. The shortcomings of these state-of-the-art models and how attack patterns have evolved over the past decade have also been presented. Our goal is to assess how effective these machine learning techniques are against the ever-increasing threat of malware that plagues our online community.} }
@article{WOS:000819919700004, title = {Choice modelling in the age of machine learning Discussion paper}, journal = {JOURNAL OF CHOICE MODELLING}, volume = {42}, year = {2022}, issn = {1755-5345}, doi = {10.1016/j.jocm.2021.100340}, author = {van Cranenburgh, Sander and Wang, Shenhao and Vij, Akshay and Pereira, Francisco and Walker, Joan}, abstract = {Since its inception, the choice modelling field has been dominated by theory-driven modelling approaches. Machine learning offers an alternative data-driven approach for modelling choice behaviour and is increasingly drawing interest in our field. Cross-pollination of machine learning models, techniques and practices could help overcome problems and limitations encountered in the current theory-driven modelling paradigm, such as subjective labour-intensive search processes for model selection, and the inability to work with text and image data. However, despite the potential benefits of using the advances of machine learning to improve choice modelling practices, the choice modelling field has been hesitant to embrace machine learning. This discussion paper aims to consolidate knowledge on the use of machine learning models, techniques and practices for choice modelling, and discuss their potential. Thereby, we hope not only to make the case that further integration of machine learning in choice modelling is beneficial, but also to further facilitate it. To this end, we clarify the similarities and differences between the two modelling paradigms; we review the use of machine learning for choice modelling; and we explore areas of opportunities for embracing machine learning models and techniques to improve our practices. To conclude this discussion paper, we put forward a set of research questions which must be addressed to better understand if and how machine learning can benefit choice modelling.} }
@article{WOS:000652706400001, title = {MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21062222}, author = {Kang, Jaeyong and Ullah, Zahid and Gwak, Jeonghwan}, abstract = {Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.} }
@article{WOS:000731150400004, title = {Machine learning techniques for analysis of hyperspectral images to determine quality of food products: A review}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {4}, pages = {28-44}, year = {2021}, doi = {10.1016/j.crfs.2021.01.002}, author = {Saha, Dhritiman and Manickavasagan, Annamalai}, abstract = {Non-destructive testing techniques have gained importance in monitoring food quality over the years. Hyperspectral imaging is one of the important non-destructive quality testing techniques which provides both spatial and spectral information. Advancement in machine learning techniques for rapid analysis with higher classification accuracy have improved the potential of using this technique for food applications. This paper provides an overview of the application of different machine learning techniques in analysis of hyperspectral images for determination of food quality. It covers the principle underlying hyperspectral imaging, the advantages, and the limitations of each machine learning technique. The machine learning techniques exhibited rapid analysis of hyperspectral images of food products with high accuracy thereby enabling robust classification or regression models. The selection of effective wavelengths from the hyperspectral data is of paramount importance since it greatly reduces the computational load and time which enhances the scope for real time applications. Due to the feature learning nature of deep learning, it is one of the most promising and powerful techniques for real time applications. However, the field of deep learning is relatively new and need further research for its full utilization. Similarly, lifelong machine learning paves the way for real time HSI applications but needs further research to incorporate the seasonal variations in food quality. Further, the research gaps in machine learning techniques for hyperspectral image analysis, and the prospects are discussed.} }
@article{WOS:000835498400003, title = {Data poisoning attacks against machine learning algorithms}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {208}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.118101}, author = {Yerlikaya, Fahri Anil and Bahtiyar, Serif}, abstract = {For the past decade, machine learning technology has increasingly become popular and it has been contributing to many areas that have the potential to influence the society considerably. Generally, machine learning is used by various industries to enhance their performances. Moreover, machine learning algorithms are used to solve some hard problems of systems that may contain very critical information. This makes machine learning algorithms a target of adversaries, which is an important problem for systems that use such algorithms. Therefore, it is significant to determine the performance and the robustness of a machine learning algorithm against attacks. In this paper, we analyze empirically the robustness and performances of six machine learning algorithms against two types of adversarial attacks by using four different datasets and three metrics. In our experiments, we analyze the robustness of Support Vector Machine, Stochastic Gradient Descent, Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest Neighbor algorithms to create learning models. We observe their performances in spam, botnet, malware, and cancer detection datasets when we launch adversarial attacks against these environments. We use data poisoning for manipulating training data during adversarial attacks, which are random label flipping and distance-based label flipping attacks. We analyze the performance of each algorithm for a specific dataset by modifying the amount of poisoned data and analyzing behaviors of accuracy rate, f1-score, and AUC score. Analyses results show that machine learning algorithms have various performance results and robustness under different adversarial attacks. Moreover, machine learning algorithms are affected differently in each stage of an adversarial attacks. Furthermore, the behavior of a machine learning algorithm highly depends on the type of the dataset. On the other hand, some machine learning algorithms have better robustness and performance results against adversarial attacks for almost all datasets.} }
@article{WOS:000799943600006, title = {A review on machine learning and deep learning for various antenna design applications}, journal = {HELIYON}, volume = {8}, year = {2022}, doi = {10.1016/j.heliyon.2022.e09317}, author = {Khan, Mohammad Monirujjaman and Hossain, Sazzad and Mozumdar, Puezia and Akter, Shamima and Ashique, Ratil H.}, abstract = {The next generation of wireless communication networks will rely heavily on machine learning and deep learning. In comparison to traditional ground-based systems, the development of various communication-based applications is projected to increase coverage and spectrum efficiency. Machine learning and deep learning can be used to optimize solutions in a variety of applications, including antennas. The latter have grown popular for obtaining effective solutions due to high computational processing, clean data, and large data storage capability. In this research, machine learning and deep learning for various antenna design applications have been discussed in detail. The general concept of machine learning and deep learning is introduced. However, the main focus is on various antenna applications, such as millimeter wave, body-centric, terahertz, satellite, unmanned aerial vehicle, global positioning system, and textiles. The feasibility of antenna applications with respect to conventional methods, acceleration of the antenna design process, reduced number of simulations, and better computational feasibility features are highlighted. Overall, machine learning and deep learning provide satisfactory results for antenna design.} }
@article{WOS:000880551800002, title = {Artificial intelligence and machine learning}, journal = {ELECTRONIC MARKETS}, volume = {32}, pages = {2235-2244}, year = {2022}, issn = {1019-6781}, doi = {10.1007/s12525-022-00598-0}, author = {Kuehl, Niklas and Schemmer, Max and Goutier, Marc and Satzger, Gerhard}, abstract = {Within the last decade, the application of ``artificial intelligence'' and ``machine learning'' has become popular across multiple disciplines, especially in information systems. The two terms are still used inconsistently in academia and industry-sometimes as synonyms, sometimes with different meanings. With this work, we try to clarify the relationship between these concepts. We review the relevant literature and develop a conceptual framework to specify the role of machine learning in building (artificial) intelligent agents. Additionally, we propose a consistent typology for AI-based information systems. We contribute to a deeper understanding of the nature of both concepts and to more terminological clarity and guidance-as a starting point for interdisciplinary discussions and future research.} }
@article{WOS:000818226100001, title = {Review on Interpretable Machine Learning in Smart Grid}, journal = {ENERGIES}, volume = {15}, year = {2022}, doi = {10.3390/en15124427}, author = {Xu, Chongchong and Liao, Zhicheng and Li, Chaojie and Zhou, Xiaojun and Xie, Renyou}, abstract = {In recent years, machine learning, especially deep learning, has developed rapidly and has shown remarkable performance in many tasks of the smart grid field. The representation ability of machine learning algorithms is greatly improved, but with the increase of model complexity, the interpretability of machine learning algorithms is worse. The smart grid is a critical infrastructure area, so machine learning models involving it must be interpretable in order to increase user trust and improve system reliability. Unfortunately, the black-box nature of most machine learning models remains unresolved, and many decisions of intelligent systems still lack explanation. In this paper, we elaborate on the definition, motivations, properties, and classification of interpretability. In addition, we review the relevant literature addressing interpretability for smart grid applications. Finally, we discuss the future research directions of interpretable machine learning in the smart grid.} }
@article{WOS:000762430500001, title = {Artificial intelligence and machine learning in emergency medicine: a narrative review}, journal = {ACUTE MEDICINE \\& SURGERY}, volume = {9}, year = {2022}, issn = {2052-8817}, doi = {10.1002/ams2.740}, author = {Mueller, Brianna and Kinoshita, Takahiro and Peebles, Alexander and Graber, Mark A. and Lee, Sangil}, abstract = {Aim: The emergence and evolution of artificial intelligence (AI) has generated increasing interest in machine learning applications for health care. Specifically, researchers are grasping the potential of machine learning solutions to enhance the quality of care in emergency medicine. Methods: We undertook a narrative review of published works on machine learning applications in emergency medicine and provide a synopsis of recent developments. Results: This review describes fundamental concepts of machine learning and presents clinical applications for triage, risk stratification specific to disease, medical imaging, and emergency department operations. Additionally, we consider how machine learning models could contribute to the improvement of causal inference in medicine, and to conclude, we discuss barriers to safe implementation of AI. Conclusion: We intend that this review serves as an introduction to AI and machine learning in emergency medicine.} }
@article{WOS:000787825500001, title = {Integration of machine learning and first principles models}, journal = {AICHE JOURNAL}, volume = {68}, year = {2022}, issn = {0001-1541}, doi = {10.1002/aic.17715}, author = {Rajulapati, Lokesh and Chinta, Sivadurgaprasad and Shyamala, Bala and Rengaswamy, Raghunathan}, abstract = {Model building and parameter estimation are traditional concepts widely used in chemical, biological, metallurgical, and manufacturing industries. Early modeling methodologies focused on mathematically capturing the process knowledge and domain expertise of the modeler. The models thus developed are termed first principles models (or white-box models). Over time, computational power became cheaper, and massive amounts of data became available for modeling. This led to the development of cutting edge machine learning models (black-box models) and artificial intelligence (AI) techniques. Hybrid models (gray-box models) are a combination of first principles and machine learning models. The development of hybrid models has captured the attention of researchers as this combines the best of both modeling paradigms. Recent attention to this field stems from the interest in explainable AI (XAI), a critical requirement as AI systems become more pervasive. This work aims at identifying and categorizing various hybrid models available in the literature that integrate machine-learning models with different forms of domain knowledge. Benefits such as enhanced predictive power, extrapolation capabilities, and other advantages of combining the two approaches are summarized. The goal of this article is to consolidate the published corpus in the area of hybrid modeling and develop a comprehensive framework to understand the various techniques presented. This framework can further be used as the foundation to explore rational associations between several models.} }
@article{WOS:001072225000001, title = {Machine learning-guided property prediction of energetic materials: Recent advances, challenges, and perspectives}, journal = {ENERGETIC MATERIALS FRONTIERS}, volume = {3}, pages = {177-186}, year = {2022}, doi = {10.1016/j.enmf.2022.07.005}, author = {Tian, Xiao-lan and Song, Si-wei and Chen, Fang and Qi, Xiu-juan and Wang, Yi and Zhang, Qing-hua}, abstract = {Predicting chemical properties is one of the most important applications of machine learning. In recent years, the prediction of the properties of energetic materials using machine learning has been receiving more attention. This review summarized recent advances in predicting energetic compounds' properties (e.g., density, detonation velocity, enthalpy of formation, sensitivity, the heat of the explosion, and decomposition temperature) using machine learning. Moreover, it presented general steps for applying machine learning to the prediction of practical chemical properties from the aspects of data, molecular representation, algorithms, and general accu-racy. Additionally, it raised some controversies specific to machine learning in energetic materials and its possible development directions. Machine learning is expected to become a new power for driving the development of energetic materials soon.} }
@article{WOS:000908283400019, title = {Exploring teachers' preconceptions of teaching machine learning in high school: A preliminary insight from Africa}, journal = {COMPUTERS AND EDUCATION OPEN}, volume = {3}, year = {2022}, issn = {2666-5573}, doi = {10.1016/j.caeo.2021.100072}, author = {Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday and Omidiora, Joseph Olamide}, abstract = {The teaching of machine learning is now considered essential and relevant in schools globally. Despite the ongoing discourse and increased research in the emerging field, teachers' conceptions of machine learning remain under-researched. This study aims at filling the gap by describing the initial conceptions of teaching machine learning by 12 African in-service teachers. We detailed the result of a phenomenographic analysis of teachers' pre-conceptions on teaching machine learning in K-12 settings. Twelve high school (Grades 10-12) computer science teachers in some selected African countries were recruited for a semi-structured interview. Five categories emerged from the analysis of the semi-structured interviews as follows: supporting student technical knowledge, having knowledge of the concept, focusing on professional development practices, contextualizing teaching resources and tools, and sustainability for development goals. These involve the relevance of teaching machine learning, the pedagogical approaches, strategies, and sustainability relating to practical implementation in schools. The results suggest the need to train in-service teachers to use existing tools designed for introducing machine learning. The teachers should also be involved in the co-designing process of resources considering contextual factors and, significantly, the curriculum to integrate machine learning into mainstream education. Involving teachers in the development process would help contextualize machine learning, contributing to real impact and societal changes.} }
@article{WOS:000833855900006, title = {A review of ultrasonic sensing and machine learning methods to monitor industrial processes}, journal = {ULTRASONICS}, volume = {124}, year = {2022}, issn = {0041-624X}, doi = {10.1016/j.ultras.2022.106776}, author = {Bowler, Alexander L. and Pound, Michael P. and Watson, Nicholas J.}, abstract = {Supervised machine learning techniques are increasingly being combined with ultrasonic sensor measurements owing to their strong performance. These techniques also offer advantages over calibration procedures of more complex fitting, improved generalisation, reduced development time, ability for continuous retraining, and the correlation of sensor data to important process information. However, their implementation requires expertise to extract and select appropriate features from the sensor measurements as model inputs, select the type of machine learning algorithm to use, and find a suitable set of model hyperparameters. The aim of this article is to facilitate implementation of machine learning techniques in combination with ultrasonic measurements for in-line and online monitoring of industrial processes and other similar applications. The article first reviews the use of ultrasonic sensors for monitoring processes, before reviewing the combination of ultrasonic measurements and machine learning. We include literature from other sectors such as structural health monitoring. This review covers feature extraction, feature selection, algorithm choice, hyperparameter selection, data augmentation, domain adaptation, semi-supervised learning and machine learning interpretability. Finally, recommendations for applying machine learning to the reviewed processes are made.} }
@article{WOS:000895081000015, title = {A Systematic Review of Machine Learning Techniques for GNSS Use Cases}, journal = {IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS}, volume = {58}, pages = {5043-5077}, year = {2022}, issn = {0018-9251}, doi = {10.1109/TAES.2022.3219366}, author = {Siemuri, Akpojoto and Selvan, Kannan and Kuusniemi, Heidi and Valisuo, Petri and Elmusrati, Mohammed S.}, abstract = {In terms of the availability and accuracy of positioning, navigation, and timing (PNT), the traditional Global Navigation Satellite System (GNSS) algorithms and models perform well under good signal conditions. In order to improve their robustness and performance in less than optimal signal environments, many researchers have proposed machine learning (ML) based GNSS models (ML models) as early as the 1990s. However, no study has been done in a systematic way to analyze the extent of the research on the utilization of ML models in GNSS and their performance. In this study, we perform a systematic review of studies from 2000 to 2021 in the literature that utilizes machine learning techniques in GNSS use cases. We assess the performance of the machine learning techniques in the existing literature on their application to GNSS. Furthermore, the strengths and weaknesses of machine learning techniques are summarized. In this paper, we have identified 213 selected studies and ten categories of machine learning techniques. The results prove the acceptable performance of machine learning techniques in several GNSS use cases. In most cases, the models using the machine learning techniques in these GNSS use cases outperform the traditional GNSS models. ML models are promising in their utilization in GNSS. However, the application of ML models in the industry is still limited. More effort and incentives are needed to facilitate the utilization of ML models in the PNT context. Therefore, based on the findings of this review, we provide recommendations for researchers and guidelines for practitioners.} }
@article{WOS:000741323700002, title = {Selecting an appropriate supervised machine learning algorithm for predictive maintenance}, journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY}, volume = {119}, pages = {4277-4301}, year = {2022}, issn = {0268-3768}, doi = {10.1007/s00170-021-08551-9}, author = {Ouadah, Abdelfettah and Zemmouchi-Ghomari, Leila and Salhi, Nedjma}, abstract = {Predictive maintenance refers to predicting malfunctions using data from monitoring equipment and process performance measurements. Machine learning algorithms and techniques are often used to analyze equipment monitoring data. Machine learning is the process in which a computer can work more precisely by collecting and analyzing data. It is often the case that machine learning algorithms use supervised learning, in which labelled data is used to feed the algorithm. However, there are many supervised machine learning algorithms available. Therefore, choosing the best-supervised machine learning algorithm to resolve predictive maintenance issues is not trivial. This paper aims to increase the performance of predictive maintenance and achieve its goals by selecting the most suitable supervised machine learning algorithm. Based on the most commonly used criteria in research articles, we selected three supervised machine learning algorithms from a comparative study: Random forest, Decision tree and KNN. We then tested selected algorithms on data from real-world and simulation scenarios. Finally, we conducted the experiment based on vibration analysis and reliability evaluation. We noticed that Random forests and Decision trees obtained slightly the same performance. KNN is a better classification algorithm for extensive volumes of data; on the contrary, Random forest performs better in the case of small datasets.} }
@article{WOS:000768294100005, title = {Conceptual challenges for interpretable machine learning}, journal = {SYNTHESE}, volume = {200}, year = {2022}, issn = {0039-7857}, doi = {10.1007/s11229-022-03485-5}, author = {Watson, David S.}, abstract = {As machine learning has gradually entered into ever more sectors of public and private life, there has been a growing demand for algorithmic explainability. How can we make the predictions of complex statistical models more intelligible to end users? A subdiscipline of computer science known as interpretable machine learning (IML) has emerged to address this urgent question. Numerous influential methods have been proposed, from local linear approximations to rule lists and counterfactuals. In this article, I highlight three conceptual challenges that are largely overlooked by authors in this area. I argue that the vast majority of IML algorithms are plagued by (1) ambiguity with respect to their true target; (2) a disregard for error rates and severe testing; and (3) an emphasis on product over process. Each point is developed at length, drawing on relevant debates in epistemology and philosophy of science. Examples and counterexamples from IML are considered, demonstrating how failure to acknowledge these problems can result in counterintuitive and potentially misleading explanations. Without greater care for the conceptual foundations of IML, future work in this area is doomed to repeat the same mistakes.} }
@article{WOS:000848617400011, title = {A Machine Learning Tutorial for Operational Meteorology. Part I: Traditional Machine Learning}, journal = {WEATHER AND FORECASTING}, volume = {37}, pages = {1509-1529}, year = {2022}, issn = {0882-8156}, doi = {10.1175/WAF-D-22-0070.1}, author = {Chase, Randy J. and Harrison, David R. and Burke, Amanda and Lackmann, Gary M. and McGovern, Amy}, abstract = {Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are ``black boxes'' and thus end-users are hesitant to apply the machine learning methods in their everyday workflow. To reduce the opaqueness of machine learning methods and lower hesitancy toward machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression, logistic regression, decision trees, random forest, gradient boosted decision trees, naive Bayes, and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyze the use of machine learning in meteorology.} }
@article{WOS:000855096700002, title = {Application of tabular data synthesis using generative adversarial networks on machine learning-based multiaxial fatigue life prediction}, journal = {INTERNATIONAL JOURNAL OF PRESSURE VESSELS AND PIPING}, volume = {199}, year = {2022}, issn = {0308-0161}, doi = {10.1016/j.ijpvp.2022.104779}, author = {He, GaoYuan and Zhao, YongXiang and Yan, ChuLiang}, abstract = {Machine learning has gradually developed into a new and effective scheme for fatigue life prediction. The novelty of this work is the proposal and verification of using virtual synthetic multiaxial fatigue data as input of machine learning models. First, the data generated by tabular generative adversarial networks are applied to machine learning models for life prediction. Then based on equivalent stress (strain) amplitude-life relationship curve, a multiaxial fatigue data generation evaluation metric is proposed. Finally, the effect of the generated sample size on the predictions of machine learning models is investigated. The method is demonstrated on 5 multiaxial fatigue data sets. The results indicate the synthetic data help machine learning models arrive at good life prediction ability. Using this method will help expand the application of machine learning-based multiaxial fatigue life prediction.} }
@article{WOS:000838619300001, title = {Quantum Machine Learning Applications in the Biomedical Domain: A Systematic Review}, journal = {IEEE ACCESS}, volume = {10}, pages = {80463-80484}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3195044}, author = {Maheshwari, Danyal and Garcia-Zapirain, Begonya and Sierra-Sosa, Daniel}, abstract = {Quantum technologies have become powerful tools for a wide range of application disciplines, which tend to range from chemistry to agriculture, natural language processing, and healthcare due to exponentially growing computational power and advancement in machine learning algorithms. Furthermore, the processing of classical data and machine learning algorithms in the quantum domain has given rise to an emerging field like quantum machine learning. Recently, quantum machine learning has become quite a challenging field in the case of healthcare applications. As a result, quantum machine learning has become a common and effective technique for data processing and classification across a wide range of domains. Consequently, quantum machine learning is the most commonly used application of quantum computing. The main objective of this work is to present a brief overview of current state-of-the-art published articles between 2013 and 2021 to identify, analyze, and classify the different QML algorithms and applications in the biomedical field. Furthermore, the approach adheres to the requirements for conducting systematic literature review techniques such as research questions and quality metrics of the articles. Initially, we discovered 3149 articles, excluded the 2847 papers, and read the 121 full papers. Therefore, this research compiled 30 articles that comply with the quantum machine learning models and quantum circuits using biomedical data. Eventually, this article provides a broad overview of quantum machine learning limitations and future prospects.} }
@article{WOS:000856097600001, title = {Quantum-Inspired Machine Learning for 6G: Fundamentals, Security, Resource Allocations, Challenges, and Future Research Directions}, journal = {IEEE OPEN JOURNAL OF VEHICULAR TECHNOLOGY}, volume = {3}, pages = {375-387}, year = {2022}, doi = {10.1109/OJVT.2022.3202876}, author = {Duong, Trung Q. and Ansere, James Adu and Narottama, Bhaskara and Sharma, Vishal and Dobre, Octavia A. and Shin, Hyundong}, abstract = {Quantum computing is envisaged as an evolving paradigm for solving computationally complex optimization problems with a large-number factorization and exhaustive search. Recently, there has been a proliferating growth of the size of multi-dimensional datasets, the input-output space dimensionality, and data structures. Hence, the conventional machine learning approaches in data training and processing have exhibited their limited computing capabilities to support the sixth-generation (6G) networks with highly dynamic applications and services. In this regard, the fast developing quantum computing with machine learning for 6G networks is investigated. Quantum machine learning algorithm can significantly enhance the processing efficiency and exponentially computational speed-up for effective quantum data representation and superposition framework, highly capable of guaranteeing high data storage and secured communications. We present the state-of-the-art in quantum computing and provide a comprehensive overview of its potential, via machine learning approaches. Furthermore, we introduce quantum-inspired machine learning applications for 6G networks in terms of resource allocation and network security, considering their enabling technologies and potential challenges. Finally, some dominating research issues and future research directions for the quantum-inspired machine learning in 6G networks are elaborated.} }
@article{WOS:000913916400001, title = {Machine learning in fermentative biohydrogen production: Advantages, challenges, and applications}, journal = {BIORESOURCE TECHNOLOGY}, volume = {370}, year = {2023}, issn = {0960-8524}, doi = {10.1016/j.biortech.2022.128502}, author = {Pandey, Ashutosh Kumar and Park, Jungsu and Ko, Jeun and Joo, Hwan-Hong and Raj, Tirath and Singh, Lalit Kumar and Singh, Noopur and Kim, Sang-Hyoun}, abstract = {Hydrogen can be produced in an environmentally friendly manner through biological processes using a variety of organic waste and biomass as feedstock. However, the complexity of biological processes limits their predictability and reliability, which hinders the scale-up and dissemination. This article reviews contemporary research and perspectives on the application of machine learning in biohydrogen production technology. Several machine learning algorithems have recently been implemented for modeling the nonlinear and complex relationships among operational and performance parameters in biohydrogen production as well as predicting the process performance and microbial population dynamics. Reinforced machine learning methods exhibited precise state prediction and retrieved the underlying kinetics effectively. Machine-learning based prediction was also improved by using microbial sequencing data as input parameters. Further research on machine learning could be instrumental in designing a process control tool to maintain reliable hydrogen production performance and identify connection between the process performance and the microbial population.} }
@article{WOS:000862449600001, title = {An introduction to machine learning for classification and prediction}, journal = {FAMILY PRACTICE}, year = {2022}, issn = {0263-2136}, doi = {10.1093/fampra/cmac104}, author = {Black, Jason E. and Kueper, Jacqueline K. and Williamson, Tyler S.}, abstract = {Classification and prediction tasks are common in health research. With the increasing availability of vast health data repositories (e.g. electronic medical record databases) and advances in computing power, traditional statistical approaches are being augmented or replaced with machine learning (ML) approaches to classify and predict health outcomes. ML describes the automated process of identifying (''learning'') patterns in data to perform tasks. Developing an ML model includes selecting between many ML models (e.g. decision trees, support vector machines, neural networks); model specifications such as hyperparameter tuning; and evaluation of model performance. This process is conducted repeatedly to find the model and corresponding specifications that optimize some measure of model performance. ML models can make more accurate classifications and predictions than their statistical counterparts and confer greater flexibility when modelling unstructured data or interactions between covariates; however, many ML models require larger sample sizes to achieve good classification or predictive performance and have been criticized as ``black box'' for their poor transparency and interpretability. ML holds potential in family medicine for risk profiling of patients' disease risk and clinical decision support to present additional information at times of uncertainty or high demand. In the future, ML approaches are positioned to become commonplace in family medicine. As such, it is important to understand the objectives that can be addressed using ML approaches and the associated techniques and limitations. This article provides a brief introduction into the use of ML approaches for classification and prediction tasks in family medicine.} }
@article{WOS:000847846800001, title = {Machine learning for energy-resource allocation, workflow scheduling and live migration in cloud computing: State-of-the-art survey}, journal = {SUSTAINABLE COMPUTING-INFORMATICS \\& SYSTEMS}, volume = {36}, year = {2022}, issn = {2210-5379}, doi = {10.1016/j.suscom.2022.100780}, author = {Kumar, Yogesh and Kaul, Surabhi and Hu, Yu-Chen}, abstract = {Machine learning and artificial intelligence techniques have been proven helpful when pragmatic to a wide range of complex problems and areas such as energy optimization, workflow scheduling, video gaming, and cloud computing. When machine learning and cloud computing algorithms are combined, they help achieve better outcomes by providing the improved performance of cloud data centers compared to solutions currently employed by various researchers. It is also helpful for migrating the virtual machines based on the current traffic condition and fluctuation due to network congestion and bandwidth availability. The survey aims to present the improvement in dynamic load allocation, task scheduling, energy optimization, live migration, mobile cloud computing, and security on the cloud using machine learning classification. Machine learning algorithms are prevailing analytical approaches that allow machines to identify patterns and simplify the human learning process. The flow of the paper consists of an introduction part, motivation, and background study, including a framework for cloud-machine learning integration, best practices of introducing machine learning in cloud computing, and the objective of the work. The paper also highlights the machine learning-based cloud services and the role of artificial intelligence in different cloud computing platforms. This comprehensive study provides mindfulness and valuable facilities to the researchers by giving thorough studies about various machine learning algorithms and their applicability in cloud computing.} }
@article{WOS:000747379200007, title = {Machine learning, artificial intelligence and the prediction of dementia}, journal = {CURRENT OPINION IN PSYCHIATRY}, volume = {35}, pages = {123-129}, year = {2022}, issn = {0951-7367}, doi = {10.1097/YCO.0000000000000768}, author = {Merkin, Alexander and Krishnamurthi, Rita and Medvedev, Oleg N.}, abstract = {Purpose of review Artificial intelligence and its division machine learning are emerging technologies that are increasingly applied in medicine. Artificial intelligence facilitates automatization of analytical modelling and contributes to prediction, diagnostics and treatment of diseases. This article presents an overview of the application of artificial intelligence in dementia research. Recent findings Machine learning and its branch Deep Learning are widely used in research to support in diagnosis and prediction of dementia. Deep Learning models in certain tasks often result in better accuracy of detection and prediction of dementia than traditional machine learning methods, but they are more costly in terms of run times and hardware requirements. Both machine learning and Deep Learning models have their own strengths and limitations. Currently, there are few datasets with limited data available to train machine learning models. There are very few commercial applications of machine learning in medical practice to date, mostly represented by mobile applications, which include questionnaires and psychometric assessments with limited machine learning data processing. Application of machine learning technologies in detection and prediction of dementia may provide an advantage to psychiatry and neurology by promoting a better understanding of the nature of the disease and more accurate evidence-based processes that are reproducible and standardized.} }
@article{WOS:000795191700002, title = {Dropout prediction in Moocs using deep learning and machine learning}, journal = {EDUCATION AND INFORMATION TECHNOLOGIES}, volume = {27}, pages = {11499-11513}, year = {2022}, issn = {1360-2357}, doi = {10.1007/s10639-022-11068-7}, author = {Basnet, Ram B. and Johnson, Clayton and Doleck, Tenzin}, abstract = {The nature of teaching and learning has evolved over the years, especially as technology has evolved. Innovative application of educational analytics has gained momentum. Indeed, predictive analytics have become increasingly salient in education. Considering the prevalence of learner-system interaction data and the potential value of such data, it is not surprising that significant scholarly attention has been directed at understanding ways of drawing insights from educational data. Although prior literature on educational big data recognizes the utility of deep learning and machine learning methods, little research examines both deep learning and machine learning together, and the differences in predictive performance have been relatively understudied. This paper aims to present a comprehensive comparison of predictive performance using deep learning and machine learning. Specifically, we use educational big data in the context of predicting dropout in MOOCs. We find that machine learning classifiers can predict equally well as deep learning classifiers. This research advances our understanding of the use of deep learning and machine learning in optimizing dropout prediction performance models.} }
@article{WOS:000887333600001, title = {Machine Learning Models for Data-Driven Prediction of Diabetes by Lifestyle Type}, journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH}, volume = {19}, year = {2022}, doi = {10.3390/ijerph192215027}, author = {Qin, Yifan and Wu, Jinlong and Xiao, Wen and Wang, Kun and Huang, Anbing and Liu, Bowen and Yu, Jingxuan and Li, Chuhao and Yu, Fengyu and Ren, Zhanbing}, abstract = {The prevalence of diabetes has been increasing in recent years, and previous research has found that machine-learning models are good diabetes prediction tools. The purpose of this study was to compare the efficacy of five different machine-learning models for diabetes prediction using lifestyle data from the National Health and Nutrition Examination Survey (NHANES) database. The 1999-2020 NHANES database yielded data on 17,833 individuals data based on demographic characteristics and lifestyle-related variables. To screen training data for machine models, the Akaike Information Criterion (AIC) forward propagation algorithm was utilized. For predicting diabetes, five machine-learning models (CATBoost, XGBoost, Random Forest (RF), Logistic Regression (LR), and Support Vector Machine (SVM)) were developed. Model performance was evaluated using accuracy, sensitivity, specificity, precision, F1 score, and receiver operating characteristic (ROC) curve. Among the five machine-learning models, the dietary intake levels of energy, carbohydrate, and fat, contributed the most to the prediction of diabetes patients. In terms of model performance, CATBoost ranks higher than RF, LG, XGBoost, and SVM. The best-performing machine-learning model among the five is CATBoost, which achieves an accuracy of 82.1\\% and an AUC of 0.83. Machine-learning models based on NHANES data can assist medical institutions in identifying diabetes patients.} }
@article{WOS:000792068500001, title = {Potential benefits and limitations of machine learning in the field of eating disorders: current research and future directions}, journal = {JOURNAL OF EATING DISORDERS}, volume = {10}, year = {2022}, issn = {2050-2974}, doi = {10.1186/s40337-022-00581-2}, author = {Fardouly, Jasmine and Crosby, Ross D. and Sukunesan, Suku}, abstract = {Plain English Summary Machine learning models are computer algorithms that learn from data to reach an optimal solution for a problem. These algorithms provide exciting potential for the accurate, accessible, and cost-effective early identification, prevention, and treatment of eating disorders, but this potential is just beginning to be explored. Research to date has mainly used machine learning to predict women's eating disorder status with relatively high levels of accuracy from responses to validated surveys, social media posts, or neuroimaging data. These studies show potential for the use of machine learning in the field, but we are far from using these methods in practice. Useful avenues for future research include the use of machine learning to personalise prevention and treatment options, provide ecological momentary interventions via smartphones, and to aid clinicians with their treatment fidelity and effectiveness. More research is needed with large samples of diverse participants to ensure that machine learning models are accurate, unbiased, and generalisable to all people with eating disorders. There are limitations and ethical considerations with using these methods in practice. If accurate and generalisable machine learning models can be created in the field of eating disorders, it could improve the way we identify, prevent, and treat these debilitating disorders. Advances in machine learning and digital data provide vast potential for mental health predictions. However, research using machine learning in the field of eating disorders is just beginning to emerge. This paper provides a narrative review of existing research and explores potential benefits, limitations, and ethical considerations of using machine learning to aid in the detection, prevention, and treatment of eating disorders. Current research primarily uses machine learning to predict eating disorder status from females' responses to validated surveys, social media posts, or neuroimaging data often with relatively high levels of accuracy. This early work provides evidence for the potential of machine learning to improve current eating disorder screening methods. However, the ability of these algorithms to generalise to other samples or be used on a mass scale is only beginning to be explored. One key benefit of machine learning over traditional statistical methods is the ability of machine learning to simultaneously examine large numbers (100s to 1000s) of multimodal predictors and their complex non-linear interactions, but few studies have explored this potential in the field of eating disorders. Machine learning is also being used to develop chatbots to provide psychoeducation and coping skills training around body image and eating disorders, with implications for early intervention. The use of machine learning to personalise treatment options, provide ecological momentary interventions, and aid the work of clinicians is also discussed. Machine learning provides vast potential for the accurate, rapid, and cost-effective detection, prevention, and treatment of eating disorders. More research is needed with large samples of diverse participants to ensure that machine learning models are accurate, unbiased, and generalisable to all people with eating disorders. There are important limitations and ethical considerations with utilising machine learning methods in practice. Thus, rather than a magical solution, machine learning should be seen as an important tool to aid the work of researchers, and eventually clinicians, in the early identification, prevention, and treatment of eating disorders.} }
@article{WOS:000795188600001, title = {Data pricing in machine learning pipelines}, journal = {KNOWLEDGE AND INFORMATION SYSTEMS}, volume = {64}, pages = {1417-1455}, year = {2022}, issn = {0219-1377}, doi = {10.1007/s10115-022-01679-4}, author = {Cong, Zicun and Luo, Xuan and Pei, Jian and Zhu, Feida and Zhang, Yong}, abstract = {Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data are critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.} }
@article{WOS:000993711000001, title = {Past, present, and future of the application of machine learning in cryptocurrency research}, journal = {RESEARCH IN INTERNATIONAL BUSINESS AND FINANCE}, volume = {63}, year = {2022}, issn = {0275-5319}, doi = {10.1016/j.ribaf.2022.101799}, author = {Ren, Yi-Shuai and Ma, Chao-Qun and Kong, Xiao-Lin and Baltas, Konstantinos and Zureigat, Qasim}, abstract = {Cryptocurrency has captured the interest of financial scholars and become a major research topic in blockchain. In cryptocurrency research, the use of machine learning algorithms is enabled by the presence of many types of data and abundant resources. However, there is currently no comprehensive review on cryptocurrencies using machine learning. Therefore, we collect papers on cryptocurrency-related using machine learning in the web of science database, and summarize these papers according to the algorithm, and draw the following conclusions: (1) The application of machine learning for cryptocurrencies research is increasing year over year; (2) Predicting cryptocurrency price trends and income fluctuations is the most relevant research topic; (3) The machine learning algorithm utilized in cryptocurrency research is not unique, and the practise of combining multiple machine learning approaches has emerged; (4) Concerns such as overfitting and interpretability still persist with machine learning methods. Finally, we suggest future research directions.} }
@article{WOS:000839022000001, title = {Improving Results of Existing Groundwater Numerical Models Using Machine Learning Techniques: A Review}, journal = {WATER}, volume = {14}, year = {2022}, doi = {10.3390/w14152307}, author = {Di Salvo, Cristina}, abstract = {This paper presents a review of papers specifically focused on the use of both numerical and machine learning methods for groundwater level modelling. In the reviewed papers, machine learning models (also called data-driven models) are used to improve the prediction or speed process of existing numerical modelling. When long runtimes inhibit the use of numerical models, machine learning models can be a valid alternative, capable of reducing the time for model development and calibration without sacrificing accuracy of detail in groundwater level forecasting. The results of this review highlight that machine learning models do not offer a complete representation of the physical system, such as flux estimates or total water balance and, thus, cannot be used to substitute numerical models in large study areas; however, they are affordable tools to improve predictions at specific observation wells. Numerical and machine learning models can be successfully used as complementary to each other as a powerful groundwater management tool. The machine learning techniques can be used to improve calibration of numerical models, whereas results of numerical models allow us to understand the physical system and select proper input variables for machine learning models. Machine learning models can be integrated in decision-making processes when rapid and effective solutions for groundwater management need to be considered. Finally, machine learning models are computationally efficient tools to correct head error prediction of numerical models.} }
@article{WOS:000741129900001, title = {Machine learning in the analysis of biomolecular simulations}, journal = {ADVANCES IN PHYSICS-X}, volume = {7}, year = {2022}, issn = {2374-6149}, doi = {10.1080/23746149.2021.2006080}, author = {Kaptan, Shreyas and Vattulainen, Ilpo}, abstract = {Machine learning has rapidly become a key method for the analysis and organization of large-scale data in all scientific disciplines. In life sciences, the use of machine learning techniques is a particularly appealing idea since the enormous capacity of computational infrastructures generates terabytes of data through millisecond simulations of atomistic and molecular-scale biomolecular systems. Due to this explosion of data, the automation, reproducibility, and objectivity provided by machine learning methods are highly desirable features in the analysis of complex systems. In this review, we focus on the use of machine learning in biomolecular simulations. We discuss the main categories of machine learning tasks, such as dimensionality reduction, clustering, regression, and classification used in the analysis of simulation data. We then introduce the most popular classes of techniques involved in these tasks for the purpose of enhanced sampling, coordinate discovery, and structure prediction. Whenever possible, we explain the scope and limitations of machine learning approaches, and we discuss examples of applications of these techniques.} }
@article{WOS:000844738900003, title = {Machine Learning in Chemoinformatics and Medicinal Chemistry}, journal = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE}, volume = {5}, pages = {43-65}, year = {2022}, issn = {2574-3414}, doi = {10.1146/annurev-biodatasci-122120-124216}, author = {Rodriguez-Perez, Raquel and Miljkovic, Filip and Bajorath, Juergen}, abstract = {In chemoinformatics and medicinal chemistry, machine learning has evolved into an important approach. In recent years, increasing computational resources and new deep learning algorithms have put machine learning onto a new level, addressing previously unmet challenges in pharmaceutical research. In silico approaches for compound activity predictions, de novo design, and reaction modeling have been further advanced by new algorithmic developments and the emergence of big data in the field. Herein, novel applications of machine learning and deep learning in chemoinformatics and medicinal chemistry are reviewed. Opportunities and challenges for new methods and applications are discussed, placing emphasis on proper baseline comparisons, robust validation methodologies, and new applicability domains.} }
@article{WOS:000867398400004, title = {Adversarial machine learning in IoT from an insider point of view}, journal = {JOURNAL OF INFORMATION SECURITY AND APPLICATIONS}, volume = {70}, year = {2022}, issn = {2214-2126}, doi = {10.1016/j.jisa.2022.103341}, author = {Aloraini, Fatimah and Javed, Amir and Rana, Omer and Burnap, Pete}, abstract = {With the rapid progress and significant successes in various applications, machine learning has been considered a crucial component in the Internet of Things ecosystem. However, machine learning models have recently been vulnerable to carefully crafted perturbations, so-called adversarial attacks. A capable insider adversary can subvert the machine learning model at either the training or testing phase, causing them to behave differently. The vulnerability of machine learning to adversarial attacks becomes one of the significant risks. Therefore, there is a need to secure machine learning models enabling the safe adoption in malicious insider cases. This paper reviews and organizes the body of knowledge in adversarial attacks and defense presented in IoT literature from an insider adversary point of view. We proposed a taxonomy of adversarial methods against machine learning models that an insider can exploit. Under the taxonomy, we discuss how these methods can be applied in real-life IoT applications. Finally, we explore defensive methods against adversarial attacks. We believe this can draw a comprehensive overview of the scattered research works to raise awareness of the existing insider threats landscape and encourages others to safeguard machine learning models against insider threats in the IoT ecosystem.} }
@article{WOS:000765072200001, title = {Transfer Learning for Radio Frequency Machine Learning: A Taxonomy and Survey}, journal = {SENSORS}, volume = {22}, year = {2022}, doi = {10.3390/s22041416}, author = {Wong, Lauren J. and Michaels, Alan J.}, abstract = {Transfer learning is a pervasive technology in computer vision and natural language processing fields, yielding exponential performance improvements by leveraging prior knowledge gained from data with different distributions. However, while recent works seek to mature machine learning and deep learning techniques in applications related to wireless communications, a field loosely termed radio frequency machine learning, few have demonstrated the use of transfer learning techniques for yielding performance gains, improved generalization, or to address concerns of training data costs. With modifications to existing transfer learning taxonomies constructed to support transfer learning in other modalities, this paper presents a tailored taxonomy for radio frequency applications, yielding a consistent framework that can be used to compare and contrast existing and future works. This work offers such a taxonomy, discusses the small body of existing works in transfer learning for radio frequency machine learning, and outlines directions where future research is needed to mature the field.} }
@article{WOS:000827662800001, title = {Toward a sociology of machine learning explainability: Human-machine interaction in deep neural network-based automated trading}, journal = {BIG DATA \\& SOCIETY}, volume = {9}, year = {2022}, issn = {2053-9517}, doi = {10.1177/20539517221111361}, author = {Borch, Christian and Hee Min, Bo}, abstract = {Machine learning systems are making considerable inroads in society owing to their ability to recognize and predict patterns. However, the decision-making logic of some widely used machine learning models, such as deep neural networks, is characterized by opacity, thereby rendering them exceedingly difficult for humans to understand and explain and, as a result, potentially risky to use. Considering the importance of addressing this opacity, this paper calls for research that studies empirically and theoretically how machine learning experts and users seek to attain machine learning explainability. Focusing on automated trading, we take steps in this direction by analyzing a trading firm's quest for explaining its deep neural network system's actionable predictions. We demonstrate that this explainability effort involves a particular form of human-machine interaction that contains both anthropomorphic and technomorphic elements. We discuss this attempt to attain machine learning explainability in light of reflections on cross-species companionship and consider it an example of human-machine companionship.} }
@article{WOS:000597401000025, title = {Landslide identification using machine learning}, journal = {GEOSCIENCE FRONTIERS}, volume = {12}, pages = {351-364}, year = {2021}, issn = {1674-9871}, doi = {10.1016/j.gsf.2020.02.012}, author = {Wang, Haojie and Zhang, Limin and Yin, Kesheng and Luo, Hongyu and Li, Jinhui}, abstract = {Landslide identification is critical for risk assessment and mitigation. This paper proposes a novel machine-learning and deep-learning method to identify natural-terrain landslides using integrated geodatabases. First, landslide-related data are compiled, including topographic data, geological data and rainfall-related data. Then, three integrated geodatabases are established; namely, Recent Landslide Database (RecLD), Relict Landslide Database (RelLD) and Joint Landslide Database (JLD). After that, five machine learning and deep learning algorithms, including logistic regression (LR), support vector machine (SVM), random forest (RF), boosting methods and convolutional neural network (CNN), are utilized and evaluated on each database. A case study in Lantau, Hong Kong, is conducted to demonstrate the application of the proposed method. From the results of the case study, CNN achieves an identification accuracy of 92.5\\% on RecLD, and outperforms other algorithms due to its strengths in feature extraction and multi dimensional data processing. Boosting methods come second in terms of accuracy, followed by RF, LR and SVM. By using machine learning and deep learning techniques, the proposed landslide identification method shows outstanding robustness and great potential in tackling the landslide identification problem.} }
@article{WOS:000864033400001, title = {Leveraging Theory for Enhanced Machine Learning}, journal = {ACS MACRO LETTERS}, year = {2022}, doi = {10.1021/acsmacrolett.2c00369}, author = {Audus, Debra J. and McDannald, Austin and DeCost, Brian}, abstract = {The application of machine learning to the materials domain has traditionally struggled with two major challenges: a lack of large, curated data sets and the need to understand the physics behind the machine-learning prediction. The former problem is particularly acute in the polymers domain. Here we aim to simultaneously tackle these challenges through the incorporation of scientific knowledge, thus, providing improved predictions for smaller data sets, both under interpolation and extrapolation, and a degree of explainability. We focus on imperfect theories, as they are often readily available and easier to interpret. Using a system of a polymer in different solvent qualities, we explore numerous methods for incorporating theory into machine learning using different machine-learning models, including Gaussian process regression. Ultimately, we find that encoding the functional form of the theory performs best followed by an encoding of the numeric values of the theory.} }
@article{WOS:000771334100001, title = {Special Issue: Geostatistics and Machine Learning}, journal = {MATHEMATICAL GEOSCIENCES}, volume = {54}, pages = {459-465}, year = {2022}, issn = {1874-8961}, doi = {10.1007/s11004-022-09998-6}, author = {De Iaco, Sandra and Hristopulos, Dionissios T. and Lin, Guang}, abstract = {Recent years have seen a steady growth in the number of papers that apply machine learning methods to problems in the earth sciences. Although they have different origins, machine learning and geostatistics share concepts and methods. For example, the kriging formalism can be cast in the machine learning framework of Gaussian process regression. Machine learning, with its focus on algorithms and ability to seek, identify, and exploit hidden structures in big data sets, is providing new tools for exploration and prediction in the earth sciences. Geostatistics, on the other hand, offers interpretable models of spatial (and spatiotemporal) dependence. This special issue on Geostatistics and Machine Learning aims to investigate applications of machine learning methods as well as hybrid approaches combining machine learning and geostatistics which advance our understanding and predictive ability of spatial processes.} }
@article{WOS:000907756500001, title = {Applying machine learning technologies to explore students' learning features and performance prediction}, journal = {FRONTIERS IN NEUROSCIENCE}, volume = {16}, year = {2022}, doi = {10.3389/fnins.2022.1018005}, author = {Su, Yu-Sheng and Lin, Yu-Da and Liu, Tai-Quan}, abstract = {To understand students' learning behaviors, this study uses machine learning technologies to analyze the data of interactive learning environments, and then predicts students' learning outcomes. This study adopted a variety of machine learning classification methods, quizzes, and programming system logs, found that students' learning characteristics were correlated with their learning performance when they encountered similar programming practice. In this study, we used random forest (RF), support vector machine (SVM), logistic regression (LR), and neural network (NN) algorithms to predict whether students would submit on time for the course. Among them, the NN algorithm showed the best prediction results. Education-related data can be predicted by machine learning techniques, and different machine learning models with different hyperparameters can be used to obtain better results.} }
@article{WOS:000910903500001, title = {A systematic literature review on the use of machine learning in code clone research}, journal = {COMPUTER SCIENCE REVIEW}, volume = {47}, year = {2023}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2022.100528}, author = {Kaur, Manpreet and Rattan, Dhavleesh}, abstract = {Context: Research related to code clones includes detection of clones in software systems, analysis, visualization and management of clones. Detection of semantic clones and management of clones have attracted use of machine learning techniques in code clone related research.Objective: The aim of this study is to report the extent of machine learning usage in code clone related research areas.Method: The paper uses a systematic review method to report the use of machine learning in research related to code clones. The study considers a comprehensive set of 57 articles published in leading conferences, workshops and journals.Results: Code clone related research using machine learning techniques is classified into different categories. Machine learning and deep learning algorithms used in the code clone research are reported. The datasets, features used to train machine learning models and metrics used to evaluate machine learning algorithms are reported. The comparative results of various machine learning algorithms presented in primary studies are reported.Conclusion: The research will help to identify the status of using machine learning in different code clone related research areas. We identify the need of more empirical studies to assess the benefits of machine learning in code clone research and give recommendations for future research.(c) 2022 Elsevier Inc. All rights reserved.} }
@article{WOS:000912711800001, title = {Sufficiency of Ensemble Machine Learning Methods for Phishing Websites Detection}, journal = {IEEE ACCESS}, volume = {10}, pages = {124103-124113}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3224781}, author = {Wei, Yi and Sekiya, Yuji}, abstract = {Phishing is a kind of worldwide spread cybercrime that uses disguised websites to trick users into downloading malware or providing personally sensitive information to attackers. With the rapid development of artificial intelligence, more and more researchers in the cybersecurity field utilize machine learning and deep learning algorithms to classify phishing websites. In order to compare the performances of various machine learning and deep learning methods, several experiments are conducted in this study. According to the experimental results, ensemble machine learning algorithms stand out among other candidates in both detection accuracy and computational consumption. Furthermore, the ensemble architectures still provide impressive capability when the amount of features decreases sharply in the dataset. Subsequently, the paper discusses the factors why ensemble machine learning methods are more suitable for the binary phishing classification challenge in up-date training and real-time detecting environment, which reflects the sufficiency of ensemble machine learning methods in anti-phishing techniques.} }
@article{WOS:000901956500001, title = {Review of Machine Learning Applications to the Modeling and Design Optimization of Switched Reluctance Motors}, journal = {IEEE ACCESS}, volume = {10}, pages = {130444-130468}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3229043}, author = {Omar, Mohamed and Sayed, Ehab and Abdalmagid, Mohamed and Bilgin, Berker and Bakr, Mohamed H. and Emadi, Ali}, abstract = {This work presents a comprehensive review of the developments in using Machine Learning (ML)-based algorithms for the modeling and design optimization of switched reluctance motors (SRMs). We reviewed Machine Learning-based numerical and analytical approaches used in modeling SRMs. We showed the difference between the supervised, unsupervised and reinforcement learning algorithms. More focus is placed on supervised learning algorithms as they are the most used algorithms in this area. The supervised learning algorithms studied in this work include the feedforward neural networks, recurrent neural networks, support vector machines, extreme learning machines, and Bayesian networks. This work also discusses several essential aspects of the considered machine learning algorithms, such as core concept, structure, and computational time. It also surveys sample data acquisition methods and data size. Finally, comparisons between the different considered ML-based algorithms are conducted in terms of electric motor type, dataset inputs and outputs, and algorithm's structure and accuracy to provide a summary overview of the ML-based algorithms for SRMs modeling and design.} }
@article{WOS:000431395700001, title = {Optimization Methods for Large-Scale Machine Learning}, journal = {SIAM REVIEW}, volume = {60}, pages = {223-311}, year = {2018}, issn = {0036-1445}, doi = {10.1137/16M1080173}, author = {Bottou, Leon and Curtis, Frank E. and Nocedal, Jorge}, abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.} }
@article{WOS:000517757300010, title = {Introduction to Machine Learning, Neural Networks, and Deep Learning}, journal = {TRANSLATIONAL VISION SCIENCE \\& TECHNOLOGY}, volume = {9}, year = {2020}, issn = {2164-2591}, doi = {10.1167/tvst.9.2.14}, author = {Choi, Rene Y. and Coyner, Aaron S. and Kalpathy-Cramer, Jayashree and Chiang, Michael F. and Campbell, J. Peter}, abstract = {Purpose: To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods: A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results: A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions: Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance: The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.} }
@article{WOS:000556899700018, title = {Machine learning methods for landslide susceptibility studies: A comparative overview of algorithm performance}, journal = {EARTH-SCIENCE REVIEWS}, volume = {207}, year = {2020}, issn = {0012-8252}, doi = {10.1016/j.earscirev.2020.103225}, author = {Merghadi, Abdelaziz and Yunus, Ali P. and Dou, Jie and Whiteley, Jim and Binh ThaiPham and Dieu Tien Bui and Avtar, Ram and Abderrahmane, Boumezbeur}, abstract = {Landslides are one of the catastrophic natural hazards that occur in mountainous areas, leading to loss of life, damage to properties, and economic disruption. Landslide susceptibility models prepared in a Geographic Information System (GIS) integrated environment can be key for formulating disaster prevention measures and mitigating future risk. The accuracy and precision of susceptibility models is evolving rapidly from opinion-driven models and statistical learning toward increased use of machine learning techniques. Critical reviews on opinion-driven models and statistical learning in landslide susceptibility mapping have been published, but an overview of current machine learning models for landslide susceptibility studies, including background information on their operation, implementation, and performance is currently lacking. Here, we present an overview of the most popular machine learning techniques available for landslide susceptibility studies. We find that only a handful of researchers use machine learning techniques in landslide susceptibility mapping studies. Therefore, we present the architecture of various Machine Learning (ML) algorithms in plain language, so as to be understandable to a broad range of geoscientists. Furthermore, a comprehensive study comparing the performance of various ML algorithms is absent from the current literature, making an assessment of comparative performance and predictive capabilities difficult. We therefore undertake an extensive analysis and comparison between different ML techniques using a case study from Algeria. We summarize and discuss the algorithm's accuracies, advantages and limitations using a range of evaluation criteria. We note that tree-based ensemble algorithms achieve excellent results compared to other machine learning algorithms and that the Random Forest algorithm offers robust performance for accurate landslide susceptibility mapping with only a small number of adjustments required before training the model.} }
@article{WOS:000546615200005, title = {Failure mode and effects analysis of RC members based on machine-learning-based SHapley Additive exPlanations (SHAP) approach}, journal = {ENGINEERING STRUCTURES}, volume = {219}, year = {2020}, issn = {0141-0296}, doi = {10.1016/j.engstruct.2020.110927}, author = {Mangalathu, Sujith and Hwang, Seong-Hoon and Jeon, Jong-Su}, abstract = {Machine learning approaches can establish the complex and non-linear relationship among input and response variables for the seismic damage assessment of structures. However, lack of explainability of complex machine learning models prevents their use in such assessment. This paper uses extensive experimental databases to suggest random forest machine learning models for failure mode predictions of reinforced concrete columns and shear walls, employs the recently developed SHapley Additive exPlanations approach to rank input variables for identification of failure modes, and explains why the machine learning model predicts a specific failure mode for a given sample or experiment. A random forest model established provides an accuracy of 84\\% and 86\\% for unknown data of columns and shear walls, respectively. The geometric variables and reinforcement indices are critical parameters that influence failure modes. The study also reveals that existing strategies of failure mode identification based solely on geometric features are not enough to properly identify failure modes.} }
@article{WOS:000566732800001, title = {Supervised Machine Learning: A Brief Primer}, journal = {BEHAVIOR THERAPY}, volume = {51}, pages = {675-687}, year = {2020}, issn = {0005-7894}, author = {Jiang, Tammy and Gradus, Jaimie L. and Rosellini, Anthony J.}, abstract = {Machine learning is increasingly used in mental health research and has the potential to advance our understanding of how to characterize, predict, and treat mental disorders and associated adverse health outcomes (e.g., suicidal behavior). Machine learning offers new tools to overcome challenges for which traditional statistical methods are not well-suited. This paper provides an overview of machine learning with a specific focus on supervised learning (i.e., methods that are designed to predict or classify an outcome of interest). Several common supervised learning methods are described, along with applied examples from the published literature. We also provide an overview of supervised learning model building, validation, and performance evaluation. Finally, challenges in creating robust and generalizable machine learning algorithms are discussed.} }
@article{WOS:000548811800021, title = {A Survey of Optimization Methods From a Machine Learning Perspective}, journal = {IEEE TRANSACTIONS ON CYBERNETICS}, volume = {50}, pages = {3668-3681}, year = {2020}, issn = {2168-2267}, doi = {10.1109/TCYB.2019.2950779}, author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing}, abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.} }
@article{WOS:000525389000018, title = {Explainable Machine Learning for Scientific Insights and Discoveries}, journal = {IEEE ACCESS}, volume = {8}, pages = {42200-42216}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2976199}, author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen}, abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.} }
@article{WOS:000582585700009, title = {A Survey on Distributed Machine Learning}, journal = {ACM COMPUTING SURVEYS}, volume = {53}, year = {2020}, issn = {0360-0300}, doi = {10.1145/3377454}, author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.}, abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.} }
@article{WOS:000509755200009, title = {A survey on machine learning for data fusion}, journal = {INFORMATION FUSION}, volume = {57}, pages = {115-129}, year = {2020}, issn = {1566-2535}, doi = {10.1016/j.inffus.2019.12.001}, author = {Meng, Tong and Jing, Xuyang and Yan, Zheng and Pedrycz, Witold}, abstract = {Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.} }
@article{WOS:000537804900016, title = {Automated machine learning: Review of the state-of-the-art and opportunities for healthcare}, journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE}, volume = {104}, year = {2020}, issn = {0933-3657}, doi = {10.1016/j.artmed.2020.101822}, author = {Waring, Jonathan and Lindvall, Charlotta and Umeton, Renato}, abstract = {Objective: This work aims to provide a review of the existing literature in the field of automated machine learning (AutoML) to help healthcare professionals better utilize machine learning models ``off-the-shelf'' with limited data science expertise. We also identify the potential opportunities and barriers to using AutoML in healthcare, as well as existing applications of AutoML in healthcare. Methods: Published papers, accompanied with code, describing work in the field of AutoML from both a computer science perspective or a biomedical informatics perspective were reviewed. We also provide a short summary of a series of AutoML challenges hosted by ChaLearn. Results: A review of 101 papers in the field of AutoML revealed that these automated techniques can match or improve upon expert human performance in certain machine learning tasks, often in a shorter amount of time. The main limitation of AutoML at this point is the ability to get these systems to work efficiently on a large scale, i.e. beyond small- and medium-size retrospective datasets. Discussion: The utilization of machine learning techniques has the demonstrated potential to improve health outcomes, cut healthcare costs, and advance clinical research. However, most hospitals are not currently deploying machine learning solutions. One reason for this is that health care professionals often lack the machine learning expertise that is necessary to build a successful model, deploy it in production, and integrate it with the clinical workflow. In order to make machine learning techniques easier to apply and to reduce the demand for human experts, automated machine learning (AutoML) has emerged as a growing field that seeks to automatically select, compose, and parametrize machine learning models, so as to achieve optimal performance on a given task and/or dataset. Conclusion: While there have already been some use cases of AutoML in the healthcare field, more work needs to be done in order for there to be widespread adoption of AutoML in healthcare.} }
@article{WOS:000523319000015, title = {How Machine Learning Will Transform Biomedicine}, journal = {CELL}, volume = {181}, pages = {92-101}, year = {2020}, issn = {0092-8674}, doi = {10.1016/j.cell.2020.03.022}, author = {Goecks, Jeremy and Jalili, Vahid and Heiser, Laura M. and Gray, Joe W.}, abstract = {This Perspective explores the application of machine learning toward improved diagnosis and treatment. We outline a vision for how machine learning can transform three broad areas of biomedicine: clinical diagnostics, precision treatments, and health monitoring, where the goal is to maintain health through a range of diseases and the normal aging process. For each area, early instances of successful machine learning applications are discussed, as well as opportunities and challenges for machine learning. When these challenges are met, machine learning promises a future of rigorous, outcomes-based medicine with detection, diagnosis, and treatment strategies that are continuously adapted to individual and environmental differences.} }
@article{WOS:000520831800008, title = {Machine Learning and Artificial Intelligence: Definitions, Applications, and Future Directions}, journal = {CURRENT REVIEWS IN MUSCULOSKELETAL MEDICINE}, volume = {13}, pages = {69-76}, year = {2020}, issn = {1935-973X}, doi = {10.1007/s12178-020-09600-8}, author = {Helm, J. Matthew and Swiergosz, Andrew M. and Haeberle, Heather S. and Karnuta, Jaret M. and Schaffer, Jonathan L. and Krebs, Viktor E. and Spitzer, I, Andrew and Ramkumar, Prem N.}, abstract = {Purpose of Review With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care. Recent Findings Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care.} }
@article{WOS:000577150900001, title = {Machine learning assisted materials design and discovery for rechargeable batteries}, journal = {ENERGY STORAGE MATERIALS}, volume = {31}, pages = {434-450}, year = {2020}, issn = {2405-8297}, doi = {10.1016/j.ensm.2020.06.033}, author = {Liu, Yue and Guo, Biru and Zou, Xinxin and Li, Yajie and Shi, Siqi}, abstract = {Machine learning plays an important role in accelerating the discovery and design process for novel electrochemical energy storage materials. This review aims to provide the state-of-the-art and prospects of machine learning for the design of rechargeable battery materials. After illustrating the key concepts of machine learning and basic procedures for applying machine learning in rechargeable battery materials science, we focus on how to obtain the most important features from the specific physical, chemical and/or other properties of material by using wrapper feature selection method, embedded feature selection method, and the combination of these two methods. And then, the applications of machine learning in rechargeable battery materials design and discovery are reviewed, including the property prediction for liquid electrolytes, solid electrolytes, electrode materials, and the discovery of novel rechargeable battery materials through component prediction and structure prediction. More importantly, we discuss the key challenges related to machine learning in rechargeable battery materials science, including the contradiction between high dimension and small sample, the conflict between the complexity and accuracy of machine learning models, and the inconsistency between learning results and domain expert knowledge. In response to these challenges, we propose possible countermeasures and forecast potential directions of future research. This review is expected to shed light on machine learning in rechargeable battery materials design and property optimization.} }
@article{WOS:000892292800005, title = {Equivalence of machine learning models in modeling chaos}, journal = {CHAOS SOLITONS \\& FRACTALS}, volume = {165}, year = {2022}, issn = {0960-0779}, doi = {10.1016/j.chaos.2022.112831}, author = {Chen, Xiaolu and Weng, Tongfeng and Li, Chunzi and Yang, Huijie}, abstract = {Recent advances have demonstrated that machine learning models are effective methods for predicting chaotic systems. Although short-term chaos prediction can be successfully realized by seemingly different machine learning models, an intriguing question of their correlation is still unknown. Here, we focus on three commonly used machine learning models that are reservoir computing, long-short term memory networks, and deep belief networks, respectively. We find that these selected models present almost identical long-term statistical properties as that of a learned chaotic system. Specifically, we show that these machine learning models have the same correlation dimension and recurrence time. Furthermore, by sharing a common signal, we realize synchronization, cascading synchronization, and coupled synchronization among machine learning models. Our findings reveal the equivalence of machine learning models in characterizing and modeling chaotic systems.} }
@article{WOS:000895564800001, title = {Human-in-the-loop machine learning with applications for population health}, journal = {CCF TRANSACTIONS ON PERVASIVE COMPUTING AND INTERACTION}, volume = {5}, pages = {1-12}, year = {2023}, issn = {2524-521X}, doi = {10.1007/s42486-022-00115-4}, author = {Chen, Long and Wang, Jiangtao and Guo, Bin and Chen, Liming}, abstract = {Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Next, a case study of our recent application study in human-in-the-loop machine learning for population health is introduced. Finally, we analyze current research gaps and point out future research directions.} }
@article{WOS:000873821800023, title = {Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?}, journal = {COMPUTER}, volume = {55}, pages = {94-99}, year = {2022}, issn = {0018-9162}, doi = {10.1109/MC.2022.3190787}, author = {Oprea, Alina and Singhal, Anoop and Vassilev, Apostol}, abstract = {Many practical applications benefit from machine learning and artificial intelligence technologies, but their security needs to be studied in more depth. We discuss the risk of poisoning attacks against the training stage of machine learning and challenges of defending against them.} }
@article{WOS:000605460600001, title = {Optimization problems for machine learning: A survey}, journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH}, volume = {290}, pages = {807-828}, year = {2021}, issn = {0377-2217}, doi = {10.1016/j.ejor.2020.08.045}, author = {Gambella, Claudio and Ghaddar, Bissan and Naoum-Sawaya, Joe}, abstract = {This paper surveys the machine learning literature and presents in an optimization framework several commonly used machine learning approaches. Particularly, mathematical optimization models are presented for regression, classification, clustering, deep learning, and adversarial learning, as well as new emerging applications in machine teaching, empirical model learning, and Bayesian network structure learning. Such models can benefit from the advancement of numerical optimization techniques which have already played a distinctive role in several machine learning settings. The strengths and the shortcomings of these models are discussed and potential research directions and open problems are highlighted. (C) 2020 Elsevier B.V. Allrights reserved.} }
@article{WOS:000643029400001, title = {Privacy Preserving Machine Learning with Homomorphic Encryption and Federated Learning}, journal = {FUTURE INTERNET}, volume = {13}, year = {2021}, issn = {1999-5903}, doi = {10.3390/fi13040094}, author = {Fang, Haokun and Qian, Quan}, abstract = {Privacy protection has been an important concern with the great success of machine learning. In this paper, it proposes a multi-party privacy preserving machine learning framework, named PFMLP, based on partially homomorphic encryption and federated learning. The core idea is all learning parties just transmitting the encrypted gradients by homomorphic encryption. From experiments, the model trained by PFMLP has almost the same accuracy, and the deviation is less than 1\\%. Considering the computational overhead of homomorphic encryption, we use an improved Paillier algorithm which can speed up the training by 25-28\\%. Moreover, comparisons on encryption key length, the learning network structure, number of learning clients, etc. are also discussed in detail in the paper.} }
@article{WOS:000744050600002, title = {Benchmark and Survey of Automated Machine Learning Frameworks}, journal = {JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH}, volume = {70}, pages = {409-472}, year = {2021}, issn = {1076-9757}, author = {Zoeller, Marc-Andre and Huber, Marco F.}, abstract = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.} }
@article{WOS:000652961400002, title = {Coronavirus disease (COVID-19) cases analysis using machine-learning applications}, journal = {APPLIED NANOSCIENCE}, year = {2021}, issn = {2190-5509}, doi = {10.1007/s13204-021-01868-7}, author = {Kwekha-Rashid, Ameer Sardar and Abduljabbar, Heamn N. and Alhayani, Bilal}, abstract = {Today world thinks about coronavirus disease that which means all even this pandemic disease is not unique. The purpose of this study is to detect the role of machine-learning applications and algorithms in investigating and various purposes that deals with COVID-19. Review of the studies that had been published during 2020 and were related to this topic by seeking in Science Direct, Springer, Hindawi, and MDPI using COVID-19, machine learning, supervised learning, and unsupervised learning as keywords. The total articles obtained were 16,306 overall but after limitation; only 14 researches of these articles were included in this study. Our findings show that machine learning can produce an important role in COVID-19 investigations, prediction, and discrimination. In conclusion, machine learning can be involved in the health provider programs and plans to assess and triage the COVID-19 cases. Supervised learning showed better results than other Unsupervised learning algorithms by having 92.9\\% testing accuracy. In the future recurrent supervised learning can be utilized for superior accuracy.} }
@article{WOS:000719871700005, title = {Machine Learning in Chemical Engineering: Strengths, Weaknesses, Opportunities, and Threats}, journal = {ENGINEERING}, volume = {7}, pages = {1201-1211}, year = {2021}, issn = {2095-8099}, doi = {10.1016/j.eng.2021.03.019}, author = {Dobbelaere, Maarten R. and Plehiers, Pieter P. and van de Vijver, Ruben and V. Stevens, Christian and Van Geem, Kevin M.}, abstract = {Chemical engineers rely on models for design, research, and daily decision-making, often with potentially large financial and safety implications. Previous efforts a few decades ago to combine artificial intelligence and chemical engineering for modeling were unable to fulfill the expectations. In the last five years, the increasing availability of data and computational resources has led to a resurgence in machine learning-based research. Many recent efforts have facilitated the roll-out of machine learning techniques in the research field by developing large databases, benchmarks, and representations for chemical applications and new machine learning frameworks. Machine learning has significant advantages over traditional modeling techniques, including flexibility, accuracy, and execution speed. These strengths also come with weaknesses, such as the lack of interpretability of these black-box models. The greatest opportunities involve using machine learning in time-limited applications such as real-time optimization and planning that require high accuracy and that can build on models with a self-learning ability to recognize patterns, learn from data, and become more intelligent over time. The greatest threat in artificial intelligence research today is inappropriate use because most chemical engineers have had limited training in computer science and data analysis. Nevertheless, machine learning will definitely become a trustworthy element in the modeling toolbox of chemical engineers. (C) 2021 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company.} }
@article{WOS:000662774900003, title = {Machine learning in construction: From shallow to deep learning}, journal = {DEVELOPMENTS IN THE BUILT ENVIRONMENT}, volume = {6}, year = {2021}, doi = {10.1016/j.dibe.2021.100045}, author = {Xu, Yayin and Zhou, Ying and Sekula, Przemyslaw and Ding, Lieyun}, abstract = {The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction ``smart''. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.} }
@article{WOS:000632782800001, title = {Reproducibility in machine learning for health research: Still a ways to go}, journal = {SCIENCE TRANSLATIONAL MEDICINE}, volume = {13}, year = {2021}, issn = {1946-6234}, doi = {10.1126/scitranslmed.abb1655}, author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Foschini, Luca and Ghassemi, Marzyeh}, abstract = {Machine learning for health must be reproducible to ensure reliable clinical use. We evaluated 511 scientific papers across several machine learning subfields and found that machine learning for health compared poorly to other areas regarding reproducibility metrics, such as dataset and code accessibility. We propose recommendations to address this problem.} }
@incollection{WOS:000652490700019, title = {Machine Learning for Social Science: An Agnostic Approach}, booktitle = {ANNUAL REVIEW OF POLITICAL SCIENCE, VOL 24, 2021}, volume = {24}, pages = {395-419}, year = {2021}, issn = {1094-2939}, doi = {10.1146/annurev-polisci-053119-015921}, author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.}, abstract = {Social scientists are now in an era of data abundance, and machine learning tools are increasingly used to extract meaning from data sets both massive and small. We explain how the inclusion of machine learning in the social sciences requires us to rethink not only applications of machine learning methods but also best practices in the social sciences. In contrast to the traditional tasks for machine learning in computer science and statistics, when machine learning is applied to social scientific data, it is used to discover new concepts, measure the prevalence of those concepts, assess causal effects, and make predictions. The abundance of data and resources facilitates the move away from a deductive social science to a more sequential, interactive, and ultimately inductive approach to inference. We explain how an agnostic approach to machine learning methods focused on the social science tasks facilitates progress across a wide range of questions.} }
@article{WOS:000655346100001, title = {Machine learning for hydrologic sciences: An introductory overview}, journal = {WILEY INTERDISCIPLINARY REVIEWS-WATER}, volume = {8}, year = {2021}, issn = {2049-1948}, doi = {10.1002/wat2.1533}, author = {Xu, Tianfang and Liang, Feng}, abstract = {The hydrologic community has experienced a surge in interest in machine learning in recent years. This interest is primarily driven by rapidly growing hydrologic data repositories, as well as success of machine learning in various academic and commercial applications, now possible due to increasing accessibility to enabling hardware and software. This overview is intended for readers new to the field of machine learning. It provides a non-technical introduction, placed within a historical context, to commonly used machine learning algorithms and deep learning architectures. Applications in hydrologic sciences are summarized next, with a focus on recent studies. They include the detection of patterns and events such as land use change, approximation of hydrologic variables and processes such as rainfall-runoff modeling, and mining relationships among variables for identifying controlling factors. The use of machine learning is also discussed in the context of integrated with process-based modeling for parameterization, surrogate modeling, and bias correction. Finally, the article highlights challenges of extrapolating robustness, physical interpretability, and small sample size in hydrologic applications.} }
@article{WOS:000953243000001, title = {Mitigating bias in machine learning for medicine}, journal = {COMMUNICATIONS MEDICINE}, volume = {1}, year = {2021}, issn = {2730-664X}, doi = {10.1038/s43856-021-00028-w}, author = {Vokinger, Kerstin N. and Feuerriegel, Stefan and Kesselheim, Aaron S.}, abstract = {Several sources of bias can affect the performance of machine learning systems used in medicine and potentially impact clinical care. Here, we discuss solutions to mitigate bias across the different development steps of machine learning-based systems for medical applications.} }
@article{WOS:000677963600002, title = {DOME: recommendations for supervised machine learning validation in biology}, journal = {NATURE METHODS}, volume = {18}, pages = {1122-1127}, year = {2021}, issn = {1548-7091}, doi = {10.1038/s41592-021-01205-4}, author = {Walsh, Ian and Fishman, Dmytro and Garcia-Gasulla, Dario and Titma, Tiina and Pollastri, Gianluca and Harrow, Jennifer and Psomopoulos, Fotis E. and Tosatto, Silvio C. E. and ELIXIR Machine Learning Focus Grp}, abstract = {DOME is a set of community-wide recommendations for reporting supervised machine learning-based analyses applied to biological studies. Broad adoption of these recommendations will help improve machine learning assessment and reproducibility.} }
@article{WOS:000684854500006, title = {Machine learning applications in microbial ecology, human microbiome studies, and environmental monitoring}, journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL}, volume = {19}, pages = {1092-1107}, year = {2021}, issn = {2001-0370}, doi = {10.1016/j.csbj.2021.01.028}, author = {Ghannam, Ryan B. and Techtmann, Stephen M.}, abstract = {Advances in nucleic acid sequencing technology have enabled expansion of our ability to profile microbial diversity. These large datasets of taxonomic and functional diversity are key to better understanding microbial ecology. Machine learning has proven to be a useful approach for analyzing microbial commu-nity data and making predictions about outcomes including human and environmental health. Machine learning applied to microbial community profiles has been used to predict disease states in human health, environmental quality and presence of contamination in the environment, and as trace evidence in forensics. Machine learning has appeal as a powerful tool that can provide deep insights into microbial communities and identify patterns in microbial community data. However, often machine learning models can be used as black boxes to predict a specific outcome, with little understanding of how the models arrived at predictions. Complex machine learning algorithms often may value higher accuracy and per-formance at the sacrifice of interpretability. In order to leverage machine learning into more translational research related to the microbiome and strengthen our ability to extract meaningful biological informa-tion, it is important for models to be interpretable. Here we review current trends in machine learning applications in microbial ecology as well as some of the important challenges and opportunities for more broad application of machine learning to understanding microbial communities. (C) 2021 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.} }
@article{WOS:000620625100026, title = {Review of machine learning methods in soft robotics}, journal = {PLOS ONE}, volume = {16}, year = {2021}, doi = {10.1371/journal.pone.0246102}, author = {Kim, Daekyum and Kim, Sang-Hun and Kim, Taekyoung and Kang, Brian Byunghyun and Lee, Minhyuk and Park, Wookeun and Ku, Subyeong and Kim, DongWook and Kwon, Junghan and Lee, Hochang and Bae, Joonbum and Park, Yong-Lae and Cho, Kyu-Jin and Jo, Sungho}, abstract = {Soft robots have been extensively researched due to their flexible, deformable, and adaptive characteristics. However, compared to rigid robots, soft robots have issues in modeling, calibration, and control in that the innate characteristics of the soft materials can cause complex behaviors due to non-linearity and hysteresis. To overcome these limitations, recent studies have applied various approaches based on machine learning. This paper presents existing machine learning techniques in the soft robotic fields and categorizes the implementation of machine learning approaches in different soft robotic applications, which include soft sensors, soft actuators, and applications such as soft wearable robots. An analysis of the trends of different machine learning approaches with respect to different types of soft robot applications is presented; in addition to the current limitations in the research field, followed by a summary of the existing machine learning methods for soft robots.} }
@article{WOS:000649692900007, title = {Application of machine learning in intelligent fish aquaculture: A review}, journal = {AQUACULTURE}, volume = {540}, year = {2021}, issn = {0044-8486}, doi = {10.1016/j.aquaculture.2021.736724}, author = {Zhao, Shili and Zhang, Song and Liu, Jincun and Wang, He and Zhu, Jia and Li, Daoliang and Zhao, Ran}, abstract = {Among the background of developments in automation and intelligence, machine learning technology has been extensively applied in aquaculture in recent years, providing a new opportunity for the realization of digital fishery farming. In the present paper, the machine learning algorithms and techniques adopted in intelligent fish aquaculture in the past five years are expounded, and the application of machine learning in aquaculture is explored in detail, including the information evaluation of fish biomass, the identification and classification of fish, behavioral analysis and prediction of water quality parameters. Further, the application of machine learning algorithms in aquaculture is outlined, and the results are analyzed. Finally, several current problems in aquaculture are highlighted, and the development trend is considered.} }
@article{WOS:001343358300003, title = {Machine learning in agriculture domain: A state-of-art survey}, journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES}, volume = {1}, year = {2021}, doi = {10.1016/j.ailsci.2021.100010}, author = {Meshram, Vishal and Patil, Kailas and Meshram, Vidula and Hanchate, Dinesh and Ramkteke, S. D.}, abstract = {Food is considered as a basic need of human being which can be satisfied through farming. Agriculture not only fulfills humans' basic needs, but also considered as source of employment worldwide. Agriculture is considered as a backbone of economy and source of employment in the developing countries like India. Agriculture contributes 15.4\\% in the GDP of India. Agriculture activities are broadly categorized into three major areas: pre-harvesting, harvesting and post harvesting. Advancement in area of machine learning has helped improving gains in agriculture. Machine learning is the current technology which is benefiting farmers to minimize the losses in the farming by providing rich recommendations and insights about the crops. This paper presents an extensive survey of latest machine learning application in agriculture to alleviate the problems in the three areas of pre-harvesting, harvesting and post-harvesting. Application of machine learning in agriculture allows more efficient and precise farming with less human manpower with high quality production.} }
@article{WOS:000605576700001, title = {Tackling Photonic Inverse Design with Machine Learning}, journal = {ADVANCED SCIENCE}, volume = {8}, year = {2021}, doi = {10.1002/advs.202002923}, author = {Liu, Zhaocheng and Zhu, Dayu and Raju, Lakshmi and Cai, Wenshan}, abstract = {Machine learning, as a study of algorithms that automate prediction and decision-making based on complex data, has become one of the most effective tools in the study of artificial intelligence. In recent years, scientific communities have been gradually merging data-driven approaches with research, enabling dramatic progress in revealing underlying mechanisms, predicting essential properties, and discovering unconventional phenomena. It is becoming an indispensable tool in the fields of, for instance, quantum physics, organic chemistry, and medical imaging. Very recently, machine learning has been adopted in the research of photonics and optics as an alternative approach to address the inverse design problem. In this report, the fast advances of machine-learning-enabled photonic design strategies in the past few years are summarized. In particular, deep learning methods, a subset of machine learning algorithms, dealing with intractable high degrees-of-freedom structure design are focused upon.} }
@article{WOS:000493720200024, title = {Definitions, methods, and applications in interpretable machine learning}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {116}, pages = {22071-22080}, year = {2019}, issn = {0027-8424}, doi = {10.1073/pnas.1900654116}, author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin}, abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.} }
@article{WOS:000688449200019, title = {Comprehensive Survey on Machine Learning in Vehicular Network: Technology, Applications and Challenges}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {23}, pages = {2027-2057}, year = {2021}, doi = {10.1109/COMST.2021.3089688}, author = {Tang, Fengxiao and Mao, Bomin and Kato, Nei and Gui, Guan}, abstract = {Towards future intelligent vehicular network, the machine learning as the promising artificial intelligence tool is widely researched to intelligentize communication and networking functions. In this paper, we provide a comprehensive survey on various machine learning techniques applied to both communication and network parts in vehicular network. To benefit reading, we first give a preliminary on communication technologies and machine learning technologies in vehicular network. Then, we detailedly describe the challenges of conventional techniques in vehicular network and corresponding machine learning based solutions. Finally, we present several open issues and emphasize potential directions that are worthy of research for the future intelligent vehicular network.} }
@article{WOS:000702995500001, title = {Innovative Materials Science via Machine Learning}, journal = {ADVANCED FUNCTIONAL MATERIALS}, volume = {32}, year = {2022}, issn = {1616-301X}, doi = {10.1002/adfm.202108044}, author = {Gao, Chaochao and Min, Xin and Fang, Minghao and Tao, Tianyi and Zheng, Xiaohong and Liu, Yangai and Wu, Xiaowen and Huang, Zhaohui}, abstract = {Nowadays, the research on materials science is rapidly entering a phase of data-driven age. Machine learning, one of the most powerful data-driven methods, have been being applied to materials discovery and performances prediction with undoubtedly tremendous application foreground. Herein, the challenges and current progress of machine learning are summarized in materials science, the design strategies are classified and highlighted, and possible perspectives are proposed for the future development. It is hoped this review can provide important scientific guidance for innovating materials science and technology via machine learning in the future.} }
@article{WOS:000660500300002, title = {Enhancing gravitational-wave science with machine learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abb93a}, author = {Cuoco, Elena and Powell, Jade and Cavaglia, Marco and Ackley, Kendall and Bejger, Michal and Chatterjee, Chayan and Coughlin, Michael and Coughlin, Scott and Easter, Paul and Essick, Reed and Gabbard, Hunter and Gebhard, Timothy and Ghosh, Shaon and Haegel, Leila and Iess, Alberto and Keitel, David and Marka, Zsuzsa and Marka, Szabolcs and Morawski, Filip and Nguyen, Tri and Ormiston, Rich and Puerrer, Michael and Razzano, Massimiliano and Staats, Kai and Vajente, Gabriele and Williams, Daniel}, abstract = {Machine learning has emerged as a popular and powerful approach for solving problems in astrophysics. We review applications of machine learning techniques for the analysis of ground-based gravitational-wave (GW) detector data. Examples include techniques for improving the sensitivity of Advanced Laser Interferometer GW Observatory and Advanced Virgo GW searches, methods for fast measurements of the astrophysical parameters of GW sources, and algorithms for reduction and characterization of non-astrophysical detector noise. These applications demonstrate how machine learning techniques may be harnessed to enhance the science that is possible with current and future GW detectors.} }
@article{WOS:000517788300001, title = {An Introduction to Machine Learning}, journal = {CLINICAL PHARMACOLOGY \\& THERAPEUTICS}, volume = {107}, pages = {871-885}, year = {2020}, issn = {0009-9236}, doi = {10.1002/cpt.1796}, author = {Badillo, Solveig and Banfai, Balazs and Birzele, Fabian and Davydov, Iakov I. and Hutchinson, Lucy and Kam-Thong, Tony and Siebourg-Polster, Juliane and Steiert, Bernhard and Zhang, Jitao David}, abstract = {In the last few years, machine learning (ML) and artificial intelligence have seen a new wave of publicity fueled by the huge and ever-increasing amount of data and computational power as well as the discovery of improved learning algorithms. However, the idea of a computer learning some abstract concept from data and applying them to yet unseen situations is not new and has been around at least since the 1950s. Many of these basic principles are very familiar to the pharmacometrics and clinical pharmacology community. In this paper, we want to introduce the foundational ideas of ML to this community such that readers obtain the essential tools they need to understand publications on the topic. Although we will not go into the very details and theoretical background, we aim to point readers to relevant literature and put applications of ML in molecular biology as well as the fields of pharmacometrics and clinical pharmacology into perspective.} }
@article{WOS:000921883400006, title = {Machine Learning, Functions and Goals}, journal = {CROATIAN JOURNAL OF PHILOSOPHY}, volume = {22}, pages = {351-370}, year = {2022}, issn = {1333-1108}, doi = {10.52685/cjp.22.66.5}, author = {Butlin, Patrick}, abstract = {Machine learning researchers distinguish between reinforcement learning and supervised learning and refer to reinforcement learning systems as ``agents''. This paper vindicates the claim that systems trained by reinforcement learning are agents while those trained by supervised learning are not. Systems of both kinds satisfy Dretske's criteria for agency, because they both learn to produce outputs selectively in response to inputs. However, reinforcement learning is sensitive to the instrumental value of outputs, giving rise to systems which exploit the effects of outputs on subsequent inputs to achieve good performance over episodes of interaction with their environments. Supervised learning systems, in contrast, merely learn to produce better outputs in response to individual inputs.} }
@article{WOS:000663500200010, title = {Machine learning algorithms for social media analysis: A survey}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100395}, author = {Balaji, T. K. and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree}, abstract = {Social Media (SM) are the most widespread and rapid data generation applications on the Internet increase the study of these data. However, the efficient processing of such massive data is challenging, so we require a system that learns from these data, like machine learning. Machine learning methods make the systems to learn itself. Many papers are published on SM using machine learning approaches over the past few decades. In this paper, we provide a comprehensive survey of multiple applications of SM analysis using robust machine learning algorithms. Initially, we discuss a summary of machine learning algorithms, which are used in SM analysis. After that, we provide a detailed survey of machine learning approaches to SM analysis. Furthermore, we summarize the challenges and benefits of Machine Learning usages in SM analysis. Finally, we presented open issues and consequences in SM analysis for further research. (c) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000751704800103, title = {A Survey of Topological Machine Learning Methods}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {4}, year = {2021}, doi = {10.3389/frai.2021.681108}, author = {Hensel, Felix and Moor, Michael and Rieck, Bastian}, abstract = {The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as ``topological machine learning,'' i.e., the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.} }
@article{WOS:000719226000003, title = {Machine-learning interpretability techniques for seismic performance assessment of infrastructure systems}, journal = {ENGINEERING STRUCTURES}, volume = {250}, year = {2022}, issn = {0141-0296}, doi = {10.1016/j.engstruct.2021.112883}, author = {Mangalathu, Sujith and Karthikeyan, Karthika and Feng, De-Cheng and Jeon, Jong-Su}, abstract = {Machine-learning has recently gained considerable attention in the earthquake engineering community, as it can map the complex relationship between the expected damage and the input parameters. It is often necessary to understand the reasons for the behavior and predictions of the machine-learning model. This paper addresses this issue through interpretable machine-learning approaches such as partial dependence plots, accumulated local effects, and Shapely additive explanations. The evaluation of these approaches is carried out (1) at a component level by analyzing the shear strength predictions by a machine-learning model and (2) at a regional level through the machine-learning model for the regional damage assessment of bridges in California. The comparison helps to identify (1) the proper implementation of these approaches for the efficient use of machine-learning models and (2) key influential variables and thresholds that govern the prediction of the machine-learning models.} }
@article{WOS:000656789000001, title = {Role of machine learning in medical research: A survey}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100370}, author = {Garg, Arunim and Mago, Vijay}, abstract = {Machine learning is one of the essential and effective tools in analyzing highly complex medical data. With vast amounts of medical data being generated, there is an urgent need to effectively use this data to benefit the medical and health care sectors all across the world. This survey paper presents a systematic literature review for the investigation of various machine learning techniques used for numerous medical applications which are published in highly reputable venues in recent years. Considering only the recent work, we are able to survey the current machine learning and deep learning models that are being used for medical data. This literature review identifies a clear shift of artificial intelligence techniques used in the medical domain, with deep learning methods taking precedence over machine learning methods. (C) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000656859400017, title = {Machine learning for biochemical engineering: A review}, journal = {BIOCHEMICAL ENGINEERING JOURNAL}, volume = {172}, year = {2021}, issn = {1369-703X}, doi = {10.1016/j.bej.2021.108054}, author = {Mowbray, Max and Savage, Thomas and Wu, Chufan and Song, Ziqi and Cho, Bovinille Anye and Del Rio-Chanona, Ehecatl A. and Zhang, Dongda}, abstract = {The field of machine learning is comprised of techniques, which have proven powerful approaches to knowledge discovery and construction of `digital twins' in the highly dimensional, nonlinear and stochastic domains common to biochemical engineering. We review the use of machine learning within biochemical engineering over the last 20 years. The most prevalent machine learning methods are demystified, and their impact across individual biochemical engineering subfields is outlined. In doing so we provide insights into the true benefits of each technique, and obstacles for their wider deployment. Finally, core challenges into the application of machine learning in biochemical engineering are thoroughly discussed, and further insight into adoption of innovative hybrid modelling and transfer learning strategies for development of new digital biotechnologies is provided.} }
@article{WOS:000664988000001, title = {Machine Learning-Reinforced Noninvasive Biosensors for Healthcare}, journal = {ADVANCED HEALTHCARE MATERIALS}, volume = {10}, year = {2021}, issn = {2192-2640}, doi = {10.1002/adhm.202100734}, author = {Zhang, Kaiyi and Wang, Jianwu and Liu, Tianyi and Luo, Yifei and Loh, Xian Jun and Chen, Xiaodong}, abstract = {The emergence and development of noninvasive biosensors largely facilitate the collection of physiological signals and the processing of health-related data. The utilization of appropriate machine learning algorithms improves the accuracy and efficiency of biosensors. Machine learning-reinforced biosensors are started to use in clinical practice, health monitoring, and food safety, bringing a digital revolution in healthcare. Herein, the recent advances in machine learning-reinforced noninvasive biosensors applied in healthcare are summarized. First, different types of noninvasive biosensors and physiological signals collected are categorized and summarized. Then machine learning algorithms adopted in subsequent data processing are introduced and their practical applications in biosensors are reviewed. Finally, the challenges faced by machine learning-reinforced biosensors are raised, including data privacy and adaptive learning capability, and their prospects in real-time monitoring, out-of-clinic diagnosis, and onsite food safety detection are proposed.} }
@article{WOS:000624582400019, title = {A Review of Using Machine Learning Approaches for Precision Education}, journal = {EDUCATIONAL TECHNOLOGY \\& SOCIETY}, volume = {24}, pages = {250-266}, year = {2021}, issn = {1176-3647}, author = {Luan, Hui and Tsai, Chin-Chung}, abstract = {In recent years, in the field of education, there has been a clear progressive trend toward precision education. As a rapidly evolving AI technique, machine learning is viewed as an important means to realize it. In this paper, we systematically review 40 empirical studies regarding machine-learning-based precision education. The results showed that the majority of studies focused on the prediction of learning performance or dropouts, and were carried out in online or blended learning environments among university students majoring in computer science or STEM, whereas the data sources were divergent. The commonly used machine learning algorithms, evaluation methods, and validation approaches are presented. The emerging issues and future directions are discussed accordingly.} }
@article{WOS:000709474600010, title = {Machine learning and applications in microbiology}, journal = {FEMS MICROBIOLOGY REVIEWS}, volume = {45}, year = {2021}, issn = {0168-6445}, doi = {10.1093/femsre/fuab015}, author = {Goodswen, Stephen J. and Barratt, Joel L. N. and Kennedy, Paul J. and Kaufer, Alexa and Calarco, Larissa and Ellis, John T.}, abstract = {To understand the intricacies of microorganisms at the molecular level requires making sense of copious volumes of data such that it may now be humanly impossible to detect insightful data patterns without an artificial intelligence application called machine learning. Applying machine learning to address biological problems is expected to grow at an unprecedented rate, yet it is perceived by the uninitiated as a mysterious and daunting entity entrusted to the domain of mathematicians and computer scientists. The aim of this review is to identify key points required to start the journey of becoming an effective machine learning practitioner. These key points are further reinforced with an evaluation of how machine learning has been applied so far in a broad scope of real-life microbiology examples. This includes predicting drug targets or vaccine candidates, diagnosing microorganisms causing infectious diseases, classifying drug resistance against antimicrobial medicines, predicting disease outbreaks and exploring microbial interactions. Our hope is to inspire microbiologists and other related researchers to join the emerging machine learning revolution.} }
@article{WOS:000646865900001, title = {Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology}, journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION}, volume = {3}, pages = {392-413}, year = {2021}, doi = {10.3390/make3020020}, author = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and Mueller, Klaus-Robert}, abstract = {Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.} }
@article{WOS:000696667700006, title = {How does Machine Learning Change Software Development Practices?}, journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING}, volume = {47}, pages = {1857-1871}, year = {2021}, issn = {0098-5589}, doi = {10.1109/TSE.2019.2937083}, author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.}, abstract = {Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.} }
@article{WOS:000628641300001, title = {A Survey of Machine Learning Techniques for Indoor Localization and Navigation Systems}, journal = {JOURNAL OF INTELLIGENT \\& ROBOTIC SYSTEMS}, volume = {101}, year = {2021}, issn = {0921-0296}, doi = {10.1007/s10846-021-01327-z}, author = {Roy, Priya and Chowdhury, Chandreyee}, abstract = {In the recent past, we have witnessed the adoption of different machine learning techniques for indoor positioning applications using WiFi, Bluetooth and other technologies. The techniques range from heuristically derived hand-crafted feature-based traditional machine learning algorithms, feature selection algorithms to the hierarchically self-evolving feature-based Deep Learning algorithms. The transient and chaotic nature of the WiFi/Bluetooth fingerprint data along with different signal sensitivity of different device configurations presents numerous challenges that influence the performance of the indoor localization system in the wild. This article is intended to offer a comprehensive state-of-the-art survey on machine learning techniques that have recently been adopted for localization purposes. Hence, we review the applicability of machine learning techniques in this domain along with basic localization principles, applications, and the underlying problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art machine learning techniques to visualize the possible future directions in the research field of indoor localization.} }
@article{WOS:000701765000004, title = {Machine learning applied to the design and inspection of reinforced concrete bridges: Resilient methods and emerging applications}, journal = {STRUCTURES}, volume = {33}, pages = {3954-3963}, year = {2021}, issn = {2352-0124}, doi = {10.1016/j.istruc.2021.06.110}, author = {Fan, Weiying and Chen, Yao and Li, Jiaqiang and Sun, Yue and Feng, Jian and Hassanin, Hany and Sareh, Pooya}, abstract = {Machine learning is one of the key pillars of industry 4.0 that has enabled rapid technological advancement through establishing complex connections among heterogeneous and highly complex engineering data automatically. Once the machine learning model is trained appropriately, it becomes able to effectively predict and make decisions. The technology is rapidly evolving and has found numerous applications in various branches of engineering due to its preponderance. This study is focused on exploring the recent advances of machine learning and its applications in reinforced concrete bridges. It covers a range of different machine learning techniques exploited in structural design, construction quality management, bridge engineering, and the inspection of reinforced concrete bridges. This review demonstrated that machine learning algorithms have established new research directions in bridge engineering, in particular for applications such as the form-finding of innovative long-span structures, structural reinforcement, and structural optimization.} }
@article{WOS:000723910100001, title = {Machine Learning (ML) in Medicine: Review, Applications, and Challenges}, journal = {MATHEMATICS}, volume = {9}, year = {2021}, doi = {10.3390/math9222970}, author = {Rahmani, Amir Masoud and Yousefpoor, Efat and Yousefpoor, Mohammad Sadegh and Mehmood, Zahid and Haider, Amir and Hosseinzadeh, Mehdi and Ali Naqvi, Rizwan}, abstract = {Today, artificial intelligence (AI) and machine learning (ML) have dramatically advanced in various industries, especially medicine. AI describes computational programs that mimic and simulate human intelligence, for example, a person's behavior in solving problems or his ability for learning. Furthermore, ML is a subset of artificial intelligence. It extracts patterns from raw data automatically. The purpose of this paper is to help researchers gain a proper understanding of machine learning and its applications in healthcare. In this paper, we first present a classification of machine learning-based schemes in healthcare. According to our proposed taxonomy, machine learning-based schemes in healthcare are categorized based on data pre-processing methods (data cleaning methods, data reduction methods), learning methods (unsupervised learning, supervised learning, semi-supervised learning, and reinforcement learning), evaluation methods (simulation-based evaluation and practical implementation-based evaluation in real environment) and applications (diagnosis, treatment). According to our proposed classification, we review some studies presented in machine learning applications for healthcare. We believe that this review paper helps researchers to familiarize themselves with the newest research on ML applications in medicine, recognize their challenges and limitations in this area, and identify future research directions.} }
@article{WOS:000731472800001, title = {Machine Learning in Action: Stroke Diagnosis and Outcome Prediction}, journal = {FRONTIERS IN NEUROLOGY}, volume = {12}, year = {2021}, issn = {1664-2295}, doi = {10.3389/fneur.2021.734345}, author = {Mainali, Shraddha and Darsie, Marin E. and Smetana, Keaton S.}, abstract = {The application of machine learning has rapidly evolved in medicine over the past decade. In stroke, commercially available machine learning algorithms have already been incorporated into clinical application for rapid diagnosis. The creation and advancement of deep learning techniques have greatly improved clinical utilization of machine learning tools and new algorithms continue to emerge with improved accuracy in stroke diagnosis and outcome prediction. Although imaging-based feature recognition and segmentation have significantly facilitated rapid stroke diagnosis and triaging, stroke prognostication is dependent on a multitude of patient specific as well as clinical factors and hence accurate outcome prediction remains challenging. Despite its vital role in stroke diagnosis and prognostication, it is important to recognize that machine learning output is only as good as the input data and the appropriateness of algorithm applied to any specific data set. Additionally, many studies on machine learning tend to be limited by small sample size and hence concerted efforts to collate data could improve evaluation of future machine learning tools in stroke. In the present state, machine learning technology serves as a helpful and efficient tool for rapid clinical decision making while oversight from clinical experts is still required to address specific aspects not accounted for in an automated algorithm. This article provides an overview of machine learning technology and a tabulated review of pertinent machine learning studies related to stroke diagnosis and outcome prediction.} }
@article{WOS:000684547900025, title = {Machine learning and earthquake forecasting-next steps}, journal = {NATURE COMMUNICATIONS}, volume = {12}, year = {2021}, doi = {10.1038/s41467-021-24952-6}, author = {Beroza, Gregory C. and Segou, Margarita and Mostafa Mousavi, S.}, abstract = {A new generation of earthquake catalogs developed through supervised machine-learning illuminates earthquake activity with unprecedented detail. Application of unsupervised machine learning to analyze the more complete expression of seismicity in these catalogs may be the fastest route to improving earthquake forecasting.} }
@article{WOS:000709466800001, title = {Semantic similarity and machine learning with ontologies}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {22}, year = {2021}, issn = {1467-5463}, doi = {10.1093/bib/bbaa199}, author = {Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and Hoehndorf, Robert}, abstract = {Ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. Recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. The methods employed to combine ontologies and machine learning are still novel and actively being developed. We provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. The methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/bio-ontology-research-group/machine-learning-with-ont ologies.} }
@article{WOS:000659549200030, title = {Data Evaluation and Enhancement for Quality Improvement of Machine Learning}, journal = {IEEE TRANSACTIONS ON RELIABILITY}, volume = {70}, pages = {831-847}, year = {2021}, issn = {0018-9529}, doi = {10.1109/TR.2021.3070863}, author = {Chen, Haihua and Chen, Jiangping and Ding, Junhua}, abstract = {Poor data quality has a direct impact on the performance of the machine learning system that is built on the data. As a demonstrated effective approach for data quality improvement, transfer learning has been widely used to improve machine learning quality. However, the ``quality improvement'' brought by transfer learning was rarely rigorously validated, and some of the quality improvement results were misleading. This article first exposed the hidden quality problem in the datasets used to build a machine learning system for normalizing medical concepts in social media text. The system was claimed to have achieved the best performance compared to existing work on a machine learning task. However, the results of our experiments showed that the ``best performance'' was due to the poor quality of the datasets and the defective validation process. To address the data quality issue and build a high-performance medical concept normalization system, we developed a transfer-learning-based strategy for data quality enhancement and system performance improvement. The results of the experiments showed a strong correlation between the quality of the datasets and the performance of the machine learning system. The results also demonstrated that a rigorous evaluation of data quality is necessary for guiding the quality improvement of machine learning. Therefore, we propose a data quality evaluation framework that includes the quality criteria and their corresponding evaluation approaches. The data validation process, the performance improvement strategy, and the data quality evaluation framework discussed in this article can be used for machine learning researchers and practitioners to build high-performance machine learning systems. The code and datasets used in this research are available in GitHub (https://github.com/haihua0913/dataEvaluationML).} }
@article{WOS:000649132600001, title = {Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft AI}, journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES}, volume = {379}, year = {2021}, issn = {1364-503X}, doi = {10.1098/rsta.2020.0083}, author = {Chantry, Matthew and Christensen, Hannah and Dueben, Peter and Palmer, Tim}, abstract = {In September 2019, a workshop was held to highlight the growing area of applying machine learning techniques to improve weather and climate prediction. In this introductory piece, we outline the motivations, opportunities and challenges ahead in this exciting avenue of research. This article is part of the theme issue `Machine learning for weather and climate modelling'.} }
@article{WOS:000627674800001, title = {Beneficial and harmful explanatory machine learning}, journal = {MACHINE LEARNING}, volume = {110}, pages = {695-721}, year = {2021}, issn = {0885-6125}, doi = {10.1007/s10994-020-05941-0}, author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, Celine and Gromowski, Mark and Schmid, Ute}, abstract = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine's involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.} }
@article{WOS:000690882400007, title = {Comparison of physical and machine learning models for estimating solar irradiance and photovoltaic power}, journal = {RENEWABLE ENERGY}, volume = {178}, pages = {1006-1019}, year = {2021}, issn = {0960-1481}, doi = {10.1016/j.renene.2021.06.079}, author = {Ramadhan, Raden A. A. and Heatubun, Yosca R. J. and Tan, Sek F. and Lee, Hyun-Jin}, abstract = {Conventional models to estimate solar irradiance and photovoltaic power rely on physics and use empirical correlations to handle regional climate and complex physics. Recently, machine learning emerges as an advanced statistical tool to construct more accurate correlations between inputs and outputs. Although machine learning has been applied for modeling solar irradiance and power, no study has reported the accuracy improvement by machine learning compared to conventional physical models. Hence, this study aims to compare the accuracies of physical and machine learning models at each step of solar power modeling, i.e., modeling of global horizontal irradiance, direct normal irradiance, global tilted irradiance, and photovoltaic power. Comparison results demonstrated that machine learning models generally outperform physical models when input parameters are appropriately selected. Machine learning models more significantly reduced the mean bias difference (MBD) than the root mean square difference (RMSD). For global horizontal irradiance and photovoltaic power, machine learning models led to substantially unbiased estimations with 0.96\\% and 0.03\\% of MBD, respectively. Among machine learning algorithms, long short-term memory and gated recurrent unit were more recommendable. However, the physical model for solar power estimation was more efficient to reduce RMSD because of their ability to consider constant parameters as input. (C) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000660500300003, title = {Quantum machine learning in high energy physics}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abc17d}, author = {Guan, Wen and Perdue, Gabriel and Pesah, Arthur and Schuld, Maria and Terashi, Koji and Vallecorsa, Sofia and Vlimant, Jean-Roch}, abstract = {Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.} }
@article{WOS:000702918200001, title = {Machine learning for predicting thermal transport properties of solids}, journal = {MATERIALS SCIENCE \\& ENGINEERING R-REPORTS}, volume = {146}, year = {2021}, issn = {0927-796X}, doi = {10.1016/j.mser.2021.100642}, author = {Qian, Xin and Yang, Ronggui}, abstract = {Quantitative descriptions of the structure-thermal property correlation have always been a challenging bottleneck in designing functional materials with superb thermal properties. In the past decade, the first-principlesbased modeling of phonon properties using density functional theory and the Boltzmann transport equation has become a common practice for predicting the thermal conductivity of new materials. However, firstprinciples calculations of thermal properties are too costly for high-throughput material screening and multiscale structural design. First-principles calculations also face several fundamental challenges in modeling thermal transport properties, for example, of crystalline materials with defects, of amorphous materials, and for materials at high temperatures. In the past five years or so, machine learning started to play a role in solving the aforementioned challenges. This review provides a comprehensive summary and discussion on the state-of-theart, future opportunities, and the remaining challenges in implementing machine learning techniques for studying thermal conductivity. After a brief introduction to the working principles of machine learning algorithms and descriptors for characterizing material structures, recent research using machine learning to study nanoscale thermal transport is discussed. Three major applications of machine learning techniques for predicting thermal properties are discussed. First, machine learning is applied to solve the challenges in modeling phonon transport of crystals with defects, in amorphous materials, and at high temperatures. In particular, machine learning is used to build high-fidelity interatomic potentials to bridge the gap between first-principles calculations and empirical molecular dynamics simulations. Second, machine learning can be used to study the correlation between thermal conductivity and other relevant properties for the high-throughput screening of functional materials. Finally, machine learning is a powerful tool for structural design to achieve target thermal conductance or thermal conductivity. This review concludes with a summary and outlook for future directions for implementing machine learning in thermal sciences.} }
@article{WOS:000637712400007, title = {Practical issues in implementing machine-learning models for building energy efficiency: Moving beyond obstacles}, journal = {RENEWABLE \\& SUSTAINABLE ENERGY REVIEWS}, volume = {143}, year = {2021}, issn = {1364-0321}, doi = {10.1016/j.rser.2021.110929}, author = {Wang, Zeyu and Liu, Jian and Zhang, Yuanxin and Yuan, Hongping and Zhang, Ruixue and Srinivasan, Ravi S.}, abstract = {Implementing machine-learning models in real applications is crucial to achieving intelligent building control and high energy efficiency. Over the past few decades, numerous studies have attempted to explore the application of machine-learning models to building energy efficiency. However, these studies have focused on analyzing the technical feasibility and superiority of machine learning algorithms for fitting building energyrelated data and have not considered methods of implementing machine learning technology in building energy efficiency applications. Therefore, this review aims to summarize the current practical issues involved in applying machine-learning models to building energy efficiency by systematically analyzing existing research findings and limitations. The paper first reviews the application status of machine learning-based building energy efficiency research by analyzing the model implementation process and summarizing the main uses of the technology in the overall building energy management life cycle. The paper then elaborates on the causes of, influences on, and potential solutions for practical issues found in the implementation and promotion of machine learning-based building energy efficiency measures. Finally, this paper discusses valuable future machine learning-based building energy efficiency research directions with regard to technology opportunity discovery, data governance, feature engineering, generalizability test, technology diffusion, and knowledge sharing. This paper will provide building researchers and practitioners with a better understanding of the current application statuses of and potential research directions for machine learning models in building energy efficiency.} }
@article{WOS:000687473600006, title = {Bayesian networks for interpretable machine learning and optimization}, journal = {NEUROCOMPUTING}, volume = {456}, pages = {648-665}, year = {2021}, issn = {0925-2312}, doi = {10.1016/j.neucom.2021.01.138}, author = {Mihaljevic, Bojan and Bielza, Concha and Larranaga, Pedro}, abstract = {As artificial intelligence is being increasingly used for high-stakes applications, it is becoming more and more important that the models used be interpretable. Bayesian networks offer a paradigm for inter-pretable artificial intelligence that is based on probability theory. They provide a semantics that enables a compact, declarative representation of a joint probability distribution over the variables of a domain by leveraging the conditional independencies among them. The representation consists of a directed acyclic graph that encodes the conditional independencies among the variables and a set of parameters that encodes conditional distributions. This representation has provided a basis for the development of algo-rithms for probabilistic reasoning (inference) and for learning probability distributions from data. Bayesian networks are used for a wide range of tasks in machine learning, including clustering, super -vised classification, multi-dimensional supervised classification, anomaly detection, and temporal mod-eling. They also provide a basis for estimation of distribution algorithms, a class of evolutionary algorithms for heuristic optimization. We illustrate the use of Bayesian networks for interpretable machine learning and optimization by presenting applications in neuroscience, the industry, and bioin-formatics, covering a wide range of machine learning and optimization tasks. (c) 2021 Published by Elsevier B.V.} }
@article{WOS:000645724900001, title = {Incorporating Machine Learning into Established Bioinformatics Frameworks}, journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}, volume = {22}, year = {2021}, issn = {1661-6596}, doi = {10.3390/ijms22062903}, author = {Auslander, Noam and Gussow, Ayal B. and Koonin, Eugene V.}, abstract = {The exponential growth of biomedical data in recent years has urged the application of numerous machine learning techniques to address emerging problems in biology and clinical research. By enabling the automatic feature extraction, selection, and generation of predictive models, these methods can be used to efficiently study complex biological systems. Machine learning techniques are frequently integrated with bioinformatic methods, as well as curated databases and biological networks, to enhance training and validation, identify the best interpretable features, and enable feature and model investigation. Here, we review recently developed methods that incorporate machine learning within the same framework with techniques from molecular evolution, protein structure analysis, systems biology, and disease genomics. We outline the challenges posed for machine learning, and, in particular, deep learning in biomedicine, and suggest unique opportunities for machine learning techniques integrated with established bioinformatics approaches to overcome some of these challenges.} }
@article{WOS:000607931400002, title = {Eight ways machine learning is assisting medicine}, journal = {NATURE MEDICINE}, volume = {27}, pages = {2-3}, year = {2021}, issn = {1078-8956}, doi = {10.1038/s41591-020-01197-2}, author = {May, Mike}, abstract = {There has been a lot of hype around the applications of machine learning in medicine. But how is machine learning actually helping bench-to-bedside scientists and clinicians do their jobs?} }
@article{WOS:000623811400031, title = {Review and analysis of supervised machine learning algorithms for hazardous events in drilling operations}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {147}, pages = {367-384}, year = {2021}, issn = {0957-5820}, doi = {10.1016/j.psep.2020.09.038}, author = {Osarogiagbon, Augustine Uhunoma and Khan, Faisal and Venkatesan, Ramachandran and Gillard, Paul}, abstract = {Results of bibliometric analysis and a detailed review are reported on the use of supervised machine learning to study hazardous drilling events. The bibliometric analysis attempts to answer pertinent questions related to progress in the use of supervised machine learning for hazardous events due to drilling fluid density/mud weight. The analysis indicates artificial neural network as the most popular algorithm among researchers. Also, deep learning, random forest and support vector machine have gained momentum in recent use. A critical review of literature on hazardous events and supervised machine learning algorithms are reported. This review was done to observe how the algorithms were used, their relative successes, limitations, as well as input parameters which aided in detection or estimation by the machine learning algorithms. An introduction to deep learning and a review of literature on the use of deep learning with respect to operations involving drilling parameters is presented. The review on deep learning and drilling parameters covered the following operations: lithology identification, drilling rig state determination, generating logging/other drilling parameters and detecting abnormality in data. The study highlights need of publicly accessible large database with data from different oilfields for development of machine learning algorithms. These algorithms could be used globally for the enhancement of machine learning for new fields or fields with limited data. The availability of such large database would aid researchers in improving or customizing deep learning algorithms in line with the unique needs of drilling activities. (C) 2020 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000700671700001, title = {Machine Learning of Spatial Data}, journal = {ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION}, volume = {10}, year = {2021}, doi = {10.3390/ijgi10090600}, author = {Nikparvar, Behnam and Thill, Jean-Claude}, abstract = {Properties of spatially explicit data are often ignored or inadequately handled in machine learning for spatial domains of application. At the same time, resources that would identify these properties and investigate their influence and methods to handle them in machine learning applications are lagging behind. In this survey of the literature, we seek to identify and discuss spatial properties of data that influence the performance of machine learning. We review some of the best practices in handling such properties in spatial domains and discuss their advantages and disadvantages. We recognize two broad strands in this literature. In the first, the properties of spatial data are developed in the spatial observation matrix without amending the substance of the learning algorithm; in the other, spatial data properties are handled in the learning algorithm itself. While the latter have been far less explored, we argue that they offer the most promising prospects for the future of spatial machine learning.} }
@article{WOS:000425056700007, title = {Implementation of machine-learning classification in remote sensing: an applied review}, journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING}, volume = {39}, pages = {2784-2817}, year = {2018}, issn = {0143-1161}, doi = {10.1080/01431161.2018.1433343}, author = {Maxwell, Aaron E. and Warner, Timothy A. and Fang, Fang}, abstract = {Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.} }
@article{WOS:000432490900005, title = {Feature selection in machine learning: A new perspective}, journal = {NEUROCOMPUTING}, volume = {300}, pages = {70-79}, year = {2018}, issn = {0925-2312}, doi = {10.1016/j.neucom.2017.11.077}, author = {Cai, Jie and Luo, Jiawei and Wang, Shulin and Yang, Sheng}, abstract = {High-dimensional data analysis is a challenge for researchers and engineers in the fields of machine learning and data mining. Feature selection provides an effective way to solve this problem by removing irrelevant and redundant data, which can reduce computation time, improve learning accuracy, and facilitate a better understanding for the learning model or data. In this study, we discuss several frequentlyused evaluation measures for feature selection, and then survey supervised, unsupervised, and semisupervised feature selection methods, which are widely applied in machine learning problems, such as classification and clustering. Lastly, future challenges about feature selection are discussed.} }
@article{WOS:000643700200002, title = {A review of possible effects of cognitive biases on interpretation of rule-based machine learning models}, journal = {ARTIFICIAL INTELLIGENCE}, volume = {295}, year = {2021}, issn = {0004-3702}, doi = {10.1016/j.artint.2021.103458}, author = {Kliegr, Tomas and Bahnik, Stepan and Fuernkranz, Johannes}, abstract = {While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. The goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain. (C) 2021 The Authors. Published by Elsevier B.V.} }
@article{WOS:000624645700001, title = {Machine Learning Techniques for THz Imaging and Time-Domain Spectroscopy}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21041186}, author = {Park, Hochong and Son, Joo-Hiuk}, abstract = {Terahertz imaging and time-domain spectroscopy have been widely used to characterize the properties of test samples in various biomedical and engineering fields. Many of these tasks require the analysis of acquired terahertz signals to extract embedded information, which can be achieved using machine learning. Recently, machine learning techniques have developed rapidly, and many new learning models and learning algorithms have been investigated. Therefore, combined with state-of-the-art machine learning techniques, terahertz applications can be performed with high performance that cannot be achieved using modeling techniques that precede the machine learning era. In this review, we introduce the concept of machine learning and basic machine learning techniques and examine the methods for performance evaluation. We then summarize representative examples of terahertz imaging and time-domain spectroscopy that are conducted using machine learning.} }
@article{WOS:000663460400001, title = {A machine learning and deep learning based approach to predict the thermal performance of phase change material integrated building envelope}, journal = {BUILDING AND ENVIRONMENT}, volume = {199}, year = {2021}, issn = {0360-1323}, doi = {10.1016/j.buildenv.2021.107927}, author = {Bhamare, Dnyandip K. and Saikia, Pranaynil and Rathod, Manish K. and Rakshit, Dibakar and Banerjee, Jyotirmay}, abstract = {This study aims to develop a machine learning and deep learning-based model for thermal performance prediction of PCM integrated roof building. Performance prediction is carried out using the newly proposed MKR index. Five machine learning and one deep learning technique are explored in order to predict the thermal performance of PCM integrated roof considering variations in thermophysical properties of PCM. Total 500 data points are generated using numerical simulations considering variations in thermophysical properties of PCM. The five machine learning models used in this study are Random forest regression, Extra trees regression, Gradient boosting regression, Extreme Gradient boosting regression, and Catboost regression. The results indicate that Gradient boosting regression is the best-performing model compared to other machine learning models. An artificial neural network is used as a deep learning approach for predicting the MKR index. The ANN-based model performed best among all five machine learning models and proved its efficacy in training, testing, and sensitivity analysis with the independent dataset.} }
@article{WOS:000697377500048, title = {mlr3proba: an R package for machine learning in survival analysis}, journal = {BIOINFORMATICS}, volume = {37}, pages = {2789-2791}, year = {2021}, issn = {1367-4803}, doi = {10.1093/bioinformatics/btab039}, author = {Sonabend, Raphael and Kiraly, Franz J. and Bender, Andreas and Bischl, Bernd and Lang, Michel}, abstract = {As machine learning has become increasingly popular over the last few decades, so too has the number of machine-learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering and more. mlr3proba provides a comprehensive machine-learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modelling and evaluation.} }
@article{WOS:000626579600078, title = {A Hybrid Posture Detection Framework: Integrating Machine Learning and Deep Neural Networks}, journal = {IEEE SENSORS JOURNAL}, volume = {21}, pages = {9515-9522}, year = {2021}, issn = {1530-437X}, doi = {10.1109/JSEN.2021.3055898}, author = {Liaqat, Sidrah and Dashtipour, Kia and Arshad, Kamran and Assaleh, Khaled and Ramzan, Naeem}, abstract = {The posture detection received lots of attention in the fields of human sensing and artificial intelligence. Posture detection can be used for the monitoring health status of elderly remotely by identifying their postures such as standing, sitting and walking. Most of the current studies used traditional machine learning classifiers to identify the posture. However, these methods do not perform well to detect the postures accurately. Therefore, in this study, we proposed a novel hybrid approach based on machine learning classifiers (i. e., support vector machine (SVM), logistic regression (KNN), decision tree, Naive Bayes, random forest, Linear discrete analysis and Quadratic discrete analysis) and deep learning classifiers (i. e., 1D-convolutional neural network (1D-CNN), 2D-convolutional neural network (2D-CNN), LSTM and bidirectional LSTM) to identify posture detection. The proposed hybrid approach uses prediction of machine learning (ML) and deep learning (DL) to improve the performance of ML and DL algorithms. The experimental results on widely benchmark dataset are shown and results achieved an accuracy of more than 98\\%.} }
@article{WOS:000533911600040, title = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence}, journal = {INFORMATION}, volume = {11}, year = {2020}, doi = {10.3390/info11040193}, author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey}, abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.} }
@article{WOS:000528284900001, title = {Engineering problems in machine learning systems}, journal = {MACHINE LEARNING}, volume = {109}, pages = {1103-1126}, year = {2020}, issn = {0885-6125}, doi = {10.1007/s10994-020-05872-w}, author = {Kuwajima, Hiroshi and Yasuoka, Hirotoshi and Nakae, Toshihiro}, abstract = {Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems-that is, in terms of requirement, design, and verification of machine learning models and systems-as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.} }
@article{WOS:000632089400001, title = {Machine Learning for Design Optimization of Electromagnetic Devices: Recent Developments and Future Directions}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11041627}, author = {Li, Yanbin and Lei, Gang and Bramerdorfer, Gerd and Peng, Sheng and Sun, Xiaodong and Zhu, Jianguo}, abstract = {This paper reviews the recent developments of design optimization methods for electromagnetic devices, with a focus on machine learning methods. First, the recent advances in multi-objective, multidisciplinary, multilevel, topology, fuzzy, and robust design optimization of electromagnetic devices are overviewed. Second, a review is presented to the performance prediction and design optimization of electromagnetic devices based on the machine learning algorithms, including artificial neural network, support vector machine, extreme learning machine, random forest, and deep learning. Last, to meet modern requirements of high manufacturing/production quality and lifetime reliability, several promising topics, including the application of cloud services and digital twin, are discussed as future directions for design optimization of electromagnetic devices.} }
@article{WOS:000704508900007, title = {Machine learning in orthopaedic surgery}, journal = {WORLD JOURNAL OF ORTHOPEDICS}, volume = {12}, pages = {685-699}, year = {2021}, issn = {2218-5836}, doi = {10.5312/wjo.v12.i9.685}, author = {Lalehzarian, Simon P. and Gowd, Anirudh K. and Liu, Joseph N.}, abstract = {Artificial intelligence and machine learning in orthopaedic surgery has gained mass interest over the last decade or so. In prior studies, researchers have demonstrated that machine learning in orthopaedics can be used for different applications such as fracture detection, bone tumor diagnosis, detecting hip implant mechanical loosening, and grading osteoarthritis. As time goes on, the utility of artificial intelligence and machine learning algorithms, such as deep learning, continues to grow and expand in orthopaedic surgery. The purpose of this review is to provide an understanding of the concepts of machine learning and a background of current and future orthopaedic applications of machine learning in risk assessment, outcomes assessment, imaging, and basic science fields. In most cases, machine learning has proven to be just as effective, if not more effective, than prior methods such as logistic regression in assessment and prediction. With the help of deep learning algorithms, such as artificial neural networks and convolutional neural networks, artificial intelligence in orthopaedics has been able to improve diagnostic accuracy and speed, flag the most critical and urgent patients for immediate attention, reduce the amount of human error, reduce the strain on medical professionals, and improve care. Because machine learning has shown diagnostic and prognostic uses in orthopaedic surgery, physicians should continue to research these techniques and be trained to use these methods effectively in order to improve orthopaedic treatment.} }
@article{WOS:000718837900001, title = {An introduction to quantum machine learning: from quantum logic to quantum deep learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {3}, year = {2021}, issn = {2524-4906}, doi = {10.1007/s42484-021-00056-8}, author = {Alchieri, Leonardo and Badalotti, Davide and Bonardi, Pietro and Bianco, Simone}, abstract = {The aim of this work is to give an introduction for a non-practical reader to the growing field of quantum machine learning, which is a recent discipline that combines the research areas of machine learning and quantum computing. This work presents the most notable scientific literature about quantum machine learning, starting from the basics of quantum logic to some specific elements and algorithms of quantum computing (such as QRAM, Grover and HHL), in order to allow a better understanding of latest quantum machine learning techniques. The main aspects of quantum machine learning are then covered, with detailed descriptions of some notable algorithms, such as quantum natural gradient and quantum support vector machines, up to the most recent quantum deep learning techniques, such as quantum neural networks.} }
@article{WOS:000739121700001, title = {Machine learning for optical fiber communication systems: An introduction and overview}, journal = {APL PHOTONICS}, volume = {6}, year = {2021}, issn = {2378-0967}, doi = {10.1063/5.0070838}, author = {Nevin, Josh W. and Nallaperuma, Sam and Shevchenko, Nikita A. and Li, Xiang and Faruk, Md. Saifuddin and Savory, Seb J.}, abstract = {Optical networks generate a vast amount of diagnostic, control, and performance monitoring data. When information is extracted from these data, reconfigurable network elements and reconfigurable transceivers allow the network to adapt not only to changes in the physical infrastructure but also to changing traffic conditions. Machine learning is emerging as a disruptive technology for extracting useful information from these raw data to enable enhanced planning, monitoring, and dynamic control. We provide a survey of the recent literature and highlight numerous promising avenues for machine learning applied to optical networks, including explainable machine learning, digital twins, and approaches in which we embed our knowledge into machine learning such as physics-informed machine learning for the physical layer and graph-based machine learning for the networking layer.} }
@article{WOS:000703568200007, title = {Introduction to Artificial Intelligence and Machine Learning for Pathology}, journal = {ARCHIVES OF PATHOLOGY \\& LABORATORY MEDICINE}, volume = {145}, pages = {1228-1254}, year = {2021}, issn = {0003-9985}, doi = {10.5858/arpa.2020-0541-CP)}, author = {Harrison, James H. Jr Jr and Gilbertson, John R. and Hanna, Matthew G. and Olson, Niels H. and Seheult, Jansen N. and Sorace, James M. and Stram, Michelle N.}, abstract = {center dot Context.-Recent developments in machine learning have stimulated intense interest in software that may augment or replace human experts. Machine learning may impact pathology practice by offering new capabilities in analysis, interpretation, and outcomes prediction using images and other data. The principles of operation and management of machine learning systems are unfamiliar to pathologists, who anticipate a need for additional education to be effective as expert users and managers of the new tools. Objective.-To provide a background on machine learning for practicing pathologists, including an overview of algorithms, model development, and performance evaluation; to examine the current status of machine learning in pathology and consider possible roles and requirements for pathologists in local deployment and management of machine learning systems; and to highlight existing challenges and gaps in deployment methodology and regulation. Data Sources.-Sources include the biomedical and engineering literature, white papers from professional organizations, government reports, electronic resources, and authors' experience in machine learning. References were chosen when possible for accessibility to practicing pathologists without specialized training in mathematics, statistics, or software development. Conclusions.-Machine learning offers an array of techniques that in recent published results show substantial promise. Data suggest that human experts working with machine learning tools outperform humans or machines separately, but the optimal form for this combination in pathology has not been established. Significant questions related to the generalizability of machine learning systems, local site verification, and performance monitoring remain to be resolved before a consensus on best practices and a regulatory environment can be established. (Arch Pathol Lab Med. 2021;145:1228-1254 ; doi: 10.5858/arpa.2020-0541-CP)} }
@incollection{WOS:000713670600026, title = {Machine Learning for Sustainable Energy Systems}, booktitle = {ANNUAL REVIEW OF ENVIRONMENT AND RESOURCES, VOL 46, 2021}, volume = {46}, pages = {719-747}, year = {2021}, issn = {1543-5938}, isbn = {978-0-8243-2346-2}, doi = {10.1146/annurev-environ-020220-061831}, author = {Donti, Priya L. and Kolter, J. Zico}, abstract = {In recent years, machine learning has proven to be a powerful tool for deriving insights from data. In this review, we describe ways in which machine learning has been leveraged to facilitate the development and operation of sustainable energy systems. We first provide a taxonomy of machine learning paradigms and techniques, along with a discussion of their strengths and limitations. We then provide an overview of existing research using machine learning for sustainable energy production, delivery, and storage. Finally, we identify gaps in this literature, propose future research directions, and discuss important considerations for deployment.} }
@article{WOS:000701828000003, title = {Do machine learning platforms provide out-of-the-box reproducibility?}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {126}, pages = {34-47}, year = {2022}, issn = {0167-739X}, doi = {10.1016/j.future.2021.06.014}, author = {Gundersen, Odd Erik and Shamsaliei, Saeid and Isdahl, Richard Juul}, abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95\\% confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted. (C) 2021 The Author(s). Published by Elsevier B.V.} }
@article{WOS:000676750300001, title = {Machine Learning-A Review of Applications in Mineral Resource Estimation}, journal = {ENERGIES}, volume = {14}, year = {2021}, doi = {10.3390/en14144079}, author = {Dumakor-Dupey, Nelson K. and Arya, Sampurna}, abstract = {Mineral resource estimation involves the determination of the grade and tonnage of a mineral deposit based on its geological characteristics using various estimation methods. Conventional estimation methods, such as geometric and geostatistical techniques, remain the most widely used methods for resource estimation. However, recent advances in computer algorithms have allowed researchers to explore the potential of machine learning techniques in mineral resource estimation. This study presents a comprehensive review of papers that have employed machine learning to estimate mineral resources. The review covers popular machine learning techniques and their implementation and limitations. Papers that performed a comparative analysis of both conventional and machine learning techniques were also considered. The literature shows that the machine learning models can accommodate several geological parameters and effectively approximate complex nonlinear relationships among them, exhibiting superior performance over the conventional techniques.} }
@article{WOS:000611115200001, title = {An Empirical Review of Automated Machine Learning}, journal = {COMPUTERS}, volume = {10}, year = {2021}, issn = {2073-431X}, doi = {10.3390/computers10010011}, author = {Vaccaro, Lorenzo and Sansonetti, Giuseppe and Micarelli, Alessandro}, abstract = {In recent years, Automated Machine Learning (AutoML) has become increasingly important in Computer Science due to the valuable potential it offers. This is testified by the high number of works published in the academic field and the significant efforts made in the industrial sector. However, some problems still need to be resolved. In this paper, we review some Machine Learning (ML) models and methods proposed in the literature to analyze their strengths and weaknesses. Then, we propose their use-alone or in combination with other approaches-to provide possible valid AutoML solutions. We analyze those solutions from a theoretical point of view and evaluate them empirically on three Atari games from the Arcade Learning Environment. Our goal is to identify what, we believe, could be some promising ways to create truly effective AutoML frameworks, therefore able to replace the human expert as much as possible, thereby making easier the process of applying ML approaches to typical problems of specific domains. We hope that the findings of our study will provide useful insights for future research work in AutoML.} }
@article{WOS:000685591400001, title = {Applying Machine Learning Approaches to Suicide Prediction Using Healthcare Data: Overview and Future Directions}, journal = {FRONTIERS IN PSYCHIATRY}, volume = {12}, year = {2021}, issn = {1664-0640}, doi = {10.3389/fpsyt.2021.707916}, author = {Boudreaux, Edwin D. and Rundensteiner, Elke and Liu, Feifan and Wang, Bo and Larkin, Celine and Agu, Emmanuel and Ghosh, Samiran and Semeter, Joshua and Simon, Gregory and Davis-Martin, Rachel E.}, abstract = {Objective: Early identification of individuals who are at risk for suicide is crucial in supporting suicide prevention. Machine learning is emerging as a promising approach to support this objective. Machine learning is broadly defined as a set of mathematical models and computational algorithms designed to automatically learn complex patterns between predictors and outcomes from example data, without being explicitly programmed to do so. The model's performance continuously improves over time by learning from newly available data. Method: This concept paper explores how machine learning approaches applied to healthcare data obtained from electronic health records, including billing and claims data, can advance our ability to accurately predict future suicidal behavior. Results: We provide a general overview of machine learning concepts, summarize exemplar studies, describe continued challenges, and propose innovative research directions. Conclusion: Machine learning has potential for improving estimation of suicide risk, yet important challenges and opportunities remain. Further research can focus on incorporating evolving methods for addressing data imbalances, understanding factors that affect generalizability across samples and healthcare systems, expanding the richness of the data, leveraging newer machine learning approaches, and developing automatic learning systems.} }
@article{WOS:000611043000001, title = {Evidence of Inflated Prediction Performance: A Commentary on Machine Learning and Suicide Research}, journal = {CLINICAL PSYCHOLOGICAL SCIENCE}, volume = {9}, pages = {129-134}, year = {2021}, issn = {2167-7026}, doi = {10.1177/2167702620954216}, author = {Jacobucci, Ross and Littlefield, Andrew K. and Millner, Alexander J. and Kleiman, Evan M. and Steinley, Douglas}, abstract = {The use of machine learning is increasing in clinical psychology, yet it is unclear whether these approaches enhance the prediction of clinical outcomes. Several studies show that machine-learning algorithms outperform traditional linear models. However, many studies that have found such an advantage use the same algorithm, random forests with the optimism-corrected bootstrap, for internal validation. Through both a simulation and empirical example, we demonstrate that the pairing of nonlinear, flexible machine-learning approaches, such as random forests with the optimism-corrected bootstrap, provide highly inflated prediction estimates. We find no advantage for properly validated machine-learning models over linear models.} }
@article{WOS:000565731100014, title = {The myth of generalisability in clinical research and machine learning in health care}, journal = {LANCET DIGITAL HEALTH}, volume = {2}, pages = {E489-E492}, year = {2020}, author = {Futoma, Joseph and Simons, Morgan and Panch, Trishan and Doshi-Velez, Finale and Celi, Leo Anthony}, abstract = {An emphasis on overly broad notions of generalisability as it pertains to applications of machine learning in health care can overlook situations in which machine learning might provide clinical utility. We believe that this narrow focus on generalisability should be replaced with wider considerations for the ultimate goal of building machine learning systems that are useful at the bedside.} }
@article{WOS:000537106200097, title = {Edge Machine Learning for AI-Enabled IoT Devices: A Review}, journal = {SENSORS}, volume = {20}, year = {2020}, doi = {10.3390/s20092533}, author = {Merenda, Massimo and Porcaro, Carlo and Iero, Demetrio}, abstract = {In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors' data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning ``Hello World''.} }
@article{WOS:000542963900016, title = {Ten Challenges in Advancing Machine Learning Technologies toward 6G}, journal = {IEEE WIRELESS COMMUNICATIONS}, volume = {27}, pages = {96-103}, year = {2020}, issn = {1536-1284}, doi = {10.1109/MWC.001.1900476}, author = {Kato, Nei and Mao, Bomin and Tang, Fengxiao and Kawamoto, Yuichi and Liu, Jiajia}, abstract = {As the 5G standard is being completed, academia and industry have begun to consider a more developed cellular communication technique, 6G, which is expected to achieve high data rates up to 1 Tb/s and broad frequency bands of 100 GHz to 3 THz. Besides the significant upgrade of the key communication metrics, Artificial Intelligence (AI) has been envisioned by many researchers as the most important feature of 6G, since the state-of-the-art machine learning technique has been adopted as the top solution in many extremely complex scenarios. Network intelligentization will be the new trend to address the challenges of exponentially increasing number of connected heterogeneous devices. However, compared with the application of machine learning in other fields, such as computer games, current research on intelligent networking still has a long way to go to realize the automatically- configured cellular communication systems. Various problems in terms of communication system, machine learning architectures, and computation efficiency should be addressed for the full use of this technique in 6G. In this paper, we analyze machine learning techniques and introduce 10 most critical challenges in advancing the intelligent 6G system.} }
@article{WOS:000594889300001, title = {Machine Learning Methods in Drug Discovery}, journal = {MOLECULES}, volume = {25}, year = {2020}, doi = {10.3390/molecules25225277}, author = {Patel, Lauv and Shukla, Tripti and Huang, Xiuzhen and Ussery, David W. and Wang, Shanzhi}, abstract = {The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.} }
@article{WOS:000551576900008, title = {Machine Learning Applications for Mass Spectrometry-Based Metabolomics}, journal = {METABOLITES}, volume = {10}, year = {2020}, doi = {10.3390/metabo10060243}, author = {Liebal, Ulf W. and Phan, An N. T. and Sudhakar, Malvika and Raman, Karthik and Blank, Lars M.}, abstract = {The metabolome of an organism depends on environmental factors and intracellular regulation and provides information about the physiological conditions. Metabolomics helps to understand disease progression in clinical settings or estimate metabolite overproduction for metabolic engineering. The most popular analytical metabolomics platform is mass spectrometry (MS). However, MS metabolome data analysis is complicated, since metabolites interact nonlinearly, and the data structures themselves are complex. Machine learning methods have become immensely popular for statistical analysis due to the inherent nonlinear data representation and the ability to process large and heterogeneous data rapidly. In this review, we address recent developments in using machine learning for processing MS spectra and show how machine learning generates new biological insights. In particular, supervised machine learning has great potential in metabolomics research because of the ability to supply quantitative predictions. We review here commonly used tools, such as random forest, support vector machines, artificial neural networks, and genetic algorithms. During processing steps, the supervised machine learning methods help peak picking, normalization, and missing data imputation. For knowledge-driven analysis, machine learning contributes to biomarker detection, classification and regression, biochemical pathway identification, and carbon flux determination. Of important relevance is the combination of different omics data to identify the contributions of the various regulatory levels. Our overview of the recent publications also highlights that data quality determines analysis quality, but also adds to the challenge of choosing the right model for the data. Machine learning methods applied to MS-based metabolomics ease data analysis and can support clinical decisions, guide metabolic engineering, and stimulate fundamental biological discoveries.} }
@incollection{WOS:000590407100004, title = {Opportunities and Challenges for Machine Learning in Materials Science}, booktitle = {ANNUAL REVIEW OF MATERIALS RESEARCH, VOL 50, 2020}, volume = {50}, pages = {71-103}, year = {2020}, issn = {1531-7331}, isbn = {978-0-8243-1750-8}, doi = {10.1146/annurev-matsci-070218-010015}, author = {Morgan, Dane and Jacobs, Ryan}, abstract = {Advances in machine learning have impacted myriad areas of materials science, such as the discovery of novel materials and the improvement ofmolecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide amore detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.} }
@article{WOS:000596015500004, title = {Machine learning techniques for biomedical image segmentation: An overview of technical aspects and introduction to state-of-art applications}, journal = {MEDICAL PHYSICS}, volume = {47}, pages = {E148-E167}, year = {2020}, issn = {0094-2405}, doi = {10.1002/mp.13649}, author = {Seo, Hyunseok and Khuzani, Masoud Badiei and Vasudevan, Varun and Huang, Charles and Ren, Hongyi and Xiao, Ruoxiu and Jia, Xiao and Xing, Lei}, abstract = {In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep-learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep-learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past 3 yr. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.} }
@article{WOS:000546550100020, title = {Statistical and machine learning models in credit scoring: A systematic literature survey}, journal = {APPLIED SOFT COMPUTING}, volume = {91}, year = {2020}, issn = {1568-4946}, doi = {10.1016/j.asoc.2020.106263}, author = {Dastile, Xolani and Celik, Turgay and Potsane, Moshe}, abstract = {In practice, as a well-known statistical method, the logistic regression model is used to evaluate the credit-worthiness of borrowers due to its simplicity and transparency in predictions. However, in literature, sophisticated machine learning models can be found that can replace the logistic regression model. Despite the advances and applications of machine learning models in credit scoring, there are still two major issues: the incapability of some of the machine learning models to explain predictions; and the issue of imbalanced datasets. As such, there is a need for a thorough survey of recent literature in credit scoring. This article employs a systematic literature survey approach to systematically review statistical and machine learning models in credit scoring, to identify limitations in literature, to propose a guiding machine learning framework, and to point to emerging directions. This literature survey is based on 74 primary studies, such as journal and conference articles, that were published between 2010 and 2018. According to the meta-analysis of this literature survey, we found that in general, an ensemble of classifiers performs better than single classifiers. Although deep learning models have not been applied extensively in credit scoring literature, they show promising results. (C) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000643857400011, title = {Interpretable machine learning with an ensemble of gradient boosting machines}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {222}, year = {2021}, issn = {0950-7051}, doi = {10.1016/j.knosys.2021.106993}, author = {Konstantinov, V, Andrei and Utkin, V, Lev}, abstract = {A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation. (C) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000503751600001, title = {Comparing different supervised machine learning algorithms for disease prediction}, journal = {BMC MEDICAL INFORMATICS AND DECISION MAKING}, volume = {19}, year = {2019}, doi = {10.1186/s12911-019-1004-8}, author = {Uddin, Shahadat and Khan, Arif and Hossain, Md Ekramul and Moni, Mohammad Ali}, abstract = {Background Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction. Methods In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction. Results We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naive Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53\\%. This was followed by SVM which topped in 41\\% of the studies it was considered. Conclusion This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.} }
@article{WOS:000466380000034, title = {Big data and machine learning algorithms for health-care delivery}, journal = {LANCET ONCOLOGY}, volume = {20}, pages = {E262-E273}, year = {2019}, issn = {1470-2045}, doi = {10.1016/S1470-2045(19)30149-4}, author = {Ngiam, Kee Yuan and Khor, Ing Wei}, abstract = {Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data preprocessing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care.} }
@article{WOS:000517852500004, title = {Machine learning for enterprises: Applications, algorithm selection, and challenges}, journal = {BUSINESS HORIZONS}, volume = {63}, pages = {157-170}, year = {2020}, issn = {0007-6813}, doi = {10.1016/j.bushor.2019.10.005}, author = {Lee, In and Shin, Yong Jae}, abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications. (C) 2019 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights reserved.} }
@article{WOS:000540393400001, title = {Machine learning for continuous innovation in battery technologies}, journal = {NATURE REVIEWS MATERIALS}, volume = {5}, pages = {725-727}, year = {2020}, issn = {2058-8437}, doi = {10.1038/s41578-020-0216-y}, author = {Aykol, Muratahan and Herring, Patrick and Anapolsky, Abraham}, abstract = {Batteries, as complex materials systems, pose unique challenges for the application of machine learning. Although a shift to data-driven, machine learning-based battery research has started, new initiatives in academia and industry are needed to fully exploit its potential.} }
@article{WOS:000506166100099, title = {explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning}, journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}, volume = {26}, pages = {1064-1074}, year = {2020}, issn = {1077-2626}, doi = {10.1109/TVCG.2019.2934629}, author = {Spinner, Thilo and Schlegel, Udo and Schaefer, Hanna and El-Assady, Mennatallah}, abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.} }
@article{WOS:000554567500001, title = {A Review of Android Malware Detection Approaches Based on Machine Learning}, journal = {IEEE ACCESS}, volume = {8}, pages = {124579-124607}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3006143}, author = {Liu, Kaijun and Xu, Shengwei and Xu, Guoai and Zhang, Miao and Sun, Dawei and Liu, Haifeng}, abstract = {Android applications are developing rapidly across the mobile ecosystem, but Android malware is also emerging in an endless stream. Many researchers have studied the problem of Android malware detection and have put forward theories and methods from different perspectives. Existing research suggests that machine learning is an effective and promising way to detect Android malware. Notwithstanding, there exist reviews that have surveyed different issues related to Android malware detection based on machine learning. We believe our work complements the previous reviews by surveying a wider range of aspects of the topic. This paper presents a comprehensive survey of Android malware detection approaches based on machine learning. We briefly introduce some background on Android applications, including the Android system architecture, security mechanisms, and classification of Android malware. Then, taking machine learning as the focus, we analyze and summarize the research status from key perspectives such as sample acquisition, data preprocessing, feature selection, machine learning models, algorithms, and the evaluation of detection effectiveness. Finally, we assess the future prospects for research into Android malware detection based on machine learning. This review will help academics gain a full picture of Android malware detection based on machine learning. It could then serve as a basis for subsequent researchers to start new work and help to guide research in the field more generally.} }
@article{WOS:001483708400001, title = {Typical applications and perspectives of machine learning for advanced precision machining: A comprehensive review}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {283}, year = {2025}, issn = {0957-4174}, doi = {10.1016/j.eswa.2025.127770}, author = {Liang, Yiji and Dai, Canwen and Wang, Jingwei and Zhang, Guoqing and To, Suet and Zhao, Zejia}, abstract = {Advanced precision machining technologies, such as micro/ultraprecision mechanical machining and atomic and close-to-atomic scale manufacturing, are critical to high-value industries like aerospace and defense. However, extreme precision requirements and nonlinear dynamics pose significant challenges for accurate modeling, as traditional methods often struggle to capture intricate interactions and inherent variability. Machine learning emerges as a transformative solution, enabling data-driven modeling with unprecedented accuracy. This paper provides a comprehensive overview of the significant advancements and typical applications of machine learning in advanced precision machining, focusing on model architectures and methodologies to guide industrial implementation. For instance, this paper presents various examples, such as the application of LSTM networks in predicting tool life by capturing temporal dependencies in force signals, which illustrates how machine learning models are tailored to address specific challenges in precision machining. However, industrial adoption of machine learning remains hindered by limited datasets and computational constraints. This paper offers forward-looking recommendations to address these issues, integrating machine learning into precision machining within the framework of Industry 5.0 and providing robust support for the further promotion and application of machine learning in actual production environments. Furthermore, this research establishes a robust framework for recognizing similarities in machine learning applications across diverse machining domains, facilitating transfer learning among various advanced precision machining processes. By bridging the gap between theoretical models and industrial scalability, this review highlights the transformative role of machine learning in advanced precision machining toward intelligent, sustainable production, ultimately supporting highperformance component manufacturing.} }
@article{WOS:000669776700006, title = {Machine Learning for Soft Robotic Sensing and Control}, journal = {ADVANCED INTELLIGENT SYSTEMS}, volume = {2}, year = {2020}, doi = {10.1002/aisy.201900171}, author = {Chin, Keene and Hellebrekers, Tess and Majidi, Carmel}, abstract = {Herein, the progress of machine learning methods in the field of soft robotics, specifically in the applications of sensing and control, is outlined. Data-driven methods such as machine learning are especially suited to systems with governing functions that are unknown, impractical or impossible to represent analytically, or computationally intractable to integrate into real-world solutions. Function approximation with careful formulation of the machine learning architecture enables the encoding of dynamic behavior and nonlinearities, with the added potential to address hysteresis and nonstationary behavior. Supervised learning and reinforcement learning in simulation and on a wide variety of physical robotic systems have shown promising results for the use of empirical data-driven methods as a solution to contemporary soft robotics problems.} }
@article{WOS:000670264800013, title = {Advances of Four Machine Learning Methods for Spatial Data Handling: a Review}, journal = {JOURNAL OF GEOVISUALIZATION AND SPATIAL ANALYSIS}, volume = {4}, year = {2020}, issn = {2509-8810}, doi = {10.1007/s41651-020-00048-5}, author = {Du, Peijun and Bai, Xuyu and Tan, Kun and Xue, Zhaohui and Samat, Alim and Xia, Junshi and Li, Erzhu and Su, Hongjun and Liu, Wei}, abstract = {Most machine learning tasks can be categorized into classification or regression problems. Regression and classification models are normally used to extract useful geographic information from observed or measured spatial data, such as land cover classification, spatial interpolation, and quantitative parameter retrieval. This paper reviews the progress of four advanced machine learning methods for spatial data handling, namely, support vector machine (SVM)-based kernel learning, semi-supervised and active learning, ensemble learning, and deep learning. These four machine learning modes are representative because they improve learning performances from different views, for example, feature space transform and decision function (SVM), optimized uses of samples (semi-supervised and active learning), and enhanced learning models and capabilities (ensemble learning and deep learning). For spatial data handling via machine learning that can be improved by the four machine learning models, three key elements are learning algorithms, training samples, and input features. To apply machine learning methods to spatial data handling successfully, a four-level strategy is suggested: experimenting and evaluating the applicability, extending the algorithms by embedding spatial properties, optimizing the parameters for better performance, and enhancing the algorithm by multiple means. Firstly, the advances of SVM are reviewed to demonstrate the merits of novel machine learning methods for spatial data, running the line from direct use and comparison with traditional classifiers, and then targeted improvements to address multiple class problems, to optimize parameters of SVM, and to use spatial and spectral features. To overcome the limits of small-size training samples, semi-supervised learning and active learning methods are then utilized to deal with insufficient labeled samples, showing the potential of learning from small-size training samples. Furthermore, considering the poor generalization capacity and instability of machine learning algorithms, ensemble learning is introduced to integrate the advantages of multiple learners and to enhance the generalization capacity. The typical research lines, including the combination of multiple classifiers, advanced ensemble classifiers, and spatial interpolation, are presented. Finally, deep learning, one of the most popular branches of machine learning, is reviewed with specific examples for scene classification and urban structural type recognition from high-resolution remote sensing images. By this review, it can be concluded that machine learning methods are very effective for spatial data handling and have wide application potential in the big data era.} }
@article{WOS:000512357000002, title = {A Perspective on Using Machine Learning in 3D Bioprinting}, journal = {INTERNATIONAL JOURNAL OF BIOPRINTING}, volume = {6}, year = {2020}, issn = {2424-7723}, doi = {10.18063/ijb.v6i1.253}, author = {Yu, Chunling and Jiang, Jingchao}, abstract = {Recently, three-dimensional (3D) printing technologies have been widely applied in industry and our daily lives. The term 3D bioprinting has been coined to describe 3D printing at the biomedical level. Machine learning is currently becoming increasingly active and has been used to improve 3D printing processes, such as process optimization, dimensional accuracy analysis, manufacturing defect detection, and material property prediction. However, few studies have been found to use machine learning in 3D bioprinting processes. In this paper, related machine learning methods used in 3D printing are briefly reviewed and a perspective on how machine learning can also benefit 3D bioprinting is discussed. We believe that machine learning can significantly affect the future development of 3D bioprinting and hope this paper can inspire some ideas on how machine learning can be used to improve 3D bioprinting.} }
@article{WOS:000591283600001, title = {Machine learning for landslides prevention: a survey}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {33}, pages = {10881-10907}, year = {2021}, issn = {0941-0643}, doi = {10.1007/s00521-020-05529-8}, author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco}, abstract = {Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.} }
@incollection{WOS:000524457700003, title = {Machine Learning in Epidemiology and Health Outcomes Research}, booktitle = {ANNUAL REVIEW OF PUBLIC HEALTH, VOL 41}, volume = {41}, pages = {21-36}, year = {2020}, issn = {0163-7525}, isbn = {978-0-8243-2741-5}, doi = {10.1146/annurev-publhealth-040119-094437}, author = {Wiemken, Timothy L. and Kelley, Robert R.}, abstract = {Machine learning approaches to modeling of epidemiologic data are becoming increasingly more prevalent in the literature. These methods have the potential to improve our understanding of health and opportunities for intervention, far beyond our past capabilities. This article provides a walkthrough for creating supervised machine learning models with current examples from the literature. From identifying an appropriate sample and selecting features through training, testing, and assessing performance, the end-to-end approach to machine learning can be a daunting task. We take the reader through each step in the process and discuss novel concepts in the area of machine learning, including identifying treatment effects and explaining the output from machine learning models.} }
@article{WOS:000573738100002, title = {Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism}, journal = {NATURE COMMUNICATIONS}, volume = {11}, year = {2020}, doi = {10.1038/s41467-020-17910-1}, author = {Zhang, Jie and Petersen, Soren D. and Radivojevic, Tijana and Ramirez, Andres and Perez-Manriquez, Andres and Abeliuk, Eduardo and Sanchez, Benjamin J. and Costello, Zak and Chen, Yu and Fero, Michael J. and Martin, Hector Garcia and Nielsen, Jens and Keasling, Jay D. and Jensen, Michael K.}, abstract = {Through advanced mechanistic modeling and the generation of large high-quality datasets, machine learning is becoming an integral part of understanding and engineering living systems. Here we show that mechanistic and machine learning models can be combined to enable accurate genotype-to-phenotype predictions. We use a genome-scale model to pinpoint engineering targets, efficient library construction of metabolic pathway designs, and high-throughput biosensor-enabled screening for training diverse machine learning algorithms. From a single data-generation cycle, this enables successful forward engineering of complex aromatic amino acid metabolism in yeast, with the best machine learning-guided design recommendations improving tryptophan titer and productivity by up to 74 and 43\\%, respectively, compared to the best designs used for algorithm training. Thus, this study highlights the power of combining mechanistic and machine learning models to effectively direct metabolic engineering efforts. In metabolic engineering, mechanistic models require prior metabolism knowledge of the chassis strain, whereas machine learning models need ample training data. Here, the authors combine the mechanistic and machine learning models to improve prediction performance of tryptophan metabolism in baker's yeast.} }
@article{WOS:000609037300001, title = {A review on speech processing using machine learning paradigm}, journal = {INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY}, volume = {24}, pages = {367-388}, year = {2021}, issn = {1381-2416}, doi = {10.1007/s10772-021-09808-0}, author = {Bhangale, Kishor Barasu and Mohanaprasad, K.}, abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human-machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.} }
@article{WOS:000668245900001, title = {Meta-learning and the new challenges of machine learning}, journal = {INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS}, volume = {36}, pages = {6240-6272}, year = {2021}, issn = {0884-8173}, doi = {10.1002/int.22549}, author = {Monteiro, Jose Pedro and Ramos, Diogo and Carneiro, Davide and Duarte, Francisco and Fernandes, Joao M. and Novais, Paulo}, abstract = {In the last years, organizations and companies in general have found the true potential value of collecting and using data for supporting decision-making. As a consequence, data are being collected at an unprecedented rate. This poses several challenges, including, for example, regarding the storage and processing of these data. Machine Learning (ML) is also not an exception, in the sense that algorithms must now deal with novel challenges, such as learn from streaming data or deal with concept drift. ML engineers also have a harder task when it comes to selecting the most appropriate model, given the wealth of algorithms and possible configurations that exist nowadays. At the same time, training time is a stronger restriction as the computational complexity of the training model increases. In this paper we propose a framework for dealing with these challenges, based on meta-learning. Specifically, we tackle two well-defined problems: automatic algorithm selection and continuous algorithm updates that do not require the retraining of the whole algorithm to adapt to new data. Results show that the proposed framework can contribute to ameliorate the identified issues.} }
@article{WOS:000594402900006, title = {Integrating Machine Learning with Human Knowledge}, journal = {ISCIENCE}, volume = {23}, year = {2020}, doi = {10.1016/j.isci.2020.101656}, author = {Deng, Changyu and Ji, Xunbi and Rainey, Colton and Zhang, Jianyu and Lu, Wei}, abstract = {Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.} }
@article{WOS:000649908600001, title = {Machine Learning Methods with Noisy, Incomplete or Small Datasets}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11094132}, author = {Caiafa, Cesar F. and Sun, Zhe and Tanaka, Toshihisa and Marti-Puig, Pere and Sole-Casals, Jordi}, abstract = {In this article, we present a collection of fifteen novel contributions on machine learning methods with low-quality or imperfect datasets, which were accepted for publication in the special issue ``Machine Learning Methods with Noisy, Incomplete or Small Datasets'', Applied Sciences (ISSN 2076-3417). These papers provide a variety of novel approaches to real-world machine learning problems where available datasets suffer from imperfections such as missing values, noise or artefacts. Contributions in applied sciences include medical applications, epidemic management tools, methodological work, and industrial applications, among others. We believe that this special issue will bring new ideas for solving this challenging problem, and will provide clear examples of application in real-world scenarios.} }
@incollection{WOS:000677831600018, title = {Probabilistic Machine Learning for Healthcare}, booktitle = {ANNUAL REVIEW OF BIOMEDICAL DATA SCIENCE, VOL 4}, volume = {4}, pages = {393-415}, year = {2021}, issn = {2574-3414}, doi = {10.1146/annurev-biodatasci-092820-033938}, author = {Chen, Irene Y. and Joshi, Shalmali and Ghassemi, Marzyeh and Ranganath, Rajesh}, abstract = {Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial, including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.} }
@article{WOS:000709064000001, title = {A Literature Review of Using Machine Learning in Software Development Life Cycle Stages}, journal = {IEEE ACCESS}, volume = {9}, pages = {140896-140920}, year = {2021}, issn = {2169-3536}, doi = {10.1109/ACCESS.2021.3119746}, author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander}, abstract = {The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.} }
@article{WOS:000704764100004, title = {Kernel extreme learning machine based hierarchical machine learning for multi-type and concurrent fault diagnosis}, journal = {MEASUREMENT}, volume = {184}, year = {2021}, issn = {0263-2241}, doi = {10.1016/j.measurement.2021.109923}, author = {Chen, Qiuan and Wei, Haipeng and Rashid, Muhammad and Cai, Zhiqiang}, abstract = {The detection and identification of faults in rotary machines are of great significance to the mechanical equipment reliability especially the gearbox. Traditional machine learning algorithms suffer from low diagnosis accuracy of faults that have multiple types and exist concurrently. A novel machine learning method called hierarchical machine learning (HML) was proposed in this study to improve the faults diagnosis accuracy. The proposed algorithm consists of two layers. The first layer comprises a traditional machine learning model to identify the faults with distinguishable features and filter out these faults with indistinguishable features. The second layer model recognizes the faults filtered out by the first layer. In order to verify the effectiveness of the proposed method, the gearbox simulation experiment is carried out in the study. The simulation results validate that the proposed method outperforms other algorithms under an identical measure.} }
@article{WOS:000742888800012, title = {Data Acquisition for Improving Machine Learning Models}, journal = {PROCEEDINGS OF THE VLDB ENDOWMENT}, volume = {14}, pages = {1832-1844}, year = {2021}, issn = {2150-8097}, doi = {10.14778/3467861.3467872}, author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick}, abstract = {The vast advances in Machine Learning (ML) over the last ten years have been powered by the availability of suitably prepared data for training purposes. The future of ML-enabled enterprise hinges on data. As such, there is already a vibrant market offering data annotation services to tailor sophisticated ML models. In this paper, inspired by the recent vision of online data markets and associated market designs, we present research on the practical problem of obtaining data in order to improve the accuracy of ML models. We consider an environment in which consumers query for data to enhance the accuracy of their models and data providers who possess data make them available for training purposes. We first formalize this interaction process laying out the suitable framework and associated parameters for data exchange. We then propose two data acquisition strategies that consider a trade-off between exploration during which we obtain data to learn about the distribution of a provider's data and exploitation during which we optimize our data inquiries utilizing the gained knowledge. In the first strategy, Estimation and Allocation (EA), we utilize queries to estimate the utilities of various predicates while learning about the distribution of the provider's data; then we proceed to the allocation stage in which we utilize those learned utility estimates to inform our data acquisition decisions. The second algorithmic proposal, named Sequential Predicate Selection (SPS), utilizes a sampling strategy to explore the distribution of the provider's data, adaptively investing more resources to parts of the data space that are statistically more promising to improve overall model accuracy. We present a detailed experimental evaluation of our proposals utilizing a variety of ML models and associated real data sets exploring all applicable parameters of interest. Our results demonstrate the relative benefits of the proposed algorithms. Depending on the models trained and the associated learning tasks we identify trade-offs and highlight the relative benefits of each algorithm to further optimize model accuracy.} }
@article{WOS:000732946500010, title = {The role of machine learning analytics and metrics in retailing research}, journal = {JOURNAL OF RETAILING}, volume = {97}, pages = {658-675}, year = {2021}, issn = {0022-4359}, doi = {10.1016/j.jretai.2020.12.001}, author = {Wang, Xin (Shane) and Ryoo, Jun Hyun (Joseph) and Bendle, Neil and Kopalle, Praveen K.}, abstract = {This research presents the use of machine learning analytics and metrics in the retailing context. We first discuss what is machine learning and explain the field's origins. We then demonstrate the strengths of machine learning methods using an online retailing dataset, noting key areas of divergence from the traditional explanatory approach to data analysis. We then provide a review of the current state of machine learning in top-level retailing and marketing research, integrating ideas for future research and showcasing potential applications for practitioners. We propose that the explanatory and machine learning approaches need not be mutually exclusive. Particularly, we discuss four key areas in the general scientific research process that can benefit from machine learning: data exploration/theory building, variable creation, estimation, and predicting an outcome metric. Due to the customer-facing nature of retailing, we anticipate several challenges researchers and practitioners might face in the adoption and implementation of machine learning, such as ethical prediction and customer privacy issues. Overall, our belief is that machine learning can enhance customer experience and, accordingly, we advance opportunities for future research. (c) 2020 New York University. Published by Elsevier Inc. All rights reserved.} }
@article{WOS:000554897200004, title = {Machine learning in materials science}, journal = {INFOMAT}, volume = {1}, pages = {338-358}, year = {2019}, doi = {10.1002/inf2.12028}, author = {Wei, Jing and Chu, Xuan and Sun, Xiang-Yu and Xu, Kun and Deng, Hui-Xiong and Chen, Jigen and Wei, Zhongming and Lei, Ming}, abstract = {Traditional methods of discovering new materials, such as the empirical trial and error method and the density functional theory (DFT)-based method, are unable to keep pace with the development of materials science today due to their long development cycles, low efficiency, and high costs. Accordingly, due to its low computational cost and short development cycle, machine learning is coupled with powerful data processing and high prediction performance and is being widely used in material detection, material analysis, and material design. In this article, we discuss the basic operational procedures in analyzing material properties via machine learning, summarize recent applications of machine learning algorithms to several mature fields in materials science, and discuss the improvements that are required for wide-ranging application.} }
@article{WOS:000477857700021, title = {Machine-learning-guided directed evolution for protein engineering}, journal = {NATURE METHODS}, volume = {16}, pages = {687-694}, year = {2019}, issn = {1548-7091}, doi = {10.1038/s41592-019-0496-6}, author = {Yang, Kevin K. and Wu, Zachary and Arnold, Frances H.}, abstract = {Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence-function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function.} }
@article{WOS:000645896700001, title = {Advances in Machine Learning and Deep Neural Networks}, journal = {PROCEEDINGS OF THE IEEE}, volume = {109}, pages = {607-611}, year = {2021}, issn = {0018-9219}, doi = {10.1109/JPROC.2021.3072172}, author = {Chellappa, Rama and Theodoridis, Sergios and van Schaik, Andre}, abstract = {We are currently experiencing the dawn of what is known as the fourth industrial revolution. At the center of this historical happening, as one of the key enabling technologies, lies a discipline that deals with data and whose goal is to extract information and related knowledge that is hidden in it, in order to make predictions and, subsequently, take decisions. Machine learning (ML) is the name that is used as an umbrella to cover a wide range of theories, methods, algorithms, and architectures that are used to this end.} }
@article{WOS:000625545300001, title = {An Introduction to Machine Learning for Panel Data}, journal = {INTERNATIONAL ADVANCES IN ECONOMIC RESEARCH}, volume = {27}, pages = {1-16}, year = {2021}, issn = {1083-0898}, doi = {10.1007/s11294-021-09815-6}, author = {Chen, James Ming}, abstract = {Machine learning has dramatically expanded the range of tools for evaluating economic panel data. This paper applies a variety of machine-learning methods to the Boston housing dataset, an iconic proving ground for machine learning. Though machine learning often lacks the overt interpretability of linear regression, methods based on decision trees score the relative importance of dataset features. In addition to addressing the theoretical tradeoff between bias and variance, this paper discusses practices rarely followed in traditional economics: the splitting of data into training, validation, and test sets; the scaling of data; and the preference for retaining all data. The choice between traditional and machine-learning methods hinges on practical rather than mathematical considerations. In settings emphasizing interpretative clarity through the scale and sign of regression coefficients, machine learning may best play an ancillary role. Wherever predictive accuracy is paramount, however, or where heteroskedasticity or high dimensionality might impair the clarity of linear methods, machine learning can deliver superior results.} }
@article{WOS:000538049100005, title = {Combining machine learning and process engineering physics towards enhanced accuracy and explainability of data-driven models}, journal = {COMPUTERS \\& CHEMICAL ENGINEERING}, volume = {138}, year = {2020}, issn = {0098-1354}, doi = {10.1016/j.compchemeng.2020.106834}, author = {Bikmukhametov, Timur and Jaschke, Johannes}, abstract = {Machine learning models are often considered as black-box solutions which is one of the main reasons why they are still not widely used in operation of process engineering systems. One approach to overcome this problem is to combine machine learning with first principles models of a process engineering system. In this work, we investigate different methods of combining machine learning with first principles and test them on a case study of multiphase flowrate estimation in a petroleum production system. However, the methods can be applied to any process engineering system. The results show that by adding physics-based models to machine learning, it is possible not only to improve the performance of the purely black-box machine learning models, but also to make them more transparent and interpretable. We also propose a step-by-step procedure for selecting a method for combining physics and machine learning depending on the process engineering system conditions. (C) 2020 The Authors. Published by Elsevier Ltd.} }
@article{WOS:000496269400199, title = {Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey}, journal = {APPLIED SCIENCES-BASEL}, volume = {9}, year = {2019}, doi = {10.3390/app9204396}, author = {Liu, Hongyu and Lang, Bo}, abstract = {Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.} }
@article{WOS:000459730200026, title = {A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {21}, pages = {686-728}, year = {2019}, doi = {10.1109/COMST.2018.2847722}, author = {Mishra, Preeti and Varadharajan, Vijay and Tupakula, Uday and Pilli, Emmanuel S.}, abstract = {Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.} }
@article{WOS:000474586200010, title = {Machine Learning for the Geosciences: Challenges and Opportunities}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {31}, pages = {1544-1554}, year = {2019}, issn = {1041-4347}, doi = {10.1109/TKDE.2018.2861006}, author = {Karpatne, Anuj and Ebert-Uphoff, Imme and Ravela, Sai and Babaie, Hassan Ali and Kumar, Vipin}, abstract = {Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)-that has been widely successful in commercial domains-offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.} }
@article{WOS:000530830800100, title = {Machine Learning Security: Threats, Countermeasures, and Evaluations}, journal = {IEEE ACCESS}, volume = {8}, pages = {74720-74742}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2987435}, author = {Xue, Mingfu and Yuan, Chengxiang and Wu, Heyi and Zhang, Yushu and Liu, Weiqiang}, abstract = {Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.} }
@article{WOS:000601374000001, title = {Applications of machine learning in spectroscopy}, journal = {APPLIED SPECTROSCOPY REVIEWS}, volume = {56}, pages = {733-763}, year = {2021}, issn = {0570-4928}, doi = {10.1080/05704928.2020.1859525}, author = {Meza Ramirez, Carlos A. and Greenop, Michael and Ashton, Lorna and Rehman, Ihtesham Ur}, abstract = {The way to analyze data in spectroscopy has changed substantially. At the same time, data science has evolved to the point where spectroscopy can find space to be housed, adapted and be functional. The integration of the two sciences has introduced a knowledge gap between data scientists who know about advanced machine learning techniques and spectroscopists who have a solid background in chemometrics. To reach a symbiosis, the knowledge gap requires bridging. This review article focuses on introducing data science subjects to non-specialist spectroscopists, or those unfamiliar with the subject. The article will explain concepts that are covered in machine learning, such as supervised learning, unsupervised learning, deep learning, and most importantly, the difference between machine learning and artificial intelligence. This article also includes examples of published spectroscopy research, in which some of the concepts explained here are applied. Machine learning together with spectroscopy can provide a useful, fast, and efficient tool to analyze samples of interest both for industrial and research purposes.} }
@article{WOS:000543560600001, title = {Review on Machine Learning Algorithm Based Fault Detection in Induction Motors}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {1929-1940}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09446-w}, author = {Kumar, Prashant and Hati, Ananda Shankar}, abstract = {Fault detection prior to their occurrence or complete shut-down in induction motor is essential for the industries. The fault detection based on condition monitoring techniques and application of machine learning have tremendous potential. The power of machine learning can be harnessed and optimally used for fault detection. The faults especially in induction motor needs to be addressed at a proper time for avoiding losses. Machine learning algorithm applications in the domain of fault detection provides a reliable and effective solution for preventive maintenance. This paper presents a review of the machine learning algorithm applications in fault detection in induction motors. This paper also presents the future prospects and challenges for an efficient machine learning based fault detection systems.} }
@article{WOS:000568991800003, title = {Machine learning in breast MRI}, journal = {JOURNAL OF MAGNETIC RESONANCE IMAGING}, volume = {52}, pages = {998-1018}, year = {2020}, issn = {1053-1807}, doi = {10.1002/jmri.26852}, author = {Reig, Beatriu and Heacock, Laura and Geras, Krzysztof J. and Moy, Linda}, abstract = {Machine-learning techniques have led to remarkable advances in data extraction and analysis of medical imaging. Applications of machine learning to breast MRI continue to expand rapidly as increasingly accurate 3D breast and lesion segmentation allows the combination of radiologist-level interpretation (eg, BI-RADS lexicon), data from advanced multiparametric imaging techniques, and patient-level data such as genetic risk markers. Advances in breast MRI feature extraction have led to rapid dataset analysis, which offers promise in large pooled multiinstitutional data analysis. The object of this review is to provide an overview of machine-learning and deep-learning techniques for breast MRI, including supervised and unsupervised methods, anatomic breast segmentation, and lesion segmentation. Finally, it explores the role of machine learning, current limitations, and future applications to texture analysis, radiomics, and radiogenomics. Technical Efficacy Stage:2 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:998-1018.} }
@article{WOS:000544436100001, title = {Prediction of significant wave height; comparison between nested grid numerical model, and machine learning models of artificial neural networks, extreme learning and support vector machines}, journal = {ENGINEERING APPLICATIONS OF COMPUTATIONAL FLUID MECHANICS}, volume = {14}, pages = {805-817}, year = {2020}, issn = {1994-2060}, doi = {10.1080/19942060.2020.1773932}, author = {Shamshirband, Shahaboddin and Mosavi, Amir and Rabczuk, Timon and Nabipour, Narjes and Chau, Kwok-wing}, abstract = {Estimation of wave height is essential for several coastal engineering applications. This study advances a nested grid numerical model and compare its efficiency with three machine learning (ML) methods of artificial neural networks (ANN), extreme learning machines (ELM) and support vector regression (SVR) for wave height modeling. The models are trained by surface wind data. The results demonstrate that all the models generally provide sound predictions. Due to the high level of variability in the bathymetry of the study area, implementation of the nested grid with different Whitecapping coefficient is a suitable approach to improve the efficiency of the numerical models. Performance on the ML models do not differ remarkably even though the ELM model slightly outperforms the other models.} }
@article{WOS:000638379600001, title = {A Survey of Machine Learning-Based System Performance Optimization Techniques}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app11073235}, author = {Choi, Hyejeong and Park, Sejin}, abstract = {Recently, the machine learning research trend expands to the system performance optimization field, where it has still been proposed by researchers based on their intuitions and heuristics. Compared to conventional major machine learning research areas such as image or speech recognition, machine learning-based system performance optimization fields are at the beginning stage. However, recent papers show that this approach is promising and has significant potential. This paper reviews 11 machine learning-based system performance optimization approaches from nine recent papers based on well-known machine learning models such as perceptron, LSTM, and RNN. This survey provides a detailed design and summarizes model, input, output, and prediction method of each approach. This paper covers various system performance areas from the data structure to essential system components of a computer system such as index structure, branch predictor, sort, and cache management. The result shows that machine learning-based system performance optimization has an important potential for future research. We expect that this paper shows a wide range of applicability of machine learning technology and provides a new perspective for system performance optimization.} }
@article{WOS:000551254000005, title = {Applied machine learning and artificial intelligence in rheumatology}, journal = {RHEUMATOLOGY ADVANCES IN PRACTICE}, volume = {4}, year = {2020}, doi = {10.1093/rap/rkaa005}, author = {Hugle, Maria and Omoumi, Patrick and van Laar, Jacob M. and Boedecker, Joschka and Hugle, Thomas}, abstract = {Machine learning as a field of artificial intelligence is increasingly applied in medicine to assist patients and physicians. Growing datasets provide a sound basis with which to apply machine learning methods that learn from previous experiences. This review explains the basics of machine learning and its subfields of supervised learning, unsupervised learning, reinforcement learning and deep learning. We provide an overview of current machine learning applications in rheumatology, mainly supervised learning methods for e-diagnosis, disease detection and medical image analysis. In the future, machine learning will be likely to assist rheumatologists in predicting the course of the disease and identifying important disease factors. Even more interestingly, machine learning will probably be able to make treatment propositions and estimate their expected benefit (e.g. by reinforcement learning). Thus, in future, shared decision-making will not only include the patient's opinion and the rheumatologist's empirical and evidence-based experience, but it will also be influenced by machine-learned evidence.} }
@article{WOS:000600294400001, title = {Machine Learning: Quantum vs Classical}, journal = {IEEE ACCESS}, volume = {8}, pages = {219275-219294}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3041719}, author = {Khan, Tariq M. and Robles-Kelly, Antonio}, abstract = {Encouraged by growing computing power and algorithmic development, machine learning technologies have become powerful tools for a wide variety of application areas, spanning from agriculture to chemistry and natural language processing. The use of quantum systems to process classical data using machine learning algorithms has given rise to an emerging research area, i.e. quantum machine learning. Despite its origins in the processing of classical data, quantum machine learning also explores the use of quantum phenomena for learning systems, the use of quantum computers for learning on quantum data and how machine learning algorithms and software can be formulated and implemented on quantum computers. Quantum machine learning can have a transformational effect on computer science. It may speed up the processing of information well beyond the existing classical speeds. Recent work has seen the development of quantum algorithms that could serve as foundations for machine learning applications. Despite its great promise, there are still significant hardware and software challenges that need to be resolved before quantum machine learning becomes practical. In this paper, we present an overview of quantum machine learning in the light of classical approaches. Departing from foundational concepts of machine learning and quantum computing, we discuss various technical contributions, strengths and similarities of the research work in this domain. We also elaborate upon the recent progress of different quantum machine learning approaches, their complexity, and applications in various fields such as physics, chemistry and natural language processing.} }
@article{WOS:000601128500001, title = {Supervised machine learning tools: a tutorial for clinicians}, journal = {JOURNAL OF NEURAL ENGINEERING}, volume = {17}, year = {2020}, issn = {1741-2560}, doi = {10.1088/1741-2552/abbff2}, author = {Lo Vercio, Lucas and Amador, Kimberly and Bannister, Jordan J. and Crites, Sebastian and Gutierrez, Alejandro and MacDonald, M. Ethan and Moore, Jasmine and Mouches, Pauline and Rajashekar, Deepthi and Schimert, Serena and Subbanna, Nagesh and Tuladhar, Anup and Wang, Nanjia and Wilms, Matthias and Winder, Anthony and Forkert, Nils D.}, abstract = {In an increasingly data-driven world, artificial intelligence is expected to be a key tool for converting big data into tangible benefits and the healthcare domain is no exception to this. Machine learning aims to identify complex patterns in multi-dimensional data and use these uncovered patterns to classify new unseen cases or make data-driven predictions. In recent years, deep neural networks have shown to be capable of producing results that considerably exceed those of conventional machine learning methods for various classification and regression tasks. In this paper, we provide an accessible tutorial of the most important supervised machine learning concepts and methods, including deep learning, which are potentially the most relevant for the medical domain. We aim to take some of the mystery out of machine learning and depict how machine learning models can be useful for medical applications. Finally, this tutorial provides a few practical suggestions for how to properly design a machine learning model for a generic medical problem.} }
@article{WOS:000530832200187, title = {Explainability of a Machine Learning Granting Scoring Model in Peer-to-Peer Lending}, journal = {IEEE ACCESS}, volume = {8}, pages = {64873-64890}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2984412}, author = {Janny Ariza-Garzon, Miller and Arroyo, Javier and Caparrini, Antonio and Segovia-Vargas, Maria-Jesus}, abstract = {Peer-to-peer (P2P) lending demands effective and explainable credit risk models. Typical machine learning algorithms offer high prediction performance, but most of them lack explanatory power. However, this deficiency can be solved with the help of the explainability tools proposed in the last few years, such as the SHAP values. In this work, we assess the well-known logistic regression model and several machine learning algorithms for granting scoring in P2P lending. The comparison reveals that the machine learning alternative is superior in terms of not only classification performance but also explainability. More precisely, the SHAP values reveal that machine learning algorithms can reflect dispersion, nonlinearity and structural breaks in the relationships between each feature and the target variable. Our results demonstrate that is possible to have machine learning credit scoring models be both accurate and transparent. Such models provide the trust that the industry, regulators and end-users demand in P2P lending and may lead to a wider adoption of machine learning in this and other risk assessment applications where explainability is required.} }
@article{WOS:000626551700002, title = {R.ROSETTA: an interpretable machine learning framework}, journal = {BMC BIOINFORMATICS}, volume = {22}, year = {2021}, issn = {1471-2105}, doi = {10.1186/s12859-021-04049-z}, author = {Garbulowski, Mateusz and Diamanti, Klev and Smolinska, Karolina and Baltzer, Nicholas and Stoll, Patricia and Bornelov, Susanne and Ohrn, Aleksander and Feuk, Lars and Komorowski, Jan}, abstract = {BackgroundMachine learning involves strategies and algorithms that may assist bioinformatics analyses in terms of data mining and knowledge discovery. In several applications, viz. in Life Sciences, it is often more important to understand how a prediction was obtained rather than knowing what prediction was made. To this end so-called interpretable machine learning has been recently advocated. In this study, we implemented an interpretable machine learning package based on the rough set theory. An important aim of our work was provision of statistical properties of the models and their components.ResultsWe present the R.ROSETTA package, which is an R wrapper of ROSETTA framework. The original ROSETTA functions have been improved and adapted to the R programming environment. The package allows for building and analyzing non-linear interpretable machine learning models. R.ROSETTA gathers combinatorial statistics via rule-based modelling for accessible and transparent results, well-suited for adoption within the greater scientific community. The package also provides statistics and visualization tools that facilitate minimization of analysis bias and noise. The R.ROSETTA package is freely available at https://github.com/komorowskilab/R.ROSETTA. To illustrate the usage of the package, we applied it to a transcriptome dataset from an autism case-control study. Our tool provided hypotheses for potential co-predictive mechanisms among features that discerned phenotype classes. These co-predictors represented neurodevelopmental and autism-related genes.ConclusionsR.ROSETTA provides new insights for interpretable machine learning analyses and knowledge-based systems. We demonstrated that our package facilitated detection of dependencies for autism-related genes. Although the sample application of R.ROSETTA illustrates transcriptome data analysis, the package can be used to analyze any data organized in decision tables.} }
@article{WOS:000546625200003, title = {Machine Learning and Natural Language Processing in Psychotherapy Research: Alliance as Example Use Case}, journal = {JOURNAL OF COUNSELING PSYCHOLOGY}, volume = {67}, pages = {438-448}, year = {2020}, issn = {0022-0167}, doi = {10.1037/cou0000382}, author = {Goldberg, Simon B. and Flemotomos, Nikolaos and Martinez, Victor R. and Tanana, Michael J. and Kuo, Patty B. and Pace, Brian T. and Villatte, Jennifer L. and Georgiou, Panayiotis G. and Van Epps, Jake and Imel, Zac E. and Narayanan, Shrikanth S. and Atkins, David C.}, abstract = {Artificial intelligence generally and machine learning specifically have become deeply woven into the lives and technologies of modern life. Machine learning is dramatically changing scientific research and industry and may also hold promise for addressing limitations encountered in mental health care and psychotherapy. The current paper introduces machine learning and natural language processing as related methodologies that may prove valuable for automating the assessment of meaningful aspects of treatment. Prediction of therapeutic alliance from session recordings is used as a case in point. Recordings from 1,235 sessions of 386 clients seen by 40 therapists at a university counseling center were processed using automatic speech recognition software. Machine learning algorithms learned associations between client ratings of therapeutic alliance exclusively from session linguistic content. Using a portion of the data to train the model, machine learning algorithms modestly predicted alliance ratings from session content in an independent test set (Spearman's rho =15, p <.001). These results highlight the potential to harness natural language processing and machine learning to predict a key psychotherapy process variable that is relatively distal from linguistic content. Six practical suggestions for conducting psychotherapy research using machine learning are presented along with several directions for future research. Questions of dissemination and implementation may be particularly important to explore as machine learning improves in its ability to automate assessment of psychotherapy process and outcome.} }
@article{WOS:000545164100001, title = {Machine learning and artificial intelligence in haematology}, journal = {BRITISH JOURNAL OF HAEMATOLOGY}, volume = {192}, pages = {239-250}, year = {2021}, issn = {0007-1048}, doi = {10.1111/bjh.16915}, author = {Shouval, Roni and Fein, Joshua A. and Savani, Bipin and Mohty, Mohamad and Nagler, Arnon}, abstract = {Digitalization of the medical record and integration of genomic methods into clinical practice have resulted in an unprecedented wealth of data. Machine learning is a subdomain of artificial intelligence that attempts to computationally extract meaningful insights from complex data structures. Applications of machine learning in haematological scenarios are steadily increasing. However, basic concepts are often unfamiliar to clinicians and investigators. The purpose of this review is to provide readers with tools to interpret and critically appraise machine learning literature. We begin with the elucidation of standard terminology and then review examples in haematology. Guidelines for designing and evaluating machine-learning studies are provided. Finally, we discuss limitations of the machine-learning approach.} }
@article{WOS:000595502700002, title = {A primer for understanding radiology articles about machine learning and deep learning}, journal = {DIAGNOSTIC AND INTERVENTIONAL IMAGING}, volume = {101}, pages = {765-770}, year = {2020}, issn = {2211-5684}, doi = {10.1016/j.diii.2020.10.001}, author = {Nakaura, Takeshi and Higaki, Toru and Awai, Kazuo and Ikeda, Osamu and Yamashita, Yasuyuki}, abstract = {The application of machine learning and deep learning in the field of imaging is rapidly growing. Although the principles of machine and deep learning are unfamiliar to the majority of clinicians, the basics are not so complicated. One of the major issues is that commentaries written by experts are difficult to understand, and are not primarily written for clinicians. The purpose of this article was to describe the different concepts behind machine learning, radiomics, and deep learning to make clinicians more familiar with these techniques. (C) 2020 Societe francaise de radiologie. Published by Elsevier Masson SAS. All rights reserved.} }
@article{WOS:000544758900009, title = {Machine learning in haematological malignancies}, journal = {LANCET HAEMATOLOGY}, volume = {7}, pages = {E541-E550}, year = {2020}, issn = {2352-3026}, author = {Radakovich, Nathan and Nagy, Matthew and Nazha, Aziz}, abstract = {Machine learning is a branch of computer science and statistics that generates predictive or descriptive models by learning from training data rather than by being rigidly programmed. It has attracted substantial attention for its many applications in medicine, both as a catalyst for research and as a means of improving clinical care across the cycle of diagnosis, prognosis, and treatment of disease. These applications include the management of haematological malignancy, in which machine learning has created inroads in pathology, radiology, genomics, and the analysis of electronic health record data. As computational power becomes cheaper and the tools for implementing machine learning become increasingly democratised, it is likely to become increasingly integrated into the research and practice landscape of haematology. As such, machine learning merits understanding and attention from researchers and clinicians alike. This narrative Review describes important concepts in machine learning for unfamiliar readers, details machine learning?s current applications in haematological malignancy, and summarises important concepts for clinicians to be aware of when appraising research that uses machine learning.} }
@article{WOS:000518473500027, title = {Expert-augmented machine learning}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {117}, pages = {4571-4577}, year = {2020}, issn = {0027-8424}, doi = {10.1073/pnas.1906831117}, author = {Gennatas, Efstathios D. and Friedman, Jerome H. and Ungar, Lyle H. and Pirracchio, Romain and Eaton, Eric and Reichmann, Lara G. and Interian, Yannet and Luna, Jose Marcio and Simone, II, Charles B. and Auerbach, Andrew and Delgado, Elier and van der Laan, Mark J. and Solberg, Timothy D. and Valdes, Gilmer}, abstract = {Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.} }
@article{WOS:000568280500001, title = {The Role of Machine Learning in Spine Surgery: The Future Is Now}, journal = {FRONTIERS IN SURGERY}, volume = {7}, year = {2020}, issn = {2296-875X}, doi = {10.3389/fsurg.2020.00054}, author = {Chang, Michael and Canseco, Jose A. and Nicholson, Kristen J. and Patel, Neil and Vaccaro, Alexander R.}, abstract = {The recent influx of machine learning centered investigations in the spine surgery literature has led to increased enthusiasm as to the prospect of using artificial intelligence to create clinical decision support tools, optimize postoperative outcomes, and improve technologies used in the operating room. However, the methodology underlying machine learning in spine research is often overlooked as the subject matter is quite novel and may be foreign to practicing spine surgeons. Improper application of machine learning is a significant bioethics challenge, given the potential consequences of over- or underestimating the results of such studies for clinical decision-making processes. Proper peer review of these publications requires a baseline familiarity of the language associated with machine learning, and how it differs from classical statistical analyses. This narrative review first introduces the overall field of machine learning and its role in artificial intelligence, and defines basic terminology. In addition, common modalities for applying machine learning, including classification and regression decision trees, support vector machines, and artificial neural networks are examined in the context of examples gathered from the spine literature. Lastly, the ethical challenges associated with adapting machine learning for research related to patient care, as well as future perspectives on the potential use of machine learning in spine surgery, are discussed specifically.} }
@article{WOS:000888566100001, title = {Moving towards reproducible machine learning}, journal = {NATURE COMPUTATIONAL SCIENCE}, volume = {1}, pages = {629-630}, year = {2021}, doi = {10.1038/s43588-021-00152-6}, author = {[Anonymous]}, abstract = {We provide some recommendations on how to report machine learning-based research in order to improve transparency and reproducibility.} }
@article{WOS:000762389600001, title = {Machine learning in tutorials - Universal applicability, underinformed application, and other misconceptions}, journal = {BIG DATA \\& SOCIETY}, volume = {8}, year = {2021}, issn = {2053-9517}, doi = {10.1177/20539517211017593}, author = {Heuer, Hendrik and Jarke, Juliane and Breiter, Andreas}, abstract = {Machine learning has become a key component of contemporary information systems. Unlike prior information systems explicitly programmed in formal languages, ML systems infer rules from data. This paper shows what this difference means for the critical analysis of socio-technical systems based on machine learning. To provide a foundation for future critical analysis of machine learning-based systems, we engage with how the term is framed and constructed in self-education resources. For this, we analyze machine learning tutorials, an important information source for self-learners and a key tool for the formation of the practices of the machine learning community. Our analysis identifies canonical examples of machine learning as well as important misconceptions and problematic framings. Our results show that machine learning is presented as being universally applicable and that the application of machine learning without special expertise is actively encouraged. Explanations of machine learning algorithms are missing or strongly limited. Meanwhile, the importance of data is vastly understated. This has implications for the manifestation of (new) social inequalities through machine learning-based systems.} }
@article{WOS:000726375800010, title = {Application of Machine Learning in Animal Disease Analysis and Prediction}, journal = {CURRENT BIOINFORMATICS}, volume = {16}, pages = {972-982}, year = {2021}, issn = {1574-8936}, doi = {10.2174/1574893615999200728195613}, author = {Zhang, Shuwen and Su, Qiang and Chen, Qin}, abstract = {Major animal diseases pose a great threat to animal husbandry and human beings. With the deepening of globalization and the abundance of data resources, the prediction and analysis of animal diseases by using big data are becoming more and more important. The focus of machine learning is to make computers how to learn from data and use the learned experience to analyze and predict. Firstly, this paper introduces the animal epidemic situation and machine learning. Then it briefly introduces the application of machine learning in animal disease analysis and prediction. Machine learning is mainly divided into supervised learning and unsupervised learning. Supervised learning includes support vector machines, naive bayes, decision trees, random forests, logistic regression, artificial neural networks, deep learning, and AdaBoost. Unsupervised learning has maximum expectation algorithm, principal component analysis hierarchical clustering algorithm and maxent. Through the discussion of this paper, people have a clearer concept of machine learning and an understanding of its application prospect in animal diseases.} }
@article{WOS:000503916400004, title = {Machine Learning Principles for Radiology Investigators}, journal = {ACADEMIC RADIOLOGY}, volume = {27}, pages = {13-25}, year = {2020}, issn = {1076-6332}, doi = {10.1016/j.acra.2019.07.030}, author = {Borstelmann, Stephen M.}, abstract = {Artificial intelligence and deep learning are areas of high interest for radiology investigators at present. However, the field of machine learning encompasses multiple statistics-based techniques useful for investigators, which may be complementary to deep learning approaches. After a refresher in basic statistical concepts, relevant considerations for machine learning practitioners are reviewed: regression, classification, decision boundaries, and bias-variance tradeoff. Regularization, ground truth, and populations are discussed along with compute and data management principles. Advanced statistical machine learning techniques including bootstrapping, bagging, boosting, decision trees, random forest, XGboost, and support vector machines are reviewed along with relevant examples from the radiology literature.} }
@article{WOS:000590917300001, title = {Machine learning for human learners: opportunities, issues, tensions and threats}, journal = {ETR\\&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT}, volume = {69}, pages = {2109-2130}, year = {2021}, issn = {1042-1629}, doi = {10.1007/s11423-020-09858-2}, author = {Webb, Mary E. and Fluck, Andrew and Magenheim, Johannes and Malyn-Smith, Joyce and Waters, Juliet and Deschenes, Michelle and Zagami, Jason}, abstract = {Machine learning systems are infiltrating our lives and are beginning to become important in our education systems. This article, developed from a synthesis and analysis of previous research, examines the implications of recent developments in machine learning for human learners and learning. In this article we first compare deep learning in computers and humans to examine their similarities and differences. Deep learning is identified as a sub-set of machine learning, which is itself a component of artificial intelligence. Deep learning often depends on backwards propagation in weighted neural networks, so is non-deterministic-the system adapts and changes through practical experience or training. This adaptive behaviour predicates the need for explainability and accountability in such systems. Accountability is the reverse of explainability. Explainability flows through the system from inputs to output (decision) whereas accountability flows backwards, from a decision to the person taking responsibility for it. Both explainability and accountability should be incorporated in machine learning system design from the outset to meet social, ethical and legislative requirements. For students to be able to understand the nature of the systems that may be supporting their own learning as well as to act as responsible citizens in contemplating the ethical issues that machine learning raises, they need to understand key aspects of machine learning systems and have opportunities to adapt and create such systems. Therefore, some changes are needed to school curricula. The article concludes with recommendations about machine learning for teachers, students, policymakers, developers and researchers.} }
@article{WOS:000519572500022, title = {The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research}, journal = {COMPUTATIONAL MATERIALS SCIENCE}, volume = {176}, year = {2020}, issn = {0927-0256}, doi = {10.1016/j.commatsci.2020.109544}, author = {Jacobs, Ryan and Mayeshiba, Tam and Afflerbach, Ben and Miles, Luke and Williams, Max and Turner, Matthew and Finkel, Raphael and Morgan, Dane}, abstract = {As data science and machine learning methods are taking on an increasingly important role in the materials research community, there is a need for the development of machine learning software tools that are easy to use (even for nonexperts with no programming ability), provide flexible access to the most important algorithms, and codify best practices of machine learning model development and evaluation. Here, we introduce the Materials Simulation Toolkit for Machine Learning (MAST-ML), an open source Python-based software package designed to broaden and accelerate the use of machine learning in materials science research. MAST-ML provides predefined routines for many input setup, model fitting, and post-analysis tasks, as well as a simple structure for executing a multi-step machine learning model workflow. In this paper, we describe how MAST-ML is used to streamline and accelerate the execution of machine learning problems. We walk through how to acquire and run MAST-ML, demonstrate how to execute different components of a supervised machine learning workflow via a customized input file, and showcase a number of features and analyses conducted automatically during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by showcasing examples of recent materials informatics studies which used MAST-ML to formulate and evaluate various machine learning models for an array of materials applications. Finally, we lay out a vision of how MAST-ML, together with complementary software packages and emerging cyberinfrastructure, can advance the rapidly growing field of materials informatics, with a focus on producing machine learning models easily, reproducibly, and in a manner that facilitates model evolution and improvement in the future.} }
@article{WOS:000592624200002, title = {Machine Learning and Computational Mathematics}, journal = {COMMUNICATIONS IN COMPUTATIONAL PHYSICS}, volume = {28}, pages = {1639-1670}, year = {2020}, issn = {1815-2406}, doi = {10.4208/cicp.OA-2020-0185}, author = {Weinan, E.}, abstract = {Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ``black box'' type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, can impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.} }
@article{WOS:000572456900001, title = {Learning from Machine Learning in Accounting and Assurance}, journal = {JOURNAL OF EMERGING TECHNOLOGIES IN ACCOUNTING}, volume = {17}, pages = {1-10}, year = {2020}, issn = {1554-1908}, doi = {10.2308/jeta-10718}, author = {Cho, Soohyun and Vasarhelyi, Miklos A. and Sun, Ting (Sophia) and Zhang, Chanyuan (Abigail)}, abstract = {Machine learning is a subset of artificial intelligence, and it is a computational method that learns patterns from large and complex data. The learning processes enable us to make predictions for future events. In the accounting and assurance profession, machine learning is gradually being applied to various tasks like reviewing source documents, analyzing business transactions or activities, and assessing risks. In academic research, machine learning has been used to make predictions of fraud, bankruptcy, material misstatements, and accounting estimates. More importantly, machine learning is generating awareness about the inductive reasoning methodology, which has long been undervalued in the mainstream of academic research in accounting and auditing. The use of machine learning in accounting/auditing research and practice is also raising concerns about its potential bias and ethical implications. Therefore, this editorial aims to call the readers' attention to these issues and encourage scholars to perform research in this domain.} }
@article{WOS:000522553100014, title = {Research on radar signal recognition based on automatic machine learning}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {32}, pages = {1959-1969}, year = {2020}, issn = {0941-0643}, doi = {10.1007/s00521-019-04494-1}, author = {Li, Peng}, abstract = {With the advancement of machine learning and radar technology, machine learning is becoming more and more widely used in the field of radar. Radar scanning, signal acquisition and processing, one-dimensional range image, radar SAR, ISAR image recognition, radar tracking and guidance are all integrated into machine learning technology, but machine learning technology relies heavily on human machine learning experts for radar signal recognition. In order to realize the automation of radar signal recognition by machine learning, this paper proposes an automatic machine learning AUTO-SKLEARN system and applies it to radar radiation source signals. Identification: Firstly, this paper briefly introduces the classification of traditional machine learning algorithms and the types of algorithms specifically included in each type of algorithm. On this basis, the machine learning Bayesian algorithm is introduced. Secondly, the automatic machine learning AUTO based on Bayesian algorithm is proposed. -SKLEARN system, elaborates the process of AUTO-SKLEARN system in solving automatic selection algorithm and hyperparameter optimization, including meta-learning and its program implementation and automatic model integration construction. Finally, this paper introduces the process of automatic machine learning applied to radar emitter signal recognition. Through data simulation and experiment, the effect of traditional machine learning k-means algorithm and automatic machine learning AUTO-SKLEARN system in radar signal recognition is compared, which shows that automatic machine learning is feasible for radar signal recognition. The automatic machine learning AUTO-SKLEARN system can significantly improve the accuracy of the radar emitter signal recognition process, and the scheme is more reliable in signal recognition stability.} }
@article{WOS:000579583300001, title = {Robustness Evaluations of Sustainable Machine Learning Models against Data Poisoning Attacks in the Internet of Things}, journal = {SUSTAINABILITY}, volume = {12}, year = {2020}, doi = {10.3390/su12166434}, author = {Dunn, Corey and Moustafa, Nour and Turnbull, Benjamin}, abstract = {With the increasing popularity of the Internet of Things (IoT) platforms, the cyber security of these platforms is a highly active area of research. One key technology underpinning smart IoT systems is machine learning, which classifies and predicts events from large-scale data in IoT networks. Machine learning is susceptible to cyber attacks, particularly data poisoning attacks that inject false data when training machine learning models. Data poisoning attacks degrade the performances of machine learning models. It is an ongoing research challenge to develop trustworthy machine learning models resilient and sustainable against data poisoning attacks in IoT networks. We studied the effects of data poisoning attacks on machine learning models, including the gradient boosting machine, random forest, naive Bayes, and feed-forward deep learning, to determine the levels to which the models should be trusted and said to be reliable in real-world IoT settings. In the training phase, a label modification function is developed to manipulate legitimate input classes. The function is employed at data poisoning rates of 5\\%, 10\\%, 20\\%, and 30\\% that allow the comparison of the poisoned models and display their performance degradations. The machine learning models have been evaluated using the ToN\_IoT and UNSW NB-15 datasets, as they include a wide variety of recent legitimate and attack vectors. The experimental results revealed that the models' performances will be degraded, in terms of accuracy and detection rates, if the number of the trained normal observations is not significantly larger than the poisoned data. At the rate of data poisoning of 30\\% or greater on input data, machine learning performances are significantly degraded.} }
@article{WOS:000722548300001, title = {Machine Learning and Small Data}, journal = {EDUCATIONAL MEASUREMENT-ISSUES AND PRACTICE}, volume = {40}, pages = {8-12}, year = {2021}, issn = {0731-1745}, doi = {10.1111/emip.12472}, author = {Cui, Zhongmin}, abstract = {Commonly used machine learning applications seem to relate to big data. This article provides a gentle review of machine learning and shows why machine learning can be applied to small data too. An example of applying machine learning to screen irregularity reports is presented. In the example, the support vector machine and multinomial naive Bayes methods were used and compared. The performance of machine learning was compared to human experts in terms of flagging records to be excluded from equating. The application of machine learning seemed to be successful, although the data only consisted of a couple of thousand records. Recommendations in using machine learning are provided.} }
@article{WOS:000518547500020, title = {What is Machine Learning? A Primer for the Epidemiologist}, journal = {AMERICAN JOURNAL OF EPIDEMIOLOGY}, volume = {188}, pages = {2222-2239}, year = {2019}, issn = {0002-9262}, doi = {10.1093/aje/kwz189}, author = {Bi, Qifang and Goodman, Katherine E. and Kaminsky, Joshua and Lessler, Justin}, abstract = {Machine learning is a branch of computer science that has the potential to transform epidemiologic sciences. Amid a growing focus on ``Big Data,'' it offers epidemiologists new tools to tackle problems for which classical methods are not well-suited. In order to critically evaluate the value of integrating machine learning algorithms and existing methods, however, it is essential to address language and technical barriers between the two fields that can make it difficult for epidemiologists to read and assess machine learning studies. Here, we provide an overview of the concepts and terminology used in machine learning literature, which encompasses a diverse set of tools with goals ranging from prediction to classification to clustering. We provide a brief introduction to 5 common machine learning algorithms and 4 ensemble-based approaches. We then summarize epidemiologic applications of machine learning techniques in the published literature. We recommend approaches to incorporate machine learning in epidemiologic research and discuss opportunities and challenges for integrating machine learning and existing epidemiologic research methods.} }
@article{WOS:000660871100001, title = {Toward a theory of machine learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abe6d7}, author = {Vanchurin, Vitaly}, abstract = {We define a neural network as a septuple consisting of (1) a state vector, (2) an input projection, (3) an output projection, (4) a weight matrix, (5) a bias vector, (6) an activation map and (7) a loss function. We argue that the loss function can be imposed either on the boundary (i.e. input and/or output neurons) or in the bulk (i.e. hidden neurons) for both supervised and unsupervised systems. We apply the principle of maximum entropy to derive a canonical ensemble of the state vectors subject to a constraint imposed on the bulk loss function by a Lagrange multiplier (or an inverse temperature parameter). We show that in an equilibrium the canonical partition function must be a product of two factors: a function of the temperature, and a function of the bias vector and weight matrix. Consequently, the total Shannon entropy consists of two terms which represent, respectively, a thermodynamic entropy and a complexity of the neural network. We derive the first and second laws of learning: during learning the total entropy must decrease until the system reaches an equilibrium (i.e. the second law), and the increment in the loss function must be proportional to the increment in the thermodynamic entropy plus the increment in the complexity (i.e. the first law). We calculate the entropy destruction to show that the efficiency of learning is given by the Laplacian of the total free energy, which is to be maximized in an optimal neural architecture, and explain why the optimization condition is better satisfied in a deep network with a large number of hidden layers. The key properties of the model are verified numerically by training a supervised feedforward neural network using the stochastic gradient descent method. We also discuss a possibility that the entire Universe at its most fundamental level is a neural network.} }
@article{WOS:000563451300001, title = {A systematic review of fuzzing based on machine learning techniques}, journal = {PLOS ONE}, volume = {15}, year = {2020}, issn = {1932-6203}, doi = {10.1371/journal.pone.0237749}, author = {Wang, Yan and Jia, Peng and Liu, Luping and Huang, Cheng and Liu, Zhonglin}, abstract = {Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzz testing faces many challenges, such as how to mutate input seed files, how to increase code coverage, and how to bypass the format verification effectively. Therefore machine learning techniques have been introduced as a new method into fuzz testing to alleviate these challenges. This paper reviews the research progress of using machine learning techniques for fuzz testing in recent years, analyzes how machine learning improves the fuzzing process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies five different stages in which machine learning has been used. Then this paper systematically studies machine learning-based fuzzing models from five dimensions of selection of machine learning algorithms, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Secondly, this paper assesses the performance of the machine learning techniques in existing research for fuzz testing. The results of the evaluation prove that machine learning techniques have an acceptable capability of prediction for fuzzing. Finally, the capability of discovering vulnerabilities both traditional fuzzers and machine learning-based fuzzers is analyzed. The results depict that the introduction of machine learning techniques can improve the performance of fuzzing. We hope to provide researchers with a systematic and more in-depth understanding of fuzzing based on machine learning techniques and provide some references for this field through analysis and summarization of multiple dimensions.} }
@article{WOS:000600895900004, title = {Machine learning and its applications in plant molecular studies}, journal = {BRIEFINGS IN FUNCTIONAL GENOMICS}, volume = {19}, pages = {40-48}, year = {2020}, issn = {2041-2649}, doi = {10.1093/bfgp/elz036}, author = {Sun, Shanwen and Wang, Chunyu and Ding, Hui and Zou, Quan}, abstract = {The advent of high-throughput genomic technologies has resulted in the accumulation of massive amounts of genomic information. However, biologists are challenged with how to effectively analyze these data. Machine learning can provide tools for better and more efficient data analysis. Unfortunately, because many plant biologists are unfamiliar with machine learning, its application in plant molecular studies has been restricted to a few species and a limited set of algorithms. Thus, in this study, we provide the basic steps for developing machine learning frameworks and present a comprehensive overview of machine learning algorithms and various evaluation metrics. Furthermore, we introduce sources of important curated plant genomic data and R packages to enable plant biologists to easily and quickly apply appropriate machine learning algorithms in their research. Finally, we discuss current applications of machine learning algorithms for identifying various genes related to resistance to biotic and abiotic stress. Broad application of machine learning and the accumulation of plant sequencing data will advance plant molecular studies.} }
@article{WOS:000577397300001, title = {The use of machine learning and deep learning algorithms in functional magnetic resonance imaging-A systematic review}, journal = {EXPERT SYSTEMS}, volume = {37}, year = {2020}, issn = {0266-4720}, doi = {10.1111/exsy.12644}, author = {Rashid, Mamoon and Singh, Harjeet and Goyal, Vishal}, abstract = {Functional Magnetic Resonance Imaging (fMRI) is presently one of the most popular techniques for analysing the dynamic states in brain images using various kinds of algorithms. From the last decade, there is an exponential rise in the use of the machine and deep learning algorithms of artificial intelligence for analysing fMRI data. However, it is a big challenge for every researcher to choose a suitable machine or deep learning algorithm for analysing fMRI data due to the availability of a large number of algorithms in the literature. It takes much time for each researcher to know about the various approaches and algorithms which are in use for fMRI data. This paper provides a review in a systematic manner for the present literature of fMRI data that makes use of the machine and deep learning algorithms. The major goals of this review paper are to (a) identify machine learning and deep learning research trends for the implementation of fMRI; (b) identify usage of Machine Learning Algorithms and deep learning in fMRI, and (c) help new researchers based on fMRI to put their new findings appropriately in existing domain of fMRI research. The results of this systematic review identified various fMRI studies and classified them based on fMRI types, mental diseases, use of machine learning and deep learning algorithms. The authors have provided the studies with the best performance of machine learning and deep learning algorithms used in fMRI. The authors believe that this systematic review will help incoming researchers on fMRI in their future works.} }
@article{WOS:000437004000009, title = {Next-Generation Machine Learning for Biological Networks}, journal = {CELL}, volume = {173}, pages = {1581-1592}, year = {2018}, issn = {0092-8674}, doi = {10.1016/j.cell.2018.05.015}, author = {Camacho, Diogo M. and Collins, Katherine M. and Powers, Rani K. and Costello, James C. and Collins, James J.}, abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology.} }
@article{WOS:000641958000001, title = {Applications for Machine Learning}, journal = {IEEE SYSTEMS MAN AND CYBERNETICS MAGAZINE}, volume = {7}, pages = {3}, year = {2021}, issn = {2380-1298}, doi = {10.1109/MSMC.2021.3058718}, author = {Nahavandi, Saeid}, abstract = {In this issue of IEEE Systems, Man, and Cybernetics Magazine, four articles are presented that relate to the fascinating topic of machine learning and its application for real-world systems.} }
@article{WOS:000691881100022, title = {Training analysis of optimization models in machine learning}, journal = {INTERNATIONAL JOURNAL OF NONLINEAR ANALYSIS AND APPLICATIONS}, volume = {12}, pages = {1453-1461}, year = {2021}, issn = {2008-6822}, doi = {10.22075/ijnaa.2021.5261}, author = {Alridha, Ahmed and Wahbi, Fadhil Abdalhasan and Kadhim, Mazin Kareem}, abstract = {Machine learning is fast evolving, with numerous theoretical advances and applications in a variety of domains. In reality, most machine learning algorithms are based on optimization issues. This interaction is also explored in the special topic on machine learning and large-scale optimization. Furthermore, machine learning optimization issues have several unique characteristics that are rarely seen in other optimization contexts. Aside from that, the notions of classical optimization vs machine learning will be discussed. Finally, this study will give an outline of these peculiar aspects of machine learning optimization.} }
@article{WOS:000453280300005, title = {Machine Learning Methods for Histopathological Image Analysis}, journal = {COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL}, volume = {16}, pages = {34-42}, year = {2018}, issn = {2001-0370}, doi = {10.1016/j.csbj.2018.01.001}, author = {Komura, Daisuke and Ishikawa, Shumpei}, abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to suchanalysis, and propose possible solutions. (C) 2018 Komura, Ishikawa. Published by Elsevier B.V. on behalf of the Research Network of Computational and Structural Biotechnology.} }
@article{WOS:000559782300004, title = {Julia language in machine learning: Algorithms, applications, and open issues}, journal = {COMPUTER SCIENCE REVIEW}, volume = {37}, year = {2020}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2020.100254}, author = {Gao, Kaifeng and Mei, Gang and Piccialli, Francesco and Cuomo, Salvatore and Tu, Jingzhi and Huo, Zenan}, abstract = {Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the applications of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning. (c) 2020 The Authors. Published by Elsevier Inc.} }
@article{WOS:000489358200007, title = {Estimation of energy consumption in machine learning}, journal = {JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING}, volume = {134}, pages = {75-88}, year = {2019}, issn = {0743-7315}, doi = {10.1016/j.jpdc.2019.07.007}, author = {Garcia-Martin, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Hakan}, abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning. (C) 2019 The Authors. Published by Elsevier Inc.} }
@article{WOS:000595025900001, title = {Quantum Driven Machine Learning}, journal = {INTERNATIONAL JOURNAL OF THEORETICAL PHYSICS}, volume = {59}, pages = {4013-4024}, year = {2020}, issn = {0020-7748}, doi = {10.1007/s10773-020-04656-1}, author = {Saini, Shivani and Khosla, P. K. and Kaur, Manjit and Singh, Gurmohan}, abstract = {Quantum computing is proving to be very beneficial for solving complex machine learning problems. Quantum computers are inherently excellent in handling and manipulating vectors and matrix operations. The ever increasing size of data has started creating bottlenecks for classical machine learning systems. Quantum computers are emerging as potential solutions to tackle big data related problems. This paper presents a quantum machine learning model based on quantum support vector machine (QSVM) algorithm to solve a classification problem. The quantum machine learning model is practically implemented on quantum simulators and real-time superconducting quantum processors. The performance of quantum machine learning model is computed in terms of processing speed and accuracy and compared against its classical counterpart. The breast cancer dataset is used for the classification problem. The results are indicative that quantum computers offer quantum speed-up.} }
@article{WOS:000848787000001, title = {Learning dynamics from large biological data sets: Machine learning meets systems biology}, journal = {CURRENT OPINION IN SYSTEMS BIOLOGY}, volume = {22}, pages = {1-7}, year = {2020}, issn = {2452-3100}, doi = {10.1016/j.coisb.2020.07.009}, author = {Gilpin, William and Huang, Yitong and Forger, Daniel B.}, abstract = {In the past few decades, mathematical models based on dynamical systems theory have provided new insight into diverse biological systems. In this review, we ask whether the recent success of machine learning techniques for large-scale biological data analysis provides a complementary or competing approach to more traditional modeling approaches. Recent applications of machine learning to the problem of learning biological dynamics in diverse systems range from neuroscience to animal behavior. We compare the underlying mechanisms and limitations of traditional dynamical models with those of machine learning models. We highlight the unique role that traditional modeling has played in providing predictive insights into biological systems, and we propose several avenues for bridging traditional dynamical systems theory with large-scale analysis enabled by machine learning.} }
@article{WOS:000468930900005, title = {Machine Learning Made Easy: A Review of Scikit-learn Package in Python Programming Language}, journal = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS}, volume = {44}, pages = {348-361}, year = {2019}, issn = {1076-9986}, doi = {10.3102/1076998619832248}, author = {Hao, Jiangang and Ho, Tin Kam}, abstract = {Machine learning is a popular topic in data analysis and modeling. Many different machine learning algorithms have been developed and implemented in a variety of programming languages over the past 20 years. In this article, we first provide an overview of machine learning and clarify its difference from statistical inference. Then, we review Scikit-learn, a machine learning package in the Python programming language that is widely used in data science. The Scikit-learn package includes implementations of a comprehensive list of machine learning methods under unified data and modeling procedure conventions, making it a convenient toolkit for educational and behavior statisticians.} }
@article{WOS:000543431000001, title = {Benchmark AFLOW Data Sets for Machine Learning}, journal = {INTEGRATING MATERIALS AND MANUFACTURING INNOVATION}, volume = {9}, pages = {153-156}, year = {2020}, issn = {2193-9764}, doi = {10.1007/s40192-020-00174-4}, author = {Clement, Conrad L. and Kauwe, Steven K. and Sparks, Taylor D.}, abstract = {Materials informatics is increasingly finding ways to exploit machine learning algorithms. Techniques such as decision trees, ensemble methods, support vector machines, and a variety of neural network architectures are used to predict likely material characteristics and property values. Supplemented with laboratory synthesis, applications of machine learning to compound discovery and characterization represent one of the most promising research directions in materials informatics. A shortcoming of this trend, in its current form, is a lack of standardized materials data sets on which to train, validate, and test model effectiveness. Applied machine learning research depends on benchmark data to make sense of its results. Fixed, predetermined data sets allow for rigorous model assessment and comparison. Machine learning publications that do not refer to benchmarks are often hard to contextualize and reproduce. In this data descriptor article, we present a collection of data sets of different material properties taken from the AFLOW database. We describe them, the procedures that generated them, and their use as potential benchmarks. We provide a compressed ZIP file containing the data sets and a GitHub repository of associated Python code. Finally, we discuss opportunities for future work incorporating the data sets and creating similar benchmark collections.} }
@article{WOS:000591678300009, title = {Macroeconomic forecasting using factor models and machine learning: an application to Japan}, journal = {JOURNAL OF THE JAPANESE AND INTERNATIONAL ECONOMIES}, volume = {58}, year = {2020}, issn = {0889-1583}, doi = {10.1016/j.jjie.2020.101104}, author = {Maehashi, Kohei and Shintani, Mototsugu}, abstract = {We perform a thorough comparative analysis of factor models and machine learning to forecast Japanese macroeconomic time series. Our main results can be summarized as follows. First, in many instances, factor models and machine learning perform better than the conventional AR model. Second, predictions made by machine learning methods perform particularly well for medium to long forecast horizons. Third, the success of machine learning mainly comes from the nonlinearity and interaction of variables, which suggests the importance of nonlinear structure in predicting the Japanese macroeconomic series. Fourth, the composite forecast of factor models and machine learning performs better than factor models or machine learning alone; and machine learning methods applied to common factors are found to be useful in the composite forecast.} }
@article{WOS:000895964000001, title = {Survey on Lie Group Machine Learning}, journal = {BIG DATA MINING AND ANALYTICS}, volume = {3}, pages = {235-258}, year = {2020}, doi = {10.26599/BDMA.2020.9020011}, author = {Lu, Mei and Li, Fanzhang}, abstract = {Lie group machine learning is recognized as the theoretical basis of brain intelligence, brain learning, higher machine learning, and higher artificial intelligence. Sample sets of Lie group matrices are widely available in practical applications. Lie group learning is a vibrant field of increasing importance and extraordinary potential and thus needs to be developed further. This study aims to provide a comprehensive survey on recent advances in Lie group machine learning. We introduce Lie group machine learning techniques in three major categories: supervised Lie group machine learning, semisupervised Lie group machine learning, and unsupervised Lie group machine learning. In addition, we introduce the special application of Lie group machine learning in image processing. This work covers the following techniques: Lie group machine learning model, Lie group subspace orbit generation learning, symplectic group learning, quantum group learning, Lie group fiber bundle learning, Lie group cover learning, Lie group deep structure learning, Lie group semisupervised learning, Lie group kernel learning, tensor learning, frame bundle connection learning, spectral estimation learning, Finsler geometric learning, homology boundary learning, category representation learning, and neuromorphic synergy learning. Overall, this survey aims to provide an insightful overview of state-of-the-art development in the field of Lie group machine learning. It will enable researchers to comprehensively understand the state of the field, identify the most appropriate tools for particular applications, and identify directions for future research.} }
@article{WOS:000456754100058, title = {Current and future applications of statistical machine learning algorithms for agricultural machine vision systems}, journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE}, volume = {156}, pages = {585-605}, year = {2019}, issn = {0168-1699}, doi = {10.1016/j.compag.2018.12.006}, author = {Rehman, Tanzeel U. and Mahmud, Md Sultan and Chang, Young K. and Jin, Jian and Shin, Jaemyung}, abstract = {With being rapid increasing population in worldwide, the need for satisfactory level of crop production with decreased amount of agricultural lands. Machine vision would ensure the increase of crop production by using an automated, non-destructive and cost-effective technique. In last few years, remarkable results have been achieved in different sectors of agriculture. These achievements are integrated with machine learning techniques on machine vision approach that cope with colour, shape, texture and spectral analysis from the image of objects. Despite having many applications of different machine learning techniques, this review only described the statistical machine learning technologies with machine vision systems in agriculture due to broad area of machine learning applications. Two types of statistical machine learning techniques such as supervised and unsupervised learning have been utilized for agriculture. This paper comprehensively surveyed current application of statistical machine learning techniques in machine vision systems, analyses each technique potential for specific application and represents an overview of instructive examples in different agricultural areas. Suggestions of specific statistical machine learning technique for specific purpose and limitations of each technique are also given. Future trends of statistical machine learning technology applications are discussed.} }
@article{WOS:000491213400062, title = {Wind power forecasting based on daily wind speed data using machine learning algorithms}, journal = {ENERGY CONVERSION AND MANAGEMENT}, volume = {198}, year = {2019}, issn = {0196-8904}, doi = {10.1016/j.enconman.2019.111823}, author = {Demolli, Halil and Dokuz, Ahmet Sakir and Ecemis, Alper and Gokcek, Murat}, abstract = {Wind energy is a significant and eligible source that has the potential for producing energy in a continuous and sustainable manner among renewable energy sources. However, wind energy has several challenges, such as initial investment costs, the stationary property of wind plants, and the difficulty in finding wind-efficient energy areas. In this study, long-term wind power forecasting was performed based on daily wind speed data using five machine learning algorithms. We proposed a method based on machine learning algorithms to forecast wind power values efficiently. We conducted several case studies to reveal performances of machine learning algorithms. The results showed that machine learning algorithms could be used for forecasting long-term wind power values with respect to historical wind speed data. Furthermore, the results showed that machine learning-based models could be applied to a location different from model-trained locations. This study demonstrated that machine learning algorithms could be successfully used before the establishment of wind plants in an unknown geographical location whether it is logical by using the model of a base location.} }
@article{WOS:000464886100002, title = {Machine learning and complex biological data}, journal = {GENOME BIOLOGY}, volume = {20}, year = {2019}, issn = {1474-760X}, doi = {10.1186/s13059-019-1689-0}, author = {Xu, Chunming and Jackson, Scott A.}, abstract = {Machine learning has demonstrated potential in analyzing large, complex biological data. In practice, however, biological information is required in addition to machine learning for successful application.} }
@article{WOS:000484866500001, title = {Machine learning applications in epilepsy}, journal = {EPILEPSIA}, volume = {60}, pages = {2037-2047}, year = {2019}, issn = {0013-9580}, doi = {10.1111/epi.16333}, author = {Abbasi, Bardia and Goldenholz, Daniel M.}, abstract = {Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre-surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.} }
@article{WOS:000398133500010, title = {Machine Learning for Medical Imaging1}, journal = {RADIOGRAPHICS}, volume = {37}, pages = {505-515}, year = {2017}, issn = {0271-5333}, doi = {10.1148/rg.2017160130}, author = {Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy L.}, abstract = {Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.} }
@article{WOS:000473818300119, title = {Machine learning for email spam filtering: review, approaches and open research problems}, journal = {HELIYON}, volume = {5}, year = {2019}, doi = {10.1016/j.heliyon.2019.e01802}, author = {Dada, Emmanuel Gbenga and Bassi, Joseph Stephen and Chiroma, Haruna and Abdulhamid, Shafi'i Muhammad and Adetunmbi, Adebayo Olusola and Ajibuwa, Opeyemi Emmanuel}, abstract = {The upsurge in the volume of unwanted emails called spam has created an intense need for the development of more dependable and robust antispam fillers. Machine learning methods of recent are being used to successfully detect and filter spam emails. We present a systematic review of some of the popular machine learning based email spam filtering approaches. Our review covers survey of the important concepts, attempts, efficiency, and the research trend in spam filtering. The preliminary discussion in the study background examines the applications of machine learning techniques to the email spam filtering process of the leading internet service providers (ISPs) like Gmail, Yahoo and Outlook emails spam fillers. Discussion on general email spam filtering process, and the various efforts by different researchers in combating spam through the use machine learning techniques was done. Our review compares the strengths and drawbacks of existing machine learning approaches and the open research problems in spam filtering. We recommended deep leaning and deep adversarial learning as the future techniques that can effectively handle the menace of spam emails.} }
@article{WOS:000542536700007, title = {Machine Learning Systems and Intelligent Applications}, journal = {IEEE SOFTWARE}, volume = {37}, pages = {43-49}, year = {2020}, issn = {0740-7459}, doi = {10.1109/MS.2020.2985224}, author = {Benton, William C.}, abstract = {Machine learning techniques are useful in a wide range of contexts, but techniques alone are insufficient to solve real business problems. We introduce the intelligent applications concept, which characterizes the structure and responsibilities of contemporary machine learning systems.} }
@article{WOS:000485253300001, title = {Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods}, journal = {ACADEMIC PATHOLOGY}, volume = {6}, year = {2019}, issn = {2374-2895}, doi = {10.1177/2374289519873088}, author = {Rashidi, Hooman H. and Tran, Nam K. and Betts, Elham Vali and Howell, Lydia P. and Green, Ralph}, abstract = {Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).} }
@article{WOS:000470026000001, title = {Diversity in Machine Learning}, journal = {IEEE ACCESS}, volume = {7}, pages = {64323-64350}, year = {2019}, issn = {2169-3536}, doi = {10.1109/ACCESS.2019.2917620}, author = {Gong, Zhiqiang and Zhong, Ping and Hu, Weidong}, abstract = {Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.} }
@article{WOS:000497715600006, title = {A taxonomy and survey of attacks against machine learning}, journal = {COMPUTER SCIENCE REVIEW}, volume = {34}, year = {2019}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2019.100199}, author = {Pitropakis, Nikolaos and Panaousis, Emmanouil and Giannetsos, Thanassis and Anastasiadis, Eleftherios and Loukas, George}, abstract = {The majority of machine learning methodologies operate with the assumption that their environment is benign. However, this assumption does not always hold, as it is often advantageous to adversaries to maliciously modify the training (poisoning attacks) or test data (evasion attacks). Such attacks can be catastrophic given the growth and the penetration of machine learning applications in society. Therefore, there is a need to secure machine learning enabling the safe adoption of it in adversarial cases, such as spam filtering, malware detection, and biometric recognition. This paper presents a taxonomy and survey of attacks against systems that use machine learning. It organizes the body of knowledge in adversarial machine learning so as to identify the aspects where researchers from different fields can contribute to. The taxonomy identifies attacks which share key characteristics and as such can potentially be addressed by the same defence approaches. Thus, the proposed taxonomy makes it easier to understand the existing attack landscape towards developing defence mechanisms, which are not investigated in this survey. The taxonomy is also leveraged to identify open problems that can lead to new research areas within the field of adversarial machine learning. (C) 2019 Elsevier Inc. All rights reserved.} }
@article{WOS:000465199900003, title = {How to develop machine learning models for healthcare}, journal = {NATURE MATERIALS}, volume = {18}, pages = {410-414}, year = {2019}, issn = {1476-1122}, doi = {10.1038/s41563-019-0345-0}, author = {Chen, Po-Hsuan Cameron and Liu, Yun and Peng, Lily}, abstract = {Rapid progress in machine learning is enabling opportunities for improved clinical decision support. Importantly, however, developing, validating and implementing machine learning models for healthcare entail some particular considerations to increase the chances of eventually improving patient care.} }
@article{WOS:000447085500015, title = {Deep learning and its applications to machine health monitoring}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {115}, pages = {213-237}, year = {2019}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2018.05.050}, author = {Zhao, Rui and Yan, Ruqiang and Chen, Zhenghua and Mao, Kezhi and Wang, Peng and Gao, Robert X.}, abstract = {Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In addition, an experimental study on the performances of these approaches has been conducted, in which the data and code have been online. Finally, some new trends of DL-based machine health monitoring methods are discussed. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:000472453300009, title = {A survey on evolutionary machine learning}, journal = {JOURNAL OF THE ROYAL SOCIETY OF NEW ZEALAND}, volume = {49}, pages = {205-228}, year = {2019}, issn = {0303-6758}, doi = {10.1080/03036758.2019.1609052}, author = {Al-Sahaf, Harith and Bi, Ying and Chen, Qi and Lensen, Andrew and Mei, Yi and Sun, Yanan and Tran, Binh and Xue, Bing and Zhang, Mengjie}, abstract = {Artificial intelligence (AI) emphasises the creation of intelligent machines/systems that function like humans. AI has been applied to many real-world applications. Machine learning is a branch of AI based on the idea that systems can learn from data, identify hidden patterns, and make decisions with little/minimal human intervention. Evolutionary computation is an umbrella of population-based intelligent/learning algorithms inspired by nature, where New Zealand has a good international reputation. This paper provides a review on evolutionary machine learning, i.e. evolutionary computation techniques for major machine learning tasks such as classification, regression and clustering, and emerging topics including combinatorial optimisation, computer vision, deep learning, transfer learning, and ensemble learning. The paper also provides a brief review of evolutionary learning applications, such as supply chain and manufacturing for milk/dairy, wine and seafood industries, which are important to New Zealand. Finally, the paper presents current issues with future perspectives in evolutionary machine learning.} }
@article{WOS:000463615200004, title = {Using Machine Learning to Advance Personality Assessment and Theory}, journal = {PERSONALITY AND SOCIAL PSYCHOLOGY REVIEW}, volume = {23}, pages = {190-203}, year = {2019}, issn = {1088-8683}, doi = {10.1177/1088868318772990}, author = {Bleidorn, Wiebke and Hopwood, Christopher James}, abstract = {Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.} }
@article{WOS:000473429200010, title = {Machine Learning for Health Services Researchers}, journal = {VALUE IN HEALTH}, volume = {22}, pages = {808-815}, year = {2019}, issn = {1098-3015}, doi = {10.1016/j.jval.2019.02.012}, author = {Doupe, Patrick and Faghmous, James and Basu, Sanjay}, abstract = {Background: Machine learning is increasingly used to predict healthcare outcomes, including cost, utilization, and quality. Objective: We provide a high-level overview of machine learning for healthcare outcomes researchers and decision makers. Methods: We introduce key concepts for understanding the application of machine learning methods to healthcare outcomes research. We first describe current standards to rigorously learn an estimator, which is an algorithm developed through machine learning to predict a particular outcome. We include steps for data preparation, estimator family selection, parameter learning, regularization, and evaluation. We then compare 3 of the most common machine learning methods: (1) decision tree methods that can be useful for identifying how different subpopulations experience different risks for an outcome; (2) deep learning methods that can identify complex nonlinear patterns or interactions between variables predictive of an outcome; and (3) ensemble methods that can improve predictive performance by combining multiple machine learning methods. Results: We demonstrate the application of common machine methods to a simulated insurance claims dataset. We specifically include statistical code in R and Python for the development and evaluation of estimators for predicting which patients are at heightened risk for hospitalization from ambulatory care-sensitive conditions. Conclusions: Outcomes researchers should be aware of key standards for rigorously evaluating an estimator developed through machine learning approaches. Although multiple methods use machine learning concepts, different approaches are best suited for different research problems.} }
@article{WOS:000502191300012, title = {Machine learning in resting-state fMRI analysis}, journal = {MAGNETIC RESONANCE IMAGING}, volume = {64}, pages = {101-121}, year = {2019}, issn = {0730-725X}, doi = {10.1016/j.mri.2019.05.031}, author = {Khosla, Meenakshi and Jamison, Keith and Ngo, Gia H. and Kuceyeski, Amy and Sabuncu, Mert R.}, abstract = {Machine learning techniques have gained prominence for the analysis of resting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we present an overview of various unsupervised and supervised machine learning applications to rs-fMRI. We offer a methodical taxonomy of machine learning methods in restingstate fMRI. We identify three major divisions of unsupervised learning methods with regard to their applications to rs-fMRI, based on whether they discover principal modes of variation across space, time or population. Next, we survey the algorithms and rs-fMRI feature representations that have driven the success of supervised subjectlevel predictions. The goal is to provide a high-level overview of the burgeoning field of rs-fMRI from the perspective of machine learning applications.} }
@article{WOS:000355286600032, title = {Probabilistic machine learning and artificial intelligence}, journal = {NATURE}, volume = {521}, pages = {452-459}, year = {2015}, issn = {0028-0836}, doi = {10.1038/nature14541}, author = {Ghahramani, Zoubin}, abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.} }
@article{WOS:000501336600003, title = {Application and comparison of several machine learning algorithms and their integration models in regression problems}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {32}, pages = {5461-5469}, year = {2020}, issn = {0941-0643}, doi = {10.1007/s00521-019-04644-5}, author = {Huang, Jui-Chan and Ko, Kuo-Min and Shu, Ming-Hung and Hsu, Bi-Min}, abstract = {With the rapid development of machine learning technology, as a regression problem that helps people to find the law from the massive data to achieve the prediction effect, more and more people pay attention. Data prediction has become an important part of people's daily life. Currently, the technology is widely used in many fields such as weather forecasting, medical diagnosis and financial forecasting. Therefore, the research of machine learning algorithms in regression problems is a research hotspot in the field of machine learning in recent years. However, real-world regression problems often have very complex internal and external factors, and various machine learning algorithms have different effects on scalability and predictive performance. In order to better study the application effect of machine learning algorithm in regression problem, this paper mainly adopts three common machine learning algorithms: BP neural network, extreme learning machine and support vector machine. Then, by comparing the effects of the single model and integrated model of these machine learning algorithms in the application of regression problems, the advantages and disadvantages of each machine learning algorithm are studied. Finally, the performance of each machine learning algorithm in regression prediction is verified by simulation experiments on four different data sets. The results show that the research on several machine learning algorithms and their integration models has certain feasibility and rationality.} }
@article{WOS:000502169900112, title = {Machine Learning for Accelerated Discovery of Solar Photocatalysts}, journal = {ACS CATALYSIS}, volume = {9}, pages = {11774-11787}, year = {2019}, issn = {2155-5435}, doi = {10.1021/acscatal.9b02531}, author = {Masood, Hassan and Toe, Cui Ying and Teoh, Wey Yang and Sethu, Vidhyasaharan and Amal, Rose}, abstract = {Robust screening of materials on the basis of structure-property-activity relationships to discover active photocatalysts is a highly sought out aspect of photocatalysis research. Recent advancements in machine learning offer considerable opportunities to evolve photocatalysts discovery practices. Machine learning has largely facilitated various areas of science and engineering, including heterogeneous catalysis, but adaptation of it in photocatalysis research is still at an elementary stage. The scarcity of consistent training data is a major bottleneck, and we foresee the integration of photo-catalysis domain knowledge in mainstream machine learning protocols as a viable solution. Here, we present a holistic framework incorporating machine learning and domain knowledge to set directions toward accelerated discovery of solar photocatalysts. This Perspective begins with a discussion on domain knowledge available in photocatalysis which could potentially be leveraged to liaise with machine learning methods. Subsequently, we present prevalent machine learning practices in heterogeneous catalysis tailored to assist discovery of photocatalysts in a purely data-driven fashion. Lastly, we conceptualize various strategies for complementing data-driven machine learning with photocatalysis domain knowledge. The strategies involve the following: (i) integration of theoretical and prior empirical knowledge during the training of machine learning models; (ii) embedding the knowledge in feature space; and (iii) utilizing existing material databases to constrain machine learning predictions. The aforementioned human-in-loop framework (leveraging both human and machine intelligence) could possibly mitigate the lack of interpretability and reliability associated with data-driven machine learning and reinforce complex model architectures irrespective of data scarcity. The concept could also offer substantial benefits to photocatalysis informatics by promoting a paradigm shift away from the Edisonian approach.} }
@article{WOS:000471357900035, title = {Radiological images and machine learning: Trends, perspectives, and prospects}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {108}, pages = {354-370}, year = {2019}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2019.02.017}, author = {Zhang, Zhenwei and Sejdic, Ervin}, abstract = {The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.} }
@article{WOS:000503827500061, title = {Machine learning in the Internet of Things: Designed techniques for smart cities}, journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}, volume = {100}, pages = {826-843}, year = {2019}, issn = {0167-739X}, doi = {10.1016/j.future.2019.04.017}, author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J. P. C. and Hassan, Suhaidi and Korotaev, Valery V.}, abstract = {Machine learning is one of the emerging technologies that has grabbed the attention of academicians and industrialists, and is expected to evolve in the near future. Machine learning techniques are anticipated to provide pervasive connections for wireless nodes. In fact, machine learning paves the way for the Internet of Things (IoT)-a network that supports communications among various devices without human interactions. Machine learning techniques are being utilized in several fields such as healthcare, smart grids, vehicular communications, and so on. In this paper, we study different IoT-based machine learning mechanisms that are used in the mentioned fields among others. In addition, the lessons learned are reported and the assessments are explored viewing the basic aim machine learning techniques are expected to play in IoT networks. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000478780200033, title = {Quantifying performance of machine learning methods for neuroimaging data}, journal = {NEUROIMAGE}, volume = {199}, pages = {351-365}, year = {2019}, issn = {1053-8119}, doi = {10.1016/j.neuroimage.2019.05.082}, author = {Jollans, Lee and Boyle, Rory and Artiges, Eric and Banaschewski, Tobias and Desrivieres, Sylvane and Grigis, Antoine and Martinot, Jean-Luc and Paus, Tomas and Smolka, Michael N. and Walter, Henrik and Schumann, Gunter and Garavan, Hugh and Whelan, Robert}, abstract = {Machine learning is increasingly being applied to neuroimaging data. However, most machine learning algorithms have not been designed to accommodate neuroimaging data, which typically has many more data points than subjects, in addition to multicollinearity and low signal-to-noise. Consequently, the relative efficacy of different machine learning regression algorithms for different types of neuroimaging data are not known. Here, we sought to quantify the performance of a variety of machine learning algorithms for use with neuroimaging data with various sample sizes, feature set sizes, and predictor effect sizes. The contribution of additional machine learning techniques - embedded feature selection and bootstrap aggregation (bagging) - to model performance was also quantified. Five machine learning regression methods - Gaussian Process Regression, Multiple Kernel Learning, Kernel Ridge Regression, the Elastic Net and Random Forest, were examined with both real and simulated MRI data, and in comparison to standard multiple regression. The different machine learning regression algorithms produced varying results, which depended on sample size, feature set size, and predictor effect size. When the effect size was large, the Elastic Net, Kernel Ridge Regression and Gaussian Process Regression performed well at most sample sizes and feature set sizes. However, when the effect size was small, only the Elastic Net made accurate predictions, but this was limited to analyses with sample sizes greater than 400. Random Forest also produced a moderate performance for small effect sizes, but could do so across all sample sizes. Machine learning techniques also improved prediction accuracy for multiple regression. These data provide empirical evidence for the differential performance of various machines on neuroimaging data, which are dependent on number of sample size, features and effect size.} }
@article{WOS:000512985500004, title = {A primer on machine learning}, journal = {RADIOLOGE}, volume = {60}, pages = {24-31}, year = {2020}, issn = {0033-832X}, doi = {10.1007/s00117-019-00616-x}, author = {Kleesiek, Jens and Murray, Jacob M. and Strack, Christian and Kaissis, Georgios and Braren, Rickmer}, abstract = {Background The methods of machine learning and artificial intelligence are slowly but surely being introduced in everyday medical practice. In the future, they will support us in diagnosis and therapy and thus improve treatment for the benefit of the individual patient. It is therefore important to deal with this topic and to develop a basic understanding of it. Objectives This article gives an overview of the exciting and dynamic field of machine learning and serves as an introduction to some methods primarily from the realm of supervised learning. In addition to definitions and simple examples, limitations are discussed. Conclusions The basic principles behind the methods are simple. Nevertheless, due to their high dimensional nature, the factors influencing the results are often difficult or impossible to understand by humans. In order to build confidence in the new technologies and to guarantee their safe application, we need explainable algorithms and prospective effectiveness studies.} }
@article{WOS:000425074100016, title = {The use of machine learning algorithms in recommender systems: A systematic review}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {97}, pages = {205-227}, year = {2018}, issn = {0957-4174}, doi = {10.1016/j.eswa.2017.12.020}, author = {Portugal, Ivens and Alencar, Paulo and Cowan, Donald}, abstract = {Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000471293100006, title = {Machine learning in cybersecurity: A review}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {9}, year = {2019}, issn = {1942-4787}, doi = {10.1002/widm.1306}, author = {Handa, Anand and Sharma, Ashu and Shukla, Sandeep K.}, abstract = {Machine learning technology has become mainstream in a large number of domains, and cybersecurity applications of machine learning techniques are plenty. Examples include malware analysis, especially for zero-day malware detection, threat analysis, anomaly based intrusion detection of prevalent attacks on critical infrastructures, and many others. Due to the ineffectiveness of signature-based methods in detecting zero day attacks or even slight variants of known attacks, machine learning-based detection is being used by researchers in many cybersecurity products. In this review, we discuss several areas of cybersecurity where machine learning is used as a tool. We also provide a few glimpses of adversarial attacks on machine learning algorithms to manipulate training and test data of classifiers, to render such tools ineffective. This article is categorized under: Application Areas > Science and Technology Technologies > Machine Learning Technologies > Classification Application Areas > Data Mining Software Tools} }
@article{WOS:000488315100001, title = {Machine learning approaches for pathologic diagnosis}, journal = {VIRCHOWS ARCHIV}, volume = {475}, pages = {131-138}, year = {2019}, issn = {0945-6317}, doi = {10.1007/s00428-019-02594-w}, author = {Komura, Daisuke and Ishikawa, Shumpei}, abstract = {Machine learning techniques, especially deep learning techniques such as convolutional neural networks, have been successfully applied to general image recognitions since their overwhelming performance at the 2012 ImageNet Large Scale Visual Recognition Challenge. Recently, such techniques have also been applied to various medical, including histopathological, images to assist the process of medical diagnosis. In some cases, deep learning-based algorithms have already outperformed experienced pathologists for recognition of histopathological images. However, pathological images differ from general images in some aspects, and thus, machine learning of histopathological images requires specialized learning methods. Moreover, many pathologists are skeptical about the ability of deep learning technology to accurately recognize histopathological images because what the learned neural network recognizes is often indecipherable to humans. In this review, we first introduce various applications incorporating machine learning developed to assist the process of pathologic diagnosis, and then describe machine learning problems related to histopathological image analysis, and review potential ways to solve these problems.} }
@article{WOS:000439847600006, title = {Machine learning in cardiovascular medicine: are we there yet?}, journal = {HEART}, volume = {104}, pages = {1156-1164}, year = {2018}, issn = {1355-6037}, doi = {10.1136/heartjnl-2017-311198}, author = {Shameer, Khader and Johnson, Kipp W. and Glicksberg, Benjamin S. and Dudley, Joel T. and Sengupta, Partho P.}, abstract = {Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.} }
@article{WOS:000481489500005, title = {Machine Learning for Stock Selection}, journal = {FINANCIAL ANALYSTS JOURNAL}, volume = {75}, pages = {70-88}, year = {2019}, issn = {0015-198X}, doi = {10.1080/0015198X.2019.1596678}, author = {Rasekhschaffe, Keywan Christian and Jones, Robert C.}, abstract = {Machine learning is an increasingly important and controversial topic in quantitative finance. A lively debate persists as to whether machine learning techniques can be practical investment tools. Although machine learning algorithms can uncover subtle, contextual. and nonlinear relationships, overfitting poses a major challenge when one is trying to extract signals from noisy historical data. We describe some of the basic concepts of machine learning and provide a simple example of how investors can use machine learning techniques to forecast the cross-section of stock returns while limiting the risk of overfitting.} }
@article{WOS:000484486600010, title = {Machine learning in predicting graft failure following kidney transplantation: A systematic review of published predictive models}, journal = {INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS}, volume = {130}, year = {2019}, issn = {1386-5056}, doi = {10.1016/j.ijmedinf.2019.103957}, author = {Senanayake, Sameera and White, Nicole and Graves, Nicholas and Healy, Helen and Baboolal, Keshwar and Kularatna, Sanjeewa}, abstract = {Introduction: Machine learning has been increasingly used to develop predictive models to diagnose different disease conditions. The heterogeneity of the kidney transplant population makes predicting graft outcomes extremely challenging. Several kidney graft outcome prediction models have been developed using machine learning, and are available in the literature. However, a systematic review of machine learning based prediction methods applied to kidney transplant has not been done to date. The main aim of our study was to perform an in-depth systematic analysis of different machine learning methods used to predict graft outcomes among kidney transplant patients, and assess their usefulness as an aid to decision-making. Methods: A systemic review of machine learning methods used to predict graft outcomes among kidney transplant patients was carried out using a search of the Medline, the Cumulative Index to Nursing and Allied Health Literature, EMBASE, PsycINFO and Cochrane databases. Results: A total of 295 articles were identified and extracted. Of these, 18 ma the inclusion criteria. Most of the studies were published in the United States after 2010. The population size used to develop the models varied from 80 to 92,844, and the number of features in the models ranged from 6 to 71. The most common machine learning methods used were artificial neural networks, decision trees and Bayesian belief networks. Most of the machine learning based predictive models predicted graft failure with high sensitivity and specificity. Only one machine learning based prediction model had modelled time-to-event (survival) information. Seven studies compared the predictive performance of machine learning models with traditional regression methods and the performance of machine learning methods was found to be mixed, when compared with traditional regression methods. Conclusion: There was a wide variation in the size of the study population and the input variables used. However, the prediction accuracy provided mixed results when machine learning and traditional predictive methods are compared. Based on reported gains in predictive performance, machine learning has the potential to improve kidney transplant outcome prediction and aid medical decision making} }
@article{WOS:000493335100001, title = {Identification of advanced spin-driven thermoelectric materials via interpretable machine learning}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, doi = {10.1038/s41524-019-0241-9}, author = {Iwasaki, Yuma and Sawada, Ryohto and Stanev, Valentin and Ishida, Masahiko and Kirihara, Akihiro and Omori, Yasutomo and Someya, Hiroko and Takeuchi, Ichiro and Saitoh, Eiji and Yorozu, Shinichi}, abstract = {Machine learning is becoming a valuable tool for scientific discovery. Particularly attractive is the application of machine learning methods to the field of materials development, which enables innovations by discovering new and better functional materials. To apply machine learning to actual materials development, close collaboration between scientists and machine learning tools is necessary. However, such collaboration has been so far impeded by the black box nature of many machine learning algorithms. It is often difficult for scientists to interpret the data-driven models from the viewpoint of material science and physics. Here, we demonstrate the development of spin-driven thermoelectric materials with anomalous Nernst effect by using an interpretable machine learning method called factorized asymptotic Bayesian inference hierarchical mixture of experts (FAB/HMEs). Based on prior knowledge of material science and physics, we were able to extract from the interpretable machine learning some surprising correlations and new knowledge about spin-driven thermoelectric materials. Guided by this, we carried out an actual material synthesis that led to the identification of a novel spin-driven thermoelectric material. This material shows the largest thermopower to date.} }
@article{WOS:000417528000001, title = {Ten quick tips for machine learning in computational biology}, journal = {BIODATA MINING}, volume = {10}, year = {2017}, issn = {1756-0381}, doi = {10.1186/s13040-017-0155-3}, author = {Chicco, Davide}, abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.} }
@article{WOS:000462868200006, title = {Machine Learning in Nuclear Medicine: Part 1-Introduction}, journal = {JOURNAL OF NUCLEAR MEDICINE}, volume = {60}, pages = {451-458}, year = {2019}, issn = {0161-5505}, doi = {10.2967/jnumed.118.223495}, author = {Uribe, Carlos F. and Mathotaarachchi, Sulantha and Gaudet, Vincent and Smith, Kenneth C. and Rosa-Neto, Pedro and Benard, Francois and Black, Sandra E. and Zukotynski, Katherine}, abstract = {Learning Objectives: On successful completion of this activity, participants should be able to (1) provide an introduction to machine learning, neural networks, and deep learning; (2) discuss common machine learning algorithms with illustrative examples and figures; and (3) compare machine learning algorithms and provide guidance on selection for a given application.} }
@article{WOS:000514113100007, title = {Sofware engneering challenges for machine learning applications: A literature review}, journal = {INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS}, volume = {13}, pages = {463-476}, year = {2019}, issn = {1872-4981}, doi = {10.3233/IDT-190160}, author = {Kumeno, Fumihiro}, abstract = {Machine learning techniques, especially deep learning, have achieved remarkable breakthroughs over the past decade. At present, machine learning applications are deployed in many fields. However, the outcomes of software engineering researches are not always easily utilized in the development and deployment of machine learning applications. The main reason for this difficulty is the many differences between machine learning applications and traditional information systems. Machine learning techniques are evolving rapidly, but face inherent technical and non-technical challenges that complicate their lifecycle activities. This review paper attempts to clarify the software engineering challenges for machine learning applications that either exist or potentially exist by conducting a systematic literature collection and by mapping the identified challenge topics to knowledge areas defined by the Software Engineering Body of Knowledge (Swebok).} }
@article{WOS:000454602800004, title = {Machine learning research that matters for music creation: A case study}, journal = {JOURNAL OF NEW MUSIC RESEARCH}, volume = {48}, pages = {36-55}, year = {2019}, issn = {0929-8215}, doi = {10.1080/09298215.2018.1515233}, author = {Sturm, Bob L. and Ben-Tal, Oded and Monaghan, Una and Collins, Nick and Herremans, Dorien and Chew, Elaine and Hadjeres, Gaetan and Deruty, Emmanuel and Pachet, Francois}, abstract = {Research applying machine learning to music modelling and generation typically proposes model architectures, training methods and datasets, and gauges system performance using quantitative measures like sequence likelihoods and/or qualitative listening tests. Rarely does such work explicitly question and analyse its usefulness for and impact on real-world practitioners, and then build on those outcomes to inform the development and application of machine learning. This article attempts to do these things for machine learning applied to music creation. Together with practitioners, we develop and use several applications of machine learning for music creation, and present a public concert of the results. We reflect on the entire experience to arrive at several ways of advancing these and similar applications of machine learning to music creation.} }
@article{WOS:000489702000038, title = {Autonomic machine learning platform}, journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT}, volume = {49}, pages = {491-501}, year = {2019}, issn = {0268-4012}, doi = {10.1016/j.ijinfomgt.2019.07.003}, author = {Lee, Keon Myung and Yoo, Jaesoo and Kim, Sang-Wook and Lee, Jee-Hyong and Hong, Jiman}, abstract = {Acquiring information properly through machine learning requires familiarity with the available algorithms and understanding how they work and how to address the given problem in the best possible way. However, even for machine-learning experts in specific industrial fields, in order to predict and acquire information properly in different industrial fields, it is necessary to attempt several instances of trial and error to succeed with the application of machine learning. For non-experts, it is much more difficult to make accurate predictions through machine learning. In this paper, we propose an autonomic machine learning platform which provides the decision factors to be made during the developing of machine learning applications. In the proposed autonomic machine learning platform, machine learning processes are automated based on the specification of autonomic levels. This autonomic machine learning platform can be used to derive a high-quality learning result by minimizing experts' interventions and reducing the number of design selections that require expert knowledge and intuition. We also demonstrate that the proposed autonomic machine learning platform is suitable for smart cities which typically require considerable amounts of security sensitive information.} }
@article{WOS:000504871100008, title = {Predictive Modeling of Outcomes After Traumatic and Nontraumatic Spinal Cord Injury Using Machine Learning: Review of Current Progress and Future Directions}, journal = {NEUROSPINE}, volume = {16}, pages = {678-685}, year = {2019}, issn = {2586-6583}, doi = {10.14245/ns.1938390.195}, author = {Khan, Omar and Badhiwala, Jetan H. and Wilson, Jamie R. F. and Jiang, Fan and Martin, Allan R. and Fehlings, Michael G.}, abstract = {Machine learning represents a promising frontier in epidemiological research on spine surgery. It consists of a series of algorithms that determines relationships between data. Machine learning maintains numerous advantages over conventional regression techniques, such as a reduced requirement for a priori knowledge on predictors and better ability to manage large datasets. Current studies have made extensive strides in employing machine learning to a greater capacity in spinal cord injury (SCI). Analyses using machine learning algorithms have been done on both traumatic SCI and nontraumatic SCI, the latter of which typically represents degenerative spine disease resulting in spinal cord compression, such as degenerative cervical myelopathy. This article is a literature review of current studies published in traumatic and nontraumatic SCI that employ machine learning for the prediction of a host of outcomes. The studies described utilize machine learning in a variety of capacities, including imaging analysis and prediction in large epidemiological data sets. We discuss the performance of these machine learning-based clinical prognostic models relative to conventional statistical prediction models. Finally, we detail the future steps needed for machine learning to become a more common modality for statistical analysis in SCI.} }
@article{WOS:000448233900010, title = {Smart Machining Process Using Machine Learning: A Review and Perspective on Machining Industry}, journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN TECHNOLOGY}, volume = {5}, pages = {555-568}, year = {2018}, issn = {2288-6206}, doi = {10.1007/s40684-018-0057-y}, author = {Kim, Dong-Hyeon and Kim, Thomas J. Y. and Wang, Xinlin and Kim, Mincheol and Quan, Ying-Jun and Oh, Jin Woo and Min, Soo-Hong and Kim, Hyungjung and Bhandari, Binayak and Yang, Insoon and Ahn, Sung-Hoon}, abstract = {The Fourth Industrial Revolution incorporates the digital revolution into the physical world, creating a new direction in a number of fields, including artificial intelligence, quantum computing, nanotechnology, biotechnology, robotics, 3D printing, autonomous vehicles, and the Internet of Things. The artificial intelligence field has encountered a turning point mainly due to advancements in machine learning, which allows machines to learn, improve, and perform a specific task through data without being explicitly programmed. Machine learning can be utilized with machining processes to improve product quality levels and productivity rates, to monitor the health of systems, and to optimize design and process parameters. This is known as smart machining, referring to a new machining paradigm in which machine tools are fully connected through a cyber-physical system. This paper reviews and summarizes machining processes using machine learning algorithms and suggests a perspective on the machining industry.} }
@article{WOS:000424709000002, title = {Machine learning, social learning and the governance of self-driving cars}, journal = {SOCIAL STUDIES OF SCIENCE}, volume = {48}, pages = {25-56}, year = {2018}, issn = {0306-3127}, doi = {10.1177/0306312717741687}, author = {Stilgoe, Jack}, abstract = {Self-driving cars, a quintessentially smart' technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking Who is learning, what are they learning and how are they learning?' Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. Self-driving' or autonomous' cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.} }
@article{WOS:000499323200025, title = {Simulation-assisted machine learning}, journal = {BIOINFORMATICS}, volume = {35}, pages = {4072-4080}, year = {2019}, issn = {1367-4803}, doi = {10.1093/bioinformatics/btz199}, author = {Deist, Timo M. and Patti, Andrew and Wang, Zhaoqi and Krane, David and Sorenson, Taylor and Craft, David}, abstract = {Motivation: In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these two extremes: some details of the system mechanics are known but not enough for creating simulations that can be used to make high quality predictions. In this context we propose using approximate simulations to build a kernel for use in kernelized machine learning methods, such as support vector machines. The results of multiple simulations (under various uncertainty scenarios) are used to compute similarity measures between every pair of samples: sample pairs are given a high similarity score if they behave similarly under a wide range of simulation parameters. These similarity values, rather than the original high dimensional feature data, are used to build the kernel. Results: We demonstrate and explore the simulation-based kernel (SimKern) concept using four synthetic complex systems-three biologically inspired models and one network flow optimization model. We show that, when the number of training samples is small compared to the number of features, the SimKern approach dominates over no-prior-knowledge methods. This approach should be applicable in all disciplines where predictive models are sought and informative yet approximate simulations are available.} }
@article{WOS:000710554200001, title = {Kernel methods in Quantum Machine Learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {1}, pages = {65-71}, year = {2019}, issn = {2524-4906}, doi = {10.1007/s42484-019-00007-4}, author = {Mengoni, Riccardo and Di Pierro, Alessandra}, abstract = {Quantum Machine Learning has established itself as one of the most promising applications of quantum computers and Noisy Intermediate Scale Quantum (NISQ) devices. In this paper, we review the latest developments regarding the usage of quantum computing for a particular class of machine learning algorithms known as kernel methods.} }
@article{WOS:000483104800010, title = {Machine Learning-Based Sentiment Analysis for Twitter Accounts}, journal = {MATHEMATICAL AND COMPUTATIONAL APPLICATIONS}, volume = {23}, year = {2018}, issn = {1300-686X}, doi = {10.3390/mca23010011}, author = {Hasan, Ali and Moin, Sana and Karim, Ahmad and Shamshirband, Shahaboddin}, abstract = {Growth in the area of opinion mining and sentiment analysis has been rapid and aims to explore the opinions or text present on different platforms of social media through machine-learning techniques with sentiment, subjectivity analysis or polarity calculations. Despite the use of various machine-learning techniques and tools for sentiment analysis during elections, there is a dire need for a state-of-the-art approach. To deal with these challenges, the contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naive Bayes and support vector machines (SVM).} }
@article{WOS:000391480800001, title = {MLlib: Machine Learning in Apache Spark}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {17}, year = {2016}, issn = {1532-4435}, author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, D. B. and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet}, abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides effcient functionality fo wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.} }
@article{WOS:000391065600020, title = {Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {18}, year = {2016}, issn = {1438-8871}, doi = {10.2196/jmir.5870}, author = {Luo, Wei and Phung, Dinh and Tran, Truyen and Gupta, Sunil and Rana, Santu and Karmakar, Chandan and Shilton, Alistair and Yearwood, John and Dimitrova, Nevenka and Ho, Tu Bao and Venkatesh, Svetha and Berk, Michael}, abstract = {Background: As more and more researchers are turning to big data for new opportunities of biomedical discoveries, machine learning models, as the backbone of big data analysis, are mentioned more often in biomedical journals. However, owing to the inherent complexity of machine learning methods, they are prone to misuse. Because of the flexibility in specifying machine learning models, the results are often insufficiently reported in research articles, hindering reliable assessment of model validity and consistent interpretation of model outputs. Objective: To attain a set of guidelines on the use of machine learning predictive models within clinical settings to make sure the models are correctly applied and sufficiently reported so that true discoveries can be distinguished from random coincidence. Methods: A multidisciplinary panel of machine learning experts, clinicians, and traditional statisticians were interviewed, using an iterative process in accordance with the Delphi method. Results: The process produced a set of guidelines that consists of (1) a list of reporting items to be included in a research article and (2) a set of practical sequential steps for developing predictive models. Conclusions: A set of guidelines was generated to enable correct application of machine learning models and consistent reporting of model specifications and results in biomedical research. We believe that such guidelines will accelerate the adoption of big data analysis, particularly with machine learning methods, in the biomedical research community.} }
@article{WOS:000433269000002, title = {Machine learning-based thermal response time ahead energy demand prediction for building heating systems}, journal = {APPLIED ENERGY}, volume = {221}, pages = {16-27}, year = {2018}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2018.03.125}, author = {Guo, Yabin and Wang, Jiangyu and Chen, Huanxin and Li, Guannan and Liu, Jiangyan and Xu, Chengliang and Huang, Ronggeng and Huang, Yao}, abstract = {Energy demand prediction of building heating is conducive to optimal control, fault detection and diagnosis and building intelligentization. In this study, energy demand prediction models are developed through machine learning methods, including extreme learning machine, multiple linear regression, support vector regression and backpropagation neural network. Seven different meteorological parameters, operating parameters, time and indoor temperature parameters are used as feature variables of the model. Correlation analysis method is utilized to optimize the feature sets. Moreover, this paper proposes a strategy for obtaining the thermal response time of building, which is used as the time ahead of prediction models. The prediction performances of extreme learning machine models with various hidden layer nodes are analyzed and contrasted. Actual data of building heating using a ground source heat pump system are collected and used to test the performances of the models. Results show that the thermal response time of the building is approximately 40 min. Four feature sets are obtained, and the performances of the models with feature set 4 are better. For different machine learning methods, the performances of extreme learning machine models are better than others. In addition, the optimal number of hidden layer nodes is 11 for the extreme learning machine model with feature set 4.} }
@article{WOS:000425839800026, title = {Machine Learning in Radiology: Applications Beyond Image Interpretation}, journal = {JOURNAL OF THE AMERICAN COLLEGE OF RADIOLOGY}, volume = {15}, pages = {350-359}, year = {2018}, issn = {1546-1440}, doi = {10.1016/j.jacr.2017.09.044}, author = {Lakhani, Paras and Prater, Adam B. and Hutson, R. Kent and Andriole, Kathy P. and Dreyer, Keith J. and Morey, Jose and Prevedello, Luciano M. and Clark, Toshi J. and Geis, J. Raymond and Itri, Jason N. and Hawkins, C. Matthew}, abstract = {Much attention has been given to machine learning and its perceived impact in radiology, particularly in light of recent success with image classification in international competitions. However, machine learning is likely to impact radiology outside of image interpretation long before a fully functional ``machine radiologist'' is implemented in practice. Here, we describe an overview of machine learning, its application to radiology and other domains, and many cases of use that do not involve image interpretation. We hope that better understanding of these potential applications will help radiology practices prepare for the future and realize performance improvement and efficiency gains.} }
@article{WOS:000416496500008, title = {From machine learning to deep learning: progress in machine intelligence for rational drug discovery}, journal = {DRUG DISCOVERY TODAY}, volume = {22}, pages = {1680-1685}, year = {2017}, issn = {1359-6446}, doi = {10.1016/j.drudis.2017.08.010}, author = {Zhang, Lu and Tan, Jianjun and Han, Dan and Zhu, Hao}, abstract = {Machine intelligence, which is normally presented as artificial intelligence, refers to the intelligence exhibited by computers. In the history of rational drug discovery, various machine intelligence approaches have been applied to guide traditional experiments, which are expensive and time-consuming. Over the past several decades, machine-learning tools, such as quantitative structure activity relationship (QSAR) modeling, were developed that can identify potential biological active molecules from millions of candidate compounds quickly and cheaply. However, when drug discovery moved into the era of `big' data, machine learning approaches evolved into deep learning approaches, which are a more powerful and efficient way to deal with the massive amounts of data generated from modern drug discovery approaches. Here, we summarize the history of machine learning and provide insight into recently developed deep learning approaches and their applications in rational drug discovery. We suggest that this evolution of machine intelligence now provides a guide for early-stage drug design and discovery in the current big data era.} }
@article{WOS:001190989000001, title = {From distributed machine to distributed deep learning: a comprehensive survey}, journal = {JOURNAL OF BIG DATA}, volume = {10}, year = {2023}, doi = {10.1186/s40537-023-00829-x}, author = {Dehghani, Mohammad and Yazdanparast, Zahra}, abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.} }
@article{WOS:000442625700004, title = {Machine Learning Methods for Analysis of Metabolic Data and Metabolic Pathway Modeling}, journal = {METABOLITES}, volume = {8}, year = {2018}, doi = {10.3390/metabo8010004}, author = {Cuperlovic-Culf, Miroslava}, abstract = {Machine learning uses experimental data to optimize clustering or classification of samples or features, or to develop, augment or verify models that can be used to predict behavior or properties of systems. It is expected that machine learning will help provide actionable knowledge from a variety of big data including metabolomics data, as well as results of metabolism models. A variety of machine learning methods has been applied in bioinformatics and metabolism analyses including self-organizing maps, support vector machines, the kernel machine, Bayesian networks or fuzzy logic. To a lesser extent, machine learning has also been utilized to take advantage of the increasing availability of genomics and metabolomics data for the optimization of metabolic network models and their analysis. In this context, machine learning has aided the development of metabolic networks, the calculation of parameters for stoichiometric and kinetic models, as well as the analysis of major features in the model for the optimal application of bioreactors. Examples of this very interesting, albeit highly complex, application of machine learning for metabolism modeling will be the primary focus of this review presenting several different types of applications for model optimization, parameter determination or system analysis using models, as well as the utilization of several different types of machine learning technologies.} }
@article{WOS:000376943000001, title = {A survey of machine learning for big data processing}, journal = {EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING}, year = {2016}, issn = {1687-6180}, doi = {10.1186/s13634-016-0355-x}, author = {Qiu, Junfei and Wu, Qihui and Ding, Guoru and Xu, Yuhua and Feng, Shuo}, abstract = {There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends.} }
@article{WOS:000479003300133, title = {Physician-Friendly Machine Learning: A Case Study with Cardiovascular Disease Risk Prediction}, journal = {JOURNAL OF CLINICAL MEDICINE}, volume = {8}, year = {2019}, doi = {10.3390/jcm8071050}, author = {Padmanabhan, Meghana and Yuan, Pengyu and Chada, Govind and Hien Van Nguyen}, abstract = {Machine learning is often perceived as a sophisticated technology accessible only by highly trained experts. This prevents many physicians and biologists from using this tool in their research. The goal of this paper is to eliminate this out-dated perception. We argue that the recent development of auto machine learning techniques enables biomedical researchers to quickly build competitive machine learning classifiers without requiring in-depth knowledge about the underlying algorithms. We study the case of predicting the risk of cardiovascular diseases. To support our claim, we compare auto machine learning techniques against a graduate student using several important metrics, including the total amounts of time required for building machine learning models and the final classification accuracies on unseen test datasets. In particular, the graduate student manually builds multiple machine learning classifiers and tunes their parameters for one month using scikit-learn library, which is a popular machine learning library to obtain ones that perform best on two given, publicly available datasets. We run an auto machine learning library called auto-sklearn on the same datasets. Our experiments find that automatic machine learning takes 1 h to produce classifiers that perform better than the ones built by the graduate student in one month. More importantly, building this classifier only requires a few lines of standard code. Our findings are expected to change the way physicians see machine learning and encourage wide adoption of Artificial Intelligence (AI) techniques in clinical domains.} }
@article{WOS:000430565600012, title = {Machine learning in heart failure: ready for prime time}, journal = {CURRENT OPINION IN CARDIOLOGY}, volume = {33}, pages = {190-195}, year = {2018}, issn = {0268-4705}, doi = {10.1097/HCO.0000000000000491}, author = {Awan, Saqib Ejaz and Sohel, Ferdous and Sanfilippo, Frank Mario and Bennamoun, Mohammed and Dwivedi, Girish}, abstract = {Purpose of review The aim of this review is to present an up-to-date overview of the application of machine learning methods in heart failure including diagnosis, classification, readmissions and medication adherence. Recent findings Recent studies have shown that the application of machine learning techniques may have the potential to improve heart failure outcomes and management, including cost savings by improving existing diagnostic and treatment support systems. Recently developed deep learning methods are expected to yield even better performance than traditional machine learning techniques in performing complex tasks by learning the intricate patterns hidden in big medical data. Summary The review summarizes the recent developments in the application of machine and deep learning methods in heart failure management.} }
@article{WOS:000427315000007, title = {What is the machine learning?}, journal = {PHYSICAL REVIEW D}, volume = {97}, year = {2018}, issn = {2470-0010}, doi = {10.1103/PhysRevD.97.056009}, author = {Chang, Spencer and Cohen, Timothy and Ostdiek, Bryan}, abstract = {Applications of machine learning tools to problems of physical interest are often criticized for producing sensitivity at the expense of transparency. To address this concern, we explore a data planing procedure for identifying combinations of variables-aided by physical intuition-that can discriminate signal from background. Weights are introduced to smooth away the features in a given variable(s). New networks are then trained on this modified data. Observed decreases in sensitivity diagnose the variable's discriminating power. Planing also allows the investigation of the linear versus nonlinear nature of the boundaries between signal and background. We demonstrate the efficacy of this approach using a toy example, followed by an application to an idealized heavy resonance scenario at the Large Hadron Collider. By unpacking the information being utilized by these algorithms, this method puts in context what it means for a machine to learn.} }
@article{WOS:000435911300004, title = {Machine learning for architectural design: Practices and infrastructure}, journal = {INTERNATIONAL JOURNAL OF ARCHITECTURAL COMPUTING}, volume = {16}, pages = {123-143}, year = {2018}, issn = {1478-0771}, doi = {10.1177/1478077118778580}, author = {Tamke, Martin and Nicholas, Paul and Zwierzycki, Mateusz}, abstract = {In this article, we propose that new architectural design practices might be based on machine learning approaches to better leverage data-rich environments and workflows. Through reference to recent architectural research, we describe how the application of machine learning can occur throughout the design and fabrication process, to develop varied relations between design, performance and learning. The impact of machine learning on architectural practices with performance-based design and fabrication is assessed in two cases by the authors. We then summarise what we perceive as current limits to a more widespread application and conclude by providing an outlook and direction for future research for machine learning in architectural design practice.} }
@article{WOS:000478765300004, title = {Machine learning in empirical asset pricing}, journal = {FINANCIAL MARKETS AND PORTFOLIO MANAGEMENT}, volume = {33}, pages = {93-104}, year = {2019}, issn = {1934-4554}, doi = {10.1007/s11408-019-00326-3}, author = {Weigand, Alois}, abstract = {The tremendous speedup in computing in recent years, the low data storage costs of today, the availability of ``big data'' as well as the broad range of free open-source software, have created a renaissance in the application of machine learning techniques in science. However, this new wave of research is not limited to computer science or software engineering anymore. Among others, machine learning tools are now used in financial problem settings as well. Therefore, this paper mentions a specific definition of machine learning in an asset pricing context and elaborates on the usefulness of machine learning in this context. Most importantly, the literature review gives the reader a theoretical overview of the most recent academic studies in empirical asset pricing that employ machine learning techniques. Overall, the paper concludes that machine learning can offer benefits for future research. However, researchers should be critical about these methodologies as machine learning has its pitfalls and is relatively new to asset pricing.} }
@article{WOS:000510726600002, title = {MODEL THEORY AND MACHINE LEARNING}, journal = {BULLETIN OF SYMBOLIC LOGIC}, volume = {25}, pages = {319-332}, year = {2019}, issn = {1079-8986}, doi = {10.1017/bsl.2018.71}, author = {Chase, Hunter and Freitag, James}, abstract = {About 25 years ago, it came to light that a single combinatorial property determines both an important dividing line in model theory (NIP) and machine learning (PAC-learnability). The following years saw a fruitful exchange of ideas between PAC-learning and the model theory of NIP structures. In this article, we point out a new and similar connection between model theory and machine learning, this time developing a correspondence between stability and learnability in various settings of online learning. In particular, this gives many new examples of mathematically interesting classes which are learnable in the online setting.} }
@article{WOS:000430730700007, title = {Machine Learning for Hardware Security: Opportunities and Risks}, journal = {JOURNAL OF ELECTRONIC TESTING-THEORY AND APPLICATIONS}, volume = {34}, pages = {183-201}, year = {2018}, issn = {0923-8174}, doi = {10.1007/s10836-018-5726-9}, author = {Elnaggar, Rana and Chakrabarty, Krishnendu}, abstract = {Recently, machine learning algorithms have been utilized by system defenders and attackers to secure and attack hardware, respectively. In this work, we investigate the impact of machine learning on hardware security. We explore the defense and attack mechanisms for hardware that are based on machine learning. Moreover, we identify suitable machine learning algorithms for each category of hardware security problems. Finally, we highlight some important aspects related to the application of machine learning to hardware security problems and show how the practice of applying machine learning to hardware security problems has changed over the past decade.} }
@article{WOS:000472240500010, title = {The Promise of Machine Learning: When Will it be Delivered?}, journal = {JOURNAL OF CARDIAC FAILURE}, volume = {25}, pages = {484-485}, year = {2019}, issn = {1071-9164}, doi = {10.1016/j.cardfail.2019.04.006}, author = {Akbilgic, Oguz and Davis, Robert L.}, abstract = {Background: The real-life applications of machine learning clinical decision making is currently lagging behind its promise. One of the critics on machine learning is that it doesn't outperform more traditional statistical approaches in every problem. Methods and Results: Authors of ``Predictive Abilities of Machine Learning Techniques May Be Limited by Dataset Characteristics: Insights From the UNOS Database'' presented in the current issue of the Journal of Cardiac Failure that machine learning approaches do not provide significantly higher performance when compared to more traditional statistical approaches in predicting mortality following heart transplant. In this brief report, we provide an insight on the possible reasons for why machine learning methods do not outperform more traditional approaches for every problem and every dataset. Conclusions: Most of the performance-focused critics on machine learning are because the bar is set unfairly too high for machine learning. In most cases, machine learning methods provides at least as good results as traditional statistical methods do. It is normal for machine learning models to provide similar performance with linear models if the actual underlying input-outcome relationship is linear. Moreover, machine learning methods outperforms linear statistical models when the underlying input-output relationship is not linear and if the dataset is large enough and include predictors capturing that nonlinear relationship.} }
@article{WOS:000895924400002, title = {Survey on Encoding Schemes for Genomic Data Representation and Feature Learning-From Signal Processing to Machine Learning}, journal = {BIG DATA MINING AND ANALYTICS}, volume = {1}, pages = {191-210}, year = {2018}, doi = {10.26599/BDMA.2018.9020018}, author = {Yu, Ning and Li, Zhihua and Yu, Zeng}, abstract = {Data-driven machine learning, especially deep learning technology, is becoming an important tool for handling big data issues in bioinformatics. In machine learning, DNA sequences are often converted to numerical values for data representation and feature learning in various applications. Similar conversion occurs in Genomic Signal Processing (GSP), where genome sequences are transformed into numerical sequences for signal extraction and recognition. This kind of conversion is also called encoding scheme. The diverse encoding schemes can greatly affect the performance of GSP applications and machine learning models. This paper aims to collect, analyze, discuss, and summarize the existing encoding schemes of genome sequence particularly in GSP as well as other genome analysis applications to provide a comprehensive reference for the genomic data representation and feature learning in machine learning.} }
@article{WOS:000405098300001, title = {A Proposal on Machine Learning via Dynamical Systems}, journal = {COMMUNICATIONS IN MATHEMATICS AND STATISTICS}, volume = {5}, pages = {1-11}, year = {2017}, issn = {2194-6701}, doi = {10.1007/s40304-017-0103-z}, author = {Weinan, E.}, abstract = {We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.} }
@article{WOS:000454710400002, title = {Deep learning-Using machine learning to study biological vision}, journal = {JOURNAL OF VISION}, volume = {18}, year = {2018}, issn = {1534-7362}, doi = {10.1167/18.13.2}, author = {Majaj, Najib J. and Pelli, Denis G.}, abstract = {Many vision science studies employ machine learning, especially the version called ``deep learning.'' Neuroscientists use machine learning to decode neural responses. Perception scientists try to understand how living organisms recognize objects. To them, deep neural networks offer benchmark accuracies for recognition of learned stimuli. Originally machine learning was inspired by the brain. Today, machine learning is used as a statistical tool to decode brain activity. Tomorrow, deep neural networks might become our best model of brain function. This brief overview of the use of machine learning in biological vision touches on its strengths, weaknesses, milestones, controversies, and current directions. Here, we hope to help vision scientists assess what role machine learning should play in their research.} }
@article{WOS:000348105800001, title = {Machine learning for neuroirnaging with scikit-learn}, journal = {FRONTIERS IN NEUROINFORMATICS}, volume = {8}, year = {2014}, doi = {10.3389/fninf.2014.00014}, author = {Abraham, Alexandre and Pedregosa, Fabian and Eickenberg, Michael and Gervais, Philippe and Mueller, Andreas and Kossaifi, Jean and Gramfort, Alexandre and Thirion, Bertrand and Varoquaux, Gael}, abstract = {Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.} }
@article{WOS:000731150400024, title = {Deep learning and machine vision for food processing: A survey}, journal = {CURRENT RESEARCH IN FOOD SCIENCE}, volume = {4}, pages = {233-249}, year = {2021}, doi = {10.1016/j.crfs.2021.03.009}, author = {Zhu, Lili and Spachos, Petros and Pensini, Erica and Plataniotis, Konstantinos N.}, abstract = {The quality and safety of food is an important issue to the whole society, since it is at the basis of human health, social development and stability. Ensuring food quality and safety is a complex process, and all stages of food processing must be considered, from cultivating, harvesting and storage to preparation and consumption. However, these processes are often labour-intensive. Nowadays, the development of machine vision can greatly assist researchers and industries in improving the efficiency of food processing. As a result, machine vision has been widely used in all aspects of food processing. At the same time, image processing is an important component of machine vision. Image processing can take advantage of machine learning and deep learning models to effectively identify the type and quality of food. Subsequently, follow-up design in the machine vision system can address tasks such as food grading, detecting locations of defective spots or foreign objects, and removing impurities. In this paper, we provide an overview on the traditional machine learning and deep learning methods, as well as the machine vision techniques that can be applied to the field of food processing. We present the current approaches and challenges, and the future trends.} }
@article{WOS:000397585800014, title = {Implementing Machine Learning in Radiology Practice and Research}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {208}, pages = {754-760}, year = {2017}, issn = {0361-803X}, doi = {10.2214/AJR.16.17224}, author = {Kohli, Marc and Prevedello, Luciano M. and Filice, Ross W. and Geis, J. Raymond}, abstract = {OBJECTIVE. The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk. CONCLUSION. Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement.} }
@incollection{WOS:000514284700008, title = {Machine Learning in Neural Networks}, booktitle = {FRONTIERS IN PSYCHIATRY: ARTIFCIAL INTELLIGENCE, PRECISION MEDICINE, AND OTHER PARADIGM SHIFTS}, volume = {1192}, pages = {127-137}, year = {2019}, issn = {0065-2598}, isbn = {978-981-32-9721-0; 978-981-32-9720-3}, doi = {10.1007/978-981-32-9721-0\_7}, author = {Lin, Eugene and Tsai, Shih-Jen}, abstract = {Evidence now suggests that precision psychiatry is becoming a cornerstone of medical practices by providing the patient of psychiatric disorders with the right medication at the right dose at the right time. In light of recent advances in neuroimaging and multi-omics, more and more biomarkers associated with psychiatric diseases and treatment responses are being discovered in precision psychiatry applications by leveraging machine learning and neural network approaches. In this article, we focus on the most recent developments for research in precision psychiatry using machine learning, deep learning, and neural network algorithms, together with neuroimaging and multi-omics data. First, we describe different machine learning approaches that are employed to assess prediction for diagnosis, prognosis, and treatment in various precision psychiatry studies. We also survey probable biomarkers that have been identified to be involved in psychiatric diseases and treatment responses. Furthermore, we summarize the limitations with respect to the mentioned precision psychiatry studies. Finally, we address a discussion of future directions and challenges.} }
@article{WOS:000439628500001, title = {Introduction to the Special Issue on Human-Centered Machine Learning}, journal = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS}, volume = {8}, year = {2018}, issn = {2160-6455}, doi = {10.1145/3205942}, author = {Fiebrink, Rebecca and Gillies, Marco}, abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.} }
@article{WOS:000793273600005, title = {Crack fault diagnosis of rotating machine in nuclear power plant based on ensemble learning}, journal = {ANNALS OF NUCLEAR ENERGY}, volume = {168}, year = {2022}, issn = {0306-4549}, doi = {10.1016/j.anucene.2021.108909}, author = {Zhong, Xianping and Ban, Heng}, abstract = {Crack faults in rotating machines can cause machine shutdown or scrapping, endangering the normal operation and safety of nuclear power plants. Intelligent diagnostic techniques based on machine learn-ing have the potential to diagnose crack faults. However, problems such as scarcity of field fault data and high noise of plant measurements pose challenges to the application of machine learning. This study pro -poses an ensemble learning approach to mitigate the negative impacts of the problems. Ensemble learn-ing is a strategy for combining multiple machine learning models into a composite model. The basic idea of ensemble learning is that even if one model makes a mistake, other models can correct it. Case studies based on bearing and gear system fault experiments show that the proposed ensemble learning models have better diagnostic results than the single model in the presence of noise and small data. (c) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000383849400001, title = {Quantum-Enhanced Machine Learning}, journal = {PHYSICAL REVIEW LETTERS}, volume = {117}, year = {2016}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.117.130501}, author = {Dunjko, Vedran and Taylor, Jacob M. and Briegel, Hans J.}, abstract = {The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.} }
@article{WOS:000353722300005, title = {An introduction to quantum machine learning}, journal = {CONTEMPORARY PHYSICS}, volume = {56}, pages = {172-185}, year = {2015}, issn = {0010-7514}, doi = {10.1080/00107514.2014.964942}, author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco}, abstract = {Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.} }
@article{WOS:000447430700018, title = {A review of automatic selection methods for machine learning algorithms and hyper-parameter values}, journal = {NETWORK MODELING AND ANALYSIS IN HEALTH INFORMATICS AND BIOINFORMATICS}, volume = {5}, year = {2016}, issn = {2192-6662}, doi = {10.1007/s13721-016-0125-6}, author = {Luo, Gang}, abstract = {Machine learning studies automatic algorithms that improve themselves through experience. It is widely used for analyzing and extracting value from large biomedical data sets, or ``big biomedical data,'' advancing biomedical research, and improving healthcare. Before a machine learning model is trained, the user of a machine learning software tool typically must manually select a machine learning algorithm and set one or more model parameters termed hyper-parameters. The algorithm and hyper-parameter values used can greatly impact the resulting model's performance, but their selection requires special expertise as well as many labor-intensive manual iterations. To make machine learning accessible to layman users with limited computing expertise, computer science researchers have proposed various automatic selection methods for algorithms and/or hyper-parameter values for a given supervised machine learning problem. This paper reviews these methods, identifies several of their limitations in the big biomedical data environment, and provides preliminary thoughts on how to address these limitations. These findings establish a foundation for future research on automatically selecting algorithms and hyper-parameter values for analyzing big biomedical data.} }
@article{WOS:000413244600057, title = {Voltage Stability Prediction Using Active Machine Learning}, journal = {IEEE TRANSACTIONS ON SMART GRID}, volume = {8}, pages = {3117-3124}, year = {2017}, issn = {1949-3053}, doi = {10.1109/TSG.2017.2693394}, author = {Malbasa, Vuk and Zheng, Ce and Chen, Po-Chen and Popovic, Tomo and Kezunovic, Mladen}, abstract = {An active machine learning technique for monitoring the voltage stability in transmission systems is presented. It has been shown that machine learning algorithms may be used to supplement the traditional simulation approach, but they suffer from the difficulties of online machine learning model update and offline training data preparation. We propose an active learning solution to enhance existing machine learning applications by actively interacting with the online prediction and offline training process. The technique identifies operating points where machine learning predictions based on power system measurements contradict with actual system conditions. By creating the training set around the identified operating points, it is possible to improve the capability of machine learning tools to predict future power system states. The technique also accelerates the offline training process by reducing the amount of simulations on a detailed power system model around operating points where correct predictions are made. Experiments show a significant advantage in relation to the training time, prediction time, and number of measurements that need to be queried to achieve high prediction accuracy.} }
@article{WOS:000382418300017, title = {Machine learning approaches in medical image analysis: From detection to diagnosis}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {33}, pages = {94-97}, year = {2016}, issn = {1361-8415}, doi = {10.1016/j.media.2016.06.032}, author = {de Bruijne, Marleen}, abstract = {Machine learning approaches are increasingly successful in image-based diagnosis, disease prognosis, and risk assessment. This paper highlights new research directions and discusses three main challenges related to machine learning in medical imaging: coping with variation in imaging protocols, learning from weak labels, and interpretation and evaluation of results. (C) 2016 Elsevier B.V. All rights reserved.} }
@article{WOS:000213214100001, title = {Supervised Machine Learning: A Review of Classification Techniques}, journal = {INFORMATICA-JOURNAL OF COMPUTING AND INFORMATICS}, volume = {31}, pages = {249-268}, year = {2007}, author = {Kotsiantis, S. B.}, abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.} }
@article{WOS:000388629300030, title = {Machine Learning-Based Antenna Selection in Wireless Communications}, journal = {IEEE COMMUNICATIONS LETTERS}, volume = {20}, pages = {2241-2244}, year = {2016}, issn = {1089-7798}, doi = {10.1109/LCOMM.2016.2594776}, author = {Joung, Jingon}, abstract = {This letter is the first attempt to conflate a machine learning technique with wireless communications. Through interpreting the antenna selection (AS) in wireless communications (i.e., an optimization-driven decision) to multiclass-classification learning (i.e., data-driven prediction), and through comparing the learning-based AS using k-nearest neighbors and support vector machine algorithms with conventional optimization-driven AS methods in terms of communications performance, computational complexity, and feedback overhead, we provide insight into the potential of fusion of machine learning and wireless communications.} }
@article{WOS:000412192800056, title = {The ALAMO approach to machine learning}, journal = {COMPUTERS \\& CHEMICAL ENGINEERING}, volume = {106}, pages = {785-795}, year = {2017}, issn = {0098-1354}, doi = {10.1016/j.compchemeng.2017.02.010}, author = {Wilson, Zachary T. and Sahinidis, Nikolaos V.}, abstract = {ALAMO is a computational methodology for learning algebraic functions from data. Given a data set, the approach begins by building a low-complexity, linear model composed of explicit non-linear transformations of the independent variables. Linear combinations of these non-linear transformations allow a linear model to better approximate complex behavior observed in real processes. The model is refined, as additional data are obtained in an adaptive fashion through error maximization sampling using derivative-free optimization. Models built using ALAMO can enforce constraints on the response variables to incorporate first-principles knowledge. The ability of ALAMO to generate simple and accurate models for a number of reaction problems is demonstrated. The error maximization sampling is compared with Latin hypercube designs to demonstrate its sampling efficiency. ALAMO's constrained regression methodology is used to further refine concentration models, resulting in models that perform better on validation data and satisfy upper and lower bounds placed on model outputs. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000412881200013, title = {Comparison of Machine Learning Techniques for Fetal Heart Rate Classification}, journal = {ACTA PHYSICA POLONICA A}, volume = {132}, pages = {451-454}, year = {2017}, issn = {0587-4246}, doi = {10.12693/APhysPolA.132.451}, author = {Comert, Z. and Kocamaz, A. F.}, abstract = {Cardiotocography is a monitoring technique providing important and vital information on fetal status during antepartum and intrapartum periods. The advances in modern obstetric practice allowed many robust and reliable machine learning techniques to be utilized in classifying fetal heart rate signals. The role of machine learning approaches in diagnosing diseases is becoming increasingly essential and intertwined. The main aim of the present study is to determine the most efficient machine learning technique to classify fetal heart rate signals. Therefore, the research has been focused on the widely used and practical machine learning techniques, such as artificial neural network, support vector machine, extreme learning machine, radial basis function network, and random forest. In a comparative way, fetal heart rate signals were classified as normal or hypoxic using the aforementioned machine learning techniques. The performance metrics derived from confusion matrix were used to measure classifiers' success. According to experimental results, although all machine learning techniques produced satisfactory results, artificial neural network yielded the rather well results with the sensitivity of 99.73\\% and specificity of 97.94\\%. The study results show that the artificial neural network was superior to other algorithms.} }
@article{WOS:000489304600003, title = {Fast and simple dataset selection for machine learning}, journal = {AT-AUTOMATISIERUNGSTECHNIK}, volume = {67}, pages = {833-842}, year = {2019}, issn = {0178-2312}, doi = {10.1515/auto-2019-0010}, author = {Peter, Timm J. and Nelles, Oliver}, abstract = {The task of data reduction is discussed and a novel selection approach which allows to control the optimal point distribution of the selected data subset is proposed. The proposed approach utilizes the estimation of probability density functions (pdfs). Due to its structure, the new method is capable of selecting a subset either by approximating the pdf of the original dataset or by approximating an arbitrary, desired target pdf. The new strategy evaluates the estimated pdfs solely on the selected data points, resulting in a simple and efficient algorithm with low computational and memory demand. The performance of the new approach is investigated for two different scenarios. For representative subset selection of a dataset, the new approach is compared to a recently proposed, more complex method and shows comparable results. For the demonstration of the capability of matching a target pdf, a uniform distribution is chosen as an example. Here the new method is compared to strategies for space-filling design of experiments and shows convincing results.} }
@article{WOS:000383095900019, title = {Machine Learning}, journal = {IEEE SOFTWARE}, volume = {33}, pages = {110-115}, year = {2016}, issn = {0740-7459}, doi = {10.1109/MS.2016.114}, author = {Louridas, Panos and Ebert, Christof}, abstract = {Machine learning is the major success factor in the ongoing digital transformation across industries. Startups and behemoths alike announce new products that will learn to perform tasks that previously only humans could do, and perform those tasks better, faster, and more intelligently. But how do they do it? What does it mean for IT developers and software engineers? Here, Panos Louridas and I present a brief overview of machine-learning technologies, with a concrete case study from code analysis. I look forward to hearing from both readers and prospective column authors.} }
@article{WOS:000371639700001, title = {Prototype-based models in machine learning}, journal = {WILEY INTERDISCIPLINARY REVIEWS-COGNITIVE SCIENCE}, volume = {7}, pages = {92-111}, year = {2016}, issn = {1939-5078}, doi = {10.1002/wcs.1378}, author = {Biehl, Michael and Hammer, Barbara and Villmann, Thomas}, abstract = {An overview is given of prototype-based models in machine learning. In this framework, observations, i.e., data, are stored in terms of typical representatives. Together with a suitable measure of similarity, the systems can be employed in the context of unsupervised and supervised analysis of potentially high-dimensional, complex datasets. We discuss basic schemes of competitive vector quantization as well as the so-called neural gas approach and Kohonen's topology-preserving self-organizing map. Supervised learning in prototype systems is exemplified in terms of learning vector quantization. Most frequently, the familiar Euclidean distance serves as a dissimilarity measure. We present extensions of the framework to nonstandard measures and give an introduction to the use of adaptive distances in relevance learning. (C) 2016 Wiley Periodicals, Inc.} }
@article{WOS:000393223300013, title = {A Review of Deep Machine Learning}, journal = {INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH IN AFRICA}, volume = {24}, pages = {124-136}, year = {2016}, issn = {1663-3571}, doi = {10.4028/www.scientific.net/JERA.24.124}, author = {Benuwa, Ben-Bright and Zhan, Yongzhao and Ghansah, Benjamin and Wornyo, Dickson Keddy and Kataka, Frank Banaseka}, abstract = {The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. Recently, Deep Learning (a surrogate of Machine Learning) have won several contests in pattern recognition and machine learning. This review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. This paper also discusses the motivations and principles regarding learning algorithms for deep architectures.} }
@article{WOS:000637534200002, title = {A Survey on Learning-Based Approaches for Modeling and Classification of Human-Machine Dialog Systems}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS}, volume = {32}, pages = {1418-1432}, year = {2021}, issn = {2162-237X}, doi = {10.1109/TNNLS.2020.2985588}, author = {Cui, Fuwei and Cui, Qian and Song, Yongduan}, abstract = {With the rapid development from traditional machine learning (ML) to deep learning (DL) and reinforcement learning (RL), dialog system equipped with learning mechanism has become the most effective solution to address human-machine interaction problems. The purpose of this article is to provide a comprehensive survey on learning-based human-machine dialog systems with a focus on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and also compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and evaluation metrics of the dialog model. Furthermore, the evaluation metrics of these dialog models are analyzed in detail. Finally, we briefly analyze the existing issues and point out the potential future direction on the human-machine dialog systems.} }
@article{WOS:000556008200001, title = {Machine Learning in Big Data}, journal = {INTERNATIONAL JOURNAL OF MATHEMATICAL ENGINEERING AND MANAGEMENT SCIENCES}, volume = {1}, pages = {52-61}, year = {2016}, issn = {2455-7749}, doi = {10.33889/IJMEMS.2016.1.2-006}, author = {Wang, Lidong and Alexander, Cheryl Ann}, abstract = {Machine learning is an artificial intelligence method of discovering knowledge for making intelligent decisions. Big Data has great impacts on scientific discoveries and value creation. This paper introduces methods in machine learning, main technologies in Big Data, and some applications of machine learning in Big Data. Challenges of machine learning applications in Big Data are discussed. Some new methods and technology progress of machine learning in Big Data are also presented.} }
@article{WOS:001136314900001, title = {Machine-learned wearable sensors for real-time hand-motion recognition: toward practical applications}, journal = {NATIONAL SCIENCE REVIEW}, volume = {11}, year = {2024}, issn = {2095-5138}, doi = {10.1093/nsr/nwad298}, author = {Pyun, Kyung Rok and Kwon, Kangkyu and Yoo, Myung Jin and Kim, Kyun Kyu and Gong, Dohyeon and Yeo, Woon-Hong and Han, Seungyong and Ko, Seung Hwan}, abstract = {Soft electromechanical sensors have led to a new paradigm of electronic devices for novel motion-based wearable applications in our daily lives. However, the vast amount of random and unidentified signals generated by complex body motions has hindered the precise recognition and practical application of this technology. Recent advancements in artificial-intelligence technology have enabled significant strides in extracting features from massive and intricate data sets, thereby presenting a breakthrough in utilizing wearable sensors for practical applications. Beyond traditional machine-learning techniques for classifying simple gestures, advanced machine-learning algorithms have been developed to handle more complex and nuanced motion-based tasks with restricted training data sets. Machine-learning techniques have improved the ability to perceive, and thus machine-learned wearable soft sensors have enabled accurate and rapid human-gesture recognition, providing real-time feedback to users. This forms a crucial component of future wearable electronics, contributing to a robust human-machine interface. In this review, we provide a comprehensive summary covering materials, structures and machine-learning algorithms for hand-gesture recognition and possible practical applications through machine-learned wearable electromechanical sensors. This review provides a thorough overview of the current research in machine-learned wearable sensors for real-time hand motion recognition, highlighting current challenges and future directions toward practical applications in reality.} }
@article{WOS:000392585400001, title = {Future Directions in Machine Learning}, journal = {FRONTIERS IN ROBOTICS AND AI}, volume = {3}, year = {2017}, issn = {2296-9144}, doi = {10.3389/frobt.2016.00079}, author = {Greenwald, Hal S. and Oertel, Carsten K.}, abstract = {Current machine learning (ML) algorithms identify statistical regularities in complex data sets and are regularly used across a range of application domains, but they lack the robustness and generalizability associated with human learning. If ML techniques could enable computers to learn from fewer examples, transfer knowledge between tasks, and adapt to changing contexts and environments, the results would have very broad scientific and societal impacts. Increased processing and memory resources have enabled larger, more capable learning models, but there is growing recognition that even greater computing resources would not be sufficient to yield algorithms capable of learning from a few examples and generalizing beyond initial training sets. This paper presents perspectives on feature selection, representation schemes and interpretability, transfer learning, continuous learning, and learning and adaptation in time-varying contexts and environments, five key areas that are essential for advancing ML capabilities. Appropriate learning tasks that require these capabilities can demonstrate the strengths of novel ML approaches that could address these challenges.} }
@article{WOS:001217367400004, title = {A survey on imbalanced learning: latest research, applications and future directions}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {57}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-10759-6}, author = {Chen, Wuxing and Yang, Kaixiang and Yu, Zhiwen and Shi, Yifan and Chen, C. L. Philip}, abstract = {Imbalanced learning constitutes one of the most formidable challenges within data mining and machine learning. Despite continuous research advancement over the past decades, learning from data with an imbalanced class distribution remains a compelling research area. Imbalanced class distributions commonly constrain the practical utility of machine learning and even deep learning models in tangible applications. Numerous recent studies have made substantial progress in the field of imbalanced learning, deepening our understanding of its nature while concurrently unearthing new challenges. Given the field's rapid evolution, this paper aims to encapsulate the recent breakthroughs in imbalanced learning by providing an in-depth review of extant strategies to confront this issue. Unlike most surveys that primarily address classification tasks in machine learning, we also delve into techniques addressing regression tasks and facets of deep long-tail learning. Furthermore, we explore real-world applications of imbalanced learning, devising a broad spectrum of research applications from management science to engineering, and lastly, discuss newly-emerging issues and challenges necessitating further exploration in the realm of imbalanced learning.} }
@article{WOS:000346856600043, title = {A systematic review of machine learning techniques for software fault prediction}, journal = {APPLIED SOFT COMPUTING}, volume = {27}, pages = {504-518}, year = {2015}, issn = {1568-4946}, doi = {10.1016/j.asoc.2014.11.023}, author = {Malhotra, Ruchika}, abstract = {Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. (C) 2014 Elsevier B.V. All rights reserved.} }
@article{WOS:000369369100004, title = {Toward a generic representation of random variables for machine learning}, journal = {PATTERN RECOGNITION LETTERS}, volume = {70}, pages = {24-31}, year = {2016}, issn = {0167-8655}, doi = {10.1016/j.patrec.2015.11.004}, author = {Donnat, Philippe and Marti, Gautier and Very, Philippe}, abstract = {This paper presents a pre-processing and a distance which improve the performance of machine learning algorithms working on independent and identically distributed stochastic processes. We introduce a novel non parametric approach to represent random variables which splits apart dependency and distribution without losing any information. We also propound an associated metric leveraging this representation and its statistical estimate. Besides experiments on synthetic datasets, the benefits of our contribution is illustrated through the example of clustering financial time series, for instance prices from the credit default swaps market. Results are available on the website http://www.datagrapple.com and an IPython Notebook tutorial is available at http://www.datagrapple.com/Tech for reproducible research. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000697475900018, title = {Exploration of machine algorithms based on deep learning model and feature extraction}, journal = {MATHEMATICAL BIOSCIENCES AND ENGINEERING}, volume = {18}, pages = {7602-7618}, year = {2021}, issn = {1547-1063}, doi = {10.3934/mbe.2021376}, author = {Qian, Yufeng}, abstract = {The study expects to solve the problems of insufficient labeling, high input dimension, and inconsistent task input distribution in traditional lifelong machine learning. A new deep learning model is proposed by combining feature representation with a deep learning algorithm. First, based on the theoretical basis of the deep learning model and feature extraction. The study analyzes several representative machine learning algorithms, and compares the performance of the optimized deep learning model with other algorithms in a practical application. By explaining the machine learning system, the study introduces two typical algorithms in machine learning, namely ELLA (Efficient lifelong learning algorithm) and HLLA (Hierarchical lifelong learning algorithm). Second, the flow of the genetic algorithm is described, and combined with mutual information feature extraction in a machine algorithm, to form a composite algorithm HLLA (Hierarchical lifelong learning algorithm). Finally, the deep learning model is optimized and a deep learning model based on the HLLA algorithm is constructed. When K = 1200, the classification error rate reaches 0.63\\%, which reflects the excellent performance of the unsupervised database algorithm based on this model. Adding the feature model to the updating iteration process of lifelong learning deepens the knowledge base ability of lifelong machine learning, which is of great value to reduce the number of labels required for subsequent model learning and improve the efficiency of lifelong learning.} }
@article{WOS:000351430600002, title = {Entanglement-Based Machine Learning on a Quantum Computer}, journal = {PHYSICAL REVIEW LETTERS}, volume = {114}, year = {2015}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.114.110504}, author = {Cai, X. -D. and Wu, D. and Su, Z. -E. and Chen, M. -C. and Wang, X. -L. and Li, Li and Liu, N. -L. and Lu, C. -Y. and Pan, J. -W.}, abstract = {Machine learning, a branch of artificial intelligence, learns from previous experience to optimize performance, which is ubiquitous in various fields such as computer sciences, financial analysis, robotics, and bioinformatics. A challenge is that machine learning with the rapidly growing ``big data'' could become intractable for classical computers. Recently, quantum machine learning algorithms [Lloyd, Mohseni, and Rebentrost, arXiv. 1307.0411] were proposed which could offer an exponential speedup over classical algorithms. Here, we report the first experimental entanglement-based classification of two-, four-, and eight-dimensional vectors to different clusters using a small-scale photonic quantum computer, which are then used to implement supervised and unsupervised machine learning. The results demonstrate the working principle of using quantum computers to manipulate and classify high-dimensional vectors, the core mathematical routine in machine learning. The method can, in principle, be scaled to larger numbers of qubits, and may provide a new route to accelerate machine learning.} }
@article{WOS:000256504400007, title = {Kernel methods in machine learning}, journal = {ANNALS OF STATISTICS}, volume = {36}, pages = {1171-1220}, year = {2008}, issn = {0090-5364}, doi = {10.1214/009053607000000677}, author = {Hofmann, Thomas and Schoelkopf, Bernhard and Smola, Alexander J.}, abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.} }
@article{WOS:001361255100001, title = {Self-Supervised Learning for Near-Wild Cognitive Workload Estimation}, journal = {JOURNAL OF MEDICAL SYSTEMS}, volume = {48}, year = {2024}, issn = {0148-5598}, doi = {10.1007/s10916-024-02122-7}, author = {Rafiei, Mohammad H. and Gauthier, Lynne V. and Adeli, Hojjat and Takabi, Daniel}, abstract = {Feedback on cognitive workload may reduce decision-making mistakes. Machine learning-based models can produce feedback from physiological data such as electroencephalography (EEG) and electrocardiography (ECG). Supervised machine learning requires large training data sets that are (1) relevant and decontaminated and (2) carefully labeled for accurate approximation, a costly and tedious procedure. Commercial over-the-counter devices are low-cost resolutions for the real-time collection of physiological modalities. However, they produce significant artifacts when employed outside of laboratory settings, compromising machine learning accuracies. Additionally, the physiological modalities that most successfully machine-approximate cognitive workload in everyday settings are unknown. To address these challenges, a first-ever hybrid implementation of feature selection and self-supervised machine learning techniques is introduced. This model is employed on data collected outside controlled laboratory settings to (1) identify relevant physiological modalities to machine approximate six levels of cognitive-physical workloads from a seven-modality repository and (2) postulate limited labeling experiments and machine approximate mental-physical workloads using self-supervised learning techniques.} }
@article{WOS:000965299600001, title = {A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection}, journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}, volume = {35}, pages = {3347-3366}, year = {2023}, issn = {1041-4347}, doi = {10.1109/TKDE.2021.3124599}, author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng}, abstract = {As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.} }
@article{WOS:001180702200015, title = {Learn to Unlearn: Insights Into Machine Unlearning}, journal = {COMPUTER}, volume = {57}, pages = {79-90}, year = {2024}, issn = {0018-9162}, doi = {10.1109/MC.2023.3333319}, author = {Qu, Youyang and Yuan, Xin and Ding, Ming and Ni, Wei and Rakotoarivelo, Thierry and Smith, David}, abstract = {This article presents a comprehensive review of recent machine unlearning techniques, verification mechanisms, and potential attacks. We highlight emerging challenges and prospective research directions, aiming to provide valuable resources for integrating privacy, equity, and resilience into machine learning systems and help them ``learn to unlearn.''} }
@article{WOS:000394061800034, title = {Generalized extreme learning machine autoencoder and a new deep neural network}, journal = {NEUROCOMPUTING}, volume = {230}, pages = {374-381}, year = {2017}, issn = {0925-2312}, doi = {10.1016/j.neucom.2016.12.027}, author = {Sun, Kai and Zhang, Jiangshe and Zhang, Chunxia and Hu, Junying}, abstract = {Extreme learning machine (ELM) is an efficient learning algorithm of training single layer feed-forward neural networks (SLFNs). With the development of unsupervised learning in recent years, integrating ELM with autoencoder has become a new perspective for extracting feature using unlabeled data. In this paper, we propose a new variant of extreme learning machine autoencoder (ELM-AE) called generalized extreme learning machine autoencoder (GELM-AE) which adds the manifold regularization to the objective of ELM-AE. Some experiments carried out on real-world data sets show that GELM-AE outperforms some state-of-the-art unsupervised learning algorithms, including k-means, laplacian embedding (LE), spectral clustering (SC) and ELM-AE. Furthermore, we also propose a new deep neural network called multilayer generalized extreme learning machine autoencoder (ML-GELM) by stacking several GELM-AE to detect more abstract representations. The experiments results show that ML-GELM outperforms ELM and many other deep models, such as multilayer ELM autoencoder (ML-ELM), deep belief network (DBN) and stacked autoencoder (SAE). Due to the utilization of ELM, ML-GELM is also faster than DBN and SAE.} }
@article{WOS:000398821300014, title = {Unsupervised extreme learning machine with representational features}, journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS}, volume = {8}, pages = {587-595}, year = {2017}, issn = {1868-8071}, doi = {10.1007/s13042-015-0351-8}, author = {Ding, Shifei and Zhang, Nan and Zhang, Jian and Xu, Xinzheng and Shi, Zhongzhi}, abstract = {Extreme learning machine (ELM) is not only an effective classifier but also a useful cluster. Unsupervised extreme learning machine (US-ELM) gives favorable performance compared to state-of-the-art clustering algorithms. Extreme learning machine as an auto encoder (ELM-AE) can obtain principal components which represent original samples. The proposed unsupervised extreme learning machine based on embedded features of ELM-AE (US-EF-ELM) algorithm applies ELM-AE to US-ELM. US-EF-ELM regards embedded features of ELM-AE as the outputs of US-ELM hidden layer, and uses US-ELM to obtain the embedded matrix of US-ELM. US-EF-ELM can handle the multi-cluster clustering. The learning capability and computational efficiency of US-EF-ELM are as same as US-ELM. By experiments on UCI data sets, we compared US-EF-ELM k-means algorithm with k-means algorithm, spectral clustering algorithm, and US-ELM k-means algorithm in accuracy and efficiency.} }
@article{WOS:000425723300001, title = {Fault diagnosis method based on wavelet packet-energy entropy and fuzzy kernel extreme learning machine}, journal = {ADVANCES IN MECHANICAL ENGINEERING}, volume = {10}, year = {2018}, issn = {1687-8140}, doi = {10.1177/1687814017751446}, author = {Ma, Jun and Wu, Jiande and Wang, Xiaodong}, abstract = {Aiming at connatural limitations of extreme learning machine in practice, a new fault diagnosis method based on wavelet packet-energy entropy and fuzzy kernel extreme learning machine is proposed. On one hand, the presented method can extract the more efficient features using the wavelet packet-energy entropy method, and on the other hand, the sample fuzzy membership degree matrix U, weight matrix W which is used to describe the sample imbalance, and the kernel function are introduced to construct the fuzzy kernel extreme learning machine model with high accuracy and reliability. The experimental results of rolling bearing and check valve are obtained and analyzed in MATLAB 2010b. The results show that the proposed fuzzy kernel extreme learning machine method can obtain fairly or slightly better classification performance than the traditional extreme learning machine, kernel extreme learning machine, back propagation, support vector machine, and fuzzy support vector machine.} }
@article{WOS:000480422500020, title = {Towards Achieving Machine Comprehension Using Deep Learning on Non-GPU Machines}, journal = {ENGINEERING TECHNOLOGY \\& APPLIED SCIENCE RESEARCH}, volume = {9}, pages = {4423-4427}, year = {2019}, issn = {2241-4487}, author = {Khan, Uzair and Khan, Khalid and Hasssan, Fadzil and Siddiqui, Anam and Afaq, Muhammad}, abstract = {Long efforts have been made to enable machines to understand human language. Nowadays such activities fall under the broad umbrella of machine comprehension. The results are optimistic due to the recent advancements in the field of machine learning. Deep learning promises to bring even better results but requires expensive and resource hungry hardware. In this paper, we demonstrate the use of deep learning in the context of machine comprehension by using non-GPU machines. Our results suggest that the good algorithm insight and detailed understanding of the dataset can help in getting meaningful results through deep learning even on non-GPU machines.} }
@article{WOS:000861328200001, title = {A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects}, journal = {IEEE ACCESS}, volume = {10}, pages = {99129-99149}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3207287}, author = {Mienye, Ibomoiye Domor and Sun, Yanxia}, abstract = {Ensemble learning techniques have achieved state-of-the-art performance in diverse machine learning applications by combining the predictions from two or more base models. This paper presents a concise overview of ensemble learning, covering the three main ensemble methods: bagging, boosting, and stacking, their early development to the recent state-of-the-art algorithms. The study focuses on the widely used ensemble algorithms, including random forest, adaptive boosting (AdaBoost), gradient boosting, extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), and categorical boosting (CatBoost). An attempt is made to concisely cover their mathematical and algorithmic representations, which is lacking in the existing literature and would be beneficial to machine learning researchers and practitioners.} }
@article{WOS:000312959700016, title = {Model-based machine learning}, journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES}, volume = {371}, year = {2013}, issn = {1364-503X}, doi = {10.1098/rsta.2012.0222}, author = {Bishop, Christopher M.}, abstract = {Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.} }
@article{WOS:000401624200005, title = {Unsupervised and semi-supervised extreme learning machine with wavelet kernel for high dimensional data}, journal = {MEMETIC COMPUTING}, volume = {9}, pages = {129-139}, year = {2017}, issn = {1865-9284}, doi = {10.1007/s12293-016-0198-x}, author = {Zhang, Nan and Ding, Shifei}, abstract = {Extreme learning machine (ELM) not only is an effective classifier in supervised learning, but also can be applied on unsupervised learning and semi-supervised learning. The model structure of unsupervised extreme learning machine (US-ELM) and semi-supervised extreme learning machine (SS-ELM) are same as ELM, the difference between them is the cost function. We introduce kernel function to US-ELM and propose unsupervised extreme learning machine with kernel (US-KELM). And SS-KELM has been proposed. Wavelet analysis has the characteristics of multivariate interpolation and sparse change, and Wavelet kernel functions have been widely used in support vector machine. Therefore, to realize a combination of the wavelet kernel function, US-ELM, and SS-ELM, unsupervised extreme learning machine with wavelet kernel function (US-WKELM) and semi-supervised extreme learning machine with wavelet kernel function (SS-WKELM) are proposed in this paper. The experimental results show the feasibility and validity of US-WKELM and SS-WKELM in clustering and classification.} }
@article{WOS:000620462600001, title = {A survey on federated learning}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {216}, year = {2021}, issn = {0950-7051}, doi = {10.1016/j.knosys.2021.106775}, author = {Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan}, abstract = {Federated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning. (C) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000435287000002, title = {Ensemble learning: A survey}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {8}, year = {2018}, issn = {1942-4787}, doi = {10.1002/widm.1249}, author = {Sagi, Omer and Rokach, Lior}, abstract = {Ensemble methods are considered the state-of-the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state-of-the-art ensemble methods and discusses current challenges and trends in the field. This article is categorized under: Algorithmic Development > Model Combining Technologies > Machine Learning Technologies > Classification} }
@article{WOS:000327394500015, title = {Matched-Pair Machine Learning}, journal = {TECHNOMETRICS}, volume = {55}, pages = {536-547}, year = {2013}, issn = {0040-1706}, doi = {10.1080/00401706.2013.838191}, author = {Theiler, James}, abstract = {Following an analogous distinction in statistical hypothesis testing and motivated by chemical plume detection in hyperspectral imagery, we investigate machine-learning algorithms where the training set is comprised of matched pairs. We find that even conventional classifiers exhibit improved performance when the input data have a matched-pair structure, and we develop an example of a ``dipole'' algorithm to directly exploit this structured input. In some scenarios, matched pairs can be generated from independent samples, with the effect of not only doubling the nominal size of the training set, but of providing the matched-pair structure that leads to better learning. The creation of matched pairs from a dataset of interest also permits a kind of transductive learning, which is found for the plume detection problem to exhibit improved performance. Supplementary materials for this article are available online.} }
@article{WOS:000209646300007, title = {PARADIGMS FOR REALIZING MACHINE LEARNING ALGORITHMS}, journal = {BIG DATA}, volume = {1}, pages = {BD207-BD214}, year = {2013}, issn = {2167-6461}, doi = {10.1089/big.2013.0006}, author = {Agneeswaran, Vijay Srinivas and Tonpay, Pranay and Tiwary, Jayati}, abstract = {The article explains the three generations of machine learning algorithms-with all three trying to operate on big data. The first generation tools are SAS, SPSS, etc., while second generation realizations include Mahout and RapidMiner (that work over Hadoop), and the third generation paradigms include Spark and GraphLab, among others. The essence of the article is that for a number of machine learning algorithms, it is important to look beyond the Hadoop's Map- Reduce paradigm in order to make them work on big data. A number of promising contenders have emerged in the third generation that can be exploited to realize deep analytics on big data.} }
@article{WOS:001068816800051, title = {Recent Advances for Quantum Neural Networks in Generative Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {45}, pages = {12321-12340}, year = {2023}, issn = {0162-8828}, doi = {10.1109/TPAMI.2023.3272029}, author = {Tian, Jinkai and Sun, Xiaoyu and Du, Yuxuan and Zhao, Shanshan and Liu, Qing and Zhang, Kaining and Yi, Wei and Huang, Wanrong and Wang, Chaoyue and Wu, Xingyao and Hsieh, Min-Hsiu and Liu, Tongliang and Yang, Wenjing and Tao, Dacheng}, abstract = {Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum variational autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relations and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.} }
@article{WOS:000645896700002, title = {Toward Causal Representation Learning}, journal = {PROCEEDINGS OF THE IEEE}, volume = {109}, pages = {612-634}, year = {2021}, issn = {0018-9219}, doi = {10.1109/JPROC.2021.3058954}, author = {Schoelkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua}, abstract = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.} }
@article{WOS:000660868300001, title = {The MLIP package: moment tensor potentials with MPI and active learning}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {2}, year = {2021}, doi = {10.1088/2632-2153/abc9fe}, author = {Novikov, Ivan S. and Gubaev, Konstantin and Podryabinkin, V, Evgeny and Shapeev, V, Alexander}, abstract = {The subject of this paper is the technology (the `how') of constructing machine-learning interatomic potentials, rather than science (the `what' and `why') of atomistic simulations using machine-learning potentials. Namely, we illustrate how to construct moment tensor potentials using active learning as implemented in the MLIP package, focusing on the efficient ways to automatically sample configurations for the training set, how expanding the training set changes the error of predictions, how to set up ab initio calculations in a cost-effective manner, etc. The MLIP package (short for Machine-Learning Interatomic Potentials) is available at https://mlip.skoltech.ru/download/.} }
@article{WOS:000314529000003, title = {DARWIN: A Framework for Machine Learning and Computer Vision Research and Development}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {13}, pages = {3533-3537}, year = {2012}, issn = {1532-4435}, author = {Gould, Stephen}, abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.} }
@article{WOS:000518368000010, title = {Deep learning and artificial intelligence methods for Raman and surface-enhanced Raman scattering}, journal = {TRAC-TRENDS IN ANALYTICAL CHEMISTRY}, volume = {124}, year = {2020}, issn = {0165-9936}, doi = {10.1016/j.trac.2019.115796}, author = {Lussier, Felix and Thibault, Vincent and Charron, Benjamin and Wallace, Gregory Q. and Masson, Jean-Francois}, abstract = {Machine learning is shaping up our lives in many ways. In analytical sciences, machine learning provides an unprecedented opportunity to extract information from complex or big datasets in chromatography, mass spectrometry, NMR, and spectroscopy, among others. This is especially the case in Raman and surface-enhanced Raman scattering (SERS) techniques where vibrational spectra of complex chemical mixtures are acquired as large datasets for the analysis or imaging of chemical systems. The classical linear methods of processing the information no longer suffice and thus machine learning methods for extracting the chemical information from Raman and SERS experiments have been implemented recently. In this review, we will provide a brief overview of the most common machine learning techniques employed in Raman, a guideline for new users to implement machine learning in their data analysis process, and an overview of modern applications of machine learning in Raman and SERS. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000302331900002, title = {Challenges and Opportunities in Applied Machine Learning}, journal = {AI MAGAZINE}, volume = {33}, pages = {11-24}, year = {2012}, issn = {0738-4602}, doi = {10.1609/aimag.v33i1.2367}, author = {Brodley, Carla E. and Rebbapragada, Umaa and Small, Kevin and Wallace, Byron C.}, abstract = {Machine-learning research is often conducted in vitro, divorced from motivating practical applications. A researcher might develop a new method for the general task of classification, then assess its utility by comparing its performance (for example, accuracy or AUC) to that of existing classification models on publicly available data sets. In terms of advancing machine learning as an academic discipline, this approach has thus far proven quite fruitful. However, it is our view that the most interesting open problems in machine learning are those that arise during its application to real-world problems. We illustrate this point by reviewing two of our interdisciplinary collaborations, both of which have posed unique machine-learning problems, providing fertile ground for novel research.} }
@article{WOS:000647800900004, title = {A deep learning based life prediction method for components under creep, fatigue and creep-fatigue conditions}, journal = {INTERNATIONAL JOURNAL OF FATIGUE}, volume = {148}, year = {2021}, issn = {0142-1123}, doi = {10.1016/j.ijfatigue.2021.106236}, author = {Zhang, Xiao-Cheng and Gong, Jian-Guo and Xuan, Fu-Zhen}, abstract = {Deep learning is a particular kind of machine learning, which achieves great power and flexibility by a nested hierarchy of concepts. A general life prediction method for components under creep, fatigue and creep-fatigue conditions is proposed. Fatigue, creep and creep-fatigue data of a typical austenitic stainless steel (i.e., 316) are integrated. Conventional machine learning models (e.g., support vector machine, random forest, Gaussian process regression, shallow neural network) and deep learning model (e.g., deep neural network) are applied for life predictions. Results show that deep learning model exhibits better prediction accuracy and generalization ability than conventional machine learning model.} }
@article{WOS:000252222600001, title = {Machine learning: a review of classification and combining techniques}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {26}, pages = {159-190}, year = {2006}, issn = {0269-2821}, doi = {10.1007/s10462-007-9052-3}, author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.}, abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy-ensembles of classifiers.} }
@article{WOS:000283941100019, title = {Learning Machine Learning: A Case Study}, journal = {IEEE TRANSACTIONS ON EDUCATION}, volume = {53}, pages = {672-676}, year = {2010}, issn = {0018-9359}, doi = {10.1109/TE.2009.2038992}, author = {Lavesson, Niklas}, abstract = {This correspondence reports on a case study conducted in the Master's-level Machine Learning (ML) course at Blekinge Institute of Technology, Sweden. The students participated in a self-assessment test and a diagnostic test of prerequisite subjects, and their results on these tests are correlated with their achievement of the course's learning objectives.} }
@article{WOS:000474499500001, title = {Deep learning and its application in geochemical mapping}, journal = {EARTH-SCIENCE REVIEWS}, volume = {192}, pages = {1-14}, year = {2019}, issn = {0012-8252}, doi = {10.1016/j.earscirev.2019.02.023}, author = {Zuo, Renguang and Xiong, Yihui and Wang, Jian and Carranza, Emmanuel John M.}, abstract = {Machine learning algorithms have been applied widely in the fields of natural science, social science and engineering. It can be expected that machine learning approaches especially deep learning algorithms will help geoscientists to discover mineral deposits through processing of various geoscience datasets. This study reviews the state-of-the-art application of deep learning algorithms for processing geochemical exploration data and mining the geochemical patterns. Deep learning algorithms can deal with complex and nonlinear problems and, therefore, can enhance the identification of geochemical anomalies and the recognition of hidden patterns. Applied geochemistry needs more applications of machine learning and/or deep learning algorithms.} }
@article{WOS:000267732400003, title = {Interacting meaningfully with machine learning systems: Three experiments}, journal = {INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES}, volume = {67}, pages = {639-662}, year = {2009}, issn = {1071-5819}, doi = {10.1016/j.ijhcs.2009.03.004}, author = {Stumpf, Simone and Rajaram, Vidya and Li, Lida and Wong, Weng-Keen and Burnett, Margaret and Dietterich, Thomas and Sullivan, Erin and Herlocker, Jonathan}, abstract = {Although machine learning is becoming commonly used in today's software, there has been little research into how end users might interact with machine learning systems, beyond communicating simple ``right/wrong'' judgments. If the users themselves could work hand-in-hand with machine learning systems, the users' understanding and trust of the system could improve and the accuracy of learning systems could be improved as well. We conducted three experiments to understand the potential for rich interactions between users and machine learning systems. The first experiment was a think-aloud study that investigated users' willingness to interact with machine learning reasoning, and what kinds of feedback users might give to machine learning systems. We then investigated the viability of introducing such feedback into machine learning systems, specifically, how to incorporate some of these types of user feedback into machine learning systems, and what their impact was on the accuracy of the system. Taken together, the results of our experiments show that supporting rich interactions between users and machine learning systems is feasible for both user and machine. This shows the potential of rich human-computer collaboration via on-the-spot interactions as a promising direction for machine learning systems and users to collaboratively share intelligence. Published by Elsevier Ltd.} }
@article{WOS:000258760000002, title = {Structured machine learning: the next ten years}, journal = {MACHINE LEARNING}, volume = {73}, pages = {3-23}, year = {2008}, issn = {0885-6125}, doi = {10.1007/s10994-008-5079-1}, author = {Dietterich, Thomas G. and Domingos, Pedro and Getoor, Lise and Muggleton, Stephen and Tadepalli, Prasad}, abstract = {The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.} }
@incollection{WOS:000448519100007, title = {Deep Learning and Its Application to LHC Physics}, booktitle = {ANNUAL REVIEW OF NUCLEAR AND PARTICLE SCIENCE, VOL 68}, volume = {68}, pages = {161-181}, year = {2018}, issn = {0163-8998}, isbn = {978-0-8243-1568-9}, doi = {10.1146/annurev-nucl-101917-021019}, author = {Guest, Dan and Cranmer, Kyle and Whiteson, Daniel}, abstract = {Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis arc explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LIIC data, and discussion of future prospects and concerns.} }
@article{WOS:000241792300002, title = {Recent advances in predictive (machine) learning}, journal = {JOURNAL OF CLASSIFICATION}, volume = {23}, pages = {175-197}, year = {2006}, issn = {0176-4268}, doi = {10.1007/s00357-006-0012-4}, author = {Friedman, Jerome H.}, abstract = {Prediction involves estimating the unknown value of an attribute of a system under study given the values of other measured attributes. In prediction (machine) learning the prediction rule is derived from data consisting of previously solved cases. Most methods for predictive learning were originated many years ago at the dawn of the computer age. Recently two new techniques have emerged that have revitalized the field. These are support vector machines and boosted decision trees. This paper provides an introduction to these two new methods tracing their respective ancestral roots to standard kernel methods and ordinary decision trees.} }
@article{WOS:000245388800005, title = {The interplay of optimization and machine learning research}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {7}, pages = {1265-1281}, year = {2006}, issn = {1532-4435}, author = {Bennett, Kristin P. and Parrado-Hernandez, Emilio}, abstract = {The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.} }
@article{WOS:000231025700002, title = {Machine-learning techniques and their applications in manufacturing}, journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF ENGINEERING MANUFACTURE}, volume = {219}, pages = {395-412}, year = {2005}, issn = {0954-4054}, doi = {10.1243/095440505X32274}, author = {Pham, D. T. and Afify, A. A.}, abstract = {Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.} }
@article{WOS:000629093200003, title = {Predictive Modeling for Machining Power Based on Multi-source Transfer Learning in Metal Cutting}, journal = {INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING-GREEN TECHNOLOGY}, volume = {9}, pages = {107-125}, year = {2022}, issn = {2288-6206}, doi = {10.1007/s40684-021-00327-6}, author = {Kim, Young-Min and Shin, Seung-Jun and Cho, Hae-Won}, abstract = {Energy efficiency has become crucial in the metal cutting industry. Machining power has therefore become an important metric because it directly affects the energy consumed during the operation of a machine tool. Attempts to predict machining power using machine learning have relied on the training datasets processed from actual machining data to derive the numerical relationship between process parameters and machining power. However, real fields hardly provide training datasets because of the difficulties in data collection; consequently, traditional learning approaches are ineffective in such data-scarce or -absent environment. This paper proposes a transfer learning approach for the predictive modeling of machining power. The proposed approach creates machining power prediction models by transferring the knowledge acquired from prior machining to the target machining context where machining power data are absent. The proposed approach performs domain adaptation by adding workpiece material properties to the original feature space for accommodating different machining power patterns dependent on the types of workpiece materials. A case study demonstrates that the training datasets obtained from the fabrication of steel and aluminum materials can be successfully used to create the power-predictive models that anticipate machining power for titanium material.} }
@article{WOS:000341593600009, title = {An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels}, journal = {COGNITIVE COMPUTATION}, volume = {6}, pages = {376-390}, year = {2014}, issn = {1866-9956}, doi = {10.1007/s12559-014-9255-2}, author = {Huang, Guang-Bin}, abstract = {Extreme learning machines (ELMs) basically give answers to two fundamental learning problems: (1) Can fundamentals of learning (i.e., feature learning, clustering, regression and classification) be made without tuning hidden neurons (including biological neurons) even when the output shapes and function modeling of these neurons are unknown? (2) Does there exist unified framework for feedforward neural networks and feature space methods? ELMs that have built some tangible links between machine learning techniques and biological learning mechanisms have recently attracted increasing attention of researchers in widespread research areas. This paper provides an insight into ELMs in three aspects, viz: random neurons, random features and kernels. This paper also shows that in theory ELMs (with the same kernels) tend to outperform support vector machine and its variants in both regression and classification applications with much easier implementation.} }
@article{WOS:000697350100008, title = {Privacy attacks against deep learning models and their countermeasures}, journal = {JOURNAL OF SYSTEMS ARCHITECTURE}, volume = {114}, year = {2021}, issn = {1383-7621}, doi = {10.1016/j.sysarc.2020.101940}, author = {Shafee, Ahmed and Awaad, Tasneem A.}, abstract = {Keywords: Adversarial machine learning Convolutional neural network Deep neural network Machine learning} }
@article{WOS:000623001500001, title = {Quantum Reinforcement Learning with Quantum Photonics}, journal = {PHOTONICS}, volume = {8}, year = {2021}, doi = {10.3390/photonics8020033}, author = {Lamata, Lucas}, abstract = {Quantum machine learning has emerged as a promising paradigm that could accelerate machine learning calculations. Inside this field, quantum reinforcement learning aims at designing and building quantum agents that may exchange information with their environment and adapt to it, with the aim of achieving some goal. Different quantum platforms have been considered for quantum machine learning and specifically for quantum reinforcement learning. Here, we review the field of quantum reinforcement learning and its implementation with quantum photonics. This quantum technology may enhance quantum computation and communication, as well as machine learning, via the fruitful marriage between these previously unrelated fields.} }
@article{WOS:000544215800001, title = {Deep kernel learning in extreme learning machines}, journal = {PATTERN ANALYSIS AND APPLICATIONS}, volume = {24}, pages = {11-19}, year = {2021}, issn = {1433-7541}, doi = {10.1007/s10044-020-00891-8}, author = {Afzal, A. L. and Nair, Nikhitha K. and Asharaf, S.}, abstract = {Emergence of extreme learning machine as a breakneck learning algorithm has marked its prominence in solitary hidden layer feed-forward networks. Kernel-based extreme learning machine (KELM) reflected its efficiency in diverse applications where feature mapping functions of hidden nodes are concealed from users. The conventional KELM algorithms involve only solitary layer of kernels, thereby emulating shallow learning architectures for its feature transformation. Trend in migrating shallow-based learning models into deep learning architectures opens up a new outlook for machine learning domains. This paper attempts to bestow deep kernel learning approach in a conventional shallow architecture. The emerging arc-cosine kernels possess the potential to mimic the prevailing deep layered frameworks to a greater extent. Unlike other kernels such as linear, polynomial and Gaussian, arc-cosine kernels have a recursive nature by itself and have the potential to express multilayer computation in learning models. This paper explores the possibility of building a new deep kernel machine with extreme learning machine and multilayer arc-cosine kernels. This framework outperforms conventional KELM and deep support vector machine in terms of training time and accuracy.} }
@article{WOS:000494800100001, title = {Learning Moore machines from input-output traces}, journal = {INTERNATIONAL JOURNAL ON SOFTWARE TOOLS FOR TECHNOLOGY TRANSFER}, volume = {23}, pages = {1-29}, year = {2021}, issn = {1433-2779}, doi = {10.1007/s10009-019-00544-0}, author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos}, abstract = {The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper, we study this problem for finite-state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. We also carry out a performance comparison against two existing tools (LearnLib and flexfringe). Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.} }
@article{WOS:000424176900011, title = {Deep multiple multilayer kernel learning in core vector machines}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {96}, pages = {149-156}, year = {2018}, issn = {0957-4174}, doi = {10.1016/j.eswa.2017.11.058}, author = {Afzal, A. L. and Asharaf, S.}, abstract = {Over the last few years, we have been witnessing a dramatic progress of deep learning in many real world applications. Deep learning concepts have been originated in the area of neural network and show a quantum leap in effective feature learning techniques such as auto-encoders, convolutional neural networks, recurrent neural networks etc. In the case of kernel machines, there are several attempts to model learning machines that mimic deep neural networks. In this direction, Multilayer Kernel Machines (MKMs) was an attempt to build a kernel machine architecture with multiple layers of feature extraction. It composed of many layers of kernel PCA based feature extraction units with support vector machine having arc-cosine kernel as the final classifier. The other approaches like Multiple Kernel Learning (MKL) and deep core vector machines solve the fixed kernel computation problem and scalability aspects of MKMs respectively. In addition to this, there are lot of avenues where the use of unsupervised MKL with both single and multilayer kernels in the multilayer feature extraction framework have to be evaluated. In this context, this paper attempts to build a scalable deep kernel machines with multiple layers of feature extraction. Each kernel PCA based feature extraction layer in this framework is modeled by the combination of both single and multilayer kernels in an unsupervised manner. Core vector machine with arc-cosine kernel is used as the final layer classifier which ensure the scalability in this model. The major contribution of this paper is a novel effort to build a deep structured kernel machine architecture similar to deep neural network architecture for classification. It opens up an extendable research avenue for researchers in deep learning based intelligent system leveraging the principles of kernel theory. Experiments show that the proposed method consistently improves the generalization performances of existing deep core vector machine. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000927333400001, title = {Smart farming using artificial intelligence: A review}, journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}, volume = {120}, year = {2023}, issn = {0952-1976}, doi = {10.1016/j.engappai.2023.105899}, author = {Akkem, Yaganteeswarudu and Biswas, Saroj Kumar and Varanasi, Aruna}, abstract = {Smart farming with artificial intelligence provides an efficient solution to today's agricultural sustainability challenges. Machine learning, Deep learning, and time series analysis are essential in smart farming. Crop selection, crop yield prediction, soil compatibility classification, water management, and many other processes are involved in agriculture. Machine learning algorithms are used for crop selection and management, Deep learning techniques are used for crop selection and forecasting crop production, and time series analysis is used for demand forecasting of crops, commodity price prediction, and crop yield production forecasting. Crops are chosen using machine learning algorithms and deep learning algorithms based on soil, soil compatibility classification, and other factors. In the agriculture industry, this article offers a thorough review of machine learning and deep learning techniques. Crop data sets can be used to classify soil fertility, crop selection, and many other aspects using machine learning algorithms. Deep learning algorithms can be applied to farming data to do time series analysis and crop selection. Because there is more need for food due to the growing population, crop production forecasting is one of the crucial tasks. Therefore, future crop production must be predicted in order to overcome food insufficiency. In this article, several time series algorithms were reviewed. Suggesting appropriate crop recommendations using machine and deep learning by estimating crop yield by using time series analysis will reduce food insufficiency in the future.} }
@article{WOS:001242376600007, title = {Recent Developments and Future Directions of Wearable Skin Biosignal Sensors}, journal = {ADVANCED SENSOR RESEARCH}, volume = {3}, year = {2024}, issn = {2751-1219}, doi = {10.1002/adsr.202300118}, author = {Kim, Dohyung and Min, JinKi and Ko, Seung Hwan}, abstract = {This review article explores the transformative advancements in wearable biosignal sensors powered by machine learning, focusing on four notable biosignals: electrocardiogram (ECG), electromyogram (EMG), electroencephalogram (EEG), and photoplethysmogram (PPG). The integration of machine learning with these biosignals has led to remarkable breakthroughs in various medical monitoring and human-machine interface applications. For ECG, machine learning enables automated heartbeat classification and accurate disease detection, improving cardiac healthcare with early diagnosis and personalized interventions. EMG technology, combined with machine learning, facilitates real-time prediction and classification of human motions, revolutionizing applications in sports medicine, rehabilitation, prosthetics, and virtual reality interfaces. EEG analysis powered by machine learning goes beyond traditional clinical applications, enabling brain activity understanding in psychology, neurology, and human-computer interaction, and holds promise in brain-computer interfaces. PPG, augmented with machine learning, has shown exceptional progress in diagnosing and monitoring cardiovascular and respiratory disorders, offering non-invasive and accurate healthcare solutions. These integrated technologies, powered by machine learning, open new avenues for medical monitoring and human-machine interaction, shaping the future of healthcare. The convergence of wearable biosignal sensors and machine learning paves the way for significant advancements in healthcare, enabling early medical diagnosis and personalized health monitoring. This review article provides an overview of recent transformative advancements in wearable biosignal sensors powered by machine learning, focusing on four notable biosignals: electrocardiogram, electromyogram, electroencephalogram, and photoplethysmogram. image} }
@article{WOS:000705073600019, title = {AutoML to Date and Beyond: Challenges and Opportunities}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3470918}, author = {Karmaker (Santu), Shubhra Kanti and Hassan, Md Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan}, abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML's main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually-generally by a data scientist-and explain how this limits domain experts' access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.} }
@article{WOS:000751250600001, title = {AutoML: state of the art with a focus on anomaly detection, challenges, and research directions}, journal = {INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS}, volume = {14}, pages = {113-126}, year = {2022}, issn = {2364-415X}, doi = {10.1007/s41060-022-00309-0}, author = {Bahri, Maroua and Salutari, Flavia and Putina, Andrian and Sozio, Mauro}, abstract = {The last decade has witnessed the explosion of machine learning research studies with the inception of several algorithms proposed and successfully adopted in different application domains. However, the performance of multiple machine learning algorithms is very sensitive to multiple ingredients (e.g., hyper-parameters tuning and data cleaning) where a significant human effort is required to achieve good results. Thus, building well-performing machine learning algorithms requires domain knowledge and highly specialized data scientists. Automated machine learning (autoML) aims to make easier and more accessible the use of machine learning algorithms for researchers with varying levels of expertise. Besides, research effort to date has mainly been devoted to autoML for supervised learning, and only a few research proposals have been provided for the unsupervised learning. In this paper, we present an overview of the autoML field with a particular emphasis on the automated methods and strategies that have been proposed for unsupervised anomaly detection.} }
@article{WOS:000703968500001, title = {Algorithmic reparation}, journal = {BIG DATA \\& SOCIETY}, volume = {8}, year = {2021}, issn = {2053-9517}, doi = {10.1177/20539517211044808}, author = {Davis, Jenny L. and Williams, Apryl and Yang, Michael W.}, abstract = {Machine learning algorithms pervade contemporary society. They are integral to social institutions, inform processes of governance, and animate the mundane technologies of daily life. Consistently, the outcomes of machine learning reflect, reproduce, and amplify structural inequalities. The field of fair machine learning has emerged in response, developing mathematical techniques that increase fairness based on anti-classification, classification parity, and calibration standards. In practice, these computational correctives invariably fall short, operating from an algorithmic idealism that does not, and cannot, address systemic, Intersectional stratifications. Taking present fair machine learning methods as our point of departure, we suggest instead the notion and practice of algorithmic reparation. Rooted in theories of Intersectionality, reparative algorithms name, unmask, and undo allocative and representational harms as they materialize (American English sp) in sociotechnical form. We propose algorithmic reparation as a foundation for building, evaluating, adjusting, and when necessary, omitting and eradicating machine learning systems.} }
@article{WOS:000452640000033, title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules}, journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}, volume = {25}, pages = {342-352}, year = {2019}, issn = {1077-2626}, doi = {10.1109/TVCG.2018.2864812}, author = {Ming, Yao and Qu, Huamin and Bertini, Enrico}, abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.} }
@article{WOS:000498849000042, title = {Preventing undesirable behavior of intelligent machines}, journal = {SCIENCE}, volume = {366}, pages = {999+}, year = {2019}, issn = {0036-8075}, doi = {10.1126/science.aag3311}, author = {Thomas, Philip S. and da Silva, Bruno Castro and Barto, Andrew G. and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma}, abstract = {Intelligent machines using machine learning algorithms are ubiquitous, ranging from simple data analysis and pattern recognition tools to complex systems that achieve superhuman performance on various tasks. Ensuring that they do not exhibit undesirable behavior-that they do not, for example, cause harm to humans-is therefore a pressing problem. We propose a general and flexible framework for designing machine learning algorithms. This framework simplifies the problem of specifying and regulating undesirable behavior. To show the viability of this framework, we used it to create machine learning algorithms that precluded the dangerous behavior caused by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning.} }
@article{WOS:000572786300004, title = {Deep support vector neural networks}, journal = {INTEGRATED COMPUTER-AIDED ENGINEERING}, volume = {27}, pages = {389-402}, year = {2020}, issn = {1069-2509}, doi = {10.3233/ICA-200635}, author = {Diaz-Vico, David and Prada, Jesus and Omari, Adil and Dorronsoro, Jose}, abstract = {Kernel based Support Vector Machines, SVM, one of the most popular machine learning models, usually achieve top performances in two-class classification and regression problems. However, their training cost is at least quadratic on sample size, making them thus unsuitable for large sample problems. However, Deep Neural Networks (DNNs), with a cost linear on sample size, are able to solve big data problems relatively easily. In this work we propose to combine the advanced representations that DNNs can achieve in their last hidden layers with the hinge and epsilon insensitive losses that are used in two-class SVM classification and regression. We can thus have much better scalability while achieving performances comparable to those of SVMs. Moreover, we will also show that the resulting Deep SVM models are competitive with standard DNNs in two-class classification problems but have an edge in regression ones.} }
@article{WOS:000569592200001, title = {Handwritten Digit Recognition: Hyperparameters-Based Analysis}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175988}, author = {Albahli, Saleh and Alhassan, Fatimah and Albattah, Waleed and Khan, Rehan Ullah}, abstract = {Neural networks have several useful applications in machine learning. However, benefiting from the neural-network architecture can be tricky in some instances due to the large number of parameters that can influence performance. In general, given a particular dataset, a data scientist cannot do much to improve the efficiency of the model. However, by tuning certain hyperparameters, the model's accuracy and time of execution can be improved. Hence, it is of utmost importance to select the optimal values of hyperparameters. Choosing the optimal values of hyperparameters requires experience and mastery of the machine learning paradigm. In this paper, neural network-based architectures are tested based on altering the values of hyperparameters for handwritten-based digit recognition. Various neural network-based models are used to analyze different aspects of the same, primarily accuracy based on hyperparameter values. The extensive experimentation setup in this article should, therefore, provide the most accurate and time-efficient solution models. Such an evaluation will help in selecting the optimized values of hyperparameters for similar tasks.} }
@article{WOS:000345135700005, title = {Multi-view Laplacian twin support vector machines}, journal = {APPLIED INTELLIGENCE}, volume = {41}, pages = {1059-1068}, year = {2014}, issn = {0924-669X}, doi = {10.1007/s10489-014-0563-8}, author = {Xie, Xijiong and Sun, Shiliang}, abstract = {Twin support vector machines are a recently proposed learning method for pattern classification. They learn two hyperplanes rather than one as in usual support vector machines and often bring performance improvements. Semi-supervised learning has attracted great attention in machine learning in the last decade. Laplacian support vector machines and Laplacian twin support vector machines have been proposed in the semi-supervised learning framework. In this paper, inspired by the recent success of multi-view learning we propose multi-view Laplacian twin support vector machines, whose dual optimization problems are quadratic programming problems. We further extend them to kernel multi-view Laplacian twin support vector machines. Experimental results demonstrate that our proposed methods are effective.} }
@article{WOS:000481413400025, title = {If machines can learn, who needs scientists?}, journal = {JOURNAL OF MAGNETIC RESONANCE}, volume = {306}, pages = {162-166}, year = {2019}, issn = {1090-7807}, doi = {10.1016/j.jmr.2019.07.044}, author = {Hoch, Jeffrey C.}, abstract = {Machine learning has been used in NMR in for decades, but recent developments signal explosive growth is on the horizon. An obstacle to the application of machine learning in NMR is the relative paucity of available training data, despite the existence of numerous public NMR data repositories. Other challenges include the problem of interpreting the results of a machine learning algorithm, and incorporating machine learning into hypothesis-driven research. This perspective imagines the potential of machine learning in NMR and speculates on possible approaches to the hurdles. (C) 2019 Published by Elsevier Inc.} }
@article{WOS:000461166500028, title = {Deep embedding kernel}, journal = {NEUROCOMPUTING}, volume = {339}, pages = {292-302}, year = {2019}, issn = {0925-2312}, doi = {10.1016/j.neucom.2019.02.037}, author = {Linh Le and Xie, Ying}, abstract = {In this paper, we propose a novel supervised learning method that is called Deep Embedding Kernel (DEK). DEK combines the advantages of deep learning and kernel methods in a unified framework. More specifically, DEK is a learnable kernel represented by a newly designed deep architecture. Compared with predefined kernels, this kernel can be explicitly trained to map data to an optimized high-level feature space where data may have favorable features toward the application. Compared with typical deep learning using SoftMax or logistic regression as the top layer, DEK is expected to be more generalizable to new data. Experimental results show that DEK has superior performance than typical machine learning methods in identity detection and classification, and transfer learning, on different types of data including images, sequences, and regularly structured data. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000467010400067, title = {Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS}, volume = {66}, pages = {7316-7325}, year = {2019}, issn = {0278-0046}, doi = {10.1109/TIE.2018.2877090}, author = {Guo, Liang and Lei, Yaguo and Xing, Saibo and Yan, Tao and Li, Naipeng}, abstract = {The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.} }
@article{WOS:000720251600001, title = {A Zeroth-Order Adaptive Learning Rate Method to Reduce Cost of Hyperparameter Tuning for Deep Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/app112110184}, author = {Li, Yanan and Ren, Xuebin and Zhao, Fangyuan and Yang, Shusen}, abstract = {Due to powerful data representation ability, deep learning has dramatically improved the state-of-the-art in many practical applications. However, the utility highly depends on fine-tuning of hyper-parameters, including learning rate, batch size, and network initialization. Although many first-order adaptive methods (e.g., Adam, Adagrad) have been proposed to adjust learning rate based on gradients, they are susceptible to the initial learning rate and network architecture. Therefore, the main challenge of using deep learning in practice is how to reduce the cost of tuning hyper-parameters. To address this, we propose a heuristic zeroth-order learning rate method, Adacomp, which adaptively adjusts the learning rate based only on values of the loss function. The main idea is that Adacomp penalizes large learning rates to ensure the convergence and compensates small learning rates to accelerate the training process. Therefore, Adacomp is robust to the initial learning rate. Extensive experiments, including comparison to six typically adaptive methods (Momentum, Adagrad, RMSprop, Adadelta, Adam, and Adamax) on several benchmark datasets for image classification tasks (MNIST, KMNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100), were conducted. Experimental results show that Adacomp is not only robust to the initial learning rate but also to the network architecture, network initialization, and batch size.} }
@article{WOS:000697551500015, title = {Deep tree-ensembles for multi-output prediction}, journal = {PATTERN RECOGNITION}, volume = {121}, year = {2022}, issn = {0031-3203}, doi = {10.1016/j.patcog.2021.108211}, author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine}, abstract = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suf-fer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply em-ploy label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demon-strated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the origi-nal feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks. (c) 2021 Elsevier Ltd. All rights reserved.} }
@article{WOS:000874639800001, title = {Designing Catalyst Descriptors for Machine Learning in Oxidative Coupling of Methane}, journal = {ACS CATALYSIS}, volume = {12}, pages = {11541-11546}, year = {2022}, issn = {2155-5435}, doi = {10.1021/acscatal.2c03142}, author = {Ishioka, Sora and Fujiwara, Aya and Nakanowatari, Sunao and Takahashi, Lauren and Taniike, Toshiaki and Takahashi, Keisuke}, abstract = {Catalysts descriptors for representing catalytic activities have been challenging in regard to machine learning. Machine learning and catalyst big data generated from high-throughput experiments are combined to explore the catalyst descriptors. Catalyst descriptors are designed using the physical quantities from the periodic table in the oxidative coupling of methane (OCM) reaction. Machine learning unveils the five key physical quantities representing ethylene/ethane selectivity (C2s) in the OCM reaction, where machine learning predicted three catalysts to have high C2s values. Experiments confirm that the proposed three catalysts have high C2s values in the OCM reaction. Hence, the physical quantities can be used as alternative descriptors for designing heterogeneous catalysts.} }
@article{WOS:001118122400001, title = {Application of Machine Learning Based on Structured Medical Data in Gastroenterology}, journal = {BIOMIMETICS}, volume = {8}, year = {2023}, doi = {10.3390/biomimetics8070512}, author = {Kim, Hye-Jin and Gong, Eun-Jeong and Bang, Chang-Seok}, abstract = {The era of big data has led to the necessity of artificial intelligence models to effectively handle the vast amount of clinical data available. These data have become indispensable resources for machine learning. Among the artificial intelligence models, deep learning has gained prominence and is widely used for analyzing unstructured data. Despite the recent advancement in deep learning, traditional machine learning models still hold significant potential for enhancing healthcare efficiency, especially for structured data. In the field of medicine, machine learning models have been applied to predict diagnoses and prognoses for various diseases. However, the adoption of machine learning models in gastroenterology has been relatively limited compared to traditional statistical models or deep learning approaches. This narrative review provides an overview of the current status of machine learning adoption in gastroenterology and discusses future directions. Additionally, it briefly summarizes recent advances in large language models.} }
@article{WOS:000429536600008, title = {Quantum Machine Learning in Chemical Compound Space}, journal = {ANGEWANDTE CHEMIE-INTERNATIONAL EDITION}, volume = {57}, pages = {4164-4169}, year = {2018}, issn = {1433-7851}, doi = {10.1002/anie.201709686}, author = {von Lilienfeld, O. Anatole}, abstract = {Rather than numerically solving the computationally demanding equations of quantum or statistical mechanics, machine learning methods can infer approximate solutions by interpolating previously acquired property data sets of molecules and materials. The case is made for quantum machine learning: An inductive molecular modeling approach which can be applied to quantum chemistry problems.} }
@article{WOS:000458997900018, title = {Survey on SDN based network intrusion detection system using machine learning approaches}, journal = {PEER-TO-PEER NETWORKING AND APPLICATIONS}, volume = {12}, pages = {493-501}, year = {2019}, issn = {1936-6442}, doi = {10.1007/s12083-017-0630-0}, author = {Sultana, Nasrin and Chilamkurti, Naveen and Peng, Wei and Alhadad, Rabei}, abstract = {Software Defined Networking Technology (SDN) provides a prospect to effectively detect and monitor network security problems ascribing to the emergence of the programmable features. Recently, Machine Learning (ML) approaches have been implemented in the SDN-based Network Intrusion Detection Systems (NIDS) to protect computer networks and to overcome network security issues. A stream of advanced machine learning approaches - the deep learning technology (DL) commences to emerge in the SDN context. In this survey, we reviewed various recent works on machine learning (ML) methods that leverage SDN to implement NIDS. More specifically, we evaluated the techniques of deep learning in developing SDN-based NIDS. In the meantime, in this survey, we covered tools that can be used to develop NIDS models in SDN environment. This survey is concluded with a discussion of ongoing challenges in implementing NIDS using ML/DL and future works.} }
@article{WOS:000602672200005, title = {Significant Applications of Machine Learning for COVID-19 Pandemic}, journal = {JOURNAL OF INDUSTRIAL INTEGRATION AND MANAGEMENT-INNOVATION AND ENTREPRENEURSHIP}, volume = {5}, pages = {453-479}, year = {2020}, issn = {2424-8622}, doi = {10.1142/S2424862220500268}, author = {Kushwaha, Shashi and Bahl, Shashi and Bagha, Ashok Kumar and Parmar, Kulwinder Singh and Javaid, Mohd and Haleem, Abid and Singh, Ravi Pratap}, abstract = {Machine learning is an innovative approach that has extensive applications in prediction. This technique needs to be applied for the COVID-19 pandemic to identify patients at high risk, their death rate, and other abnormalities. It can be used to understand the nature of this virus and further predict the upcoming issues. This literature-based review is done by searching the relevant papers on machine learning for COVID-19 from the databases of SCOPUS, Academia, Google Scholar, PubMed, and ResearchGate. This research attempts to discuss the significance of machine learning in resolving the COVID-19 pandemic crisis. This paper studied how machine learning algorithms and methods can be employed to fight the COVID-19 virus and the pandemic. It further discusses the primary machine learning methods that are helpful during the COVID-19 pandemic. We further identified and discussed algorithms used in machine learning and their significant applications. Machine learning is a useful technique, and this can be witnessed in various areas to identify the existing drugs, which also seems advantageous for the treatment of COVID-19 patients. This learning algorithm creates interferences out of unlabeled input datasets, which can be applied to analyze the unlabeled data as an input resource for COVID-19. It provides accurate and useful features rather than a traditional explicitly calculation-based method. Further, this technique is beneficial to predict the risk in healthcare during this COVID-19 crisis. Machine learning also analyses the risk factors as per age, social habits, location, and climate.} }
@article{WOS:000504871100004, title = {Applications of Machine Learning Using Electronic Medical Records in Spine Surgery}, journal = {NEUROSPINE}, volume = {16}, pages = {643-653}, year = {2019}, issn = {2586-6583}, doi = {10.14245/ns.1938386.193}, author = {Schwartz, John T. and Gao, Michael and Geng, Eric A. and Mody, Kush S. and Mikhail, Christopher M. and Cho, Samuel K.}, abstract = {Developments in machine learning in recent years have precipitated a surge in research on the applications of artificial intelligence within medicine. Machine learning algorithms are beginning to impact medicine broadly, and the field of spine surgery is no exception. Electronic medical records are a key source of medical data that can be leveraged for the creation of clinically valuable machine learning algorithms. This review examines the current state of machine learning using electronic medical records as it applies to spine surgery. Studies across the electronic medical record data domains of imaging, text, and structured data are reviewed. Discussed applications include clinical prognostication, preoperative planning, diagnostics, and dynamic clinical assistance, among others. The limitations and future challenges for machine learning research using electronic medical records are also discussed.} }
@article{WOS:000778251600007, title = {Event Detection for Distributed Acoustic Sensing: Combining Knowledge-Based, Classical Machine Learning, and Deep Learning Approaches}, journal = {SENSORS}, volume = {21}, year = {2021}, doi = {10.3390/s21227527}, author = {Bublin, Mugdim}, abstract = {Distributed Acoustic Sensing (DAS) is a promising new technology for pipeline monitoring and protection. However, a big challenge is distinguishing between relevant events, like intrusion by an excavator near the pipeline, and interference, like land machines. This paper investigates whether it is possible to achieve adequate detection accuracy with classic machine learning algorithms using simulations and real system implementation. Then, we compare classical machine learning with a deep learning approach and analyze the advantages and disadvantages of both approaches. Although acceptable performance can be achieved with both approaches, preliminary results show that deep learning is the more promising approach, eliminating the need for laborious feature extraction and offering a six times lower event detection delay and twelve times lower execution time. However, we achieved the best results by combining deep learning with the knowledge-based and classical machine learning approaches. At the end of this manuscript, we propose general guidelines for efficient system design combining knowledge-based, classical machine learning, and deep learning approaches.} }
@article{WOS:000429088600004, title = {POINTS OF SIGNIFICANCE Statistics versus machine learning}, journal = {NATURE METHODS}, volume = {15}, pages = {232-233}, year = {2018}, issn = {1548-7091}, doi = {10.1038/nmeth.4642}, author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin}, abstract = {Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.} }
@article{WOS:000453925000013, title = {State of the Art: Machine Learning Applications in Glioma Imaging}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {212}, pages = {26-37}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.18.20218}, author = {Lotan, Eyal and Lotan, Eyal and Jain, Rajan and Razavian, Narges and Fatterpekar, Girish M. and Lui, Yvonne W.}, abstract = {OBJECTIVE. Machine learning has recently gained considerable attention because of promising results for a wide range of radiology applications. Here we review recent work using machine learning in brain tumor imaging, specifically segmentation and MRI radiomics of gliomas. CONCLUSION. We discuss available resources, state`of`the`art segmentation methods, and machine learning radiomics for glioma. We highlight the challenges of these techniques as well as the future potential in clinical diagnostics, prognostics, and decision making.} }
@article{WOS:000619114800012, title = {A Contemporary Review of Machine Learning in Otolaryngology-Head and Neck Surgery}, journal = {LARYNGOSCOPE}, volume = {130}, pages = {45-51}, year = {2020}, issn = {0023-852X}, doi = {10.1002/lary.27850}, author = {Crowson, Matthew G. and Ranisau, Jonathan and Eskander, Antoine and Babier, Aaron and Xu, Bin and Kahmke, Russel R. and Chen, Joseph M. and Chan, Timothy C. Y.}, abstract = {One of the key challenges with big data is leveraging the complex network of information to yield useful clinical insights. The confluence of massive amounts of health data and a desire to make inferences and insights on these data has produced a substantial amount of interest in machine-learning analytic methods. There has been a drastic increase in the otolaryngology literature volume describing novel applications of machine learning within the past 5 years. In this timely contemporary review, we provide an overview of popular machine-learning techniques, and review recent machine-learning applications in otolaryngology-head and neck surgery including neurotology, head and neck oncology, laryngology, and rhinology. Investigators have realized significant success in validated models with model sensitivities and specificities approaching 100\\%. Challenges remain in the implementation of machine-learning algorithms. This may be in part the unfamiliarity of these techniques to clinician leaders on the front lines of patient care. Spreading awareness and confidence in machine learning will follow with further validation and proof-of-value analyses that demonstrate model performance superiority over established methods. We are poised to see a greater influx of machine-learning applications to clinical problems in otolaryngology-head and neck surgery, and it is prudent for providers to understand the potential benefits and limitations of these technologies.} }
@article{WOS:000560159100002, title = {Learning local discriminative representations via extreme learning machine for machine fault diagnosis}, journal = {NEUROCOMPUTING}, volume = {409}, pages = {275-285}, year = {2020}, issn = {0925-2312}, doi = {10.1016/j.neucom.2020.05.021}, author = {Li, Yue and Zeng, Yijie and Qing, Yuanyuan and Huang, Guang-Bin}, abstract = {Recently, learning data representations have been investigated to reduce the dependences of human intervention and improve the performance of machine fault diagnosis. However, most of the representation learning methods are computationally intensive due to complex training procedures. Extreme learning machine is well-known for its fast training speed and strong generalization ability. It also has been applied to learn data representations for clustering and classification tasks. In this paper, a local discriminant preserving extreme learning machine autoencoder (LDELM-AE) is proposed to learn data representations with the local geometry and local discriminant exploited from the input data. Specifically, LDELM-AE utilizes two graphs to enhance the within-class compactness and between-class separability, respectively. Furthermore, the hierarchical representations can be obtained by stacking several LDELM-AEs. On several benchmark datasets, the proposed method demonstrates better classification accuracies than the state-of-the-art methods. Moreover, the proposed method has been used to diagnostic the rotary machine faults and achieves the diagnostic accuracy of 99.96\\%, which proves the proposed method is an efficient tool to diagnose machine faults. (C) 2020 Elsevier B.V. All rights reserved.} }
@incollection{WOS:000433057100004, title = {Machine Learning Approaches for Clinical Psychology and Psychiatry}, booktitle = {ANNUAL REVIEW OF CLINICAL PSYCHOLOGY, VOL 14}, volume = {14}, pages = {91-118}, year = {2018}, issn = {1548-5943}, doi = {10.1146/annurev-clinpsy-032816045037}, author = {Dwyer, Dominic B. and Falkai, Peter and Koutsouleris, Nikolaos}, abstract = {Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.} }
@article{WOS:000354655800008, title = {Machine learning applications in genetics and genomics}, journal = {NATURE REVIEWS GENETICS}, volume = {16}, pages = {321-332}, year = {2015}, issn = {1471-0056}, doi = {10.1038/nrg3920}, author = {Libbrecht, Maxwell W. and Noble, William Stafford}, abstract = {The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.} }
@article{WOS:001006490400001, title = {Machine learning facilitating the rational design of nanozymes}, journal = {JOURNAL OF MATERIALS CHEMISTRY B}, volume = {11}, pages = {6466-6477}, year = {2023}, issn = {2050-750X}, doi = {10.1039/d3tb00842h}, author = {Li, Yucong and Zhang, Ruofei and Yan, Xiyun and Fan, Kelong}, abstract = {As a component substitute for natural enzymes, nanozymes have the advantages of easy synthesis, convenient modification, low cost, and high stability, and are widely used in many fields. However, their application is seriously restricted by the difficulty of rapidly creating high-performance nanozymes. The use of machine learning techniques to guide the rational design of nanozymes holds great promise to overcome this difficulty. In this review, we introduce the recent progress of machine learning in assisting the design of nanozymes. Particular attention is given to the successful strategies of machine learning in predicting the activity, selectivity, catalytic mechanisms, optimal structures and other features of nanozymes. The typical procedures and approaches for conducting machine learning in the study of nanozymes are also highlighted. Moreover, we discuss in detail the difficulties of machine learning methods in dealing with the redundant and chaotic nanozyme data and provide an outlook on the future application of machine learning in the nanozyme field. We hope that this review will serve as a useful handbook for researchers in related fields and promote the utilization of machine learning in nanozyme rational design and related topics.} }
@article{WOS:000269069200037, title = {Dropout prediction in e-learning courses through the combination of machine learning techniques}, journal = {COMPUTERS \\& EDUCATION}, volume = {53}, pages = {950-965}, year = {2009}, issn = {0360-1315}, doi = {10.1016/j.compedu.2009.05.010}, author = {Lykourentzou, Ioanna and Giannoukos, Ioannis and Nikolopoulos, Vassilis and Mpardis, George and Loumos, Vassili}, abstract = {In this paper, a dropout prediction method for e-learning courses, based on three popular machine learning techniques and detailed student data, is proposed. The machine learning techniques used are feed-forward neural networks, support vector machines and probabilistic ensemble simplified fuzzy ARTMAP. Since a single technique may fail to accurately classify some e-learning students, whereas another may succeed, three decision schemes, which combine in different ways the results of the three machine learning techniques, were also tested. The method was examined in terms of overall accuracy, sensitivity and precision and its results were found to be significantly better than those reported in relevant literature. (C) 2009 Elsevier Ltd. All rights reserved.} }
@article{WOS:000736977400012, title = {Machine-learning methods for ligand-protein molecular docking}, journal = {DRUG DISCOVERY TODAY}, volume = {27}, pages = {151-164}, year = {2022}, issn = {1359-6446}, doi = {10.1016/j.drudis.2021.09.007}, author = {Crampon, Kevin and Giorkallos, Alexis and Deldossi, Myrtille and Baud, Stephanie and Steffenel, Luiz Angelo}, abstract = {Artificial intelligence (AI) is often presented as a new Industrial Revolution. Many domains use AI, including molecular simulation for drug discovery. In this review, we provide an overview of ligand-protein molecular docking and how machine learning (ML), especially deep learning (DL), a subset of ML, is transforming the field by tackling the associated challenges.} }
@article{WOS:000768730600017, title = {Adaptive machine learning for protein engineering}, journal = {CURRENT OPINION IN STRUCTURAL BIOLOGY}, volume = {72}, pages = {145-152}, year = {2022}, issn = {0959-440X}, doi = {10.1016/j.sbi.2021.11.002}, author = {Hie, Brian L. and Yang, Kevin K.}, abstract = {Machine-learning models that learn from data to predict how protein sequence encodes function are emerging as a useful protein engineering tool. However, when using these models to suggest new protein designs, one must deal with the vast combinatorial complexity of protein sequences. Here, we review how to use a sequence-to-function machine-learning surrogate model to select sequences for experimental measurement. First, we discuss how to select sequences through a single round of machine-learning optimization. Then, we discuss sequential optimization, where the goal is to discover optimized sequences and improve the model across multiple rounds of training, optimization, and experimental measurement.} }
@article{WOS:000512216200010, title = {Applying Machine Learning Techniques in Nomogram Prediction and Analysis for SMILE Treatment}, journal = {AMERICAN JOURNAL OF OPHTHALMOLOGY}, volume = {210}, pages = {71-77}, year = {2020}, issn = {0002-9394}, doi = {10.1016/j.ajo.2019.10.015}, author = {Cui, Tong and Wang, Yan and Ji, Shufan and Li, Yan and Hao, Weiting and Zou, Haohan and Jhanji, Vishal}, abstract = {PURPOSE: To analyze the outcome of machine learning technique for prediction of small incision lenticule extraction (SMILE) nomogram. DESIGN: Prospective, comparative clinical study. METHODS: A comparative study was conducted on the outcomes of SMILE surgery between surgeon group (nomogram set by surgeon) and machine learning group (nomogram predicted by machine learning model). The machine learning model was trained by 865 ideal cases (spherical equivalent [SE] within +/- 0.5 diopter [D] 3 months postoperatively) from an experienced surgeon. The visual outcomes of both groups were compared for safety, efficacy, predictability, and SE correction. RESULTS: There was no statistically significant difference between the baseline data in both groups. The efficacy index in the machine learning group (1.48 +/- 1.08) was significantly higher than in the surgeon group (1.3 +/- 0.27) (t = -2.17, P < .05). Eighty-three percent of eyes in the surgeon group and 93\\% of eyes in the machine learning group were within +/- 0.50 D, while 98\\% of eyes in the surgeon group and 96\\% of eyes in the machine learning group were within 1.00 D. The error of SE correction was -0.09 +/- 0.024 and -0.23 +/- 0.021 for machine learning and surgeon groups, respectively. CONCLUSIONS: The machine learning technique performed as well as surgeon in safety, but significantly better than surgeon in efficacy. As for predictability, the machine learning technique was comparable to surgeon, although less predictable for high myopia and astigmatism. ((C) 2019 Published by Elsevier Inc.)} }
@article{WOS:000468859100001, title = {Prospects of deep learning for medical imaging}, journal = {PRECISION AND FUTURE MEDICINE}, volume = {2}, pages = {37-52}, year = {2018}, issn = {2508-7940}, doi = {10.23838/pfm.2018.00030}, author = {Kim, Jonghoon and Hong, Jisu and Park, Hyunjin}, abstract = {Machine learning techniques are essential components of medical imaging research. Recently, a highly flexible machine learning approach known as deep learning has emerged as a disruptive technology to enhance the performance of existing machine learning techniques and to solve previously intractable problems. Medical imaging has been identified as one of the key research fields where deep learning can contribute significantly. This review article aims to survey deep learning literature in medical imaging and describe its potential for future medical imaging research. First, an overview of how traditional machine learning evolved to deep learning is provided. Second, a survey of the application of deep learning in medical imaging research is given. Third, well-known software tools for deep learning are reviewed. Finally, conclusions with limitations and future directions of deep learning in medical imaging are provided.} }
@article{WOS:000603902400001, title = {Comparative Study of the Dynamic Back-Analysis Methods of Concrete Gravity Dams Based on Multivariate Machine Learning Models}, journal = {JOURNAL OF EARTHQUAKE ENGINEERING}, volume = {25}, pages = {1-22}, year = {2021}, issn = {1363-2469}, doi = {10.1080/13632469.2018.1452802}, author = {Cheng, Lin and Tong, Fei and Li, Yanlong and Yang, Jie and Zheng, Dongjian}, abstract = {Two different back-analysis frameworks based on multivariate machine learning models used to determine the material dynamic parameters of concrete gravity dams are proposed. For the framework I, the back-analysis is performed by solving an optimization problem and a multivariate machine learning model is trained to replace the FEM calculation during the optimization process. While the framework II uses a multivariate machine learning model directly and the material dynamic parameters are predicted using the machine learning mode. By using a numerical example and an experimental investigation, the robustness, accuracy, computation efficiency of these proposed back-analysis methods is verified.} }
@article{WOS:000544969800031, title = {Utilizing crowdsourcing and machine learning in education: Literature review}, journal = {EDUCATION AND INFORMATION TECHNOLOGIES}, volume = {25}, pages = {2971-2986}, year = {2020}, issn = {1360-2357}, doi = {10.1007/s10639-020-10102-w}, author = {Alenezi, Hadeel S. and Faisal, Maha H.}, abstract = {For many years, learning continues to be a vital developing field since it is the key measure of the world's civilization and evolution with its enormous effect on both individuals and societies. Enhancing existing learning activities in general will have a significant impact on literacy rates around the world. One of the crucial activities in education is the assessment method because it is the primary way used to evaluate the student during their studies. The main purpose of this review is to examine the existing learning and e-learning approaches that use either crowdsourcing, machine learning, or both crowdsourcing and machine learning in their proposed solutions. This review will also investigate the addressed applications to identify the existing researches related to the assessment. Identifying all existing applications will assist in finding the unexplored gaps and limitations. This study presents a systematic literature review investigating 30 papers from the following databases: IEEE and ACM Digital Library. After performing the analysis, we found that crowdsourcing is utilized in 47.8\\% of the investigated learning activities, while each of the machine learning and the hybrid solutions are utilized in 26\\% of the investigated learning activities. Furthermore, all the existing approaches regarding the exam assessment problem that are using machine learning or crowdsourcing were identified. Some of the existing assessment systems are using the crowdsourcing approach and other systems are using the machine learning, however, none of the approaches provide a hybrid assessment system that uses both crowdsourcing and machine learning. Finally, it is found that using either crowdsourcing or machine learning in the online courses will enhance the interactions between the students. It is concluded that the current learning activities need to be enhanced since it is directly affecting the student's performance. Moreover, merging both the machine learning to the crowd wisdom will increase the accuracy and the efficiency of education.} }
@article{WOS:001132712600001, title = {<sc>FairCaipi</sc>: A Combination of Explanatory Interactive and Fair Machine Learning for Human and Machine Bias Reduction}, journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION}, volume = {5}, pages = {1519-1538}, year = {2023}, doi = {10.3390/make5040076}, author = {Heidrich, Louisa and Slany, Emanuel and Scheele, Stephan and Schmid, Ute and Cabitza, Federico and Chen, Fang and Zhou, Jianlong and Holzinger, Andreas}, abstract = {The rise of machine-learning applications in domains with critical end-user impact has led to a growing concern about the fairness of learned models, with the goal of avoiding biases that negatively impact specific demographic groups. Most existing bias-mitigation strategies adapt the importance of data instances during pre-processing. Since fairness is a contextual concept, we advocate for an interactive machine-learning approach that enables users to provide iterative feedback for model adaptation. Specifically, we propose to adapt the explanatory interactive machine-learning approach Caipi for fair machine learning. FairCaipi incorporates human feedback in the loop on predictions and explanations to improve the fairness of the model. Experimental results demonstrate that FairCaipi outperforms a state-of-the-art pre-processing bias mitigation strategy in terms of the fairness and the predictive performance of the resulting machine-learning model. We show that FairCaipi can both uncover and reduce bias in machine-learning models and allows us to detect human bias.} }
@article{WOS:001225632900001, title = {Enhancing flow stress predictions in CoCrFeNiV high entropy alloy with conventional and machine learning techniques}, journal = {JOURNAL OF MATERIALS RESEARCH AND TECHNOLOGY-JMR\\&T}, volume = {30}, pages = {2377-2387}, year = {2024}, issn = {2238-7854}, doi = {10.1016/j.jmrt.2024.03.164}, author = {Dewangan, Sheetal Kumar and Jain, Reliance and Bhattacharjee, Soumyabrata and Jain, Sandeep and Paswan, Manikant and Samal, Sumanta and Ahn, Byungmin}, abstract = {A machine learning technique leveraging artificial intelligence (AI) has emerged as a promising tool for expediting the exploration and design of novel high entropy alloys (HEAs) while predicting their mechanical properties at both room and elevated temperatures. In this paper, we predict the flow stress of hot-compressed CoCrFeNiV HEAs using conventional (qualitative and quantitative models) and advanced machine learning approaches across various temperature and strain rate conditions. Conventional modeling methods, including the modified Johnson -Cook (JC), modified Zerilli - Armstrong (ZA), and Arrhenius -type constitutive equations, are employed. Simultaneously, machine learning models are utilized to forecast flow stress under different hot working conditions. The performance of both conventional and machine learning models is evaluated using metrics such as coefficient of determination (R 2 ), mean abosolute error (MAE), and root mean squared error (RMSE). The analysis reveals that the gradient boosting machine learning model shows superior prediction accuracy (with value R 2 = 0.994, MAE = 7.77\\%, and RMSE = 9.7\\%) compared to conventional models and other machine learning approaches.} }
@article{WOS:001035352200001, title = {A survey on dataset quality in machine learning}, journal = {INFORMATION AND SOFTWARE TECHNOLOGY}, volume = {162}, year = {2023}, issn = {0950-5849}, doi = {10.1016/j.infsof.2023.107268}, author = {Gong, Youdi and Liu, Guangzhen and Xue, Yunzhi and Li, Rui and Meng, Lingzhong}, abstract = {With the rise of big data, the quality of datasets has become a crucial factor affecting the performance of machine learning models. High-quality datasets are essential for the realization of data value. This survey article summarizes the research direction of dataset quality in machine learning, including the definition of related concepts, analysis of quality issues and risks, and a review of dataset quality dimensions and metrics throughout the dataset lifecycle and a review of dataset quality metrics analyzed from a dataset lifecycle perspective and summarized in literatures. Furthermore, this article introduces a comprehensive quality evaluation process, which includes a framework for dataset quality evaluation with dimensions and metrics, computation methods for quality metrics, and assessment models. These studies provide valuable guidance for evaluating dataset quality in the field of machine learning, which can help improve the accuracy, efficiency, and generalization ability of machine learning models, and promote the development and application of artificial intelligence technology.} }
@article{WOS:001061249700006, title = {Perspective: Predicting and optimizing thermal transport properties with machine learning methods}, journal = {ENERGY AND AI}, volume = {8}, year = {2022}, issn = {2666-5468}, doi = {10.1016/j.egyai.2022.100153}, author = {Wei, Han and Bao, Hua and Ruan, Xiulin}, abstract = {In recent years, (big) data science has emerged as the ``fourth paradigm'' in physical science research. Data-driven techniques, e.g. machine learning, are advantageous in dealing with problems of high-dimensional features and complex mappings between quantities, which are otherwise of great difficulty or huge cost with other scientific paradigms. In the past five years or so, there has been a rapid growth of machine learning-assisted research on thermal transport. In this perspective, we review the recent progress in the intersection between machine learning and thermal transport, where machine learning methods generally serve as surrogate models for pre-dicting the thermal transport properties, or as tools for designing structures for the desired thermal properties and exploring thermal transport mechanisms. We provide perspectives about the advantages of machine learning methods in comparison to the physics-based methods for studying thermal transport properties. We also discuss how to improve the accuracy of predictive analytics and efficiency of structural optimization, to provide guid-ance for better utilizing machine learning-based methods to advance thermal transport research. Finally, we identify several outstanding challenges in this active area as well as opportunities for future developments, including developing machine learning methods suitable for small datasets, discovering effective physics-based descriptors, generating dataset from experiments and validating machine learning results with experiments, and making breakthroughs via discovering new physics.} }
@article{WOS:000653304100001, title = {Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm}, journal = {AGRICULTURE-BASEL}, volume = {11}, year = {2021}, doi = {10.3390/agriculture11050387}, author = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu, Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and Rahman, Sk Mostafizur}, abstract = {This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96\\% using RF, 94\\% using SVM and 63\\% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.} }
@article{WOS:001062545900001, title = {Forecasting realized volatility with machine learning: Panel data perspective}, journal = {JOURNAL OF EMPIRICAL FINANCE}, volume = {73}, pages = {251-271}, year = {2023}, issn = {0927-5398}, doi = {10.1016/j.jempfin.2023.07.003}, author = {Zhu, Haibin and Bai, Lu and He, Lidan and Liu, Zhi}, abstract = {Machine learning approaches have become very popular in many fields in this big data age. This paper considers the problem of forecasting realized volatility with machine learning using high-frequency data. Instead of treating the realized volatility as a univariate time series studied by many existing works in literature, we employ panel data analysis to improve forecasting accuracy in the short term. We use six effective machine-learning methods for the realized volatility panel data. We compare our results with the traditional linear-type models under the same panel data framework and with the single time series forecasting via the same machine learning methods. The results show that the panel-data-based machine learning method (PDML) outperforms the other methods.} }
@article{WOS:000489302600009, title = {Machine Learning and Sampling Scheme: An Empirical Study of Money Laundering Detection}, journal = {COMPUTATIONAL ECONOMICS}, volume = {54}, pages = {1043-1063}, year = {2019}, issn = {0927-7099}, doi = {10.1007/s10614-018-9864-z}, author = {Zhang, Yan and Trubey, Peter}, abstract = {This paper studies the interplay of machine learning and sampling scheme in an empirical analysis of money laundering detection algorithms. Using actual transaction data provided by a U.S. financial institution, we study five major machine learning algorithms including Bayes logistic regression, decision tree, random forest, support vector machine, and artificial neural network. As the incidence of money laundering events is rare, we apply and compare two sampling techniques that increase the relative presence of the events. Our analysis reveals potential advantages of machine learning algorithms in modeling money laundering events. This paper provides insights into the use of machine learning and sampling schemes in money laundering detection specifically, and classification of rare events in general.} }
@article{WOS:001218673600001, title = {Machine learning assisted Raman spectroscopy: A viable approach for the detection of microplastics}, journal = {JOURNAL OF WATER PROCESS ENGINEERING}, volume = {60}, year = {2024}, issn = {2214-7144}, doi = {10.1016/j.jwpe.2024.105150}, author = {Sunil, Megha and Pallikkavaliyaveetil, Nazreen and Mithun, N. . and Gopinath, Anu and Chidangil, Santhosh and Kumar, Satheesh and Lukose, Jijo}, abstract = {The accumulation of microplastics (MPs) resulting from disposal of plastic waste into water sources, poses a significant threat to aquatic organisms. These are readily ingested by organisms, leading to the accumulation of harmful substances, disrupting their biological processes. Current methods for identifying microplastics have notable drawbacks, including low resolution, extended imaging time, and restricted particle size analysis. Integrating Raman spectroscopy with machine learning (ML) proves to be an effective approach for identifying and classifying MPs, especially in scenarios where they are found in environmental media or mixed with various types. Machine learning (ML) can be vital tool in assisting Raman analysis, owing to its robust feature extraction capabilities. This comprehensive review outlined the utilization of various machine learning techniques in conjunction with Raman spectral features for diverse investigations related to microplastics. The methodologies discussed encompass Principal Component Analysis, K-Nearest Neighbour, Random Forest, Support Vector Machine, and various deep learning algorithms.} }
@article{WOS:000456150600012, title = {Multimodal Machine Learning: A Survey and Taxonomy}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {41}, pages = {423-443}, year = {2019}, issn = {0162-8828}, doi = {10.1109/TPAMI.2018.2798607}, author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe}, abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.} }
@article{WOS:000823410900001, title = {Quantum machine learning for support vector machine classification}, journal = {EVOLUTIONARY INTELLIGENCE}, volume = {17}, pages = {819-828}, year = {2024}, issn = {1864-5909}, doi = {10.1007/s12065-022-00756-5}, author = {Kavitha, S. S. and Kaulgud, Narasimha}, abstract = {Quantum machine learning aims to execute machine learning algorithms in quantum computers by utilizing powerful laws like superposition and entanglement for solving problems more efficiently. Support vector machine (SVM) is proved to be one of the most efficient classification machine learning algorithms in today's world. Since in classical systems, as datasets become complex or mixed up, the SVM kernel approach tends to slow and might fail. Hence our research is focused to examine the execution speed and accuracy of quantum support vector machines classification compared to classical SVM classification by proper quantum feature mapping selection. As the size of the dataset becomes complex, a proper feature map has to be selected to outperform or equally perform the classification. Hence the paper focuses on the selection of the best feature map for some benchmark datasets. Additionally experimental results show that the processing time of the algorithm is considerably reduced concerning classical machine learning. For evaluation of quantum computation over the classical computer, Quantum labs from the IBMQ quantum computer cloud have been used.} }
@article{WOS:000219166600025, title = {A Comparison of the Effects of K-Anonymity on Machine Learning Algorithms}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {5}, pages = {155-160}, year = {2014}, issn = {2158-107X}, author = {Wimmer, Hayden and Powell, Loreen}, abstract = {While research has been conducted in machine learning algorithms and in privacy preserving in data mining (PPDM), a gap in the literature exists which combines the aforementioned areas to determine how PPDM affects common machine learning algorithms. The aim of this research is to narrow this literature gap by investigating how a common PPDM algorithm, K-Anonymity, affects common machine learning and data mining algorithms, namely neural networks, logistic regression, decision trees, and Bayesian classifiers. This applied research reveals practical implications for applying PPDM to data mining and machine learning and serves as a critical first step learning how to apply PPDM to machine learning algorithms and the effects of PPDM on machine learning. Results indicate that certain machine learning algorithms are more suited for use with PPDM techniques.} }
@article{WOS:000959408400001, title = {Easy and fast prediction of green solvents for small molecule donor-based organic solar cells through machine learning}, journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS}, volume = {25}, pages = {10417-10426}, year = {2023}, issn = {1463-9076}, doi = {10.1039/d3cp00177f}, author = {Mahmood, Asif and Sandali, Yahya and Wang, Jin-Liang}, abstract = {Solubility plays a critical role in many aspects of research (drugs to materials). Solubility parameters are very useful for selecting appropriate solvents/non-solvents for various applications. In the present study, Hansen solubility parameters are predicted using machine learning. More than 40 machine models are tried in the search for the best model. Molecular descriptors and fingerprints are used as inputs to get a comparative view. Machine learning models trained using molecular descriptors have shown higher prediction ability than the model trained using molecular fingerprints. Machine learning models trained using molecular descriptors have shown their potential to be easy and fast models compared to the density functional theory (DFT)/thermodynamic approach. Machine learning creates a ``black box'' connection to the properties. Therefore, minimal computational cost is required. With the help of the best-trained machine learning model, green solvents are selected for small molecule donors that are used in organic solar cells. Our introduced framework can help to select solvents for organic solar cells in an easy and fast way.} }
@article{WOS:000657036100005, title = {Galaxy-ML: An accessible, reproducible, and scalable machine learning toolkit for biomedicine}, journal = {PLOS COMPUTATIONAL BIOLOGY}, volume = {17}, year = {2021}, issn = {1553-734X}, doi = {10.1371/journal.pcbi.1009014}, author = {Gu, Qiang and Kumar, Anup and Bray, Simon and Creason, Allison and Khanteymoori, Alireza and Jalili, Vahid and Gruening, Bjoern and Goecks, Jeremy}, abstract = {Supervised machine learning is an essential but difficult to use approach in biomedical data analysis. The Galaxy-ML toolkit (https://galaxyproject.org/community/machine-learning/) makes supervised machine learning more accessible to biomedical scientists by enabling them to perform end-to-end reproducible machine learning analyses at large scale using only a web browser. Galaxy-ML extends Galaxy (https://galaxyproject.org), a biomedical computational workbench used by tens of thousands of scientists across the world, with a suite of tools for all aspects of supervised machine learning.} }
@article{WOS:000449215200023, title = {Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data}, journal = {JAMA INTERNAL MEDICINE}, volume = {178}, pages = {1544-1547}, year = {2018}, issn = {2168-6106}, doi = {10.1001/jamainternmed.2018.3763}, author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela}, abstract = {A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.} }
@article{WOS:001129216300001, title = {Machine learning and deep learning for brain tumor MRI image segmentation}, journal = {EXPERIMENTAL BIOLOGY AND MEDICINE}, volume = {248}, pages = {1974-1992}, year = {2023}, issn = {1535-3702}, doi = {10.1177/15353702231214259}, author = {Khan, Md Kamrul Hasan and Guo, Wenjing and Liu, Jie and Dong, Fan and Li, Zoe and Patterson, Tucker A. and Hong, Huixiao}, abstract = {Brain tumors are often fatal. Therefore, accurate brain tumor image segmentation is critical for the diagnosis, treatment, and monitoring of patients with these tumors. Magnetic resonance imaging (MRI) is a commonly used imaging technique for capturing brain images. Both machine learning and deep learning techniques are popular in analyzing MRI images. This article reviews some commonly used machine learning and deep learning techniques for brain tumor MRI image segmentation. The limitations and advantages of the reviewed machine learning and deep learning methods are discussed. Even though each of these methods has a well-established status in their individual domains, the combination of two or more techniques is currently an emerging trend.} }
@article{WOS:000788104800007, title = {Advances in Machine Learning Approaches to Heart Failure with Preserved Ejection Fraction}, journal = {HEART FAILURE CLINICS}, volume = {18}, pages = {287-300}, year = {2022}, issn = {1551-7136}, doi = {10.1016/j.hfc.2021.12.002}, author = {Ahmad, Faraz S. and Luo, Yuan and Wehbe, Ramsey M. and Thomas, James D. and Shah, Sanjiv J.}, abstract = {center dot Machine learning has the potential to guide precision medicine approaches for heart failure with preserved ejection fraction, such as identification of rare causes, subphenotyping, and increasing the efficiency of clinical trial enrollment. center dot Understanding the strengths, limitations, and pitfalls of machine learning approaches is critical to realizing the potential of machine learning to impact the health of the patient with heart failure with preserved ejection fraction.} }
@article{WOS:000306390300001, title = {Machine learning and radiology}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {16}, pages = {933-951}, year = {2012}, issn = {1361-8415}, doi = {10.1016/j.media.2012.02.005}, author = {Wang, Shijun and Summers, Ronald M.}, abstract = {In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers. (c) 2012 Published by Elsevier B.V.} }
@article{WOS:000838308500001, title = {Machine learning for microalgae detection and utilization}, journal = {FRONTIERS IN MARINE SCIENCE}, volume = {9}, year = {2022}, doi = {10.3389/fmars.2022.947394}, author = {Ning, Hongwei and Li, Rui and Zhou, Teng}, abstract = {Microalgae are essential parts of marine ecology, and they play a key role in species balance. Microalgae also have significant economic value. However, microalgae are too tiny, and there are many different kinds of microalgae in a single drop of seawater. It is challenging to identify microalgae species and monitor microalgae changes. Machine learning techniques have achieved massive success in object recognition and classification, and have attracted a wide range of attention. Many researchers have introduced machine learning algorithms into microalgae applications, and similarly significant effects are gained. The paper summarizes recent advances based on various machine learning algorithms in microalgae applications, such as microalgae classification, bioenergy generation from microalgae, environment purification with microalgae, and microalgae growth monitor. Finally, we prospect development of machine learning algorithms in microalgae treatment in the future.} }
@article{WOS:000559371800001, title = {Legal requirements on explainability in machine learning}, journal = {ARTIFICIAL INTELLIGENCE AND LAW}, volume = {29}, pages = {149-169}, year = {2021}, issn = {0924-8463}, doi = {10.1007/s10506-020-09270-4}, author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Frenay, Benoit}, abstract = {Deep learning and other black-box models are becoming more and more popular today. Despite their high performance, they may not be accepted ethically or legally because of their lack of explainability. This paper presents the increasing number of legal requirements on machine learning model interpretability and explainability in the context of private and public decision making. It then explains how those legal requirements can be implemented into machine-learning models and concludes with a call for more inter-disciplinary research on explainability.} }
@article{WOS:000587801200006, title = {Machine learning in the optimization of robotics in the operative field}, journal = {CURRENT OPINION IN UROLOGY}, volume = {30}, pages = {808-816}, year = {2020}, issn = {0963-0643}, doi = {10.1097/MOU.0000000000000816}, author = {Ma, Runzhuo and Vanstrum, Erik B. and Lee, Ryan and Chen, Jian and Hung, Andrew J.}, abstract = {Purpose of review The increasing use of robotics in urologic surgery facilitates collection of `big data'. Machine learning enables computers to infer patterns from large datasets. This review aims to highlight recent findings and applications of machine learning in robotic-assisted urologic surgery. Recent findings Machine learning has been used in surgical performance assessment and skill training, surgical candidate selection, and autonomous surgery. Autonomous segmentation and classification of surgical data have been explored, which serves as the stepping-stone for providing real-time surgical assessment and ultimately, improve surgical safety and quality. Predictive machine learning models have been created to guide appropriate surgical candidate selection, whereas intraoperative machine learning algorithms have been designed to provide 3-D augmented reality and real-time surgical margin checks. Reinforcement-learning strategies have been utilized in autonomous robotic surgery, and the combination of expert demonstrations and trial-and-error learning by the robot itself is a promising approach towards autonomy. Robot-assisted urologic surgery coupled with machine learning is a burgeoning area of study that demonstrates exciting potential. However, further validation and clinical trials are required to ensure the safety and efficacy of incorporating machine learning into surgical practice.} }
@article{WOS:000571725400014, title = {Robust Coreset Construction for Distributed Machine Learning}, journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS}, volume = {38}, pages = {2400-2417}, year = {2020}, issn = {0733-8716}, doi = {10.1109/JSAC.2020.3000373}, author = {Lu, Hanlin and Li, Ming-Ju and He, Ting and Wang, Shiqiang and Narayanan, Vijaykrishnan and Chan, Kevin S.}, abstract = {Coreset, which is a summary of the original dataset in the form of a small weighted set in the same sample space, provides a promising approach to enable machine learning over distributed data. Although viewed as a proxy of the original dataset, each coreset is only designed to approximate the cost function of a specific machine learning problem, and thus different coresets are often required to solve different machine learning problems, increasing the communication overhead. We resolve this dilemma by developing robust coreset construction algorithms that can support a variety of machine learning problems. Motivated by empirical evidence that suitably-weighted k-clustering centers provide a robust coreset, we harden the observation by establishing theoretical conditions under which the coreset provides a guaranteed approximation for a broad range of machine learning problems, and developing both centralized and distributed algorithms to generate coresets satisfying the conditions. The robustness of the proposed algorithms is verified through extensive experiments on diverse datasets with respect to both supervised and unsupervised learning problems.} }
@article{WOS:000505697300001, title = {Machine learning and the physical sciences}, journal = {REVIEWS OF MODERN PHYSICS}, volume = {91}, year = {2019}, issn = {0034-6861}, doi = {10.1103/RevModPhys.91.045002}, author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborova, Lenka}, abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.} }
@article{WOS:001319107600001, title = {Applications of machine learning to behavioral sciences: focus on categorical data}, journal = {DISCOVER PSYCHOLOGY}, volume = {2}, year = {2022}, doi = {10.1007/s44202-022-00027-5}, author = {Dehghan, Pegah and Alashwal, Hany and Moustafa, Ahmed A.}, abstract = {In the last two decades, advancements in artificial intelligence and data science have attracted researchers' attention to machine learning. Growing interests in applying machine learning algorithms can be observed in different scientific areas, including behavioral sciences. However, most of the research conducted in this area applied machine learning algorithms to imagining and physiological data such as EEG and fMRI and there are relatively limited non-imaging and non-physiological behavioral studies which have used machine learning to analyze their data. Therefore, in this perspective article, we aim to (1) provide a general understanding of models built for inference, models built for prediction (i.e., machine learning), methods used in these models, and their strengths and limitations; (2) investigate the applications of machine learning to categorical data in behavioral sciences; and (3) highlight the usefulness of applying machine learning algorithms to non-imaging and non-physiological data (e.g., clinical and categorical) data and provide evidence to encourage researchers to conduct further machine learning studies in behavioral and clinical sciences.} }
@article{WOS:000684698800020, title = {Performance Analysis on Machine Learning-Based Channel Estimation}, journal = {IEEE TRANSACTIONS ON COMMUNICATIONS}, volume = {69}, pages = {5183-5193}, year = {2021}, issn = {0090-6778}, doi = {10.1109/TCOMM.2021.3083597}, author = {Mei, Kai and Liu, Jun and Zhang, Xiaochen and Rajatheva, Nandana and Wei, Jibo}, abstract = {Recently, machine learning-based channel estimation has attracted much attention. The performance of machine learning-based estimation has been validated by simulation experiments. However, little attention has been paid to the theoretical performance analysis. In this paper, we investigate the mean square error (MSE) performance of machine learning-based estimation. Hypothesis testing is employed to analyze its MSE upper bound. Furthermore, we build a statistical model for hypothesis testing, which holds when the linear learning module with a low input dimension is used in machine learning-based channel estimation, and derive a clear analytical relation between the size of the training data and performance. Then, we simulate the machine learning-based channel estimation in orthogonal frequency division multiplexing (OFDM) systems to verify our analysis results. Finally, the design considerations for the situation where only limited training data is available are discussed. In this situation, our analysis results can be applied to assess the performance and support the design of machine learning-based channel estimation.} }
@article{WOS:000822914300001, title = {Estimating the thermal conductivity of soils using six machine learning algorithms}, journal = {INTERNATIONAL COMMUNICATIONS IN HEAT AND MASS TRANSFER}, volume = {136}, year = {2022}, issn = {0735-1933}, doi = {10.1016/j.icheatmasstransfer.2022.106139}, author = {Li, Kai-Qi and Liu, Yong and Kang, Qing}, abstract = {Many machine learning algorithms have been applied to determine soil properties in recent years. However, the prediction performances of thermal conductivity of soils via machine learning algorithms are still unclear due to the lack of sufficient databases. In this work, a large database containing 2197 data points from various literature was compiled. Six machine learning algorithms, namely multivariance linear regression (MLR), Gaussian process regression (GPR), support vector machine (SVM), decision tree (DT), random forest (RF) and adaptive boosting methods (AdaBoost) were implemented to predict the thermal conductivity of soils based on the compiled database. In addition, Spearman correlation analysis and grey relational analysis were adopted to compute the strength of influencing factors in thermal conductivity. For obtaining the best prediction model, the performances of six machine learning algorithms were assessed via eight evaluation indicators and compared with three typical empirical models. Results show that the AdaBoost can yield good predicted values of the thermal conductivity of soils with minimum errors (i.e., RMSE = 0.099). This study constructs a database of thermal conductivity and provides a reference for evaluating the thermal conductivity of soils via machine learning algorithms.} }
@article{WOS:001153683600001, title = {An overview of clinical machine learning applications in neurology}, journal = {JOURNAL OF THE NEUROLOGICAL SCIENCES}, volume = {455}, year = {2023}, issn = {0022-510X}, doi = {10.1016/j.jns.2023.122799}, author = {Smith, Colin M. and Weathers, Allison L. and Lewis, Steven L.}, abstract = {Machine learning techniques for clinical applications are evolving, and the potential impact this will have on clinical neurology is important to recognize. By providing a broad overview on this growing paradigm of clinical tools, this article aims to help healthcare professionals in neurology prepare to navigate both the opportunities and challenges brought on through continued advancements in machine learning. This narrative review first elaborates on how machine learning models are organized and implemented. Machine learning tools are then classified by clinical application, with examples of uses within neurology described in more detail. Finally, this article addresses limitations and considerations regarding clinical machine learning applications in neurology.} }
@article{WOS:000562067400001, title = {Prediction of Breast Cancer, Comparative Review of Machine Learning Techniques, and Their Analysis}, journal = {IEEE ACCESS}, volume = {8}, pages = {150360-150376}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3016715}, author = {Fatima, Noreen and Liu, Li and Hong, Sha and Ahmed, Haroon}, abstract = {Breast cancer is type of tumor that occurs in the tissues of the breast. It is most common type of cancer found in women around the world and it is among the leading causes of deaths in women. This article presents the comparative analysis of machine learning, deep learning and data mining techniques being used for the prediction of breast cancer. Many researchers have put their efforts on breast cancer diagnoses and prognoses, every technique has different accuracy rate and it varies for different situations, tools and datasets being used. Our main focus is to comparatively analyze different existing Machine Learning and Data Mining techniques in order to find out the most appropriate method that will support the large dataset with good accuracy of prediction. The main purpose of this review is to highlight all the previous studies of machine learning algorithms that are being used for breast cancer prediction and this article provides the all necessary information to the beginners who want to analyze the machine learning algorithms to gain the base of deep learning.} }
@article{WOS:000802148900003, title = {Machine-Learning-Based Lightpath QoT Estimation and Forecasting}, journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY}, volume = {40}, pages = {3115-3127}, year = {2022}, issn = {0733-8724}, doi = {10.1109/JLT.2022.3160379}, author = {Allogba, Stephanie and Aladin, Sandra and Tremblay, Christine}, abstract = {Machine learning (ML) is more and more used to address the challenges of managing the physical layer of increasingly heterogeneous and complex optical networks. In this tutorial, we illustrate how simple and more sophisticated machine learning methods can be used in lightpath quality of transmission (QoT) estimation and forecast tasks. We also discuss data processing strategies with the aim to determine relevant features to feed the ML classifiers and predictors. We then introduce a preliminary study on the application of transfer learning to try to overcome the scarcity of field data.} }
@article{WOS:000943845300001, title = {Machine learning on protein-protein interaction prediction: models, challenges and trends}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {24}, year = {2023}, issn = {1467-5463}, doi = {10.1093/bib/bbad076}, author = {Tang, Tao and Zhang, Xiaocai and Liu, Yuansheng and Peng, Hui and Zheng, Binshuang and Yin, Yanlin and Zeng, Xiangxiang}, abstract = {Protein-protein interactions (PPIs) carry out the cellular processes of all living organisms. Experimental methods for PPI detection suffer from high cost and false-positive rate, hence efficient computational methods are highly desirable for facilitating PPI detection. In recent years, benefiting from the enormous amount of protein data produced by advanced high-throughput technologies, machine learning models have been well developed in the field of PPI prediction. In this paper, we present a comprehensive survey of the recently proposed machine learning-based prediction methods. The machine learning models applied in these methods and details of protein data representation are also outlined. To understand the potential improvements in PPI prediction, we discuss the trend in the development of machine learning-based methods. Finally, we highlight potential directions in PPI prediction, such as the use of computationally predicted protein structures to extend the data source for machine learning models. This review is supposed to serve as a companion for further improvements in this field.} }
@article{WOS:001013826800001, title = {Enhancing Social Media Platforms with Machine Learning Algorithms and Neural Networks}, journal = {ALGORITHMS}, volume = {16}, year = {2023}, doi = {10.3390/a16060271}, author = {Taherdoost, Hamed}, abstract = {Network analysis aids management in reducing overall expenditures and maintenance workload. Social media platforms frequently use neural networks to suggest material that corresponds with user preferences. Machine learning is one of many methods for social network analysis. Machine learning algorithms operate on a collection of observable features that are taken from user data. Machine learning and neural network-based systems represent a topic of study that spans several fields. Computers can now recognize the emotions behind particular content uploaded by users to social media networks thanks to machine learning. This study examines research on machine learning and neural networks, with an emphasis on social analysis in the context of the current literature.} }
@article{WOS:000326889800022, title = {A survey of multi-view machine learning}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {23}, pages = {2031-2038}, year = {2013}, issn = {0941-0643}, doi = {10.1007/s00521-013-1362-6}, author = {Sun, Shiliang}, abstract = {Multi-view learning or learning with multiple distinct feature sets is a rapidly growing direction in machine learning with well theoretical underpinnings and great practical success. This paper reviews theories developed to understand the properties and behaviors of multi-view learning and gives a taxonomy of approaches according to the machine learning mechanisms involved and the fashions in which multiple views are exploited. This survey aims to provide an insightful organization of current developments in the field of multi-view learning, identify their limitations, and give suggestions for further research. One feature of this survey is that we attempt to point out specific open problems which can hopefully be useful to promote the research of multi-view machine learning.} }
@article{WOS:000704376900007, title = {Federated learning on non-IID data: A survey}, journal = {NEUROCOMPUTING}, volume = {465}, pages = {371-390}, year = {2021}, issn = {0925-2312}, doi = {10.1016/j.neucom.2021.07.098}, author = {Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu}, abstract = {Federated learning is an emerging distributed machine learning framework for privacy preservation. However, models trained in federated learning usually have worse performance than those trained in the standard centralized learning mode, especially when the training data are not independent and identically distributed (Non-IID) on the local devices. In this survey, we provide a detailed analysis of the influence of Non-IID data on both parametric and non-parametric machine learning models in both horizontal and vertical federated learning. In addition, current research work on handling challenges of NonIID data in federated learning are reviewed, and both advantages and disadvantages of these approaches are discussed. Finally, we suggest several future research directions before concluding the paper. (c) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000899349300014, title = {Asset Management in Machine Learning: State-of-research and State-of-practice}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3543847}, author = {Idowu, Samuel and Struber, Daniel and Berger, Thorsten}, abstract = {Machine learning components are essential for today's software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets. This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning assetmanagement. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces (CLIs). We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.} }
@article{WOS:000488199200019, title = {Online sequential class-specific extreme learning machine for binary imbalanced learning}, journal = {NEURAL NETWORKS}, volume = {119}, pages = {235-248}, year = {2019}, issn = {0893-6080}, doi = {10.1016/j.neunet.2019.08.018}, author = {Shukla, Sanyam and Raghuwanshi, Bhagat Singh}, abstract = {Many real-world applications suffer from the class imbalance problem, in which some classes have significantly fewer examples compared to the other classes. In this paper, we focus on online sequential learning methods, which are considerably more preferable to tackle the large size imbalanced classification problems effectively. For example, weighted online sequential extreme learning machine (WOS-ELM), voting based weighted online sequential extreme learning machine (VWOS-ELM) and weighted online sequential extreme learning machine with kernels (WOS-ELMK), etc. handle the imbalanced learning effectively. One of our recent works class-specific extreme learning machine (CS-ELM) uses class-specific regularization and has been shown to perform better for imbalanced learning. This work proposes a novel online sequential class-specific extreme learning machine (OSCSELM), which is a variant of CS-ELM. OSCSELM supports online learning technique in both chunkby-chunk and one-by-one learning mode. It targets to handle the class imbalance problem for both small and larger datasets. The proposed work has less computational complexity in contrast with WOS-ELM for imbalanced learning. The proposed method is assessed by utilizing benchmark real-world imbalanced datasets. Experimental results illustrate the effectiveness of the proposed approach as it outperforms the other methods for imbalanced learning. (C) 2019 Elsevier Ltd. All rights reserved.} }
@article{WOS:001333821800001, title = {Yoked learning in molecular data science}, journal = {ARTIFICIAL INTELLIGENCE IN THE LIFE SCIENCES}, volume = {5}, year = {2024}, doi = {10.1016/j.ailsci.2023.100089}, author = {Li, Zhixiong and Xiang, Yan and Wen, Yujing and Reker, Daniel}, abstract = {Active machine learning is an established and increasingly popular experimental design technique where the machine learning model can request additional data to improve the model's predictive performance. It is generally assumed that this data is optimal for the machine learning model since it relies on the model's predictions or model architecture and therefore cannot be transferred to other models. Inspired by research in pedagogy, we here introduce the concept of yoked machine learning where a second machine learning model learns from the data selected by another model. We found that in 48\\% of the benchmarked combinations, yoked learning performed similar or better than active learning. We analyze distinct cases in which yoked learning can improve active learning performance. In particular, we prototype yoked deep learning (YoDeL) where a classic machine learning model provides data to a deep neural network, thereby mitigating challenges of active deep learning such as slow refitting time per learning iteration and poor performance on small datasets. In summary, we expect the new concept of yoked (deep) learning to provide a competitive option to boost the performance of active learning and benefit from distinct capabilities of multiple machine learning models during data acquisition, training, and deployment.} }
@article{WOS:000688449200012, title = {Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges}, journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS}, volume = {23}, pages = {1759-1799}, year = {2021}, doi = {10.1109/COMST.2021.3090430}, author = {Khan, Latif U. and Saad, Walid and Han, Zhu and Hossain, Ekram and Hong, Choong Seon}, abstract = {The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithm for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Finally, we present several open research challenges with their possible solutions.} }
@article{WOS:000804965300003, title = {A systematic review on machine learning models for online learning and examination systems}, journal = {PEERJ COMPUTER SCIENCE}, volume = {8}, year = {2022}, doi = {10.7717/peerj-cs.986}, author = {Kaddoura, Sanaa and Popescu, Daniela Elena and Hemanth, Jude D.}, abstract = {Examinations or assessments play a vital role in every student's life; they determine their future and career paths. The COVID pandemic has left adverse impacts in all areas, including the academic field. The regularized classroom learning and face-to-face real-time examinations were not feasible to avoid widespread infection and ensure safety. During these desperate times, technological advancements stepped in to aid students in continuing their education without any academic breaks. Machine learning is a key to this digital transformation of schools or colleges from real-time to online mode. Online learning and examination during lockdown were made possible by Machine learning methods. In this article, a systematic review of the role of Machine learning in Lockdown Exam Management Systems was conducted by evaluating 135 studies over the last five years. The significance of Machine learning in the entire exam cycle from pre-exam preparation, conduction of examination, and evaluation were studied and discussed. The unsupervised or supervised Machine learning algorithms were identified and categorized in each process. The primary aspects of examinations, such as authentication, scheduling, proctoring, and cheat or fraud detection, are investigated in detail with Machine learning perspectives. The main attributes, such as prediction of at-risk students, adaptive learning, and monitoring of students, are integrated for more understanding of the role of machine learning in exam preparation, followed by its management of the post-examination process. Finally, this review concludes with issues and challenges that machine learning imposes on the examination system, and these issues are discussed with solutions.} }
@article{WOS:000460685600001, title = {Machine (Deep) Learning Methods for Image Processing and Radiomics}, journal = {IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES}, volume = {3}, pages = {104-108}, year = {2019}, issn = {2469-7311}, doi = {10.1109/TRPMS.2019.2899538}, author = {Hatt, Mathieu and Parmar, Chintan and Qi, Jinyi and El Naqa, Issam}, abstract = {Methods from the field of machine (deep) learning have been successful in tackling a number of tasks in medical imaging, from image reconstruction or processing to predictive modeling, clinical planning and decision-aid systems. The ever growing availability of data and the improving ability of algorithms to learn from them has led to the rise of methods based on neural networks to address most of these tasks with higher efficiency and often superior performance than previous, ``shallow'' machine learning methods. The present editorial aims at contextualizing within this framework the recent developments of these techniques, including these described in the papers published in the present special issue on machine (deep) learning for image processing and radiomics in radiation-based medical sciences.} }
@article{WOS:001210794000001, title = {Multi-agent modelling and analysis of the knowledge learning of a human-machine hybrid intelligent organization with human-machine trust}, journal = {SYSTEMS SCIENCE \\& CONTROL ENGINEERING}, volume = {12}, year = {2024}, doi = {10.1080/21642583.2024.2343301}, author = {Xue, Chaogai and Zhang, Haoxiang and Cao, Haiwang}, abstract = {Machine learning (ML) technologies have changed the paradigm of knowledge discovery in organizations and transformed traditional organizational learning to human-machine hybrid intelligent organizational learning. However, the general distrust among humans towards knowledge derived from machine learning has hindered effective knowledge exchange between humans and machines, thereby compromising the efficiency of human-machine hybrid intelligent organizational learning. To explore this issue, we used multi-agent simulation to construct a knowledge learning model of a human-machine hybrid intelligent organization with human-machine trust. The simulation showed that whether human-machine trust has a positive effect on knowledge level depends on the initial input and the magnitude of the effect depends on the human learning propensity (exploration and exploitation). When humans reconfigure machine learning excessively, whether human-machine trust has a positive effect on the knowledge level depends on human learning propensity (exploration and exploitation). Maintaining appropriate human-machine trust in turbulent environments assists humans in integrating diverse knowledge to meet changing knowledge needs. Our study extends the human-machine hybrid intelligence organizational learning model by modeling human-machine trust. It will assist managers in effectively designing the most economical level of human-machine trust, thereby enhancing the efficiency of human-machine collaboration in human-machine hybrid intelligent organization.} }
@article{WOS:000680962300001, title = {Machine-Learning-Assisted Intelligent Imaging Flow Cytometry: A Review}, journal = {ADVANCED INTELLIGENT SYSTEMS}, volume = {3}, year = {2021}, doi = {10.1002/aisy.202100073}, author = {Luo, Shaobo and Shi, Yuzhi and Chin, Lip Ket and Hutchinson, Paul Edward and Zhang, Yi and Chierchia, Giovanni and Talbot, Hugues and Jiang, Xudong and Bourouina, Tarik and Liu, Ai-Qun}, abstract = {Imaging flow cytometry has been widely adopted in numerous applications such as optical sensing, environmental monitoring, clinical diagnostics, and precision agriculture. The system, with the assistance of machine learning, shows unprecedented advantages in automated image analysis, thus enabling high-throughput measurement, identification, and sorting of biological entities. Recently, with the burgeoning developments of machine learning algorithms, deep learning has taken over most of data analysis and promised tremendous performance in intelligent imaging flow cytometry. Herein, an overview of the basic knowledge of intelligent imaging flow cytometry, the evolution of machine learning and the typical applications, and how machine learning can be applied to assist intelligent imaging flow cytometry is provided. Perspectives of emerging machine learning algorithms in implementing future intelligent imaging flow cytometry are also discussed.} }
@article{WOS:000801156200002, title = {Machine Learning for Cataract Classification/Grading on Ophthalmic Imaging Modalities: A Survey}, journal = {MACHINE INTELLIGENCE RESEARCH}, volume = {19}, pages = {184-208}, year = {2022}, issn = {2731-538X}, doi = {10.1007/s11633-022-1329-0}, author = {Zhang, Xiao-Qing and Hu, Yan and Xiao, Zun-Jie and Fang, Jian-Sheng and Higashita, Risa and Liu, Jiang}, abstract = {Cataracts are the leading cause of visual impairment and blindness globally. Over the years, researchers have achieved significant progress in developing state-of-the-art machine learning techniques for automatic cataract classification and grading, aiming to prevent cataracts early and improve clinicians' diagnosis efficiency. This survey provides a comprehensive survey of recent advances in machine learning techniques for cataract classification/grading based on ophthalmic images. We summarize existing literature from two research directions: conventional machine learning methods and deep learning methods. This survey also provides insights into existing works of both merits and limitations. In addition, we discuss several challenges of automatic cataract classification/grading based on machine learning techniques and present possible solutions to these challenges for future research.} }
@article{WOS:000725106400001, title = {Applying machine learning algorithms to predict default probability in the online credit market: Evidence from China}, journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS}, volume = {79}, year = {2022}, issn = {1057-5219}, doi = {10.1016/j.irfa.2021.101971}, author = {Liu, Yi and Yang, Menglong and Wang, Yudong and Li, Yongshan and Xiong, Tiancheng and Li, Anzhe}, abstract = {Using data from Renrendai and three machine learning algorithms, namely, k-nearest neighbor, support vector machine, and random forest, we predicted the default probability of online loan borrowers and compared their prediction performance with that of a logistic model. The results show that, first, based on the AUC (area under the ROC curve) value, accuracy rate and Brier score, the machine learning models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the machine learning algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that the machine learning algorithms can provide more benefits to investors.} }
@article{WOS:000559380900005, title = {Machine learning-driven new material discovery}, journal = {NANOSCALE ADVANCES}, volume = {2}, pages = {3115-3130}, year = {2020}, issn = {2516-0230}, doi = {10.1039/d0na00388c}, author = {Cai, Jiazhen and Chu, Xuan and Xu, Kun and Li, Hongbo and Wei, Jing}, abstract = {New materials can bring about tremendous progress in technology and applications. However, the commonly used trial-and-error method cannot meet the current need for new materials. Now, a newly proposed idea of using machine learning to explore new materials is becoming popular. In this paper, we review this research paradigm of applying machine learning in material discovery, including data preprocessing, feature engineering, machine learning algorithms and cross-validation procedures. Furthermore, we propose to assist traditional DFT calculations with machine learning for material discovery. Many experiments and literature reports have shown the great effects and prospects of this idea. It is currently showing its potential and advantages in property prediction, material discovery, inverse design, corrosion detection and many other aspects of life.} }
@article{WOS:001016803900001, title = {Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions}, journal = {IEEE ACCESS}, volume = {11}, pages = {60153-60170}, year = {2023}, issn = {2169-3536}, doi = {10.1109/ACCESS.2023.3286344}, author = {Mandalapu, Varun and Elluri, Lavanya and Vyas, Piyush and Roy, Nirmalya}, abstract = {Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime prediction techniques, law enforcement agencies can develop strategies to prevent and respond to criminal activities more effectively.} }
@article{WOS:000431709400001, title = {Automatic Differentiation in Machine Learning: a Survey}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {18}, year = {2018}, issn = {1532-4435}, author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark}, abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ``autodiff'', is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ``dynamic computational graphs'' and ``differentiable programming''. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ``autodiff'', ``automatic differentiation'', and ``symbolic differentiation'' as these are encountered more and more in machine learning settings.} }
@article{WOS:001395340500013, title = {Recent Advances in Optimal Transport for Machine Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {47}, pages = {1161-1180}, year = {2025}, issn = {0162-8828}, doi = {10.1109/TPAMI.2024.3489030}, author = {Montesuma, Eduardo Fernandes and Mboula, Fred Maurice Ngole and Souloumiac, Antoine}, abstract = {Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 - 2023, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport and its extensions, such as partial, unbalanced, Gromov and Neural Optimal Transport, and its interplay with Machine Learning practice.} }
@article{WOS:000297611600005, title = {Voting based extreme learning machine}, journal = {INFORMATION SCIENCES}, volume = {185}, pages = {66-77}, year = {2012}, issn = {0020-0255}, doi = {10.1016/j.ins.2011.09.015}, author = {Cao, Jiuwen and Lin, Zhiping and Huang, Guang-Bin and Liu, Nan}, abstract = {This paper proposes an improved learning algorithm for classification which is referred to as voting based extreme learning machine. The proposed method incorporates the voting method into the popular extreme learning machine (ELM) in classification applications. Simulations on many real world classification datasets have demonstrated that this algorithm generally outperforms the original ELM algorithm as well as several recent classification algorithms. (C) 2011 Elsevier Inc. All rights reserved.} }
@article{WOS:000617870300006, title = {Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment}, journal = {WIRELESS PERSONAL COMMUNICATIONS}, volume = {117}, pages = {3505-3525}, year = {2021}, issn = {0929-6212}, doi = {10.1007/s11277-021-08284-8}, author = {Chen Hongsong and Zhang Yongpeng and Cao Yongrui and Bhargava, Bharat}, abstract = {Under big data environment, machine learning has been rapidly developed and widely used. It has been successfully applied in computer vision, natural language processing, computer security and other application fields. However, there are many security problems in machine learning under big data environment. For example, attackers can add ``poisoned'' sample to the data source, and big data process system will process these ``poisoned'' sample and use machine learning methods to train model, which will directly lead to wrong prediction results. In this paper, machine learning system and machine learning pipeline are proposed. The security problems that maybe occur in each stage of machine learning system under big data processing pipeline are analyzed comprehensively. We use four different attack methods to compare the attack experimental results.The security problems are classified comprehensively, and the defense approaches to each security problem are analyzed. Drone-deploy MapEngine is selected as a case study, we analyze the security threats and defense approaches in the Drone-Cloud machine learning application envirolment. At last,the future development drections of security issues and challenages in the machine learning system are proposed.} }
@article{WOS:000838497700001, title = {Machine Learning in Precision Agriculture: A Survey on Trends, Applications and Evaluations Over Two Decades}, journal = {IEEE ACCESS}, volume = {10}, pages = {73786-73803}, year = {2022}, issn = {2169-3536}, doi = {10.1109/ACCESS.2022.3188649}, author = {Condran, Sarah and Bewong, Michael and Islam, Md Zahidul and Maphosa, Lancelot and Zheng, Lihong}, abstract = {Precision agriculture represents the new age of conventional agriculture. This is made possible by the advancement of various modern technologies such as the internet of things. The unparalleled potential for data collection and analytics has resulted in an increase in multi-disciplinary research within machine learning and agriculture. However, the application of machine learning techniques to agriculture seems to be out of step with core machine learning research. This gap is further exacerbated by the inherent challenges associated with agricultural data. In this work, we conduct a systematic review of a large body of academic literature published between 2000 and 2022, on the application of machine learning techniques to agriculture. We identify and discuss some of the key data issues such as class imbalance, data sparsity and high dimensionality. Further, we study the impact of these data issues on various machine learning approaches within the context of agriculture. Finally, we identify some of the common pitfalls in the machine learning and agriculture research including the misapplication of machine learning evaluation techniques. To this end, this survey presents a holistic view on the state of affairs in the cross-domain of machine learning and agriculture and proposes some suitable mitigation strategies to address these challenges.} }
@article{WOS:000556625500002, title = {Digital Pharmaceutical Sciences}, journal = {AAPS PHARMSCITECH}, volume = {21}, year = {2020}, issn = {1530-9932}, doi = {10.1208/s12249-020-01747-4}, author = {Damiati, Safa A.}, abstract = {Artificial intelligence (AI) and machine learning, in particular, have gained significant interest in many fields, including pharmaceutical sciences. The enormous growth of data from several sources, the recent advances in various analytical tools, and the continuous developments in machine learning algorithms have resulted in a rapid increase in new machine learning applications in different areas of pharmaceutical sciences. This review summarizes the past, present, and potential future impacts of machine learning technologies on different areas of pharmaceutical sciences, including drug design and discovery, preformulation, and formulation. The machine learning methods commonly used in pharmaceutical sciences are discussed, with a specific emphasis on artificial neural networks due to their capability to model the nonlinear relationships that are commonly encountered in pharmaceutical research. AI and machine learning technologies in common day-to-day pharma needs as well as industrial and regulatory insights are reviewed. Beyond traditional potentials of implementing digital technologies using machine learning in the development of more efficient, fast, and economical solutions in pharmaceutical sciences are also discussed.} }
@article{WOS:000678520000001, title = {The absorption and multiplication of uncertainty in machine-learning-driven finance}, journal = {BRITISH JOURNAL OF SOCIOLOGY}, volume = {72}, pages = {1015-1029}, year = {2021}, issn = {0007-1315}, doi = {10.1111/1468-4446.12880}, author = {Hansen, Kristian Bondo and Borch, Christian}, abstract = {Uncertainty about market developments and their implications characterize financial markets. Increasingly, machine learning is deployed as a tool to absorb this uncertainty and transform it into manageable risk. This article analyses machine-learning-based uncertainty absorption in financial markets by drawing on 182 interviews in the finance industry, including 45 interviews with informants who were actively applying machine-learning techniques to investment management, trading, or risk management problems. We argue that while machine-learning models are deployed to absorb financial uncertainty, they also introduce a new and more profound type of uncertainty, which we call critical model uncertainty. Critical model uncertainty refers to the inability to explain how and why the machine-learning models (particularly neural networks) arrive at their predictions and decisions-their uncertainty-absorbing accomplishments. We suggest that the dialectical relation between machine-learning models' uncertainty absorption and multiplication calls for further research in the field of finance and beyond.} }
@article{WOS:000502568900010, title = {Using machine learning to explain the heterogeneity of schizophrenia. Realizing the promise and avoiding the hype}, journal = {SCHIZOPHRENIA RESEARCH}, volume = {214}, pages = {70-75}, year = {2019}, issn = {0920-9964}, doi = {10.1016/j.schres.2019.08.032}, author = {Tandon, Neeraj and Tandon, Rajiv}, abstract = {Despite extensive research and prodigious advances in neuroscience, our comprehension of the nature of schizophrenia remains rudimentary. Our failure to make progress is attributed to the extreme heterogeneity of this condition, enormous complexity of the human brain, limitations of extant research paradigms, and inadequacy of traditional statistical methods to integrate or interpret increasingly large amounts of multidimensional information relevant to unravelling brain function. Fortunately, the rapidly developing science of machine learning appears to provide tools capable of addressing each of these impediments. Enthusiasm about the potential of machine learning methods to break the current impasse is reflected in the steep increase in the number of scientific publication about the application of machine learning to the study of schizophrenia. Machine learning approaches are, however, poorly understood by schizophrenia researchers and clinicians alike. In this paper, we provide a simple description of the nature and techniques of machine learning and their application to the study of schizophrenia. We then summarize its potential and constraints with illustrations from six studies of machine learning in schizophrenia and address some common misconceptions about machine learning. We suggest some guidelines for researchers, readers, science editors and reviewers of the burgeoning machine learning literature in schizophrenia. In order to realize its enormous promise, we suggest the need for the disciplined application of machine learning methods to the study of schizophrenia with a dear recognition of its capability and challenges accompanied by a concurrent effort to improve machine learning literacy among neuroscientists and mental health professionals. (C) 2019 Elsevier B.V. All rights reserved.} }
@article{WOS:000524750000009, title = {Analysis of Dimensionality Reduction Techniques on Big Data}, journal = {IEEE ACCESS}, volume = {8}, pages = {54776-54788}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.2980942}, author = {Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and Baker, Thar}, abstract = {Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.} }
@article{WOS:000550878000001, title = {How can machine learning aid behavioral marketing research?}, journal = {MARKETING LETTERS}, volume = {31}, pages = {361-370}, year = {2020}, issn = {0923-0645}, doi = {10.1007/s11002-020-09535-7}, author = {Hagen, Linda and Uetake, Kosuke and Yang, Nathan and Bollinger, Bryan and Chaney, Allison J. B. and Dzyabura, Daria and Etkin, Jordan and Goldfarb, Avi and Liu, Liu and Sudhir, K. and Wang, Yanwen and Wright, James R. and Zhu, Ying}, abstract = {Behavioral science and machine learning have rapidly progressed in recent years. As there is growing interest among behavioral scholars to leverage machine learning, we present strategies for how these methods that can be of value to behavioral scientists using examples centered on behavioral research.} }
@article{WOS:000444659200024, title = {Wild patterns: Ten years after the rise of adversarial machine learning}, journal = {PATTERN RECOGNITION}, volume = {84}, pages = {317-331}, year = {2018}, issn = {0031-3203}, doi = {10.1016/j.patcog.2018.07.023}, author = {Biggio, Battista and Roli, Fabio}, abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:001091638200001, title = {Machine learning in marketing: Recent progress and future research directions}, journal = {JOURNAL OF BUSINESS RESEARCH}, volume = {170}, year = {2024}, issn = {0148-2963}, doi = {10.1016/j.jbusres.2023.114254}, author = {Herhausen, Dennis and Bernritter, Stefan F. and Ngai, Eric W. T. and Kumar, Ajay and Delen, Dursun}, abstract = {Decision-making in marketing has changed dramatically in the past decade. Companies increasingly use algorithms to generate predictions for marketing decisions, such as which consumers to target with which offers. Such algorithmic decision-making promises to make marketing more intelligent, efficient, consumer-friendly, and, ultimately, more effective. Not surprisingly, machine learning is a trending topic for marketing researchers and practitioners. However, machine learning also introduces important challenges to the marketing landscape. We discuss this development by outlining recent progress and future research directions of machine learning in marketing. Specifically, we provide an overview of typical machine learning applications in marketing and present a guiding framework. We position the articles in the Journal of Business Research's Special Issue on ``Machine Learning in Marketing'' within this framework and conclude by putting forward a research agenda to further guide future research in this area.} }
@article{WOS:000703023000002, title = {Machine Learning Approaches to Retrieve High-Quality, Clinically Relevant Evidence From the Biomedical Literature: Systematic Review}, journal = {JMIR MEDICAL INFORMATICS}, volume = {9}, year = {2021}, doi = {10.2196/30401}, author = {Abdelkader, Wael and Navarro, Tamara and Parrish, Rick and Cotoi, Chris and Germini, Federico and Iorio, Alfonso and Haynes, R. Brian and Lokker, Cynthia}, abstract = {Background: The rapid growth of the biomedical literature makes identifying strong evidence a time-consuming task. Applying machine learning to the process could be a viable solution that limits effort while maintaining accuracy. Objective: The goal of the research was to summarize the nature and comparative performance of machine learning approaches that have been applied to retrieve high-quality evidence for clinical consideration from the biomedical literature. Methods: We conducted a systematic review of studies that applied machine learning techniques to identify high-quality clinical articles in the biomedical literature. Multiple databases were searched to July 2020. Extracted data focused on the applied machine learning model, steps in the development of the models, and model performance. Results: From 3918 retrieved studies, 10 met our inclusion criteria. All followed a supervised machine learning approach and applied, from a limited range of options, a high-quality standard for the training of their model. The results show that machine learning can achieve a sensitivity of 95\\% while maintaining a high precision of 86\\%. Conclusions: Machine learning approaches perform well in retrieving high-quality clinical studies. Performance may improve by applying more sophisticated approaches such as active learning and unsupervised machine learning approaches.} }
@article{WOS:001181952800001, title = {Machine learning enabled Industrial IoT Security: Challenges, Trends and Solutions}, journal = {JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION}, volume = {38}, year = {2024}, issn = {2467-964X}, doi = {10.1016/j.jii.2023.100549}, author = {Ni, Chunchun and Li, Shan Cang}, abstract = {Introduction: The increasingly integrated Industrial IoT (IIoT) with industrial systems brings benefits such as intelligent analytics, predictive maintenance, and remote monitoring. However, it also exposes industry systems to malware, cyber attacks, and other security risks. Objectives: Machine learning techniques shows promising performance in cyber security, including threats detection, vulnerability analysis, risk assessment, etc. This work aims to investigate how machine learning based techniques can be used in enhancing IIoT security and preventing cyber security events against cyber attack, safety threats, and process disruption. Methods: A comprehensive survey was conducted to show how machine learning techniques learn detection malicious activities automatically. Conclusion: This work reviewed current literature related to machine learning based methods currently being used in IIoT cyber security, and key methods have been presented and compared in terms of their capability and performance against cyber attacks.} }
@article{WOS:000509098300009, title = {Recent progress in augmenting turbulence models with physics-informed machine learning}, journal = {JOURNAL OF HYDRODYNAMICS}, volume = {31}, pages = {1153-1158}, year = {2019}, issn = {1001-6058}, doi = {10.1007/s42241-019-0089-y}, author = {Zhang, Xinlei and Wu, Jinlong and Coutier-Delgosha, Olivier and Xiao, Heng}, abstract = {In view of the long stagnation in traditional turbulence modeling, researchers have attempted using machine learning to augment turbulence models. This paper presents some of the recent progresses in our group on augmenting turbulence models with physics-informed machine learning. We also discuss our works on ensemble-based field inversion to provide training data for constructing machine learning models. Future and on-going research efforts are introduced.} }
@article{WOS:000582339000004, title = {Machine learning and AI in marketing - Connecting computing power to human insights}, journal = {INTERNATIONAL JOURNAL OF RESEARCH IN MARKETING}, volume = {37}, pages = {481-504}, year = {2020}, issn = {0167-8116}, doi = {10.1016/j.ijresmar.2020.04.005}, author = {Ma, Liye and Sun, Baohong}, abstract = {Artificial intelligence (AI) agents driven by machine learning algorithms are rapidly transforming the business world, generating heightened interest from researchers. In this paper, we review and call for marketing research to leverage machine learning methods. We provide an overview of common machine learning tasks and methods, and compare them with statistical and econometric methods that marketing researchers traditionally use. We argue that machine learning methods can process large-scale and unstructured data, and have flexible model structures that yield strong predictive performance. Meanwhile, such methods may lack model transparency and interpretability. We discuss salient AI-driven industry trends and practices, and review the still nascent academic marketing literature which uses machine learning methods. More importantly, we present a unified conceptual framework and a multi-faceted research agenda. From five key aspects of empirical marketing research: method, data, usage, issue, and theory, we propose a number of research priorities, including extending machine learning methods and using them as core components in marketing research, using the methods to extract insights from large-scale unstructured, tracking, and network data, using them in transparent fashions for descriptive, causal, and prescriptive analyses, using them to map out customer purchase journeys and develop decision-support capabilities, and connecting the methods to human insights and marketing theories. Opportunities abound for machine learning methods in marketing, and we hope our multi-faceted research agenda will inspire more work in this exciting area. (c) 2020 Elsevier B.V. All rights reserved.} }
@article{WOS:000936903600001, title = {Leakage detection in water distribution networks using machine-learning strategies}, journal = {WATER SUPPLY}, volume = {23}, pages = {1115-1126}, year = {2023}, issn = {1606-9749}, doi = {10.2166/ws.2023.054}, author = {Sousa, Diego and Du, Rong and da Silva Jr, Jose Mairton Barros and Cavalcante, Charles Casimiro and Fischione, Carlo}, abstract = {This work proposes a reliable leakage detection methodology for water distribution networks (WDNs) using machine-learning strategies. Our solution aims at detecting leakage in WDNs using efficient machine-learning strategies. We analyze pressure measurements from pumps in district metered areas (DMAs) in Stockholm, Sweden, where we consider a residential DMA of the water distribution network. Our proposed methodology uses learning strategies from unsupervised learning (K-means and cluster validation techniques), and supervised learning (learning vector quantization algorithms). The learning strategies we propose have low complexity, and the numerical experiments show the potential of using machine-learning strategies in leakage detection for monitored WDNs. Specifically, our experiments show that the proposed learning strategies are able to obtain correct classification rates up to 93.98\\%.} }
@article{WOS:001087877900001, title = {Machine learning for leaf disease classification: data, techniques and applications}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {56}, pages = {S3571-S3616}, year = {2023}, issn = {0269-2821}, doi = {10.1007/s10462-023-10610-4}, author = {Yao, Jianping and Tran, Son N. and Sawyer, Samantha and Garg, Saurabh}, abstract = {The growing demand for sustainable development brings a series of information technologies to help agriculture production. Especially, the emergence of machine learning applications, a branch of artificial intelligence, has shown multiple breakthroughs which can enhance and revolutionize plant pathology approaches. In recent years, machine learning has been adopted for leaf disease classification in both academic research and industrial applications. Therefore, it is enormously beneficial for researchers, engineers, managers, and entrepreneurs to have a comprehensive view about the recent development of machine learning technologies and applications for leaf disease detection. This study will provide a survey in different aspects of the topic including data, techniques, and applications. The paper will start with publicly available datasets. After that, we summarize common machine learning techniques, including traditional (shallow) learning, deep learning, and augmented learning. Finally, we discuss related applications. This paper would provide useful resources for future study and application of machine learning for smart agriculture in general and leaf disease classification in particular.} }
@article{WOS:000764828500048, title = {Machine-Learning-Driven Digital Twin for Lifecycle Management of Complex Equipment}, journal = {IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING}, volume = {10}, pages = {9-22}, year = {2022}, issn = {2168-6750}, doi = {10.1109/TETC.2022.3143346}, author = {Ren, Zijie and Wan, Jiafu and Deng, Pan}, abstract = {The full life cycle management of complex equipment is considered fundamental to the intelligent transformation and upgrading of the modern manufacturing industry. Digital twin technology and machine learning have been emerging technologies in recent years. The application of these two technologies in the full life cycle management of complex equipment can make each stage of the life cycle more responsive, predictable, and adaptable. This paper first proposes a technical system that embeds machine learning modules into digital twins. Next, on this basis, a full life cycle digital twin for complex equipment is constructed, and joint application of sub-models and machine learning is explored. Then, the application of a combination of the digital twin in maintenance with machine learning in predictive maintenance of diesel locomotives is presented. The effectiveness of the proposed management method is verified by experiments. The abnormal axle temperature can be alarmed about one week in advance. Lastly. possible application advantages of the combination of digital twin and machine learning in addressing future research direction in this field are introduced.} }
@article{WOS:001158492200001, title = {Machine learning models in phononic metamaterials}, journal = {CURRENT OPINION IN SOLID STATE \\& MATERIALS SCIENCE}, volume = {28}, year = {2024}, issn = {1359-0286}, doi = {10.1016/j.cossms.2023.101133}, author = {Liu, Chen-Xu and Yu, Gui-Lan and Liu, Zhanli}, abstract = {Machine learning opens up a new avenue for advancing the development of phononic crystals and elastic metamaterials. Numerous learning models have been employed and developed to address various challenges in the field of phononic metamaterials. Here, we provide an overview of mainstream machine learning models applied to phononic metamaterials, discuss their capabilities as well as limitations, and explore potential directions for future research.} }
@article{WOS:001306820800003, title = {Time efficient variants of Twin Extreme Learning Machine}, journal = {INTELLIGENT SYSTEMS WITH APPLICATIONS}, volume = {17}, year = {2023}, doi = {10.1016/j.iswa.2022.200169}, author = {Anand, Pritam and Bharti, Amisha and Rastogi, Reshma}, abstract = {Twin Extreme Learning Machine models can obtain better generalization ability than the standard Extreme Learning Machine model. But, they require to solve a pair of quadratic programming problems for this. It makes them more complex and computationally expensive than the standard Extreme Learning Machine model. In this paper, we propose two novel time-efficient formulations of the Twin Extreme Learning Machine, which only require the solution of systems of linear equations for obtaining the final classifier. In this sense, they can combine the benefits of the Twin Support Vector Machine and standard Extreme Learning Machine in the true sense. We term our first formulation as `Least Squared Twin Extreme Learning Machine'. It minimizes the L 2-norm of error variables in its optimization problem. Our second formulation `Weighted Linear loss Twin Extreme Learning Machine' uses the weighted linear loss function for calculating the empirical error, which makes it insensitive towards outliers. Numerical results obtained with multiple benchmark datasets show that proposed formulations are time efficient with better generalization ability. Further, we have used the proposed formulations in the detection of phishing websites and shown that they are much more effective in the detection of phishing websites than other Extreme Learning Machine models.} }
@article{WOS:000339429200011, title = {Investigation on Cancer Classification Using Machine Learning Approaches}, journal = {JOURNAL OF BIOMATERIALS AND TISSUE ENGINEERING}, volume = {4}, pages = {492-500}, year = {2014}, issn = {2157-9083}, doi = {10.1166/jbt.2014.1186}, author = {Bharathi, A. and Anandakumar, K.}, abstract = {The objective of this paper is to develop an effective machine learning approaches for cancer classification, which could provide reliable cancer classification with better accuracy. The work comprises of two steps. In the first step, using Analysis of Variance (ANOVA) ranking scheme to choose the important genes. The second step involves the classification task using an efficient classifier. In this paper we use the three efficient machine learning classifiers such as Fast Support Vector Machine Learning (FSVML), Fast Extreme Learning Machine Learning (FELML) and Relevance Vector Machine Learning (RVMMLML). The investigational values are computed using three datasets namely Lymphoma, Leukemia and SRBCT. The results are interpreted in terms of Testing Accuracy and Training Time. From the investigational results, it is observed that the proposed RVMMLML machine learning approach gives better testing accuracy results for all the datasets considered.} }
@article{WOS:000327552100003, title = {A tour of machine learning: An AI perspective}, journal = {AI COMMUNICATIONS}, volume = {27}, pages = {11-23}, year = {2014}, issn = {0921-7126}, doi = {10.3233/AIC-130580}, author = {Sebag, Michele}, abstract = {Machine Learning has been at the core of Artificial Intelligence since its inception. Many promises have been held, if one is to consider that Google is a living demonstration of AI. This paper presents a historical perspective on Machine Learning, describing how the emphasis was gradually shifted from logical to statistical induction, from induction to optimization, from the search of hypotheses to the search of representations. The paper concludes with a discussion about the new frontier of Machine Learning.} }
@article{WOS:000412100700016, title = {Source localization in an ocean waveguide using supervised machine learning}, journal = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA}, volume = {142}, pages = {1176-1188}, year = {2017}, issn = {0001-4966}, doi = {10.1121/1.5000165}, author = {Niu, Haiqiang and Reeves, Emma and Gerstoft, Peter}, abstract = {Source localization in ocean acoustics is posed as a machine learning problem in which data-driven methods learn source ranges directly from observed acoustic data. The pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix and used as the input for three machine learning methods: feed-forward neural networks (FNN), support vector machines (SVM), and random forests (RF). The range estimation problem is solved both as a classification problem and as a regression problem by these three machine learning algorithms. The results of range estimation for the Noise09 experiment are compared for FNN, SVM, RF, and conventional matched-field processing and demonstrate the potential of machine learning for underwater source localization. (C) 2017 Acoustical Society of America.} }
@article{WOS:000631777400001, title = {Material analysis and big data monitoring of sports training equipment based on machine learning algorithm}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {2749-2763}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-021-05852-8}, author = {Zhang, Lei and Li, Ning}, abstract = {Different machine learning algorithms predict the application effect of perovskite materials in sports training equipment. The sensitivity to material data is different on different ranges of data sets. Therefore, the algorithm needs to be selected according to specific material data samples. This study compares the prediction performance of neural network prediction algorithm (NN), genetic algorithm, and support vector machine-based machine learning algorithm (SVM) and uses statistical analysis to perform data analysis and draw corresponding curves. Moreover, this study uses a single perovskite material to verify the algorithm performance. In addition, based on the real data, the three machine learning algorithms of this study are applied to the related performance prediction, and the comparative analysis method is used to analyze the prediction performance of the machine learning algorithm. Through data analysis and chart analysis, we can see that machine learning algorithms have a certain effect in the application prediction of perovskite materials in sports training equipment. Among the three machine learning algorithms selected in this study, the performance of the machine learning algorithm based on support vector machine in all aspects is more excellent.} }
@article{WOS:000922362000001, title = {A narrative review of the application of machine learning in venous thromboembolism}, journal = {VASCULAR}, volume = {32}, pages = {698-704}, year = {2024}, issn = {1708-5381}, doi = {10.1177/17085381231153216}, author = {Zou, Shirong and Wu, Zhoupeng}, abstract = {Objective To summarize the current research progress of machine learning and venous thromboembolism. Methods The literature on risk factors, diagnosis, prevention and prognosis of machine learning and venous thromboembolism in recent years was reviewed. Results Machine learning is the future of biomedical research, personalized medicine, and computer-aided diagnosis, and will significantly promote the development of biomedical research and healthcare. However, many medical professionals are not familiar with it. In this review, we will introduce several commonly used machine learning algorithms in medicine, discuss the application of machine learning in venous thromboembolism, and reveal the challenges and opportunities of machine learning in medicine. Conclusion The incidence of venous thromboembolism is high, the diagnostic measures are diverse, and it is necessary to classify and treat machine learning, and machine learning as a research tool, it is more necessary to strengthen the special research of venous thromboembolism and machine learning.} }
@article{WOS:000922762600001, title = {Emerging Trends in Machine Learning: A Polymer Perspective}, journal = {ACS POLYMERS AU}, volume = {3}, pages = {239-258}, year = {2023}, doi = {10.1021/acspolymersau.2c00053}, author = {Martin, Tyler B. and Audus, Debra J.}, abstract = {In the last five years, there has been tremendous growth in machine learning and artificial intelligence as applied to polymer science. Here, we highlight the unique challenges presented by polymers and how the field is addressing them. We focus on emerging trends with an emphasis on topics that have received less attention in the review literature. Finally, we provide an outlook for the field, outline important growth areas in machine learning and artificial intelligence for polymer science and discuss important advances from the greater material science community.} }
@article{WOS:000447385100002, title = {Laplacian twin extreme learning machine for semi-supervised classification}, journal = {NEUROCOMPUTING}, volume = {321}, pages = {17-27}, year = {2018}, issn = {0925-2312}, doi = {10.1016/j.neucom.2018.08.028}, author = {Li, Shuang and Song, Shiji and Wan, Yihe}, abstract = {Twin extreme learning machine (TELM) is an efficient and effective method for pattern classification, based on widely known extreme learning machine (ELM). However, TELM is mainly used to deal with supervised learning problems. In this paper, we extend TELM to handle semi-supervised learning problems and propose a novel Laplacian twin extreme learning machine (LapTELM), which simultaneously trains two related and paired semi-supervised ELMs with two nonparallel separating planes for the final classification. The proposed method exploits the geometry structure property of the unlabeled samples and incorporates it as a manifold regularization term. This allows LapTELM to reap the benefits of fully exploring the plentiful unlabeled samples while retaining the learning ability and efficiency of TELM. Moreover, the paper shows that semi-supervised and supervised TELM can form an unified learning framework. Compared with several mainstream semi-supervised learning methods, the experimental results on the synthetic and several real-world data sets verify the effectiveness and efficiency of LapTELM. (c) 2018 Published by Elsevier B.V.} }
@article{WOS:001273496200001, title = {A comprehensive survey on weed and crop classi fi cation using machine learning and deep learning}, journal = {ARTIFICIAL INTELLIGENCE IN AGRICULTURE}, volume = {13}, pages = {45-63}, year = {2024}, doi = {10.1016/j.aiia.2024.06.005}, author = {Adhinata, Faisal Dharma and Wahyono and Sumiharto, Raden}, abstract = {Machine learning and deep learning are subsets of Artificial Intelligence that have revolutionized object detection and classification in images or videos. This technology plays a crucial role in facilitating the transition from conventional to precision agriculture, particularly in the context of weed control. Precision agriculture, which previously relied on manual efforts, has now embraced the use of smart devices for more efficient weed detection. However, several challenges are associated with weed detection, including the visual similarity between weed and crop, occlusion and lighting effects, as well as the need for early-stage weed control. Therefore, this study aimed to provide a comprehensive review of the application of both traditional machine learning and deep learning, as well as the combination of the two methods, for weed detection across different crop fields. The results of this review show the advantages and disadvantages of using machine learning and deep learning. Generally, deep learning produced superior accuracy compared to machine learning under various conditions. Machine learning required the selection of the right combination of features to achieve high accuracy in classifying weed and crop, particularly under conditions consisting of lighting and early growth effects. Moreover, a precise segmentation stage would be required in cases of occlusion. Machine learning had the advantage of achieving real-time processing by producing smaller models than deep learning, thereby eliminating the need for additional GPUs. However, the development of GPU technology is currently rapid, so researchers are more often using deep learning for more accurate weed identification. (c) 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:000708445300016, title = {A Review on Machine Learning for EEG Signal Processing in Bioengineering}, journal = {IEEE REVIEWS IN BIOMEDICAL ENGINEERING}, volume = {14}, pages = {204-218}, year = {2021}, issn = {1937-3333}, doi = {10.1109/RBME.2020.2969915}, author = {Hosseini, Mohammad-Parsa and Hosseini, Amin and Ahi, Kiarash}, abstract = {Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. Due to the many different types of classifiers available to use, the analysis methods are also equally numerous. In this review, we will be examining specifically machine learning methods that have been developed for EEG analysis with bioengineering applications. We reviewed literature from 1988 to 2018 to capture previous and current classification methods for EEG in multiple applications. From this information, we are able to determine the overall effectiveness of each machine learning method as well as the key characteristics. We have found that all the primary methods used in machine learning have been applied in some form in EEG classification. This ranges from Naive-Bayes to Decision Tree/Random Forest, to Support Vector Machine (SVM). Supervised learning methods are on average of higher accuracy than their unsupervised counterparts. This includes SVM and KNN. While each of the methods individually is limited in their accuracy in their respective applications, there is hope that the combination of methods when implemented properly has a higher overall classification accuracy. This paper provides a comprehensive overview of Machine Learning applications used in EEG analysis. It also gives an overview of each of the methods and general applications that each is best suited to.} }
@article{WOS:001116894500001, title = {Quantum machine learning for natural language processing application}, journal = {PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS}, volume = {627}, year = {2023}, issn = {0378-4371}, doi = {10.1016/j.physa.2023.129123}, author = {Pandey, Shyambabu and Basisth, Nihar Jyoti and Sachan, Tushar and Kumari, Neha and Pakray, Partha}, abstract = {Quantum computing is a speedily emerging area that applies quantum mechanics properties to solve complex problems that are difficult for classical computing. Machine learning is a sub-field of artificial intelligence which makes computers learn patterns from experiences. Due to the exponential growth of data, machine learning algorithms may be insufficient for big data, whereas on other side quantum computing can do fast computing. A combination of quantum computing and machine learning gave rise to a new field known as quantum machine learning. Quantum machine learning algorithms take advantage of the fast processing of quantum computing and show speedup compared to their classical counterpart. Natural language processing is another area of artificial intelligence that enables the computer to understand human languages. Now, researchers are trying to take advantage of quantum machine learning speedup in natural language processing applications. In this paper, first, we discuss the path from quantum computing to quantum machine learning. Then we review the state of the art of quantum machine learning for natural language processing applications. We also provide classical and quantum-based long short-term memory for parts of speech tagging on social media code mixed language. Our experiment shows that quantum-based long short-term memory performance is better than classical long short-term memory for parts of speech tagging of code-mixed datasets.(c) 2023 Elsevier B.V. All rights reserved.} }
@article{WOS:001181945300001, title = {A review of traditional Chinese medicine diagnosis using machine learning: Inspection, auscultation-olfaction, inquiry, and palpation}, journal = {COMPUTERS IN BIOLOGY AND MEDICINE}, volume = {170}, year = {2024}, issn = {0010-4825}, doi = {10.1016/j.compbiomed.2024.108074}, author = {Tian, Dingcheng and Chen, Weihao and Xu, Dechao and Xu, Lisheng and Xu, Gang and Guo, Yaochen and Yao, Yudong}, abstract = {Traditional Chinese medicine (TCM) is an essential part of the Chinese medical system and is recognized by the World Health Organization as an important alternative medicine. As an important part of TCM, TCM diagnosis is a method to understand a patient's illness, analyze its state, and identify syndromes. In the longterm clinical diagnosis practice of TCM, four fundamental and effective diagnostic methods of inspection, auscultation-olfaction, inquiry, and palpation (IAOIP) have been formed. However, the diagnostic information in TCM is diverse, and the diagnostic process depends on doctors' experience, which is subject to a highlevel subjectivity. At present, the research on the automated diagnosis of TCM based on machine learning is booming. Machine learning, which includes deep learning, is an essential part of artificial intelligence (AI), which provides new ideas for the objective and AI-related research of TCM. This paper aims to review and summarize the current research status of machine learning in TCM diagnosis. First, we review some key factors for the application of machine learning in TCM diagnosis, including data, data preprocessing, machine learning models, and evaluation metrics. Second, we review and summarize the research and applications of machine learning methods in TCM IAOIP and the synthesis of the four diagnostic methods. Finally, we discuss the challenges and research directions of using machine learning methods for TCM diagnosis.} }
@article{WOS:000557459300020, title = {Real-World Integration of a Sepsis Deep Learning Technology Into Routine Clinical Care: Implementation Study}, journal = {JMIR MEDICAL INFORMATICS}, volume = {8}, year = {2020}, doi = {10.2196/15182}, author = {Sendak, Mark P. and Ratliff, William and Sarro, Dina and Alderton, Elizabeth and Futoma, Joseph and Gao, Michael and Nichols, Marshall and Revoir, Mike and Yashar, Faraz and Miller, Corinne and Kester, Kelly and Sandhu, Sahil and Corey, Kristin and Brajer, Nathan and Tan, Christelle and Lin, Anthony and Brown, Tres and Engelbosch, Susan and Anstrom, Kevin and Elish, Madeleine Clare and Heller, Katherine and Donohoe, Rebecca and Theiling, Jason and Poon, Eric and Balu, Suresh and Bedoya, Armando and O'Brien, Cara}, abstract = {Background: Successful integrations of machine learning into routine clinical care are exceedingly rare, and barriers to its adoption are poorly characterized in the literature. Objective: This study aims to report a quality improvement effort to integrate a deep learning sepsis detection and management platform, Sepsis Watch, into routine clinical care. Methods: In 2016, a multidisciplinary team consisting of statisticians, data scientists, data engineers, and clinicians was assembled by the leadership of an academic health system to radically improve the detection and treatment of sepsis. This report of the quality improvement effort follows the learning health system framework to describe the problem assessment, design, development, implementation, and evaluation plan of Sepsis Watch. Results: Sepsis Watch was successfully integrated into routine clinical care and reshaped how local machine learning projects are executed. Frontline clinical staff were highly engaged in the design and development of the workflow, machine learning model, and application. Novel machine learning methods were developed to detect sepsis early, and implementation of the model required robust infrastructure. Significant investment was required to align stakeholders, develop trusting relationships, define roles and responsibilities, and to train frontline staff, leading to the establishment of 3 partnerships with internal and external research groups to evaluate Sepsis Watch. Conclusions: Machine learning models are commonly developed to enhance clinical decision making, but successful integrations of machine learning into routine clinical care are rare. Although there is no playbook for integrating deep learning into clinical care, learnings from the Sepsis Watch integration can inform efforts to develop machine learning technologies at other health care delivery systems.} }
@article{WOS:000656708300001, title = {Organizing workers and machine learning tools for a less oppressive workplace}, journal = {INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT}, volume = {59}, year = {2021}, issn = {0268-4012}, doi = {10.1016/j.ijinfomgt.2021.102353}, author = {Young, Amber Grace and Majchrzak, Ann and Kane, Gerald C.}, abstract = {Machine learning tools are increasingly infiltrating everyday work life with implications for workers. By looking at machine learning tools as part of a sociotechnical system, we explore how machine learning tools enforce oppression of workers. We theorize, normatively, that with reorganizing processes in place, oppressive characteristics could be converted to emancipatory characteristics. Drawing on Paulo Freire's critical theory of emancipatory pedagogy, we outline similarities between the characteristics Freire saw in oppressive societies and the characteristics of currently designed partnerships between humans and machine learning tools. Freire's theory offers a way forward in reorganizing humans and machine learning tools in the workplace. Rather than advocating human control or the decoupling of workers and machines, we follow Freire's theory in proposing four processes for emancipatory organizing of human and machine learning partnership: 1) awakening of a critical consciousness, 2) enabling role freedom, 3) instituting incentives and sanctions for accountability, and 4) identifying alternative emancipatory futures. Theoretical and practical implications of this emancipatory organizing theory are drawn.} }
@article{WOS:001139837400001, title = {Preserving data privacy in machine learning systems}, journal = {COMPUTERS \\& SECURITY}, volume = {137}, year = {2024}, issn = {0167-4048}, doi = {10.1016/j.cose.2023.103605}, author = {El Mestari, Soumia Zohra and Lenzini, Gabriele and Demirci, Huseyin}, abstract = {The wide adoption of Machine Learning to solve a large set of real-life problems came with the need to collect and process large volumes of data, some of which are considered personal and sensitive, raising serious concerns about data protection. Privacy-enhancing technologies (PETs) are often indicated as a solution to protect personal data and to achieve a general trustworthiness as required by current EU regulations on data protection and AI. However, an off-the-shelf application of PETs is insufficient to ensure a high-quality of data protection, which one needs to understand. This work systematically discusses the risks against data protection in modern Machine Learning systems taking the original perspective of the data owners, who are those who hold the various data sets, data models, or both, throughout the machine learning life cycle and considering the different Machine Learning architectures. It argues that the origin of the threats, the risks against the data, and the level of protection offered by PETs depend on the data processing phase, the role of the parties involved, and the architecture where the machine learning systems are deployed. By offering a framework in which to discuss privacy and confidentiality risks for data owners and by identifying and assessing privacy-preserving countermeasures for machine learning, this work could facilitate the discussion about compliance with EU regulations and directives.We discuss current challenges and research questions that are still unsolved in the field. In this respect, this paper provides researchers and developers working on machine learning with a comprehensive body of knowledge to let them advance in the science of data protection in machine learning field as well as in closely related fields such as Artificial Intelligence.} }
@article{WOS:001013949600001, title = {A Taxonomic Survey of Physics-Informed Machine Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app13126892}, author = {Pateras, Joseph and Rana, Pratip and Ghosh, Preetam}, abstract = {Physics-informed machine learning (PIML) refers to the emerging area of extracting physically relevant solutions to complex multiscale modeling problems lacking sufficient quantity and veracity of data with learning models informed by physically relevant prior information. This work discusses the recent critical advancements in the PIML domain. Novel methods and applications of domain decomposition in physics-informed neural networks (PINNs) in particular are highlighted. Additionally, we explore recent works toward utilizing neural operator learning to intuit relationships in physics systems traditionally modeled by sets of complex governing equations and solved with expensive differentiation techniques. Finally, expansive applications of traditional physics-informed machine learning and potential limitations are discussed. In addition to summarizing recent work, we propose a novel taxonomic structure to catalog physics-informed machine learning based on how the physics-information is derived and injected into the machine learning process. The taxonomy assumes the explicit objectives of facilitating interdisciplinary collaboration in methodology, thereby promoting a wider characterization of what types of physics problems are served by the physics-informed learning machines and assisting in identifying suitable targets for future work. To summarize, the major twofold goal of this work is to summarize recent advancements and introduce a taxonomic catalog for applications of physics-informed machine learning.} }
@article{WOS:001155835100001, title = {Machine learning for power transformer SFRA based fault detection}, journal = {INTERNATIONAL JOURNAL OF ELECTRICAL POWER \\& ENERGY SYSTEMS}, volume = {156}, year = {2024}, issn = {0142-0615}, doi = {10.1016/j.ijepes.2023.109779}, author = {Bjelic, Milos and Brkovic, Bogdan and Zarkovic, Mileta and Miljkovic, Tatjana}, abstract = {This paper presents machine learning methods for health assessment of power transformer based on sweep frequency response analysis. The paper presents an overview of monitoring and diagnostics based on statistical Sweep Frequency Response Analysis (SFRA) based indicators that are used to evaluate the state of the power transformer. Experimental data obtained from power transformers with internal short-circuit faults is used as a database for applying machine learning. Machine learning is implemented to achieve more precise asset management and condition-based maintenance. Unsupervised machine learning was applied through the k-means cluster method for classifying and dividing the examined power transformer state into groups with similar state and probability of failure. Artificial neural network (ANN) and Adaptive Neuro Fuzzy Inference System (ANFIS) as part of supervised machine learning are created in order to detect fault severity in tested power transformers of different lifetime. The presented machine learning methods can be used to improve health assessment of power transformers.} }
@article{WOS:000884211300001, title = {Relative Valuation with Machine Learning}, journal = {JOURNAL OF ACCOUNTING RESEARCH}, volume = {61}, pages = {329-376}, year = {2023}, issn = {0021-8456}, doi = {10.1111/1475-679X.12464}, author = {Geertsema, Paul and Lu, Helen}, abstract = {We use machine learning for relative valuation and peer firm selection. In out-of-sample tests, our machine learning models substantially outperform traditional models in valuation accuracy. This outperformance persists over time and holds across different types of firms. The valuations produced by machine learning models behave like fundamental values. Overvalued stocks decrease in price and undervalued stocks increase in price in the following month. Determinants of valuation multiples identified by machine learning models are consistent with theoretical predictions derived from a discounted cash flow approach. Profitability ratios, growth measures, and efficiency ratios are the most important value drivers throughout our sample period. We derive a novel method to express valuation multiples predicted by our machine learning models as weighted averages of peer firm multiples. These weights are a measure of peer-firm comparability and can be used for selecting peer-groups.} }
@article{WOS:001415042200001, title = {Applications of machine learning methods for design and characterization of high-performance fiber-reinforced cementitious composite (HPFRCC): a review}, journal = {JOURNAL OF SUSTAINABLE CEMENT-BASED MATERIALS}, volume = {14}, pages = {1726-1749}, year = {2025}, issn = {2165-0373}, doi = {10.1080/21650373.2025.2462183}, author = {Guo, Pengwei and Moghaddas, Seyed A. and Liu, Yiming and Meng, Weina and Li, Victor C. and Bao, Yi}, abstract = {High-Performance Fiber-Reinforced Cementitious Composite (HPFRCC) represents a family of advanced composite materials with remarkable mechanical properties and durability, but their design and characterization tasks involve unique challenges. Recently, advancements in machine learning techniques have offered new opportunities. This paper reviews the application of machine learning techniques in the design and characterization of HPFRCC. The application of machine learning to the design of HPFRCC is reviewed based on a prediction-optimization framework, and the steps for property prediction and material design considering fresh properties, cracks, and microstructures are elaborated. The latest development of knowledge-guided machine learning approach is discussed. The application of machine learning to the characterization of HPFRCC is reviewed, and the computer vision and deep learning techniques for characterizing HPFRCC are elaborated. The challenges and opportunities for the applications of machine learning methods are discussed, aiming to facilitate applications of machine learning techniques for HPFRCC.} }
@article{WOS:000697563900010, title = {When machine learning meets congestion control: A survey and comparison}, journal = {COMPUTER NETWORKS}, volume = {192}, year = {2021}, issn = {1389-1286}, doi = {10.1016/j.comnet.2021.108033}, author = {Jiang, Huiling and Li, Qing and Jiang, Yong and Shen, GengBiao and Sinnott, Richard and Tian, Chen and Xu, Mingwei}, abstract = {Machine learning has seen a significant surge and uptake across many diverse applications. The high flexibility, adaptability, and computing capabilities it provides extend traditional approaches used in multiple fields including network operation and management. Numerous surveys have explored machine learning algorithms in the context of networking, such as traffic engineering, performance optimization, and network security. Many machine learning approaches focus on clustering, classification, regression, and reinforcement learning. The innovation of this research, and the contribution of this paper lies in the detailed summary and comparison of learning-based congestion control approaches. Compared with traditional congestion control algorithms which are typically rule-based, capabilities to learn from historical experience are highly desirable. From the literature, it is observed that reinforcement learning is a crucial trend among learning-based congestion control algorithms. In this paper, we explore the performance of reinforcement learning-based congestion control algorithms and present current problems with reinforcement learning-based congestion control algorithms. Moreover, we outline challenges and trends related to learning-based congestion control algorithms.} }
@article{WOS:000397748100040, title = {Machine Learning Techniques for Optical Performance Monitoring From Directly Detected PDM-QAM Signals}, journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY}, volume = {35}, pages = {868-875}, year = {2017}, issn = {0733-8724}, doi = {10.1109/JLT.2016.2590989}, author = {Thrane, Jakob and Wass, Jesper and Piels, Molly and Diniz, Julio C. M. and Jones, Rasmus and Zibar, Darko}, abstract = {Linear signal processing algorithms areeffective in dealing with linear transmission channel and linear signal detection, whereas the nonlinear signal processing algorithms, from the machine learning community, are effective in dealing with nonlinear transmission channel and nonlinear signal detection. In this paper, a brief overview of the various machine learning methods and their application in optical communication is presented and discussed. Moreover, supervised machine learning methods, such as neural networks and support vector machine, are experimentally demonstrated for in-band optical signal to noise ratio estimation and modulation format classification, respectively. The proposed methods accurately evaluate optical signals employing up to 64 quadrature amplitude modulation, at 32 Gbd, using only directly detected data.} }
@article{WOS:000595291200006, title = {Machine learning for halide perovskite materials}, journal = {NANO ENERGY}, volume = {78}, year = {2020}, issn = {2211-2855}, doi = {10.1016/j.nanoen.2020.105380}, author = {Zhang, Lei and He, Mu and Shao, Shaofeng}, abstract = {Halide perovskite materials serve as excellent candidates for solar cell and optoelectronic devices. Recently, the design of the halide perovskite materials is greatly facilitated by machine learning techniques, which effectively identify suitable halide perovskite candidates and unveil hidden relationships by algorithms that mimic the human cognitive functions. In this manuscript, we review recent progresses on the machine learning studies of the halide perovskite materials, including the prediction and understanding of lead-free and stable halide perovskite materials. The structural descriptors to describe the property and performance of the halide perovskite materials are discussed. In addition, the design strategy of the additive species for the halide perovskite materials via the machine learning technique is provided. Suggestions to further develop the halide perovskite-based systems via the machine learning methods in the future are provided.} }
@article{WOS:000356700900001, title = {Deep Extreme Learning Machine and Its Application in EEG Classification}, journal = {MATHEMATICAL PROBLEMS IN ENGINEERING}, volume = {2015}, year = {2015}, issn = {1024-123X}, doi = {10.1155/2015/129021}, author = {Ding, Shifei and Zhang, Nan and Xu, Xinzheng and Guo, Lili and Zhang, Jian}, abstract = {Recently, deep learning has aroused wide interest in machine learning fields. Deep learning is a multilayer perceptron artificial neural network algorithm. Deep learning has the advantage of approximating the complicated function and alleviating the optimization difficulty associated with deep models. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network which takes advantages of deep learning and extreme learning machine. Not only does MLELM approximate the complicated function but it also does not need to iterate during the training process. We combining with MLELM and extreme learning machine with kernel (KELM) put forward deep extreme learning machine (DELM) and apply it to EEG classification in this paper. This paper focuses on the application of DELM in the classification of the visual feedback experiment, using MATLAB and the second brain-computer interface (BCI) competition datasets. By simulating and analyzing the results of the experiments, effectiveness of the application of DELM in EEG classification is confirmed.} }
@article{WOS:001045762300001, title = {Multiple stakeholders drive diverse interpretability requirements for machine learning in healthcare}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {5}, pages = {824-829}, year = {2023}, doi = {10.1038/s42256-023-00698-2}, author = {Imrie, Fergus and Davis, Robert and van der Schaar, Mihaela}, abstract = {Limited interpretability and understanding of machine learning methods in healthcare hinder their clinical impact. Imrie et al. discuss five types of machine learning interpretability. They examine medical stakeholders, highlight how interpretability meets their needs and emphasize the role of tailored interpretability in linking machine learning advancements to clinical impact. Applications of machine learning are becoming increasingly common in medicine and healthcare, enabling more accurate predictive models. However, this often comes at the cost of interpretability, limiting the clinical impact of machine learning methods. To realize the potential of machine learning in healthcare, it is critical to understand such models from the perspective of multiple stakeholders and various angles, necessitating different types of explanation. In this Perspective, we explore five fundamentally different types of post-hoc machine learning interpretability. We highlight the different types of information that they provide, and describe when each can be useful. We examine the various stakeholders in healthcare, delving into their specific objectives, requirements and goals. We discuss how current notions of interpretability can help meet these and what is required for each stakeholder to make machine learning models clinically impactful. Finally, to facilitate adoption, we release an open-source interpretability library containing implementations of the different types of interpretability, including tools for visualizing and exploring the explanations.} }
@article{WOS:000505074700001, title = {Artificial intelligence, machine learning and the pediatric airway}, journal = {PEDIATRIC ANESTHESIA}, volume = {30}, pages = {264-268}, year = {2020}, issn = {1155-5645}, doi = {10.1111/pan.13792}, author = {Matava, Clyde and Pankiv, Evelina and Ahumada, Luis and Weingarten, Benjamin and Simpao, Allan}, abstract = {Artificial intelligence and machine learning are rapidly expanding fields with increasing relevance in anesthesia and, in particular, airway management. The ability of artificial intelligence and machine learning algorithms to recognize patterns from large volumes of complex data makes them attractive for use in pediatric anesthesia airway management. The purpose of this review is to introduce artificial intelligence, machine learning, and deep learning to the pediatric anesthesiologist. Current evidence and developments in artificial intelligence, machine learning, and deep learning relevant to pediatric airway management are presented. We critically assess the current evidence on the use of artificial intelligence and machine learning in the assessment, diagnosis, monitoring, procedure assistance, and predicting outcomes during pediatric airway management. Further, we discuss the limitations of these technologies and offer areas for focused research that may bring pediatric airway management anesthesiology into the era of artificial intelligence and machine learning.} }
@article{WOS:000678606100001, title = {Optimizing Predictive Maintenance With Machine Learning for Reliability Improvement}, journal = {ASCE-ASME JOURNAL OF RISK AND UNCERTAINTY IN ENGINEERING SYSTEMS PART B-MECHANICAL ENGINEERING}, volume = {7}, year = {2021}, issn = {2332-9017}, doi = {10.1115/1.4049525}, author = {Ren, Yali}, abstract = {Predictive maintenance, as a form of pro-active maintenance, has increasing usage and shows significant superiority over the corrective and preventive maintenance. However, conventional methods of predictive maintenance have noteworthy limitations in maintenance optimization and reliability improvement. In the last two decades, machine learning has flourished and overcome many inherent flaws of conventional maintenance prediction methods. Meanwhile, machine learning displays unprecedented predictive power in maintenance prediction and optimization. This paper compares the features of corrective, preventive, and predictive maintenance, examines the conventional approaches to predictive maintenance, and analyzes their drawbacks. Subsequently, this paper explores the driving forces, and advantages of machine learning over conventional solutions in predictive maintenance. Specifically, this paper reviews popular supervised learning and reinforcement learning algorithms and the associated typical applications in predictive maintenance. Furthermore, this paper summarizes the four critical steps of machine learning applications in maintenance prediction. Finally, the author proposes the future researches concerning how to utilize machine learning to optimize maintenance prediction and planning, improve equipment reliability, and achieve the best possible benefit.} }
@article{WOS:000532109400001, title = {Interactive machine teaching: a human-centered approach to building machine-learned models}, journal = {HUMAN-COMPUTER INTERACTION}, volume = {35}, pages = {413-451}, year = {2020}, issn = {0737-0024}, doi = {10.1080/07370024.2020.1734931}, author = {Ramos, Gonzalo and Meek, Christopher and Simard, Patrice and Suh, Jina and Ghorashi, Soroush}, abstract = {Modern systems can augment people's capabilities by using machine-learned models to surface intelligent behaviors. Unfortunately, building these models remains challenging and beyond the reach of non-machine learning experts. We describe interactive machine teaching (IMT) and its potential to simplify the creation of machine-learned models. One of the key characteristics of IMT is its iterative process in which the human-in-the-loop takes the role of a teacher teaching a machine how to perform a task. We explore alternative learning theories as potential theoretical foundations for IMT, the intrinsic human capabilities related to teaching, and how IMT systems might leverage them. We argue that IMT processes that enable people to leverage these capabilities have a variety of benefits, including making machine learning methods accessible to subject-matter experts and the creation of semantic and debuggable machine learning (ML) models. We present an integrated teaching environment (ITE) that embodies principles from IMT, and use it as a design probe to observe how non-ML experts do IMT and as the basis of a system that helps us study how to guide teachers. We explore and highlight the benefits and challenges of IMT systems. We conclude by outlining six research challenges to advance the field of IMT.} }
@article{WOS:000691220800003, title = {Reproducibility standards for machine learning in the life sciences}, journal = {NATURE METHODS}, volume = {18}, pages = {1132-1135}, year = {2021}, issn = {1548-7091}, doi = {10.1038/s41592-021-01256-7}, author = {Heil, Benjamin J. and Hoffman, Michael M. and Markowetz, Florian and Lee, Su-In and Greene, Casey S. and Hicks, Stephanie C.}, abstract = {To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.} }
@article{WOS:000563833500002, title = {Loops, ladders and links: the recursivity of social and machine learning}, journal = {THEORY AND SOCIETY}, volume = {49}, pages = {803-832}, year = {2020}, issn = {0304-2421}, doi = {10.1007/s11186-020-09409-x}, author = {Fourcade, Marion and Johns, Fleur}, abstract = {Machine learning algorithms reshape how people communicate, exchange, and associate; how institutions sort them and slot them into social positions; and how they experience life, down to the most ordinary and intimate aspects. In this article, we draw on examples from the field of social media to review the commonalities, interactions, and contradictions between the dispositions of people and those of machines as they learn from and make sense of each other.} }
@article{WOS:000663500200003, title = {A survey on deep learning and its applications}, journal = {COMPUTER SCIENCE REVIEW}, volume = {40}, year = {2021}, issn = {1574-0137}, doi = {10.1016/j.cosrev.2021.100379}, author = {Dong, Shi and Wang, Ping and Abbas, Khushnood}, abstract = {Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal-artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning. (C) 2021 Elsevier Inc. All rights reserved.} }
@article{WOS:000413244600013, title = {Non-Intrusive Load Monitoring Using Semi-Supervised Machine Learning and Wavelet Design}, journal = {IEEE TRANSACTIONS ON SMART GRID}, volume = {8}, pages = {2648-2655}, year = {2017}, issn = {1949-3053}, doi = {10.1109/TSG.2016.2532885}, author = {Gillis, Jessie M. and Morsi, Walid G.}, abstract = {This paper presents a new approach based on semi-supervised machine learning and wavelet design applied to non-intrusive load monitoring. Co-training of two machine learning classifiers is used to automate the process of learning the load pattern after designing new wavelets. The numerical results demonstrating the effectiveness of the proposed approach are discussed and conclusions are drawn.} }
@article{WOS:000837752300005, title = {The statistical analysis in the era of big data}, journal = {INTERNATIONAL JOURNAL OF MODELLING IDENTIFICATION AND CONTROL}, volume = {40}, pages = {151-157}, year = {2022}, issn = {1746-6172}, doi = {10.1504/IJMIC.2022.124718}, author = {Wang, Zelin and Liu, Xinke and Zhang, Weiye and Zhi, Yingying and Cheng, Shi}, abstract = {In the big data environment, the traditional machine learning algorithm for data processing is somewhat inadequate. Therefore, machine learning algorithms adapted to big data environment have become a research hotspot. At the time of the marriage of big data and machine learning, it is necessary to predict the related challenges and opportunities. This paper mainly analyses and summarises the current research status of machine learning algorithms for processing big data, and discusses the new opportunities and challenges that machine learning paradigm will face in the era of big data. It also explores the new technology breakthrough that machine learning will produce in the era of big data.} }
@article{WOS:000576604100013, title = {Recent advances on constraint-based models by integrating machine learning}, journal = {CURRENT OPINION IN BIOTECHNOLOGY}, volume = {64}, pages = {85-91}, year = {2020}, issn = {0958-1669}, doi = {10.1016/j.copbio.2019.11.007}, author = {Rana, Pratip and Berry, Carter and Ghosh, Preetam and Fong, Stephen S.}, abstract = {Research that meaningfully integrates constraint-based modeling with machine learning is at its infancy but holds much promise. Here, we consider where machine learning has been implemented within the constraint-based modeling reconstruction framework and highlight the need to develop approaches that can identify meaningful features from large-scale data and connect them to biological mechanisms to establish causality to connect genotype to phenotype. We motivate the construction of iterative integrative schemes where machine learning can fine-tune the input constraints in a constraint-based model or contrarily, constraint-based model simulation results are analyzed by machine learning and reconciled with experimental data. This can iteratively refine a constraint-based model until there is consistency between experimental data, machine learning results, and constraint-based model simulations.} }
@article{WOS:000518683500002, title = {The rise of machine learning for detection and classification of malware: Research developments, trends and challenges}, journal = {JOURNAL OF NETWORK AND COMPUTER APPLICATIONS}, volume = {153}, year = {2020}, issn = {1084-8045}, doi = {10.1016/j.jnca.2019.102526}, author = {Gibert, Daniel and Mateu, Carles and Planes, Jordi}, abstract = {The struggle between security analysts and malware developers is a never-ending battle with the complexity of malware changing as quickly as innovation grows. Current state-of-the-art research focus on the development and application of machine learning techniques for malware detection due to its ability to keep pace with malware evolution. This survey aims at providing a systematic and detailed overview of machine learning techniques for malware detection and in particular, deep learning techniques. The main contributions of the paper are: (1) it provides a complete description of the methods and features in a traditional machine learning workflow for malware detection and classification, (2) it explores the challenges and limitations of traditional machine learning and (3) it analyzes recent trends and developments in the field with special emphasis on deep learning approaches. Furthermore, (4) it presents the research issues and unsolved challenges of the state-of-the-art techniques and (5) it discusses the new directions of research. The survey helps researchers to have an understanding of the malware detection field and of the new developments and directions of research explored by the scientific community to tackle the problem.} }
@article{WOS:000656417100004, title = {A review on deep learning in machining and tool monitoring: methods, opportunities, and challenges}, journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY}, volume = {115}, pages = {2683-2709}, year = {2021}, issn = {0268-3768}, doi = {10.1007/s00170-021-07325-7}, author = {Nasir, Vahid and Sassani, Farrokh}, abstract = {Data-driven methods provided smart manufacturing with unprecedented opportunities to facilitate the transition toward Industry 4.0-based production. Machine learning and deep learning play a critical role in developing intelligent systems for descriptive, diagnostic, and predictive analytics for machine tools and process health monitoring. This paper reviews the opportunities and challenges of deep learning (DL) for intelligent machining and tool monitoring. The components of an intelligent monitoring framework are introduced. The main advantages and disadvantages of machine learning (ML) models are presented and compared with those of deep models. The main DL models, including autoencoders, deep belief networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs), were discussed, and their applications in intelligent machining and tool condition monitoring were reviewed. The opportunities of data-driven smart manufacturing approach applied to intelligent machining were discussed to be (1) automated feature engineering, (2) handling big data, (3) handling high-dimensional data, (4) avoiding sensor redundancy, (5) optimal sensor fusion, and (6) offering hybrid intelligent models. Finally, the data-driven challenges in smart manufacturing, including the challenges associated with the data size, data nature, model selection, and process uncertainty, were discussed, and the research gaps were outlined.} }
@article{WOS:000484532300003, title = {Spatial extreme learning machines: An application on prediction of disease counts}, journal = {STATISTICAL METHODS IN MEDICAL RESEARCH}, volume = {28}, pages = {2583-2594}, year = {2019}, issn = {0962-2802}, doi = {10.1177/0962280218767985}, author = {Prates, Marcos O.}, abstract = {Extreme learning machines have gained a lot of attention by the machine learning community because of its interesting properties and computational advantages. With the increase in collection of information nowadays, many sources of data have missing information making statistical analysis harder or unfeasible. In this paper, we present a new model, coined spatial extreme learning machine, that combine spatial modeling with extreme learning machines keeping the nice properties of both methodologies and making it very flexible and robust. As explained throughout the text, the spatial extreme learning machines have many advantages in comparison with the traditional extreme learning machines. By a simulation study and a real data analysis we present how the spatial extreme learning machine can be used to improve imputation of missing data and uncertainty prediction estimation.} }
@article{WOS:001372855700001, title = {Applications of and issues with machine learning in medicine: Bridging the gap with explainable AI}, journal = {BIOSCIENCE TRENDS}, volume = {18}, pages = {497-504}, year = {2024}, issn = {1881-7815}, doi = {10.5582/bst.2024.01342}, author = {Karako, Kenji and Tang, Wei}, abstract = {In recent years, machine learning, and particularly deep learning, has shown remarkable potential in various fields, including medicine. Advanced techniques like convolutional neural networks and transformers have enabled high-performance predictions for complex problems, making machine learning a valuable tool in medical decision-making. From predicting postoperative complications to assessing disease risk, machine learning has been actively used to analyze patient data and assist healthcare professionals. However, the ``black box'' problem, wherein the internal workings of machine learning models are opaque and difficult to interpret, poses a significant challenge in medical applications. The lack of transparency may hinder trust and acceptance by clinicians and patients, making the development of explainable AI (XAI) techniques essential. XAI aims to provide both global and local explanations for machine learning models, offering insights into how predictions are made and which factors influence these outcomes. In this article, we explore various applications of machine learning in medicine, describe commonly used algorithms, and discuss explainable AI as a promising solution to enhance the interpretability of these models. By integrating explainability into machine learning, we aim to ensure its ethical and practical application in healthcare, ultimately improving patient outcomes and supporting personalized treatment strategies.} }
@article{WOS:000598812600004, title = {Machine learning for geographically differentiated climate change mitigation in urban areas}, journal = {SUSTAINABLE CITIES AND SOCIETY}, volume = {64}, year = {2021}, issn = {2210-6707}, doi = {10.1016/j.scs.2020.102526}, author = {Milojevic-Dupont, Nikola and Creutzig, Felix}, abstract = {Artificial intelligence and machine learning are transforming scientific disciplines, but their full potential for climate change mitigation remains elusive. Here, we conduct a systematic review of applied machine learning studies that are of relevance for climate change mitigation, focusing specifically on the fields of remote sensing, urban transportation, and buildings. The relevant body of literature spans twenty years and is growing exponentially. We show that the emergence of big data and machine learning methods enables climate solution research to overcome generic recommendations and provide policy solutions at urban, street, building and household scale, adapted to specific contexts, but scalable to global mitigation potentials. We suggest a meta-algorithmic architecture and framework for using machine learning to optimize urban planning for accelerating, improving and transforming urban infrastructure provision.} }
@article{WOS:000466837400054, title = {Machine learning as a supportive tool to recognize cardiac arrest in emergency calls}, journal = {RESUSCITATION}, volume = {138}, pages = {322-329}, year = {2019}, issn = {0300-9572}, doi = {10.1016/j.resuscitation.2019.01.015}, author = {Blomberg, Stig Nikolaj and Folke, Fredrik and Ersboll, Annette Kjaer and Christensen, Helle Collatz and Torp-Pedersen, Christian and Sayre, Michael R. and Counts, Catherine R. and Lippert, Freddy K.}, abstract = {Background: Emergency medical dispatchers fail to identify approximately 25\\% of cases of out of hospital cardiac arrest, thus lose the opportunity to provide the caller instructions in cardiopulmonary resuscitation. We examined whether a machine learning framework could recognize out-of-hospital cardiac arrest from audio files of calls to the emergency medical dispatch center. Methods: For all incidents responded to by Emergency Medical Dispatch Center Copenhagen in 2014, the associated call was retrieved. A machine learning framework was trained to recognize cardiac arrest from the recorded calls. Sensitivity, specificity, and positive predictive value for recognizing out-of-hospital cardiac arrest were calculated. The performance of the machine learning framework was compared to the actual recognition and time-to-recognition of cardiac arrest by medical dispatchers. Results: We examined 108,607 emergency calls, of which 918 (0.8\\%) were out-of-hospital cardiac arrest calls eligible for analysis. Compared with medical dispatchers, the machine learning framework had a significantly higher sensitivity (72.5\\% vs. 84.1\\%, p < 0.001) with lower specificity (98.8\\% vs. 97.3\\%, p < 0.001). The machine learning framework had a lower positive predictive value than dispatchers (20.9\\% vs. 33.0\\%, p < 0.001). Time-to-recognition was significantly shorter for the machine learning framework compared to the dispatchers (median 44 seconds vs. 54 s, p < 0.001). Conclusions: A machine learning framework performed better than emergency medical dispatchers for identifying out-of-hospital cardiac arrest in emergency phone calls. Machine learning may play an important role as a decision support tool for emergency medical dispatchers.} }
@article{WOS:000540241000001, title = {Building machine learning models without sharing patient data: A simulation-based analysis of distributed learning by ensembling}, journal = {JOURNAL OF BIOMEDICAL INFORMATICS}, volume = {106}, year = {2020}, issn = {1532-0464}, doi = {10.1016/j.jbi.2020.103424}, author = {Tuladhar, Anup and Gill, Sascha and Ismail, Zahinoor and Forkert, Nils D. and Alzheimers Dis Neuroimaging Initia}, abstract = {The development of machine learning solutions in medicine is often hindered by difficulties associated with sharing patient data. Distributed learning aims to train machine learning models locally without requiring data sharing. However, the utility of distributed learning for rare diseases, with only a few training examples at each contributing local center, has not been investigated. The aim of this work was to simulate distributed learning models by ensembling with artificial neural networks (ANN), support vector machines (SVM), and random forests (RF) and evaluate them using four medical datasets. Distributed learning by ensembling locally trained agents improved performance compared to models trained using the data from a single institution, even in cases where only a very few training examples are available per local center. Distributed learning improved when more locally trained models were added to the ensemble. Local class imbalance reduced distributed SVM performance but did not impact distributed RF and ANN classification. Our results suggest that distributed learning by ensembling can be used to train machine learning models without sharing patient data and is suitable to use with small datasets.} }
@article{WOS:000729048700005, title = {Machine Learning in Healthcare}, journal = {CURRENT GENOMICS}, volume = {22}, pages = {291-300}, year = {2021}, issn = {1389-2029}, doi = {10.2174/1389202922666210705124359}, author = {Habehh, Hafsa and Gohel, Suril}, abstract = {Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technol-ogy have brought on substantial strides in predicting and identifying health emergencies, disease populations, and disease state and immune response, amongst a few. Although, skepticism remains regarding the practical application and interpretation of results from ML-based approaches in healthcare settings, the inclusion of these approaches is increasing at a rapid pace. Here we provide a brief overview of machine learning-based approaches and learning algorithms including super -vised, unsupervised, and reinforcement learning along with examples. Second, we discuss the appli-cation of ML in several healthcare fields, including radiology, genetics, electronic health records, and neuroimaging. We also briefly discuss the risks and challenges of ML application to healthcare such as system privacy and ethical concerns and provide suggestions for future applications.} }
@article{WOS:000689068400001, title = {Deep Learning Aided Data-Driven Fault Diagnosis of Rotatory Machine: A Comprehensive Review}, journal = {ENERGIES}, volume = {14}, year = {2021}, doi = {10.3390/en14165150}, author = {Mushtaq, Shiza and Islam, M. M. Manjurul and Sohaib, Muhammad}, abstract = {This paper presents a comprehensive review of the developments made in rotating bearing fault diagnosis, a crucial component of a rotatory machine, during the past decade. A data-driven fault diagnosis framework consists of data acquisition, feature extraction/feature learning, and decision making based on shallow/deep learning algorithms. In this review paper, various signal processing techniques, classical machine learning approaches, and deep learning algorithms used for bearing fault diagnosis have been discussed. Moreover, highlights of the available public datasets that have been widely used in bearing fault diagnosis experiments, such as Case Western Reserve University (CWRU), Paderborn University Bearing, PRONOSTIA, and Intelligent Maintenance Systems (IMS), are discussed in this paper. A comparison of machine learning techniques, such as support vector machines, k-nearest neighbors, artificial neural networks, etc., deep learning algorithms such as a deep convolutional network (CNN), auto-encoder-based deep neural network (AE-DNN), deep belief network (DBN), deep recurrent neural network (RNN), and other deep learning methods that have been utilized for the diagnosis of rotary machines bearing fault, is presented.} }
@article{WOS:000999504400002, title = {Machine Learning Methods in Solving the Boolean Satisfiability Problem}, journal = {MACHINE INTELLIGENCE RESEARCH}, volume = {20}, pages = {640-655}, year = {2023}, issn = {2731-538X}, doi = {10.1007/s11633-022-1396-2}, author = {Guo, Wenxuan and Zhen, Hui-Ling and Li, Xijun and Luo, Wanqian and Yuan, Mingxuan and Jin, Yaohui and Yan, Junchi}, abstract = {This paper reviews the recent literature on solving the Boolean satisfiability problem (SAT), an archetypal NP-complete problem, with the aid of machine learning (ML) techniques. Over the last decade, the machine learning society advances rapidly and surpasses human performance on several tasks. This trend also inspires a number of works that apply machine learning methods for SAT solving. In this survey, we examine the evolving ML SAT solvers from naive classifiers with handcrafted features to emerging end-to-end SAT solvers, as well as recent progress on combinations of existing conflict-driven clause learning (CDCL) and local search solvers with machine learning methods. Overall, solving SAT with machine learning is a promising yet challenging research topic. We conclude the limitations of current works and suggest possible future directions. The collected paper list is available at https://github.com/ThinklabSJTU/awesome-ml4co.} }
@article{WOS:000363458900023, title = {Does machine learning need fuzzy logic?}, journal = {FUZZY SETS AND SYSTEMS}, volume = {281}, pages = {292-299}, year = {2015}, issn = {0165-0114}, doi = {10.1016/j.fss.2015.09.001}, author = {Huellermeier, Eyke}, abstract = {This article is a short position paper in which the author outlines his (necessarily subjective) perception of current research in fuzzy machine learning, that is, the use of formal concepts and mathematical tools from fuzzy sets and fuzzy logic in the field of machine learning. The paper starts with a critical appraisal of previous contributions to fuzzy machine learning and ends with a suggestion of some directions for future work. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000979995400004, title = {Uncovering expression signatures of synergistic drug responses via ensembles of explainable machine-learning models}, journal = {NATURE BIOMEDICAL ENGINEERING}, volume = {7}, pages = {811+}, year = {2023}, issn = {2157-846X}, doi = {10.1038/s41551-023-01034-0}, author = {Janizek, Joseph D. and Dincer, Ayse B. and Celik, Safiye and Chen, Hugh and Chen, William and Naxerova, Kamila and Lee, Su-In}, abstract = {Ensembles of explainable machine-learning models increase the quality of explanations for the molecular basis of synergetic drug combinations, as shown for the treatment of acute myeloid leukaemia. Machine learning may aid the choice of optimal combinations of anticancer drugs by explaining the molecular basis of their synergy. By combining accurate models with interpretable insights, explainable machine learning promises to accelerate data-driven cancer pharmacology. However, owing to the highly correlated and high-dimensional nature of transcriptomic data, naively applying current explainable machine-learning strategies to large transcriptomic datasets leads to suboptimal outcomes. Here by using feature attribution methods, we show that the quality of the explanations can be increased by leveraging ensembles of explainable machine-learning models. We applied the approach to a dataset of 133 combinations of 46 anticancer drugs tested in ex vivo tumour samples from 285 patients with acute myeloid leukaemia and uncovered a haematopoietic-differentiation signature underlying drug combinations with therapeutic synergy. Ensembles of machine-learning models trained to predict drug combination synergies on the basis of gene-expression data may improve the feature attribution quality of complex machine-learning models.} }
@article{WOS:000569734100001, title = {A Systematic Review of Defensive and Offensive Cybersecurity with Machine Learning}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175811}, author = {Aiyanyo, Imatitikua D. and Samuel, Hamman and Lim, Heuiseok}, abstract = {This is a systematic review of over one hundred research papers about machine learning methods applied to defensive and offensive cybersecurity. In contrast to previous reviews, which focused on several fragments of research topics in this area, this paper systematically and comprehensively combines domain knowledge into a single review. Ultimately, this paper seeks to provide a base for researchers that wish to delve into the field of machine learning for cybersecurity. Our findings identify the frequently used machine learning methods within supervised, unsupervised, and semi-supervised machine learning, the most useful data sets for evaluating intrusion detection methods within supervised learning, and methods from machine learning that have shown promise in tackling various threats in defensive and offensive cybersecurity.} }
@article{WOS:000427574900001, title = {Deep Learning Applications in Medical Image Analysis}, journal = {IEEE ACCESS}, volume = {6}, pages = {9375-9389}, year = {2018}, issn = {2169-3536}, doi = {10.1109/ACCESS.2017.2788044}, author = {Ker, Justin and Wang, Lipo and Rao, Jai and Lim, Tchoyoson}, abstract = {The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.} }
@article{WOS:000573452600007, title = {Machine learning for microbial identification and antimicrobial susceptibility testing on MALDI-TOF mass spectra: a systematic review}, journal = {CLINICAL MICROBIOLOGY AND INFECTION}, volume = {26}, pages = {1310-1317}, year = {2020}, issn = {1198-743X}, doi = {10.1016/j.cmi.2020.03.014}, author = {Weis, V, C. and Jutzeler, C. R. and Borgwardt, K.}, abstract = {Background: The matrix assisted laser desorption/ionization and time-of-flight mass spectrometry (MALDI-TOF MS) technology has revolutionized the field of microbiology by facilitating precise and rapid species identification. Recently, machine learning techniques have been leveraged to maximally exploit the information contained in MALDI-TOF MS, with the ultimate goal to refine species identification and streamline antimicrobial resistance determination. Objectives: The aim was to systematically review and evaluate studies employing machine learning for the analysis of MALDI-TOF mass spectra. Data sources: Using PubMed/Medline, Scopus and Web of Science, we searched the existing literature for machine learning-supported applications of MALDI-TOF mass spectra for microbial species and antimicrobial susceptibility identification. Study eligibility criteria: Original research studies using machine learning to exploit MALDI-TOF mass spectra for microbial specie and antimicrobial susceptibility identification were included. Studies focusing on single proteins and peptides, case studies and review articles were excluded. Methods: A systematic review according to the PRISMA guidelines was performed and a quality assessment of the machine learning models conducted. Results: From the 36 studies that met our inclusion criteria, 27 employed machine learning for species identification and nine for antimicrobial susceptibility testing. Support Vector Machines, Genetic Algorithms, Artificial Neural Networks and Quick Classifiers were the most frequently used machine learning algorithms. The quality of the studies ranged between poor and very good. The majority of the studies reported how to interpret the predictors (88.89\\%) and suggested possible clinical applications of the developed algorithm (100\\%), but only four studies (11.11\\%) validated machine learning algorithms on external datasets. Conclusions: A growing number of studies utilize machine learning to optimize the analysis of MALDITOF mass spectra. This review, however, demonstrates that there are certain shortcomings of current machine learning-supported approaches that have to be addressed to make them widely available and incorporated them in the clinical routine. (C) 2020 The Authors. Published by Elsevier Ltd on behalf of European Society of Clinical Microbiology and Infectious Diseases.} }
@article{WOS:000530237100003, title = {Semantic Adversarial Deep Learning}, journal = {IEEE DESIGN \\& TEST}, volume = {37}, pages = {8-18}, year = {2020}, issn = {2168-2356}, doi = {10.1109/MDAT.2020.2968274}, author = {Seshia, Sanjit A. and Jha, Somesh and Dreossi, Tommaso}, abstract = {Adversarial examples have emerged as a key threat for machine-learning-based systems, especially the ones that employ deep neural networks. Unlike a large body of research in this area, this Keynote article accounts for the semantic, context, and specifications of the complete system with machine learning components in resource-constrained environments. -Muhammad Shafique, Technische Universitat Wien} }
@article{WOS:000988744600001, title = {A survey on machine learning based analysis of heterogeneous data in industrial automation}, journal = {COMPUTERS IN INDUSTRY}, volume = {149}, year = {2023}, issn = {0166-3615}, doi = {10.1016/j.compind.2023.103930}, author = {Kamm, Simon and Veekati, Sushma Sri and Mueller, Timo and Jazdi, Nasser and Weyrich, Michael}, abstract = {In many application domains data from different sources are increasingly available to thoroughly monitor and describe a system or device. Especially within the industrial automation domain, heterogeneous data and its analysis gain a lot of attention from research and industry, since it has the potential to improve or enable tasks like diagnostics, predictive maintenance, and condition monitoring. For data analysis, machine learning based approaches are mostly used in recent literature, as these algorithms allow us to learn complex correlations within the data. To analyze even heterogeneous data and gain benefits from it in an application, data from different sources need to be integrated, stored, and managed to apply machine learning algorithms. In a setting with heterogeneous data sources, the analysis algorithms should also be able to handle data source failures or newly added data sources. In addition, existing knowledge should be used to improve the machine learning based analysis or its training process. To find existing approaches for the machine learning based analysis of heterogeneous data in the industrial automation domain, this paper presents the result of a systematic literature review. The publications were reviewed, evaluated, and discussed concerning five requirements that are derived in this paper. We identified promising solutions and approaches and outlined open research challenges, which are not yet covered sufficiently in the literature.} }
@article{WOS:000595568500001, title = {Use of Machine Learning Approaches in Clinical Epidemiological Research of Diabetes}, journal = {CURRENT DIABETES REPORTS}, volume = {20}, year = {2020}, issn = {1534-4827}, doi = {10.1007/s11892-020-01353-5}, author = {Basu, Sanjay and Johnson, Karl T. and Berkowitz, Seth A.}, abstract = {Purpose of Review Machine learning approaches-which seek to predict outcomes or classify patient features by recognizing patterns in large datasets-are increasingly applied to clinical epidemiology research on diabetes. Given its novelty and emergence in fields outside of biomedical research, machine learning terminology, techniques, and research findings may be unfamiliar to diabetes researchers. Our aim was to present the use of machine learning approaches in an approachable way, drawing from clinical epidemiological research in diabetes published from 1 Jan 2017 to 1 June 2020. Recent Findings Machine learning approaches using tree-based learners-which produce decision trees to help guide clinical interventions-frequently have higher sensitivity and specificity than traditional regression models for risk prediction. Machine learning approaches using neural networking and ``deep learning'' can be applied to medical image data, particularly for the identification and staging of diabetic retinopathy and skin ulcers. Among the machine learning approaches reviewed, researchers identified new strategies to develop standard datasets for rigorous comparisons across older and newer approaches, methods to illustrate how a machine learner was treating underlying data, and approaches to improve the transparency of the machine learning process. Machine learning approaches have the potential to improve risk stratification and outcome prediction for clinical epidemiology applications. Achieving this potential would be facilitated by use of universal open-source datasets for fair comparisons. More work remains in the application of strategies to communicate how the machine learners are generating their predictions.} }
@article{WOS:000879006700002, title = {Machine learning for drilling applications: A review}, journal = {JOURNAL OF NATURAL GAS SCIENCE AND ENGINEERING}, volume = {108}, year = {2022}, issn = {1875-5100}, doi = {10.1016/j.jngse.2022.104807}, author = {Zhong, Ruizhi and Salehi, Cyrus and Johnson, Ray}, abstract = {In the past several decades, machine learning has gained increasing interest in the oil and gas industry. This paper presents a comprehensive review of machine learning studies for drilling applications in the following categories: (1) drilling fluids; (2) drilling hydraulics; (3) drilling dynamics; (4) drilling problems; and (5) miscellaneous drilling applications. In each study, the machine learning algorithm(s), sample size, inputs and output(s), and performance are extracted. In addition, similarities of studies in each category are summarized and recommendations are made for future development.} }
@article{WOS:000976730100001, title = {Recent Progresses in Machine Learning Assisted Raman Spectroscopy}, journal = {ADVANCED OPTICAL MATERIALS}, volume = {11}, year = {2023}, issn = {2195-1071}, doi = {10.1002/adom.202203104}, author = {Qi, Yaping and Hu, Dan and Jiang, Yucheng and Wu, Zhenping and Zheng, Ming and Chen, Esther Xinyi and Liang, Yong and Sadi, Mohammad A. A. and Zhang, Kang and Chen, Yong P. P.}, abstract = {With the development of Raman spectroscopy and the expansion of its application domains, conventional methods for spectral data analysis have manifested many limitations. Exploring new approaches to facilitate Raman spectroscopy and analysis has become an area of intensifying focus for research. It has been demonstrated that machine learning techniques can more efficiently extract valuable information from spectral data, creating unprecedented opportunities for analytical science. This paper outlines traditional and more recently developed statistical methods that are commonly used in machine learning (ML) and ML-algorithms for different Raman spectroscopy-based classification and recognition applications. The methods include Principal Component Analysis, K-Nearest Neighbor, Random Forest, and Support Vector Machine, as well as neural network-based deep learning algorithms such as Artificial Neural Networks, Convolutional Neural Networks, etc. The bulk of the review is dedicated to the research advances in machine learning applied to Raman spectroscopy from several fields, including material science, biomedical applications, food science, and others, which reached impressive levels of analytical accuracy. The combination of Raman spectroscopy and machine learning offers unprecedented opportunities to achieve high throughput and fast identification in many of these application fields. The limitations of current studies are also discussed and perspectives on future research are provided.} }
@article{WOS:001076506500001, title = {Bridging the Worlds of Pharmacometrics and Machine Learning}, journal = {CLINICAL PHARMACOKINETICS}, volume = {62}, pages = {1551-1565}, year = {2023}, issn = {0312-5963}, doi = {10.1007/s40262-023-01310-x}, author = {Stankeviciute, Kamile and Woillard, Jean-Baptiste and Peck, Richard W. and Marquet, Pierre and van der Schaar, Mihaela}, abstract = {Precision medicine requires individualized modeling of disease and drug dynamics, with machine learning-based computational techniques gaining increasing popularity. The complexity of either field, however, makes current pharmacological problems opaque to machine learning practitioners, and state-of-the-art machine learning methods inaccessible to pharmacometricians. To help bridge the two worlds, we provide an introduction to current problems and techniques in pharmacometrics that ranges from pharmacokinetic and pharmacodynamic modeling to pharmacometric simulations, model-informed precision dosing, and systems pharmacology, and review some of the machine learning approaches to address them. We hope this would facilitate collaboration between experts, with complementary strengths of principled pharmacometric modeling and flexibility of machine learning leading to synergistic effects in pharmacological applications.} }
@article{WOS:000605079100003, title = {CAN MACHINES ``LEARN'' FINANCE?}, journal = {JOURNAL OF INVESTMENT MANAGEMENT}, volume = {18}, pages = {23-36}, year = {2020}, issn = {1545-9144}, author = {Israel, Ronen and Kelly, Bryan and Moskowitz, Tobias}, abstract = {Machine learning for asset management faces a unique set of challenges that differ markedly from other domains where machine learning has excelled. Understanding these differences is critical for developing impactful approaches and realistic expectations for machine learning in asset management. We discuss a variety of beneficial use cases and potential pitfalls, and emphasize the importance of economic theory and human expertise for achieving success through financial machine learning.} }
@article{WOS:000537076300001, title = {The virtue of simplicity: On machine learning models in algorithmic trading}, journal = {BIG DATA \\& SOCIETY}, volume = {7}, year = {2020}, issn = {2053-9517}, doi = {10.1177/2053951720926558}, author = {Hansen, Kristian Bondo}, abstract = {Machine learning models are becoming increasingly prevalent in algorithmic trading and investment management. The spread of machine learning in finance challenges existing practices of modelling and model use and creates a demand for practical solutions for how to manage the complexity pertaining to these techniques. Drawing on interviews with quants applying machine learning techniques to financial problems, the article examines how these people manage model complexity in the process of devising machine learning-powered trading algorithms. The analysis shows that machine learning quants use Ockham's razor - things should not be multiplied without necessity - as a heuristic tool to prevent excess model complexity and secure a certain level of human control and interpretability in the modelling process. I argue that understanding the way quants handle the complexity of learning models is a key to grasping the transformation of the human's role in contemporary data and model-driven finance. The study contributes to social studies of finance research on the human-model interplay by exploring it in the context of machine learning model use.} }
@article{WOS:001114383400001, title = {From pixels to insights: Machine learning and deep learning for bioimage analysis}, journal = {BIOESSAYS}, volume = {46}, year = {2024}, issn = {0265-9247}, doi = {10.1002/bies.202300114}, author = {Jan, Mahta and Spangaro, Allie and Lenartowicz, Michelle and Usaj, Mojca Mattiazzi}, abstract = {Bioimage analysis plays a critical role in extracting information from biological images, enabling deeper insights into cellular structures and processes. The integration of machine learning and deep learning techniques has revolutionized the field, enabling the automated, reproducible, and accurate analysis of biological images. Here, we provide an overview of the history and principles of machine learning and deep learning in the context of bioimage analysis. We discuss the essential steps of the bioimage analysis workflow, emphasizing how machine learning and deep learning have improved preprocessing, segmentation, feature extraction, object tracking, and classification. We provide examples that showcase the application of machine learning and deep learning in bioimage analysis. We examine user-friendly software and tools that enable biologists to leverage these techniques without extensive computational expertise. This review is a resource for researchers seeking to incorporate machine learning and deep learning in their bioimage analysis workflows and enhance their research in this rapidly evolving field. Machine learning and deep learning have revolutionized bioimage analysis, automating and enhancing tasks like image preprocessing, object segmentation and tracking, feature extraction, and classification. This review showcases the pivotal role these approaches have played in the field, and highlights user-friendly bioimage analysis tools for biologists without extensive computational expertise. image} }
@article{WOS:000880955300001, title = {A Method for Analyzing the Performance Impact of Imbalanced Binary Data on Machine Learning Models}, journal = {AXIOMS}, volume = {11}, year = {2022}, doi = {10.3390/axioms11110607}, author = {Zheng, Ming and Wang, Fei and Hu, Xiaowen and Miao, Yuhao and Cao, Huo and Tang, Mingjing}, abstract = {Machine learning models may not be able to effectively learn and predict from imbalanced data in the fields of machine learning and data mining. This study proposed a method for analyzing the performance impact of imbalanced binary data on machine learning models. It systematically analyzes 1. the relationship between varying performance in machine learning models and imbalance rate (IR); 2. the performance stability of machine learning models on imbalanced binary data. In the proposed method, the imbalanced data augmentation algorithms are first designed to obtain the imbalanced dataset with gradually varying IR. Then, in order to obtain more objective classification results, the evaluation metric AFG, arithmetic mean of area under the receiver operating characteristic curve (AUC), F-measure and G-mean are used to evaluate the classification performance of machine learning models. Finally, based on AFG and coefficient of variation (CV), the performance stability evaluation method of machine learning models is proposed. Experiments of eight widely used machine learning models on 48 different imbalanced datasets demonstrate that the classification performance of machine learning models decreases with the increase of IR on the same imbalanced data. Meanwhile, the classification performances of LR, DT and SVC are unstable, while GNB, BNB, KNN, RF and GBDT are relatively stable and not susceptible to imbalanced data. In particular, the BNB has the most stable classification performance. The Friedman and Nemenyi post hoc statistical tests also confirmed this result. The SMOTE method is used in oversampling-based imbalanced data augmentation, and determining whether other oversampling methods can obtain consistent results needs further research. In the future, an imbalanced data augmentation algorithm based on undersampling and hybrid sampling should be used to analyze the performance impact of imbalanced binary data on machine learning models.} }
@article{WOS:001279022900001, title = {A survey on fault diagnosis of rotating machinery based on machine learning}, journal = {MEASUREMENT SCIENCE AND TECHNOLOGY}, volume = {35}, year = {2024}, issn = {0957-0233}, doi = {10.1088/1361-6501/ad6203}, author = {Wang, Qi and Huang, Rui and Xiong, Jianbin and Yang, Jianxiang and Dong, Xiangjun and Wu, Yipeng and Wu, Yinbo and Lu, Tiantian}, abstract = {With the booming development of modern industrial technology, rotating machinery fault diagnosis is of great significance to improve the safety, efficiency and sustainable development of industrial production. Machine learning as an effective solution for fault identification, has advantages over traditional fault diagnosis solutions in processing complex data, achieving automation and intelligence, adapting to different fault types, and continuously optimizing. It has high application value and broad development prospects in the field of fault diagnosis of rotating machinery. Therefore, this article reviews machine learning and its applications in intelligent fault diagnosis technology and covers advanced topics in emerging deep learning techniques and optimization methods. Firstly, this article briefly introduces the theories of several main machine learning methods, including Extreme Learning Machines (ELM), Support Vector Machines (SVM), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs) and related emerging deep learning technologies such as Transformer, adversarial neural network (GAN) and graph neural network (GNN) in recent years. The optimization techniques for diagnosing faults in rotating machinery are subsequently investigated. Then, a brief introduction is given to the papers on the application of these machine learning methods in the field of rotating machinery fault diagnosis, and the application characteristics of various methods are summarized. Finally, this survey discusses the problems to be solved by machine learning in fault diagnosis of rotating machinery and proposes an outlook.} }
@article{WOS:000728330600001, title = {Machine learning techniques to predict daily rainfall amount}, journal = {JOURNAL OF BIG DATA}, volume = {8}, year = {2021}, doi = {10.1186/s40537-021-00545-4}, author = {Liyew, Chalachew Muluken and Melese, Haileyesus Amsaya}, abstract = {Predicting the amount of daily rainfall improves agricultural productivity and secures food and water supply to keep citizens healthy. To predict rainfall, several types of research have been conducted using data mining and machine learning techniques of different countries' environmental datasets. An erratic rainfall distribution in the country affects the agriculture on which the economy of the country depends on. Wise use of rainfall water should be planned and practiced in the country to minimize the problem of the drought and flood occurred in the country. The main objective of this study is to identify the relevant atmospheric features that cause rainfall and predict the intensity of daily rainfall using machine learning techniques. The Pearson correlation technique was used to select relevant environmental variables which were used as an input for the machine learning model. The dataset was collected from the local meteorological office at Bahir Dar City, Ethiopia to measure the performance of three machine learning techniques (Multivariate Linear Regression, Random Forest, and Extreme Gradient Boost). Root mean squared error and Mean absolute Error methods were used to measure the performance of the machine learning model. The result of the study revealed that the Extreme Gradient Boosting machine learning algorithm performed better than others.} }
@article{WOS:000754809300001, title = {Using Machine Learning to Predict Complications in Pregnancy: A Systematic Review}, journal = {FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY}, volume = {9}, year = {2022}, issn = {2296-4185}, doi = {10.3389/fbioe.2021.780389}, author = {Bertini, Ayleen and Salas, Rodrigo and Chabert, Steren and Sobrevia, Luis and Pardo, Fabian}, abstract = {Introduction: Artificial intelligence is widely used in medical field, and machine learning has been increasingly used in health care, prediction, and diagnosis and as a method of determining priority. Machine learning methods have been features of several tools in the fields of obstetrics and childcare. This present review aims to summarize the machine learning techniques to predict perinatal complications.Objective: To identify the applicability and performance of machine learning methods used to identify pregnancy complications.Methods: A total of 98 articles were obtained with the keywords ``machine learning,'' ``deep learning,'' ``artificial intelligence,'' and accordingly as they related to perinatal complications (''complications in pregnancy,'' ``pregnancy complications'') from three scientific databases: PubMed, Scopus, and Web of Science. These were managed on the Mendeley platform and classified using the PRISMA method.Results: A total of 31 articles were selected after elimination according to inclusion and exclusion criteria. The features used to predict perinatal complications were primarily electronic medical records (48\\%), medical images (29\\%), and biological markers (19\\%), while 4\\% were based on other types of features, such as sensors and fetal heart rate. The main perinatal complications considered in the application of machine learning thus far are pre-eclampsia and prematurity. In the 31 studies, a total of sixteen complications were predicted. The main precision metric used is the AUC. The machine learning methods with the best results were the prediction of prematurity from medical images using the support vector machine technique, with an accuracy of 95.7\\%, and the prediction of neonatal mortality with the XGBoost technique, with 99.7\\% accuracy.Conclusion: It is important to continue promoting this area of research and promote solutions with multicenter clinical applicability through machine learning to reduce perinatal complications. This systematic review contributes significantly to the specialized literature on artificial intelligence and women's health.} }
@article{WOS:000620348900005, title = {Machine Learning in Predictive Toxicology: Recent Applications and Future Directions for Classification Models}, journal = {CHEMICAL RESEARCH IN TOXICOLOGY}, volume = {34}, pages = {217-239}, year = {2021}, issn = {0893-228X}, doi = {10.1021/acs.chemrestox.0c00316}, author = {Wang, Marcus W. H. and Goodman, Jonathan M. and Allen, Timothy E. H.}, abstract = {In recent times, machine learning has become increasingly prominent in predictive toxicology as it has shifted from in vivo studies toward in silico studies. Currently, in vitro methods together with other computational methods such as quantitative structure-activity relationship modeling and absorption, distribution, metabolism, and excretion calculations are being used. An overview of machine learning and its applications in predictive toxicology is presented here, including support vector machines (SVMs), random forest (RF) and decision trees (DTs), neural networks, regression models, naive Bayes, k-nearest neighbors, and ensemble learning. The recent successes of these machine learning methods in predictive toxicology are summarized, and a comparison of some models used in predictive toxicology is presented. In predictive toxicology, SVMs, RF, and DTs are the dominant machine learning methods due to the characteristics of the data available. Lastly, this review describes the current challenges facing the use of machine learning in predictive toxicology and offers insights into the possible areas of improvement in the field.} }
@article{WOS:001239040700001, title = {Applications of machine learning in phylogenetics}, journal = {MOLECULAR PHYLOGENETICS AND EVOLUTION}, volume = {196}, year = {2024}, issn = {1055-7903}, doi = {10.1016/j.ympev.2024.108066}, author = {Mo, Yu K. and Hahn, Matthew W. and Smith, Megan L.}, abstract = {Machine learning has increasingly been applied to a wide range of questions in phylogenetic inference. Supervised machine learning approaches that rely on simulated training data have been used to infer tree topologies and branch lengths, to select substitution models, and to perform downstream inferences of introgression and diversification. Here, we review how researchers have used several promising machine learning approaches to make phylogenetic inferences. Despite the promise of these methods, several barriers prevent supervised machine learning from reaching its full potential in phylogenetics. We discuss these barriers and potential paths forward. In the future, we expect that the application of careful network designs and data encodings will allow supervised machine learning to accommodate the complex processes that continue to confound traditional phylogenetic methods.} }
@article{WOS:000671787900017, title = {Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3453158}, author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior}, abstract = {In recent years, machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This article comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker' s goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. To the best of our knowledge, this work is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.} }
@article{WOS:000708780800001, title = {Reinforcement learning applications to machine scheduling problems: a comprehensive literature review}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {34}, pages = {905-929}, year = {2023}, issn = {0956-5515}, doi = {10.1007/s10845-021-01847-3}, author = {Kayhan, Behice Meltem and Yildiz, Gokalp}, abstract = {Reinforcement learning (RL) is one of the most remarkable branches of machine learning and attracts the attention of researchers from numerous fields. Especially in recent years, the RL methods have been applied to machine scheduling problems and are among the top five most encouraging methods for scheduling literature. Therefore, in this study, a comprehensive literature review about RL methods applications to machine scheduling problems was conducted. In this regard, Scopus and Web of Science databases were searched very inclusively using the proper keywords. As a result of the comprehensive research, 80 papers were found, published between 1995 and 2020. These papers were analyzed considering different aspects of the problem such as applied algorithms, machine environments, job and machine characteristics, objectives, benchmark methods, and a detailed classification scheme was constructed. Job shop scheduling, unrelated parallel machine scheduling, and single machine scheduling problems were found as the most studied problem type. The main contributions of the study are to examine essential aspects of reinforcement learning in machine scheduling problems, identify the most frequently investigated problem types, objectives, and constraints, and reveal the deficiencies and promising areas in the related literature. This study can help researchers who wish to study in this field through the comprehensive analysis of the related literature.} }
@article{WOS:000880508300001, title = {Stability modeling for chatter avoidance in self-aware machining: an application of physics-guided machine learning}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {34}, pages = {387-413}, year = {2023}, issn = {0956-5515}, doi = {10.1007/s10845-022-01999-w}, author = {Greis, Noel P. and Nogueira, Monica L. and Bhattacharya, Sambit and Spooner, Catherine and Schmitz, Tony}, abstract = {Physics-guided machine learning (PGML) offers a new approach to stability modeling during machining that leverages experimental data generated during the machining process while incorporating decades of theoretical process modeling efforts. This approach addresses specific limitations of machine learning models and physics-based models individually. Data-driven machine learning models are typically black box models that do not provide deep insight into the underlying physics and do not reflect physical constraints for the modeled system, sometimes yielding solutions that violate physical laws or operational constraints. In addition, acquiring the large amounts of manufacturing data needed for machine learning modeling can be costly. On the other hand, many physical processes are not completely understood by domain experts and have a high degree of uncertainty. Physics-based models must make simplifying assumptions that can compromise prediction accuracy. This research explores whether data generated by an uncertain physics-based milling stability model that is used to train a physics-guided machine learning stability model, and then updated with measured data, domain knowledge, and theory-based knowledge provides a useful approximation to the unknown true stability model for a specific set of factory operating conditions. Four novel strategies for updating the machine learning model with experimental data are explored. These updating strategies differ in their assumptions about and implementation of the type of physics-based knowledge included in the PGML model. Using a simulation experiment, these strategies achieve useful approximations of the underlying true stability model while reducing the number of experimental measurements required for model update.} }
@article{WOS:000787328500007, title = {Machine learning in modelling land-use and land cover-change (LULCC): Current status, challenges and prospects}, journal = {SCIENCE OF THE TOTAL ENVIRONMENT}, volume = {822}, year = {2022}, issn = {0048-9697}, doi = {10.1016/j.scitotenv.2022.153559}, author = {Wang, Junye and Bretz, Michael and Dewan, M. Ali Akber and Delavar, Mojtaba Aghajani}, abstract = {Land-use and land-cover change (LULCC) are of importance in natural resource management, environmental modelling and assessment, and agricultural production management. However, LULCC detection and modelling is a complex, data-driven process in the remote sensing field due to the processing of massive historical and current data, real-time interaction of scenario data, and spatial environmental data. In this paper, we review principles and methods of LULCC modelling, using machine learning and beyond, such as traditional cellular automata (CA). Then, we examine the characteristics, capabilities, limitations, and perspectives of machine learning. Machine learning has not yet been dramatic in modelling LULCC, such as urbanization prediction and crop yield prediction because competition and transition between land cover types are dynamic at a local scale under varying natural drivers and human activities. Upcoming challenges of machine learning in modelling LULCC remain in the detection and prediction of LULC evolutionary processes if considering their applicability and feasibility, such as the spatio-temporal transition mechanisms to describe occurrence, transition, spreading, and spatial patterns of changes, availability of training data of all the change drivers, particularly sequence data, and identification and inclusion of local ecological, hydrological, and social-economic drivers in addressing the spectral feature change. This review points out the need for multidisciplinary research beyond image processing and pattern recognition of machine learning in accelerating and advancing studies of LULCC modelling. Despite this, we believe that machine learning has strong potentials to incorporate new exploratory variables in modelling LULCC through expanding remote sensing big data and advancing transient algorithms.} }
@article{WOS:000438855100013, title = {A survey on application of machine learning for Internet of Things}, journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS}, volume = {9}, pages = {1399-1417}, year = {2018}, issn = {1868-8071}, doi = {10.1007/s13042-018-0834-5}, author = {Cui, Laizhong and Yang, Shu and Chen, Fei and Ming, Zhong and Lu, Nan and Qin, Jing}, abstract = {Internet of Things (IoT) has become an important network paradigm and there are lots of smart devices connected by IoT. IoT systems are producing massive data and thus more and more IoT applications and services are emerging. Machine learning, as an another important area, has obtained a great success in several research fields such as computer vision, computer graphics, natural language processing, speech recognition, decision-making, and intelligent control. It has also been introduced in networking research. Many researches study how to utilize machine learning to solve networking problems, including routing, traffic engineering, resource allocation, and security. Recently, there has been a rising trend of employing machine learning to improve IoT applications and provide IoT services such as traffic engineering, network management, security, Internet traffic classification, and quality of service optimization. This survey paper focuses on providing an overview of the application of machine learning in the domain of IoT. We provide a comprehensive survey highlighting the recent progresses in machine learning techniques for IoT and describe various IoT applications. The application of machine learning for IoT enables users to obtain deep analytics and develop efficient intelligent IoT applications. This paper is different from the previously published survey papers in terms of focus, scope, and breadth; specifically, we have written this paper to emphasize the application of machine learning for IoT and the coverage of most recent advances. This paper has made an attempt to cover the major applications of machine learning for IoT and the relevant techniques, including traffic profiling, IoT device identification, security, edge computing infrastructure, network management and typical IoT applications. We also make a discussion on research challenges and open issues.} }
@article{WOS:000969652500001, title = {Primer on Machine Learning in Electrophysiology}, journal = {ARRHYTHMIA \\& ELECTROPHYSIOLOGY REVIEW}, volume = {12}, year = {2023}, issn = {2050-3369}, doi = {10.15420/aer.2022.43}, author = {Loeffler, Shane E. and Trayanova, Natalia}, abstract = {Artificial intelligence has become ubiquitous. Machine learning, a branch of artificial intelligence, leads the current technological revolution through its remarkable ability to learn and perform on data sets of varying types. Machine learning applications are expected to change contemporary medicine as they are brought into mainstream clinical practice. In the field of cardiac arrhythmia and electrophysiology, machine learning applications have enjoyed rapid growth and popularity. To facilitate clinical acceptance of these methodologies, it is important to promote general knowledge of machine learning in the wider community and continue to highlight the areas of successful application. The authors present a primer to provide an overview of common supervised (least squares, support vector machine, neural networks and random forest) and unsupervised (k-means and principal component analysis) machine learning models. The authors also provide explanations as to how and why the specific machine learning models have been used in arrhythmia and electrophysiology studies.} }
@article{WOS:000472029100018, title = {Predicting the Young's Modulus of Silicate Glasses using High-Throughput Molecular Dynamics Simulations and Machine Learning}, journal = {SCIENTIFIC REPORTS}, volume = {9}, year = {2019}, issn = {2045-2322}, doi = {10.1038/s41598-019-45344-3}, author = {Yang, Kai and Xu, Xinyi and Yang, Benjamin and Cook, Brian and Ramos, Herbert and Krishnan, N. M. Anoop and Smedskjaer, Morten M. and Hoover, Christian and Bauchy, Mathieu}, abstract = {The application of machine learning to predict materials' properties usually requires a large number of consistent data for training. However, experimental datasets of high quality are not always available or self-consistent. Here, as an alternative route, we combine machine learning with high-throughput molecular dynamics simulations to predict theYoung's modulus of silicate glasses. We demonstrate that this combined approach offers good and reliable predictions over the entire compositional domain. By comparing the performances of select machine learning algorithms, we discuss the nature of the balance between accuracy, simplicity, and interpretability in machine learning.} }
@article{WOS:000418675900003, title = {Applications of Support Vector Machine (SVM) Learning in Cancer Genomics}, journal = {CANCER GENOMICS \\& PROTEOMICS}, volume = {15}, pages = {41-51}, year = {2018}, issn = {1109-6535}, doi = {10.21873/cgp.20063}, author = {Huang, Shujun and Cai, Nianguang and Pacheco, Pedro Penzuti and Narandes, Shavira and Wang, Yang and Xu, Wayne}, abstract = {Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.} }
@article{WOS:000462604300001, title = {Machine Learning SNP Based Prediction for Precision Medicine}, journal = {FRONTIERS IN GENETICS}, volume = {10}, year = {2019}, doi = {10.3389/fgene.2019.00267}, author = {Ho, Daniel Sik Wai and Schierding, William and Wake, Melissa and Saffery, Richard and O'Sullivan, Justin}, abstract = {In the past decade, precision genomics based medicine has emerged to provide tailored and effective healthcare for patients depending upon their genetic features. Genome Wide Association Studies have also identified population based risk genetic variants for common and complex diseases. In order to meet the full promise of precision medicine, research is attempting to leverage our increasing genomic understanding and further develop personalized medical healthcare through ever more accurate disease risk prediction models. Polygenic risk scoring and machine learning are two primary approaches for disease risk prediction. Despite recent improvements, the results of polygenic risk scoring remain limited due to the approaches that are currently used. By contrast, machine learning algorithms have increased predictive abilities for complex disease risk. This increase in predictive abilities results from the ability of machine learning algorithms to handle multi-dimensional data. Here, we provide an overview of polygenic risk scoring and machine learning in complex disease risk prediction. We highlight recent machine learning application developments and describe how machine learning approaches can lead to improved complex disease prediction, which will help to incorporate genetic features into future personalized healthcare. Finally, we discuss how the future application of machine learning prediction models might help manage complex disease by providing tissue-specific targets for customized, preventive interventions.} }
@article{WOS:001085771300006, title = {Predictive modelling and analytics for diabetes using a machine learning approach}, journal = {APPLIED COMPUTING AND INFORMATICS}, volume = {18}, pages = {90-100}, year = {2022}, issn = {2634-1964}, doi = {10.1016/j.aci.2018.12.004}, author = {Kaur, Harleen and Kumari, Vinita}, abstract = {Diabetes is a major metabolic disorder which can affect entire body system adversely. Undiagnosed diabetes can increase the risk of cardiac stroke, diabetic nephropathy and other disorders. All over the world millions of people are affected by this disease. Early detection of diabetes is very important to maintain a healthy life. This disease is a reason of global concern as the cases of diabetes are rising rapidly. Machine learning (ML) is a computational method for automatic learning from experience and improves the performance to make more accurate predictions. In the current research we have utilized machine learning technique in Pima Indian diabetes dataset to develop trends and detect patterns with risk factors using R data manipulation tool. To classify the patients into diabetic and non-diabetic we have developed and analyzed five different predictive models using R data manipulation tool. For this purpose we used supervised machine learning algorithms namely linear kernel support vector machine (SVM-linear), radial basis function (RBF) kernel support vector machine, k-nearest neighbour (k-NN), artificial neural network (ANN) and multifactor dimensionality reduction (MDR).} }
@article{WOS:000908510300001, title = {Interpretable Machine Learning Techniques in ECG-Based Heart Disease Classification: A Systematic Review}, journal = {DIAGNOSTICS}, volume = {13}, year = {2023}, doi = {10.3390/diagnostics13010111}, author = {Ayano, Yehualashet Megersa and Schwenker, Friedhelm and Dufera, Bisrat Derebssa and Debelee, Taye Girma}, abstract = {Heart disease is one of the leading causes of mortality throughout the world. Among the different heart diagnosis techniques, an electrocardiogram (ECG) is the least expensive non-invasive procedure. However, the following are challenges: the scarcity of medical experts, the complexity of ECG interpretations, the manifestation similarities of heart disease in ECG signals, and heart disease comorbidity. Machine learning algorithms are viable alternatives to the traditional diagnoses of heart disease from ECG signals. However, the black box nature of complex machine learning algorithms and the difficulty in explaining a model's outcomes are obstacles for medical practitioners in having confidence in machine learning models. This observation paves the way for interpretable machine learning (IML) models as diagnostic tools that can build a physician's trust and provide evidence-based diagnoses. Therefore, in this systematic literature review, we studied and analyzed the research landscape in interpretable machine learning techniques by focusing on heart disease diagnosis from an ECG signal. In this regard, the contribution of our work is manifold; first, we present an elaborate discussion on interpretable machine learning techniques. In addition, we identify and characterize ECG signal recording datasets that are readily available for machine learning-based tasks. Furthermore, we identify the progress that has been achieved in ECG signal interpretation using IML techniques. Finally, we discuss the limitations and challenges of IML techniques in interpreting ECG signals.} }
@article{WOS:000492778000001, title = {Rethinking Drug Repositioning and Development with Artificial Intelligence, Machine Learning, and Omics}, journal = {OMICS-A JOURNAL OF INTEGRATIVE BIOLOGY}, volume = {23}, pages = {539-548}, year = {2019}, issn = {1536-2310}, doi = {10.1089/omi.2019.0151}, author = {Koromina, Maria and Pandi, Maria-Theodora and Patrinos, George P.}, abstract = {Pharmaceutical industry and the art and science of drug development are sorely in need of novel transformative technologies in the current age of digital health and artificial intelligence (AI). Often described as game-changing technologies, AI and machine learning algorithms have slowly but surely begun to revolutionize pharmaceutical industry and drug development over the past 5 years. In this expert review, we describe the most frequently used machine learning algorithms in drug development pipelines and the -omics databases well poised to support machine learning and drug discovery. Subsequently, we analyze the emerging new computational approaches to drug discovery and the in silico pipelines for drug repositioning and the synergies among -omics system sciences, AI and machine learning. As with system sciences, AI and machine learning embody a system scale and Big Data driven vision for drug discovery and development. We conclude with a future outlook on the ways in which machine learning approaches can be implemented to buttress and expedite drug discovery and precision medicine. As AI and machine learning are rapidly entering pharmaceutical industry and the art and science of drug development, we need to critically examine the attendant prospects and challenges to benefit patients and public health.} }
@article{WOS:000505643500038, title = {Equivalent circuit model recognition of electrochemical impedance spectroscopy via machine learning}, journal = {JOURNAL OF ELECTROANALYTICAL CHEMISTRY}, volume = {855}, year = {2019}, issn = {1572-6657}, doi = {10.1016/j.jelechem.2019.113627}, author = {Zhu, Shan and Sun, Xinyang and Gao, Xiaoyang and Wang, Jianrong and Zhao, Naiqin and Sha, Junwei}, abstract = {Electrochemical impedance spectroscopy (EIS) is an effective method for studying electrochemical systems. The interpretation of EIS is the biggest challenge in this technology, which requires reasonable modeling. To overcome the subjectivity of human analysis, this work uses machine learning to carry out EIS model recognition. Raw EIS data and their equivalent circuit models are collected from the literature, and the support vector machine (SVM) is used to analyze these data. Comparing with other machine learning algorithms, SVM achieves the best comprehensive performance in this database. As a result, the optimized SVM model can efficiently figure out the most suitable equivalent circuit model of the given EIS spectrum. This study demonstrates the great potential of machine learning in electrochemical researches.} }
@article{WOS:000484832800014, title = {Do no harm: a roadmap for responsible machine learning for health care}, journal = {NATURE MEDICINE}, volume = {25}, pages = {1337-1340}, year = {2019}, issn = {1078-8956}, doi = {10.1038/s41591-019-0548-6}, author = {Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and Liu, Vincent X. and Doshi-Velez, Finale and Jung, Kenneth and Heller, Katherine and Kale, David and Saeed, Mohammed and Ossorio, Pilar N. and Thadaney-Israni, Sonoo and Goldenberg, Anna}, abstract = {Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).} }
@article{WOS:000466934400012, title = {Machine Learning for Semi Linear PDEs}, journal = {JOURNAL OF SCIENTIFIC COMPUTING}, volume = {79}, pages = {1667-1712}, year = {2019}, issn = {0885-7474}, doi = {10.1007/s10915-019-00908-3}, author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier}, abstract = {Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.} }
@article{WOS:000457022100009, title = {Improved online sequential extreme learning machine for identifying crack behavior in concrete dam}, journal = {ADVANCES IN STRUCTURAL ENGINEERING}, volume = {22}, pages = {402-412}, year = {2019}, issn = {1369-4332}, doi = {10.1177/1369433218788635}, author = {Dai, Bo and Gu, Chongshi and Zhao, Erfeng and Zhu, Kai and Cao, Wenhan and Qin, Xiangnan}, abstract = {Prediction models are essential in dam crack behavior identification. Prototype monitoring data arrive sequentially in dam safety monitoring. Given such characteristic, sequential learning algorithms are preferred over batch learning algorithms as they do not require retraining whenever new data are received. A new methodology using the genetic optimized online sequential extreme learning machine and bootstrap confidence intervals is proposed as a practical tool for identifying concrete dam crack behavior. First, online sequential extreme learning machine is adopted to build an online prediction model of crack behavior. The characteristic vector of crack behavior, which is taken as the online sequential extreme learning machine input, is extracted by the statistical model. A genetic algorithm is introduced to optimize the input weights and biases of online sequential extreme learning machine. Second, the BC(a )method is proposed to produce confidence intervals based on the improved online sequential extreme learning machine prediction. The improved online sequential extreme learning machine for identifying crack behavior is then built. Third, the crack behavior of an actual concrete dam is taken as an example. The capability of the built model for predicting dam crack opening is evaluated. The comparative results demonstrate that the improved online sequential extreme learning machine can provide highly accurate forecasts and reasonably identify crack behavior.} }
@article{WOS:000818095000001, title = {Interpretable and Explainable Machine Learning for Materials Science and Chemistry}, journal = {ACCOUNTS OF MATERIALS RESEARCH}, volume = {3}, pages = {597-607}, year = {2022}, doi = {10.1021/accountsmr.1c00244}, author = {Oviedo, Felipe and Ferres, Juan Lavista and Buonassisi, Tonio and Butler, Keith T.}, abstract = {CONSPECTUS: Machine learning has become a common and powerful tool in materials research. As more data become available, with the use of high-performance computing and high-throughput experimentation, machine learning has proven potential to accelerate scientific research and technology development. Though the uptake of data-driven approaches for materials science is at an exciting, early stage, to realize the true potential of machine learning models for successful scientific discovery, they must have qualities beyond purely predictive power. The predictions and inner workings of models should provide a certain degree of explainability by human experts, permitting the identification of potential model issues or limitations, building trust in model predictions, and unveiling unexpected correlations that may lead to scientific insights. In this work, we summarize applications of interpretability and explainability techniques for materials science and chemistry and discuss how these techniques can improve the outcome of scientific studies. We start by defining the fundamental concepts of interpretability and explainability in machine learning and making them less abstract by providing examples in the field. We show how interpretability in scientific machine learning has additional constraints compared to general applications. Building upon formal definitions in machine learning, we formulate the basic trade-offs among the explainability, completeness, and scientific validity of model explanations in scientific problems. In the context of these trade-offs, we discuss how interpretable models can be constructed, what insights they provide, and what drawbacks they have. We present numerous examples of the application of interpretable machine learning in a variety of experimental and simulation studies, encompassing first-principles calculations, physicochemical characterization, materials development, and integration into complex systems. We discuss the varied impacts and uses of interpretabiltiy in these cases according to the nature and constraints of the scientific study of interest. We discuss various challenges for interpretable machine learning in materials science and, more broadly, in scientific settings. In particular, we emphasize the risks of inferring causation or reaching generalization by purely interpreting machine learning models and the need for uncertainty estimates for model explanations. Finally, we showcase a number of exciting developments in other fields that could benefit interpretability in material science problems. Adding interpretability to a machine learning model often requires no more technical know-how than building the model itself. By providing concrete examples of studies (many with associated open source code and data), we hope that this Account will encourage all practitioners of machine learning in materials science to look deeper into their models.} }
@article{WOS:000649679300001, title = {Applications of artificial intelligence and machine learning approaches in echocardiography}, journal = {ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ULTRASOUND AND ALLIED TECHNIQUES}, volume = {38}, pages = {982-992}, year = {2021}, issn = {0742-2822}, doi = {10.1111/echo.15048}, author = {Nabi, Wafa and Bansal, Agam and Xu, Bo}, abstract = {Artificial intelligence and machine learning approaches have become increasingly applied in the field of echocardiography to streamline diagnostic and prognostic assessments, and to support treatment decisions. Artificial intelligence and machine learning have been applied to aid image acquisition and automation. They have also been applied to the integration of clinical and imaging data. Applications of artificial intelligence and machine learning approaches in echocardiography in conjunction with health information databases may be promising in improving the classification and treatment of many cardiac conditions. This review article provides an overview of the applications of artificial intelligence and machine learning approaches in echocardiography.} }
@article{WOS:000471070400002, title = {Can Machine Learning Revolutionize Directed Evolution of Selective Enzymes?}, journal = {ADVANCED SYNTHESIS \\& CATALYSIS}, volume = {361}, pages = {2377-2386}, year = {2019}, issn = {1615-4150}, doi = {10.1002/adsc.201900149}, author = {Li, Guangyue and Dong, Yijie and Reetz, Manfred T.}, abstract = {Machine learning as a form of artificial intelligence consists of algorithms and statistical models for improving computer performance for different tasks. Training data are utilized for making decisions and predictions. Since directed evolution of enzymes produces huge amounts of potential training data, machine learning seems to be ideally suited to support this protein engineering technique. Machine learning has been used in protein science for a long time with different purposes. This mini-review focuses on the utility of machine learning as an aid in the directed evolution of selective enzymes. Recent studies have shown that the algorithms ASRA and Innov'SAR are well suited as guides when performing saturation mutagenesis at sites lining the binding pocket for enhancing stereoselectivity and activity.} }
@article{WOS:000855316400001, title = {Research Progress and Trend of the Machine Learning based on Fusion}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {13}, pages = {1-7}, year = {2022}, issn = {2158-107X}, author = {Yu, Chen Xiao and Ying, Song and Min, Zhang Xiao and Feng, Gao}, abstract = {Machine learning is widely used in the data processing including data classification, data regression, data mining and so on, and based on a single type of machine learning technology, it is often difficult to meet the requirements of data processing; in recent years, the machine learning based on fusion has become an important approach to improve data processing effect, and at the same time, corresponding summary study is relatively limited. In this study, we summarize and compare different types of fusion machine learning such as ensemble learning, federated learning and transfer learning from the perspectives of classification, principle and characteristics, and try to explore the research development trend, in order to provide effective reference for subsequent related research and application; furthermore, as an application of fusion machine learning,we also conduct a study on the modeling optimization for car service complaint text classification.} }
@article{WOS:000507900400001, title = {Detecting Accounting Fraud in Publicly Traded US Firms Using a Machine Learning Approach}, journal = {JOURNAL OF ACCOUNTING RESEARCH}, volume = {58}, pages = {199-235}, year = {2020}, issn = {0021-8456}, doi = {10.1111/1475-679X.12292}, author = {Bao, Yang and Ke, Bin and Li, Bin and Yu, Y. Julia and Zhang, Jie}, abstract = {We develop a state-of-the-art fraud prediction model using a machine learning approach. We demonstrate the value of combining domain knowledge and machine learning methods in model building. We select our model input based on existing accounting theories, but we differ from prior accounting research by using raw accounting numbers rather than financial ratios. We employ one of the most powerful machine learning methods, ensemble learning, rather than the commonly used method of logistic regression. To assess the performance of fraud prediction models, we introduce a new performance evaluation metric commonly used in ranking problems that is more appropriate for the fraud prediction task. Starting with an identical set of theory-motivated raw accounting numbers, we show that our new fraud prediction model outperforms two benchmark models by a large margin: the Dechow et al. logistic regression model based on financial ratios, and the Cecchini et al. support-vector-machine model with a financial kernel that maps raw accounting numbers into a broader set of ratios.} }
@article{WOS:000874966900001, title = {Colloquium: Machine learning in nuclear physics}, journal = {REVIEWS OF MODERN PHYSICS}, volume = {94}, year = {2022}, issn = {0034-6861}, doi = {10.1103/RevModPhys.94.031003}, author = {Boehnlein, Amber and Diefenthaler, Markus and Sato, Nobuo and Schram, Malachi and Ziegler, Veronique and Fanelli, Cristiano and Hjorth-Jensen, Morten and Horn, Tanja and Kuchera, Michelle P. and Lee, Dean and Nazarewicz, Witold and Ostroumov, Peter and Orginos, Kostas and Poon, Alan and Wang, Xin-Nian and Scheinker, Alexander and Smith, Michael S. and Pang, Long-Gang}, abstract = {Advances in machine learning methods provide tools that have broad applicability in scientific research. These techniques are being applied across the diversity of nuclear physics research topics, leading to advances that will facilitate scientific discoveries and societal applications. This Colloquium provides a snapshot of nuclear physics research, which has been transformed by machine learning techniques.} }
@article{WOS:000882795100003, title = {Machine learning and deep learning in phononic crystals and metamaterials-A review}, journal = {MATERIALS TODAY COMMUNICATIONS}, volume = {33}, year = {2022}, doi = {10.1016/j.mtcomm.2022.104606}, author = {Muhammad and Kennedy, John and Lim, C. W.}, abstract = {Machine learning (ML), as a component of artificial intelligence, encourages structural design exploration which leads to new technological advancements. By developing and generating data-driven methodologies that supplement conventional physics and formula-based approaches, deep learning (DL), a subset of machine learning offers an efficient way to understand and harness artificial materials and structures. Recently, acoustic and mechanics communities have observed a surge of research interest in implementing machine learning and deep learning methods in the design and optimization of artificial materials. In this review we evaluate the recent developments and present a state-of-the-art literature survey in machine learning and deep learning based phononic crystals and metamaterial designs by giving historical context, discussing network architectures and working principles. We also explain the application of these network architectures adopted for design and optimization of artificial structures. Since this multidisciplinary research field is evolving, a summary of the future prospects is also covered. This review article serves to update the acoustics, mechanics, physics, material science and deep learning communities about the recent developments in this newly emerging research direction} }
@article{WOS:000635680800006, title = {What Role Does Hydrological Science Play in the Age of Machine Learning?}, journal = {WATER RESOURCES RESEARCH}, volume = {57}, year = {2021}, issn = {0043-1397}, doi = {10.1029/2020WR028091}, author = {Nearing, Grey S. and Kratzert, Frederik and Sampson, Alden Keefe and Pelissier, Craig S. and Klotz, Daniel and Frame, Jonathan M. and Prieto, Cristina and Gupta, Hoshin V.}, abstract = {Y This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall-runoff simulation indicate that there is significantly more information in large-scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence-based preferences for models based on a certain type of ``process understanding'' that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.} }
@article{WOS:000599992800002, title = {Return on Investment in Machine Learning: Crossing the Chasm between Academia and Business}, journal = {FOUNDATIONS OF COMPUTING AND DECISION SCIENCES}, volume = {45}, pages = {281-304}, year = {2020}, issn = {0867-6356}, doi = {10.2478/fcds-2020-0015}, author = {Mizgajski, Jan and Szymczak, Adrian and Morzy, Mikolaj and Augustyniak, Lukasz and Szymanski, Piotr and Zelasko, Piotr}, abstract = {Academia remains the central place of machine learning education. While academic culture is the predominant factor influencing the way we teach machine learning to students, many practitioners question this culture, claiming the lack of alignment between academic and business environments. Drawing on professional experiences from both sides of the chasm, we describe the main points of contention, in the hope that it will help better align academic syllabi with the expectations towards future machine learning practitioners. We also provide recommendations for teaching of the applied aspects of machine learning.} }
@article{WOS:000773410500001, title = {Machine-Learning Analysis of Small-Molecule Donors for Fullerene Based Organic Solar Cells}, journal = {ENERGY TECHNOLOGY}, volume = {10}, year = {2022}, issn = {2194-4288}, doi = {10.1002/ente.202200019}, author = {Janjua, Muhammad Ramzan Saeed Ashraf and Irfan, Ahmad and Hussien, Mohamed and Ali, Muhammad and Saqib, Muhammad and Sulaman, Muhammad}, abstract = {In recent years, development in organic solar cells speeds up and performance continuously increases. From the last few years, machine learning gains fame among scientists who are researching on organic solar cells. Herein, machine learning is used to screen the small-molecule donors for organic solar cells. Molecular descriptors are used as input to train machine models. A variety of machine-learning models are tested to find the suitable one. Random forest model shows best predictive capability (Pearson's coefficient = 0.93). New small-molecule donors are also designed from easily synthesizable building units. Their power conversion efficiencies (PCEs) are predicted. Potential candidates with PCE > 11\\% are selected. The approach presented herein helps to select the efficient materials in short time with ease.} }
@article{WOS:001091207400001, title = {A novel deep-learning technique for forecasting oil price volatility using historical prices of five precious metals in context of green financing - A comparison of deep learning, machine learning, and statistical models}, journal = {RESOURCES POLICY}, volume = {86}, year = {2023}, issn = {0301-4207}, doi = {10.1016/j.resourpol.2023.104216}, author = {Mohsin, Muhammad and Jamaani, Fouad}, abstract = {This study proposes a novel deep-learning convolution neural network (CNN) to forecast crude oil prices based on historical prices of five precious metals (Gold, Silver, Platinum, Palladium, and Rhodium) in context of green financing. The proposed deep learning CNN has three components: a convolution block called a group block, a novel convolutional neural network architecture called GroupNet, and a regression layer. The proposed model is tested against seven machine learning models and three traditional statistical models for predicting oil price volatility using the same independent variables (5 precious metals). A comparison of the deep learning model (our proposed model) with machine learning/deep learning models and statistical methods indicates that the proposed deep learning model has the highest prediction accuracy. A feature selection technique is also applied using the WEKA ML tool to improve the accuracy of the proposed model and existing machine learning and traditional statistical models. The findings indicate a non-linear correlation between oil price volatility and prices of precious metals. Moreover, statistical analysis indicates that deep learning can be used to predict oil price volatility with greater accuracy than machine learning and statistical methods while using precious metals as predictors. The results also indicate that machine learning models (Decision Tables and M5rules) can be used to predict oil price volatility with considerable accuracy. Moreover, the study proves that traditional statistical models can perform better than a few machine learning models (Lazy LWL and GPR).} }
@article{WOS:000535945200001, title = {Personality Research and Assessment in the Era of Machine Learning}, journal = {EUROPEAN JOURNAL OF PERSONALITY}, volume = {34}, pages = {613-631}, year = {2020}, issn = {0890-2070}, doi = {10.1002/per.2257}, author = {Stachl, Clemens and Pargent, Florian and Hilbert, Sven and Harari, Gabriella M. and Schoedel, Ramona and Vaid, Sumer and Gosling, Samuel D. and Buehner, Markus}, abstract = {The increasing availability of high-dimensional, fine-grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.} }
@article{WOS:000614760600005, title = {Chemist versus Machine: Traditional Knowledge versus Machine Learning Techniques}, journal = {TRENDS IN CHEMISTRY}, volume = {3}, pages = {86-95}, year = {2021}, doi = {10.1016/j.trechm.2020.10.007}, author = {George, Janine and Hautier, Geoffroy}, abstract = {Chemical heuristics have been fundamental to the advancement of chemistry and materials science. These heuristics are typically established by scientists using knowledge and creativity to extract patterns from limited datasets. Machine learning offers opportunities to perfect this approach using computers and larger datasets. Here, we discuss the relationships between traditional heuristics and machine learning approaches. We show how traditional rules can be challenged by large-scale statistical assessment and how traditional concepts commonly used as features are feeding the machine learning techniques. We stress the waste involved in relearning chemical rules and the challenges in terms of data size requirements for purely data-driven approaches. Our view is that heuristic and machine learning approaches are at their best when they work together.} }
@article{WOS:000757584200001, title = {A Concise Review on Recent Developments of Machine Learning for the Prediction of Vibrational Spectra}, journal = {JOURNAL OF PHYSICAL CHEMISTRY A}, volume = {126}, pages = {801-812}, year = {2022}, issn = {1089-5639}, doi = {10.1021/acs.jpca.1c10417}, author = {Han, Ruocheng and Ketkaew, Rangsiman and Luber, Sandra}, abstract = {Machine learning has become more and more popular in computational chemistry, as well as in the important field of spectroscopy. In this concise review, we walk the reader through a short summary of machine learning algorithms and a comprehensive discussion on the connection between machine learning methods and vibrational spectroscopy, particularly for the case of infrared and Raman spectroscopy. We also briefly discuss state-of-the-art molecular representations which serve as meaningful inputs for machine learning to predict vibrational spectra. In addition, this review provides an overview of the transferability and best practices of machine learning in the prediction of vibrational spectra as well as possible future research directions.} }
@article{WOS:000680450500001, title = {Machine learning and algorithmic fairness in public and population health}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {3}, pages = {659-666}, year = {2021}, doi = {10.1038/s42256-021-00373-4}, author = {Mhasawade, Vishwali and Zhao, Yuan and Chunara, Rumi}, abstract = {Until now, much of the work on machine learning and health has focused on processes inside the hospital or clinic. However, this represents only a narrow set of tasks and challenges related to health; there is greater potential for impact by leveraging machine learning in health tasks more broadly. In this Perspective we aim to highlight potential opportunities and challenges for machine learning within a holistic view of health and its influences. To do so, we build on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities. We present a brief introduction to research in these fields, data sources and types of tasks, and use these to identify settings where machine learning is relevant and can contribute to new knowledge. Given the key foci of health equity and disparities within public and population health, we juxtapose these topics with the machine learning subfield of algorithmic fairness to highlight specific opportunities where machine learning, public and population health may synergize to achieve health equity. Algorithmic solutions to improve treatment are starting to transform health care. Mhasawade and colleagues discuss in this Perspective how machine learning applications in population and public health can extend beyond clinical practice. While working with general health data comes with its own challenges, most notably ensuring algorithmic fairness in the face of existing health disparities, the area provides new kinds of data and questions for the machine learning community.} }
@article{WOS:000928683100001, title = {Learning machine learning with young children: exploring informal settings in an African context}, journal = {COMPUTER SCIENCE EDUCATION}, volume = {34}, pages = {161-192}, year = {2024}, issn = {0899-3408}, doi = {10.1080/08993408.2023.2175559}, author = {Sanusi, Ismaila Temitayo and Sunday, Kissinger and Oyelere, Solomon Sunday and Suhonen, Jarkko and Vartiainen, Henriikka and Tukiainen, Markku}, abstract = {Background and contextResearchers have been investigating ways to demystify machine learning for students from kindergarten to twelfth grade (K-12) levels. As little evidence can be found in the literature, there is a need for additional research to understand and facilitate the learning experience of children while also considering the African context.ObjectiveThe purpose of this study was to explore how young children teach and develop their understanding of machine learning based technologies in playful and informal settings.MethodUsing a qualitative methodological approach through fine-grained analysis of video recordings and interviews, we analysed how 18 children aged 3-13 years constructed their interactions with a machine-based technology (Google's Teachable Machine).FindingsThis study provides empirical support for the claim that Google's Teachable Machine contributes to the development of data literacy and conceptual understanding across K-12 irrespective of the learners' backgrounds. The results also confirmed children's ability to infer the relationship between their own expressions and the output of the machine learning-based tool, thus, identifying the input-output relationships in machine learning. In addition, this study opens a discussion around differentials in emerging technology use across different contexts through participatory learning.ImplicationsThe results provide a baseline for future research on the topic and preliminary evidence to discern how children learn about machine learning in the African K-12 context.} }
@article{WOS:000590441600001, title = {Machine Learning for Financial Risk Management: A Survey}, journal = {IEEE ACCESS}, volume = {8}, pages = {203203-203223}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3036322}, author = {Mashrur, Akib and Luo, Wei and Zaidi, Nayyar A. and Robles-Kelly, Antonio}, abstract = {Financial risk management avoids losses and maximizes profits, and hence is vital to most businesses. As the task relies heavily on information-driven decision making, machine learning is a promising source for new methods and technologies. In recent years, we have seen increasing adoption of machine learning methods for various risk management tasks. Machine-learning researchers, however, often struggle to navigate the vast and complex domain knowledge and the fast-evolving literature. This paper fills this gap, by providing a systematic survey of the rapidly growing literature of machine learning research for financial risk management. The contributions of the paper are four-folds: First, we present a taxonomy of financial-risk-management tasks and connect them with relevant machine learning methods. Secondly, we highlight significant publications in the past decade. Thirdly, we identify major challenges being faced by researchers in this area. And finally, we point out emerging trends and promising research directions.} }
@article{WOS:000828467800001, title = {Disclosure Sentiment: Machine Learning vs. Dictionary Methods}, journal = {MANAGEMENT SCIENCE}, volume = {68}, pages = {5514-5532}, year = {2022}, issn = {0025-1909}, doi = {10.1287/mnsc.2021.4156}, author = {Frankel, Richard and Jennings, Jared and Lee, Joshua}, abstract = {We compare the ability of dictionary-based and machine-learning methods to capture disclosure sentiment at 10-K filing and conference-call dates. Like Loughran and McDonald [Loughran T, McDonald B (2011) When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. J. Finance 66(1):35-65.], we use returns to assess sentiment. We find that measures based on machine learning offer a significant improvement in explanatory power over dictionary-based measures. Specifically, machine-learning measures explain returns at 10-K filing dates, whereas measures based on the Loughran and McDonald dictionary only explain returns at 10-K filing dates during the time period of their study. Moreover, at conference-call dates, machine-learning methods offer an improvement over the Loughran and McDonald dictionary method of a greater magnitude than the improvement of the Loughran and McDonald dictionary over the Harvard Psychosociological Dictionary. We further find that the random-forest-regression-tree method better captures disclosure sentiment than alternative algorithms, simplifying the application of the machine-learning approach. Overall, our results suggest that machine-learning methods offer an easily implementable, more powerful, and reliable measure of disclosure sentiment than dictionary-based methods.} }
@article{WOS:000403753100004, title = {Machine Learning: An Applied Econometric Approach}, journal = {JOURNAL OF ECONOMIC PERSPECTIVES}, volume = {31}, pages = {87-106}, year = {2017}, issn = {0895-3309}, doi = {10.1257/jep.31.2.87}, author = {Mullainathan, Sendhil and Spiess, Jann}, abstract = {Machines are increasingly doing ``intelligent'' things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble-and thus where they can be most usefully applied.} }
@article{WOS:000576604100002, title = {Machine learning applications in systems metabolic engineering}, journal = {CURRENT OPINION IN BIOTECHNOLOGY}, volume = {64}, pages = {1-9}, year = {2020}, issn = {0958-1669}, doi = {10.1016/j.copbio.2019.08.010}, author = {Kim, Gi Bae and Kim, Won Jun and Kim, Hyun Uk and Lee, Sang Yup}, abstract = {Systems metabolic engineering allows efficient development of high performing microbial strains for the sustainable production of chemicals and materials. In recent years, increasing availability of bio big data, for example, omics data, has led to active application of machine learning techniques across various stages of systems metabolic engineering, including host strain selection, metabolic pathway reconstruction, metabolic flux optimization, and fermentation. In this paper, recent contributions of machine learning approaches to each major step of systems metabolic engineering are discussed. As the use of machine learning in systems metabolic engineering will become more widespread in accordance with the ever-increasing volume of bio big data, future prospects are also provided for the successful applications of machine learning.} }
@article{WOS:001000655300009, title = {A Survey on Machine Learning in Hardware Security}, journal = {ACM JOURNAL ON EMERGING TECHNOLOGIES IN COMPUTING SYSTEMS}, volume = {19}, year = {2023}, issn = {1550-4832}, doi = {10.1145/3589506}, author = {Koylu, Troya Cagil and Reinbrecht, Cezar Rodolfo Wedig and Gebregiorgis, Anteneh and Hamdioui, Said and Taouil, Mottaqiallah}, abstract = {Hardware security is currently a very influential domain, where each year countless works are published concerning attacks against hardware and countermeasures. A significant number of them use machine learning, which is proven to be very effective in other domains. This survey, as one of the early attempts, presents the usage of machine learning in hardware security in a full and organized manner. Our contributions include classification and introduction to the relevant fields of machine learning, a comprehensive and critical overview of machine learning usage in hardware security, and an investigation of the hardware attacks against machine learning (neural network) implementations.} }
@article{WOS:000503335100001, title = {Surveying the reach and maturity of machine learning and artificial intelligence in astronomy}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {10}, year = {2020}, issn = {1942-4787}, doi = {10.1002/widm.1349}, author = {Fluke, Christopher J. and Jacobs, Colin}, abstract = {Machine learning (automated processes that learn by example in order to classify, predict, discover, or generate new data) and artificial intelligence (methods by which a computer makes decisions or discoveries that would usually require human intelligence) are now firmly established in astronomy. Every week, new applications of machine learning and artificial intelligence are added to a growing corpus of work. Random forests, support vector machines, and neural networks are now having a genuine impact for applications as diverse as discovering extrasolar planets, transient objects, quasars, and gravitationally lensed systems, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. This review surveys contemporary, published literature on machine learning and artificial intelligence in astronomy and astrophysics. Applications span seven main categories of activity: classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights. These categories form the basis of a hierarchy of maturity, as the use of machine learning and artificial intelligence emerges, progresses, or becomes established. This article is categorized under: Application Areas > Science and Technology Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining Technologies > Machine Learning} }
@article{WOS:000510903200065, title = {Correlated Differential Privacy: Feature Selection in Machine Learning}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {16}, pages = {2115-2124}, year = {2020}, issn = {1551-3203}, doi = {10.1109/TII.2019.2936825}, author = {Zhang, Tao and Zhu, Tianqing and Xiong, Ping and Huo, Huan and Tari, Zahir and Zhou, Wanlei}, abstract = {Privacy preserving in machine learning is a crucial issue in industry informatics since data used for training in industries usually contain sensitive information. Existing differentially private machine learning algorithms have not considered the impact of data correlation, which may lead to more privacy leakage than expected in industrial applications. For example, data collected for traffic monitoring may contain some correlated records due to temporal correlation or user correlation. To fill this gap, in this article, we propose a correlation reduction scheme with differentially private feature selection considering the issue of privacy loss when data have correlation in machine learning tasks. The proposed scheme involves five steps with the goal of managing the extent of data correlation, preserving the privacy, and supporting accuracy in the prediction results. In this way, the impact of data correlation is relieved with the proposed feature selection scheme, and moreover the privacy issue of data correlation in learning is guaranteed. The proposed method can be widely used in machine learning algorithms, which provide services in industrial areas. Experiments show that the proposed scheme can produce better prediction results with machine learning tasks and fewer mean square errors for data queries compared to existing schemes.} }
@article{WOS:000582822600018, title = {Machine learning in GI endoscopy: practical guidance in how to interpret a novel field}, journal = {GUT}, volume = {69}, pages = {2035-2045}, year = {2020}, issn = {0017-5749}, doi = {10.1136/gutjnl-2019-320466}, author = {van der Sommen, Fons and de Groof, Jeroen and Struyvenberg, Maarten and van der Putten, Joost and Boers, Tim and Fockens, Kiki and Schoon, Erik J. and Curvers, Wouter and de With, Peter and Mori, Yuichi and Byrne, Michael and Bergman, Jacques J. G. H. M.}, abstract = {There has been a vast increase in GI literature focused on the use of machine learning in endoscopy. The relative novelty of this field poses a challenge for reviewers and readers of GI journals. To appreciate scientific quality and novelty of machine learning studies, understanding of the technical basis and commonly used techniques is required. Clinicians often lack this technical background, while machine learning experts may be unfamiliar with clinical relevance and implications for daily practice. Therefore, there is an increasing need for a multidisciplinary, international evaluation on how to perform high-quality machine learning research in endoscopy. This review aims to provide guidance for readers and reviewers of peer-reviewed GI journals to allow critical appraisal of the most relevant quality requirements of machine learning studies. The paper provides an overview of common trends and their potential pitfalls and proposes comprehensive quality requirements in six overarching themes: terminology, data, algorithm description, experimental setup, interpretation of results and machine learning in clinical practice.} }
@article{WOS:000911287700001, title = {Review of interpretable machine learning for process industries}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {170}, pages = {647-659}, year = {2023}, issn = {0957-5820}, doi = {10.1016/j.psep.2022.12.018}, author = {Carter, A. and Imtiaz, S. and Naterer, G. F.}, abstract = {This review article examines recent advances in the use of machine learning for process industries. The article presents common process industry tasks that researchers are solving with machine learning techniques. It then identifies a lack of consensus among past studies when selecting an appropriate model given a prescribed application. Furthermore, the article identifies that relatively few past studies have considered model inter-pretability - a ``black-box'' challenge holding back machine learning's implementation in more high-risk in-dustrial applications. This interdisciplinary field of engineering and computer science is still reasonably young. Additional research is recommended to standardize methods and establish a strategic framework to manage risk during adoption of machine learning models.} }
@article{WOS:001527924800002, title = {Domain knowledge in artificial intelligence: Using conceptual modeling to increase machine learning accuracy and explainability}, journal = {DATA \\& KNOWLEDGE ENGINEERING}, volume = {160}, year = {2025}, issn = {0169-023X}, doi = {10.1016/j.datak.2025.102482}, author = {Storey, Veda C. and Parsons, Jeffrey and Bueso, Arturo Castellanos and Tremblay, Monica Chiarini and Lukyanenko, Roman and Castillo, Alfred and Maass, Wolfgang}, abstract = {Machine learning enables the extraction of useful information from large, diverse datasets. However, despite many successful applications, machine learning continues to suffer from performance and transparency issues. These challenges can be partially attributed to the limited use of domain knowledge by machine learning models. This research proposes using the domain knowledge represented in conceptual models to improve the preparation of the data used to train machine learning models. We develop and demonstrate a method, called the Conceptual Modeling for Machine Learning (CMML), which is comprised of guidelines for data preparation in machine learning and based on conceptual modeling constructs and principles. To assess the impact of CMML on machine learning outcomes, we first applied it to two real-world problems to evaluate its impact on model performance. We then solicited an assessment by data scientists on the applicability of the method. These results demonstrate the value of CMML for improving machine learning outcomes.} }
@article{WOS:000811299400001, title = {Machine Learning Advances in Microbiology: A Review of Methods and Applications}, journal = {FRONTIERS IN MICROBIOLOGY}, volume = {13}, year = {2022}, doi = {10.3389/fmicb.2022.925454}, author = {Jiang, Yiru and Luo, Jing and Huang, Danqing and Liu, Ya and Li, Dan-dan}, abstract = {Microorganisms play an important role in natural material and elemental cycles. Many common and general biology research techniques rely on microorganisms. Machine learning has been gradually integrated with multiple fields of study. Machine learning, including deep learning, aims to use mathematical insights to optimize variational functions to aid microbiology using various types of available data to help humans organize and apply collective knowledge of various research objects in a systematic and scaled manner. Classification and prediction have become the main achievements in the development of microbial community research in the direction of computational biology. This review summarizes the application and development of machine learning and deep learning in the field of microbiology and shows and compares the advantages and disadvantages of different algorithm tools in four fields: microbiome and taxonomy, microbial ecology, pathogen and epidemiology, and drug discovery.} }
@article{WOS:000939150800001, title = {Adversarial Machine Learning Attacks against Intrusion Detection Systems: A Survey on Strategies and Defense}, journal = {FUTURE INTERNET}, volume = {15}, year = {2023}, issn = {1999-5903}, doi = {10.3390/fi15020062}, author = {Alotaibi, Afnan and Rassam, Murad A.}, abstract = {Concerns about cybersecurity and attack methods have risen in the information age. Many techniques are used to detect or deter attacks, such as intrusion detection systems (IDSs), that help achieve security goals, such as detecting malicious attacks before they enter the system and classifying them as malicious activities. However, the IDS approaches have shortcomings in misclassifying novel attacks or adapting to emerging environments, affecting their accuracy and increasing false alarms. To solve this problem, researchers have recommended using machine learning approaches as engines for IDSs to increase their efficacy. Machine-learning techniques are supposed to automatically detect the main distinctions between normal and malicious data, even novel attacks, with high accuracy. However, carefully designed adversarial input perturbations during the training or testing phases can significantly affect their predictions and classifications. Adversarial machine learning (AML) poses many cybersecurity threats in numerous sectors that use machine-learning-based classification systems, such as deceiving IDS to misclassify network packets. Thus, this paper presents a survey of adversarial machine-learning strategies and defenses. It starts by highlighting various types of adversarial attacks that can affect the IDS and then presents the defense strategies to decrease or eliminate the influence of these attacks. Finally, the gaps in the existing literature and future research directions are presented.} }
@article{WOS:000774530000001, title = {Application of Soft Computing Techniques to Predict the Strength of Geopolymer Composites}, journal = {POLYMERS}, volume = {14}, year = {2022}, doi = {10.3390/polym14061074}, author = {Wang, Qichen and Ahmad, Waqas and Ahmad, Ayaz and Aslam, Fahid and Mohamed, Abdullah and Vatin, Nikolai Ivanovich}, abstract = {Geopolymers may be the best alternative to ordinary Portland cement because they are manufactured using waste materials enriched in aluminosilicate. Research on geopolymer composites is accelerating. However, considerable work, expense, and time are needed to cast, cure, and test specimens. The application of computational methods to the stated objective is critical for speedy and cost-effective research. In this study, supervised machine learning approaches were employed to predict the compressive strength of geopolymer composites. One individual machine learning approach, decision tree, and two ensembled machine learning approaches, AdaBoost and random forest, were used. The coefficient correlation (R-2), statistical tests, and k-fold analysis were used to determine the validity and comparison of all models. It was discovered that ensembled machine learning techniques outperformed individual machine learning techniques in forecasting the compressive strength of geopolymer composites. However, the outcomes of the individual machine learning model were also within the acceptable limit. R-2 values of 0.90, 0.90, and 0.83 were obtained for AdaBoost, random forest, and decision models, respectively. The models' decreased error values, such as mean absolute error, mean absolute percentage error, and root-mean-square errors, further confirmed the ensembled machine learning techniques' increased precision. Machine learning approaches will aid the building industry by providing quick and cost-effective methods for evaluating material properties.} }
@article{WOS:000520045900001, title = {Multiscale Modeling Meets Machine Learning: What Can We Learn?}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {1017-1037}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09405-5}, author = {Peng, Grace C. Y. and Alber, Mark and Tepole, Adrian Buganza and Cannon, William and De, Suvranu and Dura-Bernal, Salvador and Garikipati, Krishna and Karniadakis, George and Lytton, William W. and Perdikaris, Paris and Petzold, Linda and Kuhl, Ellen}, abstract = {Machine learning is increasingly recognized as a promising technology in the biological, biomedical, and behavioral sciences. There can be no argument that this technique is incredibly successful in image recognition with immediate applications in diagnostics including electrophysiology, radiology, or pathology, where we have access to massive amounts of annotated data. However, machine learning often performs poorly in prognosis, especially when dealing with sparse data. This is a field where classical physics-based simulation seems to remain irreplaceable. In this review, we identify areas in the biomedical sciences where machine learning and multiscale modeling can mutually benefit from one another: Machine learning can integrate physics-based knowledge in the form of governing equations, boundary conditions, or constraints to manage ill-posted problems and robustly handle sparse and noisy data; multiscale modeling can integrate machine learning to create surrogate models, identify system dynamics and parameters, analyze sensitivities, and quantify uncertainty to bridge the scales and understand the emergence of function. With a view towards applications in the life sciences, we discuss the state of the art of combining machine learning and multiscale modeling, identify applications and opportunities, raise open questions, and address potential challenges and limitations. We anticipate that it will stimulate discussion within the community of computational mechanics and reach out to other disciplines including mathematics, statistics, computer science, artificial intelligence, biomedicine, systems biology, and precision medicine to join forces towards creating robust and efficient models for biological systems.} }
@article{WOS:000940478300001, title = {A Survey of Underwater Acoustic Target Recognition Methods Based on Machine Learning}, journal = {JOURNAL OF MARINE SCIENCE AND ENGINEERING}, volume = {11}, year = {2023}, doi = {10.3390/jmse11020384}, author = {Luo, Xinwei and Chen, Lu and Zhou, Hanlu and Cao, Hongli}, abstract = {Underwater acoustic target recognition (UATR) technology has been implemented widely in the fields of marine biodiversity detection, marine search and rescue, and seabed mapping, providing an essential basis for human marine economic and military activities. With the rapid development of machine-learning-based technology in the acoustics field, these methods receive wide attention and display a potential impact on UATR problems. This paper reviews current UATR methods based on machine learning. We focus mostly, but not solely, on the recognition of target-radiated noise from passive sonar. First, we provide an overview of the underwater acoustic acquisition and recognition process and briefly introduce the classical acoustic signal feature extraction methods. In this paper, recognition methods for UATR are classified based on the machine learning algorithms used as UATR technologies using statistical learning methods, UATR methods based on deep learning models, and transfer learning and data augmentation technologies for UATR. Finally, the challenges of UATR based on the machine learning method are summarized and directions for UATR development in the future are put forward.} }
@article{WOS:001387703100001, title = {Machine Learning Advances in High-Entropy Alloys: A Mini-Review}, journal = {ENTROPY}, volume = {26}, year = {2024}, doi = {10.3390/e26121119}, author = {Sun, Yibo and Ni, Jun}, abstract = {The efficacy of machine learning has increased exponentially over the past decade. The utilization of machine learning to predict and design materials has become a pivotal tool for accelerating materials development. High-entropy alloys are particularly intriguing candidates for exemplifying the potency of machine learning due to their superior mechanical properties, vast compositional space, and intricate chemical interactions. This review examines the general process of developing machine learning models. The advances and new algorithms of machine learning in the field of high-entropy alloys are presented in each part of the process. These advances are based on both improvements in computer algorithms and physical representations that focus on the unique ordering properties of high-entropy alloys. We also show the results of generative models, data augmentation, and transfer learning in high-entropy alloys and conclude with a summary of the challenges still faced in machine learning high-entropy alloys today.} }
@article{WOS:001102417300001, title = {Artificial Intelligence and Machine Learning in Metallurgy. Part 1. Methods and Algorithms}, journal = {METALLURGIST}, volume = {67}, pages = {886-894}, year = {2023}, issn = {0026-0894}, doi = {10.1007/s11015-023-01576-3}, author = {Muntin, A. V. and Zhikharev, P. Yu. and Ziniagin, A. G. and Brayko, D. A.}, abstract = {The article contains information about machine learning methods used in modern metallurgy. The description of machine learning methods and their role in the processing of ``big data'' formed at metallurgical enterprises are given. The topic relevance has to do with the effectiveness of solving problems aimed at improving production processes using artificial intelligence and machine learning in various metallurgical processing stages.} }
@article{WOS:001137233600005, title = {Techniques and applications of Machine Learning and Artificial Intelligence in education: a systematic review}, journal = {RIED-REVISTA IBEROAMERICANA DE EDUCACION A DISTANCIA}, volume = {27}, year = {2024}, issn = {1138-2783}, doi = {10.5944/ried.27.1.37491}, author = {Forero-Corba, Wiston and Bennasar, Francisca Negre}, abstract = {Machine learning is a field of artificial intelligence that is impacting lately in all areas of knowledge. The areas of social sciences, especially education, are no stranger to it, so, a systematic review of the literature on the techniques and applications of machine learning and artificial intelligence in Education is performed. The lack of knowledge and skills of educators in machine learning and artificial intelligence limits the optimal implementation of these technologies in education. The objective of this research is to identify opportunities for improving teaching-learning processes and educational management at all levels of the educational context through the application of machine learning and artificial intelligence. The databases used for the bibliographic search were Web of Science and Scopus and the methodology applied is based on the PRISMA statement for obtaining and analyzing 55 articles published in high impact journals between the years 2021-2023. The results showed that the studies addressed a total of 33 machine learning and artificial intelligence techniques and multiple applications that were implemented in educational contexts at primary, secondary and higher education levels in 38 countries. The conclusions showed the strong impact of the use of machine learning and artificial intelligence. This impact is reflected in the use of different intelligent techniques in educational contexts and the increase of research in secondary schools on artificial intelligence.} }
@article{WOS:001126204200001, title = {Incorporating soil knowledge into machine-learning prediction of soil properties from soil spectra}, journal = {EUROPEAN JOURNAL OF SOIL SCIENCE}, volume = {74}, year = {2023}, issn = {1351-0754}, doi = {10.1111/ejss.13438}, author = {Ma, Yuxin and Minasny, Budiman and Dematte, Jose A. M. and McBratney, Alex B.}, abstract = {Various machine-learning models have been extensively applied to predict soil properties using infrared spectroscopy. Beyond the interpretability and transparency of these models, there is an ongoing discussion on the reliability of the prediction of soil properties generated from soil spectra. In this review, we contribute to this discussion by advocating for the integration of soil knowledge into machine-learning models. By doing so, researchers can delve deeper into the underlying soil constituents, ultimately enhancing prediction accuracy. Our review explores the soil information present in spectral data, the fallacy of model interpretability, methods to incorporate soil knowledge into machine-learning techniques, and the ways in which machine learning and soil spectroscopy can assist soil science. The combination of machine learning and domain knowledge is recommended to develop more meaningful models for predicting soil properties within the field of soil science.} }
@article{WOS:000730393300003, title = {Prospects of Artificial Intelligence and Machine Learning Application in Banking Risk Management}, journal = {JOURNAL OF CENTRAL BANKING THEORY AND PRACTICE}, volume = {10}, pages = {41-57}, year = {2021}, issn = {1800-9581}, doi = {10.2478/jcbtp-2021-0023}, author = {Milojevic, Nenad and Redzepagic, Srdjan}, abstract = {Artificial intelligence and machine learning have increasing influence on the financial sector, but also on economy as a whole. The impact of artificial intelligence and machine learning on banking risk management has become particularly interesting after the global financial crisis. The research focus is on artificial intelligence and machine learning potential for further banking risk management improvement. The paper seeks to explore the possibility for successful implementation yet taking into account challenges and problems which might occur as well as potential solutions. Artificial intelligence and machine learning have potential to support the mitigation measures for the contemporary global economic and financial challenges, including those caused by the COVID-19 crisis. The main focus in this paper is on credit risk management, but also on analysing artificial intelligence and machine learning application in other risk management areas. It is concluded that a measured and well-prepared further application of artificial intelligence, machine learning, deep learning and big data analytics can have further positive impact, especially on the following risk management areas: credit, market, liquidity, operational risk, and other related areas.} }
@article{WOS:000557470100001, title = {Machine learning as ecology}, journal = {JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL}, volume = {53}, year = {2020}, issn = {1751-8113}, doi = {10.1088/1751-8121/ab956e}, author = {Howell, Owen and Cui Wenping and Marsland, III, Robert and Mehta, Pankaj}, abstract = {Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms-including support vector machines (SVMs)-have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.} }
@article{WOS:000576782300009, title = {Corporate default forecasting with machine learning}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {161}, year = {2020}, issn = {0957-4174}, doi = {10.1016/j.eswa.2020.113567}, author = {Moscatelli, Mirko and Parlapiano, Fabio and Narizzano, Simone and Viggiano, Gianluca}, abstract = {We analyze the performance of a set of machine learning models in predicting default risk, using standard statistical models, such as the logistic regression, as a benchmark. When only a limited information set is available, for example in the case of an external assessment of credit risk, we find that machine learning models provide substantial gains in discriminatory power and precision, relative to statistical models. This advantage diminishes when confidential information, such as credit behavioral indicators, is also available, and it becomes negligible when the dataset is small. Moreover, we evaluate the consequences of using a credit allocation rule based on machine learning ratings on the overall supply of credit and the number of borrowers gaining access to credit. Machine learning models concentrate a greater extent of credit towards safer and larger borrowers, which would result in lower credit losses for their lenders. (c) 2020 Elsevier Ltd. All rights reserved.} }
@article{WOS:001100527300001, title = {Air Quality Index prediction using machine learning for Ahmedabad city}, journal = {DIGITAL CHEMICAL ENGINEERING}, volume = {7}, year = {2023}, issn = {2772-5081}, doi = {10.1016/j.dche.2023.100093}, author = {Maltare, Nilesh N. and Vahora, Safvan}, abstract = {Prediction of air pollution index may help in traffic routing and identifying serious pollutants. Modeling of the complex relationships between these variables by sophisticated methods in machine learning is a promising field. The objective of this work is to compare the various machine learning methods such as SARIMA, SVM and LSTM for the prediction of air quality index for Ahmedabad city of Gujarat, India. In this research, different preprocessing methods are used to manage the data before providing to the machine learning models. This study is carried out based on the data provided by the Central Pollution Control Board of India and it focuses on the support vector machine algorithm with RBF kernel model. So, that the results availed are comparatively better as compared to other kernels of the support vector machine models as well as SARIMA and LSTM models for Ahmedabad city.} }
@article{WOS:000603445400001, title = {Novel Method of Classification in Knee Osteoarthritis: Machine Learning Application Versus Logistic Regression Model}, journal = {ANNALS OF REHABILITATION MEDICINE-ARM}, volume = {44}, pages = {415-427}, year = {2020}, issn = {2234-0645}, doi = {10.5535/arm.20071}, author = {Yang, Jung Ho and Park, Jae Hyeon and Jang, Seong-Ho and Cho, Jaesung}, abstract = {Objective To present new classification methods of knee osteoarthritis (KOA) using machine learning and compare its performance with conventional statistical methods as classification techniques using machine learning have recently been developed. Methods A total of 84 KOA patients and 97 normal participants were recruited. KOA patients were clustered into three groups according to the Kellgren-Lawrence (K-L) grading system. All subjects completed gait trials under the same experimental conditions. Machine learning- based classification using the support vector machine (SVM) classifier was performed to classify KOA patients and the severity of KOA. Logistic regression analysis was also performed to compare the results in classifying KOA patients with machine learning method. Results In the classification between KOA patients and normal subjects, the accuracy of classification was higher in machine learning method than in logistic regression analysis. In the classification of KOA severity, accuracy was enhanced through the feature selection process in the machine learning method. The most significant gait feature for classification was flexion and extension of the knee in the swing phase in the machine learning method. Conclusion The machine learning method is thought to be a new approach to complement conventional logistic regression analysis in the classification of KOA patients. It can be clinically used for diagnosis and gait correction of KOA patients.} }
@article{WOS:000865315000004, title = {Application of Deep Learning on the Prognosis of Cutaneous Melanoma Based on Full Scan Pathology Images}, journal = {BIOMED RESEARCH INTERNATIONAL}, volume = {2022}, year = {2022}, issn = {2314-6133}, doi = {10.1155/2022/4864485}, author = {Li, Anhai and Li, Xiaoyuan and Li, Wenwen and Yu, Xiaoqian and Qi, Mengmeng and Li, Ding}, abstract = {Introduction. The purpose of this study is to use deep learning and machine learning to learn and classify patients with cutaneous melanoma with different prognoses and to explore the application value of deep learning in the prognosis of cutaneous melanoma patients. Methods. In deep learning, VGG-19 is selected as the network architecture and learning model for learning and classification. In machine learning, deep features are extracted through the VGG-19 network architecture, and the support vector machine (SVM) model is selected for learning and classification. Compare and explore the application value of deep learning and machine learning in predicting the prognosis of patients with cutaneous melanoma. Result. According to receiver operating characteristic (ROC) curves and area under the curve (AUC), the average accuracy of deep learning is higher than that of machine learning, and even the lowest accuracy is better than that of machine learning. Conclusion. As the number of learning increases, the accuracy of machine learning and deep learning will increase, but in the same number of cutaneous melanoma patient pathology maps, the accuracy of deep learning will be higher. This study provides new ideas and theories for computational pathology in predicting the prognosis of patients with cutaneous melanoma.} }
@article{WOS:001222668700001, title = {Making the most of AI and machine learning in organizations and strategy research: Supervised machine learning, causal inference, and matching models}, journal = {STRATEGIC MANAGEMENT JOURNAL}, volume = {45}, pages = {1926-1953}, year = {2024}, issn = {0143-2095}, doi = {10.1002/smj.3604}, author = {Rathje, Jason and Katila, Riitta and Reineke, Philipp}, abstract = {We spotlight the use of machine learning in two-stage matching models to deal with sample selection bias. Recent advances in machine learning have unlocked new empirical possibilities for inductive theorizing. In contrast, the opportunities to use machine learning in regression studies involving large-scale data with many covariates and a causal claim are still less well understood. Our core contribution is to guide researchers in the use of machine learning approaches to choosing matching variables for enhanced causal inference in propensity score matching models. We use an analysis of real-world technology invention data of public-private relationships to demonstrate the method and find that machine learning can provide an alternative approach to ad hoc matching. However, as with any method, it is also important to understand its limitations. This article explores the use of machine learning to enhance decision-making, particularly in addressing sample selection bias in large-scale datasets. The rapid development of AI and machine learning offers new, powerful tools especially for digital ecosystems where complex data and causal relationships are complex to analyze. We offer managers and stakeholders insight into the effective integration of machine learning for selecting critical variables in propensity score matching models. Through a detailed examination of real-world data on technology inventions within public-private relationships, we demonstrate the effectiveness of machine learning as a robust alternative to traditional matching methods.} }
@article{WOS:000526339000051, title = {Quantum Chemistry in the Age of Machine Learning}, journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS}, volume = {11}, pages = {2336-2347}, year = {2020}, issn = {1948-7185}, doi = {10.1021/acs.jpclett.9b03664}, author = {Dral, Pavlo O.}, abstract = {As the quantum chemistry (QC) community embraces machine learning (ML), the number of new methods and applications based on the combination of QC and ML is surging. In this Perspective, a view of the current state of affairs in this new and exciting research field is offered, challenges of using machine learning in quantum chemistry applications are described, and potential future developments are outlined. Specifically, examples of how machine learning is used to improve the accuracy and accelerate quantum chemical research are shown. Generalization and classification of existing techniques are provided to ease the navigation in the sea of literature and to guide researchers entering the field. The emphasis of this Perspective is on supervised machine learning.} }
@article{WOS:000519447700007, title = {Machine Learning in Chemical Dynamics}, journal = {RESONANCE-JOURNAL OF SCIENCE EDUCATION}, volume = {25}, pages = {59-75}, year = {2020}, issn = {0971-8044}, doi = {10.1007/s12045-019-0922-1}, author = {Biswas, Rupayan and Rashmi, Richa and Lourderaj, Upakarasamy}, abstract = {Machine learning has been applied to various fields and is envisaged as the technology of the future. We discuss here, the applications of machine learning methods to represent potential energy surfaces - an important aspect of chemical dynamics. We illustrate the process of machine learning using simple examples, and demonstrate how it can be extended to complicated problems.} }
@article{WOS:000468604900004, title = {Survey of Machine Learning Techniques in Drug Discovery}, journal = {CURRENT DRUG METABOLISM}, volume = {20}, pages = {185-193}, year = {2019}, issn = {1389-2002}, doi = {10.2174/1389200219666180820112457}, author = {Stephenson, Natalie and Shane, Emily and Chase, Jessica and Rowland, Jason and Ries, David and Justice, Nicola and Zhang, Jie and Chan, Leong and Cao, Renzhi}, abstract = {Background: Drug discovery, which is the process of discovering new candidate medications, is very important for pharmaceutical industries. At its current stage, discovering new drugs is still a very expensive and time-consuming process, requiring Phases I, II and III for clinical trials. Recently, machine learning techniques in Artificial Intelligence (AI), especially the deep learning techniques which allow a computational model to generate multiple layers, have been widely applied and achieved state-of-the-art performance in different fields, such as speech recognition, image classification, bioinformatics, etc. One very important application of these AI techniques is in the field of drug discovery. Methods: We did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and start-up companies to understand current status of machine learning techniques in drug discovery. Results: Our experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year. Conclusion: The main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey.} }
@article{WOS:001198662400026, title = {Mechanism for feature learning in neural networks and backpropagation-free machine learning models}, journal = {SCIENCE}, volume = {383}, pages = {1461-1467}, year = {2024}, issn = {0036-8075}, doi = {10.1126/science.adi5639}, author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail}, abstract = {Understanding how neural networks learn features, or relevant patterns in data, for prediction is necessary for their reliable use in technological and scientific applications. In this work, we presented a unifying mathematical mechanism, known as average gradient outer product (AGOP), that characterized feature learning in neural networks. We provided empirical evidence that AGOP captured features learned by various neural network architectures, including transformer-based language models, convolutional networks, multilayer perceptrons, and recurrent neural networks. Moreover, we demonstrated that AGOP, which is backpropagation-free, enabled feature learning in machine learning models, such as kernel machines, that a priori could not identify task-specific features. Overall, we established a fundamental mechanism that captured feature learning in neural networks and enabled feature learning in general machine learning models.} }
@article{WOS:000704764200003, title = {On-line chatter detection in milling with hybrid machine learning and physics-based model}, journal = {CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY}, volume = {35}, pages = {25-40}, year = {2021}, issn = {1755-5817}, doi = {10.1016/j.cirpj.2021.05.006}, author = {Rahimi, M. Hossein and Huynh, Hoai Nam and Altintas, Yusuf}, abstract = {Unstable vibrations, chatter, in machining lead to poor surface finish and damage to the tool and machine. It is desired to detect and avoid chatter on-line without false alarms for improved productivity. This paper presents the application of a combined machine learning network and physics-based model to detect chatter in milling. The vibration data collected during machining is converted into moving short-time frequency spectrums, whose features are mapped to five machining states as air cut, entry into and exit from the workpiece, stable cut, and chatter conditions. The machine learning network was trained and its architecture was reduced to a computationally optimal network with 3 convolution blocks followed by a neural network with one hidden layer. A parallel algorithm, which Kalman filters the stable forced vibrations to isolate chatter signals in raw data, is used to detect the chatter and its frequency. The combination of the machine learning and physics-based model led to a 98.90\\% success rate in chatter detection while allowing to further train the network during production with the help of the physics-based, deterministic model. (c) 2021 CIRP.} }
@article{WOS:000456929100002, title = {Machine learning, deep learning and Python language in field of geology}, journal = {ACTA PETROLOGICA SINICA}, volume = {34}, pages = {3173-3178}, year = {2018}, issn = {1000-0569}, author = {Zhou YongZhang and Wang Jun and Zuo RenGuang and Xiao Fan and Shen WenJie and Wang ShuGong}, abstract = {Geological big data is exponentially expanding. It is the only way to catch up with its extraordinary growing to develop intelligent data processing. As the core of artificial intelligence, machine learning is a fundamental way to endow computer with intelligence. Machine learning has been becoming the front hotspot of geological big data mining. It will attach wings to geological big data mining, and thereby bring revolution to geological research. Machine learning is a data adaptive training process and model, resulting in giving a good performance decision. As a subclass of machine learning, deep learning develops machine learning model with various hidden layers, and makes iterative evolution of the model through massive data training, and finally extracts essential features to help more exactly classing and predicting. The convolution neural network is one of the most frequently used deep learning algorithms. It is widely used in image recognition and speech analysis. Python language is playing an increasingly important role in science research. The Python Scikit-Learn is a machine learning-oriented library to provide with data preprocessing, classification, regression, clustering, prediction, model analysis and other modules. The Keras is a Python deep learning library based on Theano and Tensorflow, and can be used to construct concise artificial neural network.} }
@article{WOS:000696993100003, title = {Advancements within Modern Machine Learning Methodology: Impacts and Prospects in Biomarker Discovery}, journal = {CURRENT MEDICINAL CHEMISTRY}, volume = {28}, pages = {6512-6531}, year = {2021}, issn = {0929-8673}, doi = {10.2174/0929867328666210208111821}, author = {Ledesma, Dakila and Symes, Steven and Richards, Sean}, abstract = {Background: The adoption of biomarkers as part of high-throughput, complex microarray or sequencing data has necessitated the discovery and validation of these data through machine learning. Machine learning has remained a fundamental and indispensable tool due to its efficacy and efficiency in both feature extraction of relevant biomarkers as well as the classification of samples as validation of the discovered biomarkers. Objectives: This review aims to present the impact and ability of various machine learning methodologies and models to process high-throughput, high-dimensionality data found within mass spectrometry, microarray, and DNA/RNA-sequence data; data that precluded biomarker discovery prior to the use of machine learning. Methods: A vast array of literature highlighting machine learning for biomarker discovery was reviewed, resulting in the eligibility of 21 machine learning algorithms/networks and 3 combinatory architectures, spanning 17 fields of study. This literature was screened to investigate the usage and development of machine learning within the framework of biomarker discovery. Results: Out of the 93 papers collected, a total of 62 biomarker studies were further reviewed across different subfields-49 of which employed machine learning algorithms, and 13 of which employed neural network-based models. Through the application, innovation, and creation of tools in biomarker-related machine learning methodologies, its use allowed for the discovery, accumulation, validation, and interpretation of biomarkers within varied data formats, sources, as well as fields of study. Conclusion: The use of machine learning methodologies for biomarker discovery is critical to the analysis of various types of data used for biomarker discovery, such as mass spectrometry, nucleotide and protein sequencing, and image (e.g. CT-scan) data. Further studies containing more standardized techniques for evaluation, and the use of cutting-edge machine learning architectures may lead to more accurate and specific results.} }
@article{WOS:000993072400001, title = {Machine learning to enhance the management of highway pavements and bridges}, journal = {INFRASTRUCTURE ASSET MANAGEMENT}, volume = {11}, pages = {119-127}, year = {2023}, issn = {2053-0242}, doi = {10.1680/jinam.22.00031}, author = {Bashar, Mohammad Z. and Torres-Machi, Cristina}, abstract = {The adoption of machine learning in transportation asset management is hindered by the perception of being a black box, the natural resistance to change, and the challenges of integration with existing management systems. This paper aims to enhance the understanding of machine learning and provide guidance for the development and implementation of machine learning to support decision-making in the management of highway pavements and bridges. The paper identifies successful research efforts using machine learning, identifies opportunities and challenges in adopting machine learning, and derives recommendations on when and how to apply different machine learning algorithms to support asset management decisions. Four main challenges were identified: the trade-off between accuracy and interpretability, the shortage of machine learning engineers, data quality, and the limitations of machine learning algorithms. Although the complexities associated with training machine learning algorithms challenge the short-term implementation, machine learning offer a wide range of opportunities when compared to traditional approaches. The development of hybrid systems combining machine learning algorithms with expert opinions and traditional approaches seems a reasonable step forward to support agencies asset management decisions.} }
@article{WOS:000856997600004, title = {Explainable machine learning in materials science}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {8}, year = {2022}, doi = {10.1038/s41524-022-00884-7}, author = {Zhong, Xiaoting and Gallagher, Brian and Liu, Shusen and Kailkhura, Bhavya and Hiszpanski, Anna and Han, T. Yong-Jin}, abstract = {Machine learning models are increasingly used in materials studies because of their exceptional accuracy. However, the most accurate machine learning models are usually difficult to explain. Remedies to this problem lie in explainable artificial intelligence (XAI), an emerging research field that addresses the explainability of complicated machine learning models like deep neural networks (DNNs). This article attempts to provide an entry point to XAI for materials scientists. Concepts are defined to clarify what explain means in the context of materials science. Example works are reviewed to show how XAI helps materials science research. Challenges and opportunities are also discussed.} }
@article{WOS:000532822600001, title = {Machine Learning and Psychological Research: The Unexplored Effect of Measurement}, journal = {PERSPECTIVES ON PSYCHOLOGICAL SCIENCE}, volume = {15}, pages = {809-816}, year = {2020}, issn = {1745-6916}, doi = {10.1177/1745691620902467}, author = {Jacobucci, Ross and Grimm, Kevin J.}, abstract = {Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science.} }
@article{WOS:001248184000003, title = {A systematic review of machine learning methods in software testing}, journal = {APPLIED SOFT COMPUTING}, volume = {162}, year = {2024}, issn = {1568-4946}, doi = {10.1016/j.asoc.2024.111805}, author = {Ajorloo, Sedighe and Jamarani, Amirhossein and Kashfi, Mehdi and Kashani, Mostafa Haghi and Najafizadeh, Abbas}, abstract = {Background: The quest for higher software quality remains a paramount concern in software testing, prompting a shift towards leveraging machine learning techniques for enhanced testing efficacy. Objective: The objective of this paper is to identify, categorize, and systematically compare the present studies on software testing utilizing machine learning methods. Method: This study conducts a systematic literature review (SLR) of 40 pertinent studies spanning from 2018 to March 2024 to comprehensively analyze and classify machine learning methods in software testing. The review encompasses supervised learning, unsupervised learning, reinforcement learning, and hybrid learning approaches. Results: The strengths and weaknesses of each reviewed paper are dissected in this study. This paper also provides an in-depth analysis of the merits of machine learning methods in the context of software testing and addresses current unresolved issues. Potential areas for future research have been discussed, and statistics of each review paper have been collected. Conclusion: By addressing these aspects, this study contributes to advancing the discourse on machine learning's role in software testing and paves the way for substantial improvements in testing efficacy and software quality.} }
@article{WOS:000529286600001, title = {Industry-scale application and evaluation of deep learning for drug target prediction}, journal = {JOURNAL OF CHEMINFORMATICS}, volume = {12}, year = {2020}, issn = {1758-2946}, doi = {10.1186/s13321-020-00428-5}, author = {Sturm, Noe and Mayr, Andreas and Thanh Le Van and Chupakhin, Vladimir and Ceulemans, Hugo and Wegner, Joerg and Golib-Dzib, Jose-Felipe and Jeliazkova, Nina and Vandriessche, Yves and Bohm, Stanislav and Cima, Vojtech and Martinovic, Jan and Greene, Nigel and Vander Aa, Tom and Ashby, Thomas J. and Hochreiter, Sepp and Engkvist, Ola and Klambauer, Guenter and Chen, Hongming}, abstract = {Artificial intelligence (AI) is undergoing a revolution thanks to the breakthroughs of machine learning algorithms in computer vision, speech recognition, natural language processing and generative modelling. Recent works on publicly available pharmaceutical data showed that AI methods are highly promising for Drug Target prediction. However, the quality of public data might be different than that of industry data due to different labs reporting measurements, different measurement techniques, fewer samples and less diverse and specialized assays. As part of a European funded project (ExCAPE), that brought together expertise from pharmaceutical industry, machine learning, and high-performance computing, we investigated how well machine learning models obtained from public data can be transferred to internal pharmaceutical industry data. Our results show that machine learning models trained on public data can indeed maintain their predictive power to a large degree when applied to industry data. Moreover, we observed that deep learning derived machine learning models outperformed comparable models, which were trained by other machine learning algorithms, when applied to internal pharmaceutical company datasets. To our knowledge, this is the first large-scale study evaluating the potential of machine learning and especially deep learning directly at the level of industry-scale settings and moreover investigating the transferability of publicly learned target prediction models towards industrial bioactivity prediction pipelines.} }
@article{WOS:000462654200002, title = {What can Android mobile app developers do about the energy consumption of machine learning?}, journal = {EMPIRICAL SOFTWARE ENGINEERING}, volume = {24}, pages = {562-601}, year = {2019}, issn = {1382-3256}, doi = {10.1007/s10664-018-9629-2}, author = {McIntosh, Andrea and Hassan, Safwat and Hindle, Abram}, abstract = {Machine learning is a popular method of learning functions from data to represent and to classify sensor inputs, multimedia, emails, and calendar events. Smartphone applications have been integrating more and more intelligence in the form of machine learning. Machine learning functionality now appears on most smartphones as voice recognition, spell checking, word disambiguation, face recognition, translation, spatial reasoning, and even natural language summarization. Excited app developers who want to use machine learning on mobile devices face one serious constraint that they did not face on desktop computers or cloud virtual machines: the end-user's mobile device has limited battery life, thus computationally intensive tasks can harm end users' phone availability by draining batteries of their stored energy. Currently, there are few guidelines for developers who want to employ machine learning on mobile devices yet are concerned about software energy consumption of their applications. In this paper, we combine empirical measurements of different machine learning algorithm implementations with complexity theory to provide concrete and theoretically grounded recommendations to developers who want to employ machine learning on smartphones. We conclude that some implementations of algorithms, such as J48, MLP, and SMO, do generally perform better than others in terms of energy consumption and accuracy, and that energy consumption is well-correlated to algorithmic complexity. However, to achieve optimal results a developer must consider their specific application as many factors dataset size, number of data attributes, whether the model will require updating, etc. affect which machine learning algorithm and implementation will provide the best results.} }
@article{WOS:000989319200011, title = {Artificial Intelligence and Machine Learning in Clinical Medicine, 2023}, journal = {NEW ENGLAND JOURNAL OF MEDICINE}, volume = {388}, pages = {1201-1208}, year = {2023}, issn = {0028-4793}, doi = {10.1056/NEJMra2302038}, author = {Haug, Charlotte J. J. and Drazen, Jeffrey M. M.}, abstract = {AI and Machine Learning in Clinical Medicine, 2023This first article in a series describes the history of artificial intelligence in medicine; the use of AI in image analysis, identification of disease outbreaks, and diagnosis; and the use of chatbots.} }
@article{WOS:000469544400001, title = {Machine learning approach for pavement performance prediction}, journal = {INTERNATIONAL JOURNAL OF PAVEMENT ENGINEERING}, volume = {22}, pages = {341-354}, year = {2021}, issn = {1029-8436}, doi = {10.1080/10298436.2019.1609673}, author = {Marcelino, Pedro and Antunes, Maria de Lurdes and Fortunato, Eduardo and Gomes, Marta Castilho}, abstract = {In recent years, there has been an increasing interest in the application of machine learning for the prediction of pavement performance. Prediction models are used to predict the future pavement condition, helping to optimally allocate maintenance and rehabilitation funds. However, few studies have proposed a systematic approach to the development of machine learning models for pavement performance prediction. Most of the studies focus on artificial neural networks models that are trained for high accuracy, disregarding other suitable machine learning algorithms and neglecting the importance of models' generalisation capability for Pavement Engineering applications. This paper proposes a general machine learning approach for the development of pavement performance prediction models in pavement management systems (PMS). The proposed approach supports different machine learning algorithms and emphasizes generalisation performance. A case study for prediction of International Roughness Index (IRI) for 5 and 10-years, using the Long-Term Pavement Performance, is presented. The proposed models were based on a random forest algorithm, using datasets comprising previous IRI measurements, structural, climatic, and traffic data.} }
@article{WOS:000449644300007, title = {Rapid estimation of activation energy in heterogeneous catalytic reactions via machine learning}, journal = {JOURNAL OF COMPUTATIONAL CHEMISTRY}, volume = {39}, pages = {2405-2408}, year = {2018}, issn = {0192-8651}, doi = {10.1002/jcc.25567}, author = {Takahashi, Keisuke and Miyazato, Itsuki}, abstract = {Estimation of activation energies within heterogeneous catalytic reactions is performed using machine learning and catalysts dataset. In particular, descriptors for determining activation energy are revealed within the 788 activation energy dataset. With the implementation of machine learning and chosen descriptors, activation energy can be instantly predicted with over 90\\% accuracy during cross-validation. Thus, rapid estimation of activation energies within heterogeneous catalytic reactions can be made achievable via machine learning, leading toward the acceleration of catalysts design and characterization. (c) 2018 Wiley Periodicals, Inc.} }
@article{WOS:001032105900001, title = {A review of machine learning applications in soccer with an emphasis on injury risk}, journal = {BIOLOGY OF SPORT}, volume = {40}, pages = {233-239}, year = {2023}, issn = {0860-021X}, author = {Nassis, George P. and Verhagen, Evert and Brito, Joao and Figueiredo, Pedro and Krustrup, Peter}, abstract = {This narrative review paper aimed to discuss the literature on machine learning applications in soccer with an emphasis on injury risk assessment. A secondary aim was to provide practical tips for the health and performance staff in soccer clubs on how machine learning can provide a competitive advantage. Performance analysis is the area with the majority of research so far. Other domains of soccer science and medicine with machine learning use are injury risk assessment, players' workload and wellness monitoring, movement analysis, players' career trajectory, club performance, and match attendance. Regarding injuries, which is a hot topic, machine learning does not seem to have a high predictive ability at the moment (models specificity ranged from 74.2\\%-97.7\\%. sensitivity from 15.2\\%-55.6\\% with area under the curve of 0.66-0.83). It seems, though, that machine learning can help to identify the early signs of elevated risk for a musculoskeletal injury. Future research should account for musculoskeletal injuries' dynamic nature for machine learning to provide more meaningful results for practitioners in soccer.} }
@article{WOS:001262652200001, title = {A Review on Machine Learning for Channel Coding}, journal = {IEEE ACCESS}, volume = {12}, pages = {89002-89025}, year = {2024}, issn = {2169-3536}, doi = {10.1109/ACCESS.2024.3412192}, author = {Lim Meng Kee, Heimrih and Ahmad, Norulhusna and Azri Mohd Izhar, Mohd and Anwar, Khoirul and Ng, Soon Xin}, abstract = {The usage of artificial intelligence and machine learning in wireless communications is the stepping stone towards a technological breakthrough in the current limitations of wireless communication systems. The trend of future coding schemes towards 6G appears to be based on rateless schemes and machine learning. Channel coding is important when transmitting data or information reliably as it provides error-correcting purposes. However, there is still a demand for more research regarding machine learning for channel coding. There is also a lack of a specific term or classification for existing machine learning applications for channel coding. This paper explores and compiles current trending machine learning techniques for channel coding. We are also introducing and proposing a new type of machine learning classification for channel coding purposes, as well as surveying some of the papers that fall under the respective class. This paper also discusses current challenges and future machine learning trends for channel coding, which are expected to impact future wireless communications development, especially in channel coding advancements.} }
@article{WOS:000435287000005, title = {Recent trends in machine learning for human activity recognition-A survey}, journal = {WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY}, volume = {8}, year = {2018}, issn = {1942-4787}, doi = {10.1002/widm.1254}, author = {Ramasamy Ramamurthy, Sreenivasan and Roy, Nirmalya}, abstract = {There has been an upsurge recently in investigating machine learning techniques for activity recognition (AR) problems as they have been very effective in extracting and learning knowledge from the activity datasets. The technique ranges from heuristically derived hand-crafted feature-based traditional machine learning algorithms to the recently developed hierarchically self-evolving feature-based deep learning algorithms. AR continues to remain a challenging problem in uncontrolled smart environments despite the amount of work contributed by the researcher in this field. The complex, volatile, and chaotic nature of the activity data presents numerous challenges that influence the performance of the AR systems in the wild. In this article, we present a comprehensive overview of recent machine learning and data mining techniques generally employed for AR and the underpinning problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art techniques in this domain in an attempt to identify the possible directions for future AR research. This article is categorized under: Application Areas > Science and Technology Algorithmic Development > Spatial and Temporal Data Mining Technologies > Machine Learning Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining} }
@article{WOS:000399838700001, title = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {18}, year = {2017}, issn = {1532-4435}, author = {Lemaitre, Guillaume and Nogueira, Fernando and Aridas, Christos K.}, abstract = {imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn.} }
@article{WOS:000712048500001, title = {Machine Learning and Deep Learning for the Pharmacogenomics of Antidepressant Treatments}, journal = {CLINICAL PSYCHOPHARMACOLOGY AND NEUROSCIENCE}, volume = {19}, pages = {577-588}, year = {2021}, issn = {1738-1088}, doi = {10.9758/cpn.2021.19.4.577}, author = {Lin, Eugene and Lin, Chieh-Hsin and Lane, Hsien-Yuan}, abstract = {A growing body of evidence now proposes that machine learning and deep learning techniques can serve as a vital foundation for the pharmacogenomics of antidepressant treatments in patients with major depressive disorder (MDD). In this review, we focus on the latest developments for pharmacogenomics research using machine learning and deep learning approaches together with neuroimaging and multi-omics data. First, we review relevant pharmacogenomics studies that leverage numerous machine learning and deep learning techniques to determine treatment prediction and potential biomarkers for antidepressant treatments in MDD. In addition, we depict some neuroimaging pharmacogenomics studies that utilize various machine learning approaches to predict antidepressant treatment outcomes in MDD based on the integration of research on pharmacogenomics and neuroimaging. Moreover, we summarize the limitations in regard to the past pharmacogenomics studies of antidepressant treatments in MDD. Finally, we outline a discussion of challenges and directions for future research. In light of latest advancements in neuroimaging and multi-omics, various genomic variants and biomarkers associated with antidepressant treatments in MDD are being identified in pharmacogenomics research by employing machine learning and deep learning algorithms.} }
@article{WOS:000685204500060, title = {AL: Autogenerating Supervised Learning Programs}, journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL}, volume = {3}, year = {2019}, doi = {10.1145/3360601}, author = {Cambronero, Jose P. and Rinard, Martin C.}, abstract = {We present AL, a novel automated machine learning system that learns to generate new supervised learning pipelines from an existing corpus of supervised learning programs. In contrast to existing automated machine learning tools, which typically implement a search over manually selected machine learning functions and classes, AL learns to identify the relevant classes in an API by analyzing dynamic program traces that use the target machine learning library. AL constructs a conditional probability model from these traces to estimate the likelihood of the generated supervised learning pipelines and uses this model to guide the search to generate pipelines for new datasets. Our evaluation shows that AL can produce successful pipelines for datasets that previous systems fail to process and produces pipelines with comparable predictive performance for datasets that previous systems process successfully.} }
@article{WOS:000741981200001, title = {Towards Multimodal Machine Learning Prediction of Individual Cognitive Evolution in Multiple Sclerosis}, journal = {JOURNAL OF PERSONALIZED MEDICINE}, volume = {11}, year = {2021}, doi = {10.3390/jpm11121349}, author = {Denissen, Stijn and Chen, Oliver Y. and De Mey, Johan and De Vos, Maarten and Van Schependom, Jeroen and Sima, Diana Maria and Nagels, Guy}, abstract = {Multiple sclerosis (MS) manifests heterogeneously among persons suffering from it, making its disease course highly challenging to predict. At present, prognosis mostly relies on biomarkers that are unable to predict disease course on an individual level. Machine learning is a promising technique, both in terms of its ability to combine multimodal data and through the capability of making personalized predictions. However, most investigations on machine learning for prognosis in MS were geared towards predicting physical deterioration, while cognitive deterioration, although prevalent and burdensome, remained largely overlooked. This review aims to boost the field of machine learning for cognitive prognosis in MS by means of an introduction to machine learning and its pitfalls, an overview of important elements for study design, and an overview of the current literature on cognitive prognosis in MS using machine learning. Furthermore, the review discusses new trends in the field of machine learning that might be adopted for future studies in the field.} }
@article{WOS:000814423300006, title = {FTAP: Feature transferring autonomous machine learning pipeline}, journal = {INFORMATION SCIENCES}, volume = {593}, pages = {385-397}, year = {2022}, issn = {0020-0255}, doi = {10.1016/j.ins.2022.02.006}, author = {Wu, Xing and Chen, Cheng and Li, Pan and Zhong, Mingyu and Wang, Jianjia and Qian, Quan and Ding, Peng and Yao, Junfeng and Guo, Yike}, abstract = {An effective method in machine learning often involves considerable experience with algorithms and domain expertise. Many existing machine learning methods highly rely on feature selection which are always domain-specific. However, the intervention by data scientists is time-consuming and labor-intensive. To meet this challenge, we propose a Feature Transferring Autonomous machine learning Pipeline (FTAP) to improve efficiency and performance. The proposed FTAP has been extensively evaluated on different modalities of data covering audios, images, and texts. Experimental results demonstrate that the proposed FTAP not only outperforms state-of-the-art methods on ESC-50 dataset with multi-class audio classification but also has good performance in distant domain transfer learning. Furthermore, FTAP outperforms TPOT, a state-of-the-art autonomous machine learning tool, on learning tasks. The quantitative and qualitative analysis proves the feasibility and robustness of the proposed FTAP. (C) 2022 Elsevier Inc. All rights reserved.} }
@article{WOS:000441903100001, title = {Rise of the machines? Machine learning approaches and mental health: opportunities and challenges}, journal = {BRITISH JOURNAL OF PSYCHIATRY}, volume = {213}, pages = {509-510}, year = {2018}, issn = {0007-1250}, doi = {10.1192/bjp.2018.105}, author = {Tiffin, Paul A. and Paton, Lewis W.}, abstract = {Machine learning methods are being increasingly applied to physical healthcare. In this article we describe some of the potential benefits, challenges and limitations of this approach in a mental health context. We provide a number of examples where machine learning could add value beyond conventional statistical modelling.Declaration of interestNone.} }
@article{WOS:000797778000010, title = {Applications of machine learning methods in port operations-A systematic literature review}, journal = {TRANSPORTATION RESEARCH PART E-LOGISTICS AND TRANSPORTATION REVIEW}, volume = {161}, year = {2022}, issn = {1366-5545}, doi = {10.1016/j.tre.2022.102722}, author = {Filom, Siyavash and Amiri, Amir M. and Razavi, Saiedeh}, abstract = {Ports are pivotal nodes in supply chain and transportation networks, in which most of the existing data remain underutilized. Machine learning methods are versatile tools to utilize and harness the hidden power of the data. Considering ever-growing adoption of machine learning as a data driven decision-making tool, the port industry is far behind other modes of transportation in this transition. To fill the gap, we aimed to provide a comprehensive systematic literature review on this topic to analyze the previous research from different perspectives such as area of the application, type of application, machine learning method, data, and location of the study. Results showed that the number of articles in the field has been increasing annually, and the most prevalent use case of machine learning methods is to predict different port characteristics. However, there are emerging prescriptive and autonomous use cases of machine learning methods in the literature. Furthermore, research gaps and challenges are identified, and future research directions have been discussed from method-centric and application-centric points of view.} }
@article{WOS:000610764000001, title = {Machine Learning in P\\&C Insurance: A Review for Pricing and Reserving}, journal = {RISKS}, volume = {9}, year = {2021}, doi = {10.3390/risks9010004}, author = {Blier-Wong, Christopher and Cossette, Helene and Lamontagne, Luc and Marceau, Etienne}, abstract = {In the past 25 years, computer scientists and statisticians developed machine learning algorithms capable of modeling highly nonlinear transformations and interactions of input features. While actuaries use GLMs frequently in practice, only in the past few years have they begun studying these newer algorithms to tackle insurance-related tasks. In this work, we aim to review the applications of machine learning to the actuarial science field and present the current state of the art in ratemaking and reserving. We first give an overview of neural networks, then briefly outline applications of machine learning algorithms in actuarial science tasks. Finally, we summarize the future trends of machine learning for the insurance industry.} }
@article{WOS:000569375400002, title = {On-the-Fly Active Learning of Interatomic Potentials for Large-Scale Atomistic Simulations}, journal = {JOURNAL OF PHYSICAL CHEMISTRY LETTERS}, volume = {11}, pages = {6946-6955}, year = {2020}, issn = {1948-7185}, doi = {10.1021/acs.jpclett.0c01061}, author = {Jinnouchi, Ryosuke and Miwa, Kazutoshi and Karsai, Ferenc and Kresse, Georg and Asahi, Ryoji}, abstract = {The on-the-fly generation of machine-learning force fields by active-learning schemes attracts a great deal of attention in the community of atomistic simulations. The algorithms allow the machine to self-learn an interatomic potential and construct machine-learned models on the fly during simulations. State-of-the-art query strategies allow the machine to judge whether new structures are out of the training data set or not. Only when the machine judges the necessity of updating the data set with the new structures are first-principles calculations carried out. Otherwise, the yet available machine-learned model is used to update the atomic positions. In this manner, most of the first-principles calculations are bypassed during training, and overall, simulations are accelerated by several orders of magnitude while retaining almost first-principles accuracy. In this Perspective, after describing essential components of the active-learning algorithms, we demonstrate the power of the schemes by presenting recent applications.} }
@article{WOS:000571944500012, title = {Crop yield prediction using machine learning: A systematic literature review}, journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE}, volume = {177}, year = {2020}, issn = {0168-1699}, doi = {10.1016/j.compag.2020.105709}, author = {van Klompenburg, Thomas and Kassahun, Ayalew and Catal, Cagatay}, abstract = {Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops. Several machine learning algorithms have been applied to support crop yield prediction research. In this study, we performed a Systematic Literature Review (SLR) to extract and synthesize the algorithms and features that have been used in crop yield prediction studies. Based on our search criteria, we retrieved 567 relevant studies from six electronic databases, of which we have selected 50 studies for further analysis using inclusion and exclusion criteria. We investigated these selected studies carefully, analyzed the methods and features used, and provided suggestions for further research. According to our analysis, the most used features are temperature, rainfall, and soil type, and the most applied algorithm is Artificial Neural Networks in these models. After this observation based on the analysis of machine learning-based 50 papers, we performed an additional search in electronic databases to identify deep learning-based studies, reached 30 deep learning-based papers, and extracted the applied deep learning algorithms. According to this additional analysis, Convolutional Neural Networks (CNN) is the most widely used deep learning algorithm in these studies, and the other widely used deep learning algorithms are Long-Short Term Memory (LSTM) and Deep Neural Networks (DNN).} }
@article{WOS:000414424200001, title = {Machine learning: novel bioinformatics approaches for combating antimicrobial resistance}, journal = {CURRENT OPINION IN INFECTIOUS DISEASES}, volume = {30}, pages = {511-517}, year = {2017}, issn = {0951-7375}, doi = {10.1097/QCO.0000000000000406}, author = {Macesic, Nenad and Polubriaginof, Fernanda and Tatonetti, Nicholas P.}, abstract = {Purpose of review Antimicrobial resistance (AMR) is a threat to global health and new approaches to combating AMR are needed. Use of machine learning in addressing AMR is in its infancy but has made promising steps. We reviewed the current literature on the use of machine learning for studying bacterial AMR. Recent findings The advent of large-scale data sets provided by next-generation sequencing and electronic health records make applying machine learning to the study and treatment of AMR possible. To date, it has been used for antimicrobial susceptibility genotype/phenotype prediction, development of AMR clinical decision rules, novel antimicrobial agent discovery and antimicrobial therapy optimization. Summary Application of machine learning to studying AMR is feasible but remains limited. Implementation of machine learning in clinical settings faces barriers to uptake with concerns regarding model interpretability and data quality. Future applications of machine learning to AMR are likely to be laboratory-based, such as antimicrobial susceptibility phenotype prediction.} }
@article{WOS:000814485300003, title = {Lessons from infant learning for unsupervised machine learning}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {4}, pages = {510-520}, year = {2022}, doi = {10.1038/s42256-022-00488-2}, author = {Zaadnoordijk, Lorijn and Besold, Tarek R. and Cusack, Rhodri}, abstract = {Unsupervised machine learning algorithms reduce the dependence on curated, labeled datasets that are characteristic of supervised machine learning. The authors argue that the developmental science of infant cognition could inform the design of unsupervised machine learning approaches. The desire to reduce the dependence on curated, labeled datasets and to leverage the vast quantities of unlabeled data has triggered renewed interest in unsupervised (or self-supervised) learning algorithms. Despite improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning and clustering optimizations, unsupervised machine learning still falls short of its hypothesized potential as a breakthrough paradigm enabling generally intelligent systems. Inspiration from cognitive (neuro)science has been based mostly on adult learners with access to labels and a vast amount of prior knowledge. To push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. We identify three crucial factors enabling infants' quality and speed of learning: (1) babies' information processing is guided and constrained; (2) babies are learning from diverse, multimodal inputs; and (3) babies' input is shaped by development and active learning. We assess the extent to which these insights from infant learning have already been exploited in machine learning, examine how closely these implementations resemble the core insights, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.} }
@article{WOS:000778886900003, title = {Use of supervised machine learning to detect abuse of COVID-19 related domain names}, journal = {COMPUTERS \\& ELECTRICAL ENGINEERING}, volume = {100}, year = {2022}, issn = {0045-7906}, doi = {10.1016/j.compeleceng.2022.107864}, author = {Wang, Zheng}, abstract = {A comprehensive evaluation of supervised machine learning models for COVID-19 related domain name detection is presented. One representative conventional machine learning implementation and nineteen state-of-the-art deep learning implementations are evaluated. The deep learning implementation architectures evaluated include the recurrent, convolutional, and hybrid models. The detection rate metrics and the computing time metrics are considered in the evaluation. The result reveals that advanced deep learning models outperform conventional machine learning models in terms of detection rate. The results also show evidence of a tradeoff between detection rate and computing speed for the selection of machine learning models/architectures. High-frequency lexical analysis is provided for a better understanding of the COVID-19 related domain names. The limitations, implications, and considerations of the use of supervised machine learning to detect abuse of COVID-19 related domain names are discussed.} }
@article{WOS:000534813100012, title = {Performance evaluation of Botnet DDoS attack detection using machine learning}, journal = {EVOLUTIONARY INTELLIGENCE}, volume = {13}, pages = {283-294}, year = {2020}, issn = {1864-5909}, doi = {10.1007/s12065-019-00310-w}, author = {Tuan, Tong Anh and Long, Hoang Viet and Son, Le Hoang and Kumar, Raghvendra and Priyadarshini, Ishaani and Son, Nguyen Thi Kim}, abstract = {Botnet is regarded as one of the most sophisticated vulnerability threats nowadays. A large portion of network traffic is dominated by Botnets. Botnets are conglomeration of trade PCs (Bots) which are remotely controlled by their originator (BotMaster) under a Command and-Control (C\\&C) foundation. They are the keys to several Internet assaults like spams, Distributed Denial of Service Attacks (DDoS), rebate distortions, malwares and phishing. To over the problem of DDoS attack, various machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) (K-means, X-means etc.) were proposed. With the increasing popularity of Machine Learning in the field of Computer Security, it will be a remarkable accomplishment to carry out performance assessment of the machine learning methods given a common platform. This could assist developers in choosing a suitable method for their case studies and assist them in further research. This paper performed an experimental analysis of the machine learning methods for Botnet DDoS attack detection. The evaluation is done on the UNBS-NB 15 and KDD99 which are well-known publicity datasets for Botnet DDoS attack detection. Machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) are investigated for Accuracy, False Alarm Rate (FAR), Sensitivity, Specificity, False positive rate (FPR), AUC, and Matthews correlation coefficient (MCC) of datasets. Performance of KDD99 dataset has been experimentally shown to be better as compared to the UNBS-NB 15 dataset. This validation is significant in computer security and other related fields.} }
@article{WOS:000821577800005, title = {Hyperspectral Image Classification: Potentials, Challenges, and Future Directions}, journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE}, volume = {2022}, year = {2022}, issn = {1687-5265}, doi = {10.1155/2022/3854635}, author = {Datta, Debaleena and Mallick, Pradeep Kumar and Bhoi, Akash Kumar and Ijaz, Muhammad Fazal and Shafi, Jana and Choi, Jaeyoung}, abstract = {Recent imaging science and technology discoveries have considered hyperspectral imagery and remote sensing. The current intelligent technologies, such as support vector machines, sparse representations, active learning, extreme learning machines, transfer learning, and deep learning, are typically based on the learning of the machines. These techniques enrich the processing of such three-dimensional, multiple bands, and high-resolution images with their precision and fidelity. This article presents an extensive survey depicting machine-dependent technologies' contributions and deep learning on landcover classification based on hyperspectral images. The objective of this study is three-fold. First, after reading a large pool of Web of Science (WoS), Scopus, SCI, and SCIE-indexed and SCIE-related articles, we provide a novel approach for review work that is entirely systematic and aids in the inspiration of finding research gaps and developing embedded questions. Second, we emphasize contemporary advances in machine learning (ML) methods for identifying hyperspectral images, with a brief, organized overview and a thorough assessment of the literature involved. Finally, we draw the conclusions to assist researchers in expanding their understanding of the relationship between machine learning and hyperspectral images for future research.} }
@article{WOS:000755564500001, title = {Machine learning political orders}, journal = {REVIEW OF INTERNATIONAL STUDIES}, volume = {49}, pages = {20-36}, year = {2023}, issn = {0260-2105}, doi = {10.1017/S0260210522000031}, author = {Amoore, Louise}, abstract = {A significant set of epistemic and political transformations are taking place as states and societies begin to understand themselves and their problems through the paradigm of deep neural network algorithms. A machine learning political order does not merely change the political technologies of governance, but is itself a reordering of politics, of what the political can be. When algorithmic systems reduce the pluridimensionality of politics to the output of a model, they simultaneously foreclose the potential for other political claims to be made and alternative political projects to be built. More than this foreclosure, a machine learning political order actively profits and learns from the fracturing of communities and the destabilising of democratic rights. The transformation from rules-based algorithms to deep learning models has paralleled the undoing of rules-based social and international orders - from the use of machine learning in the campaigns of the UK EU referendum, to the trialling of algorithmic immigration and welfare systems, and the use of deep learning in the COVID-19 pandemic - with political problems becoming reconfigured as machine learning problems. Machine learning political orders decouple their attributes, features and clusters from underlying social values, no longer tethered to notions of good governance or a good society, but searching instead for the optimal function of abstract representations of data.} }
@article{WOS:000692200100001, title = {Universal Approximation Property of Quantum Machine Learning Models in Quantum-Enhanced Feature Spaces}, journal = {PHYSICAL REVIEW LETTERS}, volume = {127}, year = {2021}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.127.090506}, author = {Goto, Takahiro and Tran, Quoc Hoan and Nakajima, Kohei}, abstract = {Encoding classical data into quantum states is considered a quantum feature map to map classical data into a quantum Hilbert space. This feature map provides opportunities to incorporate quantum advantages into machine learning algorithms to be performed on near-term intermediate-scale quantum computers. The crucial idea is using the quantum Hilbert space as a quantum-enhanced feature space in machine learning models. Although the quantum feature map has demonstrated its capability when combined with linear classification models in some specific applications, its expressive power from the theoretical perspective remains unknown. We prove that the machine learning models induced from the quantum-enhanced feature space are universal approximators of continuous functions under typical quantum feature maps. We also study the capability of quantum feature maps in the classification of disjoint regions. Our work enables an important theoretical analysis to ensure that machine learning algorithms based on quantum feature maps can handle a broad class of machine learning tasks. In light of this, one can design a quantum machine learning model with more powerful expressivity.} }
@article{WOS:000760291800021, title = {Translating promise into practice: a review of machine learning in suicide research and prevention}, journal = {LANCET PSYCHIATRY}, volume = {9}, pages = {243-252}, year = {2022}, issn = {2215-0366}, doi = {10.1016/s2215-0366(21)00254-6}, author = {Kirtley, Olivia J. and van Mens, Kasper and Hoogendoorn, Mark and Kapur, Navneet and de Beurs, Derek}, abstract = {In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need-ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.} }
@article{WOS:001209420900001, title = {Integrating Machine Learning in Metabolomics: A Path to Enhanced Diagnostics and Data Interpretation}, journal = {SMALL METHODS}, volume = {8}, year = {2024}, issn = {2366-9608}, doi = {10.1002/smtd.202400305}, author = {Xu, Yudian and Cao, Linlin and Chen, Yifan and Zhang, Ziyue and Liu, Wanshan and Li, He and Ding, Chenhuan and Pu, Jun and Qian, Kun and Xu, Wei}, abstract = {Metabolomics, leveraging techniques like NMR and MS, is crucial for understanding biochemical processes in pathophysiological states. This field, however, faces challenges in metabolite sensitivity, data complexity, and omics data integration. Recent machine learning advancements have enhanced data analysis and disease classification in metabolomics. This study explores machine learning integration with metabolomics to improve metabolite identification, data efficiency, and diagnostic methods. Using deep learning and traditional machine learning, it presents advancements in metabolic data analysis, including novel algorithms for accurate peak identification, robust disease classification from metabolic profiles, and improved metabolite annotation. It also highlights multiomics integration, demonstrating machine learning's potential in elucidating biological phenomena and advancing disease diagnostics. This work contributes significantly to metabolomics by merging it with machine learning, offering innovative solutions to analytical challenges and setting new standards for omics data analysis.} }
@article{WOS:000428582200001, title = {A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View}, journal = {IEEE ACCESS}, volume = {6}, pages = {12103-12117}, year = {2018}, issn = {2169-3536}, doi = {10.1109/ACCESS.2018.2805680}, author = {Liu, Qiang and Li, Pan and Zhao, Wentao and Cai, Wei and Yu, Shui and Leung, Victor C. M.}, abstract = {Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.} }
@article{WOS:000963897500001, title = {Data Sensitivity and Domain Specificity in Reuse of Machine Learning Applications}, journal = {INFORMATION SYSTEMS FRONTIERS}, volume = {26}, pages = {633-640}, year = {2024}, issn = {1387-3326}, doi = {10.1007/s10796-023-10388-4}, author = {Rutschi, Corinna and Berente, Nicholas and Nwanganga, Frederick}, abstract = {Data sensitivity and domain specificity challenges arise in reuse of machine learning applications. We identify four types of machine learning applications based on different reuse strategies: generic, distinctive, selective, and exclusive. We conclude with lessons for developing and deploying machine learning applications.} }
@article{WOS:000592355900013, title = {Machine learning for suicidology: A practical review of exploratory and hypothesis-driven approaches}, journal = {CLINICAL PSYCHOLOGY REVIEW}, volume = {82}, year = {2020}, issn = {0272-7358}, doi = {10.1016/j.cpr.2020.101940}, author = {Cox, Christopher R. and Moscardini, Emma H. and Cohen, Alex S. and Tucker, Raymond P.}, abstract = {Machine learning is being used to discover models to predict the progression from suicidal ideation to action in clinical populations. While quantifiable improvements in prediction accuracy have been achieved over theory -driven efforts, models discovered through machine learning continue to fall short of clinical relevance. Thus, the value of machine learning for reaching this objective is hotly contested. We agree that machine learning, treated as a ``black box'' approach antithetical to theory-building, will not discover clinically relevant models of suicide. However, such models may be developed through deliberate synthesis of dataand theory-driven approaches. By providing an accessible overview of essential concepts and common methods, we highlight how generalizable models and scientific insight may be obtained by incorporating prior knowledge and expectations to machine learning research, drawing examples from suicidology. We then discuss challenges investigators will face when using machine learning to discover models of low prevalence outcomes, such as suicide.} }
@article{WOS:000684684400001, title = {Teaching Machine Learning in K-12 Classroom: Pedagogical and Technological Trajectories for Artificial Intelligence Education}, journal = {IEEE ACCESS}, volume = {9}, pages = {110558-110572}, year = {2021}, issn = {2169-3536}, doi = {10.1109/ACCESS.2021.3097962}, author = {Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen, Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold}, abstract = {Over the past decades, numerous practical applications of machine learning techniques have shown the potential of AI-driven and data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning and AI in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based ``traditional'' programming is a central aspect and building block in developing next generation computational thinking.} }
@article{WOS:000403140800087, title = {Machine Learning With Big Data: Challenges and Approaches}, journal = {IEEE ACCESS}, volume = {5}, pages = {7776-7797}, year = {2017}, issn = {2169-3536}, doi = {10.1109/ACCESS.2017.2696365}, author = {L'Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and Capretz, Miriam A. M.}, abstract = {The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.} }
@article{WOS:000833393900003, title = {Physics-based machine learning method and the application to energy consumption prediction in tunneling construction}, journal = {ADVANCED ENGINEERING INFORMATICS}, volume = {53}, year = {2022}, issn = {1474-0346}, doi = {10.1016/j.aei.2022.101642}, author = {Zhou, Siyang and Liu, Shanglin and Kang, Yilan and Cai, Jie and Xie, Haimei and Zhang, Qian}, abstract = {Representing causality in machine learning to predict control parameters is state-of-the-art research in intelligent control. This study presents a physics-based machine learning method providing a prediction model that guarantees enhanced interpretability conforming to physical laws. The proposed approach encodes physical knowledge as mapping relationships between variables in engineering dataset into the learning procedure through dimensional analysis. This derives causal relationships between the control parameter and its influencing factors. The proposed machine learning method's objective function is further improved by the penalty term in the regularization strategy. Verifications on the energy consumption prediction of tunnel boring machine prove that, the established model accords with basic principles in this field. Moreover, the proposed approach traces the impact of three major factors (structure, operation, and geology) along the construction section, offering each component's contribution rates to energy consumption. Compared with several commonly used machine learning algorithms, the proposed method reduces the need for large amounts of training data and demonstrates higher accuracy. The results indicate that the revealed causality and enhanced prediction performance of the proposed method advance the applicability of machine learning methods to intelligent control during construction.} }
@article{WOS:000617753000001, title = {Machine learning approach for the prediction and optimization of thermal transport properties}, journal = {FRONTIERS OF PHYSICS}, volume = {16}, year = {2021}, issn = {2095-0462}, doi = {10.1007/s11467-020-1041-x}, author = {Ouyang, Yulou and Yu, Cuiqian and Yan, Gang and Chen, Jie}, abstract = {Traditional simulation methods have made prominent progress in aiding experiments for understanding thermal transport properties of materials, and in predicting thermal conductivity of novel materials. However, huge challenges are also encountered when exploring complex material systems, such as formidable computational costs. As a rising computational method, machine learning has a lot to offer in this regard, not only in speeding up the searching and optimization process, but also in providing novel perspectives. In this work, we review the state-of-the-art studies on material's thermal properties based on machine learning technique. First, the basic principles of machine learning method are introduced. We then review applications of machine learning technique in the prediction and optimization of material's thermal properties, including thermal conductivity and interfacial thermal resistance. Finally, an outlook is provided for the future studies.} }
@article{WOS:000834797800001, title = {Distributing epistemic functions and tasks-A framework for augmenting human analytic power with machine learning in science education research}, journal = {JOURNAL OF RESEARCH IN SCIENCE TEACHING}, volume = {60}, pages = {423-447}, year = {2023}, issn = {0022-4308}, doi = {10.1002/tea.21803}, author = {Kubsch, Marcus and Krist, Christina and Rosenberg, Joshua M.}, abstract = {Machine learning (ML) has become commonplace in educational research and science education research, especially to support assessment efforts. Such applications of machine learning have shown their promise in replicating and scaling human-driven codes of students' work. Despite this promise, we and other scholars argue that machine learning has not yet achieved its transformational potential. We argue that this is because our field is currently lacking frameworks for supporting creative, principled, and critical endeavors to use machine learning in science education research. To offer considerations for science education researchers' use of ML, we present a framework, Distributing Epistemic Functions and Tasks (DEFT), that highlights the functions and tasks that pertain to generating knowledge that can be carried out by either trained researchers or machine learning algorithms. Such considerations are critical decisions that should occur alongside those about, for instance, the type of data or algorithm used. We apply this framework to two cases, one that exemplifies the cutting-edge use of machine learning in science education research and another that offers a wholly different means of using machine learning and human-driven inquiry together. We conclude with strategies for researchers to adopt machine learning and call for the field to rethink how we prepare science education researchers in an era of great advances in computational power and access to machine learning methods.} }
@article{WOS:000705849400001, title = {Quantum semi-supervised kernel learning}, journal = {QUANTUM MACHINE INTELLIGENCE}, volume = {3}, year = {2021}, issn = {2524-4906}, doi = {10.1007/s42484-021-00053-x}, author = {Saeedi, Seyran and Panahi, Aliakbar and Arodz, Tom}, abstract = {Quantum machine learning methods have the potential to facilitate learning using extremely large datasets. While the availability of data for training machine learning models is steadily increasing, oftentimes it is much easier to collect feature vectors to obtain the corresponding labels. One of the approaches for addressing this issue is to use semi-supervised learning, which leverages not only the labeled samples, but also unlabeled feature vectors. Here, we present a quantum machine learning algorithm for training semi-supervised kernel support vector machines. The algorithm uses recent advances in quantum sample-based Hamiltonian simulation to extend the existing quantum LS-SVM algorithm to handle the semi-supervised term in the loss. Through a theoretical study of the algorithm's computational complexity, we show that it maintains the same speedup as the fully-supervised quantum LS-SVM.} }
@article{WOS:001138185200001, title = {State of the art in applications of machine learning in steelmaking process modeling}, journal = {INTERNATIONAL JOURNAL OF MINERALS METALLURGY AND MATERIALS}, volume = {30}, pages = {2055-2075}, year = {2023}, issn = {1674-4799}, doi = {10.1007/s12613-023-2646-1}, author = {Zhang, Runhao and Yang, Jian}, abstract = {With the development of automation and informatization in the steelmaking industry, the human brain gradually fails to cope with an increasing amount of data generated during the steelmaking process. Machine learning technology provides a new method other than production experience and metallurgical principles in dealing with large amounts of data. The application of machine learning in the steelmaking process has become a research hotspot in recent years. This paper provides an overview of the applications of machine learning in the steelmaking process modeling involving hot metal pretreatment, primary steelmaking, secondary refining, and some other aspects. The three most frequently used machine learning algorithms in steelmaking process modeling are the artificial neural network, support vector machine, and case-based reasoning, demonstrating proportions of 56\\%, 14\\%, and 10\\%, respectively. Collected data in the steelmaking plants are frequently faulty. Thus, data processing, especially data cleaning, is crucially important to the performance of machine learning models. The detection of variable importance can be used to optimize the process parameters and guide production. Machine learning is used in hot metal pretreatment modeling mainly for endpoint S content prediction. The predictions of the endpoints of element compositions and the process parameters are widely investigated in primary steelmaking. Machine learning is used in secondary refining modeling mainly for ladle furnaces, Ruhrstahl-Heraeus, vacuum degassing, argon oxygen decarburization, and vacuum oxygen decarburization processes. Further development of machine learning in the steelmaking process modeling can be realized through additional efforts in the construction of the data platform, the industrial transformation of the research achievements to the practical steelmaking process, and the improvement of the universality of the machine learning models.} }
@article{WOS:000614082300001, title = {Machine Learning Methods for Diagnosing Autism Spectrum Disorder and Attention- Deficit/Hyperactivity Disorder Using Functional and Structural MRI: A Survey}, journal = {FRONTIERS IN NEUROINFORMATICS}, volume = {14}, year = {2021}, doi = {10.3389/fninf.2020.575999}, author = {Eslami, Taban and Almuqhim, Fahad and Raiker, Joseph S. and Saeed, Fahad}, abstract = {Here we summarize recent progress in machine learning model for diagnosis of Autism Spectrum Disorder (ASD) and Attention-deficit/Hyperactivity Disorder (ADHD). We outline and describe the machine-learning, especially deep-learning, techniques that are suitable for addressing research questions in this domain, pitfalls of the available methods, as well as future directions for the field. We envision a future where the diagnosis of ASD, ADHD, and other mental disorders is accomplished, and quantified using imaging techniques, such as MRI, and machine-learning models.} }
@article{WOS:000681124300016, title = {Predicting Machine Learning Pipeline Runtimes in the Context of Automated Machine Learning}, journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, volume = {43}, pages = {3055-3066}, year = {2021}, issn = {0162-8828}, doi = {10.1109/TPAMI.2021.3056950}, author = {Mohr, Felix and Wever, Marcel and Tornede, Alexander and Huellermeier, Eyke}, abstract = {Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.} }
@article{WOS:000464121000002, title = {Machine Learning in Banking Risk Management: A Literature Review}, journal = {RISKS}, volume = {7}, year = {2019}, issn = {2227-9091}, doi = {10.3390/risks7010029}, author = {Leo, Martin and Sharma, Suneel and Maddulety, K.}, abstract = {There is an increasing influence of machine learning in business applications, with many solutions already implemented and many more being explored. Since the global financial crisis, risk management in banks has gained more prominence, and there has been a constant focus around how risks are being detected, measured, reported and managed. Considerable research in academia and industry has focused on the developments in banking and risk management and the current and emerging challenges. This paper, through a review of the available literature seeks to analyse and evaluate machine-learning techniques that have been researched in the context of banking risk management, and to identify areas or problems in risk management that have been inadequately explored and are potential areas for further research. The review has shown that the application of machine learning in the management of banking risks such as credit risk, market risk, operational risk and liquidity risk has been explored; however, it doesn't appear commensurate with the current industry level of focus on both risk management and machine learning. A large number of areas remain in bank risk management that could significantly benefit from the study of how machine learning can be applied to address specific problems.} }
@article{WOS:000253272100001, title = {Preliminary studyon Wilcoxon learning machines}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS}, volume = {19}, pages = {201-211}, year = {2008}, issn = {1045-9227}, doi = {10.1109/TNN.2007.904035}, author = {Hsieh, Jer-Guang and Lin, Yih-Lon and Jeng, Jyh-Horng}, abstract = {As is well known in statistics, the resulting linear regressors by using the rank-based Wilcoxon approach to linear regression problems are usually robust against (or insensitive to) outliers. This motivates us to introduce in this paper the Wilcoxon approach to the area, of machine learning. Specifically, we investigate four new learning machines, namely Wilcoxon neural network (WNN), Wilcoxon generalized radial basis function network (WGRBFN), Wilcoxon fuzzy neural network (WFNN), and kernel-based Wilcoxon regressor (KWR). These provide alternative learning machines when faced with general nonlinear learning problems. Simple weights updating rules based on gradient descent will be derived. Some numerical examples will be provided to compare the robustness against outliers for various learning machines. Simulation results show that the Wilcoxon learning machines proposed in this paper have good robustness against outliers. We firmly believe that the Wilcoxon approach will provide a promising methodology for many machine learning problems.} }
@article{WOS:000419350700030, title = {MoleculeNet: a benchmark for molecular machine learning}, journal = {CHEMICAL SCIENCE}, volume = {9}, pages = {513-530}, year = {2018}, issn = {2041-6520}, doi = {10.1039/c7sc02664a}, author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay}, abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.} }
@article{WOS:000808086800005, title = {Knowledge-Driven Machine Learning and Applications in Wireless Communications}, journal = {IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING}, volume = {8}, pages = {454-467}, year = {2022}, issn = {2332-7731}, doi = {10.1109/TCCN.2021.3128597}, author = {Li, Daofeng and Xu, Yamei and Zhao, Ming and Zhu, Jinkang and Zhang, Sihai}, abstract = {The power of big data and machine learning has been drastically demonstrated in many fields during the past twenty years which somehow leads to the vague even false understanding that the huge amount of precious human knowledge accumulated to date seems to no longer matter. In this paper, we are pioneering to propose the knowledge-driven machine learning (KDML) model to exhibit that knowledge can play an important role in machine learning tasks. Compared with conventional machine learning, KDML contains a unique knowledge module based on specific domain knowledge, which is able to simplify the machine learning network structures, reduce the training overhead and improve interpretability. Channel estimation problem of wireless communication is taken as a case verification because such machine learning-based solutions face huge challenges in terms of accuracy, complexity, and reliability. We integrate the classical wireless channel estimation algorithms into different machine learning neural networks and propose KDML-based channel estimators in Orthogonal Frequency Division Multiplexing (OFDM) and Massive Multiple Input Multiple Output (MIMO) system. The experimental results in both communication systems validate the effectiveness of the proposed KDML-based channel estimators.} }
@article{WOS:000831186100001, title = {PASSer2.0: Accurate Prediction of Protein Allosteric Sites Through Automated Machine Learning}, journal = {FRONTIERS IN MOLECULAR BIOSCIENCES}, volume = {9}, year = {2022}, doi = {10.3389/fmolb.2022.879251}, author = {Xiao, Sian and Tian, Hao and Tao, Peng}, abstract = {Allostery is a fundamental process in regulating protein activities. The discovery, design, and development of allosteric drugs demand better identification of allosteric sites. Several computational methods have been developed previously to predict allosteric sites using static pocket features and protein dynamics. Here, we define a baseline model for allosteric site prediction and present a computational model using automated machine learning. Our model, PASSer2.0, advanced the previous results and performed well across multiple indicators with 82.7\\% of allosteric pockets appearing among the top three positions. The trained machine learning model has been integrated with the to facilitate allosteric drug discovery.} }
@article{WOS:000612766700011, title = {Machine Learning and the Future of Cardiovascular Care JACC State-of-the-Art Review}, journal = {JACC-JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY}, volume = {77}, pages = {300-313}, year = {2021}, issn = {0735-1097}, doi = {10.1016/j.jacc.2020.11.030}, author = {Quer, Giorgio and Arnaout, Ramy and Henne, Michael and Arnaout, Rima}, abstract = {The role of physicians has always been to synthesize the data available to them to identify diagnostic patterns that guide treatment and follow response. Today, increasingly sophisticated machine learning algorithms may grow to support clinical experts in some of these tasks. Machine learning has the potential to benefit patients and cardiologists, but only if clinicians take an active role in bringing these new algorithms into practice. The aim of this review is to introduce clinicians who are not data science experts to key concepts in machine learning that will allow them to better understand the field and evaluate new literature and developments. The current published data in machine learning for cardiovascular disease is then summarized, using both a bibliometric survey, with code publicly available to enable similar analysis for any research topic of interest, and select case studies. Finally, several ways that clinicians can and must be involved in this emerging field are presented. (C) 2021 The Authors. Published by Elsevier on behalf of the American College of Cardiology Foundation.} }
@article{WOS:001292266400001, title = {Advances in Machine Learning for Wearable Sensors}, journal = {ACS NANO}, volume = {18}, pages = {22734-22751}, year = {2024}, issn = {1936-0851}, doi = {10.1021/acsnano.4c05851}, author = {Xiao, Xiao and Yin, Junyi and Xu, Jing and Tat, Trinny and Chen, Jun}, abstract = {Recent years have witnessed tremendous advances in machine learning techniques for wearable sensors and bioelectronics, which play an essential role in real-time sensing data analysis to provide clinical-grade information for personalized healthcare. To this end, supervised learning and unsupervised learning algorithms have emerged as powerful tools, allowing for the detection of complex patterns and relationships in large, high-dimensional data sets. In this Review, we aim to delineate the latest advancements in machine learning for wearable sensors, focusing on key developments in algorithmic techniques, applications, and the challenges intrinsic to this evolving landscape. Additionally, we highlight the potential of machine-learning approaches to enhance the accuracy, reliability, and interpretability of wearable sensor data and discuss the opportunities and limitations of this emerging field. Ultimately, our work aims to provide a roadmap for future research endeavors in this exciting and rapidly evolving area.} }
@article{WOS:000431737300083, title = {A machine learning model with human cognitive biases capable of learning from small and biased datasets}, journal = {SCIENTIFIC REPORTS}, volume = {8}, year = {2018}, issn = {2045-2322}, doi = {10.1038/s41598-018-25679-z}, author = {Taniguchi, Hidetaka and Sato, Hiroshi and Shirakawa, Tomohiro}, abstract = {Human learners can generalize a new concept from a small number of samples. In contrast, conventional machine learning methods require large amounts of data to address the same types of problems. Humans have cognitive biases that promote fast learning. Here, we developed a method to reduce the gap between human beings and machines in this type of inference by utilizing cognitive biases. We implemented a human cognitive model into machine learning algorithms and compared their performance with the currently most popular methods, naive Bayes, support vector machine, neural networks, logistic regression and random forests. We focused on the task of spam classification, which has been studied for a long time in the field of machine learning and often requires a large amount of data to obtain high accuracy. Our models achieved superior performance with small and biased samples in comparison with other representative machine learning methods.} }
@article{WOS:000270620400005, title = {Some single-machine and m-machine flowshop scheduling problems with learning considerations}, journal = {INFORMATION SCIENCES}, volume = {179}, pages = {3885-3892}, year = {2009}, issn = {0020-0255}, doi = {10.1016/j.ins.2009.07.011}, author = {Lee, Wen-Chiung and Wu, Chin-Chia}, abstract = {Scheduling with learning effect has drawn many researchers' attention since Biskup [D. Biskup, Single-machine scheduling with learning considerations, European journal of Opterational Research 115 (1999) 173-178] introduced the concept of learning into the scheduling field. Biskup [D. Biskup, A state-of-the-art review on scheduling with learning effect, European journal of Opterational Research 188 (2008) 315-329] classified the learning approaches in the literature into two main streams. He claimed that the position-based learning seems to be a realistic model for machine learning, while the sum-of-processing-time-based learning is a model for human learning. In some realistic situations, both the machine and human learning might exist simultaneously. For example, robots with neural networks are used in computers, motor vehicles, and many assembly lines. The actions of a robot are constantly modified through self-learning in processing the jobs. On the other hand, the operators in the control center learn how to give the commands efficiently through working experience. In this paper, we propose a new learning model that unifies the two main approaches. We show that some single-machine problems and some specified flowshop problems are polynomially solvable. (c) 2009 Elsevier Inc. All rights reserved.} }
@article{WOS:000412361900042, title = {Imbalance Learning Machine-Based Power System Short-Term Voltage Stability Assessment}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {13}, pages = {2533-2543}, year = {2017}, issn = {1551-3203}, doi = {10.1109/TII.2017.2696534}, author = {Zhu, Lipeng and Lu, Chao and Dong, Zhao Yang and Hong, Chao}, abstract = {In terms of machine learning-based power system dynamic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a challenging class imbalance problem. Besides, short-term feature extraction from scarce instability seems extremely difficult for conventional learning machines. Faced with such a dilemma, this paper develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is embedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribution. Cost-sensitive learning is employed to intensify bias toward those scarce yet valuable unstable cases. Furthermore, an incremental learning strategy is put forward for online monitoring, contributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the assessment scheme.} }
@article{WOS:000425074300032, title = {Applying spark based machine learning model on streaming big data for health status prediction}, journal = {COMPUTERS \\& ELECTRICAL ENGINEERING}, volume = {65}, pages = {393-399}, year = {2018}, issn = {0045-7906}, doi = {10.1016/j.compeleceng.2017.03.009}, author = {Nair, Lekha R. and Shetty, Sujala D. and Shetty, Siddhanth D.}, abstract = {Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action. (C) 2017 Elsevier Ltd. All rights reserved.} }
@article{WOS:000519206300017, title = {Analysis of non-iterative phase retrieval based on machine learning}, journal = {OPTICAL REVIEW}, volume = {27}, pages = {136-141}, year = {2020}, issn = {1340-6000}, doi = {10.1007/s10043-019-00574-8}, author = {Nishizaki, Yohei and Horisaki, Ryoichi and Kitaguchi, Katsuhisa and Saito, Mamoru and Tanida, Jun}, abstract = {In this paper, we analyze a machine-learning-based non-iterative phase retrieval method. Phase retrieval and its applications have been attractive research topics in optics and photonics, for example, in biomedical imaging, astronomical imaging, and so on. Most conventional phase retrieval methods have used iterative processes to recover phase information; however, the calculation speed and convergence with these methods are serious issues in real-time monitoring applications. Machine-learning-based methods are promising for addressing these issues. Here, we numerically compare conventional methods and a machine-learning-based method in which a convolutional neural network is employed. Simulations with several conditions show that the machine-learning-based method realizes fast and robust phase recovery compared with the conventional methods. We also numerically demonstrate machine-learning-based phase retrieval from noisy measurements with a noisy training data set for improving the noise robustness. The machine-learning-based approach used in this study may increase the impact of phase retrieval, which is useful in various fields, where phase retrieval has been used as a fundamental tool.} }
@article{WOS:000368151700007, title = {Classification of textile fabrics by use of spectroscopy-based pattern recognition methods}, journal = {SPECTROSCOPY LETTERS}, volume = {49}, pages = {96-102}, year = {2016}, issn = {0038-7010}, doi = {10.1080/00387010.2015.1089446}, author = {Sun, Xudong and Zhou, Mingxing and Sun, Yize}, abstract = {The combination of near-infrared spectroscopy and pattern recognition methods, including soft independent modeling of class analogy, least squares support machine, and extreme learning machine, was employed for textile fabrics classification. The fabrics of cotton, viscose, acrylic, polyamide, polyester, and blend fabric of cotton-viscose were divided into training and prediction sets (60: 60) for developing models and evaluating the classification abilities of the models. The classification accuracy and speed of soft independent modeling of class analogy, least squares support machine, and extreme learning machine were compared. Both least squares support machine and extreme learning machine achieved the classification accuracy of 100\\% for the prediction set. However, extreme learning machine performed much faster than least squares support machine, which suggested that extreme learning machine may be a promising method for real-time textile fabrics classification with a comparable accuracy based on near-infrared spectroscopy. Moreover, it might have commercial and regulatory potential to avoid time-consuming work, and costly and laborious chemical analysis for textile fabrics classification.} }
@article{WOS:000605202300001, title = {Second opinion needed: communicating uncertainty in medical machine learning}, journal = {NPJ DIGITAL MEDICINE}, volume = {4}, year = {2021}, issn = {2398-6352}, doi = {10.1038/s41746-020-00367-3}, author = {Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L.}, abstract = {There is great excitement that medical artificial intelligence (AI) based on machine learning (ML) can be used to improve decision making at the patient level in a variety of healthcare settings. However, the quantification and communication of uncertainty for individual predictions is often neglected even though uncertainty estimates could lead to more principled decision-making and enable machine learning models to automatically or semi-automatically abstain on samples for which there is high uncertainty. In this article, we provide an overview of different approaches to uncertainty quantification and abstention for machine learning and highlight how these techniques could improve the safety and reliability of current ML systems being used in healthcare settings. Effective quantification and communication of uncertainty could help to engender trust with healthcare workers, while providing safeguards against known failure modes of current machine learning approaches. As machine learning becomes further integrated into healthcare environments, the ability to say ``I'm not sure'' or ``I don't know'' when uncertain is a necessary capability to enable safe clinical deployment.} }
@article{WOS:000526850800009, title = {Machine learning in nephrology: scratching the surface}, journal = {CHINESE MEDICAL JOURNAL}, volume = {133}, pages = {687-698}, year = {2020}, issn = {0366-6999}, doi = {10.1097/CM9.0000000000000694}, author = {Li Qi and Fan Qiu-Ling and Han Qiu-Xia and Geng Wen-Jia and Zhao Huan-Huan and Ding Xiao-Nan and Yan Jing-Yao and Zhu Han-Yu}, abstract = {Machine learning shows enormous potential in facilitating decision-making regarding kidney diseases. With the development of data preservation and processing, as well as the advancement of machine learning algorithms, machine learning is expected to make remarkable breakthroughs in nephrology. Machine learning models have yielded many preliminaries to moderate and several excellent achievements in the fields, including analysis of renal pathological images, diagnosis and prognosis of chronic kidney diseases and acute kidney injury, as well as management of dialysis treatments. However, it is just scratching the surface of the field; at the same time, machine learning and its applications in renal diseases are facing a number of challenges. In this review, we discuss the application status, challenges and future prospects of machine learning in nephrology to help people further understand and improve the capacity for prediction, detection, and care quality in kidney diseases.} }
@article{WOS:000644443200006, title = {From Server-Based to Client-Based Machine Learning: A Comprehensive Survey}, journal = {ACM COMPUTING SURVEYS}, volume = {54}, year = {2021}, issn = {0360-0300}, doi = {10.1145/3424660}, author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua}, abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users' local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users' sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.} }
@article{WOS:000466446500036, title = {Machine learning-assisted directed protein evolution with combinatorial libraries}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {116}, pages = {8852-8858}, year = {2019}, issn = {0027-8424}, doi = {10.1073/pnas.1901979116}, author = {Wu, Zachary and Kan, S. B. Jennifer and Lewis, Russell D. and Wittmann, Bruce J. and Arnold, Frances H.}, abstract = {To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si-H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93\\% and 79\\% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.} }
@article{WOS:000608918500001, title = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {21}, year = {2020}, issn = {1532-4435}, author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle}, abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.} }
@article{WOS:000969320000010, title = {Machine learning analysis: general features, requirements and cardiovascular applications}, journal = {MINERVA CARDIOLOGY AND ANGIOLOGY}, volume = {70}, pages = {67-74}, year = {2022}, issn = {2724-5683}, doi = {10.23736/S2724-5683.21.05637-4}, author = {Ricciardi, Carlo and Cuocolo, Renato and Megna, Rosario and Cesarelli, Mario and Petretta, Mario}, abstract = {Artificial intelligence represents the science which will probably change the future of medicine by solving actually challenging issues. In this special article, the general features of machine learning are discussed. First, a background explanation regarding the division of artificial intelligence, machine learning and deep learning is given and a focus on the structure of machine learning subgroups is shown. The traditional process of a machine learning analysis is described, starting from the collection of data, across features engineering, modelling and till the validation and deployment phase. Due to the several applications of machine learning performed in literature in the last decades and the lack of some guidelines, the need of a standardization for reporting machine learning analysis results emerged. Some possible standards for reporting machine learning results are identified and discussed deeply; these are related to study population (number of subjects), repeatability of the analysis, validation, results, comparison with current practice. The way to the use of machine learning in clinical practice is open and the hope is that, with emerging technology and advanced digital and computational tools, available from hospitalization and subsequently after discharge, it will also be possible, with the help of increasingly powerful hardware, to build assistance strategies useful in clinical practice.} }
@article{WOS:000936226800001, title = {A Machine Learning-Combined Flexible Sensor for Tactile Detection and Voice Recognition}, journal = {ACS APPLIED MATERIALS \\& INTERFACES}, volume = {15}, pages = {12551-12559}, year = {2023}, issn = {1944-8244}, doi = {10.1021/acsami.2c22287}, author = {Xie, Jiawang and Zhao, Yuzhi and Zhu, Dezhi and Li, Jiaqun and Qiao, Ming and He, Guangzhi and Deng, Shengfa and Yan, Jianfeng}, abstract = {Intelligent sensors have attracted substantial attention for various applications, including wearable electronics, artificial intelligence, healthcare monitoring, and human-machine interactions. However, there still remains a critical challenge in developing a multifunctional sensing system for complex signal detection and analysis in practical applications. Here, we develop a machine learning-combined flexible sensor for real-time tactile sensing and voice recognition through laser-induced graphitization. The intelligent sensor with a triboelectric layer can convert local pressure to an electrical signal through a contact electrification effect without external bias, which has a characteristic response behavior when exposed to various mechanical stimuli. With the special patterning design, a smart human-machine interaction controlling system composed of a digital arrayed touch panel is constructed to control electronic devices. Based on machine learning, the real-time monitoring and recognition of the changes of voice are achieved with high accuracy. The machine learning-empowered flexible sensor provides a promising platform for the development of flexible tactile sensing, real-time health detection, human-machine interaction, and intelligent wearable devices.} }
@article{WOS:001174091000001, title = {Revolutionizing physics: a comprehensive survey of machine learning applications}, journal = {FRONTIERS IN PHYSICS}, volume = {12}, year = {2024}, doi = {10.3389/fphy.2024.1322162}, author = {Suresh, Rahul and Bishnoi, Hardik and Kuklin, Artem V. and Parikh, Atharva and Molokeev, Maxim and Harinarayanan, R. and Gharat, Sarvesh and Hiba, P.}, abstract = {In the context of the 21st century and the fourth industrial revolution, the substantial proliferation of data has established it as a valuable resource, fostering enhanced computational capabilities across scientific disciplines, including physics. The integration of Machine Learning stands as a prominent solution to unravel the intricacies inherent to scientific data. While diverse machine learning algorithms find utility in various branches of physics, there exists a need for a systematic framework for the application of Machine Learning to the field. This review offers a comprehensive exploration of the fundamental principles and algorithms of Machine Learning, with a focus on their implementation within distinct domains of physics. The review delves into the contemporary trends of Machine Learning application in condensed matter physics, biophysics, astrophysics, material science, and addresses emerging challenges. The potential for Machine Learning to revolutionize the comprehension of intricate physical phenomena is underscored. Nevertheless, persisting challenges in the form of more efficient and precise algorithm development are acknowledged within this review.} }
@article{WOS:001040009600001, title = {No more free lunch: The increasing popularity of machine learning and financial market efficiency}, journal = {ECONOMIC AND POLITICAL STUDIES-EPS}, volume = {12}, pages = {34-57}, year = {2024}, issn = {2095-4816}, doi = {10.1080/20954816.2023.2230622}, author = {Feng, Jian and Liu, Xin}, abstract = {In this paper, we show that the increasing popularity of machine learning improves market efficiency. By analysing the performance of a set of popular machine learning-based investment strategies, we find that profits from these strategies experience significant declines since the wide adoption of machine learning techniques, especially for profits based on the more preferred method of neural networks. These declines mainly come from long legs. Using the `machine learning' Google search index as a proxy for machine learning-based trading intensity, we find that returns from the neural networks-based long-short and long-only strategies are weaker following high levels of machine learning intensity, while no relation is found between machine learning intensity and the short-only neural networks-based strategy.} }
@article{WOS:000523484900001, title = {Machine learning and clinical epigenetics: a review of challenges for diagnosis and classification}, journal = {CLINICAL EPIGENETICS}, volume = {12}, year = {2020}, issn = {1868-7075}, doi = {10.1186/s13148-020-00842-4}, author = {Rauschert, S. and Raubenheimer, K. and Melton, P. E. and Huang, R. C.}, abstract = {Background Machine learning is a sub-field of artificial intelligence, which utilises large data sets to make predictions for future events. Although most algorithms used in machine learning were developed as far back as the 1950s, the advent of big data in combination with dramatically increased computing power has spurred renewed interest in this technology over the last two decades. Main body Within the medical field, machine learning is promising in the development of assistive clinical tools for detection of e.g. cancers and prediction of disease. Recent advances in deep learning technologies, a sub-discipline of machine learning that requires less user input but more data and processing power, has provided even greater promise in assisting physicians to achieve accurate diagnoses. Within the fields of genetics and its sub-field epigenetics, both prime examples of complex data, machine learning methods are on the rise, as the field of personalised medicine is aiming for treatment of the individual based on their genetic and epigenetic profiles. Conclusion We now have an ever-growing number of reported epigenetic alterations in disease, and this offers a chance to increase sensitivity and specificity of future diagnostics and therapies. Currently, there are limited studies using machine learning applied to epigenetics. They pertain to a wide variety of disease states and have used mostly supervised machine learning methods.} }
@article{WOS:001389793200001, title = {Machine learning of weighted superposition attraction algorithm for optimization diesel engine performance and emission fueled with butanol-diesel biofuel}, journal = {AIN SHAMS ENGINEERING JOURNAL}, volume = {15}, year = {2024}, issn = {2090-4479}, doi = {10.1016/j.asej.2024.103126}, author = {Veza, Ibham and Karaoglan, Aslan Deniz and Akpinar, Sener and Spraggon, Martin and Idris, Muhammad}, abstract = {Machine learning (ML) is a subset of artificial intelligence (AI) and computer science that employs data and algorithms and mimics human learning to self-enhance its accuracy. In biofuel research, butanol is widely recognized as a prospective alternative biofuel. Butanol addition in diesel or combustion engine has been more and more studied recently. Gaining a comprehensive comprehension of butanol performance and emission characteristics using machine learning approach is an essential milestone in investigating alcohol-based biofuel addition in diesel engines. However, few studies investigated butanol effect on diesel engine emissions using machine learning for optimization. A novel optimization study is needed. This work aims to investigate the newly developed and efficient machine learning, weighted superposition attraction (WSA) algorithm, to optimize the emission and performance of diesel engines fuelled with butanol-diesel biofuel. Mathematical modeling between the factors (butanol (vol.\\%) and BMEP (bar)) and the responses (BTE (\\%), BSFC (g/kWh), Exhaust Temperature Texh (oC), NOx (g/kWh), CO (g/kWh), HC (g/kWh), and Smoke Opacity (\\%)) are governed using regression modeling. The optimized and best factor levels are determined employing the machine learning of WSA Algorithm. Confirmations are carried out. Optimization results indicate that the BTE is maximized, and the remainder of the responses are minimized.} }
@article{WOS:000701254700001, title = {Autism Spectrum Disorder Studies Using fMRI Data and Machine Learning: A Review}, journal = {FRONTIERS IN NEUROSCIENCE}, volume = {15}, year = {2021}, doi = {10.3389/fnins.2021.697870}, author = {Liu, Meijie and Li, Baojuan and Hu, Dewen}, abstract = {Machine learning methods have been frequently applied in the field of cognitive neuroscience in the last decade. A great deal of attention has been attracted to introduce machine learning methods to study the autism spectrum disorder (ASD) in order to find out its neurophysiological underpinnings. In this paper, we presented a comprehensive review about the previous studies since 2011, which applied machine learning methods to analyze the functional magnetic resonance imaging (fMRI) data of autistic individuals and the typical controls (TCs). The all-round process was covered, including feature construction from raw fMRI data, feature selection methods, machine learning methods, factors for high classification accuracy, and critical conclusions. Applying different machine learning methods and fMRI data acquired from different sites, classification accuracies were obtained ranging from 48.3\\% up to 97\\%, and informative brain regions and networks were located. Through thorough analysis, high classification accuracies were found to usually occur in the studies which involved task-based fMRI data, single dataset for some selection principle, effective feature selection methods, or advanced machine learning methods. Advanced deep learning together with the multi-site Autism Brain Imaging Data Exchange (ABIDE) dataset became research trends especially in the recent 4 years. In the future, advanced feature selection and machine learning methods combined with multi-site dataset or easily operated task-based fMRI data may appear to have the potentiality to serve as a promising diagnostic tool for ASD.} }
@article{WOS:001336722300165, title = {Quantum Machine Learning for Additive Manufacturing Process Monitoring}, journal = {MANUFACTURING LETTERS}, volume = {41}, pages = {1415-1422}, year = {2024}, issn = {2213-8463}, doi = {10.1016/j.mfglet.2024.09.168}, author = {Choi, Eunsik and Sul, Jinhwan and Kim, Jungin E. and Hong, Sungjin and Gonzalez, Beatriz Izquierdo and Cembellin, Pablo and Wang, Yan}, abstract = {Machine learning is useful for analyzing and monitoring complex manufacturing processes. However, it has several limitations including the curse-of-dimensionality and lack of training data. In this paper, we propose a quantum machine learning strategy to tackle these challenges. Quantum support vector machine is applied to identify the states of machines in fused filament fabrication process based on acoustic emission data. Quantum convolutional neural network is used to detect spatters in laser powder bed fusion process based on coaxial optical images. Our results show that quantum machine learning can achieve the similar accuracy levels of predictions by classical machine learning counterparts, but with exponentially fewer parameters. (c) 2024 The Authors. Published by ELSEVIER Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)} }
@article{WOS:001301816000001, title = {An interpretable machine learning-based pitting corrosion depth prediction model for steel drinking water pipelines}, journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION}, volume = {190}, pages = {571-585}, year = {2024}, issn = {0957-5820}, doi = {10.1016/j.psep.2024.08.038}, author = {Kim, Taehyeon and Kim, Kibum and Hyung, Jinseok and Park, Haekeum and Oh, Yoojin and Koo, Jayong}, abstract = {Steel pipes are a crucial element of the water supply system and are necessary for safely delivering large quantities of water from purification plants to consumers. Corrosion is a significant factor that deteriorates the interior and exterior of the steel pipes. Although the effectiveness of machine learning has been demonstrated in various fields, machine learning has rarely been used to identify corrosion mechanisms in buried steel pipes. A hybrid machine-learning-based corrosion depth prediction model was developed by integrating a corrosion depth trend prediction model based only on elapsed years with machine-learning algorithms. Shapley additive explanation (SHAP) was used to analyze the hybrid machine-learning-based corrosion depth prediction models, revealing corrosion mechanisms and explaining the interactions among influencing factors through global and local interpretations. The SHAP local interpretation showed that the hybrid machine-learning-based corrosion depth prediction models can effectively capture the interrelationship between soil and water corrosiveness.} }
@article{WOS:000382418300016, title = {Machine learning for medical images analysis}, journal = {MEDICAL IMAGE ANALYSIS}, volume = {33}, pages = {91-93}, year = {2016}, issn = {1361-8415}, doi = {10.1016/j.media.2016.06.002}, author = {Criminisi, A.}, abstract = {This article discusses the application of machine learning for the analysis of medical images. Specifically: (i) We show how a special type of learning models can be thought of as automatically optimized, hierarchically-structured, rule-based algorithms, and (ii) We discuss how the issue of collecting large labelled datasets applies to both conventional algorithms as well as machine learning techniques. The size of the training database is a function of model complexity rather than a characteristic of machine learning methods. Crown Copyright (C) 2016 Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000485090400075, title = {Using Artificial Intelligence To Forecast Water Oxidation Catalysts}, journal = {ACS CATALYSIS}, volume = {9}, pages = {8383-8387}, year = {2019}, issn = {2155-5435}, doi = {10.1021/acscatal.9b01985}, author = {Palkovits, Regina and Palkovits, Stefan}, abstract = {Artificial intelligence and various types of machine learning are of increasing interest not only in the natural sciences but also in a wide range of applied and engineering sciences. In this study, we rethink the view on combinatorial heterogeneous catalysis and combine machine learning methods with combinatorial approaches in electrocatalysis. Several machine learning methods were used to forecast water oxidation catalysts on the basis of literature published data sets and data from our own work. The machine learning models exhibit a decent prediction precision based on the data sets available and confirm that even simple models are suitable for good forecasts.} }
@article{WOS:000793810100003, title = {Hyperspectral Anomaly Detection Based on Machine Learning: An Overview}, journal = {IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING}, volume = {15}, pages = {3351-3364}, year = {2022}, issn = {1939-1404}, doi = {10.1109/JSTARS.2022.3167830}, author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei}, abstract = {Hyperspectral anomaly detection (HAD) is an important hyperspectral image application. HAD can find pixels with anomalous spectral signatures compared with their neighbor background without any prior information. While most of the existed researches are related to statistic-based and distance-based techniques, by summarizing the background samples with certain models, and then, finding the very few outliers by various distance metrics, this review focuses on the HAD based on machine learning methods, which have witnessed remarkable progress in the recent years. In particular, these studies can generally be grouped into the traditional machine learning and deep-learning-based methods. Several representative HAD methods, including both traditional machine and deep-learning-based methods, are then conducted on four real HSIs in the experiments. Finally, conclusions regarding HAD are summarized, and prospects and future development direction are discussed.} }
@article{WOS:000282915500002, title = {The security of machine learning}, journal = {MACHINE LEARNING}, volume = {81}, pages = {121-148}, year = {2010}, issn = {0885-6125}, doi = {10.1007/s10994-010-5188-5}, author = {Barreno, Marco and Nelson, Blaine and Joseph, Anthony D. and Tygar, J. D.}, abstract = {Machine learning's ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.} }
@article{WOS:000660860700001, title = {Quantum machine learning and quantum biomimetics: A perspective}, journal = {MACHINE LEARNING-SCIENCE AND TECHNOLOGY}, volume = {1}, year = {2020}, doi = {10.1088/2632-2153/ab9803}, author = {Lamata, Lucas}, abstract = {Quantum machine learning has emerged as an exciting and promising paradigm inside quantum technologies. It may permit, on the one hand, to carry out more efficient machine learning calculations by means of quantum devices, while, on the other hand, to employ machine learning techniques to better control quantum systems. Inside quantum machine learning, quantum reinforcement learning aims at developing `intelligent' quantum agents that may interact with the outer world and adapt to it, with the strategy of achieving some final goal. Another paradigm inside quantum machine learning is that of quantum autoencoders, which may allow one for employing fewer resources in a quantum device via a training process. Moreover, the field of quantum biomimetics aims at establishing analogies between biological and quantum systems, to look for previously inadvertent connections that may enable useful applications. Two recent examples are the concepts of quantum artificial life, as well as of quantum memristors. In this Perspective, we give an overview of these topics, describing the related research carried out by the scientific community.} }
@article{WOS:000674276800001, title = {Securing Machine Learning in the Cloud: A Systematic Review of Cloud Machine Learning Security}, journal = {FRONTIERS IN BIG DATA}, volume = {3}, year = {2020}, doi = {10.3389/fdata.2020.587139}, author = {Qayyum, Adnan and Ijaz, Aneeqa and Usama, Muhammad and Iqbal, Waleed and Qadir, Junaid and Elkhatib, Yehia and Al-Fuqaha, Ala}, abstract = {With the advances in machine learning (ML) and deep learning (DL) techniques, and the potency of cloud computing in offering services efficiently and cost-effectively, Machine Learning as a Service (MLaaS) cloud platforms have become popular. In addition, there is increasing adoption of third-party cloud services for outsourcing training of DL models, which requires substantial costly computational resources (e.g., high-performance graphics processing units (GPUs)). Such widespread usage of cloud-hosted ML/DL services opens a wide range of attack surfaces for adversaries to exploit the ML/DL system to achieve malicious goals. In this article, we conduct a systematic evaluation of literature of cloud-hosted ML/DL models along both the important dimensions-attacks and defenses-related to their security. Our systematic review identified a total of 31 related articles out of which 19 focused on attack, six focused on defense, and six focused on both attack and defense. Our evaluation reveals that there is an increasing interest from the research community on the perspective of attacking and defending different attacks on Machine Learning as a Service platforms. In addition, we identify the limitations and pitfalls of the analyzed articles and highlight open research issues that require further investigation.} }
@article{WOS:000416161200002, title = {Ensemble of Efficient Minimal Learning Machines for Classification and Regression}, journal = {NEURAL PROCESSING LETTERS}, volume = {46}, pages = {751-766}, year = {2017}, issn = {1370-4621}, doi = {10.1007/s11063-017-9587-5}, author = {Mesquita, Diego P. P. and Gomes, Joao P. P. and Souza Junior, Amauri H.}, abstract = {Minimal Learning Machine (MLM) is a recently proposed supervised learning algorithm with performance comparable to most state-of-the-art machine learning methods. In this work, we propose ensemble methods for classification and regression using MLMs. The goal of ensemble strategies is to produce more robust and accurate models when compared to a single classifier or regression model. Despite its successful application, MLM employs a computationally intensive optimization problem as part of its test procedure (out-of-sample data estimation). This becomes even more noticeable in the context of ensemble learning, where multiple models are used. Aiming to provide fast alternatives to the standard MLM, we also propose the Nearest Neighbor Minimal Learning Machine and the Cubic Equation Minimal Learning Machine to cope with classification and single-output regression problems, respectively. The experimental assessment conducted on real-world datasets reports that ensemble of fast MLMs perform comparably or superiorly to reference machine learning algorithms.} }
@article{WOS:000608126900013, title = {Prediction Machines: Applied Machine Learning for Therapeutic Protein Design and Development}, journal = {JOURNAL OF PHARMACEUTICAL SCIENCES}, volume = {110}, pages = {665-681}, year = {2021}, issn = {0022-3549}, doi = {10.1016/j.xphs.2020.11.034}, author = {Kamerzell, Tim J. and Middaugh, C. Russell}, abstract = {The rapid growth in technological advances and quantity of scientific data over the past decade has led to several challenges including data storage and analysis. Accurate models of complex datasets were previously difficult to develop and interpret. However, improvements in machine learning algorithms have since enabled unparalleled classification and prediction capabilities. The application of machine learning can be seen throughout diverse industries due to their ease of use and interpretability. In this review, we describe popular machine learning algorithms and highlight their application in pharmaceutical protein development. Machine learning models have now been applied to better understand the nonlinear concentration dependent viscosity of protein solutions, predict protein oxidation and deamidation rates, classify sub-visible particles and compare the physical stability of proteins. We also applied several machine learning algorithms using previously published data and describe models with improved predictions and classification. The authors hope that this review can be used as a resource to others and encourage continued application of machine learning algorithms to problems in pharmaceutical protein development.} }
@article{WOS:000628311200001, title = {Machine Learning and Novel Biomarkers for the Diagnosis of Alzheimer's Disease}, journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}, volume = {22}, year = {2021}, issn = {1661-6596}, doi = {10.3390/ijms22052761}, author = {Chang, Chun-Hung and Lin, Chieh-Hsin and Lane, Hsien-Yuan}, abstract = {Background: Alzheimer's disease (AD) is a complex and severe neurodegenerative disease that still lacks effective methods of diagnosis. The current diagnostic methods of AD rely on cognitive tests, imaging techniques and cerebrospinal fluid (CSF) levels of amyloid-beta 1-42 (A beta 42), total tau protein and hyperphosphorylated tau (p-tau). However, the available methods are expensive and relatively invasive. Artificial intelligence techniques like machine learning tools have being increasingly used in precision diagnosis. Methods: We conducted a meta-analysis to investigate the machine learning and novel biomarkers for the diagnosis of AD. Methods: We searched PubMed, the Cochrane Central Register of Controlled Trials, and the Cochrane Database of Systematic Reviews for reviews and trials that investigated the machine learning and novel biomarkers in diagnosis of AD. Results: In additional to A beta and tau-related biomarkers, biomarkers according to other mechanisms of AD pathology have been investigated. Neuronal injury biomarker includes neurofiliament light (NFL). Biomarkers about synaptic dysfunction and/or loss includes neurogranin, BACE1, synaptotagmin, SNAP-25, GAP-43, synaptophysin. Biomarkers about neuroinflammation includes sTREM2, and YKL-40. Besides, d-glutamate is one of coagonists at the NMDARs. Several machine learning algorithms including support vector machine, logistic regression, random forest, and naive Bayes) to build an optimal predictive model to distinguish patients with AD from healthy controls. Conclusions: Our results revealed machine learning with novel biomarkers and multiple variables may increase the sensitivity and specificity in diagnosis of AD. Rapid and cost-effective HPLC for biomarkers and machine learning algorithms may assist physicians in diagnosing AD in outpatient clinics.} }
@article{WOS:000485885700001, title = {MACHINE LEARNING METHODS FOR SYSTEMIC RISK ANALYSIS IN FINANCIAL SECTORS}, journal = {TECHNOLOGICAL AND ECONOMIC DEVELOPMENT OF ECONOMY}, volume = {25}, pages = {716-742}, year = {2019}, issn = {2029-4913}, doi = {10.3846/tede.2019.8740}, author = {Kou, Gang and Chao, Xiangrui and Peng, Yi and Alsaadi, Fawaz E. and Herrera-Viedma, Enrique}, abstract = {Financial systemic risk is an important issue in economics and financial systems. Trying to detect and respond to systemic risk with growing amounts of data produced in financial markets and systems, a lot of researchers have increasingly employed machine learning methods. Machine learning methods study the mechanisms of outbreak and contagion of systemic risk in the financial network and improve the current regulation of the financial market and industry. In this paper, we survey existing researches and methodologies on assessment and measurement of financial systemic risk combined with machine learning technologies, including big data analysis, network analysis and sentiment analysis, etc. In addition, we identify future challenges, and suggest further research topics. The main purpose of this paper is to introduce current researches on financial systemic risk with machine learning methods and to propose directions for future work.} }
@article{WOS:000705637500013, title = {Machine learning models for forecasting power electricity consumption using a high dimensional dataset}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {187}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2021.115917}, author = {Albuquerque, Pedro C. and Cajueiro, Daniel O. and Rossi, Marina D. C.}, abstract = {We use regularized machine learning models to forecast Brazilian power electricity consumption for short and medium terms. We compare our models to benchmark specifications such as Random Walk and Autoregressive Integrated Moving Average. Our results show that machine learning methods, especially Random Forest and Lasso Lars, give more accurate forecasts for all horizons. Random Forest and Lasso Lars managed to keep up with the trend and the seasonality for various time horizons. The gain in predicting PEC using machine learning models relative to the benchmarks is considerably higher for the very short-term. Machine learning variable selection further shows that lagged consumption values are extremely important for very short-term forecasting due to the series high autocorrelation. Other variables such as weather and calendar variables are important for longer time horizons.} }
@article{WOS:000752230700001, title = {Machine learning for brain signal analysis}, journal = {INTERNATIONAL JOURNAL OF BIOLOGY AND CHEMISTRY}, volume = {14}, pages = {4-11}, year = {2021}, issn = {2218-7979}, doi = {10.26577/ijbch.2021.v14.i2.01}, author = {Makhmet, A. S. and Sharaev, M. G. and Dyusembaev, A. E. and Kustubayeva, A. M.}, abstract = {Machine learning (ML) is an effective tool for analysing signals from the human brain. Machine Learning techniques provide new insight into the understanding of brain function in healthy subjects and patients with neurological and mental disorders. Here we introduce the application of machine learning to resonance imaging (fMRI) and Electroencephalography (EEG). The article provides a brief overview of the theoretical concept of machine learning and its types: supervised, unsupervised and reinforcement learning. The potential of machine learning applications in pathology is discussed. Differences between EEG and fMRI methods regarding machine learning application and an overview of the techniques employed in different research studies are reviewed. The new machine learning methods invented for analysis of brain signals in the resting ate and during the performance of the different cognitive tasks would be useful and worth considering in other domains, not limited to medicine.} }
@article{WOS:000209236900011, title = {Gradient boosting machines, a tutorial}, journal = {FRONTIERS IN NEUROROBOTICS}, volume = {7}, year = {2013}, issn = {1662-5218}, doi = {10.3389/fnbot.2013.00021}, author = {Natekin, Alexey and Knoll, Alois}, abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.} }
@article{WOS:000664641500020, title = {COVID-19 detection using federated machine learning}, journal = {PLOS ONE}, volume = {16}, year = {2021}, issn = {1932-6203}, doi = {10.1371/journal.pone.0252573}, author = {Salam, Mustafa Abdul and Taha, Sanaa and Ramadan, Mohamed}, abstract = {The current COVID-19 pandemic threatens human life, health, and productivity. AI plays an essential role in COVID-19 case classification as we can apply machine learning models on COVID-19 case data to predict infectious cases and recovery rates using chest x-ray. Accessing patient's private data violates patient privacy and traditional machine learning model requires accessing or transferring whole data to train the model. In recent years, there has been increasing interest in federated machine learning, as it provides an effective solution for data privacy, centralized computation, and high computation power. In this paper, we studied the efficacy of federated learning versus traditional learning by developing two machine learning models (a federated learning model and a traditional machine learning model)using Keras and TensorFlow federated, we used a descriptive dataset and chest x-ray (CXR) images from COVID-19 patients. During the model training stage, we tried to identify which factors affect model prediction accuracy and loss like activation function, model optimizer, learning rate, number of rounds, and data Size, we kept recording and plotting the model loss and prediction accuracy per each training round, to identify which factors affect the model performance, and we found that softmax activation function and SGD optimizer give better prediction accuracy and loss, changing the number of rounds and learning rate has slightly effect on model prediction accuracy and prediction loss but increasing the data size did not have any effect on model prediction accuracy and prediction loss. finally, we build a comparison between the proposed models' loss, accuracy, and performance speed, the results demonstrate that the federated machine learning model has a better prediction accuracy and loss but higher performance time than the traditional machine learning model.} }
@article{WOS:000432883500001, title = {A machine learning framework to forecast wave conditions}, journal = {COASTAL ENGINEERING}, volume = {137}, pages = {1-10}, year = {2018}, issn = {0378-3839}, doi = {10.1016/j.coastaleng.2018.03.004}, author = {James, Scott C. and Zhang, Yushan and O'Donncha, Fearghal}, abstract = {A machine learning framework is developed to estimate ocean-wave conditions. By supervised training of machine learning models on many thousands of iterations of a physics-based wave model, accurate representations of significant wave heights and period can be used to predict ocean conditions. A model of Monterey Bay was used as the example test site; it was forced by measured wave conditions, ocean-current nowcasts, and reported winds. These input data along with model outputs of spatially variable wave heights and characteristic period were aggregated into supervised learning training and test data sets, which were supplied to machine learning models. These machine learning models replicated wave heights from the physics-based model with a root-mean-squared error of 9 cm and correctly identify over 90\\% of the characteristic periods for the test-data sets. Impressively, transforming model inputs to outputs through matrix operations requires only a fraction (< 1/1, 000 th) of the computation time compared to forecasting with the physics-based model.} }
@article{WOS:000829738400001, title = {Materials Discovery With Machine Learning and Knowledge Discovery}, journal = {FRONTIERS IN CHEMISTRY}, volume = {10}, year = {2022}, issn = {2296-2646}, doi = {10.3389/fchem.2022.930369}, author = {Oliveira Jr, Osvaldo N. and Oliveira, Maria Cristina F.}, abstract = {Machine learning and other artificial intelligence methods are gaining increasing prominence in chemistry and materials sciences, especially for materials design and discovery, and in data analysis of results generated by sensors and biosensors. In this paper, we present a perspective on this current use of machine learning, and discuss the prospects of the future impact of extending the use of machine learning to encompass knowledge discovery as an essential step towards a new paradigm of machine-generated knowledge. The reasons why results so far have been limited are given with a discussion of the limitations of machine learning in tasks requiring interpretation. Also discussed is the need to adapt the training of students and scientists in chemistry and materials sciences, to better explore the potential of artificial intelligence capabilities.} }
@article{WOS:000445712400274, title = {Machine Learning in Agriculture: A Review}, journal = {SENSORS}, volume = {18}, year = {2018}, doi = {10.3390/s18082674}, author = {Liakos, Konstantinos G. and Busato, Patrizia and Moshou, Dimitrios and Pearson, Simon and Bochtis, Dionysis}, abstract = {Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.} }
@article{WOS:000841672300001, title = {Machine Learning Approaches to TCR Repertoire Analysis}, journal = {FRONTIERS IN IMMUNOLOGY}, volume = {13}, year = {2022}, issn = {1664-3224}, doi = {10.3389/fimmu.2022.858057}, author = {Katayama, Yotaro and Yokota, Ryo and Akiyama, Taishin and Kobayashi, Tetsuya J.}, abstract = {Sparked by the development of genome sequencing technology, the quantity and quality of data handled in immunological research have been changing dramatically. Various data and database platforms are now driving the rapid progress of machine learning for immunological data analysis. Of various topics in immunology, T cell receptor repertoire analysis is one of the most important targets of machine learning for assessing the state and abnormalities of immune systems. In this paper, we review recent repertoire analysis methods based on machine learning and deep learning and discuss their prospects.} }
@article{WOS:000477891100005, title = {Machine learning in autistic spectrum disorder behavioral research: A review and ways forward}, journal = {INFORMATICS FOR HEALTH \\& SOCIAL CARE}, volume = {44}, pages = {278-297}, year = {2019}, issn = {1753-8157}, doi = {10.1080/17538157.2017.1399132}, author = {Thabtah, Fadi}, abstract = {Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.} }
@article{WOS:000450513100004, title = {eDoctor: machine learning and the future of medicine}, journal = {JOURNAL OF INTERNAL MEDICINE}, volume = {284}, pages = {603-619}, year = {2018}, issn = {0954-6820}, doi = {10.1111/joim.12822}, author = {Handelman, G. S. and Kok, H. K. and Chandra, R. V. and Razavi, A. H. and Lee, M. J. and Asadi, H.}, abstract = {Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer-aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine.} }
@article{WOS:000430994500003, title = {A robust extreme learning machine for modeling a small-scale turbojet engine}, journal = {APPLIED ENERGY}, volume = {218}, pages = {22-35}, year = {2018}, issn = {0306-2619}, doi = {10.1016/j.apenergy.2018.02.175}, author = {Zhao, Yong-Ping and Hu, Qian-Kun and Xu, Jian-Guo and Li, Bing and Huang, Gong and Pan, Ying-Ting}, abstract = {In this paper, a robust extreme learning machine is proposed. In comparison with the original extreme learning machine and the regularized extreme learning machine, this robust algorithm minimizes both the mean and variance of modeling errors in the objective function to overcome the bias-variance dilemma. As a result, its generalization performance and robustness are enhanced, and these merits are further proved theoretically. In addition, this proposed algorithm can keep the same computational efficiency as the original extreme learning machine and the regularized extreme learning machine. Then, several benchmark data sets are used to test the effectiveness and soundness of the proposed algorithm. Finally, it is employed to model a real small-scale turbojet engine. This engine is fit well. Especially, on the idle phase, where the signal-to-noise ratio is low and it is very hard to model, the proposed algorithm performs well and its robustness is sufficiently showcased. All in all, the proposed algorithm provides a candidate technique for modeling real systems.} }
@article{WOS:000448616200004, title = {Machine Learning in Compiler Optimization}, journal = {PROCEEDINGS OF THE IEEE}, volume = {106}, pages = {1879-1901}, year = {2018}, issn = {0018-9219}, doi = {10.1109/JPROC.2018.2817118}, author = {Wang, Zheng and O'Boyle, Michael}, abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.} }
@article{WOS:001239078200005, title = {Interpretable Machine Learning for Discovery: Statistical Challenges and Opportunities}, journal = {ANNUAL REVIEW OF STATISTICS AND ITS APPLICATION}, volume = {11}, pages = {97-121}, year = {2024}, issn = {2326-8298}, doi = {10.1146/annurev-statistics-040120-030919}, author = {Allen, Genevera I. and Gan, Luqin and Zheng, Lili}, abstract = {New technologies have led to vast troves of large and complex data sets across many scientific domains and industries. People routinely use machine learning techniques not only to process, visualize, and make predictions from these big data, but also to make data-driven discoveries. These discoveries are often made using interpretable machine learning, or machine learning models and techniques that yield human-understandable insights. In this article, we discuss and review the field of interpretable machine learning, focusing especially on the techniques, as they are often employed to generate new knowledge or make discoveries from large data sets.We outline the types of discoveries that can be made using interpretable machine learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science.We discuss validation both from a practical perspective, reviewing approaches based on data-splitting and stability, as well as from a theoretical perspective, reviewing statistical results on model selection consistency and uncertainty quantification via statistical inference. Finally, we conclude by highlighting open challenges in using interpretable machine learning techniques to make discoveries, including gaps between theory and practice for validating data-driven discoveries.} }
@article{WOS:001486910100001, title = {Quantum machine learning based wind turbine condition monitoring: State of the art and future prospects}, journal = {ENERGY CONVERSION AND MANAGEMENT}, volume = {332}, year = {2025}, issn = {0196-8904}, doi = {10.1016/j.enconman.2025.119694}, author = {Zhang, Zhefeng and Wu, Yueqi and Ma, Xiandong}, abstract = {Wind energy, as a popular renewable resource, has gained extensive development and application in recent decades. Effective condition monitoring and fault diagnosis are crucial for ensuring the reliable operation of wind turbines. While conventional machine learning methods have been widely used in wind turbine condition monitoring, these approaches often face challenges such as complex feature extraction, limited model generalization, and high computational costs when dealing with large-scale, high-dimensional, and complex datasets. The emergence of quantum computing has opened up a new paradigm of machine learning algorithms. Quantum machine learning combines the advantages of quantum computing and machine learning, with the potential to surpass classical computational capabilities. This paper firstly reviews applications and limitations of the state-of-the-art machine learning-based condition monitoring techniques for wind turbines. It then reviews the fundamentals of quantum computing, quantum machine learning algorithms and their applications, covering quantum-based feature extraction, classification and regression for fault detection and the use of quantum neural networks for predictive maintenance. Through comparison, it is observed that quantum machine learning methods, even without extensive optimization, can achieve accuracy levels comparable to those of optimized conventional machine learning approaches. The challenges of applying quantum machine learning are also addressed, along with the future research and development prospects. The objective of this review is to fill a gap in the published literature by providing a new paradigm approach for wind turbine condition monitoring. By promoting quantum machine learning in this field, the reliability and efficiency of wind power systems are ultimately sought to be enhanced.} }
@article{WOS:000472796800001, title = {Machine Learning Methods for Ranking}, journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING}, volume = {29}, pages = {729-761}, year = {2019}, issn = {0218-1940}, doi = {10.1142/S021819401930001X}, author = {Rahangdale, Ashwini and Raut, Shital}, abstract = {Learning-to-rank is one of the learning frameworks in machine learning and it aims to organize the objects in a particular order according to their preference, relevance or ranking. In this paper, we give a comprehensive survey for learning-to-rank. First, we discuss the different approaches along with different machine learning methods such as regression, SVM, neural network-based, evolutionary, boosting method. In order to compare different approaches: we discuss the characteristics of each approach. In addition to that, learning-to-rank algorithms combine with other machine learning paradigms such as semi-supervised learning, active learning, reinforcement learning and deep learning. The learning-to-rank models employ with parallel or big data analytics to review computational and storage advantage. Many real-time applications use learning-to-rank for preference learning. In regard to this, we introduce some representative works. Finally, we highlighted future directions to investigate learning-to-rank methods.} }
@article{WOS:001059216400001, title = {An integrated machine learning framework with uncertainty quantification for three-dimensional lithological modeling from multi-source geophysical data and drilling data}, journal = {ENGINEERING GEOLOGY}, volume = {324}, year = {2023}, issn = {0013-7952}, doi = {10.1016/j.enggeo.2023.107255}, author = {Zhang, Zhiqiang and Wang, Gongwen and Carranza, Emmanuel John M. and Liu, Chong and Li, Junjian and Fu, Chao and Liu, Xinxing and Chen, Chao and Fan, Junjie and Dong, Yulong}, abstract = {Nowadays, it is commonplace for geological surveys to integrate multi-source geophysical data and drilling data in order to construct three-dimensional (3D) lithological models. In this context, manual translation of complex geophysical data into parameters used for 3D lithological modeling is challenging. Machine learning has recently shown great potential in 3D lithological modeling. However, the performance of machine learning algorithm is influenced by the imbalance in number of categories of lithological samples. In addition, the uncertainty associated with 3D lithological modeling by machine learning has rarely been quantified. This study presents a novel integrated machine learning framework to address the imbalance issue and to quantify uncertainty in 3D lithological modeling. As its novelty, our integrated machine learning framework can subdivide total uncertainty into aleatoric and epistemic uncertainties in the 3D lithological modeling procedure by stochastic gradient Langevin boosting. Another innovation of this study is the use of Bayesian hyperparameter optimization for automatic tuning of hyperparameters of the integrated machine learning framework. The 3D lithological and uncertainty modeling case study in the Jiaojia-Sanshandao gold district of China demonstrated the superiority of our proposed integrated machine learning framework. The proposed framework has great potential in integrating multi-source geophysical and drilling data for 3D lithological and uncertainty modeling in engineering geology.} }
@article{WOS:000888574800002, title = {Tiny Machine Learning for Resource-Constrained Microcontrollers}, journal = {JOURNAL OF SENSORS}, volume = {2022}, year = {2022}, issn = {1687-725X}, doi = {10.1155/2022/7437023}, author = {Immonen, Riku and Hamalainen, Timo}, abstract = {We use 250 billion microcontrollers daily in electronic devices that are capable of running machine learning models inside them. Unfortunately, most of these microcontrollers are highly constrained in terms of computational resources, such as memory usage or clock speed. These are exactly the same resources that play a key role in teaching and running a machine learning model with a basic computer. However, in a microcontroller environment, constrained resources make a critical difference. Therefore, a new paradigm known as tiny machine learning had to be created to meet the constrained requirements of the embedded devices. In this review, we discuss the resource optimization challenges of tiny machine learning and different methods, such as quantization, pruning, and clustering, that can be used to overcome these resource difficulties. Furthermore, we summarize the present state of tiny machine learning frameworks, libraries, development environments, and tools. The benchmarking of tiny machine learning devices is another thing to be concerned about; these same constraints of the microcontrollers and diversity of hardware and software turn to benchmark challenges that must be resolved before it is possible to measure performance differences reliably between embedded devices. We also discuss emerging techniques and approaches to boost and expand the tiny machine learning process and improve data privacy and security. In the end, we form a conclusion about tiny machine learning and its future development.} }
@article{WOS:000555753600001, title = {The Number of Confirmed Cases of Covid-19 by using Machine Learning: Methods and Challenges}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {28}, pages = {2645-2653}, year = {2021}, issn = {1134-3060}, doi = {10.1007/s11831-020-09472-8}, author = {Ahmad, Amir and Garhwal, Sunita and Ray, Santosh Kumar and Kumar, Gagan and Malebary, Sharaf Jameel and Barukab, Omar Mohammed}, abstract = {Covid-19 is one of the biggest health challenges that the world has ever faced. Public health policy makers need the reliable prediction of the confirmed cases in future to plan medical facilities. Machine learning methods learn from the historical data and make predictions about the events. Machine learning methods have been used to predict the number of confirmed cases of Covid-19. In this paper, we present a detailed review of these research papers. We present a taxonomy that groups them in four categories. We further present the challenges in this field. We provide suggestions to the machine learning practitioners to improve the performance of machine learning methods for the prediction of confirmed cases of Covid-19.} }
@article{WOS:000678361100002, title = {Pairing conceptual modeling with machine learning}, journal = {DATA \\& KNOWLEDGE ENGINEERING}, volume = {134}, year = {2021}, issn = {0169-023X}, doi = {10.1016/j.datak.2021.101909}, author = {Maass, Wolfgang and Storey, Veda C.}, abstract = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.} }
@article{WOS:001079720800001, title = {The rise of the machines: A state-of-the-art technical review on process modelling and machine learning within hydrogen production with carbon capture}, journal = {GAS SCIENCE AND ENGINEERING}, volume = {118}, year = {2023}, issn = {2949-9097}, doi = {10.1016/j.jgsce.2023.205104}, author = {Davies, William George and Babamohammadi, Shervan and Yang, Yang and Soltani, Salman Masoudi}, abstract = {This study aims to present a compendious yet technical scrutiny of the current trends in process modelling as well as the implementation of machine learning within combined hydrogen production and carbon capture (i.e. blue hydrogen). The paper is intended to accurately portray the role that machine learning is anticipated to play within research and development in blue hydrogen production in the forthcoming years. This covers the implementation of machine learning at both material and process development levels. The paper provides a concise overview of the current trends in blue hydrogen production, as well as an intro to machine learning and process modelling within the same context. We have reinforced our paper by first summarising a brief description of the key ``tools'' used in machine learning and process modelling, before painstakingly examining the imple-mentation of these techniques in blue hydrogen production and the less-discovered merits and de-merits.Ultimately, the paper depicts a clear picture of the advancements in machine learning and the major role it is expected to play in accelerating research and development in blue hydrogen production on both material and process development fronts. The paper strives to shed some light on the key advantages that machine learning has to offer in blue hydrogen for future research work.} }
@article{WOS:000518864800005, title = {A Framework of Using Machine Learning Approaches for Short-Term Solar Power Forecasting}, journal = {JOURNAL OF ELECTRICAL ENGINEERING \\& TECHNOLOGY}, volume = {15}, pages = {561-569}, year = {2020}, issn = {1975-0102}, doi = {10.1007/s42835-020-00346-4}, author = {Munawar, Usman and Wang, Zhanle}, abstract = {Various machine learning approaches are widely applied for short-term solar power forecasting, which is highly demanded for renewable energy integration and power system planning. However, appropriate selection of machine learning models and data features is a significant challenge. In this study, a framework is developed to quantitatively evaluate various models and feature selection methods, and the best combination for short-term solar power forecasting is discovered. More specifically, the machine learning methods include the random forest, artificial neural network and extreme gradient boosting (XGBoost), and the feature selection techniques include the feature importance and principle component analysis (PCA). All possible combinations of these machine learning and feature selection methods are developed and evaluated for solar power forecasting. The best ensemble of machine learning methods and feature selection techniques is identified for solar power forecasting in Hawaii, US. Simulation results show that the XGBoost method with features selected by the PCA method outperforms the other approaches. In addition, the random forest and XGBoost models have rarely been used for short-term solar forecasting. This framework can be used to select appropriate machine learning approaches for short-term solar power forecasting and the simulation results can be used as a baseline for comparison.} }
@article{WOS:000982564700004, title = {Comparison of Machine Learning Based on Category Theory}, journal = {JOURNAL OF WEB ENGINEERING}, volume = {22}, pages = {41-54}, year = {2023}, issn = {1540-9589}, doi = {10.13052/jwe1540-9589.2213}, author = {Zhao, Heng and Chen, Yixing and Fu, Xianghua}, abstract = {In recent years, machine learning has been widely used in data analysis of network engineering. The increasing types of model and data enhance the complexity of machine learning. In this paper, we propose a mathematical structure based on category theory as a combination of machine learning that combines multiple theories of data mining. We aim to study machine learning from the perspective of classification theory. Category theory utilizes mathematical language to connect the various structures of machine learning. We implement the representation of machine learning with category theory. In the experimental section, slice categories and functors are introduced in detail to model the data preprocessing. We use functors to preprocess the benchmark dataset and evaluate the accuracy of nine machine learning models. A key contribution is the representation of slice categories. This study provides a structural perspective of machine learning and a general method for the combination of category theory and machine learning.} }
@article{WOS:000721705800032, title = {Documentation to facilitate communication between dataset creators and consumers}, journal = {COMMUNICATIONS OF THE ACM}, volume = {64}, pages = {86-92}, year = {2021}, issn = {0001-0782}, doi = {10.1145/3458723}, author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume, III, Hal and Crawford, Kate}, abstract = {DATA PLAYS A critical role in machine learning. Every machine learning model is trained and evaluated using data, quite often in the form of static datasets. The characteristics of these datasets fundamentally influence a model's behavior: a model is unlikely to perform well in the wild if its deployment context does not match its training or evaluation datasets, or if these datasets reflect unwanted societal biases. Mismatches like this can have especially severe consequences when machine learning models are used in high-stakes domains, such as criminal justice,(1,13,24) hiring,(19) critical infrastructure,(11,21) and finance.(18) Even in other domains, mismatches may lead to loss of revenue or public relations setbacks. Of particular concern are recent examples showing that machine learning models can reproduce or amplify unwanted societal biases reflected in training datasets.(4,5,12) For these and other reasons, the World Economic Forum suggests all entities should document the provenance, creation, and use of machine learning datasets to avoid discriminatory outcomes.(25) Although data provenance has been studied} }
@article{WOS:001269821000001, title = {Applications of Machine Learning to Optimize Tennis Performance: A Systematic Review}, journal = {APPLIED SCIENCES-BASEL}, volume = {14}, year = {2024}, doi = {10.3390/app14135517}, author = {Sampaio, Tatiana and Oliveira, Joao P. and Marinho, Daniel A. and Neiva, Henrique P. and Morais, Jorge E.}, abstract = {(1) Background: Tennis has changed toward power-driven gameplay, demanding a nuanced understanding of performance factors. This review explores the role of machine learning in enhancing tennis performance. (2) Methods: A systematic search identified articles utilizing machine learning in tennis performance analysis. (3) Results: Machine learning applications show promise in psychological state monitoring, talent identification, match outcome prediction, spatial and tactical analysis, and injury prevention. Coaches can leverage wearable technologies for personalized psychological state monitoring, data-driven talent identification, and tactical insights for informed decision-making. (4) Conclusions: Machine learning offers coaches insights to refine coaching methodologies and optimize player performance in tennis. By integrating these insights, coaches can adapt to the demands of the sport by improving the players' outcomes. As technology progresses, continued exploration of machine learning's potential in tennis is warranted for further advancements in performance optimization.} }
@article{WOS:000685103600001, title = {Training machine learning models on climate model output yields skillful interpretable seasonal precipitation forecasts}, journal = {COMMUNICATIONS EARTH \\& ENVIRONMENT}, volume = {2}, year = {2021}, doi = {10.1038/s43247-021-00225-4}, author = {Gibson, Peter B. and Chapman, William E. and Altinok, Alphan and Delle Monache, Luca and DeFlorio, Michael J. and Waliser, Duane E.}, abstract = {Seasonal forecasting skill in machine learning methods that are trained on large climate model ensembles can compete with, or out-compete, existing dynamical models, while retaining physical interpretability. A barrier to utilizing machine learning in seasonal forecasting applications is the limited sample size of observational data for model training. To circumvent this issue, here we explore the feasibility of training various machine learning approaches on a large climate model ensemble, providing a long training set with physically consistent model realizations. After training on thousands of seasons of climate model simulations, the machine learning models are tested for producing seasonal forecasts across the historical observational period (1980-2020). For forecasting large-scale spatial patterns of precipitation across the western United States, here we show that these machine learning-based models are capable of competing with or outperforming existing dynamical models from the North American Multi Model Ensemble. We further show that this approach need not be considered a `black box' by utilizing machine learning interpretability methods to identify the relevant physical processes that lead to prediction skill.} }
@article{WOS:000710922600002, title = {A survey of machine learning in credit risk}, journal = {JOURNAL OF CREDIT RISK}, volume = {17}, pages = {1-62}, year = {2021}, issn = {1744-6619}, doi = {10.21314/JCR.2021.008}, author = {Breeden, Joseph L.}, abstract = {Machine learning algorithms have come to dominate several industries. After decades of resistance from examiners and auditors, machine learning is now moving from the research desk to the application stack for credit scoring and a range of other applications in credit risk. This migration is not without novel risks and challenges. Much of the research is now shifting from how best to make the models to how best to use the models in a regulator-compliant business context. This paper surveys the impressively broad range of machine learning methods and application areas for credit risk. In the process of that survey, we create a taxonomy to think about how different machine learning components are matched to create specific algorithms. The reasons for where machine learning succeeds over simple linear methods are explored through a specific lending example. Throughout, we highlight open questions, ideas for improvements and a framework for thinking about how to choose the best machine learning method for a specific problem.} }
@article{WOS:000630189900078, title = {Evaluation of Sentiment Analysis based on AutoML and Traditional Approaches}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {12}, pages = {612-618}, year = {2021}, issn = {2158-107X}, author = {Mahima, K. T. Y. and Ginige, T. N. D. S. and De Zoysa, Kasun}, abstract = {AutoML or Automated Machine Learning is a set of tools to reduce or eliminate the necessary skills of a data scientist to build machine learning or deep learning models. Those tools are able to automatically discover the machine learning models and pipelines for the given dataset within very low interaction of the user. This concept was derived because developing a machine learning or deep learning model by applying the traditional machine learning methods is time-consuming and sometimes it is challenging for experts as well. Moreover, present AutoML tools are used in most of the areas such as image processing and sentiment analysis. In this research, the authors evaluate the implementation of a sentiment analysis classification model based on AutoML and Traditional approaches. For the evaluation, this research used both deep learning and machine learning approaches. To implement the sentiment analysis models HyperOpt SkLearn, TPot as AutoML libraries and, as the traditional method, Scikit learn libraries were used. Moreover for implementing the deep learning models Keras and Auto-Keras libraries used. In the implementation process, to build two binary classification and two multi-class classification models using the above- mentioned libraries. Thereafter evaluate the findings by each AutoML and Traditional approach. In this research, the authors were able to identify that building a machine learning or a deep learning model manually is better than using an AutoML approach.} }
@article{WOS:000899437900001, title = {Hydrogel and Machine Learning for Soft Robots' Sensing and Signal Processing: A Review}, journal = {JOURNAL OF BIONIC ENGINEERING}, volume = {20}, pages = {845-857}, year = {2023}, issn = {1672-6529}, doi = {10.1007/s42235-022-00320-y}, author = {Wang, Shuyu and Sun, Zhaojia}, abstract = {The soft robotics field is on the rise. The highly adaptive robots provide the opportunity to bridge the gap between machines and people. However, their elastomeric nature poses significant challenges to the perception, control, and signal processing. Hydrogels and machine learning provide promising solutions to the problems above. This review aims to summarize this recent trend by first assessing the current hydrogel-based sensing and actuation methods applied to soft robots. We outlined the mechanisms of perception in response to various external stimuli. Next, recent achievements of machine learning for soft robots' sensing data processing and optimization are evaluated. Here we list the strategies for implementing machine learning models from the perspective of applications. Last, we discuss the challenges and future opportunities in perception data processing and soft robots' high level tasks.} }
@article{WOS:000331851700015, title = {TOSELM: Timeliness Online Sequential Extreme Learning Machine}, journal = {NEUROCOMPUTING}, volume = {128}, pages = {119-127}, year = {2014}, issn = {0925-2312}, doi = {10.1016/j.neucom.2013.02.047}, author = {Gu, Yang and Liu, Junfa and Chen, Yiqiang and Jiang, Xinlong and Yu, Hanchao}, abstract = {For handling data and training model, existing machine learning methods do not take timeliness problem into consideration. Timeliness here means the data distribution or the data trend changes with time passing by. Based on timeliness management scheme, a novel machine learning algorithm Timeliness Online Sequential Extreme Learning Machine (TOSELM) is proposed, which improves Online Sequential Extreme Learning Machine (OSELM) with central tendency and dispersion characteristics of data to deal with timeliness problem. The performance of proposed algorithm has been validated on several simulated and realistic datasets, and experimental results show that TOSELM utilizing adaptive weight scheme and iteration scheme can achieve higher learning accuracy, faster convergence and better stability than other machine learning methods. (C) 2013 Elsevier B.V. All rights reserved.} }
@article{WOS:000352350000001, title = {Experimental Realization of a Quantum Support Vector Machine}, journal = {PHYSICAL REVIEW LETTERS}, volume = {114}, year = {2015}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.114.140504}, author = {Li, Zhaokai and Liu, Xiaomei and Xu, Nanyang and Du, Jiangfeng}, abstract = {The fundamental principle of artificial intelligence is the ability of machines to learn from previous experience and do future work accordingly. In the age of big data, classical learning machines often require huge computational resources in many practical cases. Quantum machine learning algorithms, on the other hand, could be exponentially faster than their classical counterparts by utilizing quantum parallelism. Here, we demonstrate a quantum machine learning algorithm to implement handwriting recognition on a four-qubit NMR test bench. The quantum machine learns standard character fonts and then recognizes handwritten characters from a set with two candidates. Because of the wide spread importance of artificial intelligence and its tremendous consumption of computational resources, quantum speedup would be extremely attractive against the challenges of big data.} }
@article{WOS:000494359400009, title = {Machine Learning for Intelligent Authentication in 5G and Beyond Wireless Networks}, journal = {IEEE WIRELESS COMMUNICATIONS}, volume = {26}, pages = {55-61}, year = {2019}, issn = {1536-1284}, doi = {10.1109/MWC.001.1900054}, author = {Fang, He and Wang, Xianbin and Tomasin, Stefano}, abstract = {The 5G and beyond wireless networks are critical to support diverse vertical applications by connecting heterogeneous devices and machines, which directly increase vulnerability for various spoofing attacks. Conventional cryptographic and physical layer authentication techniques are facing some challenges in complex dynamic wireless environments, including significant security overhead, low reliability, as well as difficulties in pre-designing a precise authentication model, providing continuous protection, and learning time-varying attributes. In this article, we envision new authentication approaches based on machine learning techniques by opportunistically leveraging physical layer attributes, and introduce intelligence to authentication for more efficient security provisioning. Machine learning paradigms for intelligent authentication design are presented, namely for parametric/non-parametric and supervised/ unsupervised/reinforcement learning algorithms. In a nutshell, the machine-learning-based intelligent authentication approaches utilize specific features in the multi-dimensional domain for achieving cost-effective, more reliable, model-free, continuous, and situation-aware device validation under unknown network conditions and unpredictable dynamics.} }
@article{WOS:000732559000001, title = {Early diagnosis of rice plant disease using machine learning techniques}, journal = {ARCHIVES OF PHYTOPATHOLOGY AND PLANT PROTECTION}, volume = {55}, pages = {259-283}, year = {2022}, issn = {0323-5408}, doi = {10.1080/03235408.2021.2015866}, author = {Sharma, Mayuri and Kumar, Chandan Jyoti and Deka, Aniruddha}, abstract = {There is an incredible progress in machine learning applications in the field of agricultural research. Detection of various diseases, deficiencies, and factors impacting crops' productivity is one of the major ongoing research in this field. This paper considers various machine learning and deep learning techniques (transfer learning) for rice disease detection. In this study three different rice diseases viz. bacterial blight, rice blast, and brown spot are considered. A detailed comparative analysis of the results indicates the superiority of transfer learning techniques over conventional machine learning techniques. It is observed that InceptionResNetV2 achieves the best result followed by XceptionNet. This work can be incorporated in assisting the farmers for early diagnosis of rice disease so that future course of action may be taken on time. For future studies, efforts should be directed to work with bigger datasets so as to generalize the findings of the experiment.} }
@article{WOS:000525375800050, title = {A review of machine learning kernel methods in statistical process monitoring}, journal = {COMPUTERS \\& INDUSTRIAL ENGINEERING}, volume = {142}, year = {2020}, issn = {0360-8352}, doi = {10.1016/j.cie.2020.106376}, author = {Apsemidis, Anastasios and Psarakis, Stelios and Moguerza, Javier M.}, abstract = {The complexity of modern problems turns increasingly larger in industrial environments, so the classical process monitoring techniques have to adapt to deal with those problems. This is one of the reasons why new Machine and Statistical Learning methodologies have become very popular in the statistical community. Specifically, this article is focused on machine learning kernel methods techniques in the process monitoring field. After explaining the idea of kernel methods we thoroughly examine the process monitoring articles that make use of kernel models and the way in which these models are combined with other Machine Learning approaches. Finally, we summarize the whole picture of the literature and mention some remarkable points.} }
@article{WOS:000745560100002, title = {Machine Learning Applications in Drug Repurposing}, journal = {INTERDISCIPLINARY SCIENCES-COMPUTATIONAL LIFE SCIENCES}, volume = {14}, pages = {15-21}, year = {2022}, issn = {1913-2751}, doi = {10.1007/s12539-021-00487-8}, author = {Yang, Fan and Zhang, Qi and Ji, Xiaokang and Zhang, Yanchun and Li, Wentao and Peng, Shaoliang and Xue, Fuzhong}, abstract = {The coronavirus disease (COVID-19) has led to an rush to repurpose existing drugs, although the underlying evidence base is of variable quality. Drug repurposing is a technique by taking advantage of existing known drugs or drug combinations to be explored in an unexpected medical scenario. Drug repurposing, hence, plays a vital role in accelerating the pre-clinical process of designing novel drugs by saving time and cost compared to the traditional de novo drug discovery processes. Since drug repurposing depends on massive observed data from existing drugs and diseases, the tremendous growth of publicly available large-scale machine learning methods supplies the state-of-the-art application of data science to signaling disease, medicine, therapeutics, and identifying targets with the least error. In this article, we introduce guidelines on strategies and options of utilizing machine learning approaches for accelerating drug repurposing. We discuss how to employ machine learning methods in studying precision medicine, and as an instance, how machine learning approaches can accelerate COVID-19 drug repurposing by developing Chinese traditional medicine therapy. This article provides a strong reasonableness for employing machine learning methods for drug repurposing, including during fighting for COVID-19 pandemic.} }
@article{WOS:001162350100001, title = {Exploring QCD matter in extreme conditions with Machine Learning}, journal = {PROGRESS IN PARTICLE AND NUCLEAR PHYSICS}, volume = {135}, year = {2024}, issn = {0146-6410}, doi = {10.1016/j.ppnp.2023.104084}, author = {Zhou, Kai and Wang, Lingxiao and Pang, Long -Gang and Shi, Shuzhe}, abstract = {In recent years, machine learning has emerged as a powerful computational tool and novel problem -solving perspective for physics, offering new avenues for studying strongly interacting QCD matter properties under extreme conditions. This review article aims to provide an overview of the current state of this intersection of fields, focusing on the application of machine learning to theoretical studies in high energy nuclear physics. It covers diverse aspects, including heavy ion collisions, lattice field theory, and neutron stars, and discuss how machine learning can be used to explore and facilitate the physics goals of understanding QCD matter. The review also provides a commonality overview from a methodology perspective, from data -driven perspective to physics -driven perspective. We conclude by discussing the challenges and future prospects of machine learning applications in high energy nuclear physics, also underscoring the importance of incorporating physics priors into the purely data -driven learning toolbox. This review highlights the critical role of machine learning as a valuable computational paradigm for advancing physics exploration in high energy nuclear physics.} }
@article{WOS:000478732400005, title = {Machine learning by unitary tensor network of hierarchical tree structure}, journal = {NEW JOURNAL OF PHYSICS}, volume = {21}, year = {2019}, issn = {1367-2630}, doi = {10.1088/1367-2630/ab31ef}, author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and Garcia, Raul Blazquez and Su, Gang and Lewenstein, Maciej}, abstract = {The resemblance between the methods used in quantum-many body physics and in machine learning has drawn considerable attention. In particular, tensor networks (TNs) and deep learning architectures bear striking similarities to the extent that TNs can be used for machine learning. Previous results used one-dimensional TNs in image recognition, showing limited scalability and flexibilities. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multi-scale entanglement renormalization ansatz. This approach introduces mathematical connections among quantum many-body physics, quantum information theory, and machine learning. While keeping the TN unitary in the training phase, TN states are defined, which encode classes of images into quantum many-body states. We study the quantum features of the TN states, including quantum entanglement and fidelity. We find these quantities could be properties that characterize the image classes, as well as the machine learning tasks.} }
@article{WOS:000840062800002, title = {A survey on machine learning in array databases}, journal = {APPLIED INTELLIGENCE}, volume = {53}, pages = {9799-9822}, year = {2023}, issn = {0924-669X}, doi = {10.1007/s10489-022-03979-2}, author = {Villarroya, Sebastian and Baumann, Peter}, abstract = {This paper provides an in-depth survey on the integration of machine learning and array databases. First,machine learning support in modern database management systems is introduced. From straightforward implementations of linear algebra operations in SQL to machine learning capabilities of specialized database managers designed to process specific types of data, a number of different approaches are overviewed. Then, the paper covers the database features already implemented in current machine learning systems. Features such as rewriting, compression, and caching allow users to implement more efficient machine learning applications. The underlying linear algebra computations in some of the most used machine learning algorithms are studied in order to determine which linear algebra operations should be efficiently implemented by array databases. An exhaustive overview of array data and relevant array database managers is also provided. Those database features that have been proven of special importance for efficient execution of machine learning algorithms are analyzed in detail for each relevant array database management system. Finally, current state of array databases capabilities for machine learning implementation is shown through two example implementations in Rasdaman and SciDB.} }
@article{WOS:000444245700003, title = {REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES}, journal = {QUANTUM INFORMATION \\& COMPUTATION}, volume = {18}, pages = {51-74}, year = {2018}, issn = {1533-7146}, author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S. and Ronaghe, Pooya}, abstract = {We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.} }
@article{WOS:000562309400001, title = {Incremental Cost-Sensitive Support Vector Machine With Linear-Exponential Loss}, journal = {IEEE ACCESS}, volume = {8}, pages = {149899-149914}, year = {2020}, issn = {2169-3536}, doi = {10.1109/ACCESS.2020.3015954}, author = {Ma, Yue and Zhao, Kun and Wang, Qi and Tian, Yingjie}, abstract = {Incremental learning or online learning as a branch of machine learning has attracted more attention recently. For large-scale problems and dynamic data problem, incremental learning overwhelms batch learning, because of its efficient treatment for new data. However, class imbalance problem, which always appears in online classification brings a considerable challenge for incremental learning. The serious class imbalance problem may directly lead to a useless learning system. Cost-sensitive learning is an important learning paradigm for class imbalance problems and widely used in many applications. In this article, we propose an incremental cost-sensitive learning method to tackle the class imbalance problems in the online situation. This proposed algorithm is based on a novel cost-sensitive support vector machine, which uses the Linear-exponential (LINEX) loss to implement high cost for minority class and low cost for majority class. Using the half-quadratic optimization, we first put forward the algorithm for the cost-sensitive support vector machine, called CSLINEX-SVM*. Then we propose the incremental cost-sensitive algorithm, ICSL-SVM. The results of numeric experiments demonstrate that the proposed incremental algorithm outperforms some conventional batch algorithms except the proposed CSLINEX-SVM*.} }
@article{WOS:001129260900004, title = {Machine learning and pre-medical education}, journal = {ARTIFICIAL INTELLIGENCE IN MEDICINE}, volume = {129}, year = {2022}, issn = {0933-3657}, doi = {10.1016/j.artmed.2022.102313}, author = {Kolachalama, Vijaya B.}, abstract = {Machine learning and artificial intelligence (AI)-driven technologies are contributing significantly to various facets of medicine and care management. It is likely that the next generation of healthcare professionals will be confronted with a series of innovations that are powered by AI, and they may not have sufficient time during their professional tenure to learn about the underlying machine learning frameworks that are driving these systems. Educating the aspiring clinicians and care providers with the right foundational courses in machine learning as part of postsecondary education will likely transform them as high-tech physicians and care providers of the future.} }
@article{WOS:000486611500019, title = {Machine Learning for the Interventional Radiologist}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {213}, pages = {782-784}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.19.21527}, author = {Meek, Ryan D. and Lungren, Matthew P. and Gichoya, Judy W.}, abstract = {OBJECTIVE. The purpose of this article is to describe key potential areas of application of machine learning in interventional radiology. CONCLUSION. Machine learning, although in the early stages of development within the field of interventional radiology, has great potential to influence key areas such as image analysis, clinical predictive modeling, and trainee education. A proactive approach from current interventional radiologists and trainees is needed to shape future directions for machine learning and artificial intelligence.} }
@article{WOS:000464239400001, title = {Genetic algorithms for computational materials discovery accelerated by machine learning}, journal = {NPJ COMPUTATIONAL MATERIALS}, volume = {5}, year = {2019}, issn = {2057-3960}, doi = {10.1038/s41524-019-0181-4}, author = {Jennings, Paul C. and Lysgaard, Steen and Hummelshoj, Jens Strabo and Vegge, Tejs and Bligaard, Thomas}, abstract = {Materials discovery is increasingly being impelled by machine learning methods that rely on pre-existing datasets. Where datasets are lacking, unbiased data generation can be achieved with genetic algorithms. Here a machine learning model is trained on-the-fly as a computationally inexpensive energy predictor before analyzing how to augment convergence in genetic algorithm-based approaches by using the model as a surrogate. This leads to a machine learning accelerated genetic algorithm combining robust qualities of the genetic algorithm with rapid machine learning. The approach is used to search for stable, compositionally variant, geometrically similar nanoparticle alloys to illustrate its capability for accelerated materials discovery, e.g., nanoalloy catalysts. The machine learning accelerated approach, in this case, yields a 50-fold reduction in the number of required energy calculations compared to a traditional ``brute force'' genetic algorithm. This makes searching through the space of all homotops and compositions of a binary alloy particle in a given structure feasible, using density functional theory calculations.} }
@article{WOS:000455128100005, title = {What Machine Learning Can Learn from Foresight: A Human-Centered Approach For machine learning-based forecast efforts to succeed, they must embrace lessons from corporate foresight to address human and organizational challenges.}, journal = {RESEARCH-TECHNOLOGY MANAGEMENT}, volume = {62}, pages = {30-33}, year = {2019}, issn = {0895-6308}, doi = {10.1080/08956308.2019.1541725}, author = {Crews, Christian}, abstract = {Overview: Machine learning applications in business that return forecasts or predictions of future market or consumer behavior must pay attention to nontechnical aspects of how those forecasts are created and used by leaders. Machine learning projects can generate better forecasts that have greater effect by embracing key methods developed through almost 50 years of corporate foresight practice to improve the adoption and use of forecasts in organizations.} }
@article{WOS:000452544100061, title = {Data Integration and Machine Learning: A Natural Synergy}, journal = {PROCEEDINGS OF THE VLDB ENDOWMENT}, volume = {11}, pages = {2094-2097}, year = {2018}, issn = {2150-8097}, doi = {10.14778/3229863.3229876}, author = {Dong, Xin Luna and Rekatsinas, Theodoros}, abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.} }
@article{WOS:001102779100001, title = {Machine Learning for the Control and Monitoring of Electric Machine Drives: Advances and Trends}, journal = {IEEE OPEN JOURNAL OF INDUSTRY APPLICATIONS}, volume = {4}, pages = {188-214}, year = {2023}, doi = {10.1109/OJIA.2023.3284717}, author = {Zhang, Shen and Wallscheid, Oliver and Porrmann, Mario}, abstract = {This review article systematically summarizes the existing literature on utilizing machine learning (ML) techniques for the control and monitoring of electric machine drives. It is anticipated that with the rapid progress in learning algorithms and specialized embedded hardware platforms, ML-based data-driven approaches will become standard tools for the automated high-performance control and monitoring of electric drives. In addition, this article also provides some outlook toward promoting its widespread application in the industry with a focus on deploying ML algorithms onto embedded system-on-chip field-programmable gate array devices.} }
@article{WOS:001380875400005, title = {Landscape of machine learning evolution: privacy-preserving federated learning frameworks and tools}, journal = {ARTIFICIAL INTELLIGENCE REVIEW}, volume = {58}, year = {2024}, issn = {0269-2821}, doi = {10.1007/s10462-024-11036-2}, author = {Nguyen, Giang and Sainz-Pardo Diaz, Judith and Calatrava, Amanda and Berberi, Lisana and Lytvyn, Oleksandr and Kozlov, Valentin and Tran, Viet and Molto, German and Lopez Garcia, Alvaro}, abstract = {Machine learning is one of the most widely used technologies in the field of Artificial Intelligence. As machine learning applications become increasingly ubiquitous, concerns about data privacy and security have also grown. The work in this paper presents a broad theoretical landscape concerning the evolution of machine learning and deep learning from centralized to distributed learning, first in relation to privacy-preserving machine learning and secondly in the area of privacy-enhancing technologies. It provides a comprehensive landscape of the synergy between distributed machine learning and privacy-enhancing technologies, with federated learning being one of the most prominent architectures. Various distributed learning approaches to privacy-aware techniques are structured in a review, followed by an in-depth description of relevant frameworks and libraries, more particularly in the context of federated learning. The paper also highlights the need for data protection and privacy addressed from different approaches, key findings in the field concerning AI applications, and advances in the development of related tools and techniques.} }
@article{WOS:000463601900005, title = {Nurses ``Seeing Forest for the Trees'' in the Age of Machine Learning Using Nursing Knowledge to Improve Relevance and Performance}, journal = {CIN-COMPUTERS INFORMATICS NURSING}, volume = {37}, pages = {203-212}, year = {2019}, issn = {1538-2931}, doi = {10.1097/CIN.0000000000000508}, author = {Kwon, Jae Yung and Karim, Mohammad Ehsanul and Topaz, Maxim and Currie, Leanne M.}, abstract = {Although machine learning is increasingly being applied to support clinical decision making, there is a significant gap in understanding what it is and how nurses should adopt it in practice. The purpose of this case study is to show how one application of machine learning may support nursing work and to discuss how nurses can contribute to improving its relevance and performance. Using data from 130 specialized hospitals with 101 766 patients with diabetes, we applied various advanced statistical methods (known as machine learning algorithms) to predict early readmission. The best-performing machine learning algorithm showed modest predictive ability with opportunities for improvement. Nurses can contribute to machine learning algorithms by (1) filling data gaps with nursing-relevant data that provide personalized context about the patient, (2) improving data preprocessing techniques, and (3) evaluating potential value in practice. These findings suggest that nurses need to further process the information provided by machine learning and apply ``Wisdom-in-Action'' to make appropriate clinical decisions. Nurses play a pivotal role in ensuring that machine learning algorithms are shaped by their unique knowledge of each patient's personalized context. By combining machine learning with unique nursing knowledge, nurses can provide more visibility to nursing work, advance nursing science, and better individualize patient care. Therefore, to successfully integrate and maximize the benefits of machine learning, nurses must fully participate in its development, implementation, and evaluation.} }
@article{WOS:001232580600001, title = {Machine Learning and Deep Learning Strategies for Chinese Hamster Ovary Cell Bioprocess Optimization}, journal = {FERMENTATION-BASEL}, volume = {10}, year = {2024}, doi = {10.3390/fermentation10050234}, author = {Baako, Tiffany-Marie D. and Kulkarni, Sahil Kaushik and McClendon, Jerome L. and Harcum, Sarah W. and Gilmore, Jordon}, abstract = {The use of machine learning and deep learning has become prominent within various fields of bioprocessing for countless modeling and prediction tasks. Previous reviews have emphasized machine learning applications in various fields of bioprocessing, including biomanufacturing. This comprehensive review highlights many of the different machine learning and multivariate analysis techniques that have been utilized within Chinese hamster ovary cell biomanufacturing, specifically due to their rising significance in the industry. Applications of machine and deep learning within other bioprocessing industries are also briefly discussed.} }
@article{WOS:000618359200001, title = {Comparison of machine learning and deep learning algorithms for hourly global/diffuse solar radiation predictions}, journal = {INTERNATIONAL JOURNAL OF ENERGY RESEARCH}, volume = {46}, pages = {10052-10073}, year = {2022}, issn = {0363-907X}, doi = {10.1002/er.6529}, author = {Bamisile, Olusola and Oluwasanmi, Ariyo and Ejiyi, Chukwuebuka and Yimen, Nasser and Obiora, Sandra and Huang, Qi}, abstract = {Due to the advancement and wide adoption/application of solar-based technologies, the prediction of solar irradiance has attracted research attention in recent years. In this study, the predictive performance of machine learning models is compared with that of deep learning models for both global solar radiation (GSR) and diffuse solar radiation (DSR) prediction. Different studies have proposed the use of different models for solar radiation prediction. While some used machine learning models, the use of deep learning algorithms were considered by others. Although these algorithms were concluded to be appropriate for solar radiation prediction, variation in their performances brings about an intriguing quest to compare and determine the most appropriate algorithm. The three most common deep learning models in the literature namely; artificial neural network, convolutional neural network, and recurrent neural network (RNN) are considered within the scope of this study. Also, two traditional machine learning models namely polynomial regression and support vector regression (SVR) is considered as well as an ensemble machine learning model called random forest. These models have been applied to four different locations in Nigeria and the typical meteorological year data for 12 years in an hourly time step was used to train/test the model developed. Results from this study show that deep learning models have a better GSR and DSR prediction accuracy in comparison to machine learning models. However, the duration for training and testing the machine learning models (except SVR) is shorter than that of deep learning models making it more desirable for low computational applications. The application of RNN for GSR prediction in Yobe (with an r value of 0.9546 and root means square error/mean absolute error of 82.22 W/m(2)/36.52 W/m(2)) had the overall best model performance of all the models developed in this study. This study contributes to the existing literature in this field as it highlights the disparities between machine learning and deep learning algorithms application for solar radiation forecast.} }
@article{WOS:001028542300001, title = {A state-of-the-art review on the utilization of machine learning in nanofluids, solar energy generation, and the prognosis of solar power}, journal = {ENGINEERING ANALYSIS WITH BOUNDARY ELEMENTS}, volume = {155}, pages = {62-86}, year = {2023}, issn = {0955-7997}, doi = {10.1016/j.enganabound.2023.06.003}, author = {Singh, Santosh Kumar and Tiwari, Arun Kumar and Paliwal, H. K.}, abstract = {In the contemporary data-driven era, the fields of machine learning, deep learning, big data, statistics, and data science are essential for forecasting outcomes and getting insights from data. This paper looks at how machine learning approaches can be used to anticipate solar power generation, assess heat exchanger heat transfer efficiency, and predict the thermo-physical properties of nanofluids. The review specifically focuses on the potential use of machine learning in solar thermal applications, perovskites, and photovoltaic power forecasting. Predictions of nanofluid characteristics and device performance may be more accurately made with the development of machine learning algorithms. The use of machine learning in the creation of new perovskites and the assessment of their effectiveness and stability is also included in the review. Additionally, the paper explores developments in artificial intelligence, particularly deep learning, in this area and offers insights into techniques for forecasting solar power, including PV production, cloud motion, and weather classification.} }
@article{WOS:000704195500004, title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review}, journal = {COMPUTER COMMUNICATIONS}, volume = {178}, pages = {37-63}, year = {2021}, issn = {0140-3664}, doi = {10.1016/j.comcom.2021.07.009}, author = {Miglani, Arzoo and Kumar, Neeraj}, abstract = {Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.} }
@article{WOS:000494273000002, title = {Machine Learning Education for Artists, Musicians, and Other Creative Practitioners}, journal = {ACM TRANSACTIONS ON COMPUTING EDUCATION}, volume = {19}, year = {2019}, issn = {1946-6226}, doi = {10.1145/3294008}, author = {Fiebrink, Rebecca}, abstract = {This article aims to lay a foundation for the research and practice of machine learning education for creative practitioners. It begins by arguing that it is important to teach machine learning to creative practitioners and to conduct research about this teaching, drawing on related work in creative machine learning, creative computing education, and machine learning education. It then draws on research about design processes in engineering and creative practice to motivate a set of learning objectives for students who wish to design new creative artifacts with machine learning. The article then draws on education research and knowledge of creative computing practices to propose a set of teaching strategies that can be used to support creative computing students in achieving these objectives. Explanations of these strategies are accompanied by concrete descriptions of how they have been employed to develop new lectures and activities, and to design new experiential learning and scaffolding technologies, for teaching some of the first courses in the world focused on teaching machine learning to creative practitioners. The article subsequently draws on data collected from these courses-an online course as well as undergraduate and masters-level courses taught at a university-to begin to understand how this curriculum supported student learning, to understand learners' challenges and mistakes, and to inform future teaching and research.} }
@article{WOS:000401022300001, title = {Addressing uncertainty in atomistic machine learning}, journal = {PHYSICAL CHEMISTRY CHEMICAL PHYSICS}, volume = {19}, pages = {10978-10985}, year = {2017}, issn = {1463-9076}, doi = {10.1039/c7cp00375g}, author = {Peterson, Andrew A. and Christensen, Rune and Khorshidi, Alireza}, abstract = {Machine-learning regression has been demonstrated to precisely emulate the potential energy and forces that are output from more expensive electronic-structure calculations. However, to predict new regions of the potential energy surface, an assessment must be made of the credibility of the predictions. In this perspective, we address the types of errors that might arise in atomistic machine learning, the unique aspects of atomistic simulations that make machine-learning challenging, and highlight how uncertainty analysis can be used to assess the validity of machine-learning predictions. We suggest this will allow researchers to more fully use machine learning for the routine acceleration of large, high-accuracy, or extended-time simulations. In our demonstrations, we use a bootstrap ensemble of neural network-based calculators, and show that the width of the ensemble can provide an estimate of the uncertainty when the width is comparable to that in the training data. Intriguingly, we also show that the uncertainty can be localized to specific atoms in the simulation, which may offer hints for the generation of training data to strategically improve the machine-learned representation.} }
@article{WOS:001130161000001, title = {Machine Learning and Genetic Algorithms: A case study on image reconstruction}, journal = {KNOWLEDGE-BASED SYSTEMS}, volume = {284}, year = {2024}, issn = {0950-7051}, doi = {10.1016/j.knosys.2023.111194}, author = {Cavallaro, Claudia and Cutello, Vincenzo and Pavone, Mario and Zito, Francesco}, abstract = {In this research, we investigate the application of machine learning techniques to optimization problems and propose a novel integration between metaheuristics and machine learning for the problem of image reconstruction. We propose a modified version of the standard genetic algorithm that uses machine learning to quickly drive the search towards good solutions by dynamically adjusting its parameters. We conducted experiments to compare the performance of our proposed algorithm with other metaheuristic algorithms, including Tabu Search, Iterated Local Search, and Artificial Immune System. Our results demonstrate the effectiveness of our algorithm in finding better solutions and in achieving faster convergence times compared to the other algorithms. The significant computational time difference between the standard genetic algorithm and the genetic algorithm with machine learning highlights the innovation of our approach and its potential to improve real-world applications.} }
@article{WOS:000784962100001, title = {A Review of Data-Driven Machinery Fault Diagnosis Using Machine Learning Algorithms}, journal = {JOURNAL OF VIBRATION ENGINEERING \\& TECHNOLOGIES}, volume = {10}, pages = {2481-2507}, year = {2022}, issn = {2523-3920}, doi = {10.1007/s42417-022-00498-9}, author = {Cen, Jian and Yang, Zhuohong and Liu, Xi and Xiong, Jianbin and Chen, Honghua}, abstract = {Purpose This article aims to systematically review the recent research advances in data-driven machinery fault diagnosis based on machine learning algorithms, and provide valuable guidance for future research directions in this field. Methods This article reviews the research results of data-driven fault diagnosis methods of recent years, and it includes the application status and research progress of machinery fault diagnosis in three frameworks: shallow machine learning (SML), deep learning (DL), and transfer learning (TL). Many publications on this topic are classified and summarized. The related theories, application research, advantages, and disadvantages of several main algorithms under each framework are discussed. Results It has shown that SML-based diagnosis models are simple, reliable, and fast to train. For relatively uncomplicated systems, SML-based diagnosis models still have important applications. For diagnosis tasks with large amounts of training samples and the pursuit of higher accuracy, DL-based diagnosis models can provide end-to-end diagnostic services for complex systems as well as compound faults. TL-based diagnosis models can realize knowledge transfer across conditions, machines, and even fields to solve the problems of data scarcity and sample imbalance that often occur in fault diagnosis. However, in the face of increasingly complex engineering systems, the applications of machine learning algorithms in machinery fault diagnosis are still challenging. Conclusions In future research, the fusion of different machine learning frameworks could solve the problems of inadequate feature extraction and slow training of diagnostic models. Transformer neural network based on pure attention mechanism breaks through the shortcomings of LSTM neural network which cannot be computed in parallel, and it is a worthy research direction in the field of fault diagnosis. In addition, machinery fault diagnosis method based on machine learning algorithms also has great potential for improvement in transferability, federated transfer learning, and strong noise background. These proposed future research directions can provide new ideas for researchers to promote the development of machine learning algorithms in machinery fault diagnosis.} }
@article{WOS:000933899400002, title = {Applications of machine learning in relativistic heavy ion physics}, journal = {SCIENTIA SINICA-PHYSICA MECHANICA \\& ASTRONOMICA}, volume = {52}, year = {2022}, issn = {1674-7275}, doi = {10.1360/SSPMA-2021-0321}, author = {Zhou Meng and Luo YiQun and Song HuiChao}, abstract = {Recently, with rapid hardware and algorithms development, machine learning has been widely used as a significant data analysis method. This article reviews the application of di fferent machine learning algorithms in heavy ion collisions, including impact parameter prediction, nuclear deformation parameter prediction, phase transitions classification, fluid evolution simulation, etc. The machine learning algorithms comprise classic machine learning algorithms, such as ensemble learning and principal component analysis, and deep learning algorithms, such as convolutional neural networks and point cloud networks. Because of the excellent performance and e fficiency of machine learning, these applications will receive much attention in our field.} }
@article{WOS:000609117400001, title = {An inclusive survey on machine learning for CRM: a paradigm shift}, journal = {DECISION}, volume = {47}, pages = {447-457}, year = {2020}, issn = {0304-0941}, doi = {10.1007/s40622-020-00261-7}, author = {Singh, Narendra and Singh, Pushpa and Gupta, Mukul}, abstract = {Customer relationship management (CRM) is the tool to enhance customer relationship in any business. Due to the exponential growth of data volume, in any field, it is significant to develop new techniques to discover the customer knowledge, automation of the system and moreover customer satisfaction to win customer lifetime value. CRM with machine learning could bring a catalytic change in business. Several supervised and unsupervised machine learning techniques are utilized to improve the customer experience and profitability of business. This paper reviews the available literature on the CRM with machine learning techniques for customer identification, customer attraction, and customer retention and customer development. This study reveals that supervised learning techniques are 48.48\\% utilized, unsupervised learning techniques are utilized 15.15\\%, and 9.09\\% utilized other techniques in CRM. Paradigm is also shifted toward the deep learning from machine learning as 28.28\\% text has been reported to deep learning. Decision tree-based algorithm and support vector machine algorithms are most utilized algorithm of supervised learning. E-commerce and telecommunication sectors are the most important areas identified with the exponential growth of the users and hence need a suitable machine learning techniques for customer satisfaction and business profitability.} }
@article{WOS:001238605200001, title = {Interpretable machine learning for creditor recovery rates}, journal = {JOURNAL OF BANKING \\& FINANCE}, volume = {164}, year = {2024}, issn = {0378-4266}, doi = {10.1016/j.jbankfin.2024.107187}, author = {Nazemi, Abdolreza and Fabozzi, Frank J.}, abstract = {Machine learning methods have achieved great success in modeling complex patterns in finance such as asset pricing and credit risk that enable them to outperform statistical models. In addition to the predictive accuracy of machine learning methods, the ability to interpret what a model has learned is crucial in the finance industry. We address this challenge by adapting interpretable machine learning to the context of corporate bond recovery rate modeling. In addition to the best performance, we show the value of interpretable machine learning by finding drivers of recovery rates and their relationship that cannot be discovered by the use of traditional machine learning methods. Our findings are financially meaningful and consistent with the findings in the existing credit risk literature.} }
@article{WOS:000471643600004, title = {Stochastic Gradient Descent and Its Variants in Machine Learning}, journal = {JOURNAL OF THE INDIAN INSTITUTE OF SCIENCE}, volume = {99}, pages = {201-213}, year = {2019}, issn = {0970-4140}, doi = {10.1007/s41745-019-0098-4}, author = {Netrapalli, Praneeth}, abstract = {Stochastic gradient descent (SGD) is a fundamental algorithm which has had a profound impact on machine learning. This article surveys some important results on SGD and its variants that arose in machine learning.} }
@article{WOS:000523935300001, title = {A survey of surveys on the use of visualization for interpreting machine learning models}, journal = {INFORMATION VISUALIZATION}, volume = {19}, pages = {207-233}, year = {2020}, issn = {1473-8716}, doi = {10.1177/1473871620904671}, author = {Chatzimparmpas, Angelos and Martins, Rafael M. and Jusufi, Ilir and Kerren, Andreas}, abstract = {Research in machine learning has become very popular in recent years, with many types of models proposed to comprehend and predict patterns and trends in data originating from different domains. As these models get more and more complex, it also becomes harder for users to assess and trust their results, since their internal operations are mostly hidden in black boxes. The interpretation of machine learning models is currently a hot topic in the information visualization community, with results showing that insights from machine learning models can lead to better predictions and improve the trustworthiness of the results. Due to this, multiple (and extensive) survey articles have been published recently trying to summarize the high number of original research papers published on the topic. But there is not always a clear definition of what these surveys cover, what is the overlap between them, which types of machine learning models they deal with, or what exactly is the scenario that the readers will find in each of them. In this article, we present a meta-analysis (i.e. a ``survey of surveys'') of manually collected survey papers that refer to the visual interpretation of machine learning models, including the papers discussed in the selected surveys. The aim of our article is to serve both as a detailed summary and as a guide through this survey ecosystem by acquiring, cataloging, and presenting fundamental knowledge of the state of the art and research opportunities in the area. Our results confirm the increasing trend of interpreting machine learning with visualizations in the past years, and that visualization can assist in, for example, online training processes of deep learning models and enhancing trust into machine learning. However, the question of exactly how this assistance should take place is still considered as an open challenge of the visualization community.} }
@article{WOS:000962500000001, title = {Differentiation of Bone Metastasis in Elderly Patients With Lung Adenocarcinoma Using Multiple Machine Learning Algorithms}, journal = {CANCER CONTROL}, volume = {30}, year = {2023}, issn = {1073-2748}, doi = {10.1177/10732748231167958}, author = {Zhou, Cheng-Mao and Wang, Ying and Xue, Qiong and Zhu, Yu}, abstract = {ObjectiveWe tested the performance of general machine learning and joint machine learning algorithms in the classification of bone metastasis, in patients with lung adenocarcinoma.MethodsWe used R version 3.5.3 for statistical analysis of the general information, and Python to construct machine learning models.ResultsWe first used the average classifiers of the 4 machine learning algorithms to rank the features and the results showed that race, sex, whether they had surgery and marriage were the first 4 factors affecting bone metastasis. Machine learning results in the training group: for area under the curve (AUC), except for RF and LR, the AUC values of all machine learning classifiers were greater than .8, but the joint algorithm did not improve the AUC for any single machine learning algorithm. Among the results related to accuracy and precision, the accuracy of other machine learning classifiers except the RF algorithm was higher than 70\\%, and only the precision of the LGBM algorithm was higher than 70\\%. Machine learning results in the test group: Similarly, for areas under the curve (AUC), except RF and LR, the AUC values for all machine learning classifiers were greater than .8, but the joint algorithm did not improve the AUC value for any single machine learning algorithm. For accuracy, except for the RF algorithm, the accuracy of other machine learning classifiers was higher than 70\\%. The highest precision for the LGBM algorithm was .675.ConclusionThe results of this concept verification study show that machine learning algorithm classifiers can distinguish the bone metastasis of patients with lung cancer. This will provide a new research idea for the future use of non-invasive technology to identify bone metastasis in lungcancer. However, more prospective multicenter cohort studies are needed.} }
@article{WOS:001221463000016, title = {Machine Learning in Materials Chemistry: An Invitation}, journal = {MACHINE LEARNING WITH APPLICATIONS}, volume = {8}, year = {2022}, doi = {10.1016/j.mlwa.2022.100265}, author = {Packwood, Daniel and Nguyen, Linh Thi Hoai and Cesana, Pierluigi and Zhang, Guoxi and Staykov, Aleksandar and Fukumoto, Yasuhide and Nguyen, Dinh Hoa}, abstract = {Materials chemistry is being profoundly influenced by the uptake of machine learning methodologies. Machine learning techniques, in combination with established techniques from computational physics, promise to accelerate the discovery of new materials by elucidating complex structure-property relationships from massive material databases. Despite exciting possibilities, further methodological developments call for a greater synergism between materials chemists, physicists, and engineers on one side, with computer science and math majors on the other. In this review, we provide a non -exhaustive account of machine learning in materials chemistry for computer scientists and applied mathematicians, with an emphasis on molecule datasets and materials chemistry problems. The first part of this review provides a tutorial on how to prepare such datasets for subsequent model building, with an emphasis on the construction of feature vectors. We also provide a self-contained introduction to density functional theory, a method from computational physics which is widely used to generate datasets and compute response variables. The second part reviews two machine learning methodologies which represent the status quo in materials chemistry at present - kernelized machine learning and Bayesian machine learning - and discusses their application to real datasets. In the third part of the review, we introduce some emerging machine learning techniques which have not been widely adopted by materials scientists and therefore present potential avenues for computer science and applied math majors. In the final concluding section, we discuss some recent machine learning -based approaches to real materials discovery problems and speculate on some promising future directions.} }
@article{WOS:000635324300001, title = {RETRACTED: User acceptance of machine learning models - Integrating several important external variables with technology acceptance model (Retracted Article)}, journal = {INTERNATIONAL JOURNAL OF ELECTRICAL ENGINEERING EDUCATION}, year = {2021}, issn = {0020-7209}, doi = {10.1177/00207209211005271}, author = {Zhang, Xiaohang and Wang, Yuan and Li, Zhengren}, abstract = {Machine learning models enable data-based decision-making in many areas and have attracted extensive attention. By testing the factors that influence the adoption of machine learning models, this study expands the scope of machine learning models in information technology adoption research. Based on the machine learning background and Technology Acceptance Model, this study integrates the necessary external variables, proposes a research model, and further verifies the validity of the model through the survey of 192 users of machine learning models. The results showed that organizational factors, trust, perceived usefulness, and perceived ease of use are positively correlated with the attitude of machine learning models. Moreover, our findings show that the interpretability of the model has an important positive effect on trust. The factors examined in this study are the basis for the development and use of reliable machine learning models. And it has important practical significance for promoting user adoption of machine learning model. Meanwhile, these theoretical studies also provide a strong literature support for the adoption of machine learning models and fill the theoretical research gap in this field.} }
@article{WOS:000357246100026, title = {Multi-task proximal support vector machine}, journal = {PATTERN RECOGNITION}, volume = {48}, pages = {3249-3257}, year = {2015}, issn = {0031-3203}, doi = {10.1016/j.patcog.2015.01.014}, author = {Li, Ya and Tian, Xinmei and Song, Mingli and Tao, Dacheng}, abstract = {With the explosive growth of the use of imagery, visual recognition plays an important role in many applications and attracts increasing research attention. Given several related tasks, single-task learning learns each task separately and ignores the relationships among these tasks. Different from single-task learning, multi-task learning can explore more information to learn all tasks jointly by using relationships among these tasks. In this paper, we propose a novel multi-task learning model based on the proximal support vector machine. The proximal support vector machine uses the large-margin idea as does the standard support vector machines but with looser constraints and much lower computational cost. Our multi-task proximal support vector machine inherits the merits of the proximal support vector machine and achieves better performance compared with other popular multi-task learning models. Experiments are conducted on several multi-task learning datasets, including two classification datasets and one regression dataset. All results demonstrate the effectiveness and efficiency of our proposed multi-task proximal support vector machine. (C) 2015 Elsevier Ltd. All rights reserved.} }
@article{WOS:001326938800003, title = {A mini review of machine learning in inorganic phosphors}, journal = {JOURNAL OF MATERIALS INFORMATICS}, volume = {2}, year = {2022}, doi = {10.20517/jmi.2022.21}, author = {Jiang, Lipeng and Jiang, Xue and Lv, Guocai and Su, Yanjing}, abstract = {Machine learning has promoted the rapid development of materials science. In this review, we provide an overview of recent advances in machine learning for inorganic phosphors. We take two aspects of material properties prediction and optimization based on iterative experiments as entry points to outline the applications of machine learning for inorganic phosphors in terms of Debye temperature prediction and luminescence intensity and thermal stability optimization. By analyzing the machine learning methods and their application objectives, current problems are summarized and suggestions for subsequent development are proposed.} }
@article{WOS:000360416900003, title = {An improved cuckoo search based extreme learning machine for medical data classification}, journal = {SWARM AND EVOLUTIONARY COMPUTATION}, volume = {24}, pages = {25-49}, year = {2015}, issn = {2210-6502}, doi = {10.1016/j.swevo.2015.05.003}, author = {Mohapatra, P. and Chakravarty, S. and Dash, P. K.}, abstract = {Machine learning techniques are being increasingly used for detection and diagnosis of diseases for its accuracy and efficiency in pattern classification. In this paper, improved cuckoo search based extreme learning machine (ICSELM) is proposed to classify binary medical datasets. Extreme learning machine (ELM) is widely used as a learning algorithm for training single layer feed forward neural networks (SLFN) in the field of classification. However, to make the model more stable, an evolutionary algorithm improved cuckoo search (ICS) is used to pre-train ELM by selecting the input weights and hidden biases. Like ELM, Moore-Penrose (MP) generalized inverse is used in ICSELM to analytically determines the output weights. To evaluate the effectiveness of the proposed model, four benchmark datasets, i.e. Breast Cancer, Diabetes, Bupa and Hepatitis from the UCI Repository of Machine Learning are used. A number of useful performance evaluation measures including accuracy, sensitivity, specificity, confusion matrix, Gmean, F-score and norm of the output weights as well as the area under the receiver operating characteristic (ROC) curve are computed. The results are analyzed and compared with both ELM based models like ELM, on-line sequential extreme learning algorithm (OSELM), CSELM and other artificial neural networks i.e. multi-layered perceptron (MLP), MLPCS, MLPICS and radial basis function neural network (RBFNN), RBFNNCS, RBFNNICS. The experimental results demonstrate that the ICSELM model outperforms other models. (C) 2015 Elsevier B.V. All rights reserved.} }
@article{WOS:000546324500005, title = {Exploring chemical compound space with quantum-based machine learning}, journal = {NATURE REVIEWS CHEMISTRY}, volume = {4}, pages = {347-358}, year = {2020}, doi = {10.1038/s41570-020-0189-9}, author = {von Lilienfeld, O. Anatole and Mueller, Klaus-Robert and Tkatchenko, Alexandre}, abstract = {Machine-learning techniques have enabled, among many other applications, the exploration of molecular properties throughout chemical space. The specific development of quantum-based approaches in machine learning can now help us unravel new chemical insights. Rational design of compounds with specific properties requires understanding and fast evaluation of molecular properties throughout chemical compound space - the huge set of all potentially stable molecules. Recent advances in combining quantum-mechanical calculations with machine learning provide powerful tools for exploring wide swathes of chemical compound space. We present our perspective on this exciting and quickly developing field by discussing key advances in the development and applications of quantum-mechanics-based machine-learning methods to diverse compounds and properties, and outlining the challenges ahead. We argue that significant progress in the exploration and understanding of chemical compound space can be made through a systematic combination of rigorous physical theories, comprehensive synthetic data sets of microscopic and macroscopic properties, and modern machine-learning methods that account for physical and chemical knowledge.} }
@article{WOS:000995149000001, title = {Twenty Years of Machine-Learning-Based Text Classification: A Systematic Review}, journal = {ALGORITHMS}, volume = {16}, year = {2023}, doi = {10.3390/a16050236}, author = {Palanivinayagam, Ashokkumar and El-Bayeh, Claude Ziad and Damasevicius, Robertas}, abstract = {Machine-learning-based text classification is one of the leading research areas and has a wide range of applications, which include spam detection, hate speech identification, reviews, rating summarization, sentiment analysis, and topic modelling. Widely used machine-learning-based research differs in terms of the datasets, training methods, performance evaluation, and comparison methods used. In this paper, we surveyed 224 papers published between 2003 and 2022 that employed machine learning for text classification. The Preferred Reporting Items for Systematic Reviews (PRISMA) statement is used as the guidelines for the systematic review process. The comprehensive differences in the literature are analyzed in terms of six aspects: datasets, machine learning models, best accuracy, performance evaluation metrics, training and testing splitting methods, and comparisons among machine learning models. Furthermore, we highlight the limitations and research gaps in the literature. Although the research works included in the survey perform well in terms of text classification, improvement is required in many areas. We believe that this survey paper will be useful for researchers in the field of text classification.} }
@article{WOS:000751673300089, title = {Integration of AI and Machine Learning in Radiotherapy QA}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {3}, year = {2020}, doi = {10.3389/frai.2020.577620}, author = {Chan, Maria F. and Witztum, Alon and Valdes, Gilmer}, abstract = {The use of machine learning and other sophisticated models to aid in prediction and decision making has become widely popular across a breadth of disciplines. Within the greater diagnostic radiology, radiation oncology, and medical physics communities promising work is being performed in tissue classification and cancer staging, outcome prediction, automated segmentation, treatment planning, and quality assurance as well as other areas. In this article, machine learning approaches are explored, highlighting specific applications in machine and patient-specific quality assurance (QA). Machine learning can analyze multiple elements of a delivery system on its performance over time including the multileaf collimator (MLC), imaging system, mechanical and dosimetric parameters. Virtual Intensity-Modulated Radiation Therapy (IMRT) QA can predict passing rates using different measurement techniques, different treatment planning systems, and different treatment delivery machines across multiple institutions. Prediction of QA passing rates and other metrics can have profound implications on the current IMRT process. Here we cover general concepts of machine learning in dosimetry and various methods used in virtual IMRT QA, as well as their clinical applications.} }
@article{WOS:000569585500001, title = {Using Machine Learning Models and Actual Transaction Data for Predicting Real Estate Prices}, journal = {APPLIED SCIENCES-BASEL}, volume = {10}, year = {2020}, doi = {10.3390/app10175832}, author = {Pai, Ping-Feng and Wang, Wen-Chang}, abstract = {Real estate price prediction is crucial for the establishment of real estate policies and can help real estate owners and agents make informative decisions. The aim of this study is to employ actual transaction data and machine learning models to predict prices of real estate. The actual transaction data contain attributes and transaction prices of real estate that respectively serve as independent variables and dependent variables for machine learning models. The study employed four machine learning models-namely, least squares support vector regression (LSSVR), classification and regression tree (CART), general regression neural networks (GRNN), and backpropagation neural networks (BPNN), to forecast real estate prices. In addition, genetic algorithms were used to select parameters of machine learning models. Numerical results indicated that the least squares support vector regression outperforms the other three machine learning models in terms of forecasting accuracy. Furthermore, forecasting results generated by the least squares support vector regression are superior to previous related studies of real estate price prediction in terms of the average absolute percentage error. Thus, the machine learning-based model is a substantial and feasible way to forecast real estate prices, and the least squares support vector regression can provide relatively competitive and satisfactory results.} }
@article{WOS:000797054700007, title = {Machine Learning in Causal Inference: Application in Pharmacovigilance}, journal = {DRUG SAFETY}, volume = {45}, pages = {459-476}, year = {2022}, issn = {0114-5916}, doi = {10.1007/s40264-022-01155-6}, author = {Zhao, Yiqing and Yu, Yue and Wang, Hanyin and Li, Yikuan and Deng, Yu and Jiang, Guoqian and Luo, Yuan}, abstract = {Monitoring adverse drug events or pharmacovigilance has been promoted by the World Health Organization to assure the safety of medicines through a timely and reliable information exchange regarding drug safety issues. We aim to discuss the application of machine learning methods as well as causal inference paradigms in pharmacovigilance. We first reviewed data sources for pharmacovigilance. Then, we examined traditional causal inference paradigms, their applications in pharmacovigilance, and how machine learning methods and causal inference paradigms were integrated to enhance the performance of traditional causal inference paradigms. Finally, we summarized issues with currently mainstream correlation-based machine learning models and how the machine learning community has tried to address these issues by incorporating causal inference paradigms. Our literature search revealed that most existing data sources and tasks for pharmacovigilance were not designed for causal inference. Additionally, pharmacovigilance was lagging in adopting machine learning-causal inference integrated models. We highlight several currently trending directions or gaps to integrate causal inference with machine learning in pharmacovigilance research. Finally, our literature search revealed that the adoption of causal paradigms can mitigate known issues with machine learning models. We foresee that the pharmacovigilance domain can benefit from the progress in the machine learning field.} }
@article{WOS:000534638300001, title = {Applications of Machine Learning to Friction Stir Welding Process Optimization}, journal = {JURNAL KEJURUTERAAN}, volume = {32}, pages = {171-186}, year = {2020}, issn = {0128-0198}, doi = {10.17576/jkukm-2020-32(2)-01}, author = {Nasir, Tauqir and Asmael, Mohammed and Zeeshan, Qasim and Solyali, Davut}, abstract = {Machine learning (ML) is a branch of artificial intelligent which involve the study and development of algorithm for computer to learn from data. A computational method used in machine learning to learn or get directly information from data without relying on a prearranged model equation. The applications of ML applied in the domains of all industries. In the field of manufacturing the ability of ML approach is utilized to predict the failure before occurrence. FSW and FSSW is an advanced form of friction welding and it is a solid state joining technique which is mostly used to weld the dissimilar alloys. FSW, FSSW has become a dominant joining method in aerospace, railway and ship building industries. It observed that the number of applications of machine learning increased in FSW, FSSW process which sheared the Machine-learning approaches like, artificial Neural Network (ANN), Regression model (RSM), Support Vector Machine (SVM) and Adaptive Neuro-Fuzzy Inference System (ANFIS). The main purpose of this study is to review and summarize the emerging research work of machine learning techniques in FSW and FSSW. Previous researchers demonstrate that the Machine Learning applications applied to predict the response of FSW and FSSW process. The prediction in error percentage in result of ANN and RSM model in overall is less than 5\\%. In comparison between ANN/RSM the obtain result shows that ANN is provide better and accurate than RSM. In application of SVM algorithm the prediction accuracy found 100\\% for training and testing process.} }
@article{WOS:001003539600001, title = {Application of Machine Learning in Fuel Cell Research}, journal = {ENERGIES}, volume = {16}, year = {2023}, doi = {10.3390/en16114390}, author = {Su, Danqi and Zheng, Jiayang and Ma, Junjie and Dong, Zizhe and Chen, Zhangjie and Qin, Yanzhou}, abstract = {A fuel cell is an energy conversion device that utilizes hydrogen energy through an electrochemical reaction. Despite their many advantages, such as high efficiency, zero emissions, and fast startup, fuel cells have not yet been fully commercialized due to deficiencies in service life, cost, and performance. Efficient evaluation methods for performance and service life are critical for the design and optimization of fuel cells. The purpose of this paper was to review the application of common machine learning algorithms in fuel cells. The significance and status of machine learning applications in fuel cells are briefly described. Common machine learning algorithms, such as artificial neural networks, support vector machines, and random forests are introduced, and their applications in fuel cell performance prediction and optimization are comprehensively elaborated. The review revealed that machine learning algorithms can be successfully used for performance prediction, service life prediction, and fault diagnosis in fuel cells, with good accuracy in solving nonlinear problems. Combined with optimization algorithms, machine learning models can further carry out the optimization of design and operating parameters to achieve multiple optimization goals with good accuracy and efficiency. It is expected that this review paper could help the reader comprehend the state of the art of machine learning applications in fuel fuels and shed light on further development directions in fuel cell research.} }
@article{WOS:000695210000001, title = {A review on utilizing machine learning technology in the fields of electronic emergency triage and patient priority systems in telemedicine: Coherent taxonomy, motivations, open research challenges and recommendations for intelligent future work}, journal = {COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE}, volume = {209}, year = {2021}, issn = {0169-2607}, doi = {10.1016/j.cmpb.2021.106357}, author = {Salman, Omar H. and Taha, Zahraa and Alsabah, Muntadher Q. and Hussein, Yaseein S. and Mohammed, Ahmed S. and Aal-Nouman, Mohammed}, abstract = {Background: With the remarkable increasing in the numbers of patients, the triaging and prioritizing patients into multi-emergency level is required to accommodate all the patients, save more lives, and manage the medical resources effectively. Triaging and prioritizing patients becomes particularly challenging especially for the patients who are far from hospital and use telemedicine system. To this end, the researchers exploiting the useful tool of machine learning to address this challenge. Hence, carrying out an intensive investigation and in-depth study in the field of using machine learning in E-triage and patient priority are essential and required. Objectives: This research aims to (1) provide a literature review and an in-depth study on the roles of machine learning in the fields of electronic emergency triage (E-triage) and prioritize patients for fast healthcare services in telemedicine applications. (2) highlight the effectiveness of machine learning methods in terms of algorithms, medical input data, output results, and machine learning goals in remote healthcare telemedicine systems. (3) present the relationship between machine learning goals and the electronic triage processes specifically on the: triage levels, medical features for input, outcome results as outputs, and the relevant diseases. (4), the outcomes of our analyses are subjected to organize and propose a cross-over taxonomy between machine learning algorithms and telemedicine structure. (5) present lists of motivations, open research challenges and recommendations for future intelligent work for both academic and industrial sectors in telemedicine and remote healthcare applications. Methods: An intensive research is carried out by reviewing all articles related to the field of E-triage and remote priority systems that utilise machine learning algorithms and sensors. We have searched all related keywords to investigate the databases of Science Direct, IEEE Xplore, Web of Science, PubMed, and Medline for the articles, which have been published from January 2012 up to date. Results: A new crossover matching between machine learning methods and telemedicine taxonomy is proposed. The crossover-taxonomy is developed in this study to identify the relationship between machine learning algorithm and the equivalent telemedicine categories whereas the machine learning algorithm has been utilized. The impact of utilizing machine learning is composed in proposing the telemedicine architecture based on synchronous (real-time/ online) and asynchronous (store-and-forward / offline) structure. In addition to that, list of machine learning algorithms, list of the performance metrics, list of inputs data and outputs results are presented. Moreover, open research challenges, the benefits of utilizing machine learning and the recommendations for new research opportunities that need to be addressed for the synergistic integration of multidisciplinary works are organized and presented accordingly. Discussion: The state-of-the-art studies on the E-triage and priority systems that utilise machine learning algorithms in telemedicine architecture are discussed. This approach allows the researchers to understand the modernisation of healthcare systems and the efficient use of artificial intelligence and machine learning. In particular, the growing worldwide population and various chronic diseases such as heart chronic diseases, blood pressure and diabetes, require smart health monitoring systems in E-triage and priority systems, in which machine learning algorithms could be greatly beneficial. Conclusions: Although research directions on E-triage and priority systems that use machine learning algorithms in telemedicine vary, they are equally essential and should be considered. Hence, we provide a comprehensive review to emphasise the advantages of the existing research in multidisciplinary works of artificial intelligence, machine learning and healthcare services. (c) 2021 Elsevier B.V. All rights reserved.} }
@article{WOS:000669541400017, title = {Implementation of Quantum Machine Learning for Electronic Structure Calculations of Periodic Systems on Quantum Computing Devices}, journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING}, volume = {61}, pages = {2667-2674}, year = {2021}, issn = {1549-9596}, doi = {10.1021/acs.jcim.1c00294}, author = {Sureshbabu, Shree Hari and Sajjan, Manas and Oh, Sangchul and Kais, Sabre}, abstract = {Quantum machine learning algorithms, the extensions of machine learning to quantum regimes, are believed to be more powerful as they leverage the power of quantum properties. Quantum machine learning methods have been employed to solve quantum many-body systems and have demonstrated accurate electronic structure calculations of lattice models, molecular systems, and recently periodic systems. A hybrid approach using restricted Boltzmann machines and a quantum algorithm to obtain the probability distribution that can be optimized classically is a promising method due to its efficiency and ease of implementation. Here, we implement the benchmark test of the hybrid quantum machine learning on the IBM-Q quantum computer to calculate the electronic structure of typical two-dimensional crystal structures: hexagonal-boron nitride and graphene. The band structures of these systems calculated using the hybrid quantum machine learning approach are in good agreement with those obtained by the conventional electronic structure calculations. This benchmark result implies that the hybrid quantum machine learning method, empowered by quantum computers, could provide a new way of calculating the electronic structures of quantum many-body systems.} }
@article{WOS:001306200600001, title = {Machine learning to predict the production of bio-oil, biogas, and biochar by pyrolysis of biomass: a review}, journal = {ENVIRONMENTAL CHEMISTRY LETTERS}, volume = {22}, pages = {2669-2698}, year = {2024}, issn = {1610-3653}, doi = {10.1007/s10311-024-01767-7}, author = {Khandelwal, Kapil and Nanda, Sonil and Dalai, Ajay K.}, abstract = {The world energy consumption has increased by + 195\\% since 1970 with more than 80\\% of the energy mix originating from fossil fuels, thus leading to pollution and global warming. Alternatively, pyrolysis of modern biomass is considered carbon neutral and produces value-added biogas, bio-oils, and biochar, yet actual pyrolysis processes are not fully optimized. Here, we review the use of machine learning to improve the pyrolysis of lignocellulosic biomass, with emphasis on machine learning algorithms and prediction of product characteristics. Algorithms comprise regression analysis, artificial neural networks, decision trees, and the support vector machine. Machine learning allows for the prediction of yield, quality, surface area, reaction kinetics, techno-economics, and lifecycle assessment of biogas, bio-oil, and biochar. The robustness of machine learning techniques and engineering applications are discussed.} }
@article{WOS:000670096500001, title = {Machine learning to advance the prediction, prevention and treatment of eating disorders}, journal = {EUROPEAN EATING DISORDERS REVIEW}, volume = {29}, pages = {683-691}, year = {2021}, issn = {1072-4133}, doi = {10.1002/erv.2850}, author = {Wang, Shirley B.}, abstract = {Machine learning approaches are just emerging in eating disorders research. Promising early results suggest that such approaches may be a particularly promising and fruitful future direction. However, there are several challenges related to the nature of eating disorders in building robust, reliable and clinically meaningful prediction models. This article aims to provide a brief introduction to machine learning and to discuss several such challenges, including issues of sample size, measurement, imbalanced data and bias; I also provide concrete steps and recommendations for each of these issues. Finally, I outline key outstanding questions and directions for future research in building, testing and implementing machine learning models to advance our prediction, prevention, and treatment of eating disorders. Highlights Machine learning holds significant promise to advance eating disorders research Some key considerations for responsible machine learning application in eating disorders research include issues of sample size, measurement, imbalanced data and bias Future research should prioritize external validation of machine learning models} }
@article{WOS:000658810900038, title = {QUBO formulations for training machine learning models}, journal = {SCIENTIFIC REPORTS}, volume = {11}, year = {2021}, issn = {2045-2322}, doi = {10.1038/s41598-021-89461-4}, author = {Date, Prasanna and Arthur, Davis and Pusey-Nazzaro, Lauren}, abstract = {Training machine learning models on classical computers is usually a time and compute intensive process. With Moore's law nearing its inevitable end and an ever-increasing demand for large-scale data analysis using machine learning, we must leverage non-conventional computing paradigms like quantum computing to train machine learning models efficiently. Adiabatic quantum computers can approximately solve NP-hard problems, such as the quadratic unconstrained binary optimization (QUBO), faster than classical computers. Since many machine learning problems are also NP-hard, we believe adiabatic quantum computers might be instrumental in training machine learning models efficiently in the post Moore's law era. In order to solve problems on adiabatic quantum computers, they must be formulated as QUBO problems, which is very challenging. In this paper, we formulate the training problems of three machine learning models-linear regression, support vector machine (SVM) and balanced k-means clustering-as QUBO problems, making them conducive to be trained on adiabatic quantum computers. We also analyze the computational complexities of our formulations and compare them to corresponding state-of-the-art classical approaches. We show that the time and space complexities of our formulations are better (in case of SVM and balanced k-means clustering) or equivalent (in case of linear regression) to their classical counterparts.} }
@article{WOS:000571258300007, title = {Machine learning for active matter}, journal = {NATURE MACHINE INTELLIGENCE}, volume = {2}, pages = {94-103}, year = {2020}, doi = {10.1038/s42256-020-0146-9}, author = {Cichos, Frank and Gustavsson, Kristian and Mehlig, Bernhard and Volpe, Giovanni}, abstract = {The availability of large datasets has boosted the application of machine learning in many fields and is now starting to shape active-matter research as well. Machine learning techniques have already been successfully applied to active-matter data-for example, deep neural networks to analyse images and track objects, and recurrent nets and random forests to analyse time series. Yet machine learning can also help to disentangle the complexity of biological active matter, helping, for example, to establish a relation between genetic code and emergent bacterial behaviour, to find navigation strategies in complex environments, and to map physical cues to animal behaviours. In this Review, we highlight the current state of the art in the application of machine learning to active matter and discuss opportunities and challenges that are emerging. We also emphasize how active matter and machine learning can work together for mutual benefit. This Review surveys machine learning techniques that are currently developed for a range of research topics in biological and artificial active matter and also discusses challenges and exciting opportunities. This research direction promises to help disentangle the complexity of active matter and gain fundamental insights for instance in collective behaviour of systems at many length scales from colonies of bacteria to animal flocks.} }
@article{WOS:000918184200006, title = {Crop yield prediction using machine learning techniques}, journal = {ADVANCES IN ENGINEERING SOFTWARE}, volume = {175}, year = {2023}, issn = {0965-9978}, doi = {10.1016/j.advengsoft.2022.103326}, author = {Iniyan, S. and Varma, V. Akhil and Naidu, Ch Teja}, abstract = {Machine Learning is a successful dynamic device for foreseeing crop yields, just as for choosing which harvests to plant and what to do about them during the developing season. Since it operates with a large amount of data produced by several variables, the farming system is highly complicated. Methods of machine learning can aid intelligent system decision-making. The following paper investigates a variety of methods for predicting crop yields using a variety of soil and environmental variables. The main purpose of this project is to make a machine learning model make predictions. By taking into account several variables, machine learning algorithms can help farmers decide which crop to grow in addition to increasing yield. Farmers can benefit from yield estimation because it allows them to minimize crop loss and obtain the best prices for their crops. A machine learning model may be descriptive or predictive, depending on the research question and study objectives.} }
@article{WOS:000636526200001, title = {Fault diagnosis of various rotating equipment using machine learning approaches - A review}, journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART E-JOURNAL OF PROCESS MECHANICAL ENGINEERING}, volume = {235}, pages = {629-642}, year = {2021}, issn = {0954-4089}, doi = {10.1177/0954408920971976}, author = {Manikandan, S. and Duraivelu, K.}, abstract = {Fault diagnosis of various rotating equipment plays a significant role in industries as it guarantees safety, reliability and prevents breakdown and loss of any source of energy. Early identification is a fundamental aspect for diagnosing the faults which saves both time and costs and in fact it avoids perilous conditions. Investigations are being carried out for intelligent fault diagnosis using machine learning approaches. This article analyses various machine learning approaches used for fault diagnosis of rotating equipment. In addition to this, a detailed study of different machine learning strategies which are incorporated on various rotating equipment in the context of fault diagnosis is also carried out. Mainly, the benefits and advance patterns of deep neural network which are applied to multiple components for fault diagnosis are inspected in this study. Finally, different algorithms are proposed to propagate the quality of fault diagnosis and the conceivable research ideas of applying machine learning approaches on various rotating equipment are condensed in this article.} }
@article{WOS:000588277100001, title = {Tutorial: Applying Machine Learning in Behavioral Research}, journal = {PERSPECTIVES ON BEHAVIOR SCIENCE}, volume = {43}, pages = {697-723}, year = {2020}, issn = {2520-8969}, doi = {10.1007/s40614-020-00270-y}, author = {Turgeon, Stephanie and Lanovaz, Marc J.}, abstract = {Machine-learning algorithms hold promise for revolutionizing how educators and clinicians make decisions. However, researchers in behavior analysis have been slow to adopt this methodology to further develop their understanding of human behavior and improve the application of the science to problems of applied significance. One potential explanation for the scarcity of research is that machine learning is not typically taught as part of training programs in behavior analysis. This tutorial aims to address this barrier by promoting increased research using machine learning in behavior analysis. We present how to apply the random forest, support vector machine, stochastic gradient descent, and k-nearest neighbors algorithms on a small dataset to better identify parents of children with autism who would benefit from a behavior analytic interactive web training. These step-by-step applications should allow researchers to implement machine-learning algorithms with novel research questions and datasets.} }
@article{WOS:001283397300001, title = {Towards advanced uncertainty and sensitivity analysis of building energy performance using machine learning techniques}, journal = {JOURNAL OF BUILDING PERFORMANCE SIMULATION}, volume = {17}, pages = {655-662}, year = {2024}, issn = {1940-1493}, doi = {10.1080/19401493.2024.2387071}, author = {Tian, Wei}, abstract = {Uncertainty analysis quantifies the inherently uncertain nature of building energy performance, whereas sensitivity analysis identifies key factors to explain variations in building energy performance. With the ability to handle complex relationships, machine learning techniques offer an effective approach to more accurate and reliable uncertainty and sensitivity analysis. This paper provides valuable insights into the current state and future prospects of machine learning-based uncertainty and sensitivity analysis for building energy performance. The development of machine learning-based uncertainty analysis is discussed from three perspectives: observational data-based probabilistic prediction, surrogate model-based uncertainty quantification, and inverse uncertainty quantification. Variance-based sensitivity analysis using surrogate machine learning models decomposes output variance associated with each input. In contrast, machine learning-based variable importance refers to the change of model predictive performance using model-specific or model-agnostic approaches. Finally, future research directions on machine learning-based uncertainty and sensitivity analysis of building energy performance are presented.} }
@article{WOS:000557303500009, title = {Machine Learning With Neuroimaging: Evaluating Its Applications in Psychiatry}, journal = {BIOLOGICAL PSYCHIATRY-COGNITIVE NEUROSCIENCE AND NEUROIMAGING}, volume = {5}, pages = {791-798}, year = {2020}, issn = {2451-9022}, doi = {10.1016/j.bpsc.2019.11.007}, author = {Nielsen, Ashley N. and Barch, Deanna M. and Petersen, Steven E. and Schlaggar, Bradley L. and Greene, Deanna J.}, abstract = {Psychiatric disorders are complex, involving heterogeneous symptomatology and neurobiology that rarely involves the disruption of single, isolated brain structures. In an attempt to better describe and understand the complexities of psychiatric disorders, investigators have increasingly applied multivariate pattern classification approaches to neuroimaging data and in particular supervised machine learning methods. However, supervised machine learning approaches also come with unique challenges and trade-offs, requiring additional study design and interpretation considerations. The goal of this review is to provide a set of best practices for evaluating machine learning applications to psychiatric disorders. We discuss how to evaluate two common efforts: 1) making predictions that have the potential to aid in diagnosis, prognosis, and treatment and 2) interrogating the complex neurophysiological mechanisms underlying psychopathology. We focus here on machine learning as applied to functional connectivity with magnetic resonance imaging, as an example to ground discussion. We argue that for machine learning classification to have translational utility for individual-level predictions, investigators must ensure that the classification is clinically informative, independent of confounding variables, and appropriately assessed for both performance and generalizability. We contend that shedding light on the complex mechanisms underlying psychiatric disorders will require consideration of the unique utility, interpretability, and reliability of the neuroimaging features (e.g., regions, networks, connections) identified from machine learning approaches. Finally, we discuss how the rise of large, multisite, publicly available datasets may contribute to the utility of machine learning approaches in psychiatry.} }
@article{WOS:000577262800006, title = {Retrospective on a decade of machine learning for chemical discovery}, journal = {NATURE COMMUNICATIONS}, volume = {11}, year = {2020}, doi = {10.1038/s41467-020-18556-9}, author = {von Lilienfeld, O. Anatole and Burke, Kieron}, abstract = {Over the last decade, we have witnessed the emergence of ever more machine learning applications in all aspects of the chemical sciences. Here, we highlight specific achievements of machine learning models in the field of computational chemistry by considering selected studies of electronic structure, interatomic potentials, and chemical compound space in chronological order.} }
@article{WOS:000428618900040, title = {Using human brain activity to guide machine learning}, journal = {SCIENTIFIC REPORTS}, volume = {8}, year = {2018}, issn = {2045-2322}, doi = {10.1038/s41598-018-23618-6}, author = {Fong, Ruth C. and Scheirer, Walter J. and Cox, David D.}, abstract = {Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of ``neurally-weighted'' machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data.} }
@article{WOS:000928006100002, title = {Context-Aware Machine Learning for Intelligent Transportation Systems: A Survey}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {24}, pages = {17-36}, year = {2023}, issn = {1524-9050}, doi = {10.1109/TITS.2022.3216462}, author = {Huang, Guang-Li and Zaslavsky, Arkady and Loke, Seng W. and Abkenar, Amin and Medvedev, Alexey and Hassani, Alireza}, abstract = {Context awareness adds intelligence to and enriches data for applications, services and systems while enabling underlying algorithms to sense dynamic changes in incoming data streams. Context-aware machine learning is often adopted in intelligent services by endowing meaning to Internet of Things(IoT)/ubiquitous data. Intelligent transportation systems (ITS) are at the forefront of applying context awareness with marked success. In contrast to non-context-aware machine learning models, context-aware machine learning models often perform better in traffic prediction/classification and are capable of supporting complex and more intelligent ITS decision-making. This paper presents a comprehensive review of recent studies in context-aware machine learning for intelligent transportation, especially focusing on road transportation systems. State-of-the-art techniques are discussed from several perspectives, including contextual data (e.g., location, time, weather, road condition and events), applications (i.e., traffic prediction and decision making), modes (i.e., specialised and general), learning methods (e.g., supervised, unsupervised, semi-supervised and transfer learning). Two main frameworks of context-aware machine learning models are summarised. In addition, open challenges and future research directions of developing context-aware machine learning models for ITS are discussed, and a novel context-aware machine learning layered engine (CAMILLE) architecture is proposed as a potential solution to address identified gaps in the studied body of knowledge.} }
@article{WOS:001130213200001, title = {Machine Learning Algorithm for Malware Detection: Taxonomy, Current Challenges, and Future Directions}, journal = {IEEE ACCESS}, volume = {11}, pages = {141045-141089}, year = {2023}, issn = {2169-3536}, doi = {10.1109/ACCESS.2023.3256979}, author = {Gorment, Nor Zakiah and Selamat, Ali and Cheng, Lim Kok and Krejcar, Ondrej}, abstract = {Malware has emerged as a cyber security threat that continuously changes to target computer systems, smart devices, and extensive networks with the development of information technologies. As a result, malware detection has always been a major worry and a difficult issue, owing to shortcomings in performance accuracy, analysis type, and malware detection approaches that fail to identify unexpected malware attacks. This paper seeks to conduct a thorough systematic literature review (SLR) and offer a taxonomy of machine learning methods for malware detection that considers these problems by analyzing 77 chosen research works related to malware detection using machine learning algorithm. The research investigates malware and machine learning in the context of cybersecurity, including malware detection taxonomy and machine learning algorithm classification into numerous categories. Furthermore, the taxonomy was used to evaluate the most recent machine learning algorithm and analysis. The paper also examines the obstacles and associated concerns encountered in malware detection and potential remedies. Finally, to address the related issues that would motivate researchers in their future work, an empirical study was utilized to assess the performance of several machine learning algorithms.} }
@article{WOS:000639523700001, title = {Artificial intelligence to deep learning: machine intelligence approach for drug discovery}, journal = {MOLECULAR DIVERSITY}, volume = {25}, pages = {1315-1360}, year = {2021}, issn = {1381-1991}, doi = {10.1007/s11030-021-10217-3}, author = {Gupta, Rohan and Srivastava, Devesh and Sahu, Mehar and Tiwari, Swati and Ambasta, Rashmi K. and Kumar, Pravir}, abstract = {Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure-activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. Graphic abstract The primary concern associated with drug design and development is time consumption and production cost. Further, inefficiency, inaccurate target delivery, and inappropriate dosage are other hurdles that inhibit the process of drug delivery and development. With advancements in technology, computer-aided drug design integrating artificial intelligence algorithms can eliminate the challenges and hurdles of traditional drug design and development. Artificial intelligence is referred to as superset comprising machine learning, whereas machine learning comprises supervised learning, unsupervised learning, and reinforcement learning. Further, deep learning, a subset of machine learning, has been extensively implemented in drug design and development. The artificial neural network, deep neural network, support vector machines, classification and regression, generative adversarial networks, symbolic learning, and meta-learning are examples of the algorithms applied to the drug design and discovery process. Artificial intelligence has been applied to different areas of drug design and development process, such as from peptide synthesis to molecule design, virtual screening to molecular docking, quantitative structure-activity relationship to drug repositioning, protein misfolding to protein-protein interactions, and molecular pathway identification to polypharmacology. Artificial intelligence principles have been applied to the classification of active and inactive, monitoring drug release, pre-clinical and clinical development, primary and secondary drug screening, biomarker development, pharmaceutical manufacturing, bioactivity identification and physiochemical properties, prediction of toxicity, and identification of mode of action.} }
@article{WOS:000668074700001, title = {An Ensemble Machine Learning Model for Enhancing the Prediction Accuracy of Energy Consumption in Buildings}, journal = {ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING}, volume = {47}, pages = {4105-4117}, year = {2022}, issn = {2193-567X}, doi = {10.1007/s13369-021-05927-7}, author = {Ngoc-Tri Ngo and Anh-Duc Pham and Thi Thu Ha Truong and Ngoc-Son Truong and Nhat-To Huynh and Tuan Minh Pham}, abstract = {Predicting building energy use is necessary for energy planning, management, and conservation. It is difficult to achieve accurate prediction results due to the inherent complexity of building thermal characteristics and occupant behavior. Machine learning has been recently applied for predicting energy consumption. Improving its predictive accuracy and generalization ability is essential. Therefore, this study proposed a machine learning model for an ensemble approach to forecasting energy consumption in non-residential buildings. Various datasets from non-residential buildings were collected to assess the predictive performance. Artificial neural networks, support vector regression, and M5Rules models were used as baseline models in this study. Evaluation results have confirmed the effectiveness of the ensemble machine learning model in the next 24-h energy consumption prediction in buildings. The mean absolute error (MAE) and mean absolute percentage error (MAPE) obtained by the ensemble machine learning model were 2.858 kWh and 16.141 kWh, respectively. The ensemble machine learning model can improve the MAE by 123.4\\% and the MAPE by 209.3\\% as compared to baseline models. This study contributes to highlighting the advantages of machine learning applications for the building sector. Ensemble machine learning models can be proposed as an effective method for forecasting energy consumption in buildings.} }
@article{WOS:000929540300001, title = {Insights into the Application of Machine Learning in Reservoir Engineering: Current Developments and Future Trends}, journal = {ENERGIES}, volume = {16}, year = {2023}, doi = {10.3390/en16031392}, author = {Wang, Hai and Chen, Shengnan}, abstract = {In the past few decades, the machine learning (or data-driven) approach has been broadly adopted as an alternative to scientific discovery, resulting in many opportunities and challenges. In the oil and gas sector, subsurface reservoirs are heterogeneous porous media involving a large number of complex phenomena, making their characterization and dynamic prediction a real challenge. This study provides a comprehensive overview of recent research that has employed machine learning in three key areas: reservoir characterization, production forecasting, and well test interpretation. The results show that machine learning can automate and accelerate many reservoirs engineering tasks with acceptable level of accuracy, resulting in more efficient and cost-effective decisions. Although machine learning presents promising results at this stage, there are still several crucial challenges that need to be addressed, such as data quality and data scarcity, the lack of physics nature of machine learning algorithms, and joint modelling of multiple data sources/formats. The significance of this research is that it demonstrates the potential of machine learning to revolutionize the oil and gas sector by providing more accurate and efficient solutions for challenging problems.} }
@article{WOS:000760455600001, title = {Machine Learning in Assessing the Performance of Hydrological Models}, journal = {HYDROLOGY}, volume = {9}, year = {2022}, doi = {10.3390/hydrology9010005}, author = {Rozos, Evangelos and Dimitriadis, Panayiotis and Bellos, Vasilis}, abstract = {Machine learning has been employed successfully as a tool virtually in every scientific and technological field. In hydrology, machine learning models first appeared as simple feed-forward networks that were used for short-term forecasting, and have evolved into complex models that can take into account even the static features of catchments, imitating the hydrological experience. Recent studies have found machine learning models to be robust and efficient, frequently outperforming the standard hydrological models (both conceptual and physically based). However, and despite some recent efforts, the results of the machine learning models require significant effort to interpret and derive inferences. Furthermore, all successful applications of machine learning in hydrology are based on networks of fairly complex topology that require significant computational power and CPU time to train. For these reasons, the value of the standard hydrological models remains indisputable. In this study, we suggest employing machine learning models not as a substitute for hydrological models, but as an independent tool to assess their performance. We argue that this approach can help to unveil the anomalies in catchment data that do not fit in the employed hydrological model structure or configuration, and to deal with them without compromising the understanding of the underlying physical processes.} }
@article{WOS:000640517400008, title = {Human and Machine Learning}, journal = {COMPUTATIONAL ECONOMICS}, volume = {57}, pages = {889-909}, year = {2021}, issn = {0927-7099}, doi = {10.1007/s10614-018-9803-z}, author = {Kao, Ying-Fang and Venkatachalam, Ragupathy}, abstract = {In this paper, we consider learning by human beings and machines in the light of Herbert Simon's pioneering contributions to the theory of Human Problem Solving. Using board games of perfect information as a paradigm, we explore differences in human and machine learning in complex strategic environments. In doing so, we contrast theories of learning in classical game theory with computational game theory proposed by Simon. Among theories that invoke computation, we make a further distinction between computable and computational or machine learning theories. We argue that the modern machine learning algorithms, although impressive in terms of their performance, do not necessarily shed enough light on human learning. Instead, they seem to take us further away from Simon's lifelong quest to understand the mechanics of actual human behaviour.} }
@article{WOS:000727217500001, title = {Artificial Intelligence Methodologies for Data Management}, journal = {SYMMETRY-BASEL}, volume = {13}, year = {2021}, doi = {10.3390/sym13112040}, author = {Serey, Joel and Quezada, Luis and Alfaro, Miguel and Fuertes, Guillermo and Vargas, Manuel and Ternero, Rodrigo and Sabattin, Jorge and Duran, Claudia and Gutierrez, Sebastian}, abstract = {This study analyses the main challenges, trends, technological approaches, and artificial intelligence methods developed by new researchers and professionals in the field of machine learning, with an emphasis on the most outstanding and relevant works to date. This literature review evaluates the main methodological contributions of artificial intelligence through machine learning. The methodology used to study the documents was content analysis; the basic terminology of the study corresponds to machine learning, artificial intelligence, and big data between the years 2017 and 2021. For this study, we selected 181 references, of which 120 are part of the literature review. The conceptual framework includes 12 categories, four groups, and eight subgroups. The study of data management using AI methodologies presents symmetry in the four machine learning groups: supervised learning, unsupervised learning, semi-supervised learning, and reinforced learning. Furthermore, the artificial intelligence methods with more symmetry in all groups are artificial neural networks, Support Vector Machines, K-means, and Bayesian Methods. Finally, five research avenues are presented to improve the prediction of machine learning.} }
@article{WOS:000441141200005, title = {Discrete space reinforcement learning algorithm based on support vector machine classification}, journal = {PATTERN RECOGNITION LETTERS}, volume = {111}, pages = {30-35}, year = {2018}, issn = {0167-8655}, doi = {10.1016/j.patrec.2018.04.012}, author = {An, Yuexuan and Ding, Shifei and Shi, Songhui and Li, Jingcan}, abstract = {When facing discrete space learning problems, the traditional reinforcement learning algorithms often have the problems of slow convergence and poor convergence accuracy. Deep reinforcement learning needs a large number of learning samples in its learning process, so it often faces with the problems that the algorithm is difficult to converge and easy to fall into local minimums. In view of the above problems, we apply support vector machines classification to reinforcement learning, and propose an algorithm named Advantage Actor-Critic with Support Vector Machine Classification (SVM-A2C). Our algorithm adopts the actor-critic framework and uses the support vector machine classification as a result of the actor's action output, while Critic uses the advantage function to improve and optimize the parameters of support vector machine. In addition, since the environment is changing all the time in reinforcement learning, it is difficult to find a global optimal solution for the support vector machines, the gradient descent method is applied to optimize the parameters of support vector machine. So that the agent can quickly learn a more precise action selection policy. Finally, the effectiveness of the proposed method is proved by the classical experimental environment of reinforcement learning. It is proved that the algorithm proposed in this paper has shorter episodes to convergence and more accurate results than other algorithms. (C) 2018 Elsevier B.V. All rights reserved.} }
@article{WOS:000401789000024, title = {A Review of Machine Learning Applications in Veterinary Field}, journal = {KAFKAS UNIVERSITESI VETERINER FAKULTESI DERGISI}, volume = {23}, pages = {673-680}, year = {2017}, issn = {1300-6045}, doi = {10.9775/kvfd.2016.17281}, author = {Cihan, Pinar and Gokce, Erhan and Kalipsiz, Oya}, abstract = {Machine learning is a sub field of artificial intelligence which allows forecasting through learning past behaviors and rules from old data. In today's world, machine learning is being used almost in any fields such as education, medicine, veterinary, banking, telecommunication, security, and bio-medical sciences. In human health, although machine learning is generally preferred particularly in predicting diseases and identifying respective risk factors, it is obvious that there are a limited number of publications where this method was applied on veterinary or indicates whether it is correct and applicable. In this review, it was observed that the neural network, logistic regression, linear regression, multiple regression, principle component analysis and k-means methods were frequently used in examined publications and machine learning application in veterinary field upward momentum. Additionally, it was observed that recent developments in the field of machine learning (deep learning, ensemble learning, voice recognition, emotion recognition, etc.) is still new in the field of veterinary. In this review, publications are examined under clustering, classification, regression, multivariate data analysis and image processing topics. This review aims at providing basic information on machine learning and to increase the number of multidisciplinary publications on computer sciences/engineering and veterinary field.} }
@article{WOS:000342248100012, title = {Online sequential extreme learning machine with kernels for nonstationary time series prediction}, journal = {NEUROCOMPUTING}, volume = {145}, pages = {90-97}, year = {2014}, issn = {0925-2312}, doi = {10.1016/j.neucom.2014.05.068}, author = {Wang, Xinying and Han, Min}, abstract = {In this paper, an online sequential extreme learning machine with kernels (OS-ELMK) has been proposed for nonstationary time series prediction. An online sequential learning algorithm, which can learn samples one-by-one or chunk-by-chunk, is developed for extreme learning machine with kernels. A limited memory prediction strategy based on the proposed OS-ELMK is designed to model the nonstationary time series. Performance comparisons of OS-ELMK with other existing algorithms are presented using artificial and real life nonstationary time series data. The results show that the proposed OS-ELMK produces similar or better accuracies with at least an order-of-magnitude reduction in the learning time. (C) 2014 Elsevier B.V. All rights reserved.} }
@article{WOS:000506280400001, title = {Machine-learning-based damage identification methods with features derived from moving principal component analysis}, journal = {MECHANICS OF ADVANCED MATERIALS AND STRUCTURES}, volume = {27}, pages = {1789-1802}, year = {2020}, issn = {1537-6494}, doi = {10.1080/15376494.2019.1710308}, author = {Zhang, Ge and Tang, Liqun and Liu, Zejia and Zhou, Licheng and Liu, Yiping and Jiang, Zhenyu}, abstract = {This paper aims to propose machine-learning-based damage identification methods with features derived from moving principal component analysis (MPCA) to improve the damage identification performance for engineering structures. Previously, machine learning algorithms have usually used structural responses as inputs directly. These methods show low damage identification capabilities and are susceptible to noise. In this paper, the eigenvectors of structural responses derived from MPCA are employed as inputs instead. Several traditional machine learning algorithms are applied for verification. The results demonstrate that as compared to strains and frequencies, their eigenvectors as inputs for machine learning algorithms render better performances for damage identification.} }
@article{WOS:000601113600025, title = {How Machine Learning Accelerates the Development of Quantum Dots?†}, journal = {CHINESE JOURNAL OF CHEMISTRY}, volume = {39}, pages = {181-188}, year = {2021}, issn = {1001-604X}, doi = {10.1002/cjoc.202000393}, author = {Peng, Jia and Muhammad, Ramzan and Wang, Shu-Liang and Zhong, Hai-Zheng}, abstract = {With the rapid developments in the field of information technology, the material research society is looking for an alternate scientific route to the traditional methods of trial and error in material research and process development. Machine learning emerges as a new research paradigm to accelerate the application-oriented material discovery. Quantum dots are expanded as functional nanomaterials to enhance cutting-edge photonic technology. However, they suffer from uncertainty in industrial fabrication and application. Here, we discuss how machine learning accelerates the development of quantum dots. The basic principles and operation procedures of machine learning are described with a few representative examples of quantum dots. We emphasize how machine learning contributes to the optimization of synthesis and the analysis of material characterizations. To conclude, we give a short perspective discussing the problems of combining machine learning and quantum dots.} }
@article{WOS:000500381500027, title = {Applications of machine learning methods for engineering risk assessment - A review}, journal = {SAFETY SCIENCE}, volume = {122}, year = {2020}, issn = {0925-7535}, doi = {10.1016/j.ssci.2019.09.015}, author = {Hegde, Jeevith and Rokseth, Borge}, abstract = {The purpose of this article is to present a structured review of publications utilizing machine learning methods to aid in engineering risk assessment. A keyword search is performed to retrieve relevant articles from the databases of Scopus and Engineering Village. The search results are filtered according to seven selection criteria. The filtering process resulted in the retrieval of one hundred and twenty-four relevant research articles. Statistics based on different categories from the citation database is presented. By reviewing the articles, additional categories, such as the type of machine learning algorithm used, the type of input source used, the type of industry targeted, the type of implementation, and the intended risk assessment phase are also determined. The findings show that the automotive industry is leading the adoption of machine learning algorithms for risk assessment. Artificial neural networks are the most applied machine learning method to aid in engineering risk assessment. Additional findings from the review process are also presented in this article.} }
@article{WOS:000379822600001, title = {Machine learning in manufacturing: advantages, challenges, and applications}, journal = {PRODUCTION AND MANUFACTURING RESEARCH-AN OPEN ACCESS JOURNAL}, volume = {4}, pages = {23-45}, year = {2016}, doi = {10.1080/21693277.2016.1192517}, author = {Wuest, Thorsten and Weimer, Daniel and Irgens, Christopher and Thoben, Klaus-Dieter}, abstract = {The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.} }
@article{WOS:000510903200023, title = {Diagnosing Rotating Machines With Weakly Supervised Data Using Deep Transfer Learning}, journal = {IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}, volume = {16}, pages = {1688-1697}, year = {2020}, issn = {1551-3203}, doi = {10.1109/TII.2019.2927590}, author = {Li, Xiang and Zhang, Wei and Ding, Qian and Li, Xu}, abstract = {Rotating machinery fault diagnosis problems have been well-addressed when sufficient supervised data of the tested machine are available using the latest data-driven methods. However, it is still challenging to develop effective diagnostic method with insufficient training data, which is highly demanded in real-industrial scenarios, since high-quality data are usually difficult and expensive to collect. Considering the underlying similarities of rotating machines, data mining on different but related equipments potentially benefit the diagnostic performance on the target machine. Therefore, a novel transfer learning method for diagnostics based on deep learning is proposed in this article, where the diagnostic knowledge learned from sufficient supervised data of multiple rotating machines is transferred to the target equipment with domain adversarial training. Different from the existing studies, a more generalized transfer learning problem with different label spaces of domains is investigated, and different fault severities are also considered in fault diagnostics. The experimental results on four datasets validate the effectiveness of the proposed method, and show it is feasible and promising to explore different datasets to improve diagnostic performance.} }
@article{WOS:000521984900003, title = {Presenting machine learning model information to clinical end users with model facts labels}, journal = {NPJ DIGITAL MEDICINE}, volume = {3}, year = {2020}, issn = {2398-6352}, doi = {10.1038/s41746-020-0253-3}, author = {Sendak, Mark P. and Gao, Michael and Brajer, Nathan and Balu, Suresh}, abstract = {There is tremendous enthusiasm surrounding the potential for machine learning to improve medical prognosis and diagnosis. However, there are risks to translating a machine learning model into clinical care and clinical end users are often unaware of the potential harm to patients. This perspective presents the ``Model Facts'' label, a systematic effort to ensure that front-line clinicians actually know how, when, how not, and when not to incorporate model output into clinical decisions. The ``Model Facts'' label was designed for clinicians who make decisions supported by a machine learning model and its purpose is to collate relevant, actionable information in 1-page. Practitioners and regulators must work together to standardize presentation of machine learning model information to clinical end users in order to prevent harm to patients. Efforts to integrate a model into clinical practice should be accompanied by an effort to clearly communicate information about a machine learning model with a ``Model Facts'' label.} }
@article{WOS:001441035000001, title = {Advancements in cache management: a review of machine learning innovations for enhanced performance and security}, journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE}, volume = {8}, year = {2025}, doi = {10.3389/frai.2025.1441250}, author = {Krishna, Keshav}, abstract = {Machine learning techniques have emerged as a promising tool for efficient cache management, helping optimize cache performance and fortify against security threats. The range of machine learning is vast, from reinforcement learning-based cache replacement policies to Long Short-Term Memory (LSTM) models predicting content characteristics for caching decisions. Diverse techniques such as imitation learning, reinforcement learning, and neural networks are extensively useful in cache-based attack detection, dynamic cache management, and content caching in edge networks. The versatility of machine learning techniques enables them to tackle various cache management challenges, from adapting to workload characteristics to improving cache hit rates in content delivery networks. A comprehensive review of various machine learning approaches for cache management is presented, which helps the community learn how machine learning is used to solve practical challenges in cache management. It includes reinforcement learning, deep learning, and imitation learning-driven cache replacement in hardware caches. Information on content caching strategies and dynamic cache management using various machine learning techniques in cloud and edge computing environments is also presented. Machine learning-driven methods to mitigate security threats in cache management have also been discussed.} }
@article{WOS:000641162100002, title = {A Comprehensive Survey on Graph Neural Networks}, journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS}, volume = {32}, pages = {4-24}, year = {2021}, issn = {2162-237X}, doi = {10.1109/TNNLS.2020.2978386}, author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.}, abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.} }
@article{WOS:000572537200011, title = {Machine learning in materials genome initiative: A review}, journal = {JOURNAL OF MATERIALS SCIENCE \\& TECHNOLOGY}, volume = {57}, pages = {113-122}, year = {2020}, issn = {1005-0302}, doi = {10.1016/j.jmst.2020.01.067}, author = {Liu, Yingli and Niu, Chen and Wang, Zhuo and Gan, Yong and Zhu, Yan and Sun, Shuhong and Shen, Tao}, abstract = {Discovering new materials with excellent performance is a hot issue in the materials genome initiative. Traditional experiments and calculations often waste large amounts of time and money and are also limited by various conditions. Therefore, it is imperative to develop a new method to accelerate the discovery and design of new materials. In recent years, material discovery and design methods using machine learning have attracted much attention from material experts and have made some progress. This review first outlines available materials database and material data analytics tools and then elaborates on the machine learning algorithms used in materials science. Next, the field of application of machine learning in materials science is summarized, focusing on the aspects of structure determination, performance prediction, fingerprint prediction, and new material discovery. Finally, the review points out the problems of data and machine learning in materials science and points to future research. Using machine learning algorithms, the authors hope to achieve amazing results in material discovery and design. (C) 2020 Published by Elsevier Ltd on behalf of The editorial office of Journal of Materials Science \\& Technology.} }
@article{WOS:000823964300001, title = {Machine-designed biotherapeutics: opportunities, feasibility and advantages of deep learning in computational antibody discovery}, journal = {BRIEFINGS IN BIOINFORMATICS}, volume = {23}, year = {2022}, issn = {1467-5463}, doi = {10.1093/bib/bbac267}, author = {Wilman, Wiktoria and Wrobel, Sonia and Bielska, Weronika and Deszynski, Piotr and Dudzic, Pawel and Jaszczyszyn, Igor and Kaniewski, Jedrzej and Mlokosiewicz, Jakub and Rouyan, Anahita and Satlawa, Tadeusz and Kumar, Sandeep and Greiff, Victor and Krawczyk, Konrad}, abstract = {Antibodies are versatile molecular binders with an established and growing role as therapeutics. Computational approaches to developing and designing these molecules are being increasingly used to complement traditional lab-based processes. Nowadays, in silico methods fill multiple elements of the discovery stage, such as characterizing antibody-antigen interactions and identifying developability liabilities. Recently, computational methods tackling such problems have begun to follow machine learning paradigms, in many cases deep learning specifically. This paradigm shift offers improvements in established areas such as structure or binding prediction and opens up new possibilities such as language-based modeling of antibody repertoires or machine-learning-based generation of novel sequences. In this review, we critically examine the recent developments in (deep) machine learning approaches to therapeutic antibody design with implications for fully computational antibody design.} }
@article{WOS:000498675500022, title = {How to Read Articles That Use Machine Learning Users' Guides to the Medical Literature}, journal = {JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION}, volume = {322}, pages = {1806-1816}, year = {2019}, issn = {0098-7484}, doi = {10.1001/jama.2019.16489}, author = {Liu, Yun and Chen, Po-Hsuan Cameron and Krause, Jonathan and Peng, Lily}, abstract = {In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.} }
@article{WOS:001026236800015, title = {Quantum machine learning beyond kernel methods}, journal = {NATURE COMMUNICATIONS}, volume = {14}, year = {2023}, doi = {10.1038/s41467-023-36159-y}, author = {Jerbi, Sofiene and Fiderer, Lukas J. and Poulsen Nautrup, Hendrik and Kuebler, Jonas M. and Briegel, Hans J. and Dunjko, Vedran}, abstract = {Machine learning algorithms based on parametrized quantum circuits are prime candidates for near-term applications on noisy quantum computers. In this direction, various types of quantum machine learning models have been introduced and studied extensively. Yet, our understanding of how these models compare, both mutually and to classical models, remains limited. In this work, we identify a constructive framework that captures all standard models based on parametrized quantum circuits: that of linear quantum models. In particular, we show using tools from quantum information theory how data re-uploading circuits, an apparent outlier of this framework, can be efficiently mapped into the simpler picture of linear models in quantum Hilbert spaces. Furthermore, we analyze the experimentally-relevant resource requirements of these models in terms of qubit number and amount of data needed to learn. Based on recent results from classical machine learning, we prove that linear quantum models must utilize exponentially more qubits than data re-uploading models in order to solve certain learning tasks, while kernel methods additionally require exponentially more data points. Our results provide a more comprehensive view of quantum machine learning models as well as insights on the compatibility of different models with NISQ constraints. Comparing the capabilities of different quantum machine learning protocols is difficult. Here, the authors show that different learning models based on parametrized quantum circuits can all be seen as quantum linear models, thus driving general conclusions on their resource requirements and capabilities.} }
@article{WOS:000636768300001, title = {Radiomic and Genomic Machine Learning Method Performance for Prostate Cancer Diagnosis: Systematic Literature Review}, journal = {JOURNAL OF MEDICAL INTERNET RESEARCH}, volume = {23}, year = {2021}, issn = {1438-8871}, doi = {10.2196/22394}, author = {Castaldo, Rossana and Cavaliere, Carlo and Soricelli, Andrea and Salvatore, Marco and Pecchia, Leandro and Franzese, Monica}, abstract = {Background: Machine learning algorithms have been drawing attention at the joining of pathology and radiology in prostate cancer research. However, due to their algorithmic learning complexity and the variability of their architecture, there is an ongoing need to analyze their performance. Objective: This study assesses the source of heterogeneity and the performance of machine learning applied to radiomic, genomic, and clinical biomarkers for the diagnosis of prostate cancer. One research focus of this study was on clearly identifying problems and issues related to the implementation of machine learning in clinical studies. Methods: Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) protocol, 816 titles were identified from the PubMed, Scopus, and OvidSP databases. Studies that used machine learning to detect prostate cancer and provided performance measures were included in our analysis. The quality of the eligible studies was assessed using the QUADAS-2 (quality assessment of diagnostic accuracy studies-version 2) tool. The hierarchical multivariate model was applied to the pooled data in a meta-analysis. To investigate the heterogeneity among studies, I-2 statistics were performed along with visual evaluation of coupled forest plots. Due to the internal heterogeneity among machine learning algorithms, subgroup analysis was carried out to investigate the diagnostic capability of machine learning systems in clinical practice. Results: In the final analysis, 37 studies were included, of which 29 entered the meta-analysis pooling. The analysis of machine learning methods to detect prostate cancer reveals the limited usage of the methods and the lack of standards that hinder the implementation of machine learning in clinical applications. Conclusions: The performance of machine learning for diagnosis of prostate cancer was considered satisfactory for several studies investigating the multiparametric magnetic resonance imaging and urine biomarkers; however, given the limitations indicated in our study, further studies are warranted to extend the potential use of machine learning to clinical settings. Recommendations on the use of machine learning techniques were also provided to help researchers to design robust studies to facilitate evidence generation from the use of radiomic and genomic biomarkers.} }
@article{WOS:000530096900011, title = {Privacy preservation for machine learning training and classification based on homomorphic encryption schemes}, journal = {INFORMATION SCIENCES}, volume = {526}, pages = {166-179}, year = {2020}, issn = {0020-0255}, doi = {10.1016/j.ins.2020.03.041}, author = {Li, Jing and Kuang, Xiaohui and Lin, Shujie and Ma, Xu and Tang, Yi}, abstract = {In recent years, more and more machine learning algorithms depend on the cloud computing. When a machine learning system is trained or classified in the cloud environment, the cloud server obtains data from the user side. Then, the privacy of the data depends on the service provider, it is easy to induce the malicious acquisition and utilization of data. On the other hand, the attackers can detect the statistical characteristics of machine learning data and infer the parameters of machine learning model through reverse attacks. Therefore, it is urgent to design an effective encryption scheme to protect the data's privacy without breaking the performance of machine learning. In this paper, we propose a novel homomorphic encryption framework over non-abelian rings, and define the homomorphism operations in ciphertexts space. The scheme can achieve one-way security based on the Conjugacy Search Problem. After that, a homomorphic encryption was proposed over a matrix-ring. It supports real numbers encryption based on the homomorphism of 2-order displacement matrix coding function and achieves fast ciphertexts homomorphic comparison without decrypting any ciphetexts operations' intermediate result. Furthermore, we use the scheme to realize privacy preservation for machine learning training and classification in data ciphertexts environment. The analysis shows that our proposed schemes are efficient for encryption/decryption and homomorphic operations. (C) 2020 Elsevier Inc. All rights reserved.} }
@article{WOS:000663165500001, title = {River: machine learning for streaming data in Python}, journal = {JOURNAL OF MACHINE LEARNING RESEARCH}, volume = {22}, year = {2021}, issn = {1532-4435}, author = {Montiel, Jacob and Halford, Max and Mastelini, Saulo Martiello and Bolmier, Geoffrey and Sourty, Raphael and Vaysse, Robin and Zouitine, Adil and Gomes, Heitor Murilo and Read, Jesse and Abdessalem, Talel and Bifet, Albert}, abstract = {River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of two popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.} }
@article{WOS:001411195100001, title = {Causal machine learning for supply chain risk prediction and intervention planning}, journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH}, volume = {63}, pages = {5629-5648}, year = {2025}, issn = {0020-7543}, doi = {10.1080/00207543.2025.2458121}, author = {Wyrembek, Mateusz and Baryannis, George and Brintrup, Alexandra}, abstract = {The ultimate goal for developing machine learning models in supply chain management is to make optimal interventions. However, most machine learning models identify correlations in data rather than inferring causation, making it difficult to systematically plan for better outcomes. In this article, we propose and evaluate the use of causal machine learning for developing supply chain risk intervention models, and demonstrate its use with a case study in supply chain risk management in the maritime engineering sector. Our findings highlight that causal machine learning enhances decision-making processes by identifying changes that can be achieved under different supply chain interventions, allowing `what-if' scenario planning. We therefore propose different machine learning developmental pathways for predicting risk and planning for interventions to minimise risk and outline key steps for supply chain researchers to explore causal machine learning and harness its capabilities.} }
@article{WOS:000705187000001, title = {Exploiting machine learning for bestowing intelligence to microfluidics}, journal = {BIOSENSORS \\& BIOELECTRONICS}, volume = {194}, year = {2021}, issn = {0956-5663}, doi = {10.1016/j.bios.2021.113666}, author = {Zheng, Jiahao and Cole, Tim and Zhang, Yuxin and Kim, Jeeson and Tang, Shi-Yang}, abstract = {Intelligent microfluidics is an emerging cross-discipline research area formed by combining microfluidics with machine learning. It uses the advantages of microfluidics, such as high throughput and controllability, and the powerful data processing capabilities of machine learning, resulting in improved systems in biotechnology and chemistry. Compared to traditional microfluidics using manual analysis methods, intelligent microfluidics needs less human intervention, and results in a more user-friendly experience with faster processing. There is a paucity of literature reviewing this burgeoning and highly promising cross-discipline. Therefore, we herein comprehensively and systematically summarize several aspects of microfluidic applications enabled by machine learning. We list the types of microfluidics used in intelligent microfluidic applications over the last five years, as well as the machine learning algorithms and the hardware used for training. We also present the most recent advances in key technologies, developments, challenges, and the emerging opportunities created by intelligent microfluidics.} }
@article{WOS:000457664700017, title = {Generalized class-specific kernelized extreme learning machine for multiclass imbalanced learning}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {121}, pages = {244-255}, year = {2019}, issn = {0957-4174}, doi = {10.1016/j.eswa.2018.12.024}, author = {Raghuwanshi, Bhagat Singh and Shukla, Sanyam}, abstract = {Class imbalanced learning is a well-known issue, which exists in real-world applications. Datasets that have skewed class distribution raise hindrance to the traditional learning algorithms. Traditional classifiers give the same importance to all the samples, which leads to the prediction biased towards the majority classes. To solve this intrinsic deficiency, numerous strategies have been proposed such as weighted extreme learning machine (WELM), weighted support vector machine (WSVM), class-specific extreme learning machine (CS-ELM) and class-specific kernelized extreme learning machine (CSKELM). This work focuses on multiclass imbalance problems, which are more difficult compared to the binary class imbalance problems. Kernelized extreme learning machine (KELM) yields better results compared to the traditional extreme learning machine (ELM), which uses random input parameters. This work presents a generalized CSKELM (GCSKELM), the extension of our recently proposed CSKELM, which addresses the multiclass imbalanced problems more effectively. The proposed GCSKELM can be applied directly to solve the multiclass imbalanced problems. GCSKELM with Gaussian kernel function avoids the non-optimal hidden node problem associated with CS-ELM and other existing variants of ELM. The proposed work also has less computational cost in contrast with kernelized WELM (KWELM) for multiclass imbalanced learning. This work employs class-specific regularization parameters, which are determined by employing class proportion. The extensive experimental analysis shows that the proposed work obtains promising generalization performance in contrast with the other state-of-the-art imbalanced learning methods. (C) 2018 Elsevier Ltd. All rights reserved.} }
@article{WOS:000425010500008, title = {On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products}, journal = {BIG DATA}, volume = {5}, pages = {246-255}, year = {2017}, issn = {2167-6461}, doi = {10.1089/big.2016.0051}, author = {Varshney, Kush R. and Alemzadeh, Homa}, abstract = {Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.} }
@article{WOS:001169793700001, title = {Machine learning and IoT - Based predictive maintenance approach for industrial applications}, journal = {ALEXANDRIA ENGINEERING JOURNAL}, volume = {88}, pages = {298-309}, year = {2024}, issn = {1110-0168}, doi = {10.1016/j.aej.2023.12.065}, author = {Elkateb, Sherien and Metwalli, Ahmed and Shendy, Abdelrahman and Abu-Elanien, Ahmed E. B.}, abstract = {Unplanned outage in industry due to machine failures can lead to significant production losses and increased maintenance costs. Predictive maintenance methods use the data collected from IoT-enabled devices installed in working machines to detect incipient faults and prevent major failures. In this study, a predictive maintenance system based on machine learning algorithms, specifically AdaBoost, is presented to classify different types of machines stops in real-time with application in knitting machines. The data collected from the machines include machine speeds and steps, which were pre-processed and fed into the machine learning model to classify six types of machines stops: gate stop, feeder stop, needle stop, completed roll stop, idle stop, and lycra stop. The model is trained and optimized using a combination of hyperparameter tuning and cross-validation techniques to achieve an accuracy of 92\\% on the test set. The results demonstrate the potential of the proposed system to accurately classify machine stops and enable timely maintenance actions; thereby, improving the overall efficiency and productivity of the textile industry.} }
@article{WOS:000799624800014, title = {Deceptive Logic Locking for Hardware Integrity Protection Against Machine Learning Attacks}, journal = {IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS}, volume = {41}, pages = {1716-1729}, year = {2022}, issn = {0278-0070}, doi = {10.1109/TCAD.2021.3100275}, author = {Sisejkovic, Dominik and Merchant, Farhad and Reimann, Lennart M. and Leupers, Rainer}, abstract = {Logic locking has emerged as a prominent key-driven technique to protect the integrity of integrated circuits. However, novel machine-learning-based attacks have recently been introduced to challenge the security foundations of locking schemes. These attacks are able to recover a significant percentage of the key without having access to an activated circuit. This article address this issue through two focal points. First, we present a theoretical model to test locking schemes for key-related structural leakage that can be exploited by machine learning. Second, based on the theoretical model, we introduce D-MUX: a deceptive multiplexer-based logic-locking scheme that is resilient against structure-exploiting machine learning attacks. Through the design of D-MUX, we uncover a major fallacy in the existing multiplexer-based locking schemes in the form of a structural-analysis attack. Finally, an extensive cost evaluation of D-MUX is presented. To the best of our knowledge, D-MUX is the first machine-learning-resilient locking scheme capable of protecting against all known learning-based attacks. Hereby, the presented work offers a starting point for the design and evaluation of future-generation logic locking in the era of machine learning.} }
@article{WOS:000701874800010, title = {Adversarial machine learning in Network Intrusion Detection Systems}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {186}, year = {2021}, issn = {0957-4174}, doi = {10.1016/j.eswa.2021.115782}, author = {Alhajjar, Elie and Maxwell, Paul and Bastian, Nathaniel}, abstract = {Adversarial examples are inputs to a machine learning system intentionally crafted by an attacker to fool the model into producing an incorrect output. These examples have achieved a great deal of success in several domains such as image recognition, speech recognition and spam detection. In this paper, we study the nature of the adversarial problem in Network Intrusion Detection Systems (NIDS). We focus on the attack perspective, which includes techniques to generate adversarial examples capable of evading a variety of machine learning models. More specifically, we explore the use of evolutionary computation (particle swarm optimization and genetic algorithm) and deep learning (generative adversarial networks) as tools for adversarial example generation. To assess the performance of these algorithms in evading a NIDS, we apply them to two publicly available data sets, namely the NSL-KDD and UNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo simulation. The results show that our adversarial example generation techniques cause high misclassification rates in eleven different machine learning models, along with a voting classifier. Our work highlights the vulnerability of machine learning based NIDS in the face of adversarial perturbation.} }
@article{WOS:000883846400002, title = {Human vs. supervised machine learning: Who learns patterns faster?}, journal = {COGNITIVE SYSTEMS RESEARCH}, volume = {76}, pages = {78-92}, year = {2022}, issn = {2214-4366}, doi = {10.1016/j.cogsys.2022.09.002}, author = {Kuehl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik}, abstract = {The capabilities of supervised machine learning (SML), especially compared to human abilities, are being discussed in scientific research and in the usage of SML. This study provides an answer to how learning performance differs between humans and machines when there is limited training data. We have designed an experiment in which 44 humans and three different machine learning algorithms identify patterns in labeled training data and have to label instances according to the patterns they find. The results show a high dependency between performance and the underlying patterns of the task. Whereas humans perform relatively similarly across all patterns, machines show large performance differences for the various patterns in our experiment. After seeing 20 instances in the experiment, human performance does not improve anymore, which we relate to theories of cognitive overload. Machines learn slower but can reach the same level or may even outperform humans in 2 of the 4 of used patterns. However, machines need more instances compared to humans for the same results. The performance of machines is comparably lower for the other 2 patterns due to the difficulty of combining input features.} }
@article{WOS:000534712200003, title = {A stacked ensemble learning model for intrusion detection in wireless network}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {15387-15395}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-020-04986-5}, author = {Rajadurai, Hariharan and Gandhi, Usha Devi}, abstract = {Intrusion detection pretended to be a major technique for revealing the attacks and guarantee the security on the network. As the data increases tremendously every year on the Internet, a single algorithm is not sufficient for the network security. Because, deploying a single learning approach may suffer from statistical, computational and representational issues. To eliminate these issues, this paper combines multiple machine learning algorithms called stacked ensemble learning, to detect the attacks in a better manner than conventional learning, where a single algorithm is used to identify the attacks. The stacked ensemble system has been taken the benchmark data set, NSL-KDD, to compare its performance with other popular machine learning algorithms such as ANN, CART, random forest, SVM and other machine learning methods proposed by researchers. The experimental results show that stacked ensemble learning is a proper technique for classifying attacks than other existing methods. And also, the proposed system shows better accuracy compare to other intrusion detection models.} }
@article{WOS:000444816000005, title = {Machine Learning for the Developing World}, journal = {ACM TRANSACTIONS ON MANAGEMENT INFORMATION SYSTEMS}, volume = {9}, year = {2018}, issn = {2158-656X}, doi = {10.1145/3210548}, author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur}, abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.} }
@article{WOS:001025397400001, title = {A review on the applications of machine learning and deep learning in agriculture section for the production of crop biomass raw materials}, journal = {ENERGY SOURCES PART A-RECOVERY UTILIZATION AND ENVIRONMENTAL EFFECTS}, volume = {45}, pages = {9178-9201}, year = {2023}, issn = {1556-7036}, doi = {10.1080/15567036.2023.2232322}, author = {Peng, Wei and Karimi Sadaghiani, Omid}, abstract = {The application of biomass, as an energy resource, depends on four main steps of production, pre-treatment, bio-refinery, and upgrading. This work reviews Machine Learning applications in the biomass production step with focusing on agriculture crops. By investigating numerous related works, it is concluded that there is a considerable reviewing gap in collecting the applications of Machine Learning in crop biomass production. To fill this gap by the current work, the origin of biomass raw materials is explained, and the application of Machine Learning in this section is scrutinized. Then, the kinds and resources of biomass as well as the role of machine learning in these fields are reviewed. Meanwhile, the sustainable production of farming-origin biomass and the effective factors in this issue are explained, and the application of Machine Learning in these areas are surveyed. Summarily, after analysis of numerous papers, it is concluded that Machine Learning and Deep Learning are widely utilized in crop biomass production areas to enhance the crops production quantity, quality, and sustainability, improve the predictions, decrease the costs, and diminish the products losses. According to the statistical analysis, in 19\\% of the studies conducted about the application of Machine Learning and Deep Learning in crop biomass raw materials, Artificial Neural Network (ANN) algorithm has been applied. Afterward, the Random Forest (RF) and Super Vector Machine (SVM) are the second and third most-utilized algorithms applied in 17\\% and 15\\% of studies, respectively. Meanwhile, 26\\% of studies focused on the applications of Machine Learning and Deep Learning in the sugar crops. At the second and third places, the starchy crops and algae with 23\\% and 21\\% received more attention of researchers in the utilization of Machine Learning and Deep Learning techniques.} }
@article{WOS:000642962200001, title = {Federated Quantum Machine Learning}, journal = {ENTROPY}, volume = {23}, year = {2021}, doi = {10.3390/e23040460}, author = {Chen, Samuel Yen-Chi and Yoo, Shinjae}, abstract = {Distributed training across several quantum computers could significantly improve the training time and if we could share the learned model, not the data, it could potentially improve the data privacy as the training would happen where the data is located. One of the potential schemes to achieve this property is the federated learning (FL), which consists of several clients or local nodes learning on their own data and a central node to aggregate the models collected from those local nodes. However, to the best of our knowledge, no work has been done in quantum machine learning (QML) in federation setting yet. In this work, we present the federated training on hybrid quantum-classical machine learning models although our framework could be generalized to pure quantum machine learning model. Specifically, we consider the quantum neural network (QNN) coupled with classical pre-trained convolutional model. Our distributed federated learning scheme demonstrated almost the same level of trained model accuracies and yet significantly faster distributed training. It demonstrates a promising future research direction for scaling and privacy aspects.} }
@article{WOS:000621832300030, title = {Nowcasting GDP using machine-learning algorithms: A real-time assessment}, journal = {INTERNATIONAL JOURNAL OF FORECASTING}, volume = {37}, pages = {941-948}, year = {2021}, issn = {0169-2070}, doi = {10.1016/j.ijforecast.2020.10.005}, author = {Richardson, Adam and Mulder, Thomas van Florenstein and Vehbi, Tugrul}, abstract = {Can machine-learning algorithms help central banks understand the current state of the economy? Our results say yes! We contribute to the emerging literature on forecasting macroeconomic variables using machine-learning algorithms by testing the nowcast performance of common algorithms in a full `real-time' setting-that is, with real-time vintages of New Zealand GDP growth (our target variable) and real-time vintages of around 600 predictors. Our results show that machine-learning algorithms are able to significantly improve over a simple autoregressive benchmark and a dynamic factor model. We also show that machine-learning algorithms have the potential to add value to, and in one case improve on, the official forecasts of the Reserve Bank of New Zealand. (C) 2020 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.} }
@article{WOS:000616713000005, title = {A Comprehensive Review on Medical Diagnosis Using Machine Learning}, journal = {CMC-COMPUTERS MATERIALS \\& CONTINUA}, volume = {67}, pages = {1997-2014}, year = {2021}, issn = {1546-2218}, doi = {10.32604/cmc.2021.014943}, author = {Bhavsar, Kaustubh Arun and Abugabah, Ahed and Singla, Jimmy and AlZubi, Ahmad Ali and Bashir, Ali Kashif and Nikita}, abstract = {The unavailability of sufficient information for proper diagnosis, incomplete or miscommunication between patient and the clinician, or among the healthcare professionals, delay or incorrect diagnosis, the fatigue of clinician, or even the high diagnostic complexity in limited time can lead to diagnostic errors. Diagnostic errors have adverse effects on the treatment of a patient. Unnecessary treatments increase the medical bills and deteriorate the health of a patient. Such diagnostic errors that harm the patient in various ways could be minimized using machine learning. Machine learning algorithms could be used to diagnose various diseases with high accuracy. The use of machine learning could assist the doctors in making decisions on time, and could also be used as a second opinion or supporting tool. This study aims to provide a comprehensive review of research articles published from the year 2015 to mid of the year 2020 that have used machine learning for diagnosis of various diseases. We present the various machine learning algorithms used over the years to diagnose various diseases. The results of this study show the distribution of machine learning methods by medical disciplines. Based on our review, we present future research directions that could be used to conduct further research.} }
@article{WOS:000792828100003, title = {Machine learning techniques for pavement condition evaluation}, journal = {AUTOMATION IN CONSTRUCTION}, volume = {136}, year = {2022}, issn = {0926-5805}, doi = {10.1016/j.autcon.2022.104190}, author = {Sholevar, Nima and Golroo, Amir and Esfahani, Sahand Roghani}, abstract = {Pavement management systems play a significant role in country's economy since road authorities are concerned about preserving their priceless road assets for a longer time to save maintenance costs. An essential part of such systems is how to collect and analyze pavement condition data. This paper reviews the state-of-the-art techniques in pavement condition data evaluation using machine learning techniques, more specifically, the application of machine learning methods: image classification, object detection, and segmentation in pavement distress assessment is investigated. Furthermore, the pavement automated data collection tools and pavement condition indices have been reviewed from the lens of machine learning applications. The review concludes that the overall trends in pavement condition evaluation is to apply machine learning techniques although there are some limitations not only in detection of few pavement distresses with complicated patterns but also in indication of the severity and density of distresses leading to avenues for future research.} }
@article{WOS:001199751600001, title = {Theoretical Calculation Assisted by Machine Learning Accelerate Optimal Electrocatalyst Finding for Hydrogen Evolution Reaction}, journal = {CHEMELECTROCHEM}, volume = {11}, year = {2024}, issn = {2196-0216}, doi = {10.1002/celc.202400084}, author = {Zhang, Yuefei and Liu, Xuefei and Wang, Wentao}, abstract = {Electrocatalytic hydrogen evolution reaction (HER) is a promising strategy to solve and mitigate the coming energy shortage and global environmental pollution. Searching for efficient electrocatalysts for HER remains challenging through traditional trial-and-error methods from numerous potential material candidates. Theoretical high throughput calculation assisted by machine learning is a possible method to screen excellent HER electrocatalysts effectively. This will pave the way for high-efficiency and low-price electrocatalyst findings. In this review, we comprehensively introduce the machine learning workflow and standard models for hydrogen reduction reactions. This mainly illustrates how machine learning is used in catalyst filtration and descriptor exploration. Subsequently, several applications, including surface electrocatalysts, two-dimensional (2D) electrocatalysts, and single/dual atom electrocatalysts using machine learning in electrocatalytic HER, are highlighted and introduced. Finally, the corresponding challenge and perspective for machine learning in electrocatalytic hydrogen reduction reactions are concluded. We hope this critical review can provide a comprehensive understanding of machine learning for HER catalyst design and guide the future theoretical and experimental investigation of HER catalyst findings. In this review, we comprehensively introduce the machine learning workflow and standard models for hydrogen reduction reactions (HER). This mainly illustrates how machine learning is used in catalyst filtration and descriptor exploration. Several applications, including surface electrocatalysts, two-dimensional electrocatalysts, and single/dual atom electrocatalysts using machine learning in electrocatalytic HER, are highlighted and introduced. image} }
@article{WOS:000702351700085, title = {A deep learning approach for imbalanced crash data in predicting highway-rail grade crossings accidents}, journal = {RELIABILITY ENGINEERING \\& SYSTEM SAFETY}, volume = {216}, year = {2021}, issn = {0951-8320}, doi = {10.1016/j.ress.2021.108019}, author = {Gao, Lu and Lu, Pan and Ren, Yihao}, abstract = {Accurate accident prediction for highway-rail grade crossings (HRGCs) is critically important for assisting at-grade safety improvement decision making. Numerous machine-learning methods were developed focusing on predicting accidents and identifying contributing physical and operational characteristics. A more advanced deep learning-based model is explored as a more accurate means of predicting HRGC crashes compared to machine learning-based approaches. In particular, the prediction performance of the convolution neural network (CNN) model is compared to the most commonly used machine learning methods, such as decision tree (DT) and random forests (RF). A 19-year HRGCs data in North Dakota (ND) is used in this study. Training a machine learning model on an imbalanced data (e.g., unequal distribution of labeled data in accident and no-accident classes) introduce unique challenges for accurate prediction especially for minority class. In this paper, a resampling approach was used to address the imbalanced data issue. Various performance measurements are used to compare the models' prediction performance. The results indicate that resampling the imbalanced dataset significantly improves the recall rate. The results also show that the proposed deep learning-based approach which deepens the layer levels and adapts to the training dataset has better prediction performance compared to other machine learning-based methods.} }
@article{WOS:000436866400004, title = {People's Councils for Ethical Machine Learning}, journal = {SOCIAL MEDIA + SOCIETY}, volume = {4}, year = {2018}, issn = {2056-3051}, doi = {10.1177/2056305118768303}, author = {McQuillan, Dan}, abstract = {Machine learning is a form of knowledge production native to the era of big data. It is at the core of social media platforms and everyday interactions. It is also being rapidly adopted for research and discovery across academia, business, and government. This article will explores the way the affordances of machine learning itself, and the forms of social apparatus that it becomes a part of, will potentially erode ethics and draw us in to a drone-like perspective. Unconstrained machine learning enables and delimits our knowledge of the world in particular ways: the abstractions and operations of machine learning produce a ``view from above'' whose consequences for both ethics and legality parallel the dilemmas of drone warfare. The family of machine learning methods is not somehow inherently bad or dangerous, nor does implementing them signal any intent to cause harm. Nevertheless, the machine learning assemblage produces a targeting gaze whose algorithms obfuscate the legality of its judgments, and whose iterations threaten to create both specific injustices and broader states of exception. Given the urgent need to provide some kind of balance before machine learning becomes embedded everywhere, this article proposes people's councils as a way to contest machinic judgments and reassert openness and discourse.} }
@article{WOS:000654535500001, title = {Machine learning paradigm for structural health monitoring}, journal = {STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL}, volume = {20}, pages = {1353-1372}, year = {2021}, issn = {1475-9217}, doi = {10.1177/1475921720972416}, author = {Bao, Yuequan and Li, Hui}, abstract = {Structural health diagnosis and prognosis is the goal of structural health monitoring. Vibration-based structural health monitoring methodology has been extensively investigated. However, the conventional vibration-based methods find it difficult to detect damages of actual structures because of a high incompleteness in the monitoring information (the number of sensors is much fewer with respect to the number of degrees of freedom of a structure), intense uncertainties in the structural conditions and monitoring systems, and coupled effects of damage and environmental actions on modal parameters. It is a truth that the performance and conditions of a structure must be embedded in the monitoring data (vehicles, wind, etc.; acceleration, displacement, cable force, strain, images, videos, etc.). Therefore, there is a need to develop completely novel structural health diagnosis and prognosis methodology based on the various monitoring data. Machine learning provides the advanced mathematical frameworks and algorithms that can help discover and model the performance and conditions of a structure through deep mining of monitoring data. Thus, machine learning takes an opportunity to establish novel machine learning paradigm for structural health diagnosis and prognosis theory termed the machine learning paradigm for structural health monitoring. This article sheds light on principles for machine learning paradigm for structural health monitoring with some examples and reviews the existing challenges and open questions in this field.} }
@article{WOS:000700585500007, title = {An efficient parallel machine learning-based blockchain framework}, journal = {ICT EXPRESS}, volume = {7}, pages = {300-307}, year = {2021}, issn = {2405-9595}, doi = {10.1016/j.icte.2021.08.014}, author = {Tsai, Chun-Wei and Chen, Yi-Ping and Tang, Tzu-Chieh and Luo, Yu-Chen}, abstract = {The unlimited possibilities of machine learning have been shown in several successful reports and applications. However, how to make sure that the searched results of a machine learning system are not tampered by anyone and how to prevent the other users in the same network environment from easily getting our private data are two critical research issues when we immerse into powerful machine learning-based systems or applications. This situation is just like other modern information systems that confront security and privacy issues. The development of blockchain provides us an alternative way to address these two issues. That is why some recent studies have attempted to develop machine learning systems with blockchain technologies or to apply machine learning methods to blockchain systems. To show what the combination of blockchain and machine learning is capable of doing, in this paper, we proposed a parallel framework to find out suitable hyperparameters of deep learning in a blockchain environment by using a metaheuristic algorithm. The proposed framework also takes into account the issue of communication cost, by limiting the number of information exchanges between miners and blockchain. (C) 2021 The Korean Institute of Communications and Information Sciences (KICS). Publishing services by Elsevier B.V.} }
@article{WOS:001395051600001, title = {Network embedding: The bridge between water distribution network hydraulics and machine learning}, journal = {WATER RESEARCH}, volume = {273}, year = {2025}, issn = {0043-1354}, doi = {10.1016/j.watres.2024.123011}, author = {Zhou, Xiao and Guo, Shuyi and Xin, Kunlun and Tang, Zhenheng and Chu, Xiaowen and Fu, Guangtao}, abstract = {Machine learning has been increasingly used to solve management problems of water distribution networks (WDNs). A critical research gap, however, remains in the effective incorporation of WDN hydraulic characteristics in machine learning. Here we present a new water distribution network embedding (WDNE) method that transforms the hydraulic relationships of WDN topology into a vector form to be best suited for machine learning algorithms. The nodal relationships are characterized by local structure, global structure and attribute information. A conjoint use of two deep auto-encoder embedding models ensures that the hydraulic relationships and attribute information are simultaneously preserved and are effectively utilized by machine learning models. WDNE provides a new way to bridge WDN hydraulics with machine learning. It is first applied to a pipe burst localization problem. The results show that it can increase the performance of machine learning algorithms, and enable a lightweight machine learning algorithm to achieve better accuracy with less training data compared with a deep learning method reported in the literature. Then, applications in node grouping problems show that WDNE enables machine learning algorithms to make use of WDN hydraulic information, and integrates WDN structural relationships to achieve better grouping results. The results highlight the potential of WDNE to enhance WDN management by improving the efficiency of machine learning models and broadening the range of solvable problems. Codes are available at https://github.com/ZhouGroupHFUT/WDNE} }
@article{WOS:000929283300001, title = {An Overview of Machine Learning, Deep Learning, and Reinforcement Learning-Based Techniques in Quantitative Finance: Recent Progress and Challenges}, journal = {APPLIED SCIENCES-BASEL}, volume = {13}, year = {2023}, doi = {10.3390/app13031956}, author = {Sahu, Santosh Kumar and Mokhade, Anil and Bokde, Neeraj Dhanraj}, abstract = {Forecasting the behavior of the stock market is a classic but difficult topic, one that has attracted the interest of both economists and computer scientists. Over the course of the last couple of decades, researchers have investigated linear models as well as models that are based on machine learning (ML), deep learning (DL), reinforcement learning (RL), and deep reinforcement learning (DRL) in order to create an accurate predictive model. Machine learning algorithms can now extract high-level financial market data patterns. Investors are using deep learning models to anticipate and evaluate stock and foreign exchange markets due to the advantage of artificial intelligence. Recent years have seen a proliferation of the deep reinforcement learning algorithm's application in algorithmic trading. DRL agents, which combine price prediction and trading signal production, have been used to construct several completely automated trading systems or strategies. Our objective is to enable interested researchers to stay current and easily imitate earlier findings. In this paper, we have worked to explain the utility of Machine Learning, Deep Learning, Reinforcement Learning, and Deep Reinforcement Learning in Quantitative Finance (QF) and the Stock Market. We also outline potential future study paths in this area based on the overview that was presented before.} }
@article{WOS:000866803100001, title = {Deep Learning and Machine Learning with Grid Search to Predict Later Occurrence of Breast Cancer Metastasis Using Clinical Data}, journal = {JOURNAL OF CLINICAL MEDICINE}, volume = {11}, year = {2022}, doi = {10.3390/jcm11195772}, author = {Jiang, Xia and Xu, Chuhan}, abstract = {Background: It is important to be able to predict, for each individual patient, the likelihood of later metastatic occurrence, because the prediction can guide treatment plans tailored to a specific patient to prevent metastasis and to help avoid under-treatment or over-treatment. Deep neural network (DNN) learning, commonly referred to as deep learning, has become popular due to its success in image detection and prediction, but questions such as whether deep learning outperforms other machine learning methods when using non-image clinical data remain unanswered. Grid search has been introduced to deep learning hyperparameter tuning for the purpose of improving its prediction performance, but the effect of grid search on other machine learning methods are under-studied. In this research, we take the empirical approach to study the performance of deep learning and other machine learning methods when using non-image clinical data to predict the occurrence of breast cancer metastasis (BCM) 5, 10, or 15 years after the initial treatment. We developed prediction models using the deep feedforward neural network (DFNN) methods, as well as models using nine other machine learning methods, including naive Bayes (NB), logistic regression (LR), support vector machine (SVM), LASSO, decision tree (DT), k-nearest neighbor (KNN), random forest (RF), AdaBoost (ADB), and XGBoost (XGB). We used grid search to tune hyperparameters for all methods. We then compared our feedforward deep learning models to the models trained using the nine other machine learning methods. Results: Based on the mean test AUC (Area under the ROC Curve) results, DFNN ranks 6th, 4th, and 3rd when predicting 5-year, 10-year, and 15-year BCM, respectively, out of 10 methods. The top performing methods in predicting 5-year BCM are XGB (1st), RF (2nd), and KNN (3rd). For predicting 10-year BCM, the top performers are XGB (1st), RF (2nd), and NB (3rd). Finally, for 15-year BCM, the top performers are SVM (1st), LR and LASSO (tied for 2nd), and DFNN (3rd). The ensemble methods RF and XGB outperform other methods when data are less balanced, while SVM, LR, LASSO, and DFNN outperform other methods when data are more balanced. Our statistical testing results show that at a significance level of 0.05, DFNN overall performs comparably to other machine learning methods when predicting 5-year, 10-year, and 15-year BCM. Conclusions: Our results show that deep learning with grid search overall performs at least as well as other machine learning methods when using non-image clinical data. It is interesting to note that some of the other machine learning methods, such as XGB, RF, and SVM, are very strong competitors of DFNN when incorporating grid search. It is also worth noting that the computation time required to do grid search with DFNN is much more than that required to do grid search with the other nine machine learning methods.} }
@article{WOS:000805800300001, title = {Novel Graph-Based Machine Learning Technique to Secure Smart Vehicles in Intelligent Transportation Systems}, journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS}, volume = {24}, pages = {8483-8491}, year = {2023}, issn = {1524-9050}, doi = {10.1109/TITS.2022.3174333}, author = {Gupta, Brij Bhooshan and Gaurav, Akshat and Marin, Enrique Cano and Alhalabi, Wadee}, abstract = {Intelligent Transport Systems (ITS) is a developing technology that will significantly alter the driving experience. In such systems, smart vehicles and Road-Side Units (RSUs) communicate through the VANET. Safety apps use these data to identify and prevent hazardous situations in real-time. Detection of malicious nodes and attack traffic in Intelligent Transportation Systems (ITS) is a current research subject. Recently, researchers are proposing graph-based machine learning techniques to identify malicious users in the ITS environment, through which it is easy to analyze the network traffic and detect the malicious devices. Therefore, graph-based machine learning techniques could be a technique that efficiently detect malicious nodes in the ITS environment. In this context, this article aims to provide a technique for resolving authentication and security issues in ITS using lightweight cryptography and graph-based machine learning. Our solution uses the concepts of identity based authentication technique and graph-based machine learning in order to provide authentication and security to the smart vehicle in ITS. By authenticating smart vehicles in ITS and identifying various cyber threats, our proposed method substantially contributes to the development of intelligent transportation communication environment.} }
@article{WOS:000803735500004, title = {Embedding metric learning into an extreme learning machine for scene recognition}, journal = {EXPERT SYSTEMS WITH APPLICATIONS}, volume = {203}, year = {2022}, issn = {0957-4174}, doi = {10.1016/j.eswa.2022.117505}, author = {Wang, Chen and Peng, Guohua and De Baets, Bernard}, abstract = {Metric learning can be very useful to improve the performance of a distance-dependent classifier. However, separating metric learning from the classifier learning possibly degenerates the performance, for instance in scene recognition, especially for some complicated scene images. To address this issue, we propose to embed metric learning into an extreme learning machine (EML-ELM) to tackle scene recognition. Specifically, metric learning is conducted to fully explore discriminative features by taking into account all label information, rendering samples of the same class more compact and those of different classes more separable. An extreme learning machine is employed as a classifier thanks to its effectiveness and fast learning process. By explicitly embedding metric learning into an extreme learning machine, we can jointly learn the discriminative features and an effective classifier in a unified framework, thereby improving the recognition performance for complicated scene images. Extensive experiments on four benchmark scene datasets demonstrate the competitive performance of EML-ELM in comparison with state-of-the-art methods.} }
@article{WOS:000470880900001, title = {Machine Learning Topological Phases with a Solid-State Quantum Simulator}, journal = {PHYSICAL REVIEW LETTERS}, volume = {122}, year = {2019}, issn = {0031-9007}, doi = {10.1103/PhysRevLett.122.210503}, author = {Lian, Wenqian and Wang, Sheng-Tao and Lu, Sirui and Huang, Yuanyuan and Wang, Fei and Yuan, Xinxing and Zhang, Wengang and Ouyang, Xiaolong and Wang, Xin and Huang, Xianzhi and He, Li and Chang, Xiuying and Deng, Dong-Ling and Duan, Luming}, abstract = {We report an experimental demonstration of a machine learning approach to identify exotic topological phases, with a focus on the three-dimensional chiral topological insulators. We show that the convolutional neural networks-a class of deep feed-forward artificial neural networks with widespread applications in machine learning-can be trained to successfully identify different topological phases protected by chiral symmetry from experimental raw data generated with a solid-state quantum simulator. Our results explicitly showcase the exceptional power of machine learning in the experimental detection of topological phases, which paves a way to study rich topological phenomena with the machine learning toolbox.} }
@article{WOS:000639862400012, title = {Optimized Extreme Learning Machine for Intelligent Spectrum Sensing in 5G systems}, journal = {JOURNAL OF COMMUNICATIONS TECHNOLOGY AND ELECTRONICS}, volume = {66}, pages = {322-332}, year = {2021}, issn = {1064-2269}, doi = {10.1134/S1064226921040045}, author = {Kansal, P. and Kumar, A. and Gangadharappa, M.}, abstract = {A two-level learned distributed networking (LDN) structure that uses existing machine learning (ML) algorithms and the novel Optimized Extreme Learning Machine (OELM) algorithm to perform intelligent spectrum sensing for 5G systems has been proposed and implemented. This novel technique uses input vectors like received signal strength indicator, the distance between Cognitive Radio users and gateways, and energy vectors to train the model. Extreme Learning Machine optimized by BAT algorithm outperforms the existing Machine Learning techniques in terms of detection accuracy, false alarm, detection probability and cross validation curves at different SNR scenarios.} }
@article{WOS:000564994700001, title = {Soul and machine (learning)}, journal = {MARKETING LETTERS}, volume = {31}, pages = {393-404}, year = {2020}, issn = {0923-0645}, doi = {10.1007/s11002-020-09538-4}, author = {Proserpio, Davide and Hauser, John R. and Liu, Xiao and Amano, Tomomichi and Burnap, Alex and Guo, Tong and Lee, Dokyun (DK) and Lewis, Randall and Misra, Kanishka and Schwarz, Eric and Timoshenko, Artem and Xu, Lilei and Yoganarasimhan, Hema}, abstract = {Machine learning is bringing us self-driving cars, medical diagnoses, and language translation, but how can machine learning help marketers improve marketing decisions? Machine learning models predict extremely well, are scalable to ``big data,'' and are a natural fit to analyze rich media content, such as text, images, audio, and video. Examples of current marketing applications include identification of customer needs from online data, accurate prediction of consumer response to advertising, personalized pricing, and product recommendations. But without the human input and insight-the soul-the applications of machine learning are limited. To create competitive or cooperative strategies, to generate creative product designs, to be accurate for ``what-if'' and ``but-for'' applications, to devise dynamic policies, to advance knowledge, to protect consumer privacy, and avoid algorithm bias, machine learning needs a soul. The brightest future is based on the synergy of what the machine can do well and what humans do well. We provide examples and predictions for the future.} }
@article{WOS:000651052400024, title = {Machine learning for molecular thermodynamics}, journal = {CHINESE JOURNAL OF CHEMICAL ENGINEERING}, volume = {31}, pages = {227-239}, year = {2021}, issn = {1004-9541}, doi = {10.1016/j.cjche.2020.10.044}, author = {Ding, Jiaqi and Xu, Nan and Manh Tien Nguyen and Qiao, Qi and Shi, Yao and He, Yi and Shao, Qing}, abstract = {Thermodynamic properties of complex systems play an essential role in developing chemical engineering processes. It remains a challenge to predict the thermodynamic properties of complex systems in a wide range and describe the behavior of ions and molecules in complex systems. Machine learning emerges as a powerful tool to resolve this issue because it can describe complex relationships beyond the capacity of traditional mathematical functions. This minireview will summarize some fundamental concepts of machine learning methods and their applications in three aspects of the molecular thermodynamics using several examples. The first aspect is to apply machine learning methods to predict the thermodynamic properties of a broad spectrum of systems based on known data. The second aspect is to integer machine learning and molecular simulations to accelerate the discovery of materials. The third aspect is to develop machine learning force field that can eliminate the barrier between quantum mechanics and all-atom molecular dynamics simulations. The applications in these three aspects illustrate the potential of machine learning in molecular thermodynamics of chemical engineering. We will also discuss the perspective of the broad applications of machine learning in chemical engineering. (C) 2021 The Chemical Industry and Engineering Society of China, and Chemical Industry Press Co., Ltd. All rights reserved.} }
@article{WOS:001096766000001, title = {Digital geotechnics: from data-driven site characterisation towards digital transformation and intelligence in geotechnical engineering}, journal = {GEORISK-ASSESSMENT AND MANAGEMENT OF RISK FOR ENGINEERED SYSTEMS AND GEOHAZARDS}, volume = {18}, pages = {8-32}, year = {2024}, issn = {1749-9518}, doi = {10.1080/17499518.2023.2278136}, author = {Wang, Yu and Tian, Hua-Ming}, abstract = {Geotechnical engineering is experiencing a paradigm shift towards digital transformation and intelligence, driven by Industry 4.0 and emerging digital technologies, such as machine learning. However, development and application of machine learning are relatively slow in geotechnical practice, because extensive training databases are a key to the success of machine learning, but geotechnical data are often small and ugly, leading to the difficulty in developing a suitable training database required for machine learning. In addition, convincing examples from real projects are rare that demonstrate the immediate added value of machine learning to geotechnical practices. To facilitate digital transformation and machine learning in geotechnical engineering, this study proposes to develop a project-specific training database that leverages on digital transformation of geotechnical workflow and reflects both project-specific data collected from various stages of the geotechnical workflow and domain knowledge in geotechnical practices, such as soil mechanics, numerical analysis principles, and prior engineering experience and judgment. A real ground improvement project is presented to illustrate the proposed method and demonstrate the added value of digital transformation and machine learning in geotechnical practices.} }
@article{WOS:001003548300006, title = {Machine learning and deep learning techniques for breast cancer diagnosis and classification: a comprehensive review of medical imaging studies}, journal = {JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY}, volume = {149}, pages = {10473-10491}, year = {2023}, issn = {0171-5216}, doi = {10.1007/s00432-023-04956-z}, author = {Radak, Mehran and Lafta, Haider Yabr and Fallahi, Hossein}, abstract = {BackgroundBreast cancer is a major public health concern, and early diagnosis and classification are critical for effective treatment. Machine learning and deep learning techniques have shown great promise in the classification and diagnosis of breast cancer.PurposeIn this review, we examine studies that have used these techniques for breast cancer classification and diagnosis, focusing on five groups of medical images: mammography, ultrasound, MRI, histology, and thermography. We discuss the use of five popular machine learning techniques, including Nearest Neighbor, SVM, Naive Bayesian Network, DT, and ANN, as well as deep learning architectures and convolutional neural networks.ConclusionOur review finds that machine learning and deep learning techniques have achieved high accuracy rates in breast cancer classification and diagnosis across various medical imaging modalities. Furthermore, these techniques have the potential to improve clinical decision-making and ultimately lead to better patient outcomes.} }
@article{WOS:000794708000001, title = {Rainfall Prediction System Using Machine Learning Fusion for Smart Cities}, journal = {SENSORS}, volume = {22}, year = {2022}, doi = {10.3390/s22093504}, author = {Rahman, Atta-ur and Abbas, Sagheer and Gollapalli, Mohammed and Ahmed, Rashad and Aftab, Shabib and Ahmad, Munir and Khan, Muhammad Adnan and Mosavi, Amir}, abstract = {Precipitation in any form-such as rain, snow, and hail-can affect day-to-day outdoor activities. Rainfall prediction is one of the challenging tasks in weather forecasting process. Accurate rainfall prediction is now more difficult than before due to the extreme climate variations. Machine learning techniques can predict rainfall by extracting hidden patterns from historical weather data. Selection of an appropriate classification technique for prediction is a difficult job. This research proposes a novel real-time rainfall prediction system for smart cities using a machine learning fusion technique. The proposed framework uses four widely used supervised machine learning techniques, i.e., decision tree, Naive Bayes, K-nearest neighbors, and support vector machines. For effective prediction of rainfall, the technique of fuzzy logic is incorporated in the framework to integrate the predictive accuracies of the machine learning techniques, also known as fusion. For prediction, 12 years of historical weather data (2005 to 2017) for the city of Lahore is considered. Pre-processing tasks such as cleaning and normalization were performed on the dataset before the classification process. The results reflect that the proposed machine learning fusion-based framework outperforms other models.} }
@article{WOS:000615376800001, title = {Machine Learning Control Based on Approximation of Optimal Trajectories}, journal = {MATHEMATICS}, volume = {9}, year = {2021}, doi = {10.3390/math9030265}, author = {Diveev, Askhat and Konstantinov, Sergey and Shmalko, Elizaveta and Dong, Ge}, abstract = {The paper is devoted to an emerging trend in control-a machine learning control. Despite the popularity of the idea of machine learning, there are various interpretations of this concept, and there is an urgent need for its strict mathematical formalization. An attempt to formalize the concept of machine learning is presented in this paper. The concepts of an unknown function, work area, training set are introduced, and a mathematical formulation of the machine learning problem is presented. Based on the presented formulation, the concept of machine learning control is considered. One of the problems of machine learning control is the general synthesis of control. It implies finding a control function that depends on the state of the object, which ensures the achievement of the control goal with the optimal value of the quality criterion from any initial state of some admissible region. Supervised and unsupervised approaches to solving a problem based on symbolic regression methods are considered. As a computational example, a problem of general synthesis of optimal control for a spacecraft landing on the surface of the Moon is considered as supervised machine learning control with a training set.} }
@article{WOS:000995071700009, title = {Green-IN Machine Learning at a Glance}, journal = {COMPUTER}, volume = {56}, pages = {35-43}, year = {2023}, issn = {0018-9162}, doi = {10.1109/MC.2023.3254646}, author = {Gutierrez, Maria and Moraga, Ma Angeles and Garcia, Felix and Calero, Coral}, abstract = {The use of machine learning (ML) algorithms has an environmental impact to be fully considered. This article presents a green-in-driven approach to the development of ML models. The aim thereof is to meet operational requirements while ensuring a suitable tradeoff between performance/reliability and energy consumption.} }
@article{WOS:001139775100136, title = {MLpronto: A tool for democratizing machine learning}, journal = {PLOS ONE}, volume = {18}, year = {2023}, doi = {10.1371/journal.pone.0294924}, author = {Tjaden, Jacob and Tjaden, Brian}, abstract = {The democratization of machine learning is a popular and growing movement. In a world with a wealth of publicly available data, it is important that algorithms for analysis of data are accessible and usable by everyone. We present MLpronto, a system for machine learning analysis that is designed to be easy to use so as to facilitate engagement with machine learning algorithms. With its web interface, MLpronto requires no computer programming or machine learning background, and it normally returns results in a matter of seconds. As input, MLpronto takes a file of data to be analyzed. MLpronto then executes some of the more commonly used supervised machine learning algorithms on the data and reports the results of the analyses. As part of its execution, MLpronto generates computer programming code corresponding to its machine learning analysis, which it also supplies as output. Thus, MLpronto can be used as a no-code solution for citizen data scientists with no machine learning or programming background, as an educational tool for those learning about machine learning, and as a first step for those who prefer to engage with programming code in order to facilitate rapid development of machine learning projects. MLpronto is freely available for use at https://mlpronto.org/.} }
@article{WOS:000947665000001, title = {Machine Learning Adoption in Educational Institutions: Role of Internet of Things and Digital Educational Platforms}, journal = {SUSTAINABILITY}, volume = {15}, year = {2023}, doi = {10.3390/su15054000}, author = {Li, Jiuxiang and Wang, Rufeng}, abstract = {The ever-increasing development of information technologies has led to the adoption of advanced learning techniques. In this regard, e-learning and machine learning are two of the emerging instructional means for educational institutes. The current study investigates the role of the Internet of Things (IoT) and digital educational platforms (DEPs) in the adoption of machine learning. The present research additionally investigated the function of DEPs as mediators between IoT and machine learning adoption. The department chairs or heads of 310 departments at 91 Chinese institutions provided the information. In order to analyze the data, we used SPSS 25.0 and SEM (structural equation modeling). The results demonstrated how crucial an impact IoT has on DEPs and the uptake of machine learning. DEPs directly affect machine learning adoption and also act as mediators. The findings also support the mediating role of DEPs in the IoT and machine learning adoption link. The current study contributes to both theory and practical management by examining how IoT is helpful for achieving machine learning adoption. Based on the responses of 91 educational departments, this is a unique study of the mechanisms to achieve machine learning adoption through IoT and DEPs.} }
@article{WOS:000756609900013, title = {Using Machine Learning for measuring democracy: A practitioners guide and a new updated dataset for 186 countries from 1919 to 2019}, journal = {EUROPEAN JOURNAL OF POLITICAL ECONOMY}, volume = {70}, year = {2021}, issn = {0176-2680}, doi = {10.1016/j.ejpoleco.2021.102047}, author = {Grundler, Klaus and Krieger, Tommy}, abstract = {We provide a comprehensive overview of the literature on the measurement of democracy and present an extensive update of the Machine Learning indicator of Grundler and Krieger (2016). Four improvements are particularly notable: First, we produce a continuous and a dichotomous version of the Machine Learning democracy indicator. Second, we calculate intervals that reflect the degree of measurement uncertainty. Third, we refine the conceptualization of the Machine Learning Index. Finally, we significantly expand the data coverage by providing democracy indices for 186 countries in the period from 1919 to 2019.} }
@article{WOS:001230482400001, title = {Accommodating Machine Learning Algorithms in Professional Service Firms}, journal = {ORGANIZATION STUDIES}, volume = {45}, year = {2024}, issn = {0170-8406}, doi = {10.1177/01708406241252930}, author = {Faulconbridge, James R. and Sarwar, Atif and Spring, Martin}, abstract = {Machine learning algorithms, as one form of artificial intelligence, are significant for professional work because they create the possibility for some predictions, interpretations and judgements that inform decision-making to be made by algorithms. However, little is known about whether it is possible to transform professional work to incorporate machine learning while also addressing negative responses from professionals whose work is changed by inscrutable algorithms. Through original empirical analysis of the effects of machine learning algorithms on the work of accountants and lawyers, this paper identifies the role of accommodating machine learning algorithms in professional service firms. Accommodating machine learning algorithms involves strategic responses that both justify adoption in the context of the possibilities and new contributions of machine learning algorithms and respond to the algorithms' limitations and opaque and inscrutable nature. The analysis advances understanding of the processes that enable or inhibit the cooperative adoption of artificial intelligence in professional service firms and develops insights relevant when examining the long-term impacts of machine learning algorithms as they become ever more sophisticated.} }
@article{WOS:000573712500001, title = {A translucent box: interpretable machine learning in ecology}, journal = {ECOLOGICAL MONOGRAPHS}, volume = {90}, year = {2020}, issn = {0012-9615}, doi = {10.1002/ecm.1422}, author = {Lucas, Tim C. D.}, abstract = {Machine learning has become popular in ecology but its use has remained restricted to predicting, rather than understanding, the natural world. Many researchers consider machine learning algorithms to be a black box. These models can, however, with careful examination, be used to inform our understanding of the world. They are translucent boxes. Furthermore, the interpretation of these models can be an important step in building confidence in a model or in a specific prediction from a model. Here I review a number of techniques for interpreting machine learning models at the level of the system, the variable, and the individual prediction as well as methods for handling non-independent data. I also discuss the limits of interpretability for different methods and demonstrate these approaches using a case example of understanding litter sizes in mammals.} }
@article{WOS:000955359500001, title = {Artificial intelligence and machine learning overview in pathology \\& laboratory medicine: A general review of data preprocessing and basic supervised concepts}, journal = {SEMINARS IN DIAGNOSTIC PATHOLOGY}, volume = {40}, pages = {71-87}, year = {2023}, issn = {0740-2570}, doi = {10.1053/j.semdp.2023.02.002}, author = {Albahra, Samer and Gorbett, Tom and Robertson, Scott and D'Aleo, Giana and Ockunzzi, Samuel and Lallo, Daniel and Hu, Bo and Rashidi, Hooman H.}, abstract = {Machine learning (ML) is becoming an integral aspect of several domains in medicine. Yet, most pathologists and laboratory professionals remain unfamiliar with such tools and are unprepared for their inevitable integration. To bridge this knowledge gap, we present an overview of key elements within this emerging data science discipline. First, we will cover general, well-established concepts within ML, such as data type concepts, data preprocessing methods, and ML study design. We will describe common supervised and unsupervised learning algorithms and their associated common machine learning terms (provided within a comprehensive glossary of terms that are discussed within this review). Overall, this review will offer a broad overview of the key concepts and algorithms in machine learning, with a focus on pathology and laboratory medicine. The objective is to provide an updated useful reference for those new to this field or those who require a refresher.} }
@article{WOS:000877748800001, title = {The Role of Machine Learning in Tribology: A Systematic Review}, journal = {ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING}, volume = {30}, pages = {1345-1397}, year = {2023}, issn = {1134-3060}, doi = {10.1007/s11831-022-09841-5}, author = {Paturi, Uma Maheshwera Reddy and Palakurthy, Sai Teja and Reddy, N. S.}, abstract = {The machine learning (ML) approach, motivated by artificial intelligence (AI), is an inspiring mathematical algorithm that accurately simulates many engineering processes. Machine learning algorithms solve nonlinear and complex relationships through data training; additionally, they can infer previously unknown relationships, allowing for a simplified model and estimation of hidden data. Unlike other statistical tools, machine learning does not impose process parameter restrictions and yields an accurate association between input and output parameters. Tribology is a branch of surface science concerned with studying and managing friction, lubrication, and wear on relatively interacting surfaces. While AI-based machine learning approaches have been adopted in tribology applications, modern tribo-contact simulation requires a deliberate decomposition of complex design challenges into simpler sub-threads, thereby identifying the relationships between the numerous interconnected features and processes. Numerous studies have established that artificial intelligence techniques can accurately model tribological processes and their properties based on various process parameters. The primary objective of this review is to conduct a thorough examination of the role of machine learning in tribological research and pave the way for future researchers by providing a specific research direction. In terms of future research directions and developments, the expanded application of artificial intelligence and various machine learning methods in tribology has been emphasized, including the characterization and design of complex tribological systems. Additionally, by combining machine learning methods with tribological experimental data, interdisciplinary research can be conducted to understand efficient resource utilization and resource conservation better. At the conclusion of this article, a detailed discussion of the limitations and future research opportunities associated with implementing various machine learning algorithms in tribology and its interdisciplinary fields is presented.} }
@article{WOS:000737752600001, title = {Intelligent on-demand design of phononic metamaterials}, journal = {NANOPHOTONICS}, volume = {11}, pages = {439-460}, year = {2022}, issn = {2192-8606}, doi = {10.1515/nanoph-2021-0639}, author = {Jin, Yabin and He, Liangshu and Wen, Zhihui and Mortazavi, Bohayra and Guo, Hongwei and Torrent, Daniel and Djafari-Rouhani, Bahram and Rabczuk, Timon and Zhuang, Xiaoying and Li, Yan}, abstract = {With the growing interest in the field of artificial materials, more advanced and sophisticated functionalities are required from phononic crystals and acoustic metamaterials. This implies a high computational effort and cost, and still the efficiency of the designs may be not sufficient. With the help of third-wave artificial intelligence technologies, the design schemes of these materials are undergoing a new revolution. As an important branch of artificial intelligence, machine learning paves the way to new technological innovations by stimulating the exploration of structural design. Machine learning provides a powerful means of achieving an efficient and accurate design process by exploring nonlinear physical patterns in high-dimensional space, based on data sets of candidate structures. Many advanced machine learning algorithms, such as deep neural networks, unsupervised manifold clustering, reinforcement learning and so forth, have been widely and deeply investigated for structural design. In this review, we summarize the recent works on the combination of phononic metamaterials and machine learning. We provide an overview of machine learning on structural design. Then discuss machine learning driven on-demand design of phononic metamaterials for acoustic and elastic waves functions, topological phases and atomic-scale phonon properties. Finally, we summarize the current state of the art and provide a prospective of the future development directions.} }
@article{WOS:001088970300001, title = {Assessing rainfall prediction models: Exploring the advantages of machine learning and remote sensing approaches}, journal = {ALEXANDRIA ENGINEERING JOURNAL}, volume = {82}, pages = {16-25}, year = {2023}, issn = {1110-0168}, doi = {10.1016/j.aej.2023.09.060}, author = {Latif, Sarmad Dashti and Hazrin, Nur Alyaa Binti and Koo, Chai Hoon and Ng, Jing Lin and Chaplot, Barkha and Huang, Yuk Feng and El-Shafie, Ahmed and Ahmed, Ali Najah}, abstract = {Using a comparison of three different major types, the best predictive model was determined. Statistical models and machine learning algorithms automatically learn and improve based on data. Deep learning uses neural networks to learn complex data patterns and relationships. A combination of satellite imagery, radar data, and ground-based observations are used and using aircraft or satellites, and remote sensing (RS) collects data on distant objects or locations. Satellites and radar are used to gather regional precipitation data for hybrid models. An algorithm trained on historical rainfall measurements would then process the data. Using remote monitoring instrument input features, the machine-learning model can predict precipitation. Evaluation of machine learning regression methods is based on the degree of agreement between predicted and observed values. The RMSE, R2, and MAE statistical measures check on the precision of a prediction or forecasting model. Machine learning excels at rainfall prediction regardless of climate or timescale. As one of the more popular models for predicting rainfall, the LSTM models demonstrate their superiority. Remote sensing and hybrid predictive models should be investigated further due to their scarcity.} }
@article{WOS:000695342100003, title = {Federated Learning: A Distributed Shared Machine Learning Method}, journal = {COMPLEXITY}, volume = {2021}, year = {2021}, issn = {1076-2787}, doi = {10.1155/2021/8261663}, author = {Hu, Kai and Li, Yaogen and Xia, Min and Wu, Jiasheng and Lu, Meixia and Zhang, Shuai and Weng, Liguo}, abstract = {Federated learning (FL) is a distributed machine learning (ML) framework. In FL, multiple clients collaborate to solve traditional distributed ML problems under the coordination of the central server without sharing their local private data with others. This paper mainly sorts out FLs based on machine learning and deep learning. First of all, this paper introduces the development process, definition, architecture, and classification of FL and explains the concept of FL by comparing it with traditional distributed learning. Then, it describes typical problems of FL that need to be solved. On the basis of classical FL algorithms, several federated machine learning algorithms are briefly introduced, with emphasis on deep learning and classification and comparisons of those algorithms are carried out. Finally, this paper discusses possible future developments of FL based on deep learning.} }
@article{WOS:000509281100001, title = {Machine Learning in Psychometrics and Psychological Research}, journal = {FRONTIERS IN PSYCHOLOGY}, volume = {10}, year = {2020}, issn = {1664-1078}, doi = {10.3389/fpsyg.2019.02970}, author = {Orru, Graziella and Monaro, Merylin and Conversano, Ciro and Gemignani, Angelo and Sartori, Giuseppe}, abstract = {Recent controversies about the level of replicability of behavioral research analyzed using statistical inference have cast interest in developing more efficient techniques for analyzing the results of psychological experiments. Here we claim that complementing the analytical workflow of psychological experiments with Machine Learning-based analysis will both maximize accuracy and minimize replicability issues. As compared to statistical inference, ML analysis of experimental data is model agnostic and primarily focused on prediction rather than inference. We also highlight some potential pitfalls resulting from adoption of Machine Learning based experiment analysis. If not properly used it can lead to over-optimistic accuracy estimates similarly observed using statistical inference. Remedies to such pitfalls are also presented such and building model based on cross validation and the use of ensemble models. ML models are typically regarded as black boxes and we will discuss strategies aimed at rendering more transparent the predictions.} }
@article{WOS:000423046300023, title = {Information discriminative extreme learning machine}, journal = {SOFT COMPUTING}, volume = {22}, pages = {677-689}, year = {2018}, issn = {1432-7643}, doi = {10.1007/s00500-016-2372-y}, author = {Yan, Deqin and Chu, Yonghe and Zhang, Haiying and Liu, Deshan}, abstract = {Extreme learning machine (ELM) has become one of the new research hotspots in the field of pattern recognition and machine learning. However, the existing extreme learning machine algorithms cannot better use identification information of data. Aiming at solving this problem, we propose a regularized extreme learning machine (algorithm) based on discriminative information (called IELM). In order to evaluate and verify the effectiveness of the proposed method, experiments use widely used image data sets. The comparative experimental results show that the proposed algorithm in the paper can significantly improve the classification performance and generalization ability of ELM.} }
@article{WOS:000737368700001, title = {Assessing Surface Water Flood Risks in Urban Areas Using Machine Learning}, journal = {WATER}, volume = {13}, year = {2021}, doi = {10.3390/w13243520}, author = {Li, Zhufeng and Liu, Haixing and Luo, Chunbo and Fu, Guangtao}, abstract = {Urban flooding is a devastating natural hazard for cities around the world. Flood risk mapping is a key tool in flood management. However, it is computationally expensive to produce flood risk maps using hydrodynamic models. To this end, this paper investigates the use of machine learning for the assessment of surface water flood risks in urban areas. The factors that are considered in machine learning models include coordinates, elevation, slope gradient, imperviousness, land use, land cover, soil type, substrate, distance to river, distance to road, and normalized difference vegetation index. The machine learning models are tested using the case study of Exeter, UK. The performance of machine learning algorithms, including naive Bayes, perceptron, artificial neural networks (ANNs), and convolutional neural networks (CNNs), is compared based on a spectrum of indicators, e.g., accuracy, F-beta score, and receiver operating characteristic curve. The results obtained from the case study show that the flood risk maps can be accurately generated by the machine learning models. The performance of models on the 30-year flood event is better than 100-year and 1000-year flood events. The CNNs and ANNs outperform the other machine learning algorithms tested. This study shows that machine learning can help provide rapid flood mapping, and contribute to urban flood risk assessment and management.} }
@article{WOS:001101098500001, title = {Machine learning in physics: A short guide}, journal = {EPL}, volume = {144}, year = {2023}, issn = {0295-5075}, doi = {10.1209/0295-5075/ad0575}, author = {Rodrigues, Francisco A.}, abstract = {Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives. Copyright (c) 2023 EPLA} }
@article{WOS:000838722000001, title = {Oil futures volatility predictability: New evidence based on machine learning models}, journal = {INTERNATIONAL REVIEW OF FINANCIAL ANALYSIS}, volume = {83}, year = {2022}, issn = {1057-5219}, doi = {10.1016/j.irfa.2022.102299}, author = {Lu, Xinjie and Ma, Feng and Xu, Jin and Zhang, Zehui}, abstract = {This paper comprehensively examines the connection between oil futures volatility and the financial market based on a model-rich environment, which contains traditional predicting models, machine learning models, and combination models. The results highlight the efficiency of machine learning models for oil futures volatility forecasting, particularly the ensemble models and neural network models. Most interestingly, we consider the ``forecast combination puzzle'' in machine learning models, and find that combination models continue to have more satisfactory performances in all types of situations. We also discuss the model interpretability and each indicator's contribution to the prediction. Our paper provides new insights for machine learning methods' applications in futures market volatility prediction, which is helpful for academics, policy-makers, and investors.} }
@article{WOS:001166813800001, title = {Advances in Machine Learning Processing of Big Data from Disease Diagnosis Sensors}, journal = {ACS SENSORS}, volume = {9}, pages = {1134-1148}, year = {2024}, issn = {2379-3694}, doi = {10.1021/acssensors.3c02670}, author = {Lu, Shasha and Yang, Jianyu and Gu, Yu and He, Dongyuan and Wu, Haocheng and Sun, Wei and Xu, Dong and Li, Changming and Guo, Chunxian}, abstract = {Exploring accurate, noninvasive, and inexpensive disease diagnostic sensors is a critical task in the fields of chemistry, biology, and medicine. The complexity of biological systems and the explosive growth of biomarker data have driven machine learning to become a powerful tool for mining and processing big data from disease diagnosis sensors. With the development of bioinformatics and artificial intelligence (AI), machine learning models formed by data mining have been able to guide more sensitive and accurate molecular computing. This review presents an overview of big data collection approaches and fundamental machine learning algorithms and discusses recent advances in machine learning and molecular computational disease diagnostic sensors. More specifically, we highlight existing modular workflows and key opportunities and challenges for machine learning to achieve disease diagnosis through big data mining.} }
@article{WOS:000911819300007, title = {Neurology education in the era of artificial intelligence}, journal = {CURRENT OPINION IN NEUROLOGY}, volume = {36}, pages = {51-58}, year = {2023}, issn = {1350-7540}, doi = {10.1097/WCO.0000000000001130}, author = {Kedar, Sachin and Khazanchi, Deepak}, abstract = {Purpose of reviewThe practice of neurology is undergoing a paradigm shift because of advances in the field of data science, artificial intelligence, and machine learning. To ensure a smooth transition, physicians must have the knowledge and competence to apply these technologies in clinical practice. In this review, we describe physician perception and preparedness, as well as current state for clinical applications of artificial intelligence and machine learning in neurology.Recent findingsDigital health including artificial intelligence-based/machine learning-based technology has made significant inroads into various aspects of healthcare including neurological care. Surveys of physicians and healthcare stakeholders suggests an overall positive perception about the benefits of artificial intelligence/machine learning in clinical practice. This positive perception is tempered by concerns for lack of knowledge and limited opportunities to build competence in artificial intelligence/machine learning technology. Literature about neurologist's perception and preparedness towards artificial intelligence/machine learning-based technology is scant. There are very few opportunities for physicians particularly neurologists to learn about artificial intelligence/machine learning-based technology.Neurologists have not been surveyed about their perception and preparedness to adopt artificial intelligence/machine learning-based technology in clinical practice. We propose development of a practical artificial intelligence/machine learning curriculum to enhance neurologists' competence in these newer technologies.} }
@article{WOS:000498861800006, title = {Machine learning approaches for wind speed forecasting using long-term monitoring data: a comparative study}, journal = {SMART STRUCTURES AND SYSTEMS}, volume = {24}, pages = {733-744}, year = {2019}, issn = {1738-1584}, doi = {10.12989/sss.2019.24.6.733}, author = {Ye, X. W. and Ding, Y. and Wan, H. P.}, abstract = {Wind speed forecasting is critical for a variety of engineering tasks, such as wind energy harvesting, scheduling of a wind power system, and dynamic control of structures (e.g., wind turbine, bridge, and building). Wind speed, which has characteristics of random, nonlinear and uncertainty, is difficult to forecast. Nowadays, machine learning approaches (generalized regression neural network (GRNN), back propagation neural network (BPNN), and extreme learning machine (ELM)) are widely used for wind speed forecasting. In this study, two schemes are proposed to improve the forecasting performance of machine learning approaches. One is that optimization algorithms, i.e., cross validation (CV), genetic algorithm (GA), and particle swarm optimization (P SO), are used to automatically find the optimal model parameters. The other is that the combination of different machine learning methods is proposed by finite mixture (FM) method. Specifically, CV-GRNN, GA-BPNN, PSO-ELM belong to optimization algorithm-assisted machine learning approaches, and FM is a hybrid machine learning approach consisting of GRNN, BPNN, and ELM. The effectiveness of these machine learning methods in wind speed forecasting are fully investigated by one-year field monitoring data, and their performance is comprehensively compared.} }
@article{WOS:000446413400006, title = {Comparing Multiple Machine Learning Algorithms and Metrics for Estrogen Receptor Binding Prediction}, journal = {MOLECULAR PHARMACEUTICS}, volume = {15}, pages = {4361-4370}, year = {2018}, issn = {1543-8384}, doi = {10.1021/acs.molpharmaceut.8b00546}, author = {Russo, Daniel P. and Zorn, Kimberley M. and Clark, Alex M. and Zhu, Hao and Ekins, Sean}, abstract = {Many chemicals that disrupt endocrine function have been linked to a variety of adverse biological outcomes. However, screening for endocrine disruption using in vitro or in vivo approaches is costly and time-consuming. Computational methods, e.g., quantitative structure activity relationship models, have become more reliable due to bigger training sets, increased computing power, and advanced machine learning algorithms, such as multilayered artificial neural networks. Machine learning models can be used to predict compounds for endocrine disrupting capabilities, such as binding to the estrogen receptor (ER), and allow for prioritization and further testing. In this work, an exhaustive comparison of multiple machine learning algorithms, chemical spaces, and evaluation metrics for ER binding was performed on public data sets curated using in-house cheminformatics software (Assay Central). Chemical features utilized in modeling consisted of binary fingerprints (ECFP6, FCFP6, ToxPrint, or MACCS keys) and continuous molecular descriptors from RDKit. Each feature set was subjected to classic machine learning algorithms (Bernoulli Naive Bayes, AdaBoost Decision Tree, Random Forest, Support Vector Machine) and Deep Neural Networks (DNN). Models were evaluated using a variety of metrics: recall, precision, F1-score, accuracy, area under the receiver operating characteristic curve, Cohen's Kappa, and Matthews correlation coefficient. For predicting compounds within the training set, DNN has an accuracy higher than that of other methods; however, in 5-fold cross validation and external test set predictions, DNN and most classic machine learning models perform similarly regardless of the data set or molecular descriptors used. We have also used the rank normalized scores as a performance-criteria for each machine learning method, and Random Forest performed best on the validation set when ranked by metric or by data sets. These results suggest classic machine learning algorithms may be sufficient to develop high quality predictive models of ER activity.} }
@article{WOS:001136727600004, title = {A primer on the use of machine learning to distil knowledge from data in biological psychiatry}, journal = {MOLECULAR PSYCHIATRY}, year = {2024}, issn = {1359-4184}, doi = {10.1038/s41380-023-02334-2}, author = {Quinn, Thomas P. and Hess, Jonathan L. and Marshe, Victoria S. and Barnett, Michelle M. and Hauschild, Anne-Christin and Maciukiewicz, Malgorzata and Elsheikh, Samar S. M. and Men, Xiaoyu and Schwarz, Emanuel and Trakadis, Yannis J. and Breen, Michael S. and Barnett, Eric J. and Zhang-James, Yanli and Ahsen, Mehmet Eren and Cao, Han and Chen, Junfang and Hou, Jiahui and Salekin, Asif and Lin, I, Ping- and Nicodemus, Kristin K. and Meyer-Lindenberg, Andreas and Bichindaritz, Isabelle and Faraone, Stephen V. and Cairns, Murray J. and Pandey, Gaurav and Mueller, Daniel J. and Glatt, Stephen J. and Machine Learning Psychiat MLPsych}, abstract = {Applications of machine learning in the biomedical sciences are growing rapidly. This growth has been spurred by diverse cross-institutional and interdisciplinary collaborations, public availability of large datasets, an increase in the accessibility of analytic routines, and the availability of powerful computing resources. With this increased access and exposure to machine learning comes a responsibility for education and a deeper understanding of its bases and bounds, borne equally by data scientists seeking to ply their analytic wares in medical research and by biomedical scientists seeking to harness such methods to glean knowledge from data. This article provides an accessible and critical review of machine learning for a biomedically informed audience, as well as its applications in psychiatry. The review covers definitions and expositions of commonly used machine learning methods, and historical trends of their use in psychiatry. We also provide a set of standards, namely Guidelines for REporting Machine Learning Investigations in Neuropsychiatry (GREMLIN), for designing and reporting studies that use machine learning as a primary data-analysis approach. Lastly, we propose the establishment of the Machine Learning in Psychiatry (MLPsych) Consortium, enumerate its objectives, and identify areas of opportunity for future applications of machine learning in biological psychiatry. This review serves as a cautiously optimistic primer on machine learning for those on the precipice as they prepare to dive into the field, either as methodological practitioners or well-informed consumers.} }
@article{WOS:000412062500001, title = {Progressive sampling-based Bayesian optimization for efficient and automatic machine learning model selection}, journal = {HEALTH INFORMATION SCIENCE AND SYSTEMS}, volume = {5}, year = {2017}, issn = {2047-2501}, doi = {10.1007/s13755-017-0023-z}, author = {Zeng, Xueqiang and Luo, Gang}, abstract = {Purpose: Machine learning is broadly used for clinical data analysis. Before training a model, a machine learning algorithm must be selected. Also, the values of one or more model parameters termed hyper-parameters must be set. Selecting algorithms and hyper-parameter values requires advanced machine learning knowledge and many labor-intensive manual iterations. To lower the bar to machine learning, miscellaneous automatic selection methods for algorithms and/or hyper-parameter values have been proposed. Existing automatic selection methods are inefficient on large data sets. This poses a challenge for using machine learning in the clinical big data era. Methods: To address the challenge, this paper presents progressive sampling-based Bayesian optimization, an efficient and automatic selection method for both algorithms and hyper-parameter values. Results: We report an implementation of the method. We show that compared to a state of the art automatic selection method, our method can significantly reduce search time, classification error rate, and standard deviation of error rate due to randomization. Conclusions: This is major progress towards enabling fast turnaround in identifying high-quality solutions required by many machine learning-based clinical data analysis tasks.} }
@article{WOS:000663077000011, title = {FeARH: Federated machine learning with anonymous random hybridization on electronic medical records}, journal = {JOURNAL OF BIOMEDICAL INFORMATICS}, volume = {117}, year = {2021}, issn = {1532-0464}, doi = {10.1016/j.jbi.2021.103735}, author = {Cui, Jianfei and Zhu, He and Deng, Hao and Chen, Ziwei and Liu, Dianbo}, abstract = {Electrical medical records are restricted and difficult to centralize for machine learning model training due to privacy and regulatory issues. One solution is to train models in a distributed manner that involves many parties in the process. However, sometimes certain parties are not trustable, and in this project, we aim to propose an alternative method to traditional federated learning with central analyzer in order to conduct training in a situation without a trustable central analyzer. The proposed algorithm is called ``federated machine learning with anonymous random hybridization (abbreviated as `FeARH')'', using mainly hybridization algorithm to degenerate the integration of connections between medical record data and models' parameters by adding randomization into the parameter sets shared to other parties. Based on our experiment, our new algorithm has similar AUCROC and AUCPR results compared with machine learning in a centralized manner and original federated machine learning.} }
@article{WOS:000483476800034, title = {Applications of Machine Learning Approaches in Emergency Medicine; a Review Article}, journal = {ARCHIVES OF ACADEMIC EMERGENCY MEDICINE}, volume = {7}, year = {2019}, author = {Shafaf, Negin and Malek, Hamed}, abstract = {Using artificial intelligence and machine learning techniques in different medical fields, especially emergency medicine is rapidly growing. In this paper, studies conducted in the recent years on using artificial intelligence in emergency medicine have been collected and assessed. These studies belonged to three categories: prediction and detection of disease; prediction of need for admission, discharge and also mortality; and machine learning based triage systems. In each of these categories, the most important studies have been chosen and accuracy and results of the algorithms have been briefly evaluated by mentioning machine learning techniques and used datasets.} }
@article{WOS:000675886300003, title = {Multi-source transfer learning network to complement knowledge for intelligent diagnosis of machines with unseen faults}, journal = {MECHANICAL SYSTEMS AND SIGNAL PROCESSING}, volume = {162}, year = {2022}, issn = {0888-3270}, doi = {10.1016/j.ymssp.2021.108095}, author = {Yang, Bin and Xu, Songci and Lei, Yaguo and Lee, Chi-Guhn and Stewart, Edward and Roberts, Clive}, abstract = {Most of the current successes of deep transfer learning-based fault diagnosis require two assumptions: 1) the health state set of source machines should overlap that of target machines; 2) the number of target machine samples is balanced across health states. However, such assumptions are unrealistic in engineering scenarios, where target machines suffer from fault types that are not seen in source machines and the target machines are mostly in a healthy state with only occasional faults. As a result, the diagnostic knowledge from source machines may not cover all fault types of target machines nor address imbalanced target samples. Therefore, we propose a framework, called a multi-source transfer learning network (MSTLN), to aggregate and transfer diagnostic knowledge from multiple source machines by combining multiple partial distribution adaptation sub-networks (PDA-Subnets) and a multi-source diagnostic knowledge fusion module. The former weights target samples by counter-balancing factors to jointly adapt partial distributions of source and target pairs, and the latter releases negative effects due to discrepancy among multiple source machines and further fuses diagnostic decisions output from multiple PDA-Subnets. Two case studies demonstrate that MSTLN can reduce the misdiagnosis rate and obtain better transfer performance for imbalanced target samples than other conventional methods.} }
@article{WOS:000405536900025, title = {Twin extreme learning machines for pattern classification}, journal = {NEUROCOMPUTING}, volume = {260}, pages = {235-244}, year = {2017}, issn = {0925-2312}, doi = {10.1016/j.neucom.2017.04.036}, author = {Wan, Yihe and Song, Shiji and Huang, Gao and Li, Shuang}, abstract = {Extreme learning machine (ELM) is an efficient and effective learning algorithm for pattern classification. For binary classification problem, traditional ELM learns only one hyperplane to separate different classes in the feature space. In this paper, we propose a novel twin extreme learning machine (TELM) to simultaneously train two ELMs with two nonparallel classification hyperplanes. Specifically, TELM first utilizes the random feature mapping mechanism to construct the feature space, and then two nonparallel separating hyperplanes are learned for the final classification. For each hyperplane, TELM jointly minimizes its distance to one class and requires it to be far away from the other class. TELM incorporates the idea of twin support vector machine (TSVM) into the basic framework of ELM, thus TELM could have the advantages of the both algorithms. Moreover, compared to TSVM, TELM has fewer optimization constraint variables but with better classification performance. We also introduce a successive over-relaxation technique to speed up the training of our algorithm. Comprehensive experimental results on a large number of datasets verify the effectiveness and efficiency of TELM. (C) 2017 Published by Elsevier B.V.} }
@article{WOS:000747895100011, title = {Evaluating machine learning models for sepsis prediction: A systematic review of methodologies}, journal = {ISCIENCE}, volume = {25}, year = {2022}, doi = {10.1016/j.isci.2021.103651}, author = {Deng, Hong-Fei and Sun, Ming-Wei and Wang, Yu and Zeng, Jun and Yuan, Ting and Li, Ting and Li, Di-Huan and Chen, Wei and Zhou, Ping and Wang, Qi and Jiang, Hua}, abstract = {Studies for sepsis prediction using machine learning are developing rapidly in medical science recently. In this review, we propose a set of new evaluation criteria and reporting standards to assess 21 qualified machine learning models for quality analysis based on PRISMA. Our assessment shows that (1.) the definition of sepsis is not consistent among the studies; (2.) data sources and data preprocessing methods, machine learning models, feature engineering, and inclusion types vary widely among the studies; (3.) the closer to the onset of sepsis, the higher the value of AUROC is; (4.) the improvement in AUROC is primarily due to using machine learning as a feature engineering tool; (5.) deep neural networks coupled with Sepsis-3 diagnostic criteria tend to yield better results on the time series data collected from patients with sepsis. The new evaluation criteria and reporting standards will facilitate the development of improved machine learning models for clinical applications.} }
@article{WOS:000618935000001, title = {Applying machine learning approach in recycling}, journal = {JOURNAL OF MATERIAL CYCLES AND WASTE MANAGEMENT}, volume = {23}, pages = {855-871}, year = {2021}, issn = {1438-4957}, doi = {10.1007/s10163-021-01182-y}, author = {Erkinay Ozdemir, Merve and Ali, Zaara and Subeshan, Balakrishnan and Asmatulu, Eylem}, abstract = {Waste generation has been increasing drastically based on the world's population and economic growth. This has significantly affected human health, natural life, and ecology. The utilization of limited natural resources, and the harming of the earth in the process of mineral extraction, and waste management have far exceeded limits. The recycling rate are continuously increasing; however, assessments show that humans will be creating more waste than ever before. Some difficulties during recycling include the significant expense involved during the separation of recyclable waste from non-disposable waste. Machine learning is the utilization of artificial intelligence (AI) that provides a framework to take as a structural improvement of the fact without being programmed. Machine learning concentrates on the advancement of programs that can obtain the information and use it to learn to make future decisions. The classification and separation of materials in a mixed recycling application in machine learning is a division of AI that is playing an important role for better separation of complex waste. The primary purpose of this study is to analyze AI by focusing on machine learning algorithms used in recycling systems. This study is a compilation of the most recent developments in machine learning used in recycling industries.} }
@article{WOS:000826145900001, title = {Machine learning-driven credit risk: a systemic review}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, volume = {34}, pages = {14327-14339}, year = {2022}, issn = {0941-0643}, doi = {10.1007/s00521-022-07472-2}, author = {Shi, Si and Tse, Rita and Luo, Wuman and D'Addona, Stefano and Pau, Giovanni}, abstract = {Credit risk assessment is at the core of modern economies. Traditionally, it is measured by statistical methods and manual auditing. Recent advances in financial artificial intelligence stemmed from a new wave of machine learning (ML)-driven credit risk models that gained tremendous attention from both industry and academia. In this paper, we systematically review a series of major research contributions (76 papers) over the past eight years using statistical, machine learning and deep learning techniques to address the problems of credit risk. Specifically, we propose a novel classification methodology for ML-driven credit risk algorithms and their performance ranking using public datasets. We further discuss the challenges including data imbalance, dataset inconsistency, model transparency, and inadequate utilization of deep learning models. The results of our review show that: 1) most deep learning models outperform classic machine learning and statistical algorithms in credit risk estimation, and 2) ensemble methods provide higher accuracy compared with single models. Finally, we present summary tables in terms of datasets and proposed models.} }
@article{WOS:001255993200001, title = {Strategies to Enrich Electrochemical Sensing Data with Analytical Relevance for Machine Learning Applications: A Focused Review}, journal = {SENSORS}, volume = {24}, year = {2024}, doi = {10.3390/s24123855}, author = {Kang, Mijeong and Kim, Donghyeon and Kim, Jihee and Kim, Nakyung and Lee, Seunghun}, abstract = {In this review, recent advances regarding the integration of machine learning into electrochemical analysis are overviewed, focusing on the strategies to increase the analytical context of electrochemical data for enhanced machine learning applications. While information-rich electrochemical data offer great potential for machine learning applications, limitations arise when sensors struggle to identify or quantitatively detect target substances in a complex matrix of non-target substances. Advanced machine learning techniques are crucial, but equally important is the development of methods to ensure that electrochemical systems can generate data with reasonable variations across different targets or the different concentrations of a single target. We discuss five strategies developed for building such electrochemical systems, employed in the steps of preparing sensing electrodes, recording signals, and analyzing data. In addition, we explore approaches for acquiring and augmenting the datasets used to train and validate machine learning models. Through these insights, we aim to inspire researchers to fully leverage the potential of machine learning in electroanalytical science.} }
@article{WOS:000884481200001, title = {Interpretable machine learning methods for predictions in systems biology from omics data}, journal = {FRONTIERS IN MOLECULAR BIOSCIENCES}, volume = {9}, year = {2022}, doi = {10.3389/fmolb.2022.926623}, author = {Sidak, David and Schwarzerova, Jana and Weckwerth, Wolfram and Waldherr, Steffen}, abstract = {Machine learning has become a powerful tool for systems biologists, from diagnosing cancer to optimizing kinetic models and predicting the state, growth dynamics, or type of a cell. Potential predictions from complex biological data sets obtained by ``omics `` experiments seem endless, but are often not the main objective of biological research. Often we want to understand the molecular mechanisms of a disease to develop new therapies, or we need to justify a crucial decision that is derived from a prediction. In order to gain such knowledge from data, machine learning models need to be extended. A recent trend to achieve this is to design ``interpretable `` models. However, the notions around interpretability are sometimes ambiguous, and a universal recipe for building well-interpretable models is missing. With this work, we want to familiarize systems biologists with the concept of model interpretability in machine learning. We consider data sets, data preparation, machine learning methods, and software tools relevant to omics research in systems biology. Finally, we try to answer the question: ``What is interpretability? `` We introduce views from the interpretable machine learning community and propose a scheme for categorizing studies on omics data. We then apply these tools to review and categorize recent studies where predictive machine learning models have been constructed from non-sequential omics data.} }
@article{WOS:001068492000001, title = {An empirical evaluation of extreme learning machine uncertainty quantification for automated breast cancer detection}, journal = {NEURAL COMPUTING \\& APPLICATIONS}, year = {2023}, issn = {0941-0643}, doi = {10.1007/s00521-023-08992-1}, author = {Muduli, Debendra and Kumar, Rakesh Ranjan and Pradhan, Jitesh and Kumar, Abhinav}, abstract = {Early detection and diagnosis are the key factors in decreasing the breast cancer mortality rate in medical image analysis. A randomized learning technique called extreme learning machine (ELM) plays a vital role in learning the single hidden layer feed-forward network with fast learning speed and good generalization. The input weight and bias are randomly generated and fixed during the ELM training phase, and subsequently, the analytical procedure determines the output weight. The extreme learning machine's learning ability is based on three uncertainty factors: the number of hidden nodes, an input weight initialization, and the type of activation function in the hidden layer. Various breast classification works have experimented with extreme learning machine techniques and did not investigate the following factors. This paper evaluates the extreme learning machine model's performance with different configurations on the standard ultra-sound breast cancer dataset, BUSI. The proposed extreme learning machine configuration model experimented on original and filtered ultra-sound images. A fivefold stratified cross-validation scheme is applied here to enhance the model's generalization performance. The proposed computer-aided diagnosis (CAD) model provides 100\\% accuracy with the best extreme learning machine configurations. Then, we compare the classification results of the proposed model with typical variants of extreme learning machines like Hybrid ELM (HELM), online-sequential ELM (OS-ELM), Weighted ELM, and complex ELM (CELM). The experimental results demonstrate that the proposed extreme learning machine model is superior to existing models, offering good generalization without any feature extraction or reduction method.} }
@article{WOS:000445815100007, title = {A Tutorial on Machine Learning for Interactive Pedagogical Systems}, journal = {INTERNATIONAL JOURNAL OF SERIOUS GAMES}, volume = {5}, pages = {79-112}, year = {2018}, issn = {2384-8766}, doi = {10.17083/ijsg.v5i3.256}, author = {Melo, Francisco S. and Mascarenhas, Samuel and Paiva, Ana}, abstract = {This paper provides a short introduction to the field of machine learning for interactive pedagogical systems. Departing from different examples encountered in interactive pedagogical systems-such as intelligent tutoring systems or serious games-we go over several representative families of methods in machine learning, introducing key concepts in this field. We discuss common challenges in machine learning and how current methods address such challenges. Conversely, by anchoring our presentation on actual interactive pedagogical systems, highlight how machine learning can benefit the development of such systems.} }
@article{WOS:000729816900003, title = {Prediction of occurrence of extreme events using machine learning}, journal = {EUROPEAN PHYSICAL JOURNAL PLUS}, volume = {137}, year = {2022}, issn = {2190-5444}, doi = {10.1140/epjp/s13360-021-02249-3}, author = {Meiyazhagan, J. and Sudharsan, S. and Venkatesan, A. and Senthilvelan, M.}, abstract = {Machine learning models play a vital role in the prediction task in several fields of study. In this work, we utilize the ability of machine learning algorithms to predict the occurrence of extreme events in a nonlinear mechanical system. Extreme events are rare events that occur ubiquitously in nature. We consider four machine learning models, namely Logistic Regression, Support Vector Machine, Random Forest and Multi-Layer Perceptron in our prediction task. We train these four machine learning models using training set data and compute the performance of each model using the test set data. We show that the Multi-Layer Perceptron model performs better among the four models in the prediction of extreme events in the considered system. The persistent behaviour of the considered machine learning models is cross-checked with randomly shuffled training set and test set data.} }
@article{WOS:000542942700029, title = {Blockchained On-Device Federated Learning}, journal = {IEEE COMMUNICATIONS LETTERS}, volume = {24}, pages = {1279-1283}, year = {2020}, issn = {1089-7798}, doi = {10.1109/LCOMM.2019.2921755}, author = {Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun}, abstract = {By leveraging blockchain, this letter proposes a blockchained federated learning (BlockFL) architecture where local learning model updates are exchanged and verified. This enables on-device machine learning without any centralized training data or coordination by utilizing a consensus mechanism in blockchain. Moreover, we analyze an end-to-end latency model of BlockFL and characterize the optimal block generation rate by considering communication, computation, and consensus delays.} }
@article{WOS:000453925000014, title = {Peering Into the Black Box of Artificial Intelligence: Evaluation Metrics of Machine Learning Methods}, journal = {AMERICAN JOURNAL OF ROENTGENOLOGY}, volume = {212}, pages = {38-43}, year = {2019}, issn = {0361-803X}, doi = {10.2214/AJR.18.20224}, author = {Handelman, Guy S. and Kok, Hong Kuan and Chandra, Ronil V. and Razavi, Amir H. and Huang, Shiwei and Brooks, Mark and Lee, Michael J. and Asadi, Hamed}, abstract = {OBJECTIVE. Machine learning (ML) and artificial intelligence (AI) are rapidly becoming the most talked about and controversial topics in radiology and medicine. Over the past few years, the numbers of ML- or AI-focused studies in the literature have increased almost exponentially, and ML has become a hot topic at academic and industry conferences. However, despite the increased awareness of ML as a tool, many medical professionals have a poor understanding of how ML works and how to critically appraise studies and tools that are presented to us. Thus, we present a brief overview of ML, explain the metrics used in ML and how to interpret them, and explain some of the technical jargon associated with the field so that readers with a medical background and basic knowledge of statistics can feel more comfortable when examining ML applications. CONCLUSION. Attention to sample size, overfitting, underfitting, cross validation, as well as a broad knowledge of the metrics of machine learning, can help those with little or no technical knowledge begin to assess machine learning studies. However, transparency in methods and sharing of algorithms is vital to allow clinicians to assess these tools themselves.} }
@article{WOS:000424191300048, title = {Active learning machine learns to create new quantum experiments}, journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA}, volume = {115}, pages = {1221-1226}, year = {2018}, issn = {0027-8424}, doi = {10.1073/pnas.1714936115}, author = {Melnikov, Alexey A. and Nautrup, Hendrik Poulsen and Krenn, Mario and Dunjko, Vedran and Tiersch, Markus and Zeilinger, Anton and Briegel, Hans J.}, abstract = {How useful can machine learning be in a quantum laboratory? Here we raise the question of the potential of intelligent machines in the context of scientific research. A major motivation for the present work is the unknown reachability of various entanglement classes in quantum experiments. We investigate this question by using the projective simulation model, a physics-oriented approach to artificial intelligence. In our approach, the projective simulation system is challenged to design complex photonic quantum experiments that produce high-dimensional entangled multiphoton states, which are of high interest in modern quantum experiments. The artificial intelligence system learns to create a variety of entangled states and improves the efficiency of their realization. In the process, the system autonomously (re)discovers experimental techniques which are only now becoming standard in modern quantum optical experiments-a trait which was not explicitly demanded from the system but emerged through the process of learning. Such features highlight the possibility that machines could have a significantly more creative role in future research.} }
@article{WOS:000700566400002, title = {Boltzmann machine learning with a variational quantum algorithm}, journal = {PHYSICAL REVIEW A}, volume = {104}, year = {2021}, issn = {2469-9926}, doi = {10.1103/PhysRevA.104.032413}, author = {Shingu, Yuta and Seki, Yuya and Watabe, Shohei and Endo, Suguru and Matsuzaki, Yuichiro and Kawabata, Shiro and Nikuni, Tetsuro and Hakoshima, Hideaki}, abstract = {A Boltzmann machine is a powerful tool for modeling probability distributions that govern the training data. A thermal equilibrium state is typically used for the Boltzmann machine learning to obtain a suitable probability distribution. The Boltzmann machine learning consists of calculating the gradient of the loss function given in terms of the thermal average, which is the most time-consuming procedure. Here, we propose a method to implement the Boltzmann machine learning by using noisy intermediate-scale quantum devices. We prepare an initial pure state that contains all possible computational basis states with the same amplitude, and we apply a variational imaginary time simulation. Readout of the state after the evolution in the computational basis approximates the probability distribution of the thermal equilibrium state that is used for the Boltzmann machine learning. We perform the numerical simulations of our scheme and confirm that the Boltzmann machine learning works well. Our scheme leads to a significant step toward an efficient machine learning using quantum hardware.} }
@article{WOS:001109664000001, title = {Tiny Machine Learning: Progress and Futures}, journal = {IEEE CIRCUITS AND SYSTEMS MAGAZINE}, volume = {23}, pages = {8-34}, year = {2023}, issn = {1531-636X}, doi = {10.1109/MCAS.2023.3302182}, author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Han, Song}, abstract = {Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today's ``large'' model might be tomorrow's ``tiny'' model. The scope of TinyML should evolve and adapt over time.} }
@article{WOS:000730729600001, title = {Predictably Unequal? The Effects of Machine Learning on Credit Markets}, journal = {JOURNAL OF FINANCE}, volume = {77}, pages = {5-47}, year = {2022}, issn = {0022-1082}, doi = {10.1111/jofi.13090}, author = {Fuster, Andreas and Goldsmith-Pinkham, Paul and Ramadorai, Tarun and Walther, Ansgar}, abstract = {Innovations in statistical technology in functions including credit-screening have raised concerns about distributional impacts across categories such as race. Theoretically, distributional effects of better statistical technology can come from greater flexibility to uncover structural relationships or from triangulation of otherwise excluded characteristics. Using data on U.S. mortgages, we predict default using traditional and machine learning models. We find that Black and Hispanic borrowers are disproportionately less likely to gain from the introduction of machine learning. In a simple equilibrium credit market model, machine learning increases disparity in rates between and within groups, with these changes attributable primarily to greater flexibility.} }
@article{WOS:000604147200010, title = {Quantum adversarial machine learning}, journal = {PHYSICAL REVIEW RESEARCH}, volume = {2}, year = {2020}, doi = {10.1103/PhysRevResearch.2.033212}, author = {Lu, Sirui and Duan, Lu-Ming and Deng, Dong-Ling}, abstract = {Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and recently has attracted tremendous attention across different communities. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals another perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.} }
@article{WOS:000487122600008, title = {Importance of machine learning for enhancing ecological studies using information-rich imagery}, journal = {ENDANGERED SPECIES RESEARCH}, volume = {39}, pages = {91-104}, year = {2019}, issn = {1863-5407}, doi = {10.3354/esr00958}, author = {Dujon, Antoine M. and Schofield, Gail}, abstract = {There is increasing demand for efficient ways to process large volumes of data from visual-based remote-technology, such as unmanned aerial vehicles (UAVs) in ecology and conservation, with machine learning methods representing a promising avenue to address varying user demands. Here, we evaluated current trends in how machine learning and UAVs are used to process imagery data for detecting animals and vegetation across habitats, placing emphasis on their utility for endangered species. We reviewed 213 publications that used UAVs at 256 study sites, of which just 89 (42 \\%) used machine learning to assess the visual data. We evaluated geographical and temporal trends and identified how each technology is used at a global scale. We also identified the most commonly encountered machine-learning methods, including potential reasons for their limited use in ecology and possible solutions. Thirteen out of the 17 habitats defined by the International Union for Conservation of Nature (IUCN) habitat classification scheme were monitored using UAVs, while 12 habitats were monitored using both UAVs and machine learning. Our results show that, while machine learning is already being used across many habitat types, it is primarily restricted to more uniform habitats at present. Out of 173 plant and animal species monitored using UAV surveys, 30 were of conservation concern, with machine learning being used to assess UAV imagery data for 9 of these species. In conclusion, we anticipate that the joint use of UAVs and machine learning for ecological research and conservation will expand as machine learning methods become more accessible.} }
@article{WOS:000874987300005, title = {Image-based machine learning for materials science}, journal = {JOURNAL OF APPLIED PHYSICS}, volume = {132}, year = {2022}, issn = {0021-8979}, doi = {10.1063/5.0087381}, author = {Zhang, Lei and Shao, Shaofeng}, abstract = {Materials research studies are dealing with a large number of images, which can now be facilitated via image-based machine learning techniques. In this article, we review recent progress of machine learning-driven image recognition and analysis for the materials and chemical domains. First, the image-based machine learning that facilitates the property prediction of chemicals or materials is discussed. Second, the analysis of nanoscale images including those from a scanning electron microscope and a transmission electron microscope is discussed, which is followed by the discussion about the identification of molecular structures via image recognition. Subsequently, the image-based machine learning works to identify and classify various practical materials such as metal, ceramics, and polymers are provided, and the image recognition for a range of real-scenario device applications such as solar cells is provided in detail. Finally, suggestions and future outlook for image based machine learning for classification and prediction tasks in the materials and chemical science are presented. This article highlights the importance of the integration of the image-based machine learning method into materials and chemical science and calls for a large-scale deployment of image-based machine learning methods for prediction and classification of images in materials and chemical science. Published under an exclusive license by AIP Publishing.} }
@article{WOS:001162062000001, title = {A comprehensive review of machine learning and its application to dairy products}, journal = {CRITICAL REVIEWS IN FOOD SCIENCE AND NUTRITION}, volume = {65}, pages = {1878-1893}, year = {2025}, issn = {1040-8398}, doi = {10.1080/10408398.2024.2312537}, author = {Freire, Paulina and Freire, Diego and Licon, Carmen C.}, abstract = {Machine learning (ML) technology is a powerful tool in food science and engineering offering numerous advantages, from recognizing patterns and predicting outcomes to customizing and adjusting to individual needs. Its further development can enable researchers and industries to significantly enhance the efficiency of dairy processing while providing valuable insights into the field. This paper presents an overview of the role of machine learning in the dairy industry and its potential to improve the efficiency of dairy processing. We performed a systematic search for articles published between January 2003 and January 2023 related to machine learning in dairy products and highlighted the algorithms used. 48 studies are discussed to assist researchers in identifying the best methods that could be applied in their field and providing relevant ideas for future research directions. Moreover, a step-by-step guide to the machine learning process, including a classification of different machine learning algorithms, is provided. This review focuses on state-of-the-art machine learning applications in milk products and their transformation into other dairy products, but it also presents future perspectives and conclusions. The study serves as a valuable guide for individuals in the dairy industry interested in learning about or getting involved with ML.} }
@article{WOS:000435400900032, title = {Arabic Text Categorization using Machine Learning Approaches}, journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS}, volume = {9}, pages = {226-230}, year = {2018}, issn = {2158-107X}, author = {Alshammari, Riyad}, abstract = {Arabic Text categorization is considered one of the severe problems in classification using machine learning algorithms. Achieving high accuracy in Arabic text categorization depends on the preprocessing techniques used to prepare the data set. Thus, in this paper, an investigation of the impact of the preprocessing methods concerning the performance of three machine learning algorithms, namely, Naive Bayesian, DMNBtext and C4.5 is conducted. Results show that the DMNBtext learning algorithm achieved higher performance compared to other machine learning algorithms in categorizing Arabic text.} }
@article{WOS:000802887500001, title = {Matrix product state pre-training for quantum machine learning}, journal = {QUANTUM SCIENCE AND TECHNOLOGY}, volume = {7}, year = {2022}, issn = {2058-9565}, doi = {10.1088/2058-9565/ac7073}, author = {Dborin, James and Barratt, Fergus and Wimalaweera, Vinul and Wright, Lewis and Green, Andrew G.}, abstract = {Hybrid quantum-classical algorithms are a promising candidate for developing uses for NISQ devices. In particular, parametrised quantum circuits (PQCs) paired with classical optimizers have been used as a basis for quantum chemistry and quantum optimization problems. Tensor network methods are being increasingly used as a classical machine learning tool, as well as a tool for studying quantum systems. We introduce a circuit pre-training method based on matrix product state machine learning methods, and demonstrate that it accelerates training of PQCs for both supervised learning, energy minimization, and combinatorial optimization.} }
@article{WOS:000890647400007, title = {Machine Learning for Computer Systems and Networking: A Survey}, journal = {ACM COMPUTING SURVEYS}, volume = {55}, year = {2023}, issn = {0360-0300}, doi = {10.1145/3523057}, author = {Kanakis, Marios Evangelos and Khalili, Ramin and Wang, Lin}, abstract = {Machine learning (ML) has become the de-facto approach for various scientific domains such as computer vision and natural language processing. Despite recent breakthroughs, machine learning has only made its way into the fundamental challenges in computer systems and networking recently. This article attempts to shed light on recent literature that appeals for machine learning-based solutions to traditional problems in computer systems and networking. To this end, we first introduce a taxonomy based on a set of major research problem domains. Then, we present a comprehensive review per domain, where we compare the traditional approaches against the machine learning-based ones. Finally, we discuss the general limitations of machine learning for computer systems and networking, including lack of training data, training overhead, real-time performance, and explainability, and reveal future research directions targeting these limitations.} }
@article{WOS:000931530800001, title = {A hybrid agent-based machine learning method for human-centred energy consumption prediction}, journal = {ENERGY AND BUILDINGS}, volume = {283}, year = {2023}, issn = {0378-7788}, doi = {10.1016/j.enbuild.2023.112797}, author = {Qiao, Qingyao and Yunusa-Kaltungo, Akilu}, abstract = {Occupant behaviour has significant impacts on the performance of machine learning algorithms when predicting building energy consumption. Due to a variety of reasons (e.g., underperforming building energy management systems or restrictions due to privacy policies), the availability of occupational data has long been an obstacle that hinders the performance of machine learning algorithms in predicting building energy consumption. Therefore, this study proposed an agent-based machine learning model whereby agent-based modelling was employed to generate simulated occupational data as input features for machine learning algorithms for building energy consumption prediction. Boruta feature selection was also introduced in this study to select all relevant features. The results indicated that the perfor-mances of machine learning algorithms in predicting building energy consumption were significantly improved when using simulated occupational data, with even greater improvements after conducting Boruta feature selection.(c) 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).} }
@article{WOS:000715130600001, title = {Comparison of different machine learning models for mass appraisal of real estate}, journal = {SURVEY REVIEW}, volume = {55}, pages = {32-43}, year = {2023}, issn = {0039-6265}, doi = {10.1080/00396265.2021.1996799}, author = {Bilgilioglu, Suleyman Sefa and Yilmaz, Haci Murat}, abstract = {The present study aimed to compare five machine learning techniques, namely, artificial neural network (ANN), support vector machine (SVM), chi-square automatic interaction detection (CHAID), classification and regression tree (CART), and random forest (RF) for mass appraisal of real estate. Firstly, 1982 precedent data was collected throughout the entire study area for train and test models. Secondly, a total of 68 variables were considered for the mass appraisal. Subsequently, the five machine learning techniques were applied. Finally, the receiver operating characteristic (ROC) and various statistical methods were applied to compare five machine learning techniques.} }
@article{WOS:000540819200006, title = {Using machine learning in physics-based simulation of fire}, journal = {FIRE SAFETY JOURNAL}, volume = {114}, year = {2020}, issn = {0379-7112}, doi = {10.1016/j.firesaf.2020.102991}, author = {Lattimer, B. Y. and Hodges, J. L. and Lattimer, A. M.}, abstract = {There is a current need to provide rapid, high fidelity predictions of fires to support hazard/risk assessments, use sparse data to understand conditions, and develop mitigation strategies. Machine learning is one approach that has been used to provide rapid predictions based on large amounts of data in business, robotics, and image analysis; however, there have been limited applications to support physics-based or science applications. This paper provides a general overview of machine learning with details on specific techniques being explored for performing low-cost, high fidelity fire predictions. Examples of using both dimensionality reduction (reduced-order models) and deep learning with neural networks are provided. When compared with CFD results, these initial studies show that machine learning can provide full-field predictions 2-3 orders of magnitude faster than CFD simulations. Further work is needed to improve machine learning accuracy and extend these models to more general scenarios.} }
@article{WOS:000630618200001, title = {Machine learning in continuous casting of steel: a state-of-the-art survey}, journal = {JOURNAL OF INTELLIGENT MANUFACTURING}, volume = {33}, pages = {1561-1579}, year = {2022}, issn = {0956-5515}, doi = {10.1007/s10845-021-01754-7}, author = {Cemernek, David and Cemernek, Sandra and Gursch, Heimo and Pandeshwar, Ashwini and Leitner, Thomas and Berger, Matthias and Klosch, Gerald and Kern, Roman}, abstract = {Continuous casting is the most important route for the production of steel today. Due to the physical, mechanical, and chemical components involved in the production, continuous casting is a very complex process, pushing conventional methods of monitoring and control to their limits. In recent years, this complexity and the increasing global competition created a demand for new methods to monitor and control the continuous casting process. Due to the success and associated rise of machine learning techniques in recent years, machine learning nowadays plays an essential role in monitoring and controlling complex processes. This publication presents a scientific survey of machine learning techniques for the analysis of the continuous casting process. We provide an introduction to both the involved fields: an overview of machine learning, and an overview of the continuous casting process. Therefore, we first analyze the existing work concerning machine learning in continuous casting of steel and then synthesize the common concepts into categories, supporting the identification of common use cases and approaches. This analysis is concluded with the elaboration of challenges, potential solutions, and a future outlook of further research directions.} }
@article{WOS:000850207400010, title = {A Review on Machine Learning Strategies for Real-World Engineering Applications}, journal = {MOBILE INFORMATION SYSTEMS}, volume = {2022}, year = {2022}, issn = {1574-017X}, doi = {10.1155/2022/1833507}, author = {Jhaveri, Rutvij H. and Revathi, A. and Ramana, Kadiyala and Raut, Roshani and Dhanaraj, Rajesh Kumar}, abstract = {Huge amounts of data are circulating in the digital world in the era of the Industry 5.0 revolution. Machine learning is experiencing success in several sectors such as intelligent control, decision making, speech recognition, natural language processing, computer graphics, and computer vision, despite the requirement to analyze and interpret data. Due to their amazing performance, Deep Learning and Machine Learning Techniques have recently become extensively recognized and implemented by a variety of real-time engineering applications. Knowledge of machine learning is essential for designing automated and intelligent applications that can handle data in fields such as health, cyber-security, and intelligent transportation systems. There are a range of strategies in the field of machine learning, including reinforcement learning, semi-supervised, unsupervised, and supervised algorithms. This study provides a complete study of managing real-time engineering applications using machine learning, which will improve an application's capabilities and intelligence. This work adds to the understanding of the applicability of various machine learning approaches in real-world applications such as cyber security, healthcare, and intelligent transportation systems. This study highlights the research objectives and obstacles that Machine Learning approaches encounter while managing real-world applications. This study will act as a reference point for both industry professionals and academics, and from a technical standpoint, it will serve as a benchmark for decision-makers on a range of application domains and real-world scenarios.} }
@article{WOS:000396957800011, title = {Entropy-based matrix learning machine for imbalanced data sets}, journal = {PATTERN RECOGNITION LETTERS}, volume = {88}, pages = {72-80}, year = {2017}, issn = {0167-8655}, doi = {10.1016/j.patrec.2017.01.014}, author = {Zhu, Changming and Wang, Zhe}, abstract = {Imbalance problem occurs when negative class contains many more patterns than that of positive class. Since conventional Support Vector Machine (SVM) and Neural Networks (NN) have been proven not to effectively handle imbalanced data, some improved learning machines including Fuzzy SVM (FSVM) have been proposed. FSVM applies a fuzzy membership to each training pattern such that different patterns can give different contributions to the learning machine. However, how to evaluate fuzzy membership becomes the key point to FSVM. Moreover, these learning machines present disadvantages to process matrix patterns. In order to process matrix patterns and to tackle the imbalance problem, this paper proposes an entropy-based matrix learning machine for imbalanced data sets, adopting the Matrix-pattern oriented Ho-Kashyap learning machine with regularization learning (MatMHKS) as the base classifier. The new leaning machine is named EMatMHKS and its contributions are: (1) proposing a new entropy-based fuzzy membership evaluation approach which enhances the importance of patterns, (2) guaranteeing the importance of positive patterns and get a more flexible decision surface. Experiments on real-world imbalanced data sets validate that EMatMHKS outperforms compared learning machines. (C) 2017 Elsevier B.V. All rights reserved.} }
@article{WOS:000495542300001, title = {Can machine learning on economic data better forecast the unemployment rate?}, journal = {APPLIED ECONOMICS LETTERS}, volume = {27}, pages = {1434-1437}, year = {2020}, issn = {1350-4851}, doi = {10.1080/13504851.2019.1688237}, author = {Kreiner, Aaron and Duca, John V.}, abstract = {Using FRED data, a machine-learning model outperforms the Survey of Professional Forecasters and other models since 2001 in forecasting the unemployment rate.} }
@article{WOS:000540984000015, title = {Machine learning, the kidney, and genotype-phenotype analysis}, journal = {KIDNEY INTERNATIONAL}, volume = {97}, pages = {1141-1149}, year = {2020}, issn = {0085-2538}, doi = {10.1016/j.kint.2020.02.028}, author = {Sealfon, Rachel S. G. and Mariani, Laura H. and Kretzler, Matthias and Troyanskaya, Olga G.}, abstract = {With biomedical research transitioning into data-rich science, machine learning provides a powerful toolkit for extracting knowledge from large-scale biological data sets. The increasing availability of comprehensive kidney omics compendia (transcriptomics, proteomics, metabolomics, and genome sequencing), as well as other data modalities such as electronic health records, digital nephropathology repositories, and radiology renal images, makes machine learning approaches increasingly essential for analyzing human kidney data sets. Here, we discuss how machine learning approaches can be applied to the study of kidney disease, with a particular focus on how they can be used for understanding the relationship between genotype and phenotype.} }
@article{WOS:000454421100001, title = {A Review on Industrial Applications of Machine Learning}, journal = {INTERNATIONAL JOURNAL OF DISASTER RECOVERY AND BUSINESS CONTINUITY}, volume = {9}, pages = {1-9}, year = {2018}, issn = {2005-4289}, doi = {10.14257/ijdrbc.2018.9.01}, author = {Rao, N. Thirupathi}, abstract = {Machine learning is the rapidly growing technology in the field of almost all recent technologies in the market. With the successful application of machine learning in almost all the recent technologies, the growth in all the areas was splendid. The growth in those areas has crossed the expectations of the scientists. Recently, the application of machine learning in the areas of medicine and pharma is growing in the recent times in a rapid fast. In the current paper, the authors represent the seven applications or the areas in the field of medicine and pharma where the applications of the machine learning were implementing and good results are obtaining.} }
@article{WOS:000567789900003, title = {A Taxonomy of ML for Systems Problems}, journal = {IEEE MICRO}, volume = {40}, pages = {8-16}, year = {2020}, issn = {0272-1732}, doi = {10.1109/MM.2020.3012883}, author = {Maas, Martin}, abstract = {Machine learning has the potential to significantly improve systems, but only under certain conditions. We describe a taxonomy to help identify whether or not machine learning should be applied to particular systems problems, and which approaches are most promising. We believe that this taxonomy can help practitioners and researchers decide how to most effectively use machine learning in their systems, and provide the community with a framework and vocabulary to discuss different approaches for applying machine learning in systems.} }
@article{WOS:000853623600001, title = {RETRACTED: Performance analysis of machine learning algorithms in heart disease prediction (Retracted Article)}, journal = {CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS}, volume = {30}, pages = {335-343}, year = {2022}, issn = {1063-293X}, doi = {10.1177/1063293X221125231}, author = {Dhasaradhan, K. and Jaichandran, R.}, abstract = {This work presents performance analysis of machine learning algorithms such as logistic regression, naive bayes, decision tree, k nearest neighbour, random forest, support vector machine, and extreme gradient boosting in heart disease prediction. Machine learning algorithms are implemented in python using Scikit learn library in Jupiter notebook. Experiments are conducted by training and testing machine learning algorithms using kaggle heart disease dataset under six test cases. Performance of machine learning algorithms are evaluated using accuracy, precision, recall, F1 score and ROC as metrics. Results show random forest reported high accuracy, precision, recall, F1 score and ROC in heart disease prediction compared to other machine learning algorithms in all six test cases. Results show RF is effective in heart disease prediction in Case 3 with 80\\% train data and 20\\% test data.} }
@article{WOS:001139560900001, title = {Batteries temperature prediction and thermal management using machine learning: An overview}, journal = {ENERGY REPORTS}, volume = {10}, pages = {2277-2305}, year = {2023}, issn = {2352-4847}, doi = {10.1016/j.egyr.2023.08.043}, author = {Al Miaari, Ahmad and Ali, Hafiz Muhammad}, abstract = {Batteries, particularly lithium-ion batteries, play an important role in powering our modern world, from portable devices to electric vehicles and renewable energy storage. However, during charging and discharging, they generate heat due to chemical reactions within them. This heat can lead to reduced performance, shortened lifespan, and even safety risks if not properly managed. To address this problem, Machine learning has been emerged as a changing tool in battery technology due to its ability to analyze large datasets that can be used in predicting battery temperatures and enhancing their thermal management. In this work, we address machine learning features along with a look at its various learning categories, frameworks, and applications. In a comprehensive study, various machine learning methods and neural networks used in battery temperature prediction and thermal management are analyzed and discussed along with its various training algorithms. Moreover, the paper reviews and summarizes various research publications examining battery temperature prediction and battery thermal management using the various machine learning algorithms. As a result, there is no superior machine learning algorithm for battery temperature prediction and thermal management, as the performance of the model may vary depending on the data set, training algorithm, and other parameters. However, among these machine learning algorithms researchers are preferring to use artificial neural networks due to its accuracy and model complexity. In particular, artificial neural network integrated with proper cooling technology can reduce the battery temperature by more than 25\\%.} }
@article{WOS:000558696600001, title = {Applying DevOps Practices of Continuous Automation for Machine Learning}, journal = {INFORMATION}, volume = {11}, year = {2020}, doi = {10.3390/info11070363}, author = {Karamitsos, Ioannis and Albarhami, Saeed and Apostolopoulos, Charalampos}, abstract = {This paper proposes DevOps practices for machine learning application, integrating both the development and operation environment seamlessly. The machine learning processes of development and deployment during the experimentation phase may seem easy. However, if not carefully designed, deploying and using such models may lead to a complex, time-consuming approaches which may require significant and costly efforts for maintenance, improvement, and monitoring. This paper presents how to apply continuous integration (CI) and continuous delivery (CD) principles, practices, and tools so as to minimize waste, support rapid feedback loops, explore the hidden technical debt, improve value delivery and maintenance, and improve operational functions for real-world machine learning applications.} }
@article{WOS:000852913100007, title = {Differentiation of Plastics by Combining Raman Spectroscopy and Machine Learning}, journal = {JOURNAL OF APPLIED SPECTROSCOPY}, volume = {89}, pages = {790-798}, year = {2022}, issn = {0021-9037}, doi = {10.1007/s10812-022-01426-1}, author = {Yang, Y. and Zhang, W. and Wang, Zh and Li, Y.}, abstract = {We combined Raman spectroscopy with machine learning for the classification of 11 plastic samples. A confocal Raman system with an excitation wavelength of 532 nm was used to collect the Raman spectral data of plastic samples and principal component analysis was used for feature extraction. The prediction models of plastic classification based on three machine learning algorithms are compared. The results show that all three machine learning algorithms are able to classify 11 plastics well. This indicates that the combination of Raman spectroscopy and machine learning has great potential in the rapid and nondestructive classification of plastics.} }
