@inproceedings{10.1145/3744199.3744635, title = {Automated Video Segmentation Machine Learning Pipeline}, booktitle = {Proceedings of the Digital Production Symposium}, year = {2025}, isbn = {9798400720086}, doi = {10.1145/3744199.3744635}, url = {https://doi.org/10.1145/3744199.3744635}, author = {Merz, Johannes and Fostier, Lucien}, keywords = {Image Processing, Video Segmentation, Object Detection, Machine Learning}, abstract = {Visual effects (VFX) production often struggles with slow, resource-intensive mask generation. This paper presents an automated video segmentation pipeline that creates temporally consistent instance masks. It employs machine learning for: (1) flexible object detection via text prompts, (2) refined per-frame image segmentation and (3) robust video tracking to ensure temporal stability. Deployed using containerization and leveraging a structured output format, the pipeline was quickly adopted by our artists. It significantly reduces manual effort, speeds up the creation of preliminary composites, and provides comprehensive segmentation data, thereby enhancing overall VFX production efficiency.} }
@inproceedings{10.1145/3769394.3769411, title = {Sustainable Machine Learning: Course 1}, booktitle = {Proceedings of the Conference on 6th ACM Europe Summer School on Data Science}, pages = {17}, year = {2025}, doi = {10.1145/3769394.3769411}, url = {https://doi.org/10.1145/3769394.3769411}, author = {Kemme, Bettina}, abstract = {Machine learning has become increasingly data and processing hungry. A recent report from the International Energy Agency projects that the electricity demand for data centers specialized in AI will more than quadruple by 2030. As such, it has become a pressing need to include energy awareness and environmental sustainability into the Machine Learning life cycle. In fact, a considerable amount of research efforts have been conducted in the last years in this direction.The first part of this tutorial will discuss various mechanisms to assess the environmental impact of machine learning, from power and energy consumption to carbon footprint. This will be put in relation to more traditional performance metrics used in the research literature, from the "goodness" of a ML solution, measured by metrics such as accuracy, to systems performance metrics such as runtime, throughput and scalability. From there, the tutorial will present several concrete research efforts for a quantitative analysis of the environmental footprint of various ML tasks.The second part of the tutorial will outline recent solutions to tackle the huge energy consumption of modern ML. For instance, there have been an increasing number of research efforts to make both the learning and the inference tasks more efficient while providing similar performance in terms of traditional ML performance metrics such as accuracy. A further line of research focuses on adjusting the infrastructure or the execution of ML tasks to be more energy aware, e.g., through scheduling approaches.} }
@inproceedings{10.1145/3757110.3757152, title = {Telecom Customer Churn Prediction with Explainable Machine Learning}, booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning}, pages = {246--250}, year = {2025}, isbn = {9798400714344}, doi = {10.1145/3757110.3757152}, url = {https://doi.org/10.1145/3757110.3757152}, author = {Yuan, Jiaming and Liu, Hao}, keywords = {CatBoost, SHAP, customer churn, machine learning, telecommunication}, abstract = {In the fast-paced era of digital transformation, the telecoms industry is grappling with an increasing issue of customer churn, which endangers stable revenue and operational efficiency. This paper therefore proposes an interpretable machine learning framework that aims to identify customers who are likely to churn more accurately, while also enhancing the interpretability of the model. The performance of the model is evaluated using the publicly available Iranian churn dataset by means of data preprocessing (duplicate removal, outlier detection, feature selection and class imbalance mitigation), training of seven machine learning models tuned with hyper-parameters and selection of five key performance indicators. Experimental results demonstrate that the CatBoost model achieves excellent performance, with an accuracy of 0.9554, precision of 0.9010, recall of 0.8072, F1 score of 0.8503 and AUC-ROC of 0.9823 — surpassing the performance of the other six models. Additionally, this study explored the stacking model, which ultimately proved to be inferior to the CatBoost model. Finally, the optimal model was interpreted using the SHAP technique. The analysis identified three key characteristics affecting customer churn: seconds of use (negative correlation), call failures (positive correlation) and frequency of SMS (negative correlation). This led to the derivation of an operational strategy for customer retention in the telecommunications industry.} }
@article{10.1145/3704807, title = {Collaborative Distributed Machine Learning}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3704807}, url = {https://doi.org/10.1145/3704807}, author = {Jin, David and Kannengieer, Niclas and Rank, Sascha and Sunyaev, Ali}, keywords = {Collaborative distributed machine learning (CDML), privacy-enhancing technologies (PETs), assisted learning, federated learning (FL), split learning, swarm learning, multi-agent systems (MAS)}, abstract = {Various collaborative distributed machine learning (CDML) systems, including federated learning systems and swarm learning systems, with different key traits were developed to leverage resources for the development and use of machine learning models in a confidentiality-preserving way. To meet use case requirements, suitable CDML systems need to be selected. However, comparison between CDML systems to assess their suitability for use cases is often difficult. To support comparison of CDML systems and introduce scientific and practical audiences to the principal functioning and key traits of CDML systems, this work presents a CDML system conceptualization and CDML archetypes.} }
@article{10.1145/3767157, title = {How do Machine Learning Models Change?}, journal = {ACM Trans. Softw. Eng. Methodol.}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3767157}, url = {https://doi.org/10.1145/3767157}, author = {Casta\~no, Joel and Caba\~nas, Rafael and Salmer\'on, Antonio and Lo, David and Mart\'nez-Fern\'andez, Silverio}, keywords = {ML Software Evolution, ML Model Changes, ML Software Releases, Commit Type Classification, Bayesian Networks in Software Engineering}, abstract = {The proliferation of Machine Learning (ML) models and their open-source implementations has transformed Artificial Intelligence research and applications. Platforms like Hugging Face (HF) enable this evolving ecosystem, yet a large-scale longitudinal study of how these models change is lacking. This study addresses this gap by analyzing over 680,000 commits from 100,000 models and 2,251 releases from 202 of these models on HF using repository mining and longitudinal methods. We apply an extended ML change taxonomy to classify commits and use Bayesian networks to model temporal patterns in commit and release activities. Our findings show that commit activities align with established data science methodologies, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM), emphasizing iterative refinement. Release patterns tend to consolidate significant updates, particularly in model outputs, sharing, and documentation, distinguishing them from granular commits. Furthermore, projects with higher popularity exhibit distinct evolutionary paths, often starting from a more mature baseline with fewer foundational commits in their public history. In contrast, those with intensive collaboration show unique documentation and technical evolution patterns. These insights enhance the understanding of model changes on community platforms and provide valuable guidance for best practices in model maintenance.} }
@inproceedings{10.1145/3721251.3736530, title = {Introduction To Generative Machine Learning}, booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Labs}, year = {2025}, isbn = {9798400715501}, doi = {10.1145/3721251.3736530}, url = {https://doi.org/10.1145/3721251.3736530}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {This is an intermediate level course for attendees to gain a strong understanding of the basic principles of generative AI. The course will help build intuition around several topics with easy-to-understand explanations and examples from some of the prevalent algorithms and models including Autoencoders, CNN, Diffusion Models, Transformers, and NeRFs.} }
@inproceedings{10.1145/3767052.3767072, title = {Machine Learning Approaches to Creditworthiness Classification}, booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy}, pages = {127--134}, year = {2025}, isbn = {9798400716010}, doi = {10.1145/3767052.3767072}, url = {https://doi.org/10.1145/3767052.3767072}, author = {Hua, Tianhao}, keywords = {Credit scoring, Random forest, Supervised learning, Support vector machine}, abstract = {Credit risk evaluation is fundamental in financial decision-making, directly influencing lending strategies and default prevention. With the growing availability of structured financial data, machine learning methods have become increasingly prominent in building predictive credit scoring models. This research evaluates Decision Tree, Random Forest, and SVM classifiers for creditworthiness assessment. The Statlog (German Credit Data) dataset from UCI is used with a standardized preprocessing pipeline. Each model was trained and tested on the same dataset split and evaluated using standard classification metrics such as accuracy, precision, recall, and F1 score. Results show that the Random Forest classifier achieved the highest overall performance, particularly in identifying good credit applicants. At the same time, the Decision Tree maintained interpretability, and SVM offered a balanced trade-off. The findings highlight key considerations for model selection in credit scoring applications and suggest ensemble methods as strong candidates for future deployment.} }
@article{10.1145/3761822, title = {Sample Selection Bias in Machine Learning for Healthcare}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761822}, url = {https://doi.org/10.1145/3761822}, author = {Chauhan, Vinod Kumar and Clifton, Lei and Sala\"un, Achille and Lu, Huiqi Yvonne and Branson, Kim and Schwab, Patrick and Nigam, Gaurav and Clifton, David A.}, keywords = {Sample Selection Bias, Target Population, Machine Learning, Healthcare, Risk Prediction}, abstract = {While machine learning algorithms hold promise for personalised medicine, their clinical adoption remains limited, partly due to biases that can compromise the reliability of predictions. In this article, we focus on sample selection bias (SSB), a specific type of bias where the study population is less representative of the target population, leading to biased and potentially harmful decisions. Despite being well-known in the literature, SSB remains scarcely studied in machine learning for healthcare. Moreover, the existing machine learning techniques try to correct the bias mostly by balancing distributions between the study and the target populations, which may result in a loss of predictive performance. To address these problems, our study illustrates the potential risks associated with SSB by examining SSB’s impact on the performance of machine learning algorithms. Most importantly, we propose a new research direction for addressing SSB, based on the target population identification rather than the bias correction. Specifically, we propose two independent networks (T-Net) and a multitasking network (MT-Net) for addressing SSB, where one network/task identifies the target subpopulation which is representative of the study population and the second makes predictions for the identified subpopulation. Our empirical results with synthetic and semi-synthetic datasets highlight that SSB can lead to a large drop in the performance of an algorithm for the target population as compared with the study population, as well as a substantial difference in the performance for the target subpopulations that are representative of the selected and the non-selected patients from the study population. Furthermore, our proposed techniques demonstrate robustness across various settings, including different dataset sizes, event rates and selection rates, outperforming the existing bias correction techniques.} }
@inproceedings{10.1145/3712256.3737464, title = {Rethinking Efficiency in Machine Learning}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, pages = {1}, year = {2025}, isbn = {9798400714658}, doi = {10.1145/3712256.3737464}, url = {https://doi.org/10.1145/3712256.3737464}, author = {Alonso-Betanzos, Amparo}, keywords = {green AI, scalable machine learning, location = NH Malaga Hotel, Malaga, Spain}, abstract = {The success of Artificial Intelligence (AI) has so far relied on developing increasingly precise models. However, this has come at the cost of greater complexity, requiring a higher number of parameters to estimate. As a result, model transparency and explainability have diminished, while the energy demands for training and deployment have skyrocketed. It is estimated that by 2030, AI could account for more than 30\% of the planet's total energy consumption.In this context, green and responsible AI has emerged as a promising alternative, characterized by lower carbon footprints, reduced model sizes, decreased computational complexity, and improved transparency. Various strategies can help achieve these goals, such as improving data quality, developing more energy-efficient execution models, and optimizing energy efficiency in model training and inference. These innovation approaches highlight the potential of green AI to challenge the prevailing paradigm of ever-growing models.} }
@article{10.1145/3705309, title = {Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach}, journal = {ACM Trans. Softw. Eng. Methodol.}, volume = {34}, year = {2025}, issn = {1049-331X}, doi = {10.1145/3705309}, url = {https://doi.org/10.1145/3705309}, author = {Noei, Shayan and Li, Heng and Zou, Ying}, keywords = {Code Refactoring, Refactoring Detection, Python Refactoring, Machine Learning, Code Quality}, abstract = {Refactoring aims to improve the quality of software without altering its functional behaviors. Understanding developers’ refactoring activities is essential to improve software maintainability. The use of machine learning (ML) libraries and frameworks in software systems has significantly increased in recent years, making the maximization of their maintainability crucial. Due to the data-driven nature of ML libraries and frameworks, they often undergo a different development process compared to traditional projects. As a result, they may experience various types of refactoring, such as those related to the data. The state-of-the-art refactoring detection tools have not been tested in the ML technical domain, and they are not specifically designed to detect ML-specific refactoring types (e.g., data manipulation) in ML projects; therefore, they may not adequately find all potential refactoring operations, specifically the ML-specific refactoring operations. Furthermore, a vast number of ML libraries and frameworks are written in Python, which has limited tooling support for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations with relatively high precision. In contrast, for other languages such as Java, state-of-the-art tools are capable of detecting a much more comprehensive list of refactorings. For example, Rminer can detect 99 types of refactoring for Java projects. Inspired by previous work that leverages commit messages to detect refactoring, we introduce MLRefScanner, a prototype tool that applies ML techniques to detect refactoring commits in ML Python projects. MLRefScanner detects commits involving both ML-specific refactoring operations and additional refactoring operations beyond the scope of state-of-the-art refactoring detection tools. To demonstrate the effectiveness of our approach, we evaluate MLRefScanner on 199 ML open source libraries and frameworks and compare MLRefScanner against other refactoring detection tools for Python projects. Our findings show that MLRefScanner outperforms existing tools in detecting refactoring-related commits, achieving an overall precision of 94\% and recall of 82\% for identifying refactoring-related commits. MLRefScanner can identify commits with ML-specific and additional refactoring operations compared to state-of-the-art refactoring detection tools. When combining MLRefScanner with PyRef, we can further increase the precision and recall to 95\% and 99\%, respectively. MLRefScanner provides a valuable contribution to the Python ML community, as it allows ML developers to detect refactoring-related commits more effectively in their ML Python projects. Our study sheds light on the promising direction of leveraging machine learning techniques to detect refactoring activities for other programming languages or technical domains where the commonly used rule-based refactoring detection approaches are not sufficient.} }
@inproceedings{10.1145/3736181.3747139, title = {Introducing Machine Learning to Children in Nigeria}, booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 1}, pages = {204--210}, year = {2025}, isbn = {9798400719295}, doi = {10.1145/3736181.3747139}, url = {https://doi.org/10.1145/3736181.3747139}, author = {John, Avong Emmanuel and Sanusi, Ismaila Temitayo and Oyelere, Solomon Sunday}, keywords = {basic education, children, machine learning education, nigeria, location = Gaborone, Botswana}, abstract = {This study explores the potential of machine learning (ML) education for children in Nigeria, addressing the significant dearth of such initiatives in Africa. We propose the design of ML intervention programs aimed at creating engaging learning experiences for children aged 7 to 16 years, who have no prior exposure to ML concepts. Through hands-on activities using online platforms, LearningML, which introduces classification techniques and image/text recognition and DoodleIt, which focuses on teaching convolutional neural networks, we investigate how these tools facilitate learning in a non-formal setting. Employing a mixed-method approach that includes pre- and post-surveys, as well as participant interviews, we analyze the data using descriptive and thematic methods. Our findings reveal that children grasp the importance of data model, training and other fundamental ML concepts, demonstrating that user-friendly ML tools can ignite curiosity and foster a desire to learn. The implications of this study are significant for advancing ML education among young learners in Africa.} }
@article{10.1145/3737650, title = {Race Against the Machine Learning Courses}, journal = {ACM Trans. Intell. Syst. Technol.}, year = {2025}, issn = {2157-6904}, doi = {10.1145/3737650}, url = {https://doi.org/10.1145/3737650}, author = {Deshpande, Riddhi and Mlombwa, Donald and Celi, Leo Anthony and Gallifant, Jack and D’couto, Helen}, keywords = {1.32 Tutoring and educational systems, 1 Systems and Applications, 2.6 Highly scalable AI algorithms, 2 AI Technology, 1.20 Medical and health systems, 1 Systems and Applications, 1.14 AI in science, 1 Systems and Applications}, abstract = {Despite the rapid integration of AI in healthcare, a critical gap exists in current machine learning courses: the lack of education on identifying and mitigating bias in datasets. This oversight risks perpetuating existing health disparities through biased AI models. Analyzing 11 prominent online courses, we found only 5 addressed dataset bias, often dedicating minimal time compared to technical aspects. This paper urges course developers to prioritize education on data context, equipping learners with the tools to critically evaluate the origin, collection methods, and potential biases inherent in the data. This approach fosters the creation of fair algorithms and the incorporation of diverse data sources, ultimately mitigating the harmful effects of bias in healthcare AI. While this analysis focused on publicly available courses, it underscores the urgency of addressing bias in all healthcare machine learning education. Early intervention in algorithm development is crucial to prevent the amplification of dataset and model bias, ensuring responsible and equitable AI implementation in healthcare.} }
@inproceedings{10.1145/3674029.3674033, title = {Diabetes Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {16--20}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674033}, url = {https://doi.org/10.1145/3674029.3674033}, author = {Tian, Stephanie and Hui, Guanghui}, keywords = {Binary classification, Deep Neural Network (DNN), Diabetes prediction, Machine Learning (ML), Modified Sigmoid Function, location = Oslo, Norway}, abstract = {Machine learning (ML) techniques for healthcare informatics provide health professional insight into disease development. Many healthcare topics are suitable for ML research, such as diabetes prediction and classification. Common ML approaches use a classification method to predict the outcome of the disease for given test data, though these solutions tend to have limited accuracy rates. Further tuning with extra manipulation of the dataset helps improve the model's accuracy to a certain level, but this requires certain professional knowledge in the medical domain. In this research, we propose using a DNN (Deep Neural Network) approach to predict the outcome of diabetes from the test data. Based on the dataset statistics, we simply transform 1D diabetes test data arrays to 2D Farrays without complex medical knowledge. We use a 2D convolution function to extract the features for prediction in addition to modifying the final stage activation function, to which the response is similar to a unit step function for binary classification problems. Our DNN model prediction accuracy has improved over the known non-deep learning classification models.} }
@inproceedings{10.1145/3711896.3737860, title = {8th Workshop on Machine Learning in Finance}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6288--6289}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737860}, url = {https://doi.org/10.1145/3711896.3737860}, author = {Nagrecha, Saurabh and Chaturvedi, Isha and Kumar, Senthil and Chawla, Nitesh and Das, Mahashweta and Yadav, Daksha and Rodriguez-Serrano, Jose A. and Kurshan, Eren}, keywords = {ai, finance, genai, machine learning, location = Toronto ON, Canada}, abstract = {The financial industry leverages machine learning in more ways than just finding the right alpha signal. It grapples with supply chains, business processes, marketing, churn, fraud, and money laundering, all while maintaining compliance with the various regulatory frameworks it is beholden to. Due to the sheer volume of wealth being handled by the financial industry and its critical role in everyday life, it has been a lucrative target for a wide spectrum of ever-evolving bad actors. With each successive iteration of this workshop, we have attempted to capture the breadth of these actors - fraudsters, money launderers, market manipulators, and potentially nation-state-level risks. The emerging advances in Generative AI make this a particularly exciting time to host this workshop. GenAI offers groundbreaking approaches to handling the various data types prevalent in the financial sector. From a security point of view, bad actors are actively using Generative AI creatively to thwart conventional defenses (e.g. voice cloning, better synthetic identities), and this workshop's audience would benefit from commonly applicable defenses \&amp; best practices against such threats. Last but not the least, there is now an increasing willingness from the financial industry towards deeper engagement and data sharing with academia.} }
@inproceedings{10.1145/3696271.3696274, title = {Customer Clusterization using Machine Learning Approach}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {15--19}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696274}, url = {https://doi.org/10.1145/3696271.3696274}, author = {Purnamasari, Fanindia and Putri Nasution, Umaya Ramadhani and Elveny, Marischa and Hayatunnufus, Hayatunnufus}, keywords = {clustering, customer segmentation, k-means, machine learning, silhouette}, abstract = {Understanding customer is crucial for marketing strategies and increasing customer satisfaction in today's business environment. One method to fulfill of marketing strategies is segment customer based on their purchasing habits and demographic characteristics. This study describes a complete approach to customer segmentation based on K-means clustering, an unsupervised machine learning algorithm. There are three stages namely preprocessing to select feature and variable is used to develop clustering model, clustering model implementation, and validation of model. There are four clusters that compare the relationship of marital status and recency to the grocery purchases (product) made by each customer to find out which ingredients we will use to make better products for customers.} }
@inproceedings{10.1145/3623509.3633370, title = {Embodied Machine Learning}, booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}, year = {2024}, isbn = {9798400704024}, doi = {10.1145/3623509.3633370}, url = {https://doi.org/10.1145/3623509.3633370}, author = {Bakogeorge, Alexander and Imtiaz, Syeda Aniqa and Abu Hantash, Nour and Manshaei, Roozbeh and Mazalek, Ali}, keywords = {Embodied interaction, collaboration, machine learning, medical data, tabletop interaction, trust, location = Cork, Ireland}, abstract = {Machine learning becomes more prevalent in specialized domains such as medicine and biology every year, but domain expert trust in machine learning continues to lag behind. Researchers have explored increasing rational trust in AI but little research exists focusing on systems that foster affective and normative trust between domain experts and data scientists who create the models. Tools like Project Jupyter have attempted to bridge this gap between data scientists and domain experts, but failed to see uptake in applied fields or to promote collaboration through co-located synchronous work. To address this we present a proof-of-concept tabletop interactive machine learning system for synchronous, co-located model fine tuning. We tested our system with biology experts and data scientists on a cell biology dataset. Results show that our system promotes interactions between domain experts, data scientists, and the model-in-training and fosters domain expert affective and normative trust in the resulting AI model.} }
@inproceedings{10.1145/3711896.3736560, title = {Data Heterogeneity Modeling for Trustworthy Machine Learning}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6086--6095}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3736560}, url = {https://doi.org/10.1145/3711896.3736560}, author = {Liu, Jiashuo and Cui, Peng}, keywords = {data heterogeneity, out-of-distribution generalization, stability, trustworthy machine learning, location = Toronto ON, Canada}, abstract = {Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline-from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.} }
@inproceedings{10.1145/3641525.3663630, title = {The Idealized Machine Learning Pipeline (IMLP) for Advancing Reproducibility in Machine Learning}, booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability}, pages = {110--120}, year = {2024}, isbn = {9798400705304}, doi = {10.1145/3641525.3663630}, url = {https://doi.org/10.1145/3641525.3663630}, author = {Zheng, Yantong and Stodden, Victoria}, keywords = {Computational Reproducibility, CyberInfrastructure, Human Factors, Machine Learning, Open Code, Open Data, Reproducibility Policy, location = Rennes, France}, abstract = {We investigate the influence of Human Factors on reproducible machine learning, using a novel “Idealized Machine Learning Pipeline” we introduce as a conceptual framework. The study of Human Factors seeks to ensure that systems meet the needs and expectations of people who use and interact with these systems. It also, importantly, seeks to ensure that the capabilities and limitations of those people are accommodated by the system. As increasing the reproducibility of Machine Learning continues to be a community priority, we believe an improved understanding the Human Factors associated with the complex human-machine system of the Machine Learning pipeline could help facilitiate useful steps toward reproducibility. To do this, we first define a practical abstraction of the steps that comprise a typical Machine Learning pipeline for a well-known use case, from raw data through to model estimation used for inference and prediction, that we call the “Idealized Machine Learning Pipeline.” We emphasize that our proposed “Idealized Machine Learning Pipeline” is intended as an abstraction, rather than a directive or description of all Machine Learning workflows, and meant to harmonize and integrate different viewpoints, expertise, and other elements vital to Machine Learning research. Our goal is to enable the research community to coalesce around priorities for theoretical and applied research on reproducible Machine Learning including tools, frameworks, comparisons, and policies, and provide a foundation for communicating and teaching its myriad aspects as an integrated whole. We define the principal steps as follows: 0) Documentation, 1) Problem Definition, 2) Input Data, 3) Data Preparation, 4) Feature Selection, 5) Model Training, 6) Model Evaluation, 7) Preservation \&amp; Publication. To then understand barriers and opportunities attending to the adoption of reproducible Machine Learning pipelines, we leverage the “Idealized Machine Learning Pipeline” for our chosen use case to identify and motivate relevant Human Factors that affect various steps in a reproducible Machine Learning pipeline. We find the identified Human Factors fall into three groups: Incentives; Training; and Error-based Human Factors.} }
@inproceedings{10.1145/3631461.3632516, title = {Distributed Machine Learning}, booktitle = {Proceedings of the 25th International Conference on Distributed Computing and Networking}, pages = {4--7}, year = {2024}, isbn = {9798400716737}, doi = {10.1145/3631461.3632516}, url = {https://doi.org/10.1145/3631461.3632516}, author = {Chatterjee, Bapi}, keywords = {Distributed Machine Learning, Federated Learning, Machine Learning, location = Chennai, India}, abstract = {We explore the landscape of distributed machine learning, focusing on advancements, challenges, and potential future directions in this rapidly evolving field. We delve into the motivation for distributed machine learning, its essential techniques, real-world applications, and open research questions. The theoretical discussion will give an overview of proving the convergence of popular Stochastic Gradient Descent (SGD) Algorithms to train contemporary machine learning models, including the deep learning models with the assumption of non-convexity, in a distributed setting. We will specify the convergence of data parallel SGD for various distributed systems properties, such as asynchronous and compressed communication. We will also discuss distributed machine learning techniques such as model parallelism and tensor parallelism to train large language models (LLMs).} }
@inproceedings{10.1145/3748355.3748363, title = {Empowering machine-learning assisted kernel decisions with eBPFML}, booktitle = {Proceedings of the 3rd Workshop on EBPF and Kernel Extensions}, pages = {28--30}, year = {2025}, isbn = {9798400720840}, doi = {10.1145/3748355.3748363}, url = {https://doi.org/10.1145/3748355.3748363}, author = {Sodhi, Prabhpreet Singh and Liargkovas, Georgios and Kaffes, Kostis}, keywords = {Operating systems, eBPF, hardware acceleration, machine learning, location = Coimbra, Portugal}, abstract = {Machine-learning (ML) techniques can optimize core operating system paths---scheduling, I/O, power, and memory---yet practical deployments remain rare. Existing prototypes either (i) bake simple heuristics directly into the kernel or (ii) off-load inference to user space to exploit discrete accelerators, both of which incur unacceptable engineering or latency cost. We argue that eBPF, the Linux kernel's safe, hot-swappable byte-code runtime, is the missing substrate for moderately complex in-kernel ML. We present eBPFML, a design that (1) extends the eBPF instruction set with matrix-multiply helpers, (2) leverages upcoming CPU matrix engines such as Intel Advanced Matrix Extensions (AMX) through the eBPF JIT, and (3) retains verifier guarantees and CO-RE portability.} }
@article{10.1145/3729432, title = {Performance Evaluation for Detecting and Alleviating Biases in Predictive Machine Learning Models}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3729432}, url = {https://doi.org/10.1145/3729432}, author = {Khakurel, Utsab and Abdelmoumin, Ghada and Rawat, Danda B.}, keywords = {machine learning, bias, identification, mitigation, predictions, classification, ethical AI}, abstract = {Machine Learning (ML) is widely used in various domains but is susceptible to biases that can lead to unfair decisions. Bias can arise from biased data, algorithms, or data collection processes, making it crucial to develop methods that ensure fairness. This article introduces the Detect and Alleviate Bias (DAB) framework, a novel approach designed to identify and mitigate bias in ML models, focusing on sensitive attributes such as gender and race. The key contributions of DAB include: (1) a holistic pipeline that combines data pre-processing, model enhancement, situation testing, and bias mitigation techniques; (2) the application of Counterfactual Fairness testing to detect individual biases; and (3) the integration of multiple bias mitigation strategies to improve fairness in binary classification tasks. We demonstrate the practical implications of DAB through empirical experiments on two widely used datasets, showing that it reduces bias and improves fairness with a slight compromise in model performance, with a significant potential for making an impact in sensitive domains such as healthcare and criminal justice. The results show that pre-processing and post-processing mitigation techniques achieve the best improvements in fairness for unrepresentative datasets, while in-processing methods are more effective in enhancing fairness for representative datasets; however, an inverse relationship between model performance and fairness is observed.} }
@inproceedings{10.1145/3711896.3737246, title = {Maturity Framework for Enhancing Machine Learning Quality}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {4296--4307}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737246}, url = {https://doi.org/10.1145/3711896.3737246}, author = {Castelli, Angelantonio and Chouliaras, Georgios Christos and Goldenberg, Dmitri}, keywords = {machine learning maturity framework, machine learning quality, quality framework, reproducibility, location = Toronto ON, Canada}, abstract = {With the rapid integration of Machine Learning (ML) in business applications and processes, it is crucial to ensure the quality, reliability and reproducibility of such systems. We suggest a methodical approach towards ML system quality assessment and introduce a structured Maturity framework for governance of ML. We emphasize the importance of quality in ML and the need for rigorous assessment, driven by issues in ML governance and gaps in existing frameworks. Our primary contribution is a comprehensive open-sourced quality assessment method, validated with empirical evidence, accompanied by a systematic maturity framework tailored to ML systems. Drawing from applied experience at Booking.com, we discuss challenges and lessons learned during large-scale adoption within organizations. The study presents empirical findings, highlighting quality improvement trends and showcasing business outcomes. The maturity framework for ML systems, aims to become a valuable resource to reshape industry standards and enable a structural approach to improve ML maturity in any organization.} }
@article{10.1145/3701031, title = {Overview of Multimodal Machine Learning}, journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.}, volume = {24}, year = {2025}, issn = {2375-4699}, doi = {10.1145/3701031}, url = {https://doi.org/10.1145/3701031}, author = {Al-Zoghby, Aya M. and Al-Awadly, Esraa Mohamed K. and Ebada, Ahmed Ismail and Awad, Wael A.}, keywords = {Multimodal Machine Learning(MML), Natural Language Processing(NLP), Deep Learning(DL), Fusion Techniques, Cross-Modal}, abstract = {Human nature is fundamentally driven by the need for interaction and attention, which are fulfilled through various sensory modalities, including hearing, sight, touch, taste, and smell. These senses enable us to perceive, understand, and engage with the world around us. The quality and depth of our interactions change considerably when we use multiple senses simultaneously, highlighting the importance of multimodal interactions in our daily lives. In the realm of technology, multimodal integration offers immense value, as it aims to create systems that can replicate or complement these natural human abilities for enhanced interaction.This article explores the significance of spatial multimodalities in machine learning, highlighting their role in improving model performance in applications such as autonomous driving, healthcare, and virtual assistants. It addresses challenges like the complexity of fusing diverse sensory data types and proposes solutions such as advanced data fusion techniques, adaptive learning algorithms, and transformer architectures. The goal is to provide an overview of state-of-the-art research and future directions for advancing human–computer interaction.} }
@inproceedings{10.1145/3696271.3696276, title = {Smart Drying with Machine Learning Methods}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {27--33}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696276}, url = {https://doi.org/10.1145/3696271.3696276}, author = {Chandra, Nicholas Li Jian and Chen, Zhiyuan and Law, Chung Lim}, keywords = {Fuzzy Logic, Moisture Content, Random Forest, Smart Drying, Support Vector Machine}, abstract = {This research aims to improve the prediction of process kinetics in food drying by evaluating and comparing various machine learning models and a hybrid model. Traditional methods for modeling the thermal processes in drying agricultural commodities often fall short due to their complexity. This study investigates four commonly used algorithms in this research domain—Artificial Neural Networks (ANN), Fuzzy Logic (FL), Random Forest (RF), Support Vector Machine (SVM)—and one hybrid model, Fuzzy SVM (FSVM). Data preprocessing included handling missing values, scaling numerical features, and one-hot encoding categorical variables. The models were evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²). Results indicated that the RF model with 100 decision trees and the ANN with 1000 epochs provided promising accuracy. However, the FSVM hybrid model did not demonstrate enhanced predictive capabilities beyond those of its base models.} }
@inproceedings{10.1145/3658644.3690862, title = {Privacy Analyses in Machine Learning}, booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security}, pages = {5110--5112}, year = {2024}, isbn = {9798400706363}, doi = {10.1145/3658644.3690862}, url = {https://doi.org/10.1145/3658644.3690862}, author = {Ye, Jiayuan}, abstract = {Machine learning models sometimes memorize sensitive training data features, posing privacy risks. To control such privacy risks, Dwork et al. proposed the definition of differential privacy (DP) to measure the privacy risks of an algorithm. However, existing DP models either have significantly lower accuracy than their non-private variants or are computationally expensive to train by requiring to incorporate a large amount of public prior knowledge in terms of data or a large pre-trained model. Thus, the fundamental problem of efficiently training an accurate model while preserving DP is not fully addressed. To tackle this problem, we investigate the potential of improving privacy analysis of machine learning algorithms, thus subsequently allowing improved privacy-utility trade-off. First, we observe that the standard DP bound is not tight for large, overparameterized models. Specifically, the DP bound worsens with the number of iterations, and the privacy-accuracy trade-off worsens with the model dimension. This is despite the algorithm converging during training and the finite dimension of training data space. Such potential untightness is more severe for a realistic adversary that does not observe all model parameters, where prior works suggest empirical privacy amplification, and we investigate theoretically. Finally, we take a close look at the privacy risk of each model prediction about individual training data and analyze how to attribute privacy risk to the properties of the data and the choice of model (e.g., architectures). If successful, our research would enable tighter and more informative privacy bounds for differentially private learning, thus in turn allowing improved privacy-utility trade-offs.} }
@inproceedings{10.1145/3747227.3747245, title = {Research on Intrusion Detection Based on Interpretable Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {109--114}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747245}, url = {https://doi.org/10.1145/3747227.3747245}, author = {Chen, Mao and Ma, Bowen and Jiang, Hao}, keywords = {Deep neural networks, Explainable machine learning, Feature importance analysis, Intrusion detection, Port scanning}, abstract = {Intrusion Detection Systems (IDS) grapple with challenges such as delayed updates and a high false positive rate in traditional rule-based approaches under high-dimensional network traffic data. Although deep learning has significantly improved detection accuracy, the conflict between its "black box" nature and traceability requirements still persists. This paper focuses on port scanning attack detection and provides a “dual-wheel-driven” solution of "architecture optimization and interpretability enhancement" to build a high-precision interpretable learning framework. To tackle the vanishing gradient and overfitting problems of traditional Deep Neural Networks (DNN), an enhanced DNN model architecture is designed: through dynamic residual structure, "expansion-compression" design, and adaptive regularization strategy, the learning ability of high-dimensional network traffic features is improved. Moreover, to address the common "black-box" problem in current models, this paper introduces a feature importance quantification method based on gradient-weighting, which integrates dynamic prediction confidence weights to enhance the interpretability analysis of model decisions. In this study, the CICIDS-2017 dataset is chosen for training and evaluation. The results indicated that the enhanced DNN model had significantly better accuracy on the test set than traditional DNN and traditional machine learning models. More importantly, the enhanced DNN model can also identify key attack features, providing high-precision and transparent methodological support for cybersecurity defense.} }
@inproceedings{10.1145/3708360.3708380, title = {Research on Multi-Factor Investment Strategies Utilizing Machine Learning}, booktitle = {Proceedings of the 2024 International Conference on Mathematics and Machine Learning}, pages = {123--126}, year = {2025}, isbn = {9798400711657}, doi = {10.1145/3708360.3708380}, url = {https://doi.org/10.1145/3708360.3708380}, author = {Wang, Bojing}, keywords = {machine learning, machine learning, multi-factor, stock selection strategy}, abstract = {In today's complex and changeable financial market, stock selection strategies have always been the focus of investors' attention. The application of machine learning technology in finance is becoming increasingly widespread. As a classic investment approach, the multi-factor stock selection strategy, when combined with machine learning techniques, is expected to enhance the accuracy and effectiveness of stock selection. This paper uses the overall and individual stock performance of the Shanghai 50 index from June 2023 to June 2024 as a case study to analyze a multi-factor stock selection strategy based on machine learning.} }
@article{10.1145/3708479, title = {Bayesian Machine Learning Meets Formal Methods: An Application to Spatio-Temporal Data}, journal = {ACM Trans. Probab. Mach. Learn.}, volume = {1}, year = {2025}, doi = {10.1145/3708479}, url = {https://doi.org/10.1145/3708479}, author = {Vana-G\"ur, Laura and Visconti, Ennio and Nenzi, Laura and Cadonna, Annalisa and Kastner, Gregor}, keywords = {Bayesian predictive inference, spatio-temporal models, formal verification methods, posterior predictive verification, urban mobility}, abstract = {We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in machine learning, with formal methods, rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.} }
@article{10.1145/3768158, title = {Physics-informed Machine Learning for Medical Image Analysis}, journal = {ACM Comput. Surv.}, volume = {58}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3768158}, url = {https://doi.org/10.1145/3768158}, author = {Banerjee, Chayan and Nguyen, Kien and Salvado, Olivier and Tran, Truyen and Fookes, Clinton}, keywords = {Physics-informed, physics-guided, physics-constrained, PINNs, neural networks, medical image analysis}, abstract = {The incorporation of physical information in machine learning frameworks is transforming medical image analysis (MIA). Integrating fundamental knowledge and governing physical laws not only improves analysis performance but also enhances the model’s robustness and interpretability. This work presents a systematic review of over 100 articles on the utility of PINNs dedicated to MIA (PIMIA) tasks. We propose a unified taxonomy to investigate what physics knowledge and processes are modeled, how they are represented, and the strategies to incorporate them into MIA models. We delve deep into a wide range of image analysis tasks, from imaging, generation, prediction, inverse imaging (super-resolution and reconstruction), registration, and image analysis (segmentation and classification). For each task, we thoroughly examine and present the central physics-guided operation, the region of interest (with respect to human anatomy), the corresponding imaging modality, the datasets used for model training, the deep network architectures employed, and the primary physical processes, equations, or principles utilized. Additionally, we also introduce a novel metric to compare the performance of PIMIA methods across different tasks and datasets. Based on this review, we summarize and distill our perspectives on the challenges, and highlight open research questions and directions for future research.} }
@inbook{10.1145/3696630.3728523, title = {Learning to Edit Interactive Machine Learning Notebooks}, booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}, pages = {681--685}, year = {2025}, isbn = {9798400712760}, url = {https://doi.org/10.1145/3696630.3728523}, author = {Jin, Bihui and Wang, Jiayue and Nie, Pengyu}, abstract = {Machine learning (ML) developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Notebooks provide a convenient tool for writing ML pipelines and interactively observing outputs. However, maintaining notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the ML pipeline code. Moreover, there is no existing benchmark related to developer edits on notebooks.In this paper, we present early results of the first study on learning to edit ML pipeline code in notebooks using large language models (LLMs). We collect the first dataset of 48,398 notebook edits derived from 20,095 revisions of 792 ML-related GitHub repositories. Our dataset captures granular details of file-level and cell-level modifications, offering a foundation for understanding real-world maintenance patterns in ML pipelines. We observe that the edits on notebooks are highly localized. Although LLMs have been shown to be effective on general-purpose code generation and editing, our results reveal that the same LLMs, even after finetuning, have low accuracy on notebook editing, demonstrating the complexity of real-world ML pipeline maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing LLMs' capabilities in engineering ML code.} }
@article{10.5555/3722577.3722593, title = {Localized debiased machine learning: efficient inference on quantile treatment effects and beyond}, journal = {J. Mach. Learn. Res.}, volume = {25}, year = {2024}, issn = {1532-4435}, author = {Kallus, Nathan and Mao, Xiaojie and Uehara, Masatoshi}, keywords = {causal inference, Neyman orthogonality, cross-fitting, instrumental variables, conditional value at risk, expectiles}, abstract = {We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisance functions that depend on the target parameter as an input. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Existing approaches based on flexibly estimating the nuisances and plugging in the estimates, such as debiased machine learning (DML), require we learn the nuisance at all possible inputs. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the target parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or exible relationships, as we demonstrate in empirical studies.} }
@inproceedings{10.1145/3749096.3750027, title = {Towards Blind Quantum Machine Learning in Entanglement Networks}, booktitle = {Proceedings of the 2nd Workshop on Quantum Networks and Distributed Quantum Computing}, pages = {8--13}, year = {2025}, isbn = {9798400720970}, doi = {10.1145/3749096.3750027}, url = {https://doi.org/10.1145/3749096.3750027}, author = {de Abreu, Diego Medeiros and Abel\'em, Ant\^onio}, keywords = {Blind Quantum Computing, Quantum Networks, location = Coimbra, Portugal}, abstract = {Blind Quantum Computation (BQC) enables clients to delegate quantum computations to a quantum server while maintaining the privacy of their data and algorithms, even when the server is untrusted. In this work, we extend BQC frameworks to Quantum Machine Learning (QML) by implementing a network of entangled clients and a quantum server. Specifically, we explore the integration of Variational Quantum Classifiers (VQC) and Quantum Convolutional Neural Networks (QCNNs) within this paradigm. Our proposed model allows clients to perform classical preprocessing and optimization locally while leveraging the quantum server for computationally expensive quantum tasks. The entanglement-based network is managed by a controller that dynamically allocates resources according to the BQC protocol, allowing secure and efficient execution. We present simulation results indicating the feasibility of this approach, including an analysis of network efficiency and resource consumption, alongside the F1 score on QML benchmark datasets.} }
@inproceedings{10.1145/3637528.3671488, title = {Machine Learning in Finance}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6703}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671488}, url = {https://doi.org/10.1145/3637528.3671488}, author = {Akoglu, Leman and Chawla, Nitesh and Domingo-Ferrer, Josep and Kurshan, Eren and Kumar, Senthil and Naware, Vidyut and Rodriguez-Serrano, Jose A. and Chaturvedi, Isha and Nagrecha, Saurabh and Das, Mahashweta and Faruquie, Tanveer}, keywords = {ai, finance, genai, machine learning, location = Barcelona, Spain}, abstract = {This workshop aims to explore the intersection of Generative AI with the rich tapestry of financial data types, seeking to uncover new methodologies and techniques that can enhance predictive analytics, fraud detection, and customer insights across the sector. By harnessing these advancements in AI, we can pave the way to not only understand customer behavior but also anticipate their needs more effectively, leading to superior customer outcomes and more personalized services. Our objective is to shed light on the challenges and opportunities presented by the diverse data formats in finance. We aim to bridge the gap between the dominance of traditional models for tabular data analysis and the emerging potential of Generative AI to revolutionize the treatment of time series, click streams, and other unstructured data forms.} }
@inproceedings{10.1145/3759023.3759127, title = {Using Machine Learning Algorithms to Classify Recyclable Waste}, booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems}, year = {2025}, isbn = {9798400714276}, doi = {10.1145/3759023.3759127}, url = {https://doi.org/10.1145/3759023.3759127}, author = {Sibanda, Mthokozisi and Bradshaw, Karen}, keywords = {Image classification, Transfer learning, Convolutional neural network}, abstract = {Waste management poses a significant global challenge, with 90\% of South Africa’s waste directed to landfills, leading to environmental and health risks. In many regions in Africa, manual waste sorting exposes workers to hazardous conditions, underscoring the need for safer, more efficient recycling practices. This research explores the possibility of removing human involvement in waste recycling through the application of image processing and machine learning techniques, specifically using convolutional neural networks (CNNs) with transfer learning, to automate the sorting of recyclable materials to identify recyclable waste. By accurately classifying waste into categories such as paper, plastics, glass, metals, cardboard, and other waste, the proposed model aims to reduce landfill waste, promote recycling, and create safer working environments.Several proof-of-concept pretrained models are used for binary and multi-class classification tasks, highlighting the performance of CNNs in distinguishing waste types. While binary classification demonstrates high accuracy for specific waste categories, multi-class classification reflects real-world complexities in sorting mixed waste streams. Though limited in its effectiveness for class imbalance, data augmentation points to potential improvements through oversampling techniques. Ultimately, this research could contribute to the development of an automated waste sorting system, enhancing recycling efficiency and reducing the environmental footprint of waste disposal.} }
@inproceedings{10.1145/3718391.3718417, title = {Malware Traffic Analysis using Machine Learning}, booktitle = {Proceedings of the 2024 the 12th International Conference on Information Technology (ICIT)}, pages = {62--67}, year = {2025}, isbn = {9798400717376}, doi = {10.1145/3718391.3718417}, url = {https://doi.org/10.1145/3718391.3718417}, author = {Ji, Jie and Mogos, Gabriela}, keywords = {Machine learning, malware, security}, abstract = {Malware refers to computer code or software that is installed and operated on a user's computer or other terminal without explicit notification or permission, engaging in activities such as stealing, encrypting, modifying, and deleting data, and monitoring the legitimate rights and interests of users. The types of malwares include viruses, worms, Trojans, ransomware, spyware, and so on. Different types of malwares have different attack methods and can cause different damages, resulting in potential financial losses for users. Five machine learning algorithms were used to conduct comparative analysis and find the best performing model to predict potential malware traffic issues in networks. We used the CIC-IDS-2017 dataset, Pearson correlation coefficient to select features and 5-fold cross validation to evaluate the model's generalization ability.} }
@inproceedings{10.1145/3715275.3732195, title = {The Data Minimization Principle in Machine Learning}, booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency}, pages = {3075--3093}, year = {2025}, isbn = {9798400714825}, doi = {10.1145/3715275.3732195}, url = {https://doi.org/10.1145/3715275.3732195}, author = {Ganesh, Prakhar and Tran, Cuong and Shokri, Reza and Fioretto, Ferdinando}, keywords = {Data minimization, Privacy, Data Protection Regulations}, abstract = {The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches. Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations. However, its implementation remains a challenge due to the lack of a rigorous formulation. Our paper addresses this gap and introduces an optimization framework to operationalize the legal definitions of data minimization. It adapts several optimization algorithms to perform data minimization in machine learning and conducts a comprehensive evaluation of their compliance with minimization objectives and their impact on user privacy. Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for data minimization approaches that account for multiple facets of real-world privacy risks.} }
@inproceedings{10.1145/3702653.3744293, title = {Supporting Structured Problem-Solving in Machine Learning Education}, booktitle = {Proceedings of the 2025 ACM Conference on International Computing Education Research V.2}, pages = {63--64}, year = {2025}, isbn = {9798400713415}, doi = {10.1145/3702653.3744293}, url = {https://doi.org/10.1145/3702653.3744293}, author = {Witt, Clemens}, keywords = {Machine Learning Education, Problem-Solving Strategies, Stealth Assessment}, abstract = {This PhD project investigates how students develop problem-solving strategies in machine learning (ML) education. Applying a design-based research approach, it integrates stealth assessment techniques and adaptive feedback mechanisms to foster more structured and systematic engagement with complex ML learning tasks. Initial empirical findings inform the iterative development of a transferable framework to support systematic learning processes across diverse ML contexts, thereby advancing understanding of students’ problem-solving in ML education.} }
@inproceedings{10.1145/3670474.3685973, title = {Machine Learning for High Sigma Analog Designs (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685973}, url = {https://doi.org/10.1145/3670474.3685973}, author = {Jallepalli, Srinivas}, keywords = {Monte Carlo: machine learning, high sigma, importance sampling, parametric yield, scaled sigma sampling, statistical blockade, location = Salt Lake City, UT, USA}, abstract = {Monte Carlo simulations have been the gold standard for assessing parametric yields of analog, mixed signal, and RF circuits as they offer one of the most direct representations of the variation induced by semiconductor manufacturing. However, Monte Carlo analyses are often too expensive for understanding high sigma yields with fewer defects than 1000ppm. To quantify the impact of rare events on circuit yield, we need insights into their probability densities. All rare event sampling techniques that seek to provide this insight employ a machine learning flow of some kind. The various implementations of importance sampling and statistical blockade, for example, try to locate the rare event populations in parametric space through input domain mapping. Despite their popularity, they can sometimes pose a significant challenge, especially when the dimensionality of the input variation space is high, as both feature selection and machine learning can be non-trivial. A good alternative to machine learning in the input parametric domain is the innovative scaled sigma sampling technique that leverages machine learning of the probability density differences produced by scaling input standard deviations. This paper reviews these key approaches for determining the high sigma yields of analog circuits.} }
@inproceedings{10.1145/3745133.3745174, title = {Forecasting ESG Index Based on Machine Learning Methods}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Information Systems}, pages = {241--246}, year = {2025}, isbn = {9798400714375}, doi = {10.1145/3745133.3745174}, url = {https://doi.org/10.1145/3745133.3745174}, author = {Zhao, Juantong}, keywords = {ESG, machine learning, sustainable development, time series prediction}, abstract = {ESG (Environmental, Social, and Governance) is an indicator of corporate sustainability and responsible performance in terms of environmental, social and governance, as well as an important framework for measuring corporate non-financial performance, which is important for balancing economic efficiency with global sustainable development. This study constructs the impact factor system of ESG index in the aspects of environment, society and governance, and then constructs six machine learning models to predict ESG index. The results show that LSTM model performs best, and MSE is 0.14-25.14 times better than other machine learning models BP, CNN, GRU, RNN and MLP. This study provides a scalable, intelligent solution for ESG assessment and reveals the potential value of deep learning in sustainable finance.} }
@article{10.5555/3722479.3722535, title = {Finiteness Considerations in Machine Learning}, journal = {J. Comput. Sci. Coll.}, volume = {40}, pages = {250--262}, year = {2024}, issn = {1937-4771}, author = {Jackson, Jeffrey C.}, abstract = {Many machine learning textbooks include at least some coverage of one or both of the No Free Lunch theorems for learning and probably-approximately correct generalization error bounds. However, it is not a simple matter to reconcile the implications of these two topics and provide advice to students (and practitioners) regarding when learning claims such as "this learned hypothesis will be at least 95\% accurate on previously-unseen data" can reasonably be made. This paper shows how finiteness considerations can potentially provide such a reconciliation. It also suggests that finiteness considerations can be used to simplify certain generalization error bounds by eliminating their reliance on the VC-dimension of hypothesis classes, which might be of independent pedagogical interest.} }
@proceedings{10.1145/3721146, title = {EuroMLSys '25: Proceedings of the 5th Workshop on Machine Learning and Systems}, year = {2025}, isbn = {9798400715389}, abstract = {EuroMLSys gathers AI researchers and practitioners to share innovative advancements in software infrastructure, tools, design principles, theoretical foundations, algorithms, and applications—all viewed from a systems-oriented perspective and harnessing the power of machine learning.} }
@article{10.1145/3616537, title = {Byzantine Machine Learning: A Primer}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3616537}, url = {https://doi.org/10.1145/3616537}, author = {Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael}, keywords = {Byzantine machine learning, distributed SGD, robust aggregation}, abstract = {The problem of Byzantine resilience in distributed machine learning, a.k.a. Byzantine machine learning, consists of designing distributed algorithms that can train an accurate model despite the presence of Byzantine nodes—that is, nodes with corrupt data or machines that can misbehave arbitrarily. By now, many solutions to this important problem have been proposed, most of which build upon the classical stochastic gradient descent scheme. Yet, the literature lacks a unified structure of this emerging field. Consequently, the general understanding on the principles of Byzantine machine learning remains poor. This article addresses this issue by presenting a primer on Byzantine machine learning. In particular, we introduce three pillars of Byzantine machine learning, namely the concepts of breakdown point, robustness, and gradient complexity, to curate the efficacy of a solution. The introduced systematization enables us to (i) bring forth the merits and limitations of the state-of-the-art solutions, and (ii) pave a clear path for future advancements in this field.} }
@inproceedings{10.1145/3760544.3764125, title = {Machine Learning-Based Distance Estimation for Molecular Communication}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {134--138}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764125}, url = {https://doi.org/10.1145/3760544.3764125}, author = {Cheng, Zhen and Zheng, Jianlong and Xu, Ziyan}, keywords = {molecular communication, distance estimation, machine learning, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication (MC) transmits information through the release, diffusion, and reception of molecules, holding great potential in the field of drug delivery. In an MC system, the prediction of the distance between the transmitter and the receiver is crucial for the receiver's resource consumption. Traditional distance detection strategies mainly focus on known channel state information (CSI). To address this limitation, this paper proposes a method for estimating the distance between the transmitters and the receiver in MC system with unknown CSI using a deep neural network (DNN) model. We employ Monte Carlo simulation to capture the positions of molecules in a three-dimensional environment. The dataset is generated based on the molecular coordinates at each position. Numerical results indicate that the DNN model can accurately estimate the distance between the transmitters and the receiver, demonstrating good detection capabilities and generalization ability. Additionally, the minimum distance between the transmitters and the receiver's boundary also affects the accuracy of the distance estimation.} }
@inproceedings{10.1145/3728199.3728288, title = {Analysis of Passenger Satisfaction Evaluation Metrics Based on Machine Learning}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {538--543}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728288}, url = {https://doi.org/10.1145/3728199.3728288}, author = {Wang, Xiucui and Dumlao, Menchita F.}, keywords = {HistGBDT model, Passenger satisfaction, machine learning algorithms}, abstract = {The research in this paper is based on a dataset on airline passenger satisfaction downloaded from the Kaggle data platform, which contains 26 characteristics of passengers, totaling nearly 130,000 records. Using this data set, eight machine learning algorithms are used, such as GBDT, Decision tree, AdaBoost, XGBoost, Random forest, Extremerandom tree, CatBoost, HistGBDT. By analyzing the evaluation indexes of these 8 machine learning algorithms, it is concluded that HistGBDT algorithm has higher values in precision and accuracy. This result page verifies that HistGBDT model can improve the predictive ability of passenger satisfaction. Increase the generalization ability of their models. It also shows that the model is superior in predicting airline satisfaction. Through this result analysis, we can help airlines to better provide passengers with better quality services and meet their different needs, so as to provide airlines with more personalized services and decisions, and enhance their competitive advantages.} }
@inproceedings{10.1145/3728199.3728243, title = {Machine Learning Based Structural Design and Performance Prediction of Advanced Materials}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {269--272}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728243}, url = {https://doi.org/10.1145/3728199.3728243}, author = {You, Weichen}, keywords = {Graph neural network, Intelligent optimisation, Machine learning, Material structure design, Performance prediction}, abstract = {This paper explores the core methods of machine learning in material modelling, structure-property correlation analysis and optimal design, including data pre-processing, feature engineering, algorithm selection and model training strategies. Combining deep learning, graph neural network (GNN) and gradient boosting decision tree (GBDT) methods, accurate prediction of material properties can be achieved and material structures can be optimised by Bayesian optimisation, genetic algorithms and other intelligent search strategies. It is shown that the machine learning-based material design method can significantly improve the efficiency of material screening and accelerate the development process of new materials.} }
@inproceedings{10.1145/3722237.3722398, title = {Machine Learning and Graduate Education in Finance}, booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education}, pages = {937--943}, year = {2025}, isbn = {9798400712692}, doi = {10.1145/3722237.3722398}, url = {https://doi.org/10.1145/3722237.3722398}, author = {Zhang, Kan and Wang, Fengqingyang and Wang, Suze}, keywords = {Causal Inference, Data Analysis, Financial Education, Machine Learning, Prediction}, abstract = {This paper discusses the application of machine learning in graduate education in finance, emphasizing its importance in data analysis, prediction, and causal inference. The paper points out that machine learning can enhance students' professional competence, expand their knowledge areas, and enhance their work abilities. Through theoretical teaching and research applications, machine learning methods can help process large amounts of unstructured data and combine with traditional econometric methods to improve the accuracy and explanatory power of research. The survey results support the effectiveness of machine learning teaching. At the end of the paper, it is proposed to continuously update the teaching content of machine learning technology, while paying attention to its limitations, in order to promote the improvement of students' comprehensive abilities.} }
@inproceedings{10.1145/3673791.3698439, title = {Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, booktitle = {Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region}, pages = {299--302}, year = {2024}, isbn = {9798400707247}, doi = {10.1145/3673791.3698439}, url = {https://doi.org/10.1145/3673791.3698439}, author = {Diaz, Fernando and Drozdov, Andrew and Kim, To Eun and Salemi, Alireza and Zamani, Hamed}, keywords = {information retrieval, machine learning, location = Tokyo, Japan}, abstract = {Retrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.} }
@inproceedings{10.1145/3641555.3705218, title = {Mathematics for Machine Learning: A Bridge Course}, booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2}, pages = {1437--1438}, year = {2025}, isbn = {9798400705328}, doi = {10.1145/3641555.3705218}, url = {https://doi.org/10.1145/3641555.3705218}, author = {Deng, Samuel}, keywords = {bridge course, curriculum, machine learning education, mathematics education, location = Pittsburgh, PA, USA}, abstract = {We present Mathematics for Machine Learning, a one-semester mathematics course designed to strengthen students' foundations before further study and research in machine learning (ML) and data science. Oftentimes, the mathematical prerequisites needed for serious study of ML are taught in a disjointed manner. Our course is designed to bridge this gap and provide emphasis on concepts heavily employed in modern ML, such as spectral analysis in linear algebra or convex optimization in calculus. We structured our course around the three "pillars'' of math that underlie much of modern ML: (i) linear algebra, (ii) calculus and optimization, and (iii) probability and statistics. Weaving each of these together is a central story --- all concepts, ideas, and proofs are introduced relative to two ubiquitous concepts in machine learning: least squares regression and gradient descent, providing a consistent anchoring narrative and constant motivation for mathematical ideas.} }
@inproceedings{10.1145/3746972.3747002, title = {Hybrid Machine Learning for Used Car Price Determinants}, booktitle = {Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing}, pages = {181--186}, year = {2025}, isbn = {9798400713576}, doi = {10.1145/3746972.3747002}, url = {https://doi.org/10.1145/3746972.3747002}, author = {Feng, Jianli}, keywords = {Gradient Descent, Logistic Regression, Machine Learning, Multiple Linear Regression, Support Vector Machine}, abstract = {This study develops a methodological framework for used car valuation through comparative analysis of regression techniques. Initial factor significance evaluation was conducted using Multiple Linear Regression (MLR) and Logistic Regression (LR), identifying ten key pricing determinants. Twelve regression models were subsequently implemented with these predictors, revealing LR's superior performance over MLR in coefficient stability and interpretability. The predictive phase comparatively evaluates Gradient Descent (GD) and Support Vector Machine (SVM) algorithms, with systematic residual analysis demonstrating SVM's optimal prediction accuracy. The findings provide empirical guidance for machine learning applications in automotive residual value estimation.} }
@article{10.1145/3761823, title = {Integration of fNIRS and Machine Learning for Identifying Parkinson’s Disease}, journal = {ACM Trans. Comput. Healthcare}, volume = {6}, year = {2025}, doi = {10.1145/3761823}, url = {https://doi.org/10.1145/3761823}, author = {Sousani, Maryam and Rojas, Raul Fernandez and Preston, Elisabeth and Ghahramani, Maryam}, keywords = {Parkinson’s Disease, fNIRS, Machine Learning, Biomarker}, abstract = {Parkinson’s disease (PD) is a neurodegenerative disorder where early diagnosis is crucial for effective management. However, current diagnostic methods are often invasive or delayed, hindering early intervention. This study evaluates the effectiveness of combining functional near-infrared spectroscopy (fNIRS) with machine learning to distinguish individuals with PD from age-matched controls.Data were collected using fNIRS from 28 people with PD and 32 age-matched controls while performing the Timed Up and Go (TUG) test under three conditions: simple TUG, cognitive dual-task TUG and motor dual-task TUG. Changes in cerebral blood oxygenation in the prefrontal cortex (PFC) were analysed using four machine learning models: Support Vector Machine (SVM), K-Nearest Neighbours (KNN), Random Forest (RF) and Extreme Gradient Boosting (XGB), along with statistical analyses. Two feature selection models identified key features and channels for differentiating PD from controls.The SVM model achieved the highest accuracy (0.85 (pm) 0.35) in distinguishing PD from CG. Feature selection and statistical analysis showed that dual-task activities were more effective than simple tasks for distinguishing PD from CG. Specific PFC subregions exhibited distinct activation patterns, which could serve as potential biomarkers for PD detection. Combining fNIRS with machine learning shows promise for PD diagnosis, with dual-task activities enhancing accuracy. Further investigation into PFC subregion behaviour could reveal stronger biomarkers.} }
@inproceedings{10.1145/3724363.3729032, title = {Are Interactive Visualizations in Machine Learning Education Helping Students?}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {2--8}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729032}, url = {https://doi.org/10.1145/3724363.3729032}, author = {Rentea, Ilinca and Migut, Gosia and Krijthe, Jesse}, keywords = {controlled experiment, education, interactive visualizations, knowledge gain, machine learning, motivation, location = Nijmegen, Netherlands}, abstract = {With the fast integration of Machine Learning (ML) across industries, effective pedagogical strategies are essential for teaching this complex and evolving field. Machine Learning is now widely integrated into various university programs and introduced at earlier educational stages, including high school and secondary school. However, ML pedagogy lacks standardized teaching methods compared to other science-related subjects, which have established norms for topic introduction, teaching tools, and assessment methods. Inspired by other fields, this research explores the use of interactive visualizations in teaching ML topics, more specifically in teaching Gradient Descent (GD) and Principal Component Analysis (PCA). The target population consists of Computer Science and Engineering Bachelor students who have not yet followed any Machine Learning courses but have foundational knowledge in calculus, linear algebra, and statistics. The evaluation measures knowledge gained and student motivation, compared to a static version of the materials. Results show a significant positive effect in knowledge related to PCA with interactive visualizations, but no differences in knowledge gain for GD or in learning motivation for either topic. With these results, we contribute to the body of evidence-based teaching methods in Machine Learning and identify further research needed to generalize the effect of interactive visualizations as a teaching method for teaching ML basic concepts.} }
@inproceedings{10.1145/3711896.3737858, title = {3rd Workshop on Causal Inference and Machine Learning in Practice}, booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2}, pages = {6290--6291}, year = {2025}, isbn = {9798400714542}, doi = {10.1145/3711896.3737858}, url = {https://doi.org/10.1145/3711896.3737858}, author = {Lee, Jeong-Yoon and Pan, Jing and Wu, Yifeng and Harinen, Totte and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Yin, Sichao and Stevenson, Roland and Wang, Jingshen and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, keywords = {causal inference, machine learning, location = Toronto ON, Canada}, abstract = {The 3rd Workshop on Causal Inference and Machine Learning in Practice at KDD 2025 aims to bring together researchers, industry professionals, and practitioners to explore the application of causal inference within machine learning models. As causal machine learning techniques gain traction across industries, practical challenges related to trustworthiness, robustness, and fairness remain at the forefront. This workshop will provide a forum to discuss methodologies for evaluating causal models in real-world scenarios and explore innovative applications that integrate causal inference with generative AI (GenAI) and large language models (LLMs). Topics of interest include using GenAI and LLMs to facilitate causal inference tasks and leveraging causal inference techniques for evaluating and improving GenAI/LLM models. Building on the success of the previous workshop editions at KDD 2023 and KDD 2024, which attracted over 200 and 250 participants, respectively, this workshop will continue fostering collaboration between academia and industry. Through invited talks, contributed papers, and interactive discussions, we will address key challenges and opportunities at the intersection of causal inference and machine learning. As the field continues to evolve, this workshop serves as a crucial platform for knowledge exchange and innovation, driving forward the application of causal techniques in machine learning and AI.} }
@article{10.1145/3719663, title = {Machine Learning for Infectious Disease Risk Prediction: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3719663}, url = {https://doi.org/10.1145/3719663}, author = {Liu, Mutong and Liu, Yang and Liu, Jiming}, keywords = {Machine learning, data-driven modeling, epidemiology-inspired learning, infectious disease risk prediction, transmission dynamics characterization}, abstract = {Infectious diseases place a heavy burden on public health worldwide. In this article, we systematically investigate how machine learning (ML) can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation for using ML for infectious disease risk prediction. Next, we describe the development and application of various ML models for infectious disease risk prediction, categorizing them according to the models’ alignment with vital public health concerns specific to two distinct phases of infectious disease propagation: (1) the pandemic and epidemic phases (the P-E phases) and (2) the endemic and elimination phases (the E-E phases), with each presenting its own set of critical questions. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluations. We conclude with a discussion of open questions and future directions.} }
@article{10.1145/3725809, title = {Differentiable Economics: Strategic Behavior, Mechanisms, and Machine Learning}, journal = {Commun. ACM}, volume = {68}, pages = {80--88}, year = {2025}, issn = {0001-0782}, doi = {10.1145/3725809}, url = {https://doi.org/10.1145/3725809}, author = {Bichler, Martin and Parkes, David C.}, abstract = {Economics studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these agents. Game theory had a substantial impact on economic modeling because it allows us to model the outcome of such economic interaction while taking the incentives of individual agents into account. Mechanism design does the same when designing the rules of economic institutions. Unfortunately, these economic models have turned out to be computationally hard to solve. For example, finding equilibrium in some incomplete-information models of markets with continuous valuation and action spaces are hard in PP, and designing a revenue-maximizing multi-item auction is #P-hard. This computational complexity poses a fundamental barrier in modeling economic systems but is worst-case and considers non-generic instances. Differentiable economics describes a new approach to solving these central problems in the economic sciences. It uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems. In particular, neural networks and learning algorithms such as Stochastic Gradient Descent have been shown to be very effective. Machine learning has led to breakthroughs in many sciences, and it also holds the potential to fundamentally alter how we analyze and design economic systems.A new approach to solving central problems in the economic sciences uses learning algorithms to find or approximate solutions to equilibrium computation or economic design problems.} }
@article{10.1145/3643456, title = {Pitfalls in Machine Learning for Computer Security}, journal = {Commun. ACM}, volume = {67}, pages = {104--112}, year = {2024}, issn = {0001-0782}, doi = {10.1145/3643456}, url = {https://doi.org/10.1145/3643456}, author = {Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad}, abstract = {With the growing processing power of computing systems and the increasing availability of massive datasets, machine learning algorithms have led to major breakthroughs in many different areas. This development has influenced computer security, spawning a series of work on learning-based security systems, such as for malware detection, vulnerability discovery, and binary code analysis. Despite great potential, machine learning in security is prone to subtle pitfalls that undermine its performance and render learning-based systems potentially unsuitable for security tasks and practical deployment.In this paper, we look at this problem with critical eyes. First, we identify common pitfalls in the design, implementation, and evaluation of learning-based security systems. We conduct a study of 30 papers from top-tier security conferences within the past 10 years, confirming that these pitfalls are widespread in the current security literature. In an empirical analysis, we further demonstrate how individual pitfalls can lead to unrealistic performance and interpretations, obstructing the understanding of the security problem at hand. As a remedy, we propose actionable recommendations to support researchers in avoiding or mitigating the pitfalls where possible. Furthermore, we identify open problems when applying machine learning in security and provide directions for further research.} }
@article{10.1145/3733838, title = {Practitioners and Bias in Machine Learning: A Study}, journal = {ACM Trans. Interact. Intell. Syst.}, volume = {15}, year = {2025}, issn = {2160-6455}, doi = {10.1145/3733838}, url = {https://doi.org/10.1145/3733838}, author = {Cinca, Robert and Costanza, Enrico and Musolesi, Mirco}, keywords = {ML Bias, Operationalizing Bias, machine learning, machine learning practitioners, interview study}, abstract = {The increasing adoption of machine learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with 22 participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.} }
@inproceedings{10.1145/3674029.3674050, title = {Machine Learning Tool for Wildlife Image Classification}, booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies}, pages = {127--132}, year = {2024}, isbn = {9798400716379}, doi = {10.1145/3674029.3674050}, url = {https://doi.org/10.1145/3674029.3674050}, author = {Seljebotn, Karoline and Lawal, Isah A.}, keywords = {Camera Traps, Deep Learning, Wildlife Classification, location = Oslo, Norway}, abstract = {Wildlife researchers gather a large amount of image data during fieldwork. Reviewing this data is time-consuming and requires specialized expertise. To address this issue, machine learning models can automatically classify animals in these images. This study introduces a new method for classifying animals in both benchmark and camera trap images using a single model. The model achieved a top-1 accuracy of 93\% for benchmark images and 56\% for camera trap images previously unseen. The model was integrated into a web application, making it accessible to wildlife researchers without programming knowledge.} }
@inproceedings{10.1145/3627673.3679095, title = {Data Quality-aware Graph Machine Learning}, booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages = {5534--5537}, year = {2024}, isbn = {9798400704369}, doi = {10.1145/3627673.3679095}, url = {https://doi.org/10.1145/3627673.3679095}, author = {Wang, Yu and Ding, Kaize and Liu, Xiaorui and Kang, Jian and Rossi, Ryan and Derr, Tyler}, keywords = {data-centric artificial intelligence, graph machine learning, location = Boise, ID, USA}, abstract = {Recent years have seen a significant shift in Artificial Intelligence from model-centric to data-centric approaches, highlighted by the success of large foundational models. Following this trend, despite numerous innovations in graph machine learning model design, graph-structured data often suffers from data quality issues, jeopardizing the progress of Data-centric AI in graph-structured applications. Our proposed tutorial addresses this gap by raising awareness about data quality issues within the graph machine-learning community. We provide an overview of existing topology, imbalance, bias, limited data, and abnormality issues in graph data. Additionally, we highlight recent developments in foundational graph models that focus on identifying, investigating, mitigating, and resolving these issues.} }
@article{10.1145/3773898, title = {Eye-Tracking Indicators of Novice Programmers’ Proficiency: A Machine Learning Approach}, journal = {ACM Trans. Comput. Educ.}, year = {2025}, doi = {10.1145/3773898}, url = {https://doi.org/10.1145/3773898}, author = {Ahsan, Zubair and Obaidellah, Unaizah}, keywords = {Machine Learning, Computer Education, Human-Computer Interaction}, abstract = {This study investigates the efficacy of machine learning algorithms in classifying different levels of programming expertise among 60 first-year undergraduate computer science students from Asian demographic backgrounds using eye-tracking data. Existing studies offer limited detail on the construction and selection of feature sets for machine learning modeling. The study identifies Total Fixation Duration (TFD) and Total Visit Duration (TVD) as robust indicators for machine learning models when distinguishing between high and low performers (two levels) achieving accuracy as high as 76\%. However, performance declines notably when classifying expertise into three levels (high, average, and low), with performance dropping below 50\%, indicating that binary labels yield more reliable predictions than finer-grained categorization. Our findings suggest that such fixation-based metrics can provide real-time insights into student engagement and potentially cognitive effort, offering opportunities for adaptive instruction and targeted support. Hence, this model can be utilized for real-time screening of novice students during programming tasks in classroom settings, allowing educators to identify students requiring additional support, thereby enhancing programming education. Future research should address study limitations by increasing sample size, diversifying participant demographics, and cross-validating model performance with students’ grades.} }
@inproceedings{10.1145/3747227.3747246, title = {A Review of Machine Learning Algorithms Applied to Reservoir Exploration and Prediction}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {115--120}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747246}, url = {https://doi.org/10.1145/3747227.3747246}, author = {Lu, Bing and Wang, Hanqing and Zhao, Huilan and Song, Huilan and Li, Yan}, keywords = {Deep learning, Low-permeability reservoirs, Machine learning, Reservoir prediction}, abstract = {Reservoir exploration and prediction are highly complex tasks. They require knowledge from multiple disciplines. Key challenges include interpreting multidimensional geological data and evaluating low-permeability reservoirs. Traditional methods mainly rely on porosity and permeability. However, they often fail to reflect the heterogeneity and true storage capacity of tight reservoirs. In recent years, machine learning has become a promising alternative. Algorithms such as Support Vector Machines (SVM), Random Forests (RF), clustering, and deep learning models like CNNs and RNNs provide new tools for reservoir analysis. These methods are well-suited for handling complex, nonlinear, and high-dimensional data. In this work, we review recent developments in the use of machine learning for reservoir evaluation. It highlights how these techniques improve quantitative assessment and support better decision-making in oil and gas development.} }
@inproceedings{10.1145/3670865.3673573, title = {Machine Learning-Powered Course Allocation}, booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation}, pages = {1099}, year = {2024}, isbn = {9798400707049}, doi = {10.1145/3670865.3673573}, url = {https://doi.org/10.1145/3670865.3673573}, author = {Soumalias, Ermis and Zamanlooy, Behnoosh and Weissteiner, Jakob and Seuken, Sven}, keywords = {course allocation, preference elicitation, combinatorial assignment, location = New Haven, CT, USA}, abstract = {We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7\%--11\% and 17\%--29\%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM while making the upgrade process simple for universities and seamless for their students.} }
@article{10.1145/3736751, title = {Maintainability and Scalability in Machine Learning: Challenges and Solutions}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3736751}, url = {https://doi.org/10.1145/3736751}, author = {Shivashankar, Karthik and Al Hajj, Ghadi and Martini, Antonio}, keywords = {Machine learning, deep learning, maintainability, scalability}, abstract = {Rapid advancements in Machine Learning (ML) introduce unique maintainability and scalability challenges. Our research addresses the evolving challenges and identifies ML maintainability and scalability solutions by conducting a thorough literature review of over 17,000 papers, ultimately refining our focus to 124 relevant sources that meet our stringent selection criteria. We present a catalogue of 41 Maintainability and 13 Scalability challenges and solutions across Data, Model Engineering and the overall development of ML applications and systems. This study equips practitioners with insights on building robust ML applications, laying the groundwork for future research on improving ML system robustness at different workflow stages.} }
@article{10.5555/3648699.3648808, title = {Dimensionless machine learning: imposing exact units equivariance}, journal = {J. Mach. Learn. Res.}, volume = {24}, year = {2023}, issn = {1532-4435}, author = {Villar, Soledad and Yao, Weichi and Hogg, David W. and Blum-Smith, Ben and Dumitrascu, Bianca}, abstract = {Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods that are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.} }
@article{10.1145/3709705, title = {Modyn: Data-Centric Machine Learning Pipeline Orchestration}, journal = {Proc. ACM Manag. Data}, volume = {3}, year = {2025}, doi = {10.1145/3709705}, url = {https://doi.org/10.1145/3709705}, author = {B\"other, Maximilian and Robroek, Ties and Gsteiger, Viktor and Holzinger, Robin and Ma, Xianzhe and T\"oz\"un, Pnar and Klimovic, Ana}, keywords = {data-centric ai, machine learning pipelines, online learning}, abstract = {In real-world machine learning (ML) pipelines, datasets are continuously growing. Models must incorporate this new training data to improve generalization and adapt to potential distribution shifts. The cost of model retraining is proportional to how frequently the model is retrained and how much data it is trained on, which makes the naive approach of retraining from scratch each time impractical. We present Modyn, a data-centric end-to-end machine learning platform. Modyn's ML pipeline abstraction enables users to declaratively describe policies for continuously training a model on a growing dataset. Modyn pipelines allow users to apply data selection policies (to reduce the number of data points) and triggering policies (to reduce the number of trainings). Modyn executes and orchestrates these continuous ML training pipelines. The system is open-source and comes with an ecosystem of benchmark datasets, models, and tooling. We formally discuss how to measure the performance of ML pipelines by introducing the concept of composite models, enabling fair comparison of pipelines with different data selection and triggering policies. We empirically analyze how various data selection and triggering policies impact model accuracy, and also show that Modyn enables high throughput training with sample-level data selection.} }
@inproceedings{10.1145/3749566.3749567, title = {Prediction of employment market trends for college students based on machine learning}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {1--5}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749567}, url = {https://doi.org/10.1145/3749566.3749567}, author = {Shao, Chen and Wang, Xinyan and Qiao, Shengnan and Liu, Shipeng}, keywords = {STEM field, employment market trends, machine learning}, abstract = {This article provides an in-depth prediction and analysis of the employment market trends for college students through machine learning algorithms. By collecting and processing relevant employment data from the past decade, a predictive model including time series analysis and regression analysis was established, aiming to provide valuable information on employment market trends for college students, educational institutions, and governments, and to provide scientific basis for understanding and adapting to the rapidly changing employment environment. The research results indicate that the development trend of the employment market for college students is influenced by various factors, among which economic environment, technological progress, and improvement in education level play a decisive role.} }
@inproceedings{10.1145/3664475.3664574, title = {Machine Learning \&amp; Neural Networks}, booktitle = {ACM SIGGRAPH 2024 Courses}, year = {2024}, isbn = {9798400706837}, doi = {10.1145/3664475.3664574}, url = {https://doi.org/10.1145/3664475.3664574}, author = {Sharma, Rajesh and Tang, Mia}, abstract = {Use and development of computer systems that are able to learn and adapt without following explicit instructions by using algorithms and statistical models to analyze and draw inferences from patterns in data.} }
@article{10.1145/3735969, title = {Instrumental Variables in Causal Inference and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3735969}, url = {https://doi.org/10.1145/3735969}, author = {Wu, Anpeng and Kuang, Kun and Xiong, Ruoxuan and Wu, Fei}, keywords = {Causal machine learning, instrumental variable, control function, unmeasured confounders}, abstract = {Causal inference is the process of drawing conclusions about causal relationships between variables using a combination of assumptions, study designs, and estimation strategies. In machine learning, causal inference is crucial for uncovering the mechanisms behind complex systems and making informed decisions. This article provides a comprehensive overview of using Instrumental Variables (IVs) in causal inference and machine learning, with a focus on addressing unobserved confounding that affects both treatment and outcome variables. We review identification conditions under standard assumptions in the IV literature. In this article, we explore three key research areas of IV methods: Two-Stage Least Squares (2SLS) regression, control function (CFN) approaches, and recent advances in IV learning methods. These methods cover both classical causal inference approaches and recent advancements in machine learning research. Additionally, we provide a summary of available datasets and algorithms for implementing these methods. Furthermore, we introduce a variety of applications of IV methods in real-world scenarios. Lastly, we identify open problems and suggest future research directions to further advance the field. A toolkit of reviewed IV methods with machine learning (MLIV) is available at .} }
@inbook{10.1145/3728725.3728813, title = {Loan Default Prediction Based on Machine Learning Approaches}, booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security}, pages = {557--564}, year = {2025}, isbn = {9798400713453}, url = {https://doi.org/10.1145/3728725.3728813}, author = {Cai, Xinyu and Dai, Wenbo and Lu, Jingyu}, abstract = {To address the credit risk losses incurred by commercial banks due to loan defaults, this study utilizes the loan default prediction dataset from the Alibaba Tianchi platform to develop machine learning models for predicting customer defaults, aiming to mitigate credit risk. Given the characteristics of class imbalance and high dimensionality of loan data, data preprocessing and exploratory data analysis are conducted. Based on a comparative analysis of various models, seven machine learning algorithms that demonstrate superior performance are selected for experimental comparison, including Decision Tree, Random Forest, AdaBoost, Bagging, XGBoost, LightGBM, and CatBoost. The results indicate that ensemble learning algorithms exhibit higher accuracy and predictive performance compared to single algorithms, with the CatBoost model performing best across various indicators, including AUC. The study identifies key features highly correlated with loan defaults, including loan grade, annual income, loan amount, credit history length, and debt-to-income ratio.} }
@inproceedings{10.1145/3701716.3715280, title = {Towards Democratized Machine Learning: A Semantic Web Approach}, booktitle = {Companion Proceedings of the ACM on Web Conference 2025}, pages = {697--700}, year = {2025}, isbn = {9798400713316}, doi = {10.1145/3701716.3715280}, url = {https://doi.org/10.1145/3701716.3715280}, author = {Klironomos, Antonis}, keywords = {knowledge graphs, machine learning, semantic web, similarity measures, location = Sydney NSW, Australia}, abstract = {The rapid growth of machine learning (ML) research has produced a vast and expanding collection of algorithms, datasets, and pipelines available on the Web. However, fragmented and dispersed documentation of these resources hampers accessibility, transparency, and effective use, posing challenges for users seeking to understand, adapt, and create ML pipelines. To address these challenges, we leverage Knowledge Graphs (KGs) and ontologies to represent ML pipelines as executable KGs (ExeKGs). This approach fosters an intuitive understanding of pipeline components and their relationships while defined constraints streamline the creation of valid and efficient pipelines. Furthermore, the structure of our KGs enables intelligent exploration and discovery of relevant ML artifacts, including pipelines and datasets. By incorporating KG-based ML techniques, we enhance the discovery and reuse of these artifacts. To consolidate these functionalities and provide users with an intuitive interface, we are developing ExeKGLab, a GUI-based platform for interacting with ExeKGs. This thesis explores the potential of KGs to democratize the ML landscape. We present our ongoing efforts to build a KG for ML, emphasizing its role in simplifying pipeline design, enhancing comprehension, and enabling smart exploration. By creating a structured and interconnected framework, our approach seeks to bridge gaps in accessibility and foster a more collaborative ML ecosystem. We invite discussion and feedback to advance this promising direction for future ML research.} }
@inproceedings{10.1145/3749566.3749594, title = {Predicting New York City Rent through Machine Learning —— Based on Airbnb Data}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {115--122}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749594}, url = {https://doi.org/10.1145/3749566.3749594}, author = {Gong, Youzhe}, keywords = {Airbnb, feature importance analysis, machine learning, rent prediction}, abstract = {This research aims to predict the rent of Airbnb listings in New York City through machine learning models and analyze the key factors may influence the prices. Utilizing Airbnb data from New York City for the year 2024, the study employs four models: Ridge Regression, Decision Tree, Random Forest, and XGBoost. Through feature engineering and parameter tuning, the models’ performances were optimized and compared. The Random Forest model was determined to perform the best, achieving the lowest test set RMSE of 29.6926. Additionally, the study reveals the impacts of key variables such as the number of rooms, location, number of amenities, and minimum stay requirements on rent prediction, based on feature importance ranking and OLS regression results. The significance of this study lies in providing an effective method for housing price prediction and offering a reference for hosts to set reasonable pricing mechanisms.} }
@inproceedings{10.1145/3736733.3736744, title = {Explanations for Machine Learning Pipelines under Data Drift}, booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics}, year = {2025}, isbn = {9798400719592}, doi = {10.1145/3736733.3736744}, url = {https://doi.org/10.1145/3736733.3736744}, author = {Hasan, Jahid and Pradhan, Romila}, keywords = {pipeline robustness, data preparation, explainable AI, data drift, location = Intercontinental Berlin, Berlin, Germany}, abstract = {Ensuring the robustness of data preprocessing pipelines is essential for maintaining the reliability of machine learning model performance in the face of real-world data shifts. Traditional methods optimize preprocessing sequences for specific datasets but often overlook their vulnerability to future data variations. This research introduces a vulnerability score to quantify the susceptibility of preprocessing components to data shift. We propose a Linear Regression approach to establish a predictive relationship between the vulnerability of the pipeline components and changes in the model's performance. The generated relationships act as explanations for practitioners of the system and help them quantify the robustness of the pipeline to data shift. For a given pipeline, we generate an explanation that highlights a tolerable threshold beyond which a component is considered shift-vulnerable and is likely to contribute to performance degradation. For the shift-vulnerable scenarios, we further suggest a new pipeline for system maintainers that preserves the model performance without retraining. The proposed framework delivers a risk-aware assessment, empowering practitioners to anticipate potential performance changes and adapt their pipeline strategies accordingly. Experimental results on several real-world datasets generate valid explanations for pipeline robustness and demonstrate the opportunities in this field of research.} }
@proceedings{10.1145/3733965, title = {WiseML '25: Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning}, year = {2025}, isbn = {9798400715310}, abstract = {We are delighted to welcome you to the ACM Workshop on Wireless Security and Machine Learning (WiseML) 2025. Continuing its tradition as a premier forum, WiseML brings together researchers and practitioners from the machine learning, privacy, security, wireless communications, and networking communities worldwide. The workshop serves as a dynamic platform for presenting cutting-edge research, exchanging innovative ideas, and fostering collaborations that advance these rapidly evolving fields. This year's event will be held in Arlington, VA, USA, and will feature a single-track program to encourage focused and engaging discussions. This year's call for papers attracted submissions from Europe, Asia, and the United States, which were carefully reviewed by 17 technical program committee (TPC) members representing both academia and industrial research labs. We are proud to present an outstanding technical program featuring eight papers that address a broad spectrum of topics in security, privacy, and adversarial machine learning as applied to wireless networks, mobile communications, 5G/IoT systems, cloud and edge computing, vehicular networks, and emerging applications.} }
@inproceedings{10.1145/3724363.3729107, title = {Student Perspectives on the Challenges in Machine Learning}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {9--15}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729107}, url = {https://doi.org/10.1145/3724363.3729107}, author = {Sibia, Naaz and Richardson, Amber and Gao, Alice and Petersen, Andrew and Zhang, Lisa}, keywords = {artificial intelligence education, barriers and challenges, computing education, machine learning education, retention, student success, support, theory and practice, location = Nijmegen, Netherlands}, abstract = {Machine learning (ML) has become increasingly important for students, yet university-level ML courses are often perceived as challenging and time-intensive. This study explores the perceived challenges and motivations of students in a university ML course to inform curricular and teaching strategies. Through 5 surveys conducted in two instances of a 12-week introductory ML course, we examined students' engagement with both theoretical and practical aspects of ML. Results indicate that while students initially express strong interest in applying ML concepts, their reported interests can shift toward theoretical foundations. Challenges in both theory and practice are reported, including difficulties in mathematical notation and vectorization of gradient components, as well as model implementation. Students also discuss the time commitment required in a course with both theoretical and practical content. We recommend aligning course content with student motivations, providing targeted support for mathematical notation and vectorization, and balancing theoretical depth with practical application.} }
@inproceedings{10.1145/3670474.3685972, title = {When Device Modeling Meets Machine Learning: Opportunities and Challenges (Invited)}, booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD}, year = {2024}, isbn = {9798400706998}, doi = {10.1145/3670474.3685972}, url = {https://doi.org/10.1145/3670474.3685972}, author = {Zhang, Lining and Peng, Baokang and Li, Yu and Liu, Hengyi and Dai, Wu and Wang, Runsheng}, keywords = {artificial neural network, compact model, device modeling, machine learning, location = Salt Lake City, UT, USA}, abstract = {Device modeling is essential for circuit simulations and designs in terms of constructing the circuit matrix equations of KCL and KVL. While there are classical methodologies, machine learning techniques are promising to bring innovations in the landscape of device modeling. This work reviews the device modeling from a top-down perspective, covering two different interpretations of modeling. Then the recent process in the domain-specific machine learning approaches is briefly summarized for logic and memory devices. The challenges ahead, for the machine learning model to support the industry's practical needs, are analyzed. A concept of fusion model, by deeply merging device physics and neural networks, is also explained.} }
@inproceedings{10.5555/3712729.3712752, title = {LLM Enhanced Machine Learning Estimators for Classification}, booktitle = {Proceedings of the Winter Simulation Conference}, pages = {288--298}, year = {2025}, isbn = {9798331534202}, author = {Wu, Yuhang and Wang, Yingfei and Wang, Chu and Zheng, Zeyu}, abstract = {Pre-trained large language models (LLM) have emerged as a powerful tool for simulating various scenarios and generating informative output given specific instructions and multimodal input. In this work, we analyze the specific use of LLM to enhance a classical supervised machine learning method for classification problems. We propose a few approaches to integrate LLM into a classical machine learning estimator to further enhance the prediction performance. We examine the performance of the proposed approaches through both standard supervised learning binary classification tasks, and a transfer learning task where the test data observe distribution changes compared to the training data. Numerical experiments using four publicly available datasets are conducted and suggest that using LLM to enhance classical machine learning estimators can provide significant improvement on prediction performance.} }
@inproceedings{10.1145/3718751.3718943, title = {Multivariate machine learning algorithm for stock return prediction modeling Machine learning algorithm for stock return prediction}, booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management}, pages = {1168--1172}, year = {2025}, isbn = {9798400709753}, doi = {10.1145/3718751.3718943}, url = {https://doi.org/10.1145/3718751.3718943}, author = {Liu, Zile}, keywords = {BP neural network, Double machine learning algorithm, Multiple linear regression, Stock return rate}, abstract = {With the increase of market risk and volatility, it has become an important topic to evaluate stock return rate by using multi-algorithm comprehensive prediction. [Methods]: Multiple linear regression model, BP neural network model and double machine learning (DML) model were used to model stock return. [Data]: Stock weekly regular data of China A-share listed companies from 2010 to 2020 were used. [Results]: The interpretation degree of the model was 46\% by using multiple linear regression model. The F score of BP neural network model is 92\%. The degree of model explanation using DML model is more than 99\%.} }
@article{10.1145/3664595, title = {Creativity and Machine Learning: A Survey}, journal = {ACM Comput. Surv.}, volume = {56}, year = {2024}, issn = {0360-0300}, doi = {10.1145/3664595}, url = {https://doi.org/10.1145/3664595}, author = {Franceschelli, Giorgio and Musolesi, Mirco}, keywords = {Computational creativity, machine learning, generative deep learning, creativity evaluation methods}, abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.} }
@inproceedings{10.1145/3749566.3749612, title = {Predictive Modeling of Airbnb Listing Prices in Boston Using Machine Learning Techniques}, booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning}, pages = {223--229}, year = {2025}, isbn = {9798400713927}, doi = {10.1145/3749566.3749612}, url = {https://doi.org/10.1145/3749566.3749612}, author = {Tang, Jiaqi}, keywords = {Airbnb prices, Boston, Feature importance, Machine learning, Regression models}, abstract = {This paper aims to build a price prediction model based on the Airbnb official website listing indicators. Airbnb is currently a popular online platform that provides short-term and long-term homestays in multiple countries and regions. It plays the role of an intermediary and charges commission from each booking. This research combines predictive algorithms and explainable machine learning techniques to forecast the prices of Airbnb listings in the Boston, the United States, after the dataset is cleaned and preprocessed. Through multi-model comparison, this research finds the best performing model from a variety of methods. In addition, this study also evaluates the importance of features in the best performing model. Extreme Gradient Boosting (XGBoost) model outperforms other models on both training and test datasets, with a Mean Squared Error (MSE) and Mean Absolute Error (MAE) of 0.1536 and 0.2808 on the test set, respectively. Factors such as “is_entire_home”, “number_of_reviews”, “number of records presented by neighborhoods”, “minimum_nights”, and “pop_density” contribute the most to the prediction of listing prices. Overall, this study provides practical suggestions for Airbnb to optimize its listing strategy and improve its price recommendation system.} }
@article{10.1145/3729234, title = {Privacy-Preserving Machine Learning Based on Cryptography: A Survey}, journal = {ACM Trans. Knowl. Discov. Data}, volume = {19}, year = {2025}, issn = {1556-4681}, doi = {10.1145/3729234}, url = {https://doi.org/10.1145/3729234}, author = {Chen, Congcong and Wei, Lifei and Xie, Jintao and Shi, Yang}, keywords = {Machine learning, secure multi-party computation, homomorphic encryption, privacy preserving, cryptography}, abstract = {Machine learning has profoundly influenced various aspects of our lives. However, privacy breaches have caused significant unease and concern among the general public. Preserving the privacy of sensitive data during the training and inference phases of machine learning is a key challenge. Cryptography-based privacy-preserving machine learning (crypto-based PPML) offers a viable solution to this challenge. In this article, we studied over 100 publications on crypto-based PPML frameworks published between 2016 and 2024, including 55 client-server architecture frameworks and 64 multi-party architecture frameworks. We provide a comprehensive overview of these frameworks, highlighting their features across various dimensions. Furthermore, we conduct an in-depth analysis, delving into scenarios, privacy goals, threat models, and optimization techniques that underpin these innovative solutions. We also discuss the challenges in the field of crypto-based PPML, including aspects of security and privacy, efficiency, and availability and usability. Finally, we offer an outlook on future research directions, aiming to provide valuable insights for both scholars and practitioners.} }
@inproceedings{10.1145/3747227.3747273, title = {Intelligent Agricultural Greenhouse Control System Based on Internet of Things and Machine Learning}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {292--300}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747273}, url = {https://doi.org/10.1145/3747227.3747273}, author = {Xiao, Yu and Gong, Jiangchuan and Wang, Yanze and Wang, Cangqing}, keywords = {Agricultural greenhouse, Internet of Things (IoT), Machine Learning, RNN model}, abstract = {This study endeavors to conceptualize and execute a sophisticated agricultural greenhouse control system grounded in the amalgamation of the Internet of Things (IoT) and machine learning. Through meticulous monitoring of intrinsic environmental parameters within the greenhouse and the integration of machine learning algorithms, the conditions within the greenhouse are aptly modulated. The envisaged outcome is an enhancement in crop growth efficiency and yield, accompanied by a reduction in resource wastage. In the backdrop of escalating global population figures and the escalating exigencies of climate change, agriculture confronts unprecedented challenges. Conventional agricultural paradigms have proven inadequate in addressing the imperatives of food safety and production efficiency. Against this backdrop, greenhouse agriculture emerges as a viable solution, proffering a controlled milieu for crop cultivation to augment yields, refine quality, and diminish reliance on natural resources. Nevertheless, greenhouse agriculture contends with a gamut of challenges. Traditional greenhouse management strategies, often grounded in experiential knowledge and predefined rules, lack targeted personalized regulation, thereby resulting in resource inefficiencies. The exigencies of real-time monitoring and precise control of the greenhouse's internal environment gain paramount importance with the burgeoning scale of agriculture. To redress this challenge, the study introduces IoT technology and machine learning algorithms into greenhouse agriculture, aspiring to institute an intelligent agricultural greenhouse control system conducive to augmenting the efficiency and sustainability of agricultural production.} }
@inproceedings{10.1145/3712255.3716543, title = {Machine Learning Assisted Evolutionary Multi-objective Optimization}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, pages = {1025--1041}, year = {2025}, isbn = {9798400714641}, doi = {10.1145/3712255.3716543}, url = {https://doi.org/10.1145/3712255.3716543}, author = {Deb, Kalyanmoy and Saxena, Dhish Kumar and Mittal, Sukrit} }
@inproceedings{10.1145/3676642.3729206, title = {Has Machine Learning for Systems Reached an Inflection Point?}, booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages = {3--4}, year = {2025}, isbn = {9798400710803}, doi = {10.1145/3676642.3729206}, url = {https://doi.org/10.1145/3676642.3729206}, author = {Maas, Martin}, keywords = {cluster scheduling, code optimization, data centers, large language models, machine learning for systems, memory allocation, storage systems, location = Rotterdam, Netherlands}, abstract = {A wide range of research areas - from natural language processing to computer vision and software engineering - have been (or are being) revolutionized by machine learning and artificial intelligence. Each of these areas went through an inflection point where they transitioned from ML as one of many approaches to ML becoming a predominant approach of the field. No example symbolizes this better than the AlexNet paper from 2012, which fundamentally transformed the field of computer vision. Computer systems remain a notable exception. In this talk, I will discuss emerging trends in the ML for Systems domain, how systems differ from these other areas, and what an ''AlexNet Moment'' for systems might look like. Along the way, I will describe a framework for categorizing work in the field and discuss emerging research problems and opportunities.} }
@article{10.1145/3763132, title = {AccelerQ: Accelerating Quantum Eigensolvers with Machine Learning on Quantum Simulators}, journal = {Proc. ACM Program. Lang.}, volume = {9}, year = {2025}, doi = {10.1145/3763132}, url = {https://doi.org/10.1145/3763132}, author = {Bensoussan, Avner and Chachkarova, Elena and Even-Mendoza, Karine and Fortz, Sophie and Lenihan, Connor}, keywords = {Genetic Algorithms, Machine Learning, Optimisation, Quantum Computing, Quantum Program Analysis, Search-based Software Engineering}, abstract = {We present AccelerQ, a framework for automatically tuning quantum eigensolver (QE) implementations–these are quantum programs implementing a specific QE algorithm–using machine learning and search-based optimisation. Rather than redesigning quantum algorithms or manually tweaking the code of an already existing implementation, AccelerQ treats QE implementations as black-box programs and learns to optimise their hyperparameters to improve accuracy and efficiency by incorporating search-based techniques and genetic algorithms (GA) alongside ML models to efficiently explore the hyperparameter space of QE implementations and avoid local minima. Our approach leverages two ideas: 1) train on data from smaller, classically simulable systems, and 2) use program-specific ML models, exploiting the fact that local physical interactions in molecular systems persist across scales, supporting generalisation to larger systems. We present an empirical evaluation of AccelerQ on two fundamentally different QE implementations: ADAPT-QSCI and QCELS. For each, we trained a QE predictor model, a lightweight XGBoost Python regressor, using data extracted classically from systems of up to 16 qubits. We deployed the model to optimise hyperparameters for executions on larger systems of 20-, 24-, and 28-qubit Hamiltonians, where direct classical simulation becomes impractical. We observed a reduction in error from 5.48\% to 5.3\% with only the ML model and further to 5.05\% with GA for ADAPT-QSCI, and from 7.5\% to 6.5\%, with no additional gain with GA for QCELS. Given inconclusive results for some 20- and 24-qubit systems, we recommend further analysis of training data concerning Hamiltonian characteristics. Nonetheless, our results highlight the potential of ML and optimisation techniques for quantum programs and suggest promising directions for integrating software engineering methods into quantum software stacks.} }
@inproceedings{10.1145/3747227.3747228, title = {Comparison and Prediction of Earth Pressure Based on Multiple Machine Learning Algorithms}, booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks}, pages = {1--5}, year = {2025}, isbn = {9798400714382}, doi = {10.1145/3747227.3747228}, url = {https://doi.org/10.1145/3747227.3747228}, author = {Li, Daimao and Xiao, Haohan and Guo, Qinghua and Han, Ke and Chen, Siyang}, keywords = {Data Processing, Earth Pressure, Machine Learning Algorithms, TBM}, abstract = {This paper investigates the application of machine learning algorithms in processing data from the first 1500 excavation segments of an Earth Pressure Balanced (EPB) Shield Tunnel Boring Machine (TBM). First, various models developed based on classical algorithms such as Decision Trees, Neural Networks, and Support Vector Machines are introduced, including Random Forest, XGBoost, and BPNN. The paper then elaborates on key data processing steps, noting that all models, except for the three Decision Tree-based models, require data normalization according to specific formulas. The model evaluation criteria are defined, encompassing multiple metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Additionally, the paper discusses how the dataset partitioning method varies based on the characteristics of the models and the configuration of hyperparameters. Finally, a comparative analysis of the predictive performance of each model is presented. The results show that, with the exception of ELM, most models perform similarly and meet engineering requirements. The different models exhibit varying strengths and weaknesses across different excavation segments, providing valuable insights for the application and improvement of machine learning models in related engineering projects.} }
@inbook{10.1145/3745238.3745358, title = {Pricing Strategy Optimization by Machine Learning in E-commerce}, booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence}, pages = {760--765}, year = {2025}, isbn = {9798400712791}, url = {https://doi.org/10.1145/3745238.3745358}, author = {Liu, Quan and Song, Yunkui}, abstract = {This paper introduces the concept of pricing strategy and its importance in e-commerce, analyzes the limitations of traditional pricing methods. Subsequently, the article elaborates on the application of machine learning in E-commerce pricing strategies. In addition, the article also presents the key technologies for optimizing pricing strategies using machine learning. Finally, it looks forward to the development trend of future pricing systems.} }
@article{10.1145/3728474, title = {Machine Learning for Blockchain Data Analysis: Progress and Opportunities}, journal = {Distrib. Ledger Technol.}, year = {2025}, doi = {10.1145/3728474}, url = {https://doi.org/10.1145/3728474}, author = {Azad, Poupak and Akcora, Cuneyt and Khan, Arijit}, keywords = {Machine Learning, Blockchain, Cryptocurrency, Graph Neural Networks, Temporal Data, Smart Contracts}, abstract = {Blockchain technology has rapidly emerged to mainstream attention. At the same time, its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data. Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts. Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity, such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies. These unique characteristics present opportunities and challenges for machine learning on blockchain data.On the one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for improving blockchain technology, such as e-crime detection and trends prediction. On the other hand, we shed light on blockchain's pivotal role by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem. This paper is a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field.} }
@inproceedings{10.1145/3694860.3694868, title = {Dementia Deterioration Prediction Using Machine Learning}, booktitle = {Proceedings of the 2024 8th International Conference on Cloud and Big Data Computing}, pages = {54--59}, year = {2024}, isbn = {9798400717253}, doi = {10.1145/3694860.3694868}, url = {https://doi.org/10.1145/3694860.3694868}, author = {Dawood Almardoud, Layla and Tawfik, Hissam and Majzoub, Sohaib}, keywords = {Dementia prognosis, Interpretable models, Machine Learning, Mild Cognitive Impairment, Permutation Importance}, abstract = {Dementia is a disease that imposes medical, social, and economic challenges on medical professionals, caregivers, and the patients themselves. Dementia monitoring and prognosis are critical factors besides dementia diagnosis. However, recent studies on dementia prognosis involve people with diagnosed dementia and not non-demented with a high risk of being demented, like people with cognitive difficulties. The aim of this paper is to use Machine Learning (ML) algorithms to predict patients with the risk of deterioration from medical histories containing clinical, cognitive, and profile data collected from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The best model was the Random Forest model with a sensitivity of 0.79, accuracy of 0.77, specificity of 0.76, F1-score of 0.78, and an AUROC of 0.83. Moreover, the model was interpreted through permutation importance. Using the permutation importance tool, the study highlighted the strong effect of diagnosis information and specific symptoms like muscle pain for dementia deterioration prediction.} }
@inproceedings{10.1145/3760544.3764127, title = {Breath Patterns as Signals: A Machine Learning-based Molecular Communication Perspective}, booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication}, pages = {22--27}, year = {2025}, isbn = {9798400721663}, doi = {10.1145/3760544.3764127}, url = {https://doi.org/10.1145/3760544.3764127}, author = {Bhattacharjee, Sunasheer and Pal, Saswati and Scheepers, P\'eter and Dressler, Falko}, keywords = {biological system, diagnostics, communication system model, machine learning, molecular communication, location = University of Electronic Science and Technology of China, Chengdu, China}, abstract = {Molecular communication is a core pillar of the Internet of Bio-Nano Things. Exhaled breath, rich in water vapor, offers a viable medium for air-based molecular communication. This paper presents a low-cost, non-invasive approach using a DHT22 sensor to classify breath patterns, namely Eupnea, Bradypnea, and Tachypnea. Humidity and temperature signals from the mouth and nose are processed using machine learning (ML). The model achieves strong classification performance, showing that ML can effectively distinguish breath patterns despite sensor constraints.} }
@inproceedings{10.1145/3637528.3671442, title = {Practical Machine Learning for Streaming Data}, booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {6418--6419}, year = {2024}, isbn = {9798400704901}, doi = {10.1145/3637528.3671442}, url = {https://doi.org/10.1145/3637528.3671442}, author = {Gomes, Heitor Murilo and Bifet, Albert}, keywords = {classification, concept drift, data streams, prediction intervals, regression, semi-supervised learning, location = Barcelona, Spain}, abstract = {Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/.} }
@article{10.1145/3744237, title = {Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey}, journal = {ACM Comput. Surv.}, volume = {57}, year = {2025}, issn = {0360-0300}, doi = {10.1145/3744237}, url = {https://doi.org/10.1145/3744237}, author = {Zhong, Zhiqiang and Barkova, Anastasia and Mottin, Davide}, keywords = {Graph machine learning, knowledge-augmented methods, drug discovery, knowledge database, knowledge graph}, abstract = {Artificial Intelligence has become integral to intelligent drug discovery, with Graph Machine Learning (GML) emerging as a powerful structure-based method for modelling graph-structured biomedical data and investigating their properties. However, GML faces challenges such as limited interpretability and heavy dependency on abundant high-quality training data. On the other hand, knowledge-based methods leverage biomedical knowledge databases, e.g., Knowledge Graphs (KGs), to explore unknown knowledge. Nevertheless, KG construction is resource-intensive and often neglects crucial structural information in biomedical data. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with scarce training data. Nevertheless, a systematic definition for this burgeoning research direction is yet to be established. This survey formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery and organises collected KaGML works into four categories following a novel-defined taxonomy. We also present a comprehensive overview of long-standing drug discovery principles and provide the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases. To facilitate research in this promptly emerging field, we share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.} }
@article{10.1145/3723356, title = {PredicTor: A Global, Machine Learning Approach to Tor Path Selection}, journal = {ACM Trans. Priv. Secur.}, volume = {28}, year = {2025}, issn = {2471-2566}, doi = {10.1145/3723356}, url = {https://doi.org/10.1145/3723356}, author = {Barton, Armon and Walsh, Timothy and Imani, Mohsen and Ming, Jiang and Wright, Matthew}, keywords = {Anonymous communications, Tor, machine learning, path selection, network performance}, abstract = {Tor users derive anonymity in part from the size of the Tor user base, but Tor struggles to attract and support more users due to performance limitations. Previous works have proposed modifications to Tor’s path selection algorithm to enhance both performance and security, but many proposals have unintended consequences due to incorporating information related to client location. We instead propose selecting paths using a global view of the network, independent of client location, and we propose doing so with a machine learning classifier to predict the performance of a given path before building a circuit. We show through a variety of simulated and live experimental settings, across different time periods, that this approach can significantly improve performance compared to Tor’s default path selection algorithm and two previously proposed approaches. In addition to evaluating the security of our approach with traditional metrics, we propose a novel anonymity metric that captures information leakage resulting from location-aware path selection, and we show that our path selection approach leaks no more information than the default path selection algorithm.} }
@inproceedings{10.1145/3724363.3729085, title = {K-12 Students' (Mis-)Conceptions of Machine Learning Paradigms}, booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1}, pages = {347--353}, year = {2025}, isbn = {9798400715679}, doi = {10.1145/3724363.3729085}, url = {https://doi.org/10.1145/3724363.3729085}, author = {Kr\"uger, Jan Jakob and Gromann, Gabriel and Dengel, Andreas}, keywords = {ai literacy, computer science curriculum, k-12 education, machine learning, student misconceptions}, abstract = {Understanding Artificial Intelligence (AI) is on its way to becoming a key competence in the coming years due to its rapid advancements and growing societal impact. Learning about different paradigms in AI, particularly in machine learning, is crucial to enabling students to critically engage with and shape an AI-driven world. The study investigates how students of three grade levels (5th, 8th, and 11th) understand AI concepts, addressing the challenge of widespread misconceptions and the limited presence of AI in school curricula. A deductive qualitative analysis was used to analyze the students' preconceptions. The results indicate that younger students typically exhibit minimal or anthropomorphic views, reflecting limited exposure to basic AI principles. Older students show somewhat more advanced, yet still partial, understandings, with misconceptions persisting across all groups. The findings underscore the importance of integrating AI literacy into school curricula and aligning instruction with students' developmental stages. For younger learners, hands-on activities can introduce basic AI functionality while dispelling human-like attributions. Older students benefit from exploring ethical, technical, and societal dimensions of AI. This research highlights the need for age-appropriate AI education to foster informed, responsible users and creators of AI systems.} }
@inproceedings{10.1145/3696271.3696272, title = {Predicting Foreign Exchange EUR/USD Direction Using Machine Learning}, booktitle = {Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)}, pages = {1--9}, year = {2024}, isbn = {9798400717833}, doi = {10.1145/3696271.3696272}, url = {https://doi.org/10.1145/3696271.3696272}, author = {Guyard, Kevin Cedric and Deriaz, Michel}, keywords = {Bayesian search, Forex prediction, Machine learning, Meta estimator}, abstract = {The Foreign Exchange market is a significant market for speculators, characterized by substantial transaction volumes and high volatility. Accurately predicting the directional movement of currency pairs is essential for formulating a sound financial investment strategy. This paper conducts a comparative analysis of various machine learning models for predicting the daily directional movement of the EUR/USD currency pair in the Foreign Exchange market. The analysis includes both decorrelated and non-decorrelated feature sets using Principal Component Analysis. Additionally, this study explores meta-estimators, which involve stacking multiple estimators as input for another estimator, aiming to achieve improved predictive performance. Ultimately, our approach yielded a prediction accuracy of 58.52\% for one-day ahead forecasts, coupled with an annual return of 32.48\% for the year 2022.} }
@inproceedings{10.1145/3701047.3701067, title = {Comparative Analysis of Various Machine Learning Techniques for Rockburst Risk Prediction}, booktitle = {Proceedings of the 2024 2nd International Conference on Communication Networks and Machine Learning}, pages = {105--109}, year = {2025}, isbn = {9798400711688}, doi = {10.1145/3701047.3701067}, url = {https://doi.org/10.1145/3701047.3701067}, author = {Yu, Hongtao and Li, Mingyao and Xiao, Haohan and Cao, Ruilang}, keywords = {Data processing, Feature clustering, Machine learning, Model evaluation, Rockburst risk prediction}, abstract = {This paper investigates the application of machine learning algorithms in rockburst risk prediction models. It begins with an overview of fundamental machine learning principles, followed by a detailed analysis of common classification algorithms, including Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), Random Forest (RF), and AdaBoost. Subsequently, the paper examines the use of clustering algorithms for feature extraction and reorganization. The technical process for developing accurate rockburst risk prediction models is outlined, encompassing data preprocessing, feature clustering, reorganization, and model evaluation. The findings offer valuable insights and references for the integration of machine learning in rockburst risk prediction, as well as innovative approaches for research and practical applications in related fields.} }
@inproceedings{10.1145/3728199.3728203, title = {Data Analysis of College Practice Teaching Based on Machine Learning Algorithm}, booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning}, pages = {18--23}, year = {2025}, isbn = {9798400713231}, doi = {10.1145/3728199.3728203}, url = {https://doi.org/10.1145/3728199.3728203}, author = {Wang, Xinwu}, keywords = {association rule analysis, cluster analysis, college practice teaching, data mining, machine learning algorithm}, abstract = {With the rapid development of information technology, especially the wide application of big data and artificial intelligence technology, the field of education is undergoing unprecedented changes. Among them, practical teaching in colleges and universities, as an important part of modern education system, is facing severe challenges. However, the traditional practice teaching mode often has some problems, such as uneven distribution of resources, disjointed teaching content and market demand, and single teaching method, which limit the improvement of practice teaching effect. In this regard, based on the actual needs, this paper will deeply analyze the application feasibility of artificial intelligence and machine learning in the reform of practical teaching in colleges and universities, and put forward a set of data analysis and processing scheme of practical teaching in colleges and universities, aiming at improving the effectiveness and personalized level of practical teaching through the integration, mining and analysis of multi-source data. Practice has proved that machine learning algorithms such as fuzzy C-means (FCM) clustering analysis and frequent pattern growth (FP-growth) correlation analysis are adopted in the scheme, and the data involved in college practice teaching are deeply analyzed, and the feasibility and effectiveness of the model are verified by practical tests, which provides scientific basis and decision support for college practice teaching.} }
@inproceedings{10.1145/3745812.3745873, title = {Predictive Analytics for Chemotherapy Effectiveness Using Machine Learning}, booktitle = {Proceedings of the 6th International Conference on Information Management \&amp; Machine Intelligence}, year = {2025}, isbn = {9798400711220}, doi = {10.1145/3745812.3745873}, url = {https://doi.org/10.1145/3745812.3745873}, author = {kaur, Lakhwinder and Bamne, Shrikrishna N. and Dehankar, Jiwan and Khetani, Vinit and Pawar, S K and Goyal, Dinesh, -}, keywords = {Cancer Treatment, Chemotherapy, Machine Learning, Personalized Medicine, Predictive Analytics}, abstract = {Abstract: Chemotherapy is still an important part of treating cancer, but how well it works for each patient depends on things like the type of tumor, their genetics, and their general health. Accurately predicting how chemotherapy will work is necessary to make treatment plans work better, lower side effects, and raise patient mortality rates. This study looks into how machine learning (ML) methods can be used in predictive analytics to figure out how well treatment works. Machine learning models like Support Vector Machines (SVM), Random Forests, and Neural Networks are used to figure out which patients are most likely to respond to or not respond to chemotherapy. They do this by looking at information about each patient's medical history, genomic information, treatment plans, and the characteristics of their tumors. The information used to train and test the model comes from clinical studies and patient records. This makes sure that it has a wide range of cancer types and patient traits. To deal with complicated, high-dimensional data and find key drivers of treatment results, both controlled and untrained learning methods are used together. The models are judged by their performance measures, which include their accuracy, precision, recall, and the area under the receiver operating characteristic curve (AUC). The results show that prediction models based on machine learning can accurately predict how well chemotherapy will work, which lets doctors make personalized treatment plans. The results show that using predictive analytics in clinical decision- making could make cancer procedures more efficient and effective, reducing side effects and improving patient outcomes.} }
@article{10.1145/3757743, title = {Enhancing Blockchain Scalability using Off-Chain and Machine Learning Techniques}, journal = {J. Emerg. Technol. Comput. Syst.}, year = {2025}, issn = {1550-4832}, doi = {10.1145/3757743}, url = {https://doi.org/10.1145/3757743}, author = {Pawar, Manjula and Patil, Prakashgoud and Hiremath, P.S.}, keywords = {Blockchain, Scalability, Machine learning, Off-chain, KNN classification, Artificial Intelligence}, abstract = {Blockchain Technology is a nascent technology that possesses attributes such as immutability, security, transparency, openness, and decentralization. It is widely used in industry and business applications. Though it has the best features, it still suffers from some main characteristics, such as scalability and privacy. Scalability is measured through throughput (transactions per second), space,cost and latency. Bitcoin and Ethereum, which are prominent Blockchain platforms, carry out 7 and 20 transactions per second, respectively. This is much less than popular platforms such as Visa, PayPal, and Amazon, which perform thousands of transactions per second. Therefore, this paper presents comprehensive study of scalability improving techniques for Blockchain and case studies for improving scalability by using some of the techniques. The scalability of Blockchain systems can be enhanced by on-chain, off-chain, and machine learning algorithms.The proposed methodology improves the scalability using off-chain technique for supply chain management and KNN classification for healthcare domain.} }
@inproceedings{10.1145/3716368.3735251, title = {Enhancing Modern SAT Solver With Machine Learning Method}, booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025}, pages = {886--892}, year = {2025}, isbn = {9798400714962}, doi = {10.1145/3716368.3735251}, url = {https://doi.org/10.1145/3716368.3735251}, author = {Chen, Guanting and Wang, Jia}, keywords = {SAT Solver, CDCL, Machine Learning, GNN}, abstract = {Satisfiability (SAT), a well-known NP-complete problem, has been widely studied and drives numerous research fields. State-of-the-art SAT solvers rely on the Conflict-Driven Clause Learning (CDCL) algorithm to solve practical SAT instances with two possible outcomes – a solution for SAT instances or a proof proving no solution exists for UNSAT instances. While many heuristics were manually designed to improve CDCL in the past, recent efforts focus on applying machine and deep learning models, e.g. Graph Neural Networks (GNN), with the hope to make heuristics more effective and adaptive. Nevertheless, the demand for significant GPU resources and the effectiveness in a broader set of SAT and UNSAT instances remain the major challenges. In this paper, we present a GNN-based algorithm that predicts at the same time backbone variables for SAT instances and UNSAT-core variables for UNSAT instances. Leveraging offline model inference, our trained GNN model, and so the whole SAT solver, is able to run entirely on CPU, removing the need of GPU resources. Experimental results confirm that with our algorithm, a modern SAT solver is able to solve up to 5\% and 7\% more instances for different baseline solvers.} }
@article{10.1145/3711713, title = {Applications of Certainty Scoring for Machine Learning Classification and Out-of-Distribution Detection}, journal = {ACM Trans. Probab. Mach. Learn.}, year = {2025}, doi = {10.1145/3711713}, url = {https://doi.org/10.1145/3711713}, author = {Berenbeim, Alexander M. and Cobb, Adam D. and Roy, Anirban and Jha, Susmit and Bastian, Nathaniel D.}, keywords = {Machine Learning, Uncertainty Quantification, Out-of-Distribution Detection, Computer Vision, Network Security}, abstract = {Quantitative characterizations and estimations of uncertainty are of fundamental importance for machine learning classification, particularly in safety-critical settings where continuous real-time monitoring requires explainable and reliable scoring. Reliance on the maximum a posteriori principle to determine label classification can obscure the certainty of a label assignment. We develop a theoretical framework for quantitative scores of certainty and competence based on predicted probability estimates, formally prove their properties, and empirically confirm the inferential power of these properties across different data modalities, tasks and model architectures. Our theoretical results establish that competent models have distinct distributions of certainty for true and false positives conditioned on inputs similar to training and testing data, and prove that this framework provides a reliable means to infer the quality of model predictions and detect false positives. Our empirical results bear out that there are distinct distributions of certainty scores on training and holdout data, as well as data that is a priori out-of-distribution. For expert models, at least 62.1\% of false positives could be identified when using a cut-off at at the bottom 5\% TP threshold. Further, we found a strong negative correlation between empirical competence and the FPR95TPR rate for EnergyBased out-of-distribution (OOD) detectors. Finally, we developed two forms of an OOD detector that were able to reliably distinguish in-distribution data from OOD data for both frequentist and Bayesian models, performing better on average than previous state-of-the-art EnergyBased OOD detection methods, and improving upon the baseline Monte Carlo Dropout AUPR-OUT performance on average by 14.4\% and 16.5\%, and reducing the FPR95TPR by 54.2\% and 37.6\%.} }
@inproceedings{10.1145/3746027.3764193, title = {Toward Fast and Exact Machine Learning Platform for Big Data}, booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia}, pages = {14372}, year = {2025}, isbn = {9798400720352}, doi = {10.1145/3746027.3764193}, url = {https://doi.org/10.1145/3746027.3764193}, author = {Fujiwara, Yasuhiro}, keywords = {ai-based data analysis, big data, computational pruning, scalable machine learning, location = Dublin, Ireland}, abstract = {Data has become the foundation of knowledge, and many companies are growing interested in harnessing AI-based data analysis to unlock its value. The volume of digital data is increasing at an unprecedented pace: market research reports estimate that global data volume, approximately 12.5 zettabytes in 2014, will reach around 180 zettabytes by 2025. Extracting patterns and trends from such big data is crucial for enabling data-driven decision-making. However, a key challenge lies in the enormous computational costs required for large-scale analysis, due to the inherent complexities of the task. Approximate methods are often employed to reduce these costs, but they inevitably trade exactness for efficiency. To overcome this limitation, our research aims to develop a machine learning platform that delivers both speed and accuracy. The core of our platform is computational pruning. This talk will introduce three representative pruning strategies. Specifically, it first introduces a pruning method that uses upper and lower bounds to omit computations. This method efficiently identifies unnecessary processes by using upper and lower bounds of scores to skip unnecessary computations. Next, this talk introduces a method to terminate computations that cannot yield solutions. This method maintains patterns that failed during the search process to avoid repeated futile searches. This talk finally introduces a method that prunes computations through optimistic processing. This method temporarily removes a constraint to find a solution quickly and then verifies if the obtained solution meets the constraint. These strategies can open the path to data analysis techniques that are both efficient and exact, ultimately empowering companies to make more reliable and timely decisions in today's increasingly data-driven world.} }
