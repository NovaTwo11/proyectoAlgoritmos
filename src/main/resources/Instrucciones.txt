flujo completo que usa el proyecto para generar los dendrogramas de abstracts:
Lectura de datos: el endpoint de “abstracts_comparasion” lee los artículos desde los .bib en la carpeta researchFiles con ReaderImplementation.read_bib_files() y toma los campos title y abstract.
Preprocesamiento: con TextPreprocessing se aplica, en este orden, minúsculas, eliminación de números, puntuación y espacios extra, stopwords (NLTK) y stemming (Porter). Se asegura descargar los recursos de NLTK (stopwords y tokenizer).
Vectorización: TextVectorization.transform_text convierte los textos preprocesados a TF‑IDF (TfidfVectorizer). Los títulos se usan como etiquetas.
Distancias y clustering: calcula distancias euclidianas entre los vectores TF‑IDF (pdist(..., metric='euclidean')) y realiza clustering jerárquico con linkage en dos variantes: método "ward" y método "average".
Graficado: con scipy.cluster.hierarchy.dendrogram y Matplotlib (backend "Agg") genera dos imágenes PNG y las guarda en images/abstracts_dendograms/ward_dendogram.png y images/abstracts_dendograms/average_dendogram.png.
Respuesta del API: el endpoint retorna ambas imágenes en base64 mediante Utils.image_to_base64, bajo las claves "ward" y "average".
Notas:
Las etiquetas de los dendrogramas son los títulos de los artículos.
El backend “Agg” permite generar las imágenes sin entorno gráfico.

"""
This module contains all the endpoints for the abstract comparassion
"""
from fastapi import APIRouter, HTTPException
from services.abstractComparasion.text_preprocessing import TextPreprocessing
from services.abstractComparasion.dendrogram_ploting import TextVectorization
from reader_resources.reader_implementation import ReaderImplementation
from utils.utils import Utils

router = APIRouter()


@router.get("/")
def get_abstracts_comparasion():
    """
    This method returns the results form the abstract comparasion
    """
    try:
        reader = ReaderImplementation()
        articles = reader.read_bib_files()

        preprocessing = TextPreprocessing()
        for article in articles:
            if 'abstract' in article:
                preprocessing.preprocess_text(
                    article['abstract'], article['title'])
        preprocessed_abstracts = preprocessing.preprocessed_abstracts

        dendrogram = TextVectorization()
        results = dendrogram.transform_text(
            preprocessed_abstracts=preprocessed_abstracts)

        return {"ward": Utils.image_to_base64(results["ward_dendogram"]),
                "average": Utils.image_to_base64(results["average_dendogram"])}

    except (IOError, ValueError) as e:
        return HTTPException(status_code=500, detail=str(e))


"""
This module uses nltk to preprocess abstrac content
"""
import string
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
stemmer = PorterStemmer()


class TextPreprocessing:
    """
    This class includes all the required methods to preproccess abstracts text
    """

    def __init__(self):
        self.preprocessed_abstracts = {}
        self.ensure_nltk_resources()

    def ensure_nltk_resources(self):
        """
        Verifies and downloads necessary NLTK resources if missing.
        """
        resources = ['stopwords', 'punkt', 'punkt_tab']
        for resource in resources:
            try:
                nltk.data.find(
                    f'corpora/{resource}') if resource == 'stopwords' else nltk.data.find(f'tokenizers/{resource}')
            except LookupError:
                print(f"Downloading NLTK resource: {resource}...")
                nltk.download(resource)

    def preprocess_text(self, text, title):
        """
          Preprocess all the texto from the abstracts
        """
        lowercase = self.lowercase_text(text)

        no_numbers = self.remove_numbers(lowercase)

        no_punctuation = self.remove_punctuation(no_numbers)

        no_white_spaces = self.remove_whitespace(no_punctuation)

        no_stopwords = self.remove_stopwords(no_white_spaces){
	"inputCount": 1987,
	"outputCount": 1956,
	"dedupCriteria": "title: normalized",
	"options": {
		"keepPlus": true,
		"useBigrams": false,
		"removeNumbers": true,
		"minTokenLength": 3
	},
	"articles": [
		{
			"id": "10.1145/3744199.3744635",
			"title": "Automated Video Segmentation Machine Learning Pipeline",
			"normalizedText": "visual effects (vfx) production often struggles with slow, resource-intensive mask generation. this paper presents an automated video segmentation pipeline that creates temporally consistent instance masks. it employs machine learning for: (1) flexible object detection via text prompts, (2) refined per-frame image segmentation and (3) robust video tracking to ensure temporal stability. deployed using containerization and leveraging a structured output format, the pipeline was quickly adopted by our artists. it significantly reduces manual effort, speeds up the creation of preliminary composites, and provides comprehensive segmentation data, thereby enhancing overall vfx production efficiency.",
			"tokens": [
				"visual",
				"effects",
				"vfx",
				"production",
				"often",
				"struggles",
				"slow",
				"resource",
				"intensive",
				"mask",
				"generation",
				"presents",
				"automated",
				"video",
				"segmentation",
				"pipeline",
				"creates",
				"temporally",
				"consistent",
				"instance",
				"masks",
				"employs",
				"machine",
				"learning",
				"flexible",
				"object",
				"detection",
				"text",
				"prompts",
				"refined",
				"per",
				"frame",
				"image",
				"segmentation",
				"robust",
				"video",
				"tracking",
				"ensure",
				"temporal",
				"stability",
				"deployed",
				"containerization",
				"leveraging",
				"structured",
				"output",
				"format",
				"pipeline",
				"quickly",
				"adopted",
				"artists",
				"significantly",
				"reduces",
				"manual",
				"effort",
				"speeds",
				"creation",
				"preliminary",
				"composites",
				"provides",
				"comprehensive",
				"segmentation",
				"thereby",
				"enhancing",
				"overall",
				"vfx",
				"production",
				"efficiency"
			]
		},
		{
			"id": "10.1145/3769394.3769411",
			"title": "Sustainable Machine Learning: Course 1",
			"normalizedText": "machine learning has become increasingly data and processing hungry. a recent report from the international energy agency projects that the electricity demand for data centers specialized in ai will more than quadruple by 2030. as such, it has become a pressing need to include energy awareness and environmental sustainability into the machine learning life cycle. in fact, a considerable amount of research efforts have been conducted in the last years in this direction.the first part of this tutorial will discuss various mechanisms to assess the environmental impact of machine learning, from power and energy consumption to carbon footprint. this will be put in relation to more traditional performance metrics used in the research literature, from the \"goodness\" of a ml solution, measured by metrics such as accuracy, to systems performance metrics such as runtime, throughput and scalability. from there, the tutorial will present several concrete research efforts for a quantitative analysis of the environmental footprint of various ml tasks.the second part of the tutorial will outline recent solutions to tackle the huge energy consumption of modern ml. for instance, there have been an increasing number of research efforts to make both the learning and the inference tasks more efficient while providing similar performance in terms of traditional ml performance metrics such as accuracy. a further line of research focuses on adjusting the infrastructure or the execution of ml tasks to be more energy aware, e.g., through scheduling approaches.",
			"tokens": [
				"machine",
				"learning",
				"become",
				"increasingly",
				"processing",
				"hungry",
				"recent",
				"report",
				"international",
				"energy",
				"agency",
				"projects",
				"electricity",
				"demand",
				"centers",
				"specialized",
				"will",
				"more",
				"than",
				"quadruple",
				"such",
				"become",
				"pressing",
				"need",
				"include",
				"energy",
				"awareness",
				"environmental",
				"sustainability",
				"machine",
				"learning",
				"life",
				"cycle",
				"fact",
				"considerable",
				"amount",
				"research",
				"efforts",
				"been",
				"conducted",
				"last",
				"years",
				"direction",
				"first",
				"part",
				"tutorial",
				"will",
				"discuss",
				"various",
				"mechanisms",
				"assess",
				"environmental",
				"impact",
				"machine",
				"learning",
				"power",
				"energy",
				"consumption",
				"carbon",
				"footprint",
				"will",
				"put",
				"relation",
				"more",
				"traditional",
				"performance",
				"metrics",
				"research",
				"literature",
				"goodness",
				"solution",
				"measured",
				"metrics",
				"such",
				"accuracy",
				"systems",
				"performance",
				"metrics",
				"such",
				"runtime",
				"throughput",
				"scalability",
				"there",
				"tutorial",
				"will",
				"present",
				"several",
				"concrete",
				"research",
				"efforts",
				"quantitative",
				"analysis",
				"environmental",
				"footprint",
				"various",
				"tasks",
				"second",
				"part",
				"tutorial",
				"will",
				"outline",
				"recent",
				"solutions",
				"tackle",
				"huge",
				"energy",
				"consumption",
				"modern",
				"instance",
				"there",
				"been",
				"increasing",
				"number",
				"research",
				"efforts",
				"make",
				"both",
				"learning",
				"inference",
				"tasks",
				"more",
				"efficient",
				"while",
				"providing",
				"similar",
				"performance",
				"terms",
				"traditional",
				"performance",
				"metrics",
				"such",
				"accuracy",
				"further",
				"line",
				"research",
				"focuses",
				"adjusting",
				"infrastructure",
				"execution",
				"tasks",
				"more",
				"energy",
				"aware",
				"through",
				"scheduling"
			]
		},

        stemmed = self.stem_words(no_stopwords)
        self.preprocessed_abstracts[title] = stemmed

    def lowercase_text(self, text):
        """Turns the provided texto to lower case"""
        return text.lower()

    def remove_numbers(self, text):
        """Removes the numbers from the provided text"""
        result = re.sub(r'\d+', '', text)
        return result

    def remove_punctuation(self, text):
        """Removes punctuation or specia chars from the provided text"""
        translator = str.maketrans('', '', string.punctuation)
        return text.translate(translator)

    def remove_whitespace(self, text):
        """Removes whitespaces"""
        return " ".join(text.split())

    def remove_stopwords(self, text):
        """Removes conectors an irrelevant words"""
        stop_words = set(stopwords.words("english"))
        word_tokens = word_tokenize(text)
        filtered_words = [
            word for word in word_tokens if word not in stop_words]

        filtered_text = ' '.join(filtered_words)
        return filtered_text

    def stem_words(self, text):
        """Stemming for text processing"""
        word_tokens = word_tokenize(text)
        stems = [stemmer.stem(word) for word in word_tokens]
        stems_string = ' '.join(stems)
        return stems_string


"""
This module is used to vectorize,
calculate matrix length, hierarchical clustering
and dendrogram ploting
"""
import os
from scipy.cluster.hierarchy import dendrogram
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage
from fastapi.responses import FileResponse
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use("Agg")


class TextVectorization:
    """
    Class used to vectorize,
    calculate matrix length and hierarchical clustering
    """

    def __init__(self):
        pass

    def transform_text(self, preprocessed_abstracts):
        """
        Method used to vectorize, calculate matrix length, hierarchical clustering
        """
        texts = list(preprocessed_abstracts.values())

        titles = list(preprocessed_abstracts.keys())

        vectorizer = TfidfVectorizer()

        x = vectorizer.fit_transform(texts)

        distances = pdist(x.toarray(), metric='euclidean')

        z_ward = linkage(distances, method='ward')

        z_average = linkage(distances, method='average')

        return self.plot_dendogram(z_ward, z_average, titles)

    def plot_dendogram(self, z_ward, z_average, titles):
        """
        Method used to plot a dendrogram
        """
        results = {
        }

        project_dir = os.path.abspath(
            os.path.join(os.path.dirname(__file__), '../../images'))
        research_files_dir = os.path.join(project_dir, "abstracts_dendograms")
        os.makedirs(research_files_dir, exist_ok=True)

        # Ward dendrogram
        file_path = os.path.join(research_files_dir, "ward_dendogram.png")
        plt.figure(figsize=(12, 6))
        dendrogram(z_ward, labels=titles)
        plt.title("Dendrograma - Ward")
        plt.savefig(file_path)
        plt.close()
        results["ward_dendogram"] = file_path

        # Average dendrogram
        file_path = os.path.join(
            research_files_dir, "average_dendogram.png")
        plt.figure(figsize=(12, 6))
        dendrogram(z_average, labels=titles)
        plt.title("Dendrograma - Average")
        plt.savefig(file_path)
        plt.close()
        results["average_dendogram"] = file_path

        return results


"""
  This file contains utility functions that are used in the project.
"""

import os
import glob
import shutil
import json
import base64


class Utils:
    """
        Utility functions for the project.
    """

    def __init__(self):
        self.project_root = os.path.dirname(
            os.path.dirname(os.path.abspath(__file__)))
        self.destination_folder = os.path.join(
            self.project_root, "researchFiles")

    def move_downloaded_files(self):
        """
        Moves downloaded files from the Downloads folder to the specified destination folder.
        """

        file_type = ['bib']

        # Get user's downloads folder
        downloads_folder = os.path.join(
            os.path.expanduser('~'), 'Downloads')

        # Use glob to find files with the specified extension
        for file in file_type:
            pattern = os.path.join(downloads_folder, f"*.{file}")
            for file_path in glob.glob(pattern):
                file_name = os.path.basename(file_path)
                destination = os.path.join(self.destination_folder, file_name)

                # Ensure the destination directory exists
                os.makedirs(self.destination_folder, exist_ok=True)

                # Move the file
                shutil.move(file_path, destination)
                print(f"Moved {file_name} to {self.destination_folder}")

    def list_chrome_profiles(self):
        """
        Lists all available Chrome profiles on macOS and returns their paths
        """
        # Path to Chrome profiles on macOS
        base_path = os.path.expanduser(
            "~/Library/Application Support/Google/Chrome")

        try:
            # Check if the directory exists
            if not os.path.exists(base_path):
                print(f"Chrome directory not found at {base_path}")
                return []

            # Get all profile directories
            profiles = []

            # Always add Default profile if it exists
            default_profile = os.path.join(base_path, "Default")
            if os.path.exists(default_profile) and os.path.isdir(default_profile):
                profiles.append({"name": "Default", "path": default_profile})

            # Add numbered profiles
            for item in os.listdir(base_path):
                if item.startswith("Profile ") and os.path.isdir(os.path.join(base_path, item)):
                    profile_path = os.path.join(base_path, item)
                    profiles.append({"name": item, "path": profile_path})

            # Try to get profile names from Preferences files
            for profile in profiles:
                pref_file = os.path.join(profile["path"], "Preferences")
                if os.path.exists(pref_file):
                    try:
                        with open(pref_file, 'r', encoding='utf-8') as f:
                            prefs = json.load(f)
                            if "profile" in prefs and "name" in prefs["profile"]:
                                profile["display_name"] = prefs["profile"]["name"]
                    except (json.JSONDecodeError, UnicodeDecodeError, IOError) as e:
                        print(
                            f"Error reading preferences for {profile['name']}: {e}")

            # Print available profiles
            if profiles:
                print("Available Chrome profiles:")
                for i, profile in enumerate(profiles):
                    display_name = profile.get("display_name", "No Name")
                    print(f"{i+1}. {profile['name']} ({display_name})")
                    print(f"   Path: {profile['path']}")
            else:
                print("No Chrome profiles found.")

            return profiles

        except (OSError, IOError, json.JSONDecodeError) as e:
            print(f"Error listing Chrome profiles: {e}")
            return []

    @staticmethod
    def image_to_base64(image_path: str) -> str:
        """
        Turns a image into base64
        """
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(
                image_file.read()).decode('utf-8')
            return f"data:image/png;base64,{encoded_string}"

